[{"title":"[WIP] BUG: Alternative attempt to fix sf_errstate","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nThis PR is an alternative to #20316. I just wanted to see what the minimal set of changes needed would be, and what would happen in CI with this. I'm doing this more to help me review #20316 than to propose it as a serious alternative.\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":[],"labels":["defect","scipy.special","C\/C++","Meson"]},{"title":"ENH: Generalised ufuncs in special","body":"This PR follows the [one](https:\/\/github.com\/scipy\/scipy\/pull\/20260) merged yesterday about overhauling the ufunc machinery. The purpose here is to extend that slightly (and only slightly) so that SciPy special can support generalised ufuncs for some of its existing functions. In particular, I am thinking of all the functions in _specfun.pyx. These include the Legendre polynomials, Airy and Bessel function zeros, and so on. No doubt others will follow.\r\n\r\nThese functions differ from their counterparts in that their kernels may take arrays as their inputs or outputs. A canonical example is matrix multiplication over stacks of matrices. Here it is a bit simpler, essentially we have special functions that return outputs with one or more additional dimensions. Usually these additional dimensions corrrespond simply to a value `n` in the input, so we have a function producing an array of shape `(n, ...)`. Currently, these functions in SciPy only work for scalar input, so given a value `x` the output is an array of size n. But there is no reason to have such a restriction, we could easily be given an array `x` of shape `(d0, ..., dm)` and get output of shape `(n, d0, ..., dm)`. Here we introduce such functionality.\r\n\r\nThis is actually quite a small change. We need to use NumPy's generalised ufunc machinery, namely `PyUFunc_FromFuncAndDataAndSignature` and pass an appropriate signature. Let's focus on `lpn`. With the new machinery, this is just:\r\n```\r\nPyObject *_lpn = SpecFun_NewGUFunc({special::lpn}, 2, \"_lpn\", \"docstring here\", \"()->(n),(n)\");\r\nPyModule_AddObjectRef(_special_ufuncs, \"_lpn\", _lpn);\r\n```\r\n\r\nThen, at the Cython \/ Python level, The Legendre polynomials `lpn` went from:\r\n```\r\ndef lpn(int n, double z):\r\n    \"\"\"\r\n    Compute Legendre polynomials Pn(x) and their derivatives\r\n    Pn'(x). This is a wrapper for the function 'specfun_lpn'.\r\n    \"\"\"\r\n    cdef double *ppn\r\n    cdef double *ppd\r\n    cdef cnp.npy_intp dims[1]\r\n    dims[0] = n + 1\r\n\r\n    pn = cnp.PyArray_ZEROS(1, dims, cnp.NPY_FLOAT64, 0)\r\n    pd = cnp.PyArray_ZEROS(1, dims, cnp.NPY_FLOAT64, 0)\r\n    ppn = <cnp.float64_t *>cnp.PyArray_DATA(pn)\r\n    ppd = <cnp.float64_t *>cnp.PyArray_DATA(pd)\r\n    specfun_lpn(n, z, ppn, ppd)\r\n    return pn, pd\r\n```\r\nto:\r\n```\r\ndef lpn(n, z):\r\n    \"\"\"\r\n    Compute Legendre polynomials Pn(x) and their derivatives\r\n    Pn'(x). This is a wrapper for the function 'specfun_lpn'.\r\n    \"\"\"\r\n\r\n    pn = np.zeros((n + 1,) + np.shape(z), dtype = np.float64)\r\n    pd = np.zeros((n + 1,) + np.shape(z), dtype = np.float64)\r\n    return _lpn(z, out = (pn, pd))\r\n```\r\nThis also has the additional benefit of eliminating the need for Cython in these functions.\r\n\r\nThe last change is at the C++ level. This one is perhaps mostly for @steppi to check out. We need a very simple struct or class that can act as the C++-equivalent of a view (pointer, shape, and strides). Although there is nothing yet in C++17, that is actually already accepted into later versions of the standard, namely [std::span](https:\/\/en.cppreference.com\/w\/cpp\/container\/span) in C++20 and [std::mdspan](https:\/\/en.cppreference.com\/w\/cpp\/container\/mdspan) in C++23. Since we only support C++17, we can either create our own versions of these or create another data structure that is equivalent. It is just pointer, shape, and strides. CuPy does exactly the latter.\r\n\r\nIn this PR I made my own `std::span`. The Legendre polynomials then become, in C++.\r\n```\r\ninline void lpn(double x, std::span<double> pn, std::span<double> pd) {\r\n    specfun::lpn(pn.size() - 1, x, pn.data(), pd.data());\r\n}\r\n```\r\nAnd the generalised ufunc is published as above.\r\n\r\nPutting this here now for thoughts.\r\n\r\n\r\n","comments":[],"labels":["enhancement","scipy.special","C\/C++","Cython"]},{"title":"BUG: signal: Fix scalar input issue of signal.lfilter","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nFix #11359 \r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nFixed the scalar input issue of `signal.lfilter` and add a test for it.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":[],"labels":["defect","scipy.signal"]},{"title":"MAINT\/DEV: rename `skip_if_array_api` to `skip_xp_backends`","body":"#### Reference issue\r\nDiscussion in gh-20285 @mdhaber \r\n\r\n#### What does this implement\/fix?\r\nRenames the marker and the fixture to avoid confusion.\r\n\r\n#### Additional information\r\nOpen to suggestions for a better name.","comments":["I can't immediately see the CI failure on other PRs - strange...","Doesn't look related, though. I'll give others a bit to suggest names, but LGTM (assuming the CI failure is indeed unrelated), and I can merge if you remind me : ) Might want the history to ignore this commit."],"labels":["maintenance","DX"]},{"title":"[WIP] BUG: special: Fix special function errstate","body":"The `special` module in `scipy` has functions for error handling, namely `geterr`, `seterr`, and `errstate`. They are analogous to the ones in NumPy. I've personally never used them, but in the process of doing [other work](https:\/\/github.com\/scipy\/scipy\/pull\/20260) for `special` it became clear these do not appear to be working correctly.\r\n\r\nWhat's the problem? Well, they rely on static variables in `sf_error.c` to be set. For that to behave properly, there must be only one copy of these variables. From what I see in the `meson.build` of `special`, `sf_error.c` is getting compiled in all over the place. That means there would be many copies of these state variables.\r\n\r\nI've never used this functionality from `special`, but I don't want it to hold up other efforts. So I'm proposing a fix in this PR. The way forward is to turn `sf_error.c` into a single shared library that everything else links to. This was not as easy as it sounds because `sf_error.c` currently has all kinds of Python C API calls. That's not really workable. So I adapted it to take in a generic callback, which we can set from Cython to replicate the same behaviour as before.\r\n\r\nAt the end of the day, this is not a feature that I use. So I'd greatly appreciate if someone that **does** use this feature can check this out.\r\n\r\ncc @steppi","comments":["Nice. I spent 30 minutes on this today and also discovered the Python C api calls blocking making it a shared library. If I understand correctly, I think you\u2019ve taken the right approach, but I haven\u2019t looked at the code yet. ","Getting the following error on CI.\r\n\r\n```\r\n~\/work\/scipy\/scipy\/tools ~\/work\/scipy\/scipy\r\n----------- All the test files were installed --------------\r\n----------- All the .pyi files were installed --------------\r\n..\/build\/scipy\/special\/libspecial_error.so: too many public symbols!\r\n00000000000014f0 T set_error\r\n0000000000001370 T sf_error\r\n0000000000001420 T sf_error_check_fpe\r\n00000000000011e0 T sf_error_get_action\r\n0000000000004080 D sf_error_messages\r\n00000000000011c0 T sf_error_set_action\r\n0000000000001200 T sf_error_set_callback\r\n0000000000001210 T sf_error_set_callback_fpe\r\n0000000000001220 T sf_error_v\r\n```\r\n\r\nThis seems to be part of SciPy's build process. Not sure what the deal is, but @rgommers I see you've run into it [before](https:\/\/github.com\/scipy\/scipy\/issues\/15996). ","@izaid, could we have a shared library just for the shared state, and then sprinkle in the Python API stuff separately?","> @izaid, could we have a shared library just for the shared state, and then sprinkle in the Python API stuff separately?\r\n\r\n@steppi I think that's what I've done here? There are no Python API calls in the shared library I've pushed. They come in through a function pointer. Let me know if you see differently.","That error is a check for symbol visibility. We've had issues with symbol clashes when multiple copies of SciPy were floating around (IIRC in an embedding scenario, can't remember exactly). So we introduced a linker script to hide everything except `PyInit_xxx` in extension modules (on top of Meson using `-fvisibility=hidden|inlineshidden` for extension modules by default): https:\/\/github.com\/scipy\/scipy\/blob\/e4655a65036585d2c91ff8fb336b0cbce01a3b3f\/meson.build#L119-L131\r\n\r\nWe are shipping exactly zero shared libraries right now; they can be problematic. If it's only used from multiple extension modules in a single directory it may work; if not then it's going to be a problem (especially on Windows, because of the lack of RPATH support). \r\n\r\nI'll have a closer look later. Once things work in regular CI we should run a full set of wheel builds on this PR (pushing a commit with `[wheel build]` in the first line of the commit message will do that).","> That error is a check for symbol visibility. We've had issues with symbol clashes when multiple copies of SciPy were floating around (IIRC in an embedding scenario, can't remember exactly). So we introduced a linker script to hide everything except `PyInit_xxx` in extension modules (on top of Meson using `-fvisibility=hidden|inlineshidden` for extension modules by default):\r\n> \r\n> https:\/\/github.com\/scipy\/scipy\/blob\/e4655a65036585d2c91ff8fb336b0cbce01a3b3f\/meson.build#L119-L131\r\n> \r\n> We are shipping exactly zero shared libraries right now; they can be problematic. If it's only used from multiple extension modules in a single directory it may work; if not then it's going to be a problem (especially on Windows, because of the lack of RPATH support).\r\n> \r\n> I'll have a closer look later. Once things work in regular CI we should run a full set of wheel builds on this PR (pushing a commit with `[wheel build]` in the first line of the commit message will do that).\r\n\r\nThanks @rgommers! Yes, that's what I thought, that the error was checking if there was exactly one visible symbol (i.e. is this a Python extension).\r\n\r\nAgain, my goal here is to fix what appears to be an existing issue. I wouldn't have proposed the shared library solution if I saw another easy way. And, if there is one, happy to discuss it. ","If it works in our CI and wheel builds, then it should be fine - all other methods to build\/distribute binaries are more sane and won't have issues with shared libraries.\r\n\r\nIt would be useful to make the symbol names more specific if possible. Something like `sf_error` (and definitely `set_error`) is so generic that it's likely to result in conflicts. Is it possible to prepend `scipy_` to the symbol names?","> If it works in our CI and wheel builds, then it should be fine - all other methods to build\/distribute binaries are more sane and won't have issues with shared libraries.\r\n> \r\n> It would be useful to make the symbol names more specific if possible. Something like `sf_error` (and definitely `set_error`) is so generic that it's likely to result in conflicts. Is it possible to prepend `scipy_` to the symbol names?\r\n\r\nSure, happy to do that. I'll do that once you let me know what needs to happen with the linking issues above.","You can skip all checking for this extension in the test script (`tools\/check_pyext_symbol_hiding.sh`).","> > @izaid, could we have a shared library just for the shared state, and then sprinkle in the Python API stuff separately?\r\n> \r\n> @steppi I think that's what I've done here? There are no Python API calls in the shared library I've pushed. They come in through a function pointer. Let me know if you see differently.\r\n\r\nSorry, the **just** was doing a lot of work there. I know you're doing that already. I meant, why not make a more minimal set of changes like here, https:\/\/github.com\/scipy\/scipy\/compare\/main...steppi:scipy:sf_error_state?expand=1?\r\n\r\nI still get linking errors though.\r\n\r\nUpdate: ah, sorry, I meant to write \"just\" before and my dyslexia made me not see that I didn't","> Sorry, the **just** was doing a lot of work there. I know you're doing that already. I meant, why not make a more minimal set of changes like here, https:\/\/github.com\/scipy\/scipy\/compare\/main...steppi:scipy:sf_error_state?expand=1?\r\n\r\nNevermind, that doesn't even work to make the error handling work. I'm out of my element here. At some point I'd like to understand what's going on though.","> > Sorry, the **just** was doing a lot of work there. I know you're doing that already. I meant, why not make a more minimal set of changes like here, https:\/\/github.com\/scipy\/scipy\/compare\/main...steppi:scipy:sf_error_state?expand=1?\r\n> \r\n> Nevermind, that doesn't even work to make the error handling work. I'm out of my element here. At some point I'd like to understand what's going on though.\r\n\r\nI think a working version of that is just the same thing ultimately? Shared library that needs to be linked to multiple locations.","fwiw, If I build with  `python dev.py build` on this branch and then `import scipy.special as special` I get\r\n\r\n```\r\nIn [1]: import scipy.special as special\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 import scipy.special as special\r\n\r\nFile ~\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/special\/__init__.py:777\r\n    773 import warnings\r\n    775 from ._sf_error import SpecialFunctionWarning, SpecialFunctionError\r\n--> 777 from . import _ufuncs\r\n    778 from ._ufuncs import *\r\n    780 # Replace some function definitions from _ufuncs to add Array API support\r\n\r\nImportError: libspecial_error.so: cannot open shared object file: No such file or directory\r\n```\r\n","> fwiw, If I build with `python dev.py build` on this branch and then `import scipy.special as special` I get\r\n> \r\n> ```\r\n> In [1]: import scipy.special as special\r\n> ---------------------------------------------------------------------------\r\n> ImportError                               Traceback (most recent call last)\r\n> Cell In[1], line 1\r\n> ----> 1 import scipy.special as special\r\n> \r\n> File ~\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/special\/__init__.py:777\r\n>     773 import warnings\r\n>     775 from ._sf_error import SpecialFunctionWarning, SpecialFunctionError\r\n> --> 777 from . import _ufuncs\r\n>     778 from ._ufuncs import *\r\n>     780 # Replace some function definitions from _ufuncs to add Array API support\r\n> \r\n> ImportError: libspecial_error.so: cannot open shared object file: No such file or directory\r\n> ```\r\n\r\nThat might have something to do with how it gets installed in `meson.build`, but I don't know. I don't normally use meson, I'm just getting used to it now with SciPy.","> I think a working version of that is just the same thing ultimately? Shared library that needs to be linked to multiple locations.\r\n\r\nSorry, I just don't understand why some of the changes you've made are needed due to my own ignorance of these things, and I don't understand why the naive thing I tried doesn't work. You're work shouldn't be blocked by me not being an expert in these matters though, but I should try to catch up.","Huh, \r\n\r\n```\r\nIn [2]: with special.errstate(all=\"raise\"):\r\n   ...:     special.gammaln(-1)\r\n   ...:\r\n```\r\n\r\ndoesn't raise for me locally on your branch either. This is installing with `pip install .` for each.","> Huh,\r\n> \r\n> ```\r\n> In [2]: with special.errstate(all=\"raise\"):\r\n>    ...:     special.gammaln(-1)\r\n>    ...:\r\n> ```\r\n> \r\n> doesn't raise for me locally on your branch either. This is installing with `pip install .` for each.\r\n\r\n@steppi You should now, I just tested. Forgot to put `-DSP_SPECFUN_ERROR` back in the `cpp_args` for `_special_ufuncs`.","> fwiw, If I build with `python dev.py build` on this branch and then `import scipy.special as special` I get\r\n> \r\n> ```\r\n> In [1]: import scipy.special as special\r\n> ---------------------------------------------------------------------------\r\n> ImportError                               Traceback (most recent call last)\r\n> Cell In[1], line 1\r\n> ----> 1 import scipy.special as special\r\n> \r\n> File ~\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/special\/__init__.py:777\r\n>     773 import warnings\r\n>     775 from ._sf_error import SpecialFunctionWarning, SpecialFunctionError\r\n> --> 777 from . import _ufuncs\r\n>     778 from ._ufuncs import *\r\n>     780 # Replace some function definitions from _ufuncs to add Array API support\r\n> \r\n> ImportError: libspecial_error.so: cannot open shared object file: No such file or directory\r\n> ```\r\n\r\nI don't see this on my Mac, what OS are you on?","> > Huh,\r\n> > ```\r\n> > In [2]: with special.errstate(all=\"raise\"):\r\n> >    ...:     special.gammaln(-1)\r\n> >    ...:\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > doesn't raise for me locally on your branch either. This is installing with `pip install .` for each.\r\n> \r\n> @steppi You should now, I just tested. Forgot to put `-DSP_SPECFUN_ERROR` back in the `cpp_args` for `_special_ufuncs`.\r\n\r\nNice! My version of this also works now; I'd also left that out. I'm far in the weeds of trying to get this to work on meson now. I'm on Linux.","> > > Huh,\r\n> > > ```\r\n> > > In [2]: with special.errstate(all=\"raise\"):\r\n> > >    ...:     special.gammaln(-1)\r\n> > >    ...:\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > doesn't raise for me locally on your branch either. This is installing with `pip install .` for each.\r\n> > \r\n> > \r\n> > @steppi You should now, I just tested. Forgot to put `-DSP_SPECFUN_ERROR` back in the `cpp_args` for `_special_ufuncs`.\r\n> \r\n> Nice! My version of this also works now; I'd also left that out. I'm far in the weeds of trying to get this to work on meson now. I'm on Linux.\r\n\r\nI suspect the issue is to do with the relative path for shared libraries, perhaps meson defaults to absolute path or something.  I'm looking at the meson docs. ","@steppi I think this is relevant, see https:\/\/github.com\/mesonbuild\/meson\/issues\/3038\r\n\r\nIs there a SciPy meson expert?","> @steppi I think this is relevant, see [mesonbuild\/meson#3038](https:\/\/github.com\/mesonbuild\/meson\/issues\/3038)\r\n> \r\n> Is there a SciPy meson expert?\r\n\r\nI\u2019ve been reading about rpath too. It\u2019s Ralf, but let\u2019s not bug him too much, he\u2019s busy enough as it is. ","@steppi and I have been looking into this, we're not sure, but we think the rpath isn't being set correctly on the shared_library. I see:\r\n\r\n```\r\nreadelf -d ~\/git\/scipy\/build-install\/lib\/python3\/dist-packages\/scipy\/special\/_special_ufuncs.cpython-310-x86_64-linux-gnu.so \r\n\r\nDynamic section at offset 0x25db0 contains 28 entries:\r\n  Tag        Type                         Name\/Value\r\n 0x0000000000000001 (NEEDED)             Shared library: [libspecial_error.so]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n 0x000000000000000c (INIT)               0x2000\r\n 0x000000000000000d (FINI)               0x19174\r\n 0x0000000000000019 (INIT_ARRAY)         0x26da0\r\n 0x000000000000001b (INIT_ARRAYSZ)       8 (bytes)\r\n 0x000000000000001a (FINI_ARRAY)         0x26da8\r\n 0x000000000000001c (FINI_ARRAYSZ)       8 (bytes)\r\n 0x000000006ffffef5 (GNU_HASH)           0x2f0\r\n 0x0000000000000005 (STRTAB)             0x948\r\n 0x0000000000000006 (SYMTAB)             0x318\r\n 0x000000000000000a (STRSZ)              991 (bytes)\r\n 0x000000000000000b (SYMENT)             24 (bytes)\r\n 0x0000000000000003 (PLTGOT)             0x27000\r\n 0x0000000000000002 (PLTRELSZ)           1320 (bytes)\r\n 0x0000000000000014 (PLTREL)             RELA\r\n 0x0000000000000017 (JMPREL)             0x1300\r\n 0x0000000000000007 (RELA)               0xeb0\r\n 0x0000000000000008 (RELASZ)             1104 (bytes)\r\n 0x0000000000000009 (RELAENT)            24 (bytes)\r\n 0x000000006ffffffe (VERNEED)            0xdb0\r\n 0x000000006fffffff (VERNEEDNUM)         4\r\n 0x000000006ffffff0 (VERSYM)             0xd28\r\n 0x000000006ffffff9 (RELACOUNT)          37\r\n 0x0000000000000000 (NULL)               0x0\r\n```\r\n\r\nI wonder if we just need to inject `.\/` before `libspecial_error.so` somehow.","That should fix it. I needed to add the `install_dir` for the shared library."],"labels":["scipy.special","C\/C++","Cython","Meson"]},{"title":"DOC: add docs on how to debug linear algebra related issues","body":"We have a lot of bug reports where it's not clear if it's a SciPy issue or a bug in a BLAS\/LAPACK library. This guide should help with:\r\n\r\n1. Diagnostics: what library (including version and build options) is being used?\r\n2. Document ways to switch between BLAS libraries without having to rebuild SciPy, in order to figure out if a problem shows up for only one or for all BLAS libraries (in the latter case, it's much more likely it's a SciPy bug).\r\n3. Once it looks like an upstream BLAS library bug, how to write a standalone reproducer for it, so the bug can be reported upstream in a constructive manner.","comments":[],"labels":["scipy.linalg","Documentation","DX"]},{"title":"BUG: sparse: Restore random coordinate ordering to pre-1.12 results","body":"#### Reference issue\r\nFixes gh-20027.\r\n\r\n#### What does this implement\/fix?\r\nThis restores the coordinate ordering that we used to return in version 1.11, which was inadvertently changed when we overhauled the sparse array creation functions for 1.12.\r\n\r\n#### Additional information\r\nWe don't 100% guarantee that the random creation functions will produce exactly the same results from version to version, but this was a simple enough fix that I think it's worth maintaining consistency.\r\n\r\nDemo of the new behavior with this PR (see gh-20027 for reference):\r\n\r\n```\r\nIn [3]: x = ss.random(3, 3, density=0.7, random_state=1)\r\n\r\nIn [4]: print(str(x))\r\n  (2, 2)        0.23608897695197606\r\n  (2, 0)        0.3965807272960261\r\n  (0, 2)        0.3879107411620074\r\n  (1, 2)        0.66974603680348\r\n  (1, 0)        0.9355390708060318\r\n  (0, 0)        0.8463109166860171\r\n```\r\n","comments":[],"labels":["defect","scipy.sparse","backport-candidate"]},{"title":"BUG: sparse.block_array incorrectly throwing dimension error when dense arrays passed","body":"### Describe your issue.\n\n`sparse.block_array` throws an error saying `\"blocks must be 2-D\"` when all blocks are 2d numpy arrays.\r\n\r\n\n\n### Reproducing Code Example\n\n```python\nfrom scipy import sparse\r\nimport numpy as np\r\n\r\n# This occurs regardless of whether there's a single column\r\nblocks = [np.ones((5, 1)), np.ones((3, 1)) * 2, np.ones((4, 1)) * 3]\r\nassert all([x.ndim == 2 for x in blocks])\r\n\r\nsparse.block_diag(blocks)\r\n\r\nsparse.block_array(blocks)\n```\n\n\n### Error message\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[1], line 9\r\n      5 assert all([x.ndim == 2 for x in blocks])\r\n      7 sparse.block_diag(blocks)\r\n----> 9 sparse.block_array(blocks)\r\n\r\nFile \/mnt\/workspace\/mambaforge\/envs\/scanpy-dev\/lib\/python3.11\/site-packages\/scipy\/sparse\/_construct.py:891, in block_array(blocks, format, dtype)\r\n    848 def block_array(blocks, *, format=None, dtype=None):\r\n    849     \"\"\"\r\n    850     Build a sparse array from sparse sub-blocks\r\n    851 \r\n   (...)\r\n    889 \r\n    890     \"\"\"\r\n--> 891     return _block(blocks, format, dtype)\r\n\r\nFile \/mnt\/workspace\/mambaforge\/envs\/scanpy-dev\/lib\/python3.11\/site-packages\/scipy\/sparse\/_construct.py:898, in _block(blocks, format, dtype, return_spmatrix)\r\n    895 blocks = np.asarray(blocks, dtype='object')\r\n    897 if blocks.ndim != 2:\r\n--> 898     raise ValueError('blocks must be 2-D')\r\n    900 M,N = blocks.shape\r\n    902 # check for fast path cases\r\n\r\nValueError: blocks must be 2-D\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.13.0rc1 1.26.3 sys.version_info(major=3, minor=11, micro=7, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= ZEN MAX_THREADS=64\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= ZEN MAX_THREADS=64\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.9\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-3p3v09un\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp311-cp311\/bin\/python\r\n  version: '3.11'\n```\n","comments":[],"labels":["defect","scipy.sparse"]},{"title":"BUG: sparse: sparsetools uses index array dtype for shape scalars","body":"### Describe your issue.\n\nOur templated functions in sparsetools uses the same integer type for both scalars (like the array shape) and index arrays. But this means we can't produce a CSR array with 2^32 rows, even though none of the actual indices exceed int32max.\r\n\r\nThis only arises when using int32 index arrays.\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nimport scipy.sparse as ss\r\n\r\na = ss.coo_array((2**33, 2))\r\na.coords = (np.arange(3, dtype=np.int32), np.zeros(3, dtype=np.int32))\r\na.data = np.array([3,4,5])\r\na.tocsr()  # raises ValueError because a.shape[0] doesn't fit in int32\n```\n\n\n### Error message\n\n```shell\n...\/scipy\/sparse\/_coo.py:366, in _coo_base._coo_to_compressed(self, swap)\r\n    363 indices = np.empty_like(minor, dtype=idx_dtype)\r\n    364 data = np.empty_like(self.data, dtype=self.dtype)\r\n--> 366 coo_tocsr(M, N, nnz, major, minor, self.data, indptr, indices, data)\r\n    367 return indptr, indices, data, self.shape\r\n\r\nValueError: could not convert integer scalar\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nIn [27]: scipy.__version__\r\nOut[27]: '1.14.0.dev0+586.235602c'\n```\n","comments":["Here's an example of the kind of signature I'm talking about: https:\/\/github.com\/scipy\/scipy\/blob\/101a5245d7d0c04c95a8be642b694284521d72c6\/scipy\/sparse\/sparsetools\/coo.h#L33-L42\r\n\r\nIn this case, I think `n_row` and `n_col` should use the native pointer-sized integer type (`size_t`, probably)."],"labels":["defect","scipy.sparse"]},{"title":"MAINT\/DEV: `unuran`, io.matlab: `pragma diagnostic pop could not pop` warnings","body":"Wall of warnings like\r\n\r\n```\r\nscipy\/io\/matlab\/_streams.cpython-310-darwin.so.p\/_streams.c:6214:28: warning: pragma diagnostic pop could not pop, no matching push [-Wunknown-pragmas]\r\n    #pragma GCC diagnostic pop\r\n                           ^\r\n...\r\nscipy\/stats\/_unuran\/unuran_wrapper.cpython-310-darwin.so.p\/unuran_wrapper.c:27497:26: warning: pragma diagnostic pop could not pop, no matching push [-Wunknown-pragmas]\r\n  #pragma GCC diagnostic pop\r\n                         ^\r\n```\r\n\r\non `python dev.py build`.","comments":[],"labels":["maintenance","DX"]},{"title":"DOC: Clarify handling of multidimensional inputs in scipy.signal.freqz","body":"#### Reference issue\r\nCloses #17387\r\n\r\n#### What does this implement\/fix?\r\nThis pull request addresses several documentation mistakes in the `scipy.signal.freqz` function. The changes include clarifications of parameter descriptions and accurate information regarding the handling of multidimensional inputs.\r\n\r\n### Changes Made\r\n- Added clarification regarding the compatibility of shapes for broadcasting when `b` and `a` have dimensions greater than 1.\r\n- Revised the examples to demonstrate proper usage of multidimensional coefficient arrays.\r\n- Fixed formatting issues and improved readability throughout the documentation.\r\n","comments":[],"labels":["scipy.signal"]},{"title":"TYP: special: add type stubs to `_basic.py`","body":"#### Reference issue\r\n#11749 \r\n\r\n#### What does this implement\/fix?\r\nAdding the stubs for `_basic.py` and enable the type cheking for `test_basic`","comments":["thanks @JPena-code, here is what MyPy is saying in CI:\n\n```\n\n\ud83d\udcbb  mypy.api.run --config-file \/home\/runner\/work\/scipy\/scipy\/mypy.ini scipy\nscipy\/special\/tests\/test_basic.py:46: error: Module \"scipy.special._basic\" has no attribute \"_FACTORIALK_LIMITS_64BITS\"  [attr-defined]\nscipy\/special\/tests\/test_basic.py:46: error: Module \"scipy.special._basic\" has no attribute \"_FACTORIALK_LIMITS_32BITS\"  [attr-defined]\nscipy\/stats\/tests\/test_resampling.py:1140: error: Unsupported operand types for ** (\"_SupportsArray[dtype[Any]]\" and \"int\")  [operator]\nscipy\/stats\/tests\/test_resampling.py:1140: error: Unsupported operand types for ** (\"_NestedSequence[_SupportsArray[dtype[Any]]]\" and \"int\")  [operator]\nscipy\/stats\/tests\/test_resampling.py:1140: error: Unsupported operand types for ** (\"str\" and \"int\")  [operator]\nscipy\/stats\/tests\/test_resampling.py:1140: error: Unsupported operand types for ** (\"bytes\" and \"int\")  [operator]\nscipy\/stats\/tests\/test_resampling.py:1140: error: Unsupported operand types for ** (\"_NestedSequence[Union[bool, int, float, complex, str, bytes]]\" and \"int\")  [operator]\nscipy\/stats\/tests\/test_resampling.py:1140: note: Left operand is of type \"Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]]\"\nFound 7 errors in 2 files (checked 858 source files)\n\n```","Thanks @lucascolley, I added those two private constants that were missing. Additionally, I reviewed other PRs and saw that I also needed to modify the `mypy.ini` file, also after doing some research I saw that I was incorrectly using `ArrayLike` from `numpy.typing` and that the correct one is `NDArray`"],"labels":["scipy.special","static typing"]},{"title":"ENH: scipy.signal.sos2tf(sos)","body":"### Is your feature request related to a problem? Please describe.\n\nI saw a gap in the scipy.signal.sos2tf(sos) function.\r\nIn the sos2tf function, there is no option for setting the overall system gain G, and I suggest adding another optional argument, \u2018G\u2019, specified as a real array that multiplies the coefficients b.\r\nI will be more than happy to implement it\r\n\n\n### Describe the solution you'd like.\n\n returns the transfer function coefficients of a discrete-time system described in second-order section form by SOS with gain G\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["See also https:\/\/github.com\/scipy\/scipy\/issues\/17789 , which is loosely related.\r\n\r\nI'm not sure this is that useful: it's not difficult to do already (sos2zpk, adjust k, zpk2sos), and introduces asymmetry (we aren't going to add a gain output to, say, tf2sos)."],"labels":["enhancement","scipy.signal"]},{"title":"ENH: Allow Akima extrapolation","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\nMinor change. Closes #20247 \r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nWe add the extrapolation parameter to the construction; passing it to the parent classes. We keep False as the default as that is the behavior of the function before this change.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":["Apologies @j-bowhay. Added the missing unit test(s). If there is a missing test, please let me know so I may add it."],"labels":["enhancement","scipy.interpolate"]},{"title":"DOC: stats: Convert sampling tutorial to MyST-md","body":"#### What does this implement\/fix?\r\nThis PR converts the [Universal Non-Uniform Random Number Sampling in SciPy](https:\/\/scipy.github.io\/devdocs\/tutorial\/stats\/sampling.html) tutorial to be in [MyST Markdown](https:\/\/mystmd.org\/) format instead of rst. This is possible by using [MyST-nb](https:\/\/myst-nb.readthedocs.io\/en\/latest\/authoring\/basics.html#myst-markdown)\r\n\r\n\r\n#### Additional information\r\nUnfortunately, I haven't figured out how to display a \"download this file as a notebook\" button, but I will keep investigating.\r\n\r\ncc @mdhaber ","comments":["See https:\/\/github.com\/scipy\/scipy\/issues\/20296 for the docs build failure.","Rendered docs link in CircleCI: https:\/\/output.circle-artifacts.com\/output\/job\/5defee78-e493-487b-826d-be2ad0c948db\/artifacts\/0\/html\/tutorial\/stats\/sampling.html","Looks good! We discussed that we'd merge this after investigating:\r\n- use of jupyterlite for execution in browser\r\n- offering a \"download as ipynb\" link\r\n- use of $ to begin\/end math\r\n- removal of >>> from executable code"],"labels":["scipy.stats","Documentation"]},{"title":"BUG: scipy.optimize.nnls fails with exception","body":"### Describe your issue.\r\n\r\nThe method `scipy.optimize.nnls` fails in certain input matrices. The problem seems to be in the implemenation `_nnls` where there is the line\r\n```\r\nwhile (iter < maxiter) and (s[P].min() <= tol):  # C.1\r\n```\r\nThe matrix `s[P]` can be empty, in which case the `.min()` raises an error. Maybe we can replace `s[P].min()` with `s[P].min(initial=0)`, but I am not sure about the algorithm convergence with that modification.\r\n\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.optimize import nnls\r\nw=b'\\xc4\\\\\\x98\\xf3\\xa8a\\xd5?\\xb0{ \\x9a6\\x82\\xbc?\\'\\xdd\\xaf\\x90\\xfb(\\xa9?\\xca\\xa7\\x15\\xbcy\\x01\\xa3?\\xb0{ \\x9a6\\x82\\xac?\\xee\\xc2\\x03z\\x84M\\xc5?=YZM[g\\xbc?\\xd7<N\\xd2G\\x11\\xb9?\\xd3\\x90\\x913\\x92\\xef\\xc2?=YZM[g\\xdc?\\xe8o\\xbd\\x89+j\\xbc?\\xe8o\\xbd\\x89+j\\xbc?\\xea|\\\\\\x98\\xd2\\xce\\xc2?\\xee\\x13N\\xa7\\xa0O\\xd5?l\\xd67\\xbc\\x94\\xf9\\xf7?\\xe2x\\x7f7\\x0cO\\xb5?.\\xa1T\\x9fei\\xbc?\\xffy\\x8f\\xca\\x14\\x13\\xc9?sk8j\\xee\\xf0\\xe2?.\\xa1T\\x9fei\\x0c@\\xcd\\x9b\\xceJ\\xdb\\x0b\\xb1?\\x00YX\\'\\x18i\\xbc?\\x1b\\x90\\xfd\\x86\\x84W\\xcf?\\x0b\\xb2\\xa6\\xd3#\\x98\\xed?\\xeaFl\\x96\\xa1\\xbe\\x1b@ZI\\xdd\\xd1\\\\g\\xac?ZI\\xdd\\xd1\\\\g\\xbc?\\xc49\\xe3\\xde\\xf6\\xcc\\xd2?\\x04\\xf7e\\x9d\\x85M\\xf5?\\xe5\\xb5\\x12Q6\\xf7\\'@\\xf9\\x14\\xd2\\xa5\\xb8R\\xa8?\\xcdm\\xca\\x16\\x82`\\xbc?\\x01\\x911\\xf8\\xd4\\xe9\\xd5?m\\xa5\\xee!\\xda\\xf7\\xfc?\\x8f\\x9cD&\\xa7\\x023@5\\xe5\\xc6\\x0cs@\\xa5?G\\xdc\\xb3\\xbb\\xeeU\\xbc?,\\xd5&.\\xe7\\x01\\xd9?\\xda\\x92\"}\\xf4\\xe3\\x02@G\\xdc\\xb3\\xbb\\xeeU<@\\xb3E\\xa8&.\\xdb\\xa2?\\x8dh\\xfc9\\xc5H\\xbc?\\x9b7\\xf3\\xa0\\x12\\x15\\xdc?7\\xf8\\xechf\\xdd\\x07@o\\xf1\\x87h\\xce\"D@\"Xz\\x94(\\xf4\\xa0?\\xe4\\x92v\\xf7\\x98A\\xbc?\\xbb\\x85\\x94\\x16\\xf2+\\xdf?\\xad\\x83\\x1bW\\xffn\\r@r\\xcb\\xa9a\\x0f\\x98K@o\\x93s\\xed\\x1b\\xcd\\x9e?{\\x9cTD\\x04<\\xbc?\\x08\\r\\xea|\\x95!\\xe1?\\xf2\\xa7\\xba\\x05(\\xcb\\x11@2}\\xe8E\\x81YR@\\xe1\\xb0\\x9c7\\x07\\x12\\x9a?\\xf4\\x14?|2>\\xbc?\\xfc\\x8a\\xd9\\xc3\\x86@\\xe4?q\\x86\\x07p\\x14\\xdc\\x18@\\xda+\\x89\\xe88L^@\\x1b7\\x80\\xe3^\\x9c\\x96?\\xe2D`\\x9cvC\\xbc?\\x8e\\xd8\\xd5\\xc2\\x80b\\xe7?\\\\h\\xa0\\x7f\\x87\\x8f @\\xc2\\x92y\\x8b\\xd6Ig@\\x00\\xd6\\x91^\\x886\\x95?V\\x1dm\\xd3\\xb5H\\xbc?\\xa4i%\\xdb;\\xf6\\xe8?9\\xbeH\\xe2#\\xdb\"@V\\x1dm\\xd3\\xb5Hl@\\xcc\\xa2\\'\\xfa\\xc8\\xdf\\x92?2t;w\\xadO\\xbc?Z\\x90:>\\xee\\x1b\\xec?\\x0b*\\x9a\\\\:\\xe3\\'@y\\x13\">\\xb9\\'t@Q\\x93S7:1\\x90?\\xceA\\xd2\\xe0%V\\xbc?\\x80\\x11z\\xa6_i\\xf0?\\xc9\\xbb\\x18\\xc0wE0@\\xdaX\\x18\\xe1a\\x04\\x80@y\\xd6\\x99r\\xf5\\x91\\x8d?3\\x18~8\\x8bV\\xbc?\\xbb\\x87\\xd3l\\xc1\\xf9\\xf1?\\xab\\xda>\\xe5\\xf1\\x843@\\xc1\\xc3\\'\\xcbT\\x0b\\x85@\\x9fZ\\xad\\xa2Q\"\\x8a?\\xd7\\xcc;p\\xd8O\\xbc?\\x1aY)o.M\\xf4?\\xf7\\xa2t-\\x9d\\xeb8@\\x9d&f\\x8f\\'_\\x8e@\\xd5\\xc5\\x8b\\xb0Tg\\x87?\\x0c\\x8fSU\\x86G\\xbc?]\\xa2\\x8b\\x823\\x9e\\xf6?O\\x80\\x1a\\x1a\\xa8\\xf7>@6\\x03\\xbeAT\\x0c\\x95@\\xcc\\xd0\\xd8\\xcaZ\\x8c\\x84?\\x19\\x1f\\xea\\xd6\\xfc@\\xbc?\\x1fE\\x94m\\xe6\\xb6\\xf9?\\x0c\\xfd^B\\x13\\x08D@o\\xef\\xaa\\xca}\\xfc\\x9e@\\x8e\\xc2e\\xebhT\\x82?F\\xe1<\\xc0!B\\xbc?C\"1\\xb6\\xfe\\xd5\\xfc?s\\xed\\x80\\xbf\\x9b\/I@\\xe9\\xcd\\x11\\x10I\\xd7\\xa5@:H\\xa4\\xdc\\x91\\x87\\x80?\\x0f\\xa6\\x98\\x83\\xee<\\xbc?\\x9f\\x8f\\xae\\x1a+\\xee\\xff?\\xbb9Z\\xb2Y\\xe7N@z\\xb3b1P\\xb2\\xad@\\x9cl\\xca\\xbdWt}?\\xc0R7k):\\xbc?\\x82\\x00<\\x87\\xc0\\xe7\\x01@U\\x1b\/2eqS@x\\xc9\\x1a\\x1aA\\xf6\\xb4@\\xb88\\x94\\xda\\xb0\\x15z?\\xc7\\x92 B*B\\xbc?tN\\x18$_C\\x04@1\\x01\\x8a\\x87\\x92\\xdfX@t10\\x95zP\\xbe@\\x88\\x19@\\x8ev\\tw?\\xb8t\\xd9\\x0e\\xf7P\\xbc?1\\x89\\x06~\\xb6\\t\\x07@,t\\xb4\\xf9\\t\\x0b`@\\xa4\\x88K\\xcbC\/\\xc6@\\x02\\xfa\\xce(\\x07\\xa0t?\\xc3\\x97\\x1c\\xd8\\t\\\\\\xbc?\\x03\\x7f\\xaf\\x06\\x85\\xcf\\t@\\x99E\\xb8\\xfa@\\x1bd@\\xa8\\x0b\\xd5\\x83(\\x1a\\xcf@\"\\x87a\\xec]nr?\\xaa\\x05\\xc1!&j\\xbc?}\\x0f\\xf3{\\xd4\\xfe\\x0c@\\xe13\\xc0QFSi@\\xfd\\xac\\xe4\\xfa6\\xf6\\xd5@2\\x19\\x9al\\r{p?\\x92\\xcb\\x1f\\xe1q\\x7f\\xbc?\\xa7J\\x81\\x8a\\xfdN\\x10@6\\xb1v\\xa5\\x10\\xf4o@]r\\xf00d\\x14\\xdf@\\xa2\\x1d\\x08\\x1a\\xce\\x82m?\\xb6\\xdc7\\xa9\\xb7\\x96\\xbc?TeM\\x97\\xecT\\x12@\\xa9\\x01keU\\x1ft@\\xcf\\x9e\\x87\\x11&\\xee\\xe5@\\xb6h+\\xe2\\t:j?\\x88z_\\xcfz\\xaf\\xbc?\\x1f\\x83\\x84\\xc4c\\xc4\\x14@?\\xfa\\xee\\x7f\\xb1\\xbcy@\\xeb\\rkh.\\xab\\xef@K\\xf2\\xcc\\xdf\\xc8tg?|\\x94\\x16\\x93\\xe1\\xd4\\xbc?\\xc9fo\\xf9\\tu\\x17@\\x0b\\xeb\\xb5M\\xc7U\\x80@\\x05\\x8fu\\x99\\x9d\\x96\\xf6@\\xc8\\xd1\\xd0;\\xfb\\x0be?s ?ry\\xf0\\xbc?\\xc1\\xbbVK\\x9dV\\x1a@\\x81\\xc1\\x7f\\x1a~\\x84\\x84@T\\x9f\\xfd\\x10\\xf3\\xbc\\xff@\\xb9\\xea\\xcd\\x8e\\xf6\\xd5b?4\\x1f(\\x1c\\xdc\\t\\xbd?\\xf7\\x02M(\\xce\\xa1\\x1d@%m\\x18\\x03\\x9f\\xe1\\x89@\\xa6.\\xaf\\xe4\\xa7q\\x06A9\\xc0\\xcd\\xd4\\xa8\\xc0`?\\xb8\\xeb\\x85\\x07{$\\xbd?]\\xa7\\xdb\\xcd\\'\\xc7 @\\xb0pS\\xa9\\x19\\x89\\x90@\\xc5%0\\x9c\\'.\\x10AN}>%\\xec\\x00^?\\xb5\\xdc\\xf0\\xfe\\xe58\\xbd?\\xed\\xc5\\x95G\\xb5\\xd6\"@\\x9b\\x19\\x87\\x96A\\xca\\x94@\\x8f\\x07o\\xdd\\x9c\\xc7\\x16AGG\\x10\\x8dG\\xcfZ?\\xf7\\xcdIB\\xb6R\\xbd?JH\\x90;\\x90:%@\\xfa\\x97\\x99\\xe6%O\\x9a@\\x01\\x7f_\\xd1\\xb1\/ A\\xf6\\x96\\x05\\xdc\\x19\\xebW?94\\x1c\\xc9Of\\xbd?Y\\xde\\x07>\\\\\\xeb\\'@\\xc1Z\\x91\\xc9,\\xa8\\xa0@\\x7f\\xff\\xba\\xee\\x8d\\x08\\'A\\\\O\\xa8\\x87gFU?G\\x83F\\x19\\x8ay\\xbd?\\xdf\\x9c\\xb6\\xc2b\\x07+@cC!>\\x087\\xa5@\\x04\\xecj\\xecb\\x880A\\xd9\\xbc\\x93\\xc1\\x92\\x07S?\\xc7\\xf7\\xc7q\\x16\\x89\\xbd?5\\xc4\\xd8-\\xc4W.@Sg.Id\\xae\\xaa@5\\x03\\xe4\\x8d@K7AF\\xcd\\xc2k\\x0b\\xf4P?~\\xe3\\xafBy\\x94\\xbd?\\xd4\\x19\\x94_\\xaf\\x141@K\\x11Y\\x93i\\xe2\\xb0@8*\\xdd\\x1d6\\x92@Ab\\xf0\\xda?1AN?\\xb4`\\x86\\x8e\\xd5\\x9f\\xbd?\\xda.\\xe1b633@\\xb4 \\xe7zYM\\xb5@\\x07\\x9ab\\x914wGA\\xc1K\\xd7\\xc6\\xe0\\xf6J?r)\\t\\x9b\\xf3\\xb3\\xbd?\\xf0\\xa2\\x9b?H\\xa85@\\x8bOY44\\x08\\xbb@\\xea\\xd6\\xefX\\xf5\\xbfPA\"4\\x01\\x1b\\r\\x18H?\\xd2\\'\\x126\\xc0\\xcd\\xbd?5\\x93p\\xeb\\xf8f8@\\xe5\\x8b\\xa6\\xd5\\xe7\\x19\\xc1@-\\x04\\x11\\xdfL\\xccWAD\\xe1\\xd2\\xda\\xcc\\xa2E?BAM\\xe0d\\x16\\xbe?\\xb7pM\\r\\xc7\\xb1;@AF\\x81k\\xb1\\xd1\\xc5@4\\xdfW\\x08G\\x11aA6\\n+J\\x17sC?D#\\x07\\xff\\xdcV\\xbe?\\xce;y\\x8dTS?@\\xd7\\x15\\xcf\\xc5\\x17\\xaf\\xcb@\\x8a\\xd4\\xf0;\\x82JhA*$J\\x07\\xfcfA?\\xb6\\xf0\\x1b\\n\\xd3\\x7f\\xbe?\\tY\\xd2W\\xc6\\xb0A@\\xf5b\\xc9\\xa5\\xc0\\x90\\xd1@\\xeehp\\xeb\\x13QqA{<\\x98c\\x8c\\xf5>?\\x89\\xdb\\t2\\xb6y\\xbe?\\xe4\\xe6\\x03\\x85Q\\xdbC@\\xf8\\x81\\xe5\\x14\\xdc%\\xd6@n)$9\\xed\\x86xA\\xea\\xf2\\xd4q\\xc3i;?.Z\\xfcO\\x0eW\\xbe?\\xba\\x13x\\xaf\\xfb9F@mk \\x93\\xa4\\xdf\\xdb@\\xe2N\\x18\\x15[Z\\x81A\\xc2wE\\xbc\\x82u8?\\xecar\\x91\\x1fj\\xbe?\\xb1\\x87\\xa5\\xab\\x8e\\x08I@}Oq\\\\\\xc8\\xa2\\xe1@T\\x03R\\x8a\\xd8\\xab\\x88Atr\\x17l\\x8d\\xba5?m\\xba\\xe0feT\\xbe?\\x8f\\xff\/\\x8d\\x9d\\x05L@\\xe0\\xc6_\\x96\\xe9(\\xe6@&3\\x11g\\x1ff\\x91A\\xfa\\xe2\\x13[\\xe8M3?!\\x89\\x91\\x91\\xe7I\\xbe?\\xec.5\\xcd\\xbatO@y\\xd0)G1\\xf6\\xeb@mw\\xbe\\x1d\\x98\\xad\\x98A\\xe2@s\\x9b\\xb8<1?N\\x0b\\xdc9\\xf9]\\xbe?k)\\x86)\\x97\\xb4Q@W\\xd5w]\\xf2\\xab\\xf1@:\\xb0\\xff\\xbc\\x14\\x83\\xa1A\\xcbW;\\xd7!\\xf5.?\\xe0\\x0e\\xa8\\'\\x94\\xa2\\xbe?\\xbeU\\xbd\\x91\\xfb\\x10T@\\xc2\\x9350\\x08\\x80\\xf6@\\x7f\\xa5\\xab\\x1d\\x89\\x0c\\xa9A\\xa8$\\xa5\\x84\\x1a\\xbe+?<\\x87ix\\xc4\\xd4\\xbe?p\\xb5\\xf8\\xa6\\xe2\\xadV@`\\x0e\\xb2\\x18\\xf9\\x8e\\xfc@\\x8d\\x0e8\\'@\\xda\\xb1A\"\\xed\/\\x86\\x0c\\x96(?\\xc6-\\x8f\\xa3]\\xb3\\xbe?\\xe3\\xfa\\xfe\\x97\\xef_Y@:\\x04\\x0c\\x18\\x82\\xf3\\x01Ao\\xe1\\x07\\xa1\\xb37\\xb9AK3\\xea\\x01P\\xd4%?\\xedm\\xaf2\\xbc\\x9c\\xbe?X9n\\x988j\\\\@)f\\x7f\\\\M\\x93\\x06A\\xa6\\xbe&\\xd6\\xd1\\xce\\xc1A\\xb2S-fZJ#?\\xddi\\xfblzd\\xbe?k\\xdb\\x88\\x15\\xdb\\xb1_@9)i\\xb6tJ\\x0cAir(\/|\\x12\\xc9A\\xfa\\x9b\\x16l\\xf3\\xdd ?\\xc5\\x05\\xd6f\\x8f\\xdb\\xbd?9\\xb6@\\x99\\xf6}a@\\x08\\xff\\xe1;\\xe5\\x8a\\x11A\\xd9\\x7f(Q\\xb5w\\xd1A\\xf1\\xab\\xdbmXm\\x1d?o\\xe26i4A\\xbd?\/^\\xd0\\x15\\x1c@c@\\xe2\\xdf\\x04\\xf7E\\xaf\\x15A\\xa5\\xa3\\xd4\\xef\\x96@\\xd8A\\x9bT#\\x17\\x07\\xe6\\x19?\\xa2\\xf2\\xaa\\xea\\xaa\\xea\\xbc?\\xaa\\xb9\\xd5\\xe7\\xe3^e@\\x82]e\\x0bl\\t\\x1bA\\xa3A\\x88\\xe3\\x02\\xfb\\xe0A\\x10\\x19\\x19x\\xe9\\xd6\\x16?F\\'\\xd6;I\\xa7\\xbc?\\x1d\\xc5\\x07\\xb98\\xcbg@tN6\\x81j\\xe9 A\\x99\\xf5}!p\\xde\\xe7A6\\x08\\xf3\\xd4\\xf0\\x17\\x14?\\xd2y\\x15\\xa0\\xbcP\\xbc?\\x9a\\xa7\\xb9=:ij@\\xd0W\\x937\\xeb\\x15%A\\x1f\\x7f\\xb1\\xce\\xbb\\xb6\\xf0A\"d\\xd1\\x03 \\xa7\\x11?\\xc17\\x0bn\\xcf\\xf1\\xbb?\\xa2t\\xb4\\xd5\\xc9Gm@\\x98\\x87\\xe3\\xd0\\x8fB*A\\x19\\xdaL\\xf7\\x0cb\\xf7Acr\\xfd\\xbf\\x90\\x08\\x0f?\\xf0\\xba\\xad\\xae\\xb2\\x96\\xbb?\\xba\\xaf0s\\xed;p@\\xf2\\n\\xec\\tSZ0ARZ\\x84\\xdc\\xd5Z\\x00B7x\\xf54\\x1a0\\x0b?c&9,\\n\\'\\xbb?q\\xca5\\x1d\\xf3\\xf2q@<\\x99U_\\xb6O4AJ\\x8c\\xdcF\\x0f\\xd2\\x06Bw\\xd4\\xf1\\xcfc\\x9c\\x07??\\x1a\\x96%?}\\xba?\\x936p\\xe8\\xbd\\xabs@bH\\xdb\\x11\\x8c\\x019AP\\xa0\\xd9\\xe5\\xaf\\x8f\\x0fB\\xdd8\\x0c\\t\\xcex\\x04?*\\x07\\'\\xa8\\xbe\\xcc\\xb9?2\\xd0\\xcf\\xed{\\x85u@~\\xab\\x8e|p\\xbb>A>;\\xac\\xdb+\\xc9\\x15B\\ng\\xee\\x0ek+\\x02?\\x1a\\xb7B\\xab5\\xb8\\xb9?(\\x90\\x11\\x7f\\x0e\\x19x@\\x17\\x84M VSCA\\x16\\xf18\\xeby\\xc6\\x1eB\\xcf9W\\xd7,\\x82\\x00?-\\xf4\\xc2xU@\\xba?0\\x18\\xc14r\\xa1{@`\\x0f\\xb31|\\xe4HA\\xf5e\\x84\\x86\\xffC&B*\\xc9\\xe4\\x94\\xe5<\\xff>\\xec\\x966\"i\\xe6\\xbb?\\x82d\\xaf_t~\\x80@\\xc9\\x04\\xd0\\xf91\\xb1PA\\xeem\\x1cQ\\xab\\xc50B\\xcd\\t\\xa9\\xc0\\x98+\\xfe>\\xaa\\x96I\\x15\\xbdD\\xbe?\\xa8Eax\\xa3\\x19\\x84@\\xbc\\xf2\\x1e\\x80s\\xd9VA\\x18\\xeej\\xcc\\r\\xca9B\\x93\\xe3\\xaa\\xa29O\\xfd>\\x84|\\x82b2\\x84\\xc0?\\x82\\xbaW|f\\xa4\\x88@\\xd5\\x0c\\n\\xc0\\x06x_A\\x11\\x9f\\x80o,\\xf3CB{\\xb9\\xc1\\x99\\xfc%\\xfc>!D\\x1f)\\x81\\xd1\\xc1?]\\xff\\xe2\\x80\\t\\xdd\\x8d@\\x07\\xe0<\\xaa\\x82keA\\x1c\\xa0\\xb3$\\x16\\x82NB\\xbas\\xa3\\xa5n5\\xfa>ba7U\\xce\\xa2\\xc2?\\x0c\\xd1\\xfe\\x17\\xcf\\x8a\\x91@\\xe0\\xcbY\\xb6pDlA\\xf8\\xba\\xce\\x8d\\xbb\\x9cVB\\x91W\\xf2\\xed_\\x82\\xf8>\\xe6!?\\xea\\xa2\\x93\\xc3?4h\\xd8cC\\xb3\\x94@\\x9b\\xad1\\xd2\\x1b\\xbcrA\\xea\\xd7\\xeeW\\xbc\\xd5`Bs\\x8b\\x97\\xd5t\\x0e\\xf5>,L\\xf0\\x99\\xa0\\xe4\\xc2?\\xb19u:\\xf4p\\x96@\\x00\\x1e\\xab\\x1b\\xbe\\xd0vAk\\xcd\\xdb5\\xbb\\x07gB\\x1a\/\\xcb\\xa6]\\x98\\xf2>\\xd2\\xfb\\x05%#\\xbe\\xc2?U\\xdd\\x16\\x0f=\\x02\\x99@uX3]\\xdf\\x8f|A\\xec\\xde\"\\x07\\x901pB\\xe5\\xc9\\xe8\\x1c\\xe6\\x0b\\xf0>\\xc3\\xd20\\x9b#+\\xc2?\\xdb;\\x13\\x03~;\\x9b@:Ps\\x9f\\xfaw\\x81A\\x12\\x8c\\x96vI@vB\\x0bU\\x8c\\x08\\x05\\x93\\xeb>\\xa4\\x9dkW\\x02\\x89\\xc1?\\x83OM\\x1e\\x1d\\x86\\x9d@\\\\#\\xf5=\\x14F\\x85A_\\x9f\\x82xlp~B\\tN\\x0f\\x98n\\x9e\\xe8>\\xde\\xee\\xdf@-\\x96\\xc1?g\\x90\\xc1\\xd5\\x96\\xa1\\xa0@X!\\xf2\\xd0n\\xec\\x8aA\\x84V\\xb3^\\x1a\\xa3\\x85B\\xcf\\x1cc\\xa85\\x96\\xe5>\\x11\\xf3\\xc1\\x1e\\x81R\\xc1?\\x0eX\\xcf\\xe6\\xbef\\xa2@\\xa3\\x8d%\\xf1;\\xbb\\x90A\\x87\\xa3\\xd6\\xabm5\\x8eBC\\x8c?\\'\\x01L\\xe4>\\x02\\x9b#*\\xb8K\\xc2?\\xe03\\xc0K\\x10\\xd5\\xa5@a2G}\\x89L\\x96A\\x14\\xe2\\xfdp\\xef\\x9c\\x96B*\\xcc\\xba\\xf4S\\x9f\\xe8\\xbe\\xbe\\xe19\\xa00L\\xc2?\\xf8u\\x9d\\x01\\xf6\\x86\\xa8@\\x89-\\xaa\\x00\\x15$\\x9cA\\xb2\\xf8kM\\xc3\\x86\\x97\\xc2\\xac\\x11?\\x81\\xae7\\xf6\\xbe\\xd6&\\x1b>0\\xe8\\xc2?N\\xe7\\x05\\x08:x\\xac@+s?\\xc9\\xa8X\\xa2A%\\x98\\x01\\x07p\\x90\\x91\\xc2\\xbb\\xd5\"\\xea\\x14\\x95\\x16\\xbf\\x1e\\x8bX\\x84\\t\\x87\\xc4?\\xd8V\\x96\\xf9b\\\\\\xb1@`\\x0f\\xcbo\\x82\"\\xa9A\\xeef\\x8f\\xfc!\\xb4y\\xc2\\xb5\\xfdQ\\xbc\\xfb\\x1d\\n?\\xa4\\t\\xdb~b\\x8c\\xc4?~\\xad\\xc2\\xe9\\x8e\\x85\\xb3@ z\\xf3@\\xaa\\xbf\\xafA\\xcd\\x85}s\\x00\\x1a\\x8cBu\\xe9\\r\\xa3\\x0ff\\xf3>\\xf2\"~(#\\xc6\\xc3?\\xeb\\xba\\xc04J\\x1a\\xb5@`\\xc6%T\\xafF\\xb3A\\xd1\\xbaDb\\xec\\x1a\\xa6B$\\x80\\x95\\x9bu\\x9c\\r\\xbfz#\\xe2(g\\x00\\xc2?Y \\x0b\\x15\\xa2\\x94\\xb5@\\x00\\xae\\xfdq\\xd6$\\xb6A6a\\xa4\\xe3\\x19J\\x8e\\xc2I]q\\xdb7i\\xfd>w\\xbb\\x968\\xf1\\xdf\\xc0?\\x8aS\\x1ffv\\xb9\\xb6@6v_\\xd4n1\\xbaA\\x0f~\\x86j3\\xe8\\xa0B'\r\nA=np.frombuffer(w).reshape( (89, -1))\r\nb=np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\r\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\r\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\r\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\r\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\r\n       1., 1., 1., 1.])\r\n\r\nA=np.random.rand( b.size, 5)\r\n\r\nsol, rnorm = nnls(A, b)\r\n```\r\n\r\n### Error message\r\n\r\nThe minimal example above generates the following exception\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n\r\n  File C:\\develop\\env312\\Lib\\site-packages\\spyder_kernels\\py3compat.py:356 in compat_exec\r\n    exec(code, globals, locals)\r\n\r\n  File c:\\projects\\untitled5.py:24\r\n    sol, rnorm = nnls(A, b)\r\n\r\n  File C:\\develop\\env312\\Lib\\site-packages\\scipy\\optimize\\_nnls.py:91 in nnls\r\n    x, rnorm, mode = _nnls(A, b, maxiter, tol=atol)\r\n\r\n  File C:\\develop\\env312\\Lib\\site-packages\\scipy\\optimize\\_nnls.py:139 in _nnls\r\n    while (iter < maxiter) and (s[P].min() <= tol):  # C.1\r\n\r\n  File C:\\develop\\env312\\Lib\\site-packages\\numpy\\core\\_methods.py:45 in _amin\r\n    return umr_minimum(a, axis, None, out, keepdims, initial, where)\r\n\r\nValueError: zero-size array to reduction operation minimum which has no identity\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=12, micro=2, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-6226b0as\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-77ntvnsh\\cp312-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.12'\r\n```\r\n","comments":["Thanks for the report. This should hopefully also be fixed by https:\/\/github.com\/scipy\/scipy\/pull\/20168. Your example could server as an additional test for that PR."],"labels":["defect","scipy.optimize"]},{"title":"BUG: 'ValueError: Jacobian is required for Newton-CG method' occurred despite setting jac='cs'","body":"### Describe your issue.\r\n\r\nDespite setting the `jac` parameter to 'cs', I'm still receiving a `ValueError` stating that a Jacobian is required for the Newton-CG method.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy\r\ndef rosenbrock(x):\r\n    x = np.array(x)\r\n    f = np.sum(100 * (x[1:] - x[:-1] ** 2) ** 2 + (1 - x[:-1]) ** 2)\r\n    return f\r\nn = 4\r\nx0 = [3] * n\r\nscipy.optimize.minimize(rosenbrock, x0, method='newton-cg', jac='cs')\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nFile C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710 in minimize\r\n    res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\r\n\r\n  File C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:2088 in _minimize_newtoncg\r\n    raise ValueError('Jacobian is required for Newton-CG method')\r\n\r\nValueError: Jacobian is required for Newton-CG method\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-d02me32y\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-aug4qibh\\cp311-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.11'\r\n```\r\n","comments":["Here is the relevant logic:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/235602cf350e9aad7bf09ee42ddd7dd4cc4a6550\/scipy\/optimize\/_minimize.py#L591-L610","The docs say:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/235602cf350e9aad7bf09ee42ddd7dd4cc4a6550\/scipy\/optimize\/_minimize.py#L106-L108\r\n\r\nhence I think this is expected behaviour"],"labels":["scipy.optimize","query"]},{"title":"BUG: SciPy 1.13.0rc1 not buildable on old macOS due to pocketfft code","body":"This occurred while [building](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/270) rc1 in conda-forge and is exactly the same issue as https:\/\/github.com\/numpy\/numpy\/issues\/25940, but I'm raising it for release-tracking purposes, as - aside from fixing it in some way - we also need to update the submodule. Failure looks like:\r\n\r\n```\r\n[881\/1477] Compiling C++ object scipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p\/pypocketfft.cxx.o\r\nFAILED: scipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p\/pypocketfft.cxx.o \r\nx86_64-apple-darwin13.4.0-clang++ -Iscipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p -Iscipy\/fft\/_pocketfft -I..\/scipy\/fft\/_pocketfft -I..\/scipy\/_lib\/pocketfft -I$PREFIX\/include -I$PREFIX\/include\/python3.10 -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -DNDEBUG -Wall -Winvalid-pch -std=c++17 -O3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -fmessage-length=0 -isystem $PREFIX\/include -fdebug-prefix-map=$SRC_DIR=\/usr\/local\/src\/conda\/scipy-split-1.13.0rc1 -fdebug-prefix-map=$PREFIX=\/usr\/local\/src\/conda-prefix -D_FORTIFY_SOURCE=2 -isystem $PREFIX\/include -mmacosx-version-min=10.9 -DPOCKETFFT_PTHREADS -DPOCKETFFT_CACHE_SIZE=16 -MD -MQ scipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p\/pypocketfft.cxx.o -MF scipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p\/pypocketfft.cxx.o.d -o scipy\/fft\/_pocketfft\/pypocketfft.cpython-310-darwin.so.p\/pypocketfft.cxx.o -c ..\/scipy\/fft\/_pocketfft\/pypocketfft.cxx\r\nIn file included from ..\/scipy\/fft\/_pocketfft\/pypocketfft.cxx:19:\r\n..\/scipy\/_lib\/pocketfft\/pocketfft_hdronly.h:160:15: error: no member named 'aligned_alloc' in the global namespace; did you mean simply 'aligned_alloc'?\r\n  void *ptr = ::aligned_alloc(align,(size+align-1)&(~(align-1)));\r\n              ^~~~~~~~~~~~~~~\r\n              aligned_alloc\r\n..\/scipy\/_lib\/pocketfft\/pocketfft_hdronly.h:157:14: note: 'aligned_alloc' declared here\r\ninline void *aligned_alloc(size_t align, size_t size)\r\n             ^\r\n1 error generated.\r\n```\r\n\r\nQuoting myself from the numpy issue:\r\n> Looking at the [code](https:\/\/github.com\/mreineck\/pocketfft\/blob\/0f7aa1225b065938fc263b7914df16b8c1cbc9d7\/pocketfft_hdronly.h#L155-L160) in question\r\n> \r\n> ```c\r\n> \/\/ the __MINGW32__ part in the conditional below works around the problem that\r\n> \/\/ the standard C++ library on Windows does not provide aligned_alloc() even\r\n> \/\/ though the MinGW compiler and MSVC may advertise C++17 compliance.\r\n> \/\/ aligned_alloc is only supported from MacOS 10.15.\r\n> #if (__cplusplus >= 201703L) && (!defined(__MINGW32__)) && (!defined(_MSC_VER)) && (MAC_OS_X_VERSION_MIN_REQUIRED >= MAC_OS_X_VERSION_10_15)\r\n> inline void *aligned_alloc(size_t align, size_t size)\r\n> ```\r\n> \r\n> there's already a guard to only use this on macOS >=10.15, but it looks like `MAC_OS_X_VERSION_MIN_REQUIRED` might not be defined (correctly).\r\n\r\n","comments":["which version of OSX does it fail on?","10.9. we could bump to 10.13, but in either case the detection and preprocessor-guard is not working","https:\/\/github.com\/scipy\/pocketfft\/pull\/1 seems to populate the preprocessor \"variable\" in question for me locally on a newer MacOS (it was not getting defined properly before that patch). If we want the patch it may need to be applied upstream first, not sure, but anyway that may be worth trying...","@h-vetinari tried to sync in the small preprocessor directive patch to `conda-forge` without success, so this issue should remain open and we should probably just disable the alloc thing completely then, as upstream did and NumPy likely will."],"labels":["defect","Build issues","scipy.fft"]},{"title":"BUG: Optimize: NewtonCG min crashes with xtol=0","body":"The minimize function crashes if `xtol` is set to `0`. This is because the initial value of a loop variable is `2 * xtol`, which is intended to ensure the loop runs at least once. However, if `xtol = 0` then the loop never runs, a variable isn't instantiated that is used outside of the loop, and an `UnboundLocalError` is raised.\r\n\r\nTo fix this, the initial value of the loop variable is set to the max float value.\r\n\r\n<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nCloses #20214 \r\n\r\n#### What does this implement\/fix?\r\nUnboundLocalError is thrown if `xtol` is set to 0 for the `newton-cg` method in `minimize`.\r\n\r\n#### Additional information\r\n\r\n","comments":["Is there a test for the \u201enan-path\u201c?","> Is there a test for the \u201enan-path\u201c?\r\n\r\n@lorentzenchr yes, there is an existing test `test_nan_values` that checks this branch.\r\nLine 1494 in `test_optimize.py`.","@j-bowhay @lucascolley I think we have consensus now, would one of you be happy to merge this?","Thanks for the feedback @lucascolley, I thought I was doing the right thing there. I\u2019ll leave them open in the future. "],"labels":["defect","scipy.optimize","backport-candidate"]},{"title":"DOC: linalg: mention that eigenvalues_only=True\/False may change the eigenvalues","body":"[docs only]\r\n\r\nPer the discussion in https:\/\/github.com\/Reference-LAPACK\/lapack\/issues\/997 : this is apparently known (widely known in somewhat narrow circles, it looks like) that `eig(a)[0]` and `eig(a, eigenvalues_only=True)` are not exactly equivalent: The reported eigenvalues may differ by something of the order of machine epsilon. This is not a big deal unless the true eigenvalue is of the same order itself.\r\n\r\nSince this is not going to get fixed in LAPACK, let's add a note to our docs.","comments":[],"labels":["scipy.linalg","Documentation"]},{"title":"BUG: linalg: use SYEV not SYEVR for pinvh","body":"Apparently, SYEVR may sometimes give zero eigenvalue > epsilon.\r\n\r\ncross-ref https:\/\/github.com\/Reference-LAPACK\/lapack\/issues\/997 for a discussion.\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/12515\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nChange the `pinvh` driver to SYEV, which is apparently a bit more robust.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nAn alternative is to expose a driver itself. This feels to be overkill though: we do expose `tol`, give a reasonable default, and it's not taxing to redo the whole pinvh calculation using either SVD or eigenvalues of whatnot in user code. \r\n\r\nAs such, even this PR may be too much, actually.","comments":[],"labels":["defect","scipy.linalg"]},{"title":"BUG: linalg: support empty arrays","body":"#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\nsupersedes and closes gh-18673, supersedes and closes gh-18141\r\n\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/17658\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/14389\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/14244\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/9459\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/8510\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/8056\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nIncludes two previous PRs, squashed to simplify the rebasing.\r\nAn annoying thing is making sure the dtype is right: `solve(int, float32) -> float64`, `solve(float32, float32) -> float32` etc.  \r\nWe absolutely need to make sure that empty and non-empty code paths agree.\r\n\r\nFunctions (`dir(linalg)`, slightly edited):\r\n\r\n- [ ] bandwidth\r\n- [ ] block_diag\r\n- [ ] cdf2rdf\r\n- [x] cho_factor\r\n- [x] cho_solve\r\n- [x] cho_solve_banded\r\n- [x] cholesky\r\n- [x] cholesky_banded\r\n- [ ] circulant\r\n- [ ] clarkson_woodruff_transform\r\n- [ ] companion\r\n- [ ] convolution_matrix\r\n- [ ] coshm\r\n- [ ] cosm\r\n- [ ] cossin\r\n- [x] det\r\n- [ ] dft\r\n- [ ] diagsvd\r\n- [x] eig\r\n- [x] eig_banded\r\n- [x] eigh\r\n- [x] eigh_tridiagonal\r\n- [x] eigvals\r\n- [x] eigvals_banded\r\n- [x] eigvalsh\r\n- [x] eigvalsh_tridiagonal\r\n- [x] expm\r\n- [ ] expm_cond\r\n- [ ] expm_frechet\r\n- [ ] fiedler\r\n- [ ] fiedler_companion\r\n- [ ] fractional_matrix_power\r\n- [ ] funm\r\n- [ ] hadamard\r\n- [ ] hankel\r\n- [ ] helmert\r\n- [ ] hessenberg\r\n- [ ] hilbert\r\n- [x] inv\r\n- [ ] invhilbert\r\n- [ ] invpascal\r\n- [ ] khatri_rao\r\n- [ ] kron\r\n- [ ] ldl\r\n- [ ] leslie\r\n- [ ] logm\r\n- [x] lstsq\r\n- [x] lu\r\n- [x] lu_factor\r\n- [x] lu_solve\r\n- [ ] matmul_toeplitz\r\n- [x] matrix_balance\r\n- [ ] norm\r\n- [x] null_space\r\n- [ ] ordqz\r\n- [x] orth\r\n- [ ] orthogonal_procrustes\r\n- [ ] pascal\r\n- [x] pinv\r\n- [x] pinvh\r\n- [ ] polar\r\n- [x] qr\r\n- [ ] qr_delete\r\n- [ ] qr_insert\r\n- [x] qr_multiply\r\n- [ ] qr_update\r\n- [ ] qz\r\n- [x] rq\r\n- [ ] rsf2csf\r\n- [x] schur\r\n- [ ] signm\r\n- [ ] sinhm\r\n- [ ] sinm\r\n- [x] solve\r\n- [x] solve_banded\r\n- [ ] solve_circulant\r\n- [ ] solve_continuous_are\r\n- [ ] solve_continuous_lyapunov\r\n- [ ] solve_discrete_are\r\n- [ ] solve_discrete_lyapunov\r\n- [ ] solve_lyapunov\r\n- [ ] solve_sylvester\r\n- [ ] solve_toeplitz\r\n- [x] solve_triangular\r\n- [x] solveh_banded\r\n- [x] sqrtm\r\n- [ ] subspace_angles\r\n- [x] svd\r\n- [x] svdvals\r\n- [ ] tanhm\r\n- [ ] tanm\r\n- [ ] toeplitz\r\n\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nWould be great to have a review of the main approach in the last commits --- `solve`, `solve_triangular` and `inv` before I manually go over the rest of linalg functions\r\n\r\ncc @JozsefKutas @boatwrong \r\n","comments":["This is ready from my side.\r\n\r\nThe most common functionality is covered, including:\r\n- common solvers (solve, inv, det, eig)\r\n- factorizations (lu, qr, svd, schur)\r\n- matfuncs (expm, sqrtm)\r\n\r\nWhat's missing is, I believe, less commonly used --- is particular, the current state covers all open issues.\r\n\r\nThe vast  majority of the missing stuff either already works (sinhm etc via expm --- needs general test coverage though), or does not need bothering (rsf2scf). So I suggest we move the laundry list from the OP into a tracking issue for follow-ups which can be done at leisure.\r\n\r\nThe only potentially non-trivial thing is `logm`: the empty input test is xfailed in this PR. The root cause is `np.argmax(empty_array)` throwing a ValueError. This is used used somewhere in estimating the 1-norm, so the fix is not mechanical.\r\n\r\nThe doc build CI failure is unrelated, gh-20296."],"labels":["defect","scipy.linalg"]},{"title":"BUG: Hang on Windows in scikit-learn with 1.13rc1 and 1.14.dev (maybe due to OpenBLAS 0.3.26 update?)","body":"### Describe your issue.\r\n\r\nThis was originally seen in MNE-Python CI roughly a week ago (March 13) using the scipy developement wheel (scientific-python-nightly-wheel) and reported in scikit-learn see https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28625 for more details.\r\n\r\nThis can also be reproduced with scipy 1.13rc1. Could it be due to updating OpenBLAS to 0.3.26?\r\n\r\nThe scikit-learn code does nested parallelism, OpenBLAS within OpenMP. Setting `OMP_NUM_THREADS=1` or `OPENBLAS_NUM_THREADS=1` avoids the hang. A similar work-around can be used with `threadpoolctl`.\r\n\r\nInsights or suggestions more than welcome!\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom sklearn.metrics._pairwise_distances_reduction import ArgKmin\r\nimport numpy as np\r\nimport threadpoolctl\r\n\r\n# Commenting any of these two lines avoids the hang\r\n# threadpoolctl.threadpool_limits(limits=1, user_api='blas')\r\n# threadpoolctl.threadpool_limits(limits=1, user_api='openmp')\r\nX = np.zeros((20, 14000))\r\nArgKmin.compute(X=X, Y=X, k=10, metric='euclidean')\r\n```\r\n\r\n\r\n### Error message\r\n\r\nHang, i.e. the script never completes. With one of the work-around it completes in less than one second.\r\n\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.13.0rc1 1.26.4 sys.version_info(major=3, minor=12, micro=2, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= ZEN MAX_THREADS=24\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= ZEN MAX_THREADS=24\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.9\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-t0bobn5y\\overlay\\Lib\\site-packages\/pythr\r\nan\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-bxqwrdgx\\cp312-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.12'\r\n```\r\n","comments":["cc @mattip ","Hmmm actually debugging further this does seem like an issue in OpenBLAS 0.3.26. I can reproduce the hang with conda-forge packages and scipy 1.12. With OpenBLAS 0.3.25 the snippet runs fine.\r\n\r\nConda env created with:\r\n```\r\nmamba create -n lof-issue-conda-2 scikit-learn ipython openblas=0.3.26 blas=*=*openblas* -y\r\n```\r\n\r\nProblematic environment info with OpenBLAS 0.3.26 (it does not seem like OpenBLAS 0.3.26 is mentioned somehow in the output below so I added `sklearn.show_versions()` below ...)\r\n```\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=12, micro=2, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/include\r\n    lib directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: C:\\bld\\scipy-split_1706041678996\\_h_env\\Library\\lib\\pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/include\r\n    lib directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: C:\\bld\\scipy-split_1706041678996\\_h_env\\Library\\lib\\pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: clang-cl\r\n    linker: lld-link\r\n    linker args: --target=x86_64-pc-windows-msvc, -nostdlib, -Xclang, --dependent-lib=msvcrt,\r\n      -fuse-ld=lld, -Wl,-defaultlib:C:\\bld\\scipy-split_1706041678996\\_build_env\/Library\/lib\/clang\/17\/lib\/windows\/clan\r\ng_rt.builtins-x86_64.lib\r\n    name: clang-cl\r\n    version: 17.0.6\r\n  c++:\r\n    commands: clang-cl\r\n    linker: lld-link\r\n    linker args: --target=x86_64-pc-windows-msvc, -nostdlib, -Xclang, --dependent-lib=msvcrt,\r\n      -fuse-ld=lld, -Wl,-defaultlib:C:\\bld\\scipy-split_1706041678996\\_build_env\/Library\/lib\/clang\/17\/lib\/windows\/clan\r\ng_rt.builtins-x86_64.lib\r\n    name: clang-cl\r\n    version: 17.0.6\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.7\r\n  fortran:\r\n    args: -D_CRT_SECURE_NO_WARNINGS, -D_MT, -D_DLL, --target=x86_64-pc-windows-msvc,\r\n      -nostdlib\r\n    commands: flang-new\r\n    linker: lld-link\r\n    linker args: --target=x86_64-pc-windows-msvc, -nostdlib, -Xclang, --dependent-lib=msvcrt,\r\n      -fuse-ld=lld, -Wl,-defaultlib:C:\\bld\\scipy-split_1706041678996\\_build_env\/Library\/lib\/clang\/17\/lib\/windows\/clan\r\ng_rt.builtins-x86_64.lib,\r\n      -D_CRT_SECURE_NO_WARNINGS, -D_MT, -D_DLL, --target=x86_64-pc-windows-msvc, -nostdlib\r\n    name: flang\r\n    version: 17.0.6\r\n  pythran:\r\n    include directory: ..\\..\\_h_env\\Lib\\site-packages\\pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\bld\\scipy-split_1706041678996\\_h_env\\python.exe\r\n  version: '3.12'\r\n```\r\n\r\n`sklearn.show_versions()`\r\n```\r\nSystem:\r\n    python: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:42:31) [MSC v.1937 64 bit (AMD64)]\r\nexecutable: C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.1.post1\r\n          pip: 24.0\r\n   setuptools: 69.2.0\r\n        numpy: 1.26.4\r\n        scipy: 1.12.0\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.3.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 4\r\n         prefix: libomp\r\n       filepath: C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Library\\bin\\libomp.dll\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 4\r\n         prefix: libblas\r\n       filepath: C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Library\\bin\\libblas.dll\r\n        version: 0.3.26\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 4\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\vcomp140.dll\r\n        version: None\r\n```","@martin-frbg does this look familiar?\r\n\r\n> The scikit-learn code does nested parallelism\r\n\r\n@lesteve do you know how many threads\/processes are opened?","Not familiar, also not aware of any change in 0.3.26 that would be likely to cause it _if_ the library was built with OpenMP support. (If not, there is a small chance that it might be related to https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4359 - but again nothing of the sort known to date)","> Not familiar, also not aware of any change in 0.3.26 that would be likely to cause it if the library was built with OpenMP support. \r\n\r\nI believe that both for wheels and conda-forge OpenBLAS is built with pthreads as the threading layer and not OpenMP. At least the scikit-learn output shows `threading_layer: pthreads` for both the wheel and conda-forge package.\r\n\r\nI checked the `threadpoolctl` code and the `threading_layer` info is from `openblas_get_parallel` (following https:\/\/github.com\/OpenMathLib\/OpenBLAS\/wiki\/Faq#how-can-i-find-out-at-runtime-what-options-the-library-was-built-with-)","would need to revert 4359 then to test - or simply copy the 0.3.25 driver\/others\/blas_server_win32.c into 0.3.26","@lesteve can you please clarify 'The scikit-learn code does nested parallelism' for me to help narrow this down? Am I correct in interpreting that to mean that the code invokes `exec_blas_async()`, rather than `exec_blas()`, multiple times in a row to queue up several batches of work asynchronously?\r\n\r\nI will take a look when I can free up from other tasks. It will probably be about a week before I can dig into this so having a clear repro scenario will help speed things up.","> @lesteve can you please clarify 'The scikit-learn code does nested parallelism' for me to help narrow this down?\r\n\r\nThere is an outer OpenMP parallel loop and inside the OpenMP parallel loop there are some OpenBLAS calls. This is coming from Python so I am not sure about your question about `exec_blas_async` vs `exec_blas`. I may try to have a simpler reproducer in Cython at one point, but this may take some time ...\r\n\r\nAnother angle of attack I have tried and got stuck is to make sure https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4359 is the issue. I am compiling OpenBLAS on Windows inside a MSYS2 terminal with `make`. At the end of the compilation I get a `cygopenblas.dll`. The command I use is:\r\n```\r\nmake shared NOFORTRAN=1 NO_LAPACKE=1\r\n```\r\n\r\nI was naively hoping to copy this dll to my Python site-packages to replace `libblas.dll` (somewhere like `C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-3\\Library\\bin\\libblas.dll`). if I do this numpy is failing to import so I must be doing something wrong. Any insights about this, let me know!\r\n\r\nThe ImportError looks like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\__init__.py\", line 149, in <module>\r\n    from . import lib\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\lib\\__init__.py\", line 23, in <module>\r\n    from . import index_tricks\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\lib\\index_tricks.py\", line 12, in <module>\r\n    import numpy.matrixlib as matrixlib\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\matrixlib\\__init__.py\", line 4, in <module>\r\n    from . import defmatrix\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\", line 12, in <module>\r\n    from numpy.linalg import matrix_power\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\linalg\\__init__.py\", line 73, in <module>\r\n    from . import linalg\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\numpy\\linalg\\linalg.py\", line 35, in <module>\r\n    from numpy.linalg import _umath_linalg\r\nImportError: DLL load failed while importing _umath_linalg: The specified module could not be found.\r\n```",">Problematic environment info with OpenBLAS 0.3.26 (it does not seem like OpenBLAS 0.3.26 is mentioned somehow in >the output below so I added sklearn.show_versions() below ...)\r\n\r\n>1.12.0 1.26.4 sys.version_info(major=3, minor=12, micro=2, releaselevel='final', serial=0)\r\n>Build Dependencies:\r\n> blas:\r\n>    detection method: pkgconfig\r\n>    found: true\r\n>    include directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/include\r\n>    lib directory: C:\/Users\/rjxQE\/AppData\/Local\/miniforge3\/envs\/lof-issue-conda-2\/Library\/lib\r\n>   name: blas\r\n>   openblas configuration: unknown\r\n>    pc file directory: C:\\bld\\scipy-split_1706041678996\\_h_env\\Library\\lib\\pkgconfig\r\n>    version: 3.9.0\r\n\r\nthis looks strangely like it may have picked up a libblas from the \"netlib\" Reference-LAPACK project rather than OpenBLAS. Could it be that the various Python bits loaded multiple different BLAS implementations into the process ?","@lesteve can you try to `mamba install \"libopenblas=*=*openmp*\"` in your conda-forge env on windows to see if that fixes the problem on your reproducer? This command should switch the threading layer from `pthreads` to `openmp` and it the problem comes from nesting pthreads calls under OpenMP calls that might fix it.\r\n\r\nAnd if it does, it might reveal a thread-safety problem of openblas' handling of its native threading layer under Windows.\r\n\r\nThe code in scikit-learn calls OpenBLAS under an OpenMP parallel loop. To avoid oversubscription, it does call `openblas_set_num_threads(1)` (via `threadpoolctl.threadpool_limits`) prior to entering the OpenMP parallel loop.\r\n\r\nMaybe the recent changes in https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4425 introduced a thread-safety problem in this threading layer when globally changing the number of threads at runtime?\r\n\r\n\r\n\r\n","@ogrisel it looks like libopenblas with openmp is not available on conda-forge for Windows, only openmp with pthreads is, for example:\r\n\r\n```\r\n$ mamba search 'libopenblas>=0.3.25'\r\n\r\nLoading channels: done\r\n# Name                       Version           Build  Channel\r\nlibopenblas                   0.3.25 pthreads_hc140b1d_0  conda-forge\r\nlibopenblas                   0.3.26 pthreads_hc140b1d_0  conda-forge\r\n```","> Maybe the recent changes in\r\n\r\nI'd consider that unlikely especially if you're not using an OpenMP build of OpenBLAS in the first place","It does seem like https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4359 is indeed the cause of this behaviour.\r\n\r\nI managed to build OpenBLAS on Windows following https:\/\/github.com\/OpenMathLib\/OpenBLAS\/wiki\/How-to-use-OpenBLAS-in-Microsoft-Visual-Studio (Native (MSVC) ABI way) and replace the dll.\r\n\r\n- the hang does happen on https:\/\/github.com\/OpenMathLib\/OpenBLAS\/commit\/e60fb0f39731ae9c21c5fd74d432f03b83d2a7d5 (merge commit for https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4359)\r\n- there is not hang in the previous merge commit in the `develop` branch https:\/\/github.com\/OpenMathLib\/OpenBLAS\/commit\/5b09833b1c877ad1d395ee38791521ccd32386be\r\n\r\nFor completeness the way I used to setup the build:\r\n```\r\ncmake .. \\\r\n    -G \"Ninja\" -DCMAKE_CXX_COMPILER=clang-cl -DCMAKE_C_COMPILER=clang-cl \\\r\n    -DCMAKE_Fortran_COMPILER=flang -DCMAKE_MT=mt -DBUILD_WITHOUT_LAPACK=yes \\\r\n    -DNOFORTRAN=1 -DDYNAMIC_ARCH=OFF -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON\r\n```","~~weird as this should be a no-op if you are neither building with OpenMP support nor calling the newly introduced function~~","wait, e60fb0f is the merge commit for Mark's 4359 (the windows thread server rewrite) not my 4425 - that's still unfortunate but a lot more plausible","> wait, e60fb0f is the merge commit for Mark's 4359 (the windows thread server rewrite) not my 4425 - that's still unfortunate but a lot more plausible\r\n\r\nIndeed my bad, I edited my previous message to reflect this (I missed the fact that two PRs were mentioned I was focussed on https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4359 all along).","So it seems that the combined use of external openmp loops with calls to `openblas_set_num_threads(1)` is causing a deadlock with the new windows thread server.\r\n\r\nI am not familiar with windows dev, but is there a way to do attach a debugger like GDB to a running Python process (e.g. by pid) and then do `thread apply all bt` to locate where\/when the deadlock happens?","Not a Windows expert either, but this is what I get with ProcessExplorer Threads view:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/1680079\/ea05146a-f1af-4f3d-bb3d-646e9b5e5812)\r\n\r\nThe two running threads have some stack traces like this:\r\n\r\nThread 10892:\r\n```\r\n0x0000000000000000\r\nKERNEL32.DLL!SwitchToThread\r\nlibblas.dll!exec_blas+0x122\r\nlibblas.dll!blas_level1_thread_with_return_value+0x329\r\nlibblas.dll!ddot_k+0xee\r\nlibblas.dll!ddot_+0x50\r\ncython_blas.cp312-win_amd64.pyd!PyInit_cython_blas+0x231f8\r\ncython_blas.cp312-win_amd64.pyd!PyInit_cython_blas+0xa4a0\r\n_cython_blas.cp312-win_amd64.pyd+0xfd97\r\n_base.cp312-win_amd64.pyd!PyInit__base+0xbd28\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0x181a\r\nVCOMP140.DLL!vcomp_fork+0x2dd\r\nVCOMP140.DLL!vcomp_fork+0x29c\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0x1ea\r\nKERNEL32.DLL!BaseThreadInitThunk+0x14\r\nntdll.dll!RtlUserThreadStart+0x21\r\n```\r\n\r\nThread 7124:\r\n```\r\n0x0000000000000000\r\nntdll.dll!ZwYieldExecution+0x14\r\nKERNELBASE.dll!SwitchToThread+0x24\r\nlibblas.dll!exec_blas+0x122\r\nlibblas.dll!blas_level1_thread_with_return_value+0x329\r\nlibblas.dll!ddot_k+0xee\r\nlibblas.dll!ddot_+0x50\r\ncython_blas.cp312-win_amd64.pyd!PyInit_cython_blas+0x231f8\r\ncython_blas.cp312-win_amd64.pyd!PyInit_cython_blas+0xa4a0\r\n_cython_blas.cp312-win_amd64.pyd+0xfd97\r\n_base.cp312-win_amd64.pyd!PyInit__base+0xbd28\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0x181a\r\nVCOMP140.DLL!vcomp_fork+0x2dd\r\nVCOMP140.DLL!vcomp_fork+0x29c\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0x1ea\r\nKERNEL32.DLL!BaseThreadInitThunk+0x14\r\nntdll.dll!RtlUserThreadStart+0x21\r\n\r\n```\r\n\r\nThread 7832 (Python.exe so I guess main thread?)\r\n```\r\nntdll.dll!ZwWaitForSingleObject+0x14\r\nKERNELBASE.dll!WaitForSingleObjectEx+0x8e\r\nVCOMP140.DLL!vcomp_ordered_loop_end+0x1f94\r\nVCOMP140.DLL!omp_get_wtick+0x263\r\nVCOMP140.DLL!vcomp_barrier+0xe6\r\n_base.cp312-win_amd64.pyd!PyInit__base+0xbd5d\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0x181a\r\nVCOMP140.DLL!vcomp_fork+0x2dd\r\nVCOMP140.DLL!vcomp_fork+0x29c\r\nVCOMP140.DLL!vcomp_atomic_div_r8+0xb83\r\nVCOMP140.DLL!vcomp_fork+0x1bc\r\n_base.cp312-win_amd64.pyd+0xcabb\r\n_base.cp312-win_amd64.pyd+0xf291\r\n_argkmin.cp312-win_amd64.pyd+0x1211c\r\n_argkmin.cp312-win_amd64.pyd+0x10d8f\r\npython312.dll!PyType_Name+0x1b10\r\n_argkmin.cp312-win_amd64.pyd!PyInit__argkmin+0x3ef5\r\n_argkmin.cp312-win_amd64.pyd+0xcf8f\r\n_argkmin.cp312-win_amd64.pyd+0xcb49\r\npython312.dll!PyBytes_Repeat+0xf1\r\npython312.dll!PyObject_Vectorcall+0x35\r\npython312.dll!PyEval_EvalFrameDefault+0x7d0f\r\npython312.dll!PyEval_EvalCode+0xe6\r\npython312.dll!PyRun_FileExFlags+0x296\r\npython312.dll!PyRun_FileExFlags+0x393\r\npython312.dll!PyRun_StringFlags+0x178\r\npython312.dll!PyRun_SimpleFileObject+0x2fb\r\npython312.dll!Py_gitidentifier+0xb251\r\npython312.dll!Py_gitidentifier+0xbe82\r\npython312.dll!Py_RunMain+0x18\r\npython312.dll!Py_Main+0x5c\r\npython.exe!OPENSSL_Applink+0x380\r\nKERNEL32.DLL!BaseThreadInitThunk+0x14\r\nntdll.dll!RtlUserThreadStart+0x21\r\n```","Apparently it's possible to \"Configure symbols\" after installing \"Debugging Tools for Windows\" as explained here:\r\n\r\n- https:\/\/superuser.com\/a\/462970\r\n\r\nI have no idea what it entails but maybe it will give more information, (maybe even source code line numbers)?","Also, could you re-run your reproducer with faulthandler?\r\n\r\n```python\r\nfrom sklearn.metrics._pairwise_distances_reduction import ArgKmin\r\nimport numpy as np\r\nimport threadpoolctl\r\nimport faulhandler\r\n\r\n\r\nfaulthandler.dump_traceback_later(timeout=10, repeat=False, file=sys.stderr, exit=False)\r\n\r\n# Commenting any of these two lines avoids the hang\r\n# threadpoolctl.threadpool_limits(limits=1, user_api='blas')\r\n# threadpoolctl.threadpool_limits(limits=1, user_api='openmp')\r\nX = np.zeros((20, 14000))\r\nArgKmin.compute(X=X, Y=X, k=10, metric='euclidean')\r\n```\r\n\r\nThis should give us the Python-level tracebacks. We probably won't see the OpenMP and OpenBLAS managed threads but that might still be informative, in case the freeze happens when calling `threadpoolctl.threadpool_limits(1, user_api=\"blas\")` in scikit-learn at lines:\r\n\r\n- https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/9b7d176b5a85d7dd681ffa69e55a82a3338096c5\/sklearn\/metrics\/_pairwise_distances_reduction\/_argkmin.pyx.tp#L89-L93\r\n","Hmm. My ignorant bet would have been on the new \"shut down all surplus threads\" loop somehow hanging on the `openblas_set_num_threads(1)`, but it seems it made it to exec_blas() for the DDOT workload. Unfortunately I haven't the froggiest idea what could be wrong there, in particular as this does not appear to have been touched by the PR.","The faulthandler info says:\r\n```\r\nTimeout (0:00:10)!\r\nThread 0x00000458 (most recent call first):\r\n  File \"C:\\Users\\rjxQE\\AppData\\Local\\miniforge3\\envs\\lof-issue-conda-2\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 278 in compute\r\n  File \"C:\\msys64\\home\\rjxQE\\work\\test-lof.py\", line 13 in <module>\r\n```\r\n\r\nso this is probably there https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/719c0c6036ac99d45ffb45ed7c8b4e4b221384b5\/sklearn\/metrics\/_pairwise_distances_reduction\/_dispatcher.py#L278\r\n\r\nAlso I added the stack-trace of the Python (probably main thread I guess) in https:\/\/github.com\/scipy\/scipy\/issues\/20294#issuecomment-2012295803\r\n\r\nI did not manage to get more info with configuring symbols, I am guessing this is because we would need to compile scikit-learn with debug symbols on. I am not sure I am ready to do this quite yet :wink:. I think spending some time to put together a simple reproducer in Cython may be more useful, but let me know if you disagree.","I updated my comment to point in the Cython file where I think this happens:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/20294#issuecomment-2012419439\r\n\r\nThe Python level function is the direct parent. Unfortunately, `ArgKmin64.compute` is a compiled extension (Cython) hence we cannot see the callees without the debug symbols. But it the deadlock had happened in the `openblas_set_num_threads` call under `threadpool_limits` I think we would have seen it in the Python traceback because `threadpool_limits` is a Python function (called by a Cython\/native caller).\r\n\r\nSo it's most probably happening in the OpenMP Cython code (deadlock of the OpenMP runtime) or in the OpenBLAS DGEMM call. But we need debug symbols.\r\n\r\n> I think spending some time to put together a simple reproducer in Cython may be more useful, but let me know if you disagree.\r\n\r\nI agree it that crafting a simpler reproducer in Cython or even pure C might be more productive.","where does DGEMM come from now ? or did you mean DDOT as seen in the process traces ?","If you are able to build OpenBlas, it would be helpful to build a version with -DSMP_DEBUG. That will produce a log on stderr from the thread server detailing threads, task management, queue state, etc. Having that log would give a peek at the execution paths.\r\n\r\nIf the call tree goes through `exec_blas()` rather than `exec_blas_async()` my initial thoughts on what this might be are altered. I had suspected that a rapid series of calls to `exec_blas_async()` could somehow corrupt the task queue. But `exec_blas()` calls are serialized on queue completion.\r\n\r\nAnother hypothesis would be a series of (rapid) calls to `openblas_set_num_threads(1)`. Unless you've set OPENBLAS_NUM_THREADS=1, the library on startup always spins up one thread per CPU core (up to MAX_THREADS). On a subsequent call to `openblas_set_num_threads()` new threads may be added, but extra threads were not killed. Extra threads would  be left idle. I added code that trims the extra threads. If we have a simple repro, a quick test would be commenting out that code since it is a smaller change\/revert.\r\n\r\nA clarifying question about the scenario: Is OMP making parallel calls into `exec_blas()`? I think there is a (possibly bad on my part) assumption that `exec_blas()` was not re-entrant since it blocks on the task queue completing. If you can confirm, I will review that code to see if that could be the problem.","Here is the [log](https:\/\/github.com\/scipy\/scipy\/files\/14706807\/build.log) with `SMP_DEBUG`\r\n\r\nFull disclosure I did not manage to do `-DSMP_DEBUG` through `cmake` although I did `set CFLAGS=-DSMP_DEBUG` and the `ninja.build` file looks like it is using the correct flag) so I ended editing the code directly to get the debug print statements ...\r\n\r\n> I added code that trims the extra threads. If we have a simple repro, a quick test would be commenting out that code since it is a smaller change\/revert.\r\n\r\nJust to make sure I follow you, I am guessing you mean commenting out this code right?\r\nhttps:\/\/github.com\/OpenMathLib\/OpenBLAS\/blob\/3d2a9e4a6134a9033236360cd87fd0bab2f7a70f\/driver\/others\/blas_server_win32.c#L550-L572\r\n\r\nHere are my best guess about your other questions (coming from the Python world I am slightly out of my comfort zone here :grin:):\r\n- I think we use `exec_blas` and not `exec_blas_async`\r\n- there may be a series of calls of `openblas_set_num_threads(1)` and then `openblas_set_num_threads(number_of_cores)` once we are out of the parallel OpenMP loop. This may happen multiple times but I am not sure how \"rapid\" they are and how \"rapid\" this needs to be to be problematic ...\r\n- Not 100% sure but I think indeed OpenMP is making parallel calls into `exec_blas` basically the following pattern probably happens quite a few times:\r\n  ```cython\r\n  openblas_set_num_threads(1)\r\n  OpenMP parallel loop begin\r\n      blas calls through numpy\/scipy\r\n  OpenMP parallel loop end\r\n  openblas_set_num_threads(number_of_cores)\r\n  ```\r\n  and there are likely also some other blas calls outside of OpenMP parallel regions\r\n\r\n","@lesteve Yes, those lines of code are for trimming extra threads. It would be my fault for making assumptions, but I don't think I was expecting a use-case that ping-ponged between N threads, 1 thread and then N threads again, potentially with re-entrant calls to `exec_blas()`. \r\n\r\nI will go over the code to see if I can find an issue with that pattern.\r\n\r\n ","Is there any detail about which SciPy function is called by the scikit-learn function that causes the hang? I am trying to find the SciPy part of the problem.","So far I have not been able to reproduce the dead-lock outside the scikit-learn code (i.e. only with Scipy and Cython) and I have found a work-around in scikit-learn which is to protect a OpenBLAS call with OpenMP parallel loop by setting the OpenBLAS number of threads to 1 with `threadpoolctl`:\r\n\r\n```diff\r\n@@ -36,8 +37,9 @@ cdef float64_t[::1] _sqeuclidean_row_norms64_dense(\r\n         intp_t d = X.shape[1]\r\n         float64_t[::1] squared_row_norms = np.empty(n, dtype=np.float64)\r\n\r\n-    for idx in prange(n, schedule='static', nogil=True, num_threads=num_threads):\r\n-        squared_row_norms[idx] = _dot(d, X_ptr + idx * d, 1, X_ptr + idx * d, 1)\r\n+    with threadpool_limits(limits=1, user_api='blas'):\r\n+        for idx in prange(n, schedule='static', nogil=True, num_threads=num_threads):\r\n+            squared_row_norms[idx] = _dot(d, X_ptr + idx * d, 1, X_ptr + idx * d, 1)\r\n```\r\n\r\n`_dot` is actually calling `scipy.linalg.cython_blas.ddot` which I was naively expecting to be single-threaded so I don't really know why calling `openblas_set_num_threads(1)` helps but it does ...","Can you please also try with `dnrm2` and (square the result) to get the same number so we can at least triangulate the issue to `ddot` ?","The call stack above included this entry:\r\n\r\nlibblas.dll!blas_level1_thread_with_return_value+0x329\r\n\r\nwhich says that threading of ddot was requested. I believe the code that calls this is in kernel\/x86_64\/ddot.c (this is the generic C cpu path, but assembly kernels should follow a similar pattern). If only one thread is requested then the ddot kernel is called directly, otherwise the kernel is handed to the level1 threading dispatch.\r\n\r\n```\r\n#if defined(SMP)\r\n\tif (inc_x == 0 || inc_y == 0 || n <= 10000)\r\n\t\tnthreads = 1;\r\n\telse\r\n\t\tnthreads = num_cpu_avail(1);\r\n\r\n\tif (nthreads == 1) {\r\n\t\tdot = dot_compute(n, x, inc_x, y, inc_y);\r\n\t} else {\r\n\t\tint mode, i;\r\n\t\tchar result[MAX_CPU_NUMBER * sizeof(double) * 2];\r\n\t\tRETURN_TYPE *ptr;\r\n\r\n#if !defined(DOUBLE)\r\n\t\tmode = BLAS_SINGLE  | BLAS_REAL;\r\n#else\r\n\t\tmode = BLAS_DOUBLE  | BLAS_REAL;\r\n#endif\r\n\t\tblas_level1_thread_with_return_value(mode, n, 0, 0, &dummy_alpha,\r\n\t\t\t\t   x, inc_x, y, inc_y, result, 0,\r\n\t\t\t\t    (int (*)(void)) dot_thread_function, nthreads);\r\n\r\n```"],"labels":["defect","upstream bug"]},{"title":"ENH: Check return value from scipy\/optimize\/Zeros\/bisect.c","body":"### Describe your issue.\n\nI think I see a very subtle bug in scipy\/optimize\/Zeros\/bisect.c, regarding the returned position of the root `xm`.  After generating a new midpoint value `xm` the bracket is updated by either halving its size, or by updating the left hand (smaller) end `xa` .  However convergence is only checked on the fly using `fm == 0` or the bracket width, and `xm` is always returned.  This means that for cases where the left end of the initial bracket `xa` contains a very good estimate of the root, this point will not be returned.  This could also occur if one of the intermediate halving steps generates an `xa` value very close to the actual root before convergence is achieved.\r\n\r\nThe relevant code is at [bisect.c](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/optimize\/Zeros\/bisect.c) lines 29-43:\r\n```\r\ndm = xb - xa;\r\n    solver_stats->iterations = 0;\r\n    for (i=0; i<iter; i++) {\r\n        solver_stats->iterations++;\r\n        dm *= .5;      \/\/ <- The bracket is halved.\r\n        xm = xa + dm;      \/\/ <- Midpoint replaces RH end.\r\n        fm = (*f)(xm, func_data_param);\r\n        solver_stats->funcalls++;\r\n        if (signbit(fm)==signbit(fa)) {\r\n            xa = xm;      \/\/ <- The LH end is adjusted.\r\n        }\r\n        if (fm == 0 || fabs(dm) < xtol + rtol*fabs(xm)) {      \/\/ <- LH end will not be returned if fm != 0 here.\r\n            solver_stats->error_num = CONVERGED;\r\n            return xm;       \/\/ <- Only RH end can be returned this way.\r\n        }\r\n```\r\n\r\nAn example is given below with exaggerated convergence `xtol` to highlight the issue.  The returned root of `x = 1.053125` gives `f(x) =  0.10907...` which is 5x worse than the LH end value of  `abs(f(a)) = 0.0199`.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.optimize import bisect\r\n\r\ndef f(x):\r\n    return x ** 2 - 1  # Roots at x = -1 and x = 1.\r\n\r\nxa, xb = 0.99, 2.00  # 'xa' contains a very good estimate of the root. \r\nprint(f\"Initial xa = {xa}, xb = {xb}\")\r\nprint(f\"Initial |f(xa)| = {abs(f(xa))}, |f(xb)| = {abs(f(xb))}\")\r\n\r\nx, results = bisect(f, xa, xb, xtol=0.1, full_output=True)\r\nprint(results)\r\nprint(f\"f(x_returned) = {f(x)}\")\r\n\r\n# Output:\r\n#   Initial xa = 0.99, xb = 2.0\r\n#   Initial |f(xa)| = 0.01990000000000003, |f(xb)| = 3.0\r\n#         converged: True\r\n#              flag: converged\r\n#    function_calls: 6\r\n#        iterations: 4\r\n#              root: 1.053125\r\n#            method: bisect\r\n#   f(x_returned) = 0.10907226562500028\n```\n\n\n### Error message\n\n```shell\nNo associated error message.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.12.0 1.26.3 sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"cc\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"3.0.8\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"c++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.15.0\",\r\n      \"include directory\": \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-6226b0as\\\\overlay\\\\Lib\\\\site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"windows\"\r\n    },\r\n    \"build\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"windows\"\r\n    },\r\n    \"cross-compiled\": false\r\n  },\r\n  \"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/c\/opt\/64\/include\",\r\n      \"lib directory\": \"\/c\/opt\/64\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\",\r\n      \"pc file directory\": \"c:\/opt\/64\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/c\/opt\/64\/include\",\r\n      \"lib directory\": \"\/c\/opt\/64\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\",\r\n      \"pc file directory\": \"c:\/opt\/64\/lib\/pkgconfig\"\r\n    },\r\n    \"pybind11\": {\r\n      \"name\": \"pybind11\",\r\n      \"version\": \"2.11.1\",\r\n      \"detection method\": \"config-tool\",\r\n      \"include directory\": \"unknown\"\r\n    }\r\n  },\r\n  \"Python Information\": {\r\n    \"path\": \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-77ntvnsh\\\\cp312-win_amd64\\\\build\\\\venv\\\\Scripts\\\\python.exe\",\r\n    \"version\": \"3.12\"\r\n  }\r\n}\n```\n","comments":["IIUC, the returned value satisfies the tolerance, but it is not the closest possible approximation of the root given the values at which the function has been evaluated? \r\n\r\nIn that case, this would be a request for an enhancement rather than a bug, right? (If so, please update the title.) It sounds like the proposed enhancement would be to check whether the function value at the bracket endpoint(s?) have a smaller magnitude than at the midpoint, and if so, return the corresponding endpoint. Is that correct?","> IIUC, the returned value satisfies the tolerance, but it is not the closest possible approximation of the root given the values at which the function has been evaluated?\r\n> \r\n> In that case, this would be a request for an enhancement rather than a bug, right? (If so, please update the title.) It sounds like the proposed enhancement would be to check whether the function value at the bracket endpoint(s?) have a smaller magnitude than at the midpoint, and if so, return the corresponding endpoint. Is that correct?\r\n\r\nYes agreed.  Either that or I suppose the documentation could be amended to say that if it converges, `bisect` will only return the last computed midpoint value, and that this may not actually be the best estimate for the root found during the entire process.\r\n\r\nIn most normal cases users will have set very tight tolerances so the difference shouldn't be very noticeable, but if `bisect` is being used with a relaxed tolerance as an initial 'screen' to generate a bracket for some other work then it may crop up.  I guess I noticed it. :-)  In this case I'm not sure if it is an enhancement request or bug, but I will edit the title as suggested.  "],"labels":["enhancement","scipy.optimize"]},{"title":"WIP: stats.moment: add array API support","body":"#### Reference issue\r\nTowards gh-18867\r\n\r\n#### What does this implement\/fix?\r\nAdds array API support to `stats.moment`.\r\n\r\n#### Additional information\r\nIs there something I need to do to get this to run with alternative backends in CI? The first commit should have failed tests with pytorch, but it didn't.\r\n\r\nInitially, we'll raise an error when NaNs are present, `nan_policy='omit'`, and `xp != np`. When @tirthasheshpatel has the opportunity to make the `_axis_nan_policy` decorator array API compatible, we can add that support. \r\n\r\nThis only changes `_moment` right now; changes to `moment` and the `_axis_nan_policy` decorator needed to make this useful. ","comments":["> Is there something I need to do to get this to run with alternative backends in CI?\n\nAdd a new pytest call in the array API CI workflow. `-s stats` might be overkill for now, maybe you can make it a little more fine grained with `-t`."],"labels":["scipy.stats","enhancement","scipy._lib","array types"]},{"title":"BUG: Macro collision (`complex`) with Windows SDK in amos code","body":"While building 1.13.0rc1 in [conda-forge](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/270), I'm running into macro collisions with the windows SDK:\r\n\r\n```\r\n[64\/1477] Compiling C object scipy\/special\/lib_amos.a.p\/_amos.c.obj\r\nFAILED: scipy\/special\/lib_amos.a.p\/_amos.c.obj \r\n\"clang-cl\" \"-Iscipy\\special\\lib_amos.a.p\" \"-Iscipy\\special\" \"-I..\\scipy\\special\" \"-Iscipy\\_lib\" \"-I..\\scipy\\_lib\" \"-I..\\scipy\\_build_utils\\src\" \"-DNDEBUG\" \"\/MD\" \"\/nologo\" \"\/showIncludes\" \"\/utf-8\" \"\/W2\" \"\/clang:-std=c99\" \"\/O2\" \"-Wno-unused-but-set-variable\" \"-Wno-unused-function\" \"-Wno-conversion\" \"-Wno-misleading-indentation\" \"-D_USE_MATH_DEFINES\" \"\/Fdscipy\\special\\lib_amos.a.p\\_amos.c.pdb\" \/Foscipy\/special\/lib_amos.a.p\/_amos.c.obj \"\/c\" ..\/scipy\/special\/_amos.c\r\nIn file included from ..\/scipy\/special\/_amos.c:96:\r\n..\/scipy\/special\\_amos.h(106,15): error: expected ';' after top level declarator\r\n  106 | double complex amos_airy(double complex, int, int, int *, int *);\r\n      |               ^\r\n      |               ;\r\n..\/scipy\/special\\_amos.h(107,61): error: redefinition of parameter '_complex'\r\n  107 | int amos_besh(double complex, double, int, int, int, double complex *, int *);\r\n      |                                                             ^\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\\corecrt_math.h(44,29): note: expanded from macro 'complex'\r\n   44 |             #define complex _complex\r\n      |                             ^\r\n```\r\n\r\nI'm not sure if this would be solved by `WIN32_LEAN_AND_MEAN`, but in any case, looking at `corecrt_math.h` directly, the code shows that we could disable it by setting `_COMPLEX_DEFINED` (or investigating how `_CRT_INTERNAL_NONSTDC_NAMES` can be disabled).\r\n\r\n```C\r\n    \/\/ Definition of the _complex struct to be used by those who use the complex\r\n    \/\/ functions and want type checking.\r\n    #ifndef _COMPLEX_DEFINED\r\n        #define _COMPLEX_DEFINED\r\n\r\n        struct _complex\r\n        {\r\n            double x, y; \/\/ real and imaginary parts\r\n        };\r\n\r\n        #if defined(_CRT_INTERNAL_NONSTDC_NAMES) && _CRT_INTERNAL_NONSTDC_NAMES && !defined __cplusplus\r\n            \/\/ Non-ANSI name for compatibility\r\n            #define complex _complex\r\n        #endif\r\n    #endif\r\n```\r\n\r\nPS. This is ultimately a consequence of https:\/\/github.com\/scipy\/scipy\/pull\/19587, so CC @ilayn ","comments":["Since you have much more exposure to all kinds of systems, whatever makes your life easier I am fine with it. I just tried to make tests pass on CI and unfortunately do not have the best-practice information about `complex` definitions in the ecosystem. We can also steal the `complex` implementation mechanisms from the libraries we vendor. "],"labels":["defect","scipy.special","C\/C++"]},{"title":"ENH: Poisson disk sampling for arbitrary bounds","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/13918 added Poisson disk sampling of the unit hypercube to scipy.stats.qmc.  Poisson disk sampling is widely used in image processing and image generation; see https:\/\/github.com\/scikit-image\/scikit-image\/issues\/2380 for example use cases.  Sampling of the unit hypercube is not sufficient for image processing applications because an image can be any aspect ratio.  The sampling algorithm itself must be aware of the bounds from which to draw samples; scaling samples from the unit hypercube using scipy.stats.qmc.scale results in loss of the radius distance property through unequal scaling.  In the two dimensional case for example, a circular radius gets squashed into an ellipse as shown below.\r\n\r\nOriginal sampling of unit hypercube\r\n![169402582-5331019f-9093-4456-8a40-94c900de6c9c-1](https:\/\/github.com\/scipy\/scipy\/assets\/69480843\/b358d452-216a-4382-9525-38e30562047a)\r\n\r\nAfter scaling\r\n![169402582-5331019f-9093-4456-8a40-94c900de6c9c](https:\/\/github.com\/scipy\/scipy\/assets\/69480843\/1ee79e21-1761-420a-aa7b-39a1bf73e5bc)\r\n\r\n### Describe the solution you'd like.\r\n\r\nIt will be best to modify `scipy.stats.qmc.PoissonDisk` to optionally accept, along with the existing dimension parameter `d`, a d-dimensional `u_bounds` parameter similar to `scipy.stats.qmc.scale`.  If `u_bounds` is left unspecified the Poisson disk sampling will proceed on the unit hypercube.  If it is specified the samples will be scaled prior to radius consideration.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nSample scaling via `scipiy.stats.qmc.scale` is not possible due to the distortion it introduces.\r\n\r\nSubclassing `scipy.stats.qmc.PoissonDisk` to allow pre-scaling is not easy due to the tight coupling of the radius parameter and the initialization of the cell grid.\r\n\r\nCurrently it's necessary to either create a parallel PoissonDisk implementation or to not use SciPy for Poisson disk sampling from arbitrary bounds.\r\n\r\nIt's possible to calculate the image aspect ratio and do rejection sampling on the unit hypercube, rejecting samples that lie outside a \"hyperrectangle\".  For 2D that looks like this:\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.stats import qmc\r\n\r\n\r\nclass AspectRatioRejection:\r\n    def __init__(self, width: int, height: int):\r\n        \"\"\"Rejection sampling to get around scipy.stats.qmc.PoissonDisk scaling issues.\r\n\r\n        See https:\/\/github.com\/scipy\/scipy\/issues\/20288.\r\n        \"\"\"\r\n        if width >= height:\r\n            self.xmax = 1.0\r\n            self.ymax = height \/ width\r\n        else:\r\n            self.xmax = width \/ height\r\n            self.ymax = 1.0\r\n\r\n    def __call__(self, sample: tuple[float, float]) -> bool:\r\n        \"\"\"Reject sample if it's outside the aspect ratio.\"\"\"\r\n        return sample[0] > self.xmax or sample[1] > self.ymax\r\n\r\n\r\ndef fill_space_blue_noise_samples(\r\n    width: int,\r\n    height: int,\r\n    radius_fraction: float = 0.05,\r\n    rng: np.random.Generator | None = None\r\n):\r\n    \"\"\"Fill image area with blue noise samples.\r\n\r\n    Samples are spaced at least `radius_fraction` apart, where radius_fraction is a fraction of the image width.\r\n    \"\"\"\r\n    aspect_reject = AspectRatioRejection(width, height)\r\n\r\n    rng = np.random.default_rng() if rng is None else rng\r\n    engine = qmc.PoissonDisk(d=2, radius=radius_fraction, seed=rng)\r\n    samples = [s * max(width, height) for s in engine.fill_space() if not aspect_reject(s)]\r\n\r\n    return samples\r\n```\r\n\r\nHowever, the performance gets worse and worse as the space's aspect ratio gets larger.  For the case of a line, all samples will be almost surely rejected.  However, it's very easy (and performant) to generate samples in a lower dimensional space so this performance hit can be avoided entirely through prescaling.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":[],"labels":["scipy.stats","enhancement"]},{"title":"WIP: stats.gstd: add array API support","body":"#### Reference issue\r\nTowards gh-18867\r\n\r\n#### What does this implement\/fix?\r\nAdds array API support to `gstd`.\r\n\r\n#### Additional information\r\nCan we deprecate masked array support for this function to simplify things?\r\n\r\nI think this would pass tests, but:\r\n```python3\r\nimport torch\r\nfrom scipy._lib._array_api import array_namespace\r\nx = torch.asarray([1, 2, 3., 3, 4.5])\r\nxp = array_namespace(x)\r\nxp.std(x, correction=1)\r\n```\r\nresults in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/IPython\/core\/interactiveshell.py\", line 3526, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-17-a926c1507628>\", line 1, in <module>\r\n    xp.std(x, correction=1)\r\n  File \"\/Users\/matthaberland\/Desktop\/scipy\/scipy\/_lib\/array_api_compat\/array_api_compat\/torch\/_aliases.py\", line 396, in std\r\n    res = torch.std(x, tuple(range(x.ndim)), correction=_correction, **kwargs)\r\n                                                        ^^^^^^^^^^^\r\nUnboundLocalError: cannot access local variable '_correction' where it is not associated with a value\r\n```\r\nMaybe it's a real bug, or maybe I have an old version of something?","comments":["I get a different error\r\n\r\n---\r\n\r\nEdit: I've just reproduced your error on array-api-compat main without SciPy, will report upstream","Thanks @lucascolley. \r\n\r\nDo you know if there's a canonical way to get the `finfo`\/`iinfo` attributes of non-NumPy dtypes? I'm thinking for array `x` e.g.\r\n```python3\r\nnp.finfo(getattr(np, str(x.dtype)))\r\n```\r\nSometimes it's needed to get a default numerical tolerance.\r\n\r\nAlso, for operations that previously have converted to `float64` when the input was of integer `dtype`, do we now just let the operation produce nonsense results, error out, or do the conversion? (And if we're supposed to error out or do the conversion, how do we determine the dtype kind? `'int' in str(x.dtype)`?)","> Do you know if there's a canonical way to get the finfo\/iinfo attributes of non-NumPy dtypes?\n\nThey are in the spec, https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.finfo.html.\n\n> Also, for operations that previously have converted to float64 when the input was of integer dtype, do we now just let the operation produce nonsense results, error out, or do the conversion?\n\nI think we just preserve backwards compatibility here. This effort is about preserving array types, not dtypes. (Separately deprecating integer inputs is orthogonal, I remember that happening for some other function recently.)\n\n```\nxp.isdtype(x.dtype, 'integral')\n```\n\nSee https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.isdtype.html","Thanks @lucascolley I thought I knew that those *weren't* present, so I didn't look!","@lucascolley looks like the upstream issue was fixed. Can we just point the submodule to the latest commit?","> UnboundLocalError: cannot access local variable '_correction'\r\n\r\nIf you pull in a new commit of `array_api_compat@main` this should be fixed (maybe want to wait for the next release, which is planned for about ~1 week away, by the sounds of it)","I'll go ahead and do that to get CI working, then we can pull in the released version before merging.","Looks like pulling that in alone doesn't work, probably because of https:\/\/github.com\/scipy\/scipy\/pull\/19900#issuecomment-1896267203. Maybe it will be easier when NumPy 2.0 is released.","Hmm, I'm pretty sure it should work. No harm in waiting though \ud83e\udd37\u200d\u2642\ufe0f\r\n\r\n> probably because of https:\/\/github.com\/scipy\/scipy\/pull\/19900#issuecomment-1896267203\r\n\r\nThis shouldn't be an issue anymore now that we pull `array-api-strict` from `pip`.","> Hmm, I'm pretty sure it should work.\r\n\r\nI get an error when trying to run tests.\r\n```python3\r\n\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/bin\/python \/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pycharm\/_jb_pytest_runner.py --target tests\/test_stats.py::TestGeometricStandardDeviation \r\nTesting started at 8:34 AM ...\r\nLaunching pytest with arguments tests\/test_stats.py::TestGeometricStandardDeviation --no-header --no-summary -q in \/Users\/matthaberland\/Desktop\/scipy\/scipy\/stats\r\n\r\nImportError while loading conftest '\/Users\/matthaberland\/Desktop\/scipy\/scipy\/conftest.py'.\r\n..\/conftest.py:15: in <module>\r\n    from scipy._lib._array_api import SCIPY_ARRAY_API, SCIPY_DEVICE\r\n..\/_lib\/_array_api.py:17: in <module>\r\n    from scipy._lib.array_api_compat import (\r\n..\/_lib\/array_api_compat\/array_api_compat\/numpy\/__init__.py:18: in <module>\r\n    __import__(__package__ + '.fft')\r\nE   ModuleNotFoundError: No module named 'scipy._lib.array_api_compat.numpy.fft'\r\n```\r\nI manually bisected this and found that 02059d0d8f025724c08ff757baa6b48a579651ea doesn't give this error and 0b6ddcd26cac700e6ff9d5fb42d2476f6cc8aac6 does ([commit history](https:\/\/github.com\/data-apis\/array-api-compat\/commits\/main\/?before=8606188f703004ab2f65e709252a2d6db6c80972+105)).\r\n","aha, this is `meson.build` stuff - see my recent changes in gh-20085","I might put this on hold or close and reopen - I think the underlying function and tests need work before this should be merged.\r\n\r\nThere was a lot of care taken to ensure that the function raised errors with certain input. For example,\r\n```python3\r\nfrom scipy import stats\r\nstats.gstd([1, 2, 3, np.inf])\r\n# ValueError: Infinite value encountered. The geometric standard deviation is defined for strictly positive values only.\r\n```\r\nBut according to the error message itself, the geometric standard deviation is defined for strictly positive values, and `inf` is strictly positive, so the geometric standard deviation can reasonably be defined as `inf`. \r\n\r\nThis is particularly important when the input is an array: we would not want an infinite value in one slice to cause an error for all slices. \r\n\r\nI think we would prefer for the behavior to be consistent with that of `gmean`:\r\n```python3\r\nstats.gmean([1, 2, 3, np.inf])  # np.inf\r\nstats.gmean([-1, 1, 2, 3])  # np,nan\r\n```\r\nwhich itself follows NumPy's lead in returning infinities or NaNs as appropriate.\r\n\r\nFurthermore, the approach was to `try`\/`except` (for speed), but the warnings are backend dependent. `torch`, for example, doesn't emit the warnings the function is looking for. \r\n\r\nI think simplifying the implementation would allow us to simplify the tests considerably. Testing a few cases against a reference implementation plus one comprehensive property-based test would do the trick. That would really simplify this PR, too.\r\n\r\nI'll work with the rest of the stats folks to decide what we should do here.\r\n\r\n"],"labels":["scipy.stats","enhancement","array types"]},{"title":"ENH: stats.pearsonr: add Array API support","body":"#### Reference issue\r\ngh-20137\r\n\r\n#### What does this implement\/fix?\r\nThis explores the addition of Array API support to `scipy.stats.pearsonr`. Only the last commit is relevant; the others are from gh-20137.\r\n\r\n#### Additional information\r\nNeed to resolve merge conflicts and skip a test on 32-bit, but otherwise, I think this is ready to go.\r\n\r\n---\r\n*Old news:*\r\n\r\nMost of this will be pretty straightforward. There are some little things I'll want to address later (e.g. previously, `pearsonr` converted inputs to be at least `float64`, but I imagine we'd want to respect dtype with array API), but there is one big question for now:\r\n\r\nCalculation of the p-value currently relies on the incomplete beta function, which is not among the special functions for which we have experimental array API support (gh-19023). Even if it were, calculation of the p-value currently relies on the distribution infrastructure. In any case, calculation of the statistic with an alternative array backend can easily be done now, but calculation of the p-value with an alternative array backend will take more time. \r\n\r\nFor vectorized calculations, I think there is still value in calculating the statistic with the alternative array backend, then converting the statistic (which has been reduced by at least one dimension) to a NumPy array for calculation of the p-value. Are there objections to this?\r\n\r\nI know there have been objections to converting non-NumPy arrays to NumPy arrays for running compiled code, but I think this is a little different, since the statistic might not even be an array after the reducing operation, and if it is, it's smaller than the original array.","comments":["> For vectorized calculations, I think there is still value in calculating the statistic with the alternative array backend, then converting the statistic (which has been reduced by at least one dimension) to a NumPy array for calculation of the p-value. Are there objections to this?\n\nThis sounds right to me. As long as we avoid converting back-and-forth multiple times, one pair of conversions is necessary (unless someone decides to write the special functions in pure Python \ud83d\ude05 (or the special extension gets into the standard...) and the distn. infra gets support).","Yes, in the near term, the new distribution infrastructure will be able to evaluate the special functions in an array API compatible way, and I will give the resampling methods array API support soon, too. So this would just be temporary, probably for one release only, if that.\r\n\r\nFurther out, yeah, the special function array API extension (https:\/\/github.com\/data-apis\/array-api\/issues\/725) would speed things up considerably. (Oops looks like you mentioned it in an update, but maybe good to have the link here for others.)\r\n\r\nSo in the meantime, is there a canonical way to do the conversion? `np.asarray` alone doesn't cut it, right?","> So in the meantime, is there a canonical way to do the conversion? np.asarray alone doesn't cut it, right?\n\nJust use `np.asarray` for now is my suggestion (and `xp.asarray` at the end). There is no universally portable solution, but the idiomatic way going forward will be to use `from_dlpack` and co. That has seen updates quite recently and we don't use it anywhere yet, so it's probably easiest to update wholesale across submodules at a later date. Plus, `np.asarray` is sufficient for the backends with which we test in CI.","> do you want to add the CI check in this PR?\r\n\r\nOops, missed that. \r\nSo maybe:\r\n```\r\n        python dev.py --no-build test -b all -t scipy.stats.tests.test_stats -- --durations 3 --timeout=60\r\n```\r\nat the end of `array_api.yml`?\r\n","Yeah, looks correct \ud83d\udc4d","How do test with `array_api_strict` locally?\r\nIIRC, NumPy's strict array API class doesn't support indexing with `[()]`. If that's right, it's going to be a pain to return scalars instead of 0d arrays with regular NumPy. ","> How do I do test with array_api_strict locally?\r\n\r\n```\r\npip install array-api-strict\r\npython dev.py test -t scipy.stats.tests.test_stats -b array_api_strict\r\n```","Thanks!\r\nSince array API support is behind an environment variable, can we rely on the most recent version of the specification? CI is telling me that `moveaxis` is not an attribute of `array_api_strict` (probably because it is not in NumPy 1.26 `array_api`?), but `moveaxis` it is part of the [2013.12 specification](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.moveaxis.html).","> CI is telling me that moveaxis is not an attribute of array_api_strict\r\n\r\nLooks like `array-api-strict` just needs to catch up with the 2023.12 changes - `moveaxis` was added to the spec in September https:\/\/github.com\/data-apis\/array-api\/pull\/656.\r\n\r\nFor now I would recommend\r\n\r\n```python\r\n@skip_if_array_api('array_api_strict', reasons=['`xp.moveaxis` not yet implemented by array-api-strict'])\r\n```","I had to create my own `_move_axis_to_end` function to get past that failure and see what else was wrong. I also added a `_clip` function and fixed the uses of `[()]` to get all the tests passing, so I went ahead and left everything in there. We can replace `_move_axis_to_end` and `_clip` when `array_api_strict` is updated.\r\n\r\nRegarding `[()]` - These four lines used to appear all over the place in `stats` to ensure that we return scalars instead of 0D arrays:\r\n```python3\r\nif res.ndim == 0:\r\n    return res.item\r\nelse:\r\n    return res\r\n```\r\nUsing`res[()]` is not only more compact, but it also ensures that the values are NumPy scalars instead of Python scalars. This is more consistent with NumPy's behavior.\r\n\r\n`array_api_strict` allows the use of `[()]` on 0D arrays, but raises an `IndexError` when the array has 1 or more dimensions, so I replaced this idiom with a ternary like `res[()] if res.ndim == 0 else res`. This is not so bad, but is there another option I'm missing?","> array_api_strict allows the use of [()] on 0D arrays, but raises an IndexError when the array has 1 or more dimensions, so I replaced this idiom with a ternary like res[()] if res.ndim == 0 else res. This is not so bad, but is there another option I'm missing?\n\n@rgommers has this point about returning scalars come up previously?","> @rgommers has this point about returning scalars come up previously?\r\n\r\nI don't think so. This pattern is pretty specific to SciPy.\r\n\r\n> Using`res[()]` is not only more compact, but it also ensures that the values are NumPy scalars instead of Python scalars. This is more consistent with NumPy's behavior.\r\n\r\nAgreed, that is better.\r\n\r\n> so I replaced this idiom with a ternary like `res[()] if res.ndim == 0 else res`. This is not so bad, but is there another option I'm missing?\r\n\r\nThat looks okay to me; I don't think it'll get clearer\/shorter than that."],"labels":["scipy.stats","enhancement","array types"]},{"title":"ENH: Add `affine_transform2` or similar for better usability combining transforms","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nRelated to https:\/\/github.com\/scipy\/scipy\/issues\/2255\r\n\r\nIn order to perform a rotation, the matrix rotation matrix has to be inverted and the offset is in the rotated space making these operations less friendly.\r\n\r\n### Describe the solution you'd like.\r\n\r\nMake a helper function `affine_transform2(data, matrix=rotation_matrix, origin=center_of_rotation, offset=offset_in_pre_translated space)` that makes this functionality intuitive (modified from @NOhs 's post).\r\n\r\n### Describe alternatives you've considered.\r\n\r\nThe original function could be modified but that would be more work and backward incompatible.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nHere is some code I have used to do this transform that could be wrapped into this intuitive `affine_transform2` function\r\n\r\n```\r\n# first apply translation to center\r\naffine_trans = np.eye(4)\r\naffine_trans[:3, 3] = -center_of_rotation\r\nimage_trans = affine_transform(image, affine_trans)\r\n\r\n# next rotate around center\r\ncenter = 0.5 *np.array( image.shape)\r\naffine_rot = np.eye(4)\r\naffine_rot[:3, :3] = rotation_matrixi[:3, :3].T\r\naffine_rot[:3, 3] = center - np.dot(affine_rot[:3, :3], center)\r\nimage_rot = affine_transform(image_trans, affine_rot)\r\n\r\n# finally apply offset\r\naffine_trans2 = np.eye(4)\r\naffine_trans2[:3, 3] = offset_in_pre_translated_space - center_of_rotation  # put center back\r\nimage_moved = affine_transform(image_rot, affine_trans2)\r\n```\r\n\r\nAs evidenced by the above code, this is not trivial for users to figure out but I think a simple wrapper function for this code would solve this issue as long as it was referenced in the see also of `affine_transform`.\r\n\r\nAlso including @timday","comments":["I think you meant to link to gh-2255","My bad, fixed"],"labels":["enhancement","scipy.ndimage"]},{"title":"MAINT\/DEV: enforce minimum `ruff` version","body":"#### Reference issue\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/20255#issuecomment-2003146589 @ev-br \r\n\r\n#### What does this implement\/fix?\r\nInstead of the rather annoying error message observed in the linked issue, we now have a nice error message, \"Linting requires `ruff>=0.0.292`. Please upgrade `ruff`.\"\r\n\r\n#### Additional information\r\nThis is a follow-up to gh-20070.\r\n\r\nMy IDE seems to have made some stylistic changes in `pyproject.toml`. These look good to me, but let me know if they should be removed.\r\n","comments":["Much nicer indeed!\r\n\r\nI'd hit the green button straight away were it not for unrelated stylistic changes pyproject.html. I personally don't care either way, but have no idea if somebody else does.\r\n\r\n(fat-fingered the open\/close, sorry for the noise)\r\n","Also an unrelated doc improv possibility : \r\n\r\n```\r\nFound 6 errors.\r\n[*] 6 fixable with the `--fix` option.\r\n```\r\n\r\ngreat, I'd be happy to auto-fix, where does that `--fix` option go?\r\n","We don't currently have auto-fix built into `python dev.py lint` - I'll have a look at adding it"],"labels":["maintenance","CI","DX"]},{"title":"BUG: Incorrect variable assignment in optimize._trust_region_exact","body":"#### Reference issue\r\nFixes gh-20244\r\n\r\n#### What does this fix?\r\nResult of cholesky factorization in disregarded in `IterativeSubproblem.solve()`.\r\nThese changes use the result on successful factorization.","comments":["Maybe we could consider adding a test based on the original issue?"],"labels":["defect","scipy.optimize"]},{"title":"TST: new test failures (mostly crashes) with scipy 1.12 and OpenBLAS 0.3.26","body":"I hadn't gotten around to evaluating the BLAS flavour [analysis](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/267) in conda-forge for a while, but now that I did, there's a bunch of new failures on windows together with OpenBLAS (in addition to the pre-existing https:\/\/github.com\/scipy\/scipy\/issues\/17125):\r\n\r\n```\r\nFAILED _lib\/tests\/test_import_cycles.py::test_public_modules_importable - AssertionError: Failed to import scipy.cluster\r\nFAILED interpolate\/tests\/test_interpnd.py::TestCloughTocher2DInterpolator::test_dense\r\nFAILED linalg\/tests\/test_basic.py::TestLstsq::test_random_exact\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[False-float32]\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[False-float64]\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[False-complex64]\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[True-float32]\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[True-float64]\r\nFAILED sparse\/linalg\/tests\/test_propack.py::test_examples[True-complex64]\r\nFAILED spatial\/tests\/test_qhull.py::TestUtilities::test_more_barycentric_transforms\r\nFAILED spatial\/tests\/test_qhull.py::TestUtilities::test_degenerate_barycentric_transforms\r\n```\r\n\r\nIn more detail:\r\n\r\n\r\n<details>\r\n<summary>Details for failures in <code>_lib\/tests\/test_import_cycles.py<\/code><\/summary>\r\n\r\n```\r\n_______________________ test_public_modules_importable ________________________\r\n[gw0] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\n..\\_test_env\\Lib\\site-packages\\scipy\\_lib\\tests\\test_import_cycles.py:14: in test_public_modules_importable\r\n    assert pid.wait() == 0, f'Failed to import {PUBLIC_MODULES[i]}'\r\nE   AssertionError: Failed to import scipy.fftpack\r\nE   assert 1 == 0\r\nE    +  where 1 = <bound method Popen.wait of <Popen: returncode: 1 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>>()\r\nE    +    where <bound method Popen.wait of <Popen: returncode: 1 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>> = <Popen: returncode: 1 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>.wait\r\n        i          = 6\r\n        pid        = <Popen: returncode: 1 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>\r\n        pids       = [<Popen: returncode: 0 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>, <Popen: returncode: 0 args: ['C:\\\\b...t_1709765858947\\\\_test_env\\...>, <Popen: returncode: 0 args: ['C:\\\\bld\\\\scipy-split_1709765858947\\\\_test_env\\...>, ...]\r\n```\r\n\r\n<\/details>\r\n\r\nAll the other failures are crashes, though it's worth noting that they _don't_ always appear.\r\n\r\n\r\n<details>\r\n<summary>Crashes<\/summary>\r\n\r\n```\r\n_____________________ interpolate\/tests\/test_interpnd.py ______________________\r\n[gw1] win32 -- Python 3.10.13 %PREFIX%\\python.exe\r\nworker 'gw1' crashed while running 'interpolate\/tests\/test_interpnd.py::TestCloughTocher2DInterpolator::test_dense'\r\n_________________________ linalg\/tests\/test_basic.py __________________________\r\n[gw1] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw1' crashed while running 'linalg\/tests\/test_basic.py::TestLstsq::test_random_exact'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw4] win32 -- Python 3.10.13 %PREFIX%\\python.exe\r\nworker 'gw4' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[False-float32]'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw0] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw0' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[False-float64]'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw5] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw5' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[False-complex64]'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw7] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw7' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[True-float32]'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw8] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw8' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[True-float64]'\r\n_____________________ sparse\/linalg\/tests\/test_propack.py _____________________\r\n[gw9] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw9' crashed while running 'sparse\/linalg\/tests\/test_propack.py::test_examples[True-complex64]'\r\n_________________________ spatial\/tests\/test_qhull.py _________________________\r\n[gw2] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw2' crashed while running 'spatial\/tests\/test_qhull.py::TestUtilities::test_degenerate_barycentric_transforms'\r\n_________________________ spatial\/tests\/test_qhull.py _________________________\r\n[gw4] win32 -- Python 3.12.2 %PREFIX%\\python.exe\r\nworker 'gw4' crashed while running 'spatial\/tests\/test_qhull.py::TestUtilities::test_more_barycentric_transforms'\r\n```\r\n\r\n<\/details>","comments":[],"labels":["defect"]},{"title":"WIP: ENH: optimize: const qualify Cython array arguments","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/20196\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nThis PR const qualifies cython arrays where possible.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\nNote that as I'm not super familiar with the Cython side of `optimize` I'll be taking this a bit more slowly. This is still a work in progress. \r\n\r\nIn particular; I'll need help figuring out how the more complex bindings work (e.g. for the Highs solver)","comments":["@mdhaber I was looking at the signature for [_highs_wrapper](https:\/\/github.com\/scipy\/scipy\/blob\/9a5118633252eb714e1abcafba036d4295a146be\/scipy\/optimize\/_highs\/cython\/src\/_highs_wrapper.pyx#L238-L248) - I think many of these could be marked as `const`. However when I tried marking `c` as `const` I got a very complicated error message:\r\n\r\n```\r\nninja: Entering directory `\/home\/kai\/Projects\/scipy\/build'\r\n[4\/7] Generating scipy\/generate-version with a custom command\r\nfatal: bad revision '^v1.11.0'\r\n[5\/7] Generating 'scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/_highs_wrapper.cpp'\r\nwarning: \/home\/kai\/Projects\/scipy\/scipy\/optimize\/_highs\/cython\/src\/_highs_wrapper.pyx:70:21: noexcept clause is ignored for function returning Python object\r\nwarning: \/home\/kai\/Projects\/scipy\/scipy\/optimize\/_highs\/cython\/src\/_highs_wrapper.pyx:115:18: noexcept clause is ignored for function returning Python object\r\nwarning: \/home\/kai\/Projects\/scipy\/scipy\/optimize\/_highs\/cython\/src\/_highs_wrapper.pyx:612:22: Assigning to 'double *' from 'const double *' discards const qualifier\r\n[6\/7] Compiling C++ object scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/meson-generated__highs_wrapper.cpp.o\r\nFAILED: scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/meson-generated__highs_wrapper.cpp.o \r\n\/home\/kai\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ -Iscipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/optimize\/_highs -I..\/scipy\/optimize\/_highs -I..\/scipy\/optimize\/_highs\/cython\/src -I..\/scipy\/optimize\/_highs\/src -I..\/scipy\/_lib\/highs\/src -I..\/scipy\/_lib\/highs\/src\/io -I..\/scipy\/_lib\/highs\/src\/lp_data -I..\/scipy\/_lib\/highs\/src\/util -I..\/..\/..\/miniconda3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include -I\/home\/kai\/miniconda3\/envs\/scipy-dev\/include\/python3.11 -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -D_GLIBCXX_ASSERTIONS=1 -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++17 -O2 -g -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/kai\/miniconda3\/envs\/scipy-dev\/include -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/kai\/miniconda3\/envs\/scipy-dev\/include -fPIC -pthread -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -Wno-class-memaccess -Wno-format-truncation -Wno-non-virtual-dtor -Wno-sign-compare -Wno-switch -Wno-unused-but-set-variable -Wno-unused-variable '-DCMAKE_BUILD_TYPE=\"RELEASE\"' -DFAST_BUILD=ON '-DHIGHS_GITHASH=\"n\/a\"' '-DHIGHS_COMPILATION_DATE=\"2021-07-09\"' -DHIGHS_VERSION_MAJOR=1 -DHIGHS_VERSION_MINOR=2 -DHIGHS_VERSION_PATCH=0 -DHIGHS_DIR=\/home\/kai\/Projects\/scipy\/scipy\/optimize\/_highs\/..\/..\/_lib\/highs -UOPENMP -UEXT_PRESOLVE -USCIP_DEV -UHiGHSDEV -UOSI_FOUND -DNDEBUG -DCYTHON_CCOMPLEX=0 -MD -MQ scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/meson-generated__highs_wrapper.cpp.o -MF scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/meson-generated__highs_wrapper.cpp.o.d -o scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/meson-generated__highs_wrapper.cpp.o -c scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/_highs_wrapper.cpp\r\nscipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/_highs_wrapper.cpp: In function 'PyObject* __pyx_pf_5scipy_8optimize_6_highs_6cython_3src_14_highs_wrapper__highs_wrapper(PyObject*, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, __Pyx_memviewslice, PyObject*)':\r\nscipy\/optimize\/_highs\/_highs_wrapper.cpython-311-x86_64-linux-gnu.so.p\/_highs_wrapper.cpp:23496:28: error: invalid conversion from 'const double*' to 'double*' [-fpermissive]\r\n23496 |     __pyx_v_colcost_ptr = (&(*((double const  *) ( \/* dim=0 *\/ ((char *) (((double const  *) __pyx_v_c.data) + __pyx_t_5)) ))));\r\n      |                           ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                            |\r\n      |                            const double*\r\nninja: build stopped: subcommand failed.\r\nBuild failed!\r\n```\r\n\r\nI'm sure that I could go through and decipher it (eventually) but I was wondering if you knew what would be involved in converting these arguments to `const`? And if it would be worth the time to understand what is going on?","With #19255 on the horizon it might not be worth your time looking at the highs wrappers","Good catch @j-bowhay. I'll leave that off the list to `const` qualify"],"labels":["enhancement","scipy.optimize","Cython"]},{"title":"ENH:linalg:Implement explicit formula for 2x2 matrices to compute matrix exponential","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nCurrently, all sizes of arrays in `linalg.expm` go through the same Pade approximation scale\/square method. However, for 2x2 arrays there is an explicit way to compute the matrix exponential. This explicit implementation requires a numerically robust discriminant computation. Hence requires Kahan's formulas.\r\n\r\n### Describe the solution you'd like.\r\n\r\nThe formula can be found in Awad H. Al-Mohy and Nicholas J. Higham, (2009), \"A New Scaling and Squaring Algorithm for the Matrix Exponential\", SIAM J. Matrix Anal. Appl. 31(3):970-989, :doi:`10.1137\/09074721X` and also elsewhere citing this paper.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nOur previous implementation (removed in #20261); \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/ff99a9b900785c71bd049ff72f7eeecd9c08d31c\/scipy\/linalg\/_matfuncs.py#L306-L328\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":[],"labels":["enhancement","scipy.linalg"]},{"title":"MAINT, BLD: symbol visibility warnings on MacOS ARM static lib with clang c++17","body":"With `meson` `1.4.0`, on latest `main` (bd42f896a), via `FC=gfortran-12 python dev.py test -j 8` I see the build output warning below, though the full suite passes otherwise. Using `Apple clang version 15.0.0 (clang-1500.3.9.4)`.\r\n\r\n```\r\n..\/scipy\/_lib\/highs\/src\/util\/HighsDataStack.h:27:34: note: expanded from macro 'IS_TRIVIALLY_COPYABLE'\r\n#define IS_TRIVIALLY_COPYABLE(T) __has_trivial_copy(T)\r\n                                 ^\r\n29 warnings generated.\r\n[1477\/1477] Linking target scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-darwin.so\r\nld: warning: direct access in function '__pyx_pymod_exec__highs_wrapper(_object*)' from file 'scipy\/optimize\/_highs\/_highs_wrapper.cpython-311-darwin.so.p\/meson-generated__highs_wrapper.cpp.o' to global weak symbol 'std::__1::piecewise_construct' from file 'scipy\/optimize\/_highs\/libhighs.a(.._..__lib_highs_extern_filereaderlp_reader.cpp.o)' means the weak symbol cannot be overridden at runtime. This was likely caused by different translation units being compiled with different visibility settings.\r\n<snip more similar warnings>\r\n```\r\n\r\nLooks like static library symbol visibility is a bit annoying to fix on MacOS perhaps: https:\/\/stackoverflow.com\/questions\/2222162\/how-to-apply-fvisibility-option-to-symbols-in-static-libraries\r\n\r\nBisected to commit below, C++ standards fun I guess. Perhaps gh-19255 will help with this, but not fully sure what's going on over there.\r\n\r\n```\r\n10c2a45586d8a169a0407efc90753ebfc0dbd626 is the first bad commit\r\ncommit 10c2a45586d8a169a0407efc90753ebfc0dbd626\r\nAuthor: Lucas Colley <lucas.colley8@gmail.com>\r\nDate:   Fri Jan 5 14:10:28 2024 +0000\r\n\r\n    BLD: set default `cpp_std` to `c++17`\r\n    [wheel build] [skip circle]\r\n\r\n meson.build                              | 2 +-\r\n scipy\/io\/_fast_matrix_market\/meson.build | 1 -\r\n 2 files changed, 1 insertion(+), 2 deletions(-)\r\n```\r\n","comments":["Ah, I suppose I missed these warnings during that PR due to all of the other warnings which are there atm... I did notice this recently."],"labels":["Build issues","maintenance"]},{"title":"ENH: special function to compute iv(v+1,x)\/iv(v,x) and its inverse","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nTo compute the MLE of the `vonmises` (gh-20102) and the `vonmises_fisher` distribution, one needs to compute $I_{\\nu+1}(\\kappa)\/I_\\nu(\\kappa)$ (if using numerical solver) or its inverse (if using analytical formula), for $\\nu=0, 0.5, 1, \\cdots$ and $\\kappa > 0$.\r\n\r\n### Describe the solution you'd like.\r\n\r\nAdd `scipy.special.iv_ratio(v,z)` to compute $I_{\\nu+1}(z)\/I_{\\nu}(z)$ for $\\nu \\in \\mathbb{R}$ and $z \\in \\mathbb{Z}$.\r\n\r\nAdd `scipy.special.iv_ratio_inv(v,r)` to compute $x$ such that $I_{\\nu+1}(x)\/I_{\\nu}(x)=r$ for $\\nu \\ge 0$ and $0 \\le r \\le 1$.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nTo compute the ratios, the straightforward formula `ive(v+1,x)\/ive(v,x)` doesn\u2019t achieve superb accuracy at least for large `x`. (In addition, the current implementation of `ive` returns `nan` for very large `x`, though that's a separate issue to be addressed by gh-18088.)\r\n\r\nTo compute the inverse, the current implementation in `vonmises` and `vonmises_fisher` uses Brent\u2019s method. This probably works for most practical purposes, but doesn\u2019t achieve superb accuracy from a numerical perspective; for example, for $r$ very close to $1$ the relative error can easily go above 30%.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nThere are some known bounds and series expansions that may improve the accuracy and possibly efficiency for real $\\nu$ and real $z$, but I am yet to verify them.\r\n\r\nIt\u2019s always possible to start by implementing them the way they're done today (i.e. straightforward division and Brent solver), if it is deemed useful enough to have these special functions in scipy.","comments":["Below is a summary of prior results on computing $r_\\nu(x) \\equiv I_{\\nu+1}(x)\/I_\\nu(x)$ for $\\nu \\ge 0$ and $x > 0$, and $r_\\nu^{-1}(\\rho)$ for $\\nu \\ge 0$ and $0 < \\rho < 1$.\r\n\r\n_Note_. The notation for $I_{\\nu+1}\/I_\\nu(x)$ varies in the literature. Below we follow the notation used by Amos (1974) and Hornik and Gr\u00fcn (2014).\r\n\r\n**Bounds of $r_\\nu$ and $r_\\nu^{-1}$**\r\n\r\nAmos (1974) derived simple and good bounds for $r_\\nu(x)$ of the form\r\n\r\n$$\r\nB_{\\alpha,\\beta}(x) := \\frac{x}{\\alpha+\\sqrt{x^2+\\beta^2}}\r\n$$\r\n\r\nSpecifically, he showed that\r\n\r\n$$\r\n\\begin{align}\r\nB_{\\nu+1,\\nu+1}(x) &\\le r_\\nu(x) \\le B_{\\nu,\\nu+2}(x) \\textrm{ (eq. 11)} \\\\\r\nB_{\\nu+0.5,\\nu+1.5}(x) &\\le r_\\nu(x) \\le B_{\\nu+0.5,\\nu+0.5}(x) \\textrm{ (eq. 16)} \\\\\r\n\\end{align}\r\n$$\r\n\r\nSegura (2023, Lemma 3) showed that the lower bound of (16) and the upper bound of (11) is \u201cPareto optimal\u201d in the sense that there is no bound of that form that is sharper for  all $x > 0$. But the following upper bound is sharper for sufficiently large $x$ (Segura, 2023, Corollary 1):\r\n\r\n$$\r\nB_{\\nu+0.5,\\beta^\\ast(\\nu)}(x) \\equiv B_{\\nu+0.5,\\sqrt{(\\nu+0.5)(\\nu+1.5)}}(x)\r\n$$\r\n\r\nThe utility of the above bounds is that they can be conveniently inverted to produce bounds on $r_\\nu^{-1}(\\rho)$. Indeed, Hornik and Gr\u00fcn (2014, Theorem 1) showed that\r\n\r\n$$\r\n\\max\\left( B_{\\nu,\\nu+2}^{-1}(\\rho), B_{\\nu+0.5,\\beta^\\ast(\\nu)}^{-1}(\\rho) \\right) \\le r_\\nu^{-1}(\\rho) \\le B_{\\nu+0.5,\\nu+1.5}^{-1}(\\rho)\r\n$$\r\n\r\nwhere\r\n\r\n$$\r\nB_{\\alpha,\\beta}^{-1}(\\rho) = \\frac{\\rho}{1-\\rho^2}\\left(\\alpha+\\sqrt{ \\rho^2\\alpha^2 +(1-\\rho^2)\\beta^2} \\right)\r\n$$\r\n\r\nThe (absolute) width of the above interval is no larger than $1.5$ (Hornik and Gr\u00fcn , 2014, Theorem 2).\r\n\r\n**Computing $r_\\nu$**\r\n\r\nGautschi and Slavik (1978) studied the numerical properties of two continued fractions for computing $r_\\nu(x)$. _Gauss\u2019 continued fraction_ is given by\r\n\r\n$$\r\n\\frac{I_{\\nu+1}(x)}{I_\\nu(x)} = \r\n\\frac{1}{\\frac{2}{x}(\\nu+1)+\\textrm{}}\r\n\\frac{1}{\\frac{2}{x}(\\nu+2)+\\textrm{}}\r\n\\frac{1}{\\frac{2}{x}(\\nu+3)+\\textrm{}}\r\n\\cdots\r\n$$\r\n\r\n_Perron\u2019s continued fraction_ is given by (note the shift of index)\r\n\r\n$$\r\n\\frac{I_{\\nu}(x)}{I_{\\nu-1}(x)} = \r\n\\frac{x}{2\\nu+x-{}}\r\n\\frac{(2\\nu+1)x}{2(\\nu+x)+1-{}} \r\n\\frac{(2\\nu+3)x}{2(\\nu+x)+2-{}} \r\n\\frac{(2\\nu+5)x}{2(\\nu+x)+3-{}}\r\n\\cdots\r\n$$\r\n\r\nThey found Perron\u2019s c.f. to converge significantly faster than Gauss\u2019 c.f. when $x \\gg \\nu$, and only marginally slower otherwise, making it more attractive for numerical evaluation.\r\n\r\n<!--\r\nThis finding is confirmed qualitatively by Tretter and Walster (1980) using an a priori bound on the number of terms required for convergence; however, those bounds are too coarse for numerical purpose.\r\n-->\r\n\r\nSeparately, Amos (1974) proposed a recurrence formula to compute $r_\\nu(x)$. Hill (1981) derived specialized continued fractions for $r_0(x)$ and $r_{0.5}(x)$.\r\n\r\n**Computing $r_\\nu^{-1}$**\r\n\r\nUsing the bounds of $r_\\nu^{-1}$ to bracket the root, Hornik and Gr\u00fcn (2014) compared Brent\u2019s method and the [Newton-Fourier method](https:\/\/en.m.wikipedia.org\/wiki\/Newton's_method#Newton\u2013Fourier_method) to solve $r_\\nu(x)=\\rho$. (Newton\u2019s method is guaranteed to converge in the absence of round-off error as $r_\\nu(x)$ is concave.) They find that the Newton-Fourier method  requires fewer iterations on average. Separately, they showed that (Theorem 4)\r\n\r\n$$\r\nr_\\nu^{-1}(\\rho)=B_{\\nu+0.5,\\beta^\\ast(\\nu)}^{-1}(\\rho)+O(\\rho-1)\r\n$$\r\n\r\nas $\\rho \\rightarrow 1-$. That is, the lower bound is an accurate solution for large $\\rho$.\r\n\r\nTanabe et al. (2007) proposed the fixed-point iteration\r\n\r\n$$\r\nx_{n+1} = \\frac{\\rho x_n}{r_\\nu(x_n)}\r\n$$\r\n\r\nto compute  $r_\\nu^{-1}(\\rho)$, and showed that the fixed-point iteration is globally convergent at linear rate $\\rho$. The linear convergence makes the method uncompetitive to alternatives such as bisection.\r\n\r\nAs far as `vonmises_fisher` MLE is concerned, various approximations of $r_\\nu^{-1}(\\rho)$ have been proposed. Banerjee (2005) gave the simple approximation\r\n\r\n$$\r\nr_\\nu^{-1}(\\rho) \\approx \\frac{\\rho (2\\nu+2-\\rho^2)}{1-\\rho^2}\r\n$$\r\n\r\nwhich is shown to be within the above bounds (Hornik and Gr\u00fcn, 2014, Theorem 3). Sra (2012) refined this guess using two Newton steps, and Song et al. (2012) refined the guess using two Halley steps.\r\n\r\n**Reference**\r\n\r\n(Entries marked with * are important.)\r\n\r\n\\* Amos, D. E. (1974). \"Computation of Modified Bessel Functions and Their Ratios.\" _Mathematics of Computation_, 28(125):239-251. [Link](https:\/\/www.jstor.org\/stable\/i334548)\r\n\r\nBanerjee A., Dhillon, I. S., Ghosh, J., Sra, S. (2005). \"Clustering on the Unit Hypersphere using von Mises-Fisher Distributions.\" _Journal of Machine Learning Research_, 6(46):1345-1382. [PDF](https:\/\/www.jmlr.org\/papers\/volume6\/banerjee05a\/banerjee05a.pdf)\r\n\r\n\\* Gautschi, W. and Slavik, J. (1978). \"On the computation of modified Bessel function ratios.\" _Mathematics of Computation_, 32(143):865-875. [PDF](https:\/\/www.ams.org\/journals\/mcom\/1978-32-143\/S0025-5718-1978-0470267-9\/S0025-5718-1978-0470267-9.pdf)\r\n\r\nHill, G. W. (1981). \"Evaluation and Inversion of the Ratios of Modified Bessel Functions, I<sub>1<\/sub>(x) \/ I<sub>0<\/sub>(x) and I<sub>1.5<\/sub>(x) \/ I<sub>0.5<\/sub>(x).\" _ACM Transactions on Mathematical Software_, 7(2):199-208. [Link](https:\/\/dl.acm.org\/doi\/10.1145\/355945.355949)\r\n\r\n\\* Hornik, K. and Gr\u00fcn, B. (2014). \"On maximum likelihood estimation of the concentration parameter of von Mises\u2013Fisher distributions.\" _Computational Statistics_, 29:945-957. [PDF](https:\/\/link.springer.com\/content\/pdf\/10.1007\/s00180-013-0471-0.pdf)\r\n\r\nSegura, J. (2023). \"Simple bounds with best possible accuracy for ratios of modified Bessel functions.\" _Journal of Mathematical Analysis and Applications_, 526(1): 127211. [PDF](https:\/\/pdf.sciencedirectassets.com\/272578\/1-s2.0-S0022247X23X00092\/1-s2.0-S0022247X23002147\/main.pdf)\r\n\r\nSong, H., Liu, J., and Wang, G. (2012). \"High-order parameter approximation for von Mises\u2013Fisher distributions.\" _Applied Mathematics and Computation_, 218(24):11880-11890. [Link](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0096300312005656)\r\n\r\nSra, S. (2012). \"A short note on parameter approximation for von Mises-Fisher distributions: and a fast implementation of I<sub>s<\/sub>(x).\" _Computational Statistics_, 27:177-190. [PDF](https:\/\/citeseerx.ist.psu.edu\/document?repid=rep1&type=pdf&doi=7d3405adc726356c4265bb9b26cdf3c6df6e9e7d)\r\n\r\nTanabe, A., Fukumizu, K., Oba, S., Takenouchi, T., and Ishii, S. (2007). \"Parameter estimation for von Mises\u2013Fisher distributions.\" _Computational Statistics_, 22:145-157. [Link](https:\/\/link.springer.com\/article\/10.1007\/s00180-007-0030-7)\r\n\r\n<!--\r\nTretter, M. J. and Walster, J. (1980). \"Further comments on the computation of modified Bessel function ratios.\" _Mathematics of Computation_, 35(151):937-939. [PDF](https:\/\/www.ams.org\/journals\/mcom\/1980-35-151\/S0025-5718-1980-0572867-3\/S0025-5718-1980-0572867-3.pdf)\r\n-->","Thanks for all this literature review @fancidev .\r\n\r\nWould it be possible to generalize the implementation to $\\frac{I_{\\nu + n}(x)}{I_{\\nu}(x)}$? Otherwise, we have a very specialized function. From what I understand, the continued fraction approach could be generalized for that expression. The continued fraction approach also looks like the most straight forward implementation given that infrastructure for such compuations is part of #20223. Inverting the function using Brent's method is stable enough in my opinion.\r\n\r\n@steppi : any strong opinions here?","@dschmitz89 I guess continued fractions is the way to go, but I\u2019ll need to try it out to see if it really works. It has to be more accurate and\/or more efficient than direct division to be useful.\r\n\r\nAs for generalizing to $I_{\\nu+n}(x)\/I_\\nu(x)$, I haven\u2019t come across any article that elaborates that, but I can take another look later. $I_{\\nu+1}(x)\/I_\\nu(x)$ might be more common since it appears when differentiating $I_\\nu(x)$ with respect to $x$. The literature mainly talks about vonmises (Fisher), but the MLE of non-central chi square distribution with zero degrees of freedom involves $I_1(x)\/I_0(x)$ too, and the distribution is useful for compound Poisson process with exponential reward.","I don't think it's a blocker to not have implementations for $I_{\\nu+n}\/I_{\\nu}$ for general $n$. Since these two functions would be genuinely useful in `stats`, we could make them private use them there, and then this wouldn't even need to go through the mailing list.\r\n\r\nI haven't looked into the details for the best way to implement these yet. It seems like improving the implementation of `ive`  could go a long way towards solving the current problems. I wrote a [generic continued fraction evaluator](https:\/\/github.com\/steppi\/scipy\/blob\/1d129e07b5750d626fd34a4aa8af808fde720fb4\/scipy\/special\/special\/tools.h#L113-L115) which uses Thompson and Barnett's [modified Lentz's algorithm](https:\/\/en.wikipedia.org\/wiki\/Lentz%27s_algorithm) that may be helpful. This hasn't made it into a PR yet because it's not as accurate as the methods currently be used for the specific functions I've tried it with.\r\n\r\n I'd be happy to review a PR if you want to work on this @fancidev.","Thanks @steppi ! I opened this issue as a standalone function in `scipy.special` to draw more review attention :-) It's a good idea to put it private in `scipy.stats` first so that I don't have to worry about negative $\\nu$ and complex $z$ and can focus on the immediately useful stuff.\r\n\r\nThe evaluator for continued fraction from the beginning is very helpful, as the continued fractions used above doesn't have a a straightforward and tight bound on the number of terms needed.\r\n\r\nLet me make a PR!","> Thanks @steppi ! I opened this issue as a standalone function in `scipy.special` to draw more review attention :-) It's a good idea to put it private in `scipy.stats` first so that I don't have to worry about negative \u03bd and complex z and can focus on the immediately useful stuff.\r\n> \r\n> The evaluator for continued fraction from the beginning is very helpful, as the continued fractions used above doesn't have a a straightforward and tight bound on the number of terms needed.\r\n> \r\n> Let me make a PR!\r\n\r\nAwesome! We have a lot of moving parts right now in `special`. That evaluator isn't in yet, and some updates are being made to the ufunc infrastructure. My thought is you create a branch off of `main` and let me know it's name. I can PR the continued fraction evaluator to your branch, and then you can take it from there. We can deal with merge conflicts later if they pop up. Does that sound like a good plan?","@steppi Sounds good! I read your [generic continued fraction evaluator](https:\/\/github.com\/steppi\/scipy\/blob\/1d129e07b5750d626fd34a4aa8af808fde720fb4\/scipy\/special\/special\/tools.h#L113-L115). It follows closely the \"prescription\" in Numerical Recipes (with a smaller choice of `tiny`), so should be good!\r\n\r\nOn my side, I coded forward evaluation and backward evaluation (by doubling the number of terms each time until the result doesn't change) of the Perron c.f.. I compared the results and found some questions:\r\n\r\n1\/ I want to make the evaluation as accurately as possible, so I effectively set the `tol` (as in your code) to zero. Is that ok?\r\n\r\n2\/ Usually both forward and backward evaluation work fine (with `tol=0`) and achieve a relative accuracy no larger than `2.2e-16` (compared with `mpmath`). But for some inputs the forward method requires an excessively large number of iterations; apparently round-off error is in play here:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/10579828\/fced3900-3af1-4ec4-826f-374f924db835)\r\n\r\nI'll try rewrite the c.f. in a numeric-friendly way for large $x$, but otherwise do you have some general suggestion \/ experience on how to deal with such numerical error?\r\n\r\n3\/ When you mention \"This hasn't made it into a PR yet because it's not as accurate as the methods currently be used for the specific functions I've tried it with.\", what sort of inaccuracy did you observe?\r\n\r\nThanks for your attention!","I don\u2019t have time to give detailed response today but \r\n\r\n0) the consensus here is to never use numerical recipes as a reference. The licensing is onerous and It\u2019s not BSD compatible. The evaluator is a direct port from Boost. \r\n\r\n1) no. It should be set to machine epsilon or possibly something slightly larger. Additional iterations beyond may cause additional accumulated rounding error. \r\n\r\n2) maybe use backwards evaluation for sufficiently large x. For sufficiently small x you\u2019ll want to use the direct `ive` ratio. Update: or it could just be a consequence of using a tol of zero. \r\n\r\n3) almost identical accuracy to the forward evaluator used in cephes for expn, but marginally worse for some test cases out of thousands I checked. Worst case 8e-16 vs 5e-16. Almost identical rmse ","@fancidev. I\u2019ll give more specific answers and advice in the PR message when I PR to your branch. ","Thanks @steppi . I made a branch [here](https:\/\/github.com\/fancidev\/scipy\/tree\/iv_ratio).","It looks like computing $I_{\\nu+1}(x)\/I_\\nu(x)$ using Perron\u2019s continued fraction, and inverting it using Brent\u2019s method, will be good enough for this topic. I\u2019ll proceed with that.\r\n\r\nThe main task is to evaluate the Perron c.f. accurately (ideally below 1 ulp), reliably (handle all range of `float`), and efficiently (not taking \u201ctoo many\u201d iterations). It will thus be helpful to have an estimate of the number of c.f. terms required to bound truncation error below a  given threshold $\\epsilon$ (e.g. machine precision).\r\n\r\n\r\nGautschi (1967) described three general ways to evaluate a continued fraction: backward, forward, and series. Let\r\n\r\n$$\r\nf \\equiv \\frac{a_1}{b_1+{}}  \\frac{a_2}{b_2+{}}  \\frac{a_3}{b_3+{}} \\cdots\r\n$$\r\n\r\ndenote a general continued fraction, and let \r\n\r\n$$\r\nf_n \\equiv \\frac{a_1}{b_1+{}}  \\frac{a_2}{b_2+{}}  \\frac{a_3}{b_3+{}} \\cdots \\frac{a_n}{b_n +0}\r\n$$\r\n\r\ndenote the partial fraction with the first $n$ terms. The series method rewrites the partial fraction as\r\n\r\n$$\r\nf_n = \\sum_{k=1}^n \\rho_1 \\rho_2 \\cdots \\rho_k\r\n$$\r\n\r\nwhere\r\n\r\n$$\r\n\\rho_1 = \\frac{a_1}{b_1}, \r\n\\rho_k = -\\left( \\frac{b_{k}b_{k-1}}{a_k u_{k-1}}+1\\right)^{-1}, k=2,3,4,\\cdots\r\n$$\r\n\r\nwith $u_1 \\equiv 1$ and $u_k = 1+\\rho_k$, $k=2,3,4,\\cdots$.\r\n\r\nThe series method for the Perron continued fraction is elaborated in Gautschi and Slavik (1978). Here $f \\equiv I_{\\nu}(x)\/I_{\\nu-1}(x)$,\r\n\r\n$$\r\n\\begin{align*}\r\na_1 &= x c \\\\\r\nb_1 &= 2\\nu c +x c\r\n\\end{align*}\r\n$$\r\n\r\nand\r\n\r\n$$\r\n\\begin{align*}\r\na_k &= -[2\\nu c+(2k-3) c] x c \\\\\r\nb_k &=2(\\nu c +x c)+(k-1)c\r\n\\end{align*}\r\n$$\r\n\r\nfor $k=2, 3, 4, \\cdots$, where $c \\ne 0$ is an arbitrary constant used to rescale the computation within floating point range. It follows that\r\n\r\n$$\r\n\\frac{1}{\\rho_1} = 1+\\frac{2\\nu}{x} > 1\r\n$$\r\n\r\n$$\r\n\\begin{align*}\r\n\\frac{1}{\\rho_2} \r\n&= \\frac{2\\nu}{x}+\\frac{4x}{2\\nu+1}+3-\\frac{2}{2\\nu+1} \\\\\r\n&\\ge 4\\sqrt{\\frac{2\\nu}{2\\nu+1}}+3-\\frac{2}{2\\nu+1} \\\\\r\n&\\ge 4\\sqrt{\\frac{2}{3}}+\\frac{7}{3}\r\n\\end{align*}\r\n$$\r\n\r\n**Reference**\r\n\r\nGautschi, W. (1967). \u201cComputational Aspects of Three-Term Recurrence Relations.\u201d _SIAM Review_, 9(1):24-82. [Link](https:\/\/www.jstor.org\/stable\/i309438)\r\n","Awesome! For large $x$, asymptotic expansions could be even more accurate:\r\n\r\nSee https:\/\/dlmf.nist.gov\/10.40#E1 or https:\/\/math.stackexchange.com\/questions\/4051200\/approximation-of-subtraction-of-modified-bessel-functions-of-first-kind\r\n\r\nFor the fun of it and to warn contributors to do the same mistake: I asked GPT 3.5 to tell me a formula for the expansion and it gave a completely wrong answer where the ratio is larger than 1. ChatGPT does not perform well on such tasks:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/40656107\/13857aae-634d-48e2-8c40-aadc86e4de74)\r\n","Thanks for the reference @dschmitz89. I hope the continued fraction will be able to handle the full range of `float` input without resorting to asymptotic formula \u2014 let\u2019s see!","I tried three ways to evaluate the Perron continued fraction to compare their accuracy and efficiency: backward recursion (doubling the number of terms until result doesn't change), forward recursion (Lentz's algorithm), and series method (as in Gautschi and Slavik (1978), also forward but with easy convergence guarantee for this c.f.).\r\n\r\nI use $\\nu=0.5$ as an example, and use `mpmath` direct division as \"the right answer\". For benchmark, I also include `scipy.special.ive` direct division. \r\n\r\nThe **findings** are:\r\n\r\n- Backward recursion is most accurate\r\n- Series method is reasonably accurate and good at saving iterations (compared to backward)\r\n- Direct division is good for middle-ish $x$, but inaccurate for small $x$ and `nan` for large $x$.\r\n- Lentz method is not very accurate for moderate $x$, and it gives `inf` for many inputs because of floating point overflow.\r\n\r\nSo \"backward\" and \"series\" look promising. I will study them in more detail.\r\n\r\nGraph comparison:\r\n\r\nSmall range:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/10579828\/c0273f2a-638f-4292-8e42-4e49f58d74ca)\r\n\r\nLarge range:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/10579828\/ddd0506c-a8bd-4c15-8ca0-ff3ec2c1f941)\r\n\r\nI include the full script below so that you can play with the parameters (Lentz's algorithm appears quite sensitive to the choice of parameters for this c.f.)\r\n\r\n<details>\r\n\r\n<summary>Script<\/summary>\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.special\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom mpmath import mp\r\nmp.dps = 50\r\n\r\ndef perron_mp(v, x):\r\n    return float(mp.besseli(v+1,x)\/mp.besseli(v,x))\r\n\r\ndef perron_cf(v, x, method, **kwargs):\r\n    v+=1\r\n    def g():\r\n        \"\"\"plain cf formula\"\"\"\r\n        yield x, 2*v+x\r\n        k=1\r\n        while True:\r\n            yield -2*x*(v+k-0.5), 2*(v+x)+k\r\n            k += 1\r\n    def g2():\r\n        \"\"\"alternative cf formula without the first term\"\"\"\r\n        k=1\r\n        while True:\r\n            yield -2*x*(v+k-0.5), 2*(v+x)+k\r\n            k += 1\r\n    if method == 'f':  # forward\r\n        f, n = eval_cf_forward(0, g, **kwargs)\r\n    elif method == 'f2':  # forward recursion with first term stripped\r\n        f, n = eval_cf_forward(2*v+x, g2, **kwargs)\r\n        f = x\/f\r\n    elif method == 'b':  # backward recursion\r\n        f, n = eval_cf_backward(0, g, **kwargs)\r\n    elif method == 's':  # series\r\n        f, n = eval_cf_series(0, g, **kwargs)\r\n    else:\r\n        raise ValueError\r\n    return f, n\r\n\r\ndef eval_cf_forward(b0, g, *, tiny=16*np.finfo(float).tiny, safetiny=False, tol=0, max_iter=1000):\r\n    f = tiny if b0 == 0 else b0\r\n    C = f\r\n    D = 0\r\n    k = 0\r\n    for a, b in g():\r\n        k += 1\r\n        if k > max_iter:\r\n            return np.nan, k\r\n        D = b+a*D\r\n        if D == 0:\r\n            D = tiny\r\n        # if np.isinf(a\/C):\r\n        #     raise RuntimeError(f'{b=} {a=} {C=}')\r\n        C = b + a\/C\r\n        if C == 0:\r\n            C = tiny\r\n        if safetiny and abs(C) < tiny:\r\n            C = np.signbit(C)*tiny\r\n        D = 1\/D\r\n        f *= C*D\r\n        if abs(C*D-1) <= tol:\r\n            return f, k+1\r\n\r\ndef eval_cf_backward_once(b0, g, n):\r\n    g = iter(g())\r\n    terms = [next(g) for _ in range(n)]\r\n    s = 0\r\n    for k in range(len(terms), 0, -1):\r\n        a, b = terms[k-1]\r\n        s = a\/(b+s)\r\n    return s + b0\r\n\r\ndef eval_cf_backward(b0, g, tol=0, max_iter=1000):\r\n    cf = eval_cf_backward_once(b0, g, 1)\r\n    terms = 1\r\n    while terms < max_iter:\r\n        terms = min(terms*2, max_iter)\r\n        cf2 = eval_cf_backward_once(b0, g, terms)\r\n        if np.allclose(cf2, cf, rtol=tol, atol=tol):\r\n            return cf2, terms\r\n        cf = cf2\r\n    else:\r\n        print('Warning: not converging')\r\n        return cf2, max_iter+1\r\n\r\ndef eval_cf_series(b0, g, tol=0, max_iter=1000):\r\n    g = iter(g())\r\n\r\n    w = b0\r\n    \r\n    # k = 1\r\n    a, b = next(g)\r\n    rho = a\/b\r\n    u = 1\r\n    v = rho\r\n    w_ = w\r\n    w += v\r\n    if abs(w-w_)<=tol: # abs\r\n        return w,1\r\n\r\n    # k=2,3,4...\r\n    for k in range(2,max_iter+1):\r\n        aa, bb = next(g)\r\n        u = 1\/(1+aa\/(b*bb)*u)\r\n        b = bb\r\n        v *= (u-1)\r\n        w_ = w\r\n        w += v\r\n        if abs(w-w_)<=tol: # abs\r\n            return w,k\r\n    return np.nan, max_iter+1\r\n    \r\n# print('mp: ', perron_mp(0, 1.2))\r\n# print('b: ', perron_cf(0, 1.2, 'b'))\r\n# print('f: ', perron_cf(0, 1.2, 'f', tol=1e-16*0))\r\n# print('f2:', perron_cf(0, 1.2, 'f2', tol=1e-17))\r\n# print('s: ', perron_cf(0, 1.2, 's', tol=0))\r\n\r\ndef test(v, xs):\r\n    nbs, ebs = [], []\r\n    nfs, efs = [], []\r\n    nss, ess = [], []\r\n    eds = []\r\n    for x in xs:\r\n        r0 = perron_mp(v, x)\r\n        eds.append(scipy.special.ive(v+1,x)\/scipy.special.ive(v,x)\/r0-1)\r\n        rb, nb = perron_cf(v, x, 'b', tol=0)\r\n        nbs.append(nb)\r\n        ebs.append(rb\/r0-1)\r\n        rf, nf = perron_cf(v, x, 'f', tol=2.3e-16, safetiny=True)\r\n        nfs.append(nf)\r\n        efs.append(rf\/r0-1)\r\n        rs, ns = perron_cf(v, x, 's', tol=0)\r\n        nss.append(ns)\r\n        ess.append(rs\/r0-1)\r\n\r\n    plt.subplots(2,1,figsize=(9,6),sharex=True)\r\n    \r\n    plt.subplot(2,1,1)\r\n    plt.plot(xs, nfs, label='Forward cf')\r\n    plt.plot(xs, nbs, label='Backward cf')\r\n    plt.plot(xs, nss, label='Series cf')\r\n    plt.legend(loc='upper right')\r\n    plt.title('Number of terms')\r\n    \r\n    plt.subplot(2,1,2)\r\n    def errstat(x):\r\n        eps = np.finfo(float).eps\r\n        x=np.array(x)\r\n        max_ = np.max(np.abs(x[np.isfinite(x)]))\r\n        rmse = np.sqrt(np.mean(x[np.isfinite(x)]**2))\r\n        ninf = np.sum(np.isinf(x))\r\n        nnan = np.sum(np.isnan(x))\r\n        s = f'rmse={rmse\/eps:.1f}, max={max_\/eps:.1f}'\r\n        if ninf > 0:\r\n            s += f', {ninf} inf'\r\n        if nnan > 0:\r\n            s += f', {nnan} nan'\r\n        return s\r\n        \r\n    plt.plot(xs, efs\/np.finfo(float).eps, alpha=0.7, label=f'Forward cf ({errstat(efs)})')\r\n    plt.plot(xs, ebs\/np.finfo(float).eps, alpha=0.7, label=f'Backward cf ({errstat(ebs)})')\r\n    plt.plot(xs, ess\/np.finfo(float).eps, alpha=0.7, label=f'Series cf ({errstat(ess)})')\r\n    plt.plot(xs, eds\/np.finfo(float).eps, alpha=0.7, label=f'Direct division ({errstat(eds)})')\r\n    plt.axhline(y=0, lw=0.5, c='k')\r\n    plt.legend(loc='upper right')\r\n    plt.title(f'Relative error (upls)')\r\n\r\ntest(0.5, np.linspace(0.01, 100, 1000))\r\nplt.show()\r\n\r\ntest(0.5, np.geomspace(1e-16, 1e16, 1000))\r\nplt.xscale('log')\r\nplt.show()\r\n```\r\n\r\n<\/details","Nice work @fancidev. It looks like the series method is the way to go. I'm a bit curious though why your stopping criterion is `abs(w - w_) < tol`, involving the absolute error.  If `w` is a double, then the distance to an adjacent double will be between $0.5\\epsilon |x|$ and $\\epsilon |x|$, where $\\epsilon$ is [machine epsilon](https:\/\/en.wikipedia.org\/wiki\/Machine_epsilon), so it makes more sense to use the relative error for the stopping criterion, `abs(w - w_) < tol * abs(w)`. The above bounds for the difference between neighboring floats is also why we typically use machine epsilon as the tolerance. If the relative error between successive estimates in the iterative process is less than machine epsilon, then we know these are at least adjacent floats, and it is usually not productive to continue the process. Sometimes a larger tolerance is useful though if additional iterations can accumulate too much rounding error.","@steppi You\u2019re definitely right. Here I\u2019m being lazy and just wanted to code something quickly that works with `tol=0`, that\u2019s how it becomes `abs(w-w_) <= tol`. A relative error check is definitely more appropriate!","> @steppi You\u2019re definitely right. Here I\u2019m being lazy and just wanted to code something quickly that works with `tol=0`, that\u2019s how it becomes `abs(w-w_) <= tol`. A relative error check is definitely more appropriate!\r\n\r\nNo worries. It doesn't actually make a difference in the results for the backwards and series methods because the difference between successive estimates tends to zero and your using `tol=0`. At worst you just need one or two extra iterations. Also, the infinities for Lentz's method are because the default `tiny_value` I chose from Boost is too small for this problem. You'll get better results for `tiny_value=1e-100`, but still not as good as the other methods."],"labels":["enhancement","scipy.special"]},{"title":"RFC: Multidimensional (cubature) integration","body":"Hello all! This issue is seeking thoughts on a core part of `scipy.integrate`. More than that, it is proposing an enhancement that I and others are willing and able to work towards.\r\n\r\nIntegrating a function $f(x)$ is a fundamental part of numerical computing, and doing so for a function $f(\\mathbf{x})$ in $n$ dimensions is just as important as doing so in 1 dimension. SciPy currently has routines in its `integrate` submodule to do so, notably `dblquad`, `tplquad`, and `nquad`. These are wrappers that essentially do 1D iterated integrals using `quad` from the Fortran library QUADPACK. QUADPACK is very impressive software, but at the end of the day it is still written in Fortran and limited by the design of its time. The version in SciPy is from 1984.\r\n\r\nImproving `integrate` generally is important, and I'd very much like feedback on a plan on how to do that for quadrature in $n$ dimensions. If what is written here is broadly acceptable to everyone, I and others would intend to carry out this work on multidimensional integration before the end of this summer. The inspiration here is really the adaptive quadrature in `quad_vec`, which I think went a long way to providing robust integration in SciPy that works well with arrays.\r\n\r\nTo be very specific about the issues, multidimensional integration in SciPy currently has these limitations:\r\n\r\n- It does not support array-valued integrands of arbitrary shape, only scalar functions. Therefore one cannot integrate a vector or matrix or tensor function whereas `quad_vec` can.\r\n\r\n- It does not support vectorization. If one wants to speed things up by sampling all the integration points at once, you can't.\r\n\r\n- It will never support other array APIs, as it is written in Fortran. So it can never go on a GPU.\r\n\r\n- It does not support custom integration rules. If one wants to use, say, multidimensional tanh-sinh quadrature for handling a singular point, you can't.\r\n\r\nThe proposal here is to provide a multidimensional integration function that is adaptive and similar to `quad_vec`. This is what is known as \"cubature\", we can call it `cub` or `ndquad_vec` or whatever. It will take almost the same arguments as `quad_vec`, except `a`, `b`, and `points` will all become $n$ dimensional. It will use the array API to enable support for GPU integration and beyond.\r\n\r\nAn additional feature that it will have, which `quad_vec`doesn't have, is support for passing in a custom integration rule. This would allow the adaptive part to subdivide effectively, but it could then use the custom integration rule to calculate the integral in each subdivision. The built-in rules would be Gauss-Kronrod and Genz-Malik (popular for multidimensional integrands). But users could also pass in rules that effectively deal with singular (using tanh-sinh) or oscillatory integrands. QUADPACK actually has behaviour like this with its methods \"qawoe\", \"qawse\", and \"qawce\", but it is not fully custom. If successful, this behaviour could be added into `quad_vec` as well by simply making its existing `quadrature` argument also accept a callable.\r\n\r\nThis proposal would go a long way towards improving an existing capability within SciPy. It would also help a lot with removing Fortran from the codebase.\r\n\r\nI know @mdhaber, among others, has been doing work on `integrate`. I'd very much appreciate his thoughts and, of course, everyone's.","comments":["I would think inclusion of LowLevelCallable would be important.","A strong +1 from me in general. \r\n\r\nDetails:\r\n\r\n- is the proposal to only provide a new `ndim > 1` routine, or is bringing `quad_vec` to feature completeness with `quad` a part of it? \r\n- the interface needs careful design, esp around vectorization, vector-valued functions and broadcasting w.r.t. parameters. IIUC, the current `_tanhsinh` is incompatible with `quad_vec`. This is somewhat resembling the situation we have with `approx_derivative` and `misc.derivative`: vector-matrix semantics is at odds with broadcasting. It would be best to avoid the split, if at all possible. Or else, provide multiple interfaces with shared underlying integration machinery.\r\n- custom integration rules are great. Now, would be great to have a stance on a Monte Carlo rule: are you open to a possibility of some sort of an MC integration rule or does it constrain the interface or internals? Not that I've an opinion; it's just bound to show up once somebody want ndim > 5 or so, and it's best to have some plan. Also once MC is there, QMC comes right after it. \r\n- A single `nquad`-like interface for general n-dimensional functions, or a collection of specializations for 2D, 3D etc?","> A strong +1 from me in general.\r\n\r\nThanks @ev-br! Some answers.\r\n\r\n> Details:\r\n> \r\n>     * is the proposal to only provide a new `ndim > 1` routine, or is bringing `quad_vec` to feature completeness with `quad` a part of it?\r\n\r\nThe proposal is to provide a new routine for `ndim >= 1`. As you point out, there are design considerations there to make sure we get it right. If we do, a second part would be to adapt `quad_vec` so it is fully compatible. I think `quad_vec`, or any successor thereof, should remain a one-dimensional routine. For the sake of generality, I think `ndquad_vec` should also support one dimension, but it's intention is to be used for arbitrary $n$.\r\n\r\n>     * the interface needs careful design, esp around vectorization, vector-valued functions and broadcasting w.r.t. parameters. IIUC, the current `_tanhsinh` is incompatible with `quad_vec`. This is somewhat resembling the situation we have with `approx_derivative` and `misc.derivative`: vector-matrix semantics is at odds with broadcasting. It would be best to avoid the split, if at all possible. Or else, provide multiple interfaces with shared underlying integration machinery.\r\n\r\nCouldn't agree more! What I have described has already been prototyped and I see clearly all the issues you have raised.\r\n\r\n>     * custom integration rules are great. Now, would be great to have a stance on a Monte Carlo rule: are you open to a possibility of some sort of an MC integration rule or does it constrain the interface or internals? Not that I've an opinion; it's just bound to show up once somebody want ndim > 5 or so, and it's best to have some plan. Also once MC is there, QMC comes right after it.\r\n\r\nI don't see any reason why MC integration rules should be excluded. And indeed, we should talk to make sure we are as compatible for MC as possible! But I don't think we specifically would write any MC rules as part of this, that would be for someone else or for the future.\r\n\r\nThe design for each rule should be general. I'm essentially thinking of them as callables with the signature `__call__(self, func, a, b)`. If they need to calculate points and weights or whatever, they can do that and store them internally.\r\n\r\nReally, the purpose here is to make the numerical integration *composable*.\r\n\r\n>     * A single `nquad`-like interface for general n-dimensional functions, or a collection of specializations for 2D, 3D etc?\r\n\r\nA single `nquad`-like interface for general $n$-dimensional functions. Drop the 2D and 3D ones. If someone wants to make a specialised 2D or 3D one, that it could be plugged in as a custom rule I mentioned above!","> IIUC, the current _tanhsinh is incompatible with quad_vec\r\n\r\nNo, it now supports vector valued integrands. (The first three arguments of the signature are the same, but the rest of the interface is not identical, though, if that's what is meant.)\r\n\r\n> Also once MC is there, QMC comes right after it.\r\n\r\nJust thought I'd note that [`qmc_quad`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.integrate.qmc_quad.html) exists.\r\n\r\nPerhaps the point was to have a consistent interface for many methods. Sure - the only reason `_tanhsinh` is not public yet is that I didn't want to introduce yet another function (e.g. `tanhsinh`) to fragment `integrate`, and I need to get through the hurdle of the stats infrastructure before committing to working on something more general. The purpose of deprecating `quadrature` and `romberg` was to work toward unification, though.\r\n\r\nSo I think I we're working toward the same goals; I was just planning to start with 1-D integration.","I read through this more carefully, and it looks great! Since you mention that the inspiration is `quad_vec`, I thought I'd check - have you looked at [`_tanhsinh`](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/integrate\/_tanhsinh.py)? Based on benchmarks in the original PR (gh-18650), `_tanhsinh` seems to be faster and more accurate than `quad_vec`, and for integration in one dimension (because the immediate goal was to improve the univariate distribution infrastructure), `_tanhsinh` was written specifically to address most of what is suggested here.\r\n\r\n- It supports vectorization in the sense of sampling all the integration points (evaluating the integrand at all abscissae) at once.\r\n- It *also* supports vectorization in the sense of allowing the limits of integration to be of arbitrary shape. (Of course, there are faster ways of computing indefinite integrals of univariate functions - and we have plans to implement those later - but it's a handy feature for now.)\r\n- It now supports array-valued integrands of any shape, which was `quad_vec`'s claim to fame. (Enabling this support was pretty straightforward (gh-19545) because the fundamental restriction that prevented array-valued functions was *added* for efficiency with SISO functions; I just had to add an option to remove that restriction.)\r\n- Although it is not yet Array-API compatible, that has always been the goal, and @tupui and I have specific plans for implementation in the coming months.\r\n- Only one integration rule is implemented right now, but I think it will be straightforward to add others, and we discussed exposing several rules in a common interface in gh-19510. \r\n \r\nNote that this quadrature routine itself leverages a more general [elementwise iterative methods framework](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/_lib\/_elementwise_iterative_method.py). The framework takes care of a lot of the \"bookkeeping\" of writing such functions, and it has also been used to write differentiation, root-finding, and minimization routines. It would be great if you would be interested in collaborating to improve\/extend this framework to support functions with N-dimensional input. This would help reduce code duplication for commonly-needed tasks, ensure a more consistent interface among one- and multidimensional functions, and may open up possibilities for other functions (e.g. vectorized multidimensional rootfinding or minimization).\r\n\r\nStill, I didn't want to expose `_tanhsinh` publicly via the `quad_vec` function because that interface (e.g. accepting a `full_output` parameter, returning a `tuple`) is not the direction SciPy has been headed in. Also, `quad_vec` doesn't support several of `_tanhsinh`'s features (e.g. computing the `log` of the integral from the log of the integrand), and some of `quad_vec`'s options would not make sense for `_tanhsinh`, so it would get a bit confusing to try to combine them. We need a new interface designed with more flexibility in mind. I'm won't be teaching this spring or summer, so I had planned to work on such an interface this spring. It sounds like our timelines align well!\r\n\r\nDid you intend to get into specific interface discussions here? ","There's a distinction between nested\/composed 1D quadrature and \"cubature,\" algorithms for which treat all dimensions of the domain at once. The standard implementation of non-MC algorithms (to my knowledge) is [`cubature`](https:\/\/github.com\/stevengj\/cubature), which is GPL'ed; but the author has an MIT'ed [Julia port](https:\/\/github.com\/JuliaMath\/HCubature.jl) of the more commonly used algorithm, which adaptively subdivides the full domain along whatever dimension contributes the largest error at each iteration. Such methods are really necessary to efficiently integrate in dimensions higher than 2 or 3.\r\n\r\nSome of the discussion suggests to me the proposal is restricted to nested 1D quadrature - is this the case, or is the proposal actually to implement algorithms for n-dimensional adaptivity?\r\n\r\nI do think that true cubature algorithms would be a highly valuable addition to SciPy (and fill a bit of a hole in the ecosystem) and are established\/standard enough to meet SciPy's bar for inclusion. Its interface, however, would surely differ from `nquad`-alikes (in its optional arguments, at least). I would recommend reserving method names deriving from \"cubature\" for them.\r\n\r\n(To be clear, nested quadrature can be more efficient than, e.g., h-adaptive cubature - my memory is that it's often the case in 2D and maybe 3D, too. So I wouldn't drop `nquad` or an equivalent replacement in any case.)","My understanding is that @izaid is proposing \"true cubature\". I have previously suggested nested quadrature with `_tanhsinh` before, but yes, cubature would be better for N-D."],"labels":["scipy.integrate","RFC"]},{"title":"BUG: internal type error in scipy.sparse.csgraph._matching.min_weight_full_bipartite_matching","body":"### Describe your issue.\n\nscipy.sparse.csgraph._matching.min_weight_full_bipartite_matching reports an internal type error even with simple data.\n\n### Reproducing Code Example\n\n```python\nPython 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import scipy\r\n>>> scipy.sparse.csgraph.min_weight_full_bipartite_matching(scipy.sparse.coo_array(([1, 2, 3], ([0, 1, 2], [0, 1, 2]))), maximize=True)\n```\n\n\n### Error message\n\n```shell\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"_matching.pyx\", line 499, in scipy.sparse.csgraph._matching.min_weight_full_bipartite_matching\r\nValueError: Buffer dtype mismatch, expected 'ITYPE_t' but got 'long'\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.4 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/piyushs\/miniconda3\/include\r\n    lib directory: \/home\/piyushs\/miniconda3\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/home\/piyushs\/miniconda3\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/piyushs\/miniconda3\/include\r\n    lib directory: \/home\/piyushs\/miniconda3\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/home\/piyushs\/miniconda3\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/home\/piyushs\/miniconda3\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/home\/piyushs\/miniconda3\/bin\/python\r\n  version: '3.11'\n```\n","comments":["I just realized that this is technically not a  bug.  The error goes away if I use a `csr_matrix` instead of a `csr_array` as input to the `scipy.sparse.csgraph.min_weight_full_bipartite_matching` function.  This is what the documentation also asks for (it only mentions sparse matrices, not sparse arrays).  However, I was confused because the top-level documentation of `scipy.sparse` seems to recommend users to prefer arrays rather than matrices in new code.  Perhaps a warning in the documentation for `scipy.sparse.csgraph.min_weight_full_bipartite_matching` that the input must be a **sparse matrix** and not a **sparse array** would be sufficient.","That indicates it is a bug in the sparse array interface.  If it works for `csr_matrix` it should also work for `csr_array`.\r\nSo this is indeed a bug. Thanks for reporting it!","This appears to be a bug that the function does not work if the indices are of type `np.int64`. It occurs for `coo_matrix` when indices are big enough to need `int64` to be stored.\r\n\r\n```python\r\nimport scipy as sp\r\nA = sp.sparse.coo_matrix([1,2,3], ([1,2,3**34], [1,2,3]))\r\nsp.sparse.csgraph.min_weight_full_bipartite_matching(A, maximize=true)  # leads to the same error\r\n```\r\n\r\nI don't know enough of the guts of csgraph or cython to figure out if this should work with `np.int64` indices. But that's the fundamental question.\r\n\r\nA workaround for `csr_array` with smaller_than_int32max index values is to use int32 values when defining the matrix.\r\n```python\r\nA = sp.sparse.csr_array(([1, 2, 3], (np.array([0, 1, 2], dtype=np.int32), np.array([0, 1, 2], dtype=np.int32))))\r\n```\r\nBut the real fix is to make this function work for int64 indices.  [The reason csr_array uses int64 is because it follows the protocol that type should not depend on value -- only dtype of incoming values. And incoming values are python ints.)","I've had a quick look at this. It looks like we are defining the work arrays as `ITYPE` which is an alias for ``np.int32``. You can see the definitions below:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/9a5118633252eb714e1abcafba036d4295a146be\/scipy\/sparse\/csgraph\/_matching.pyx#L150-L151\r\n\r\nI'm not 100% sure if this is right, but, maybe we should work on templating the underlying function based on the dtype of the incoming array?"],"labels":["defect","scipy.sparse.csgraph"]},{"title":"ENH: Akima1DInterpolator Extrapolation","body":"### Is your feature request related to a problem? Please describe.\n\nUnlike the CubicHermiteSpline from which the Akima interpolator inherits from, attempting extrapolation with the Akima1DInterpolator returns NaN outside of the original domain. I expected this because of the hard coded `extrapolate=False` on initialization.\r\n\r\nI think adding extrapolation capabilities to the Akima1DInterpolator class would make sense. Of the four main 1D splines (CubicSpline, CubicHermiteSpline, PchipInterpolator, and Akima1DInterpolator), Akima is the only one to not support extrapolation.\n\n### Describe the solution you'd like.\n\nAllowing the specification of `extrapolate=True` and exposing a similar extrapolation parameter interface as CubicHermiteSpline is the most natural. Extrapolation should then be handled similarly to all of the other splines in the user's perspective.\n\n### Describe alternatives you've considered.\n\nFor my own project purposes, I have been using alternative splines from Scipy. The Akima1DInterpolator (and the modified Akima interpolator per #19696 ) seems to have desirable properties, but the lack of extrapolation is otherwise problematic.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["You can switch extrapolation either at call site or globally (this is inherited from Akima1DInterpolator superclass, PPoly):\r\n\r\n```\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: from scipy.interpolate import Akima1DInterpolator\r\n\r\nIn [3]: a = Akima1DInterpolator([1, 2, 3], [4, 5, 6])\r\n\r\nIn [4]: a(5)\r\nOut[4]: array(nan)\r\n\r\nIn [5]: a(5, extrapolate=True)\r\nOut[5]: array(8.)\r\n\r\nIn [6]: a.extrapolate = True\r\n\r\nIn [7]: a(5)\r\nOut[7]: array(8.)\r\n```\r\n\r\nThat said, the decision to not have the extrapolate parameter is historic. A pull request adding the `extrapolate=False` parameter to the constructor is welcome."],"labels":["enhancement","scipy.interpolate"]},{"title":"ENH: find all real roots of a univariate function over an interval","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nThe task of finding the MLE for a three-parameter (generalized) Pareto distribution boils down to finding _all_ roots of the (univariate) likelihood equation over a closed interval. The likelihood value at the roots are compared and the root that yields the largest likelihood value is the MLE.\r\n\r\nThe hard part of the problem is to locate the roots. Depending on the data being fit, there may be 0, 1, 2, 3, 4, and possibly more roots.\r\n\r\n### Describe the solution you'd like.\r\n\r\nIt would be nice if scipy could provide a function to return _all_ roots of a univariate function over a closed interval. The function may look like this:\r\n\r\n```python\r\nfind_all_roots(f, [a, b], tol, maxiter)\r\n```\r\n\r\n### Describe alternatives you've considered.\r\n\r\nGeneral purpose optimizers, local or global, do not work for small to moderate samples because  we seek the largest local maximum and _not_ the global maximum, which may be at the boundary. In large sample, the global maximum is also locally maximum in probability; Brent\u2019s method works in this case.\r\n\r\n[Grimshaw (1993)](https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00401706.1993.10485040) describes a numerical method to compute the MLE of (two-parameter) generalized Pareto distribution. This appears to be the _only_ method out there to compute the MLE for small to moderate sample. He solves the likelihood equation by repeatedly applying bound-constrained Newton-Raphson method. It is not proved that that method always works, and it probably doesn\u2019t or otherwise we\u2019d have seen repeated Newton-Raphson as a solution to the general problem, which is not the case.\r\n\r\n`scipy.pareto.fit` solves the likelihood equation by trying to bracket the root starting from a guessed solution (always one). Unfortunately it seems finding a good guess is as hard as solving the equation, and by starting from one,  estimation often fails when the solution is far from one.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nThe only method I could find out there that provides the functionality is [IntervalRootFinding.jl](https:\/\/juliaintervals.github.io\/IntervalRootFinding.jl\/stable\/). It implements the \u201cinterval Newton method\u201d described in [Hansen (1978)](https:\/\/link.springer.com\/article\/10.1007\/BF01932020), which uses interval arithmetics. However, interval arithmetic is not essential if we don\u2019t intend to provide guarantee against numerical error.","comments":["I happened to prototype this a few weeks ago. The approach is to fit an interpolator that approximates the function to sufficient accuracy, then find the roots of the interpolator.\r\n\r\n```python3\r\nimport numpy as np\r\nfrom scipy.interpolate import CubicSpline\r\nbounds = (-10, 10)\r\nf = np.sin\r\n\r\ndef all_roots(f, bounds, n=10, rtol=1e-6, atol=1e-15):\r\n    x = np.linspace(*bounds, n)\r\n    y = f(x)\r\n    spline = CubicSpline(x, y)\r\n    uerr = np.inf\r\n    tol = atol\r\n\r\n    while np.any(uerr > tol):\r\n        new_x = (x[:-1] + x[1:])\/2\r\n        new_y_ = spline(new_x)\r\n        new_y = f(new_x)\r\n        i = np.arange(1, len(new_x)+1)\r\n        x = np.insert(x, i, new_x)\r\n        y = np.insert(y, i, new_y)\r\n        spline = CubicSpline(x, y)\r\n        uerr = np.abs(new_y_ - new_y)\r\n        tol = atol + rtol*np.abs(new_y)\r\n\r\n    roots = spline.roots()\r\n    mask = (roots > bounds[0]) & (roots < bounds[1])\r\n    roots = roots[mask]\r\n    return roots\r\n\r\nall_roots(f, bounds)\r\n# array([-9.42477796, -6.28318531, -3.14159265,  0.        ,  3.14159265,\r\n#         6.28318531,  9.42477796])\r\n```\r\nTBH, it would be useful to release the part that generates the interpolator as it's own function.\r\n\r\nIt may not be the fastest possible solution, but it's simple becauses it leverages a lot of existing code, and I'd say it's best to add the functionality and optimize later.\r\n\r\nI can submit a PR if this satisfies the suggestion.","@izaid not sure if this falls under what you had in mind since it is restricted to the real line?","@j-bowhay Thanks! Indeed, this would only work for the real line, so it's valuable but different. Everyone else, @j-bowhay and I have been discussing exactly this problem for the complex plane.\r\n\r\n@fancidev @mdhaber So I've also played around with finding all roots on the real line. The book \"Solving Transcendental Equations\" by John Boyd has a good description of how to do it. You can do something similar to what @mdhaber wrote, but using a Chebyshev polynomial from NumPy. The following is a starting point that does this:\r\n\r\n```\r\ndef roots(func, a, b, n = 100, rtol = 1e-05, atol = 1e-08):\r\n    while True:\r\n        p = np.polynomial.chebyshev.Chebyshev.interpolate(func, n, domain = (a, b))\r\n\r\n        x, y = p.linspace()\r\n\r\n        if np.allclose(y, func(x), rtol, atol):\r\n            break\r\n\r\n        n *= 2\r\n\r\n    vals = p.roots()\r\n    mask = np.logical_and(np.isreal(vals), np.logical_and(a < vals, vals < b))\r\n\r\n    vals = np.real(vals[mask])\r\n\r\n    return vals\r\n```","Thanks @izaid for the great suggestion! Let me get a copy of Byod\u2019s book and take a look.\r\n\r\nAnd thanks @mdhaber for the sample code! Performance is not a concern to me as long as it works. Let me try them on my test data and come back.","After skimming through Boyd\u2019s book and doing a bit more Internet search, I find the following approaches to the problem finding all roots of $f(x)$ for $a \\le x \\le b$ for general sufficiently smooth $f$:\r\n\r\n1. Graph-based approach: Evaluate $f$ at $n$ locations (`linspace`, `geomspace`, or others), find out adjacent locations where the function changes sign, and launch a solver on these intervals. Advantage: simple and intuitive. Limitation: misses roots if there is more than one root within a sub-interval.\r\n\r\n2. Polynomial approximation approach (Boyd, 2013): Approximate the function (piecewise) by a polynomial (Chebyshev, cubic spline, or others) \u201cadequately\u201d, find the roots of the approximant, and optionally polish the results with a Newton step. \u201cAdequacy\u201d is checked by sampling function value at mid points between knots. Advantage: smooth interpolant reduces the chance of missing roots due to failure to detect function shape change. Limitation: Global method may be difficult to converge, and by sampling it may miss roots anyway. (A pathological example is `lambda x: scipy.stats.norm(-0.5,0.001).pdf(x)-10`.) \r\n\r\n3. Branch and bound approach (Hansen, 1978): Repeatedly divide the interval into sub-intervals until the first derivative of $f$ does not change sign on each sub-interval, and then launch a root solver for each interval where $f$ changes sign. A recent overview is in Gwaltney et al (2008). Advantage: no opportunistic sampling is relied upon. Limitation: User must supply bounds for $f\u2019$ over any given interval, and these bounds should not be too coarse. (The bounds also help to prune \u201chopeless\u201d subintervals.) \r\n\r\nSo it looks like the desired function could have different underlying implementations, like `root_scalar` does!\r\n\r\n**Reference**\r\n\r\nBoyd, J. P. (2013). \u201cFinding the Zeros of a Univariate Equation: Proxy Rootfinders, Chebyshev Interpolation, and the Companion Matrix.\u201d _SIAM Review_, 55(2):375-396.\r\n\r\nGwaltney C. R., Lin Y., Simoni L. D., and Stadtherr, M. A. (2008). \u201cInterval Methods for Non-Linear Equation Solving Applications.\u201d in _Handbook of Granular Computing_.\r\n\r\nHansen, E. (1978). \u201cA Globally Convergent Interval Method For Conputing and Bounding Real Roots.\u201d _BIT Numerical Mathematics_, 18:415-424.","I see. Maybe I misunderstood the title.","> I see. Maybe I misunderstood the title.\r\n\r\nYour code shows you understood the title as I understood it! And it\u2019s the same idea as Boyd\u2019s monograph, but with adaptive Chebyshev interpolation replaced by adaptive cubic spline interpolation. I don\u2019t know enough of either to tell which is better at the task.\r\n\r\nIndeed it will be of great value to expose the adaptive cubic spline interpolation itself as a functionality!","Yeah, @mdhaber I think you proposed a totally valid approach!\r\n\r\nI think Chebyshev interpolation is used because it minimizes the error. In terms of getting this into SciPy, @fancidev you could always make a pull request based on the code I pasted. Happy to guide you if that helps.","@izaid I can try make a PR. But since I\u2019m an absolute newbie in Chebyshev polynomials, this is going to take some time :-) Also I\u2019ll start without any of the advanced stuff like automatic interval split (to contain the order of polynomial; I feel spline may be better in this regard as they\u2019re inherently local\u2026)","> @izaid I can try make a PR. But since I\u2019m an absolute newbie in Chebyshev polynomials, this is going to take some time :-) Also I\u2019ll start without any of the advanced stuff like automatic interval split (to contain the order of polynomial; I feel spline may be better in this regard as they\u2019re inherently local\u2026)\r\n\r\nUp to you! Just offering some guidance if you'd like. Also feel free to ping me offline.","Thanks @izaid! The book you point out is very useful! It also leads me to take a look at [chebfun](https:\/\/www.chebfun.org\/). And a bit googling leads me to [chebpy](https:\/\/github.com\/chebpy\/chebpy\/tree\/master), a Python translation of the MATLAB code \u2014 sounds like problem solved :-) I\u2019ll try that library with my problem first.\r\n\r\n_Update_:  I tested `chebpy` with a challenging problem. The left chart below is the log likelihood function of Pareto. The right chart below is its derivative, i.e. the likelihood equation. I want to find the roots of the right function over the interval `[1e-4,1e4]`; the interval is derived analytically. From a visual check we can tell the roots are located at the two small kinks near the left of the graph.\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/10579828\/8f1fffa5-5dbf-4fcf-8b43-7f35a08dc36b)\r\n\r\n`chebpy` is unable to interpolate $f'$ to machine accuracy, but it manages to interpolate $f$ to machine accuracy, and then differentiate it. This leads to one piece of 47061 knots, and it took many seconds to find its roots due to the large number of knots. It seems automatic interval splitting is essential for this example.\r\n\r\nOf course this example is somewhat \"unfair\" because by writing $g(t) \\equiv f'(10^t)\\times 10^t$,  `chebpy` can fit $g$ easily with just 101 knots and find the roots instantly. The graph of $g$ looks like below:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/10579828\/960888c4-eb63-4bbd-8e83-68ad3c0b29b9)\r\n\r\nThis shows the user needs to do some homework and not simply throw a function to the solver.\r\n\r\n_Update 2_: @mdhaber's cubic spline interpolation method solves the transformed equation $g(t)=0$ instantly! It also solve the original equation $f'(x)=0$ correctly, but I had to put an (arbitrary) cap on the iterations because it doesn't converge even if I put `rtol = atol = 0.1`."],"labels":["enhancement","scipy.optimize"]},{"title":"BUG: Incorrect variable assignment in optimize._trust_region_exact","body":"### Describe your issue.\n\nIn the [`solve`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L285) method of the `IterativeSubproblem` class in the file `optimize._trust_region_exact`, the Cholesky factors are assigned to a variable which is never updated in the while loop. In the beginning of the [`while`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L294) loop, the Cholesky factor is assigned to the variable `U` in line [`301`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L301). As the algorithm progresses, the Cholesky factor is stored in the variable `c` in line [`359`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L359) but the computations (solving the linear system) are done with respect to `U` as shown in lines [`312`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L312) and [`322`](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_trustregion_exact.py#L322). This incorrect assignment\/updating causes the method to fail as evidenced by the below example.\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nimport scipy\r\n\r\ndimension = 2 #number of decision-variables\r\nx_initial = np.zeros(dimension) #initial point\r\ngrad = np.array([20,-10]) #input gradient\r\nhess = np.array([[15,0],[0,5]]) #input hessian\r\ntrust_region_radius = 1.0 #trust region radius\r\n\r\nexact = scipy.optimize._trustregion_exact.IterativeSubproblem(x_initial,lambda x:0,\r\n            lambda x:grad,lambda x:hess,k_easy=1e-6,k_hard=1e-9)\r\np,hits_boundary = exact.solve(trust_region_radius)\n```\n\n\n### Error message\n\n```shell\nWarning (from warnings module):\r\n  File \"\/opt\/homebrew\/lib\/python3.11\/site-packages\/scipy\/optimize\/_trustregion_exact.py\", line 375\r\n    np.sqrt(lambda_lb * lambda_ub),\r\nRuntimeWarning: invalid value encountered in sqrt\r\nTraceback (most recent call last):\r\n  File \"<pyshell#9>\", line 1, in <module>\r\n    p,hits_boundary = exact.solve(trust_region_radius)\r\n  File \"\/opt\/homebrew\/lib\/python3.11\/site-packages\/scipy\/optimize\/_trustregion_exact.py\", line 312, in solve\r\n    p = cho_solve((U, False), -self.jac)\r\n  File \"\/opt\/homebrew\/lib\/python3.11\/site-packages\/scipy\/linalg\/_decomp_cholesky.py\", line 197, in cho_solve\r\n    c = asarray_chkfinite(c)\r\n  File \"\/opt\/homebrew\/lib\/python3.11\/site-packages\/numpy\/lib\/function_base.py\", line 628, in asarray_chkfinite\r\n    raise ValueError(\r\nValueError: array must not contain infs or NaNs\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.12.0 1.24.4 sys.version_info(major=3, minor=11, micro=7, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-ewarq4oq\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-73jfxkej\/cp311-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.11'\n```\n","comments":[],"labels":["defect","scipy.optimize"]},{"title":"DOC: sparse: Correct `todense` documentation","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nCloses #20239 \r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nChanged the return documentation correct the return type of `todense`\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\nThe return type documentation has been copied from the `toarray` method. There may be some difference between how these methods work that I'm not aware of so the documentation may not be accurate. If someone more knowledgeable could check that would be appreciated. ","comments":["Unfortunately, this changes both the sparse array and the sparse matrix documentation.  So now, while the `todense` docs for e.g. `csr_array` are correct, the docs for `csr_matrix` say `ndarray` when they should say `matrix`.  Looks like we will need to split the doc_strings for each method to fix this doc issue.\r\n\r\nSee #18898 for more discussion.","As @dschult said, this is changing both array and matrix when it should only be applied to sparse arrays. To fix this, you could keep your existing changes and then assign a new docstring to the `todense` method in scipy\/sparse\/_matrix.py"],"labels":["scipy.sparse","Documentation","needs-work"]},{"title":"ENH: multiple small improvements to scipy.stats.circmean","body":"### Is your feature request related to a problem? Please describe.\n\nWhen working on the MLE of `vonmises` distribution, I came across the `circmean` function and had to read the source code to find out what exactly it\u2019s doing. A few improvements could make it easier to use the function.\n\n### Describe the solution you'd like.\n\nThe improvements I\u2019d suggest are:\r\n\r\n1. The [documentation](https:\/\/scipy.github.io\/devdocs\/reference\/generated\/scipy.stats.circmean.html#scipy.stats.circmean) can be clearer. Especially the notions of \u201csamples\u201d, \u201cboundary\u201d, and \u201csample range\u201d are really confusing. \r\n\r\n2. Deprecate the `high` and `low` arguments. They are there for radian\/degree conversion (which explains why `high` comes before `low`), but such conversion should be handled by the user. (Or otherwise all trigonometric functions would accept `high` and `low`.) The doc already provides a clear example of how to do the radian\/degree conversion.\r\n\r\n3. Rename the first argument to `a` and make it a position-only argument. The naming is consistent with e.g. `np.mean`. And making it position-only (a breaking change) ensures callers don\u2019t reference it by name.\r\n\r\n4. Before `high` and `low` are fully removed, rearrange the computation code so that the conversions don\u2019t bring [unnecessary numerical error](https:\/\/stackoverflow.com\/questions\/61761300\/how-to-not-lose-precision-on-division-and-multiplication).\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["1. Documentation improvements are always welcome.\r\n2. I think here `high` and `low` are also for handling situations where data only live on the half circle, for example. So not sure if we should remove them.\r\n3. `samples` is not the best possible name likely but not sure if `a` is better. I guess that whoever uses the function will assume that the first argument is the data array they want to compute the circular mean of. But if it confused you, I might be wrong. After all, users always surprise us devs ;).\r\n4. What would the improved code look like? Could you provide a small example where another ordering of the operations reduces the error?","I\u2019ll make separate PRs for (1) and (4). (4) is straightforward and I\u2019d simply rewrite `a*b\/b` as `a*(b\/b)`. For (1) I\u2019d say `high` and `low` correspond to the value of complete angle and zero angle, respectively, possibly with a shift. I\u2019d also mention what the function returns if the points are symmetric and the resultant vector is zero.\r\n\r\nFor (2), I don\u2019t mean that scaling is not useful, but that they should be handled outside of `circmean`. For the case where data points live on the half circle, say between 0 to 180 degrees, is the `circmean` of 45 degrees and 135 degrees supposed to be 90 degrees or mathematically undefined?","An example where two angles are symmetric _numerically_:\r\n\r\n`scipy.stats.circmean([0.32202300504740655,3.4636156586372])` returns 0.\r\n\r\nThe same data gives `inf` for `circstd`, which is mathematically (approximately) correct but we may (or may not) get rid of the `RuntimeWarning` caused.","A related corner case is that `scipy.stats.circstd([0])` returns `-0.0`. This usually has no impact but if someone writes `1\/circstd(\u2026)` there\u2019s a chance the wrong sign gets propagated. So might worth fixing."],"labels":["scipy.stats","enhancement"]},{"title":"DOC: Sparse arrays: todense() does not return numpy.matrix","body":"### Issue with current documentation:\n\nThe documentation states that the `todense()` method of each of the sparse array classes returns a `numpy.matrix` but they return a `numpy.ndarray`.\n\n### Idea or request for content:\n\nModify the documentation to state the return type is `numpy.ndarray`.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["@henry50 would you be interested in opening a PR?","See also #18898 "],"labels":["scipy.sparse","Documentation","good first issue"]},{"title":"META: Streamlined Special Function Development in SciPy SDG Tracking","body":"This issue is intended for tracking milestones for the 2023 NumFocus small development grant for working on `scipy.special` infrastructure.\r\n\r\nThe original plan for this grant was to work on developing infrastructure for implementing special functions with [Pythran](https:\/\/pythran.readthedocs.io\/en\/latest\/), using primitives which abstract over low level details. This plan was been abandoned in favor of efforts to write special functions in CUDA compatible C++ to help improve special function availability on GPU-aware array libraries like CuPy, PyTorch, and Jax, discussed [here](https:\/\/github.com\/scipy\/scipy\/issues\/19404).\r\n\r\nNumFocus has granted a no-cost extension, and the goal is to wrap up these deliverables by end of August this year.\r\n\r\nThis first block of deliverables is for general special function infrastructure.\r\n\r\n- [x] Infrastructure for special function development in CUDA compatible C++.\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/19601\r\n- [ ] Series evaluation primitives in CUDA compatible C++.\r\nWill be completed by merging https:\/\/github.com\/scipy\/scipy\/pull\/20089\r\n- [ ] Continued fraction evaluation primitive in ...\r\nI've already implemented this, and will submit a PR using it for exponential integrals after hyp2f1 gets in.\r\n- [ ] Series accelerator primitive(s).\r\nWill use for Riemann zeta for complex numbers.\r\n- [ ] Root finding primitive.\r\nCan implement the root finder from cdflib and some cdflib functions.\r\n- [ ] Generic Miller's algorithm framework for solving recurrences for argument reduction of special functions. (Stretch goal)\r\nwill implement and use in hyp2f1.\r\n- [ ] Framework for generating rational and polynomial approximations (e.g. Pade approximants, Minimax), for use in special functions.\r\n-mpmath has Pade. I plan to update the cephes Remez exchange algorithm to work with native 64 bit ints to do minimax.\r\n- [ ] Improved testing framework for special functions. (Stretch goal)\r\nThis is @mdhaber's [mparray](https:\/\/github.com\/mdhaber\/mparray)\r\n\r\nThis second block is for documentation improvements.\r\n\r\nDetailed documentation for how to use the infrastructure described above.\r\n- [ ] Document how to write special functions in CUDA compatible C++.\r\n- [ ] Document how to use series evaluators.\r\n- [ ] Document how to use continued fraction evaluators.\r\n- [ ] Document series accelerator\r\n- [ ] Document Generic Miller's algorithm framework\r\n- [ ] Document framework for generating rational and polynomial approximations.\r\n- [ ] Document best practices for implementing numerically robust functions.\r\n- [ ] Document best practices for testing new special functions.\r\n\r\nFinally, a list of issues we hope to close.\r\n\r\n- [ ] #3890\r\n- [ ] #8424 \r\n- [ ] #15582\r\n- [ ] #14117\r\n- [ ] #15650 \r\n- [ ] #15151\r\n- [ ] #10795 \r\n- [ ] #7722\r\n- [ ] #14073\r\n- [ ] #5349\r\n\r\ncc @tirthasheshpatel @rlucas7 ","comments":["Thanks @steppi, I am interested in improving Pade approximants in SciPy, see #20064 and [my recent mailing list post](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/3CBCZJCRLUZLHF2ZSV4XWUL2S3XEMOOY\/). I was planning on implementing the method described in Trefethen paper and co paper [Robust Pade Approximation via SVD](https:\/\/people.maths.ox.ac.uk\/trefethen\/publication\/PDF\/2011_144.pdf) (which is also used in Chebfun) when I eventually get some free time from university. Is there is there any crossover here?","> Thanks @steppi, I am interested in improving Pade approximants in SciPy, see #20064 and [my recent mailing list post](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/3CBCZJCRLUZLHF2ZSV4XWUL2S3XEMOOY\/). I was planning on implementing the method described in Trefethen paper and co paper [Robust Pade Approximation via SVD](https:\/\/people.maths.ox.ac.uk\/trefethen\/publication\/PDF\/2011_144.pdf) (which is also used in Chebfun) when I eventually get some free time from university. Is there is there any crossover here?\r\n\r\nThat's a cool idea and I hope you get time for it soon. I don't think there will be crossover though. For `special` we'd be interested in using arbitrary precision, which is [already in mpmath](https:\/\/mpmath.org\/doc\/current\/calculus\/approximation.html#pade-approximation-pade) and symbolic computation, which could be built on top of SymPy.","I think we should also consider the testing mechanisms we have. There is no transparency in what is being tested and with what data points.","> I think we should also consider the testing mechanisms we have. There is no transparency in what is being tested and with what data points.\r\n\r\nYeah, agreed. It wasn't in the original grant proposal, but is very important. I've added a bullet point for it above.","It was in the proposal to document best practices for testing special functions. We didn't commit to delivering `mparray` and a full framework as part of the grant, but that sort of thing was in mind.","> It was in the proposal to document best practices for testing special functions. We didn't commit to delivering `mparray` and a full framework as part of the grant, but that sort of thing was in mind.\r\n\r\nI changed it to a stretch goal above to reflect that it wasn't in the proposal, but we still plan to do it.","I've created a spreadsheet for every function that I think is in scope for the C++ translations. I was going to create a tracking issue for everything, but have decided to hold off until this one is closed. Just posting this here for anyone interested in seeing it. I plan to keep it up to date as we add things.\r\n\r\nhttps:\/\/docs.google.com\/spreadsheets\/d\/1Iw7C9--CCD6z_b3Oq4diba-x_9b303yDcNHBHbznGTQ\/edit?usp=sharing"],"labels":["scipy.special"]},{"title":"BUG: failing `sqrtm` regression test","body":"Caused by gh-20212. Marked as xfail in gh-20218. Failing with both OpenBLAS 0.3.21 and new Accelerate - see https:\/\/github.com\/scipy\/scipy\/pull\/19816#issuecomment-1985656794.\r\n\r\nFrom [this CI log](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/8201161926\/job\/22429344365):\r\n\r\n```\r\n____________________________ TestSqrtM.test_gh17918 ____________________________\r\n  ..\/venv-test\/lib\/python3.12\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py:423: in test_gh17918\r\n      assert np.isrealobj(sqrtm(M))\r\n  E   assert False\r\n  E    +  where False = <function isrealobj at 0x114d80e30>(array([[0.45485703-1.38777878e-17j, 0.20990806+3.46944695e-17j,\\n        0.20990806-1.08420217e-18j, 0.20990806-1.30104...+1.08420217e-19j,\\n        0.20990806+1.08420217e-19j, 0.20990806+1.08420217e-19j,\\n        0.45485703+1.08420217e-19j]]))\r\n  E    +    where <function isrealobj at 0x114d80e30> = np.isrealobj\r\n  E    +    and   array([[0.45485703-1.38777878e-17j, 0.20990806+3.46944695e-17j,\\n        0.20990806-1.08420217e-18j, 0.20990806-1.30104...+1.08420217e-19j,\\n        0.20990806+1.08420217e-19j, 0.20990806+1.08420217e-19j,\\n        0.45485703+1.08420217e-19j]]) = sqrtm(array([[1.  , 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94,\\n        0.94, 0.94, 0.94, 0.94, 0.94, 0.94, ..., 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94,\\n        0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.  ]]))\r\n          M          = array([[1.  , 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94,\r\n          0.94, 0.94, 0.94, 0.94, 0.94, 0.94, ..., 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94,\r\n          0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.  ]])\r\n          self       = <scipy.linalg.tests.test_matfuncs.TestSqrtM object at 0x12b4b79b0>\r\n\r\nFAILED linalg\/tests\/test_matfuncs.py::TestSqrtM::test_gh17918 - assert False\r\n```\r\n\r\nCc @ev-br ","comments":[],"labels":["defect","scipy.linalg"]},{"title":"BUG: minimize(method=\"newton-cg\") crashes with UnboundLocalError if xtol=0","body":"### Describe your issue.\n\nI was trying to debug an issue where I was unable to find the minimum of a function with Newton-CG. As one of my debugging steps, I tried setting the solver option `xtol` to zero. I expected this to disable termination due to x convergence, but what actually happens is that it crashes.\n\n### Reproducing Code Example\n\n```python\nimport math\r\nimport scipy.optimize\r\n\r\n\r\ndef cosine(x):\r\n    f = math.cos(x[0])\r\n    print(f\"f({x[0]}) = {f}\")\r\n    return f\r\n\r\n\r\ndef jac(x):\r\n    ret = -math.sin(x)\r\n    return ret\r\n\r\n\r\nx0 = [0.1]\r\nxtol = 0  # crashes with UnboundLocalError\r\n# xtol = 1e-10  # works\r\nresult = scipy.optimize.minimize(cosine, x0=x0, jac=jac, method=\"newton-cg\", options=dict(xtol=xtol))\r\n\r\nprint(result)\n```\n\n\n### Error message\n\n```shell\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\nInput In [41], in <cell line: 19>()\r\n     17 xtol = 0  # crashes with UnboundLocalError\r\n     18 # xtol = 1e-10  # works\r\n---> 19 result = scipy.optimize.minimize(cosine, x0=x0, jac=jac, method=\"newton-cg\", options=dict(xtol=xtol))\r\n     21 print(result)\r\n\r\nFile ~\/.local\/lib\/python3.10\/site-packages\/scipy\/optimize\/_minimize.py:710, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\r\n    708     res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\r\n    709 elif meth == 'newton-cg':\r\n--> 710     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\r\n    711                              **options)\r\n    712 elif meth == 'l-bfgs-b':\r\n    713     res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\r\n    714                            callback=callback, **options)\r\n\r\nFile ~\/.local\/lib\/python3.10\/site-packages\/scipy\/optimize\/_optimize.py:2233, in _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, c1, c2, **unknown_options)\r\n   2230     update_l1norm = np.linalg.norm(update, ord=1)\r\n   2232 else:\r\n-> 2233     if np.isnan(old_fval) or np.isnan(update).any():\r\n   2234         return terminate(3, _status_message['nan'])\r\n   2236     msg = _status_message['success']\r\n\r\nUnboundLocalError: local variable 'update' referenced before assignment\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.12.0 1.23.5 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-oj5r1eje\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\n```\n","comments":["I've done some reading of the code involved, and it seems to be related to the termination condition of this loop:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2151\r\n\r\n`update_l1norm` is the l1norm of the previous update. Before the loop, it is set to 2 * xtol, which is intended to cause the loop to run at least once. However, if xtol = 0, then it is comparing 0 > 0, which is false, which then checks this condition.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2233\r\n\r\nSince `update` is defined in the loop, this is implicitly assuming that the previous loop ran at least once.","I've raised a PR to close this issue: https:\/\/github.com\/scipy\/scipy\/pull\/20299\r\n\r\nI agree that the initial value of `update_l1norm` was chosen to ensure the loop runs at least once, but this fails for `xtol = 0`, so a simple fix.","`UnboundLocalError: local variable 'update' referenced before assignment` is a trivial Python programming error, and should be fixed ASAP, especially if the fix is small. I'm milestoning this for 1.13, though it's of course not a release blocker."],"labels":["defect","scipy.optimize"]},{"title":"BUG: scipy.optimize.minimize with method=\"newton-cg\" is stuck on a local maximum","body":"### Preface\r\nI either have a usage problem or there is something wrong with the source code. Because I'm unsure which is the case I both opened a [stackoverflow question](https:\/\/stackoverflow.com\/questions\/78122802\/why-is-my-scipy-optimize-minimizemethod-newton-cg-function-stuck-on-a-local) and this Issue. I will just copy the text I've already written on stackoverflow to here.\r\n\r\n### tl;dr\r\nI want to find the local minimum for a function that depends on 2 variables. For that my plan was to use the scipy.optimize.minimize function with the \"newton-cg\" method because I can calculate the jacobian and hessian analytically. However, when my starting guesses are on a local maximum, the function terminates successfully in the first iteration step on top of the local maximum, even though the hessian is negative.\r\n\r\n### Reproducing code\r\nThe reproducing code and the wrongly successfull termination message can be seen below.\r\n\r\nThe same outcome occurs if I use `hess=None`, `hess='cs'`, `hess='2-point'` or `hess='3-point'` instead of my custom hess function. Also if I use other methods like `'dogleg'`, `'trust-ncg'`, `'trust-krylov'`, `'trust-exact'` or `'trust-constr'` I basically get the same outcome except `nhev = 1` but the outcome is still wrong with x = [0,0].\r\n\r\nEither I am doing something terribly wrong here (definitely very likely) or there is a major problem with the minimize function, specifically the \"newton-cg\" method (very unlikely?).\r\n\r\nRegarding the latter case I also checked the source code to see if something's wrong there and stumpled upon something kinda weird(?). However, I don't completely understand the whole code, so I am a bit unsure if my worries are legitimate:\r\n\r\n### Taking a look at the source code\r\n\r\nWhen the minimize function is called with method=\"newton-cg\" it jumps into the _minimize_newtoncg function (see source code [here](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2053)). I want to go into detail what I believe happens here (so you can tell where I am potentially wrong):\r\n\r\nOn [line 2168](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2168) `A = sf.hess(xk)` the hessian is first calculated in dependence of `xk` which is at first the start guess `x0`. For my test case the hessian is of course\r\n\r\nA = [[f_xx, f_xy], [f_xy, f_yy]]\r\n\r\nwith f_ij being the derivatives of f after i and j. In my case f_xy = f_yx is also true.\r\n\r\nNext on [line 2183](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2183) `Ap = A.dot(psupi)` the product of the hessian `A` and `psupi` is calculated. `psupi` is basically equal to `b` which is the negative gradient of `f` at `xk`. So `Ap = A.dot(psupi)` results in\r\n\r\nAp = [f_xx f_x + f_xy f_y, f_xy f_x + f_yy f_y].\r\n\r\n### Now to the (possible) problem\r\n\r\nNext, the curvature curv is calculated on [line 2186](https:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/optimize\/_optimize.py#L2186) by `np.dot(psupi, Ap)`. As explained above, `psupis` the negative gradient of `f` so this results in\r\n\r\ncurv = f_xx f_x^2 + 2 f_xy f_x f_y + fyy_f_y^2.\r\n\r\nHowever, all of these derivatives are at `xk` which is equal to the starting parameters `x0` at first. If the starting parameters are exactly at a local maximum, the derivatives `f_x` and `f_y` are equal to `0`. Because of this, `curv = 0`. This results in a for loop break on the next line, thus, skipping to update `xsupi`, `psupi` and all the other parameters. Therefore, `pk` becomes [0,0] and `_line_search_wolfe12` is called with basically all start parameters. This is where my understanding of the source code stops, however, I feel like things already went wrong after `curv = 0` and breaking the for loop.\r\n\r\nMy concluding question is: What am I doing wrong leading to the minimize function being stuck on a local maximum?\r\n\r\n**Small update:** I now understand that this case of starting at a local maximum and finding a local minimum from there might not have been implemented. So I am unsure if that would be doable to add in the future. If it were, I'd greatly appreciate it.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.optimize as o\r\n\r\ndef get_f_df(var):\r\n    x = var[0]\r\n    y = var[1]\r\n\r\n    f = np.cos(x) + np.cos(y)\r\n    df_dx = -np.sin(x)\r\n    df_dy = -np.sin(y)\r\n\r\n    return f, (df_dx, df_dy)\r\n\r\ndef hess(var):\r\n    x = var[0]\r\n    y = var[1]\r\n\r\n    f_hess = np.zeros((2,2))\r\n    f_hess[0,0] = -np.cos(x)\r\n    f_hess[1,1] = -np.cos(y)\r\n    return f_hess\r\n\r\nmin = o.minimize(get_f_df, (0, 0), jac=True, hess=hess, method=\"newton-cg\")\r\nprint(min)\r\n```\r\n\r\n\r\n### Error message (not an error but the OptimizedResult)\r\n\r\n```shell\r\nmessage: Optimization terminated successfully.\r\n success: True\r\n  status: 0\r\n     fun: 2.0\r\n       x: [ 0.000e+00  0.000e+00]\r\n     nit: 1\r\n     jac: [-0.000e+00 -0.000e+00]\r\n    nfev: 1\r\n    njev: 1\r\n    nhev: 1\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2    \r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2    \r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-d02me32y\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-aug4qibh\\cp311-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.11'\r\n```\r\n","comments":[],"labels":["scipy.optimize","query"]},{"title":"BUG: Test failures due to `invalid value encountered in _beta_ppf` on M2 mac","body":"### Describe your issue.\n\nThe following tests fail with the warning `invalid value encountered in _beta_ppf`\r\n\r\n\n\n### Reproducing Code Example\n\n```python\n`python dev.py test`\n```\n\n\n### Error message\n\n```shell\nFAILED scipy\/stats\/tests\/test_distributions.py::TestBeta::test_issue_12635 - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_distributions.py::TestBeta::test_issue_12794 - RuntimeWarning: invalid value encountered in _beta_isf\r\nFAILED scipy\/stats\/tests\/test_distributions.py::TestBeta::test_issue_12796 - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fast_gen_inversion.py::test_rvs_and_ppf[betaprime-args6] - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_distributions.py::TestBetaPrime::test_ppf[0.375-0.25-7.0-0.002036820346115211] - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fast_gen_inversion.py::test_non_rvs_methods_with_domain - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fit.py::TestGoodnessOfFit::test_against_filliben_norm - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fit.py::TestGoodnessOfFit::test_filliben_property - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fit.py::TestGoodnessOfFit::test_against_filliben_norm_table[case0] - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fit.py::TestGoodnessOfFit::test_against_filliben_norm_table[case1] - RuntimeWarning: invalid value encountered in _beta_ppf\r\nFAILED scipy\/stats\/tests\/test_fit.py::TestGoodnessOfFit::test_against_filliben_norm_table[case2] - RuntimeWarning: invalid value encountered in _beta_ppf\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.13.0.dev0+0.9979b1d 1.26.4 sys.version_info(major=3, minor=11, micro=8, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2, -isystem,\r\n      \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -O2, -pipe, -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 16.0.6\r\n  c++:\r\n    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -O2, -pipe, -stdlib=libc++, -fvisibility-inlines-hidden, -fmessage-length=0,\r\n      -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 16.0.6\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.9\r\n  fortran:\r\n    args: -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/kai\/miniconda3\/envs\/scipy-dev\/lib, -march=armv8.3-a, -ftree-vectorize,\r\n      -fPIC, -fno-stack-protector, -O2, -pipe, -isystem, \/Users\/kai\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/miniconda3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/kai\/miniconda3\/envs\/scipy-dev\/bin\/python3.11\r\n  version: '3.11'\r\n```\n```\n","comments":["Do you get the same errors using the incomplete beta function via [special](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.betaincinv.html)? That one also uses boost under the hood but was wrapped in a different way, might be worth trying out.","No. Only in stats","We could try to replace the custom Boost wrappers for the beta distribution by directly calling the functions from `special` then. The required ones are\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/40656107\/6d897713-e456-4b9b-ba14-2766b318a050)\r\n\r\nThat would also reduce duplication between stats and special. special on its own has enough duplication already ;).\r\n\r\nWould you be able to test it out? I do not have an M2 Mac to test locally.","I reproduce these failures on my M1 Mac","Those tests are passing on my M2 with `Apple clang version 15.0.0 (clang-1500.3.9.4)`. Are you using the newer `clang` from above as well Lucas?","Yes, I'm on `clang 16.0.6` like Kai","Ah yeah, and I bet you're both using `conda` too.","I'll test out using the ones for special tomorrow. Let's see if that makes a difference \r\n\r\n**EDIT:** I didn't get the time to try out the special version. I'll try fit in some time this week, but I'm pretty time poor at the moment."],"labels":["defect","scipy.stats"]},{"title":"ENH: sparse: Validate dtype on sparse array\/matrix creation","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nDue to limitations of the native `sparsetools` module and compiler support for half-float types, we have never supported np.float16 as a data type for `scipy.sparse` arrays (or matrices). (See gh-7408 for details and discussion.)\r\n\r\nThat said, we don't currently validate the dtype users specify, which leads to confusing errors and several bug reports: gh-2481, gh-7408, gh-8903, gh-13352, gh-19655, and gh-20200.\r\n\r\n### Describe the solution you'd like.\r\n\r\nIn each sparse format's `__init__` method, we could check the input `dtype` against our existing list of supported types: https:\/\/github.com\/scipy\/scipy\/blob\/2aee5efcbe3720f41fe55f336f492ae0acbecdee\/scipy\/sparse\/_sputils.py#L16-L19\r\n\r\nInitially we should probably raise a warning if the given dtype is incompatible, and eventually we may promote this to an error.","comments":[],"labels":["enhancement","scipy.sparse"]},{"title":"DOC\/DX\/MAINT: update core-dev guide","body":"#### Reference issue\r\nNone\r\n\r\n#### What does this implement\/fix?\r\nSome minor updates to the core-dev guide. Some outdated things, some typos and a few additions.\r\n","comments":["The reversion caused by gh-20227 has shown that adding a list of modules which we vendor (but aren't git submodules) would be useful."],"labels":["Documentation","maintenance","DX"]},{"title":"TST: linalg: fix complex sort in test_bad_geneig","body":"#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\ncloses https:\/\/github.com\/scipy\/scipy\/issues\/17125\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nThe failure is relatively benign if annoying: the computation is most likely always correct, what fails is ordering of eigenvalues consistently on the complex plane, as Warren's analysis in  https:\/\/github.com\/scipy\/scipy\/issues\/17125#issuecomment-1264644368 shows.\r\n\r\nSo try a bit harder to trim numerical noise and account for complex conjugate pairs.\r\n\r\nHave to admit I cannot repro locally. @h-vetinari you mentioned it's reproducible on conda-forge? https:\/\/github.com\/scipy\/scipy\/issues\/17125#issuecomment-1609160730\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":["> @h-vetinari you mentioned it's reproducible on conda-forge? [#17125 (comment)](https:\/\/github.com\/scipy\/scipy\/issues\/17125#issuecomment-1609160730)\r\n\r\nI realized that I never finished the cross-BLAS-flavour [analysis](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/267) for 1.12. I've restarted that now, and will try to write a summary later. If you want to check yourself, you should see[^1] it in the `win_64_blas_implopenblas...` builds, if the failure is still there.\r\n\r\n[^1]: modulo getting the right agent in CI matching avx512 absence\/presence; if the test suite didn't run, just chose any other job","Curious about the cross-flavor blas analysis. Is the intention to proceed in the 1.13rc phase, or is it realistically for 1.14 and beyond? ","@h-vetinari Any chance you'd be able to check that this PR fixes the failure seen on conda-forge? An alternative is of course to just land it and see what falls out in your next round of cross-blas analysis.","> @h-vetinari Any chance you'd be able to check that this PR fixes the failure seen on conda-forge?\r\n\r\nGave it a shot in https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/267\/commits\/c0a0f4882133d342ce4bd624259e079ad394ac64. CI will run for 3-4 hours before we have results.","So it appear that this fixes the errors - I've started a second run to be sure. In any case thanks a lot for coming up with a fix! In this case I'd like to have this in 1.13, so I'm adding the backport-candidate label."],"labels":["defect","scipy.linalg","backport-candidate"]},{"title":"Audit usage of cython memoryviews, add `const` to allow readonly arrays","body":"Starting from Cython >=3.0.8, readonly fused typed memoryviews are finally allowed. Meaning we can add `const`  qualifiers to memoryviews so that readonly arrays do not get rejected. We should audit our usage of memoryviews with fused dtypes and sprinkle `const` pretty much everywhere.\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/20195 gives a representative example.","comments":["gh-20085 contains what's needed in `cluster` for the test suite to pass (having JAX support is a handy way to check read-only arrays)","@ev-br I can help out with this, but I'm not quite sure where to start. Any recommended places to look at?","That would be great @Kai-Striega !\r\nI would just grep pyx files by subpackage, along the lines of `$ grep def scipy\/optimize  -rn` to list all cython function declarations. For each of those, if it has memoryviews, add `const` to all which are not outputs.\r\n\r\nIOW,  `def func(const double[::1] x, double[::1] out)` means that `x` can be a regular or a readonly array, and `out` cannot be readonly. \r\n","Great. I'll try to audit what needs to be done tonight\/this weekend\r\n\r\nEDIT: I did a quick run on this, here is the invocation and output:\r\n\r\n```\r\n~\/Projects\/scipy on main                                                                                                                                                                            at 13:02:25\r\n\u276f grep def --include .\/scipy\\*.pyx  -rnc\r\n.\/scipy\/cluster\/_vq.pyx:32\r\n.\/scipy\/cluster\/_optimal_leaf_ordering.pyx:15\r\n.\/scipy\/cluster\/_hierarchy.pyx:117\r\n.\/scipy\/ndimage\/src\/_cytest.pyx:28\r\n.\/scipy\/ndimage\/src\/_ni_label.pyx:26\r\n.\/scipy\/linalg\/_decomp_lu_cython.pyx:9\r\n.\/scipy\/linalg\/_matfuncs_sqrtm_triu.pyx:4\r\n.\/scipy\/linalg\/_cythonized_array_utils.pyx:40\r\n.\/scipy\/linalg\/_solve_toeplitz.pyx:9\r\n.\/scipy\/optimize\/_trlib\/_trlib.pyx:38\r\n.\/scipy\/optimize\/_group_columns.pyx:11\r\n.\/scipy\/optimize\/_highs\/cython\/src\/_highs_constants.pyx:0\r\n.\/scipy\/optimize\/_highs\/cython\/src\/_highs_wrapper.pyx:35\r\n.\/scipy\/optimize\/_lsq\/givens_elimination.pyx:9\r\n.\/scipy\/optimize\/_bglu_dense.pyx:48\r\n.\/scipy\/optimize\/tnc\/_moduleTNC.pyx:14\r\n.\/scipy\/io\/matlab\/_streams.pyx:45\r\n.\/scipy\/io\/matlab\/_mio_utils.pyx:8\r\n.\/scipy\/io\/matlab\/_mio5_utils.pyx:109\r\n.\/scipy\/_lib\/_ccallback_c.pyx:16\r\n.\/scipy\/_lib\/_test_deprecation_def.pyx:3\r\n.\/scipy\/_lib\/_test_deprecation_call.pyx:2\r\n.\/scipy\/_lib\/messagestream.pyx:11\r\n.\/scipy\/special\/_specfun.pyx:117\r\n.\/scipy\/special\/_cdflib.pyx:494\r\n.\/scipy\/special\/_ellip_harm_2.pyx:39\r\n.\/scipy\/special\/_test_internal.pyx:18\r\n.\/scipy\/special\/_comb.pyx:4\r\n.\/scipy\/fftpack\/convolve.pyx:8\r\n.\/scipy\/interpolate\/interpnd.pyx:75\r\n.\/scipy\/interpolate\/_bspl.pyx:30\r\n.\/scipy\/interpolate\/_ppoly.pyx:72\r\n.\/scipy\/interpolate\/_rgi_cython.pyx:4\r\n.\/scipy\/sparse\/csgraph\/_shortest_path.pyx:63\r\n.\/scipy\/sparse\/csgraph\/_traversal.pyx:46\r\n.\/scipy\/sparse\/csgraph\/_flow.pyx:47\r\n.\/scipy\/sparse\/csgraph\/_tools.pyx:21\r\n.\/scipy\/sparse\/csgraph\/_matching.pyx:36\r\n.\/scipy\/sparse\/csgraph\/_reordering.pyx:15\r\n.\/scipy\/sparse\/csgraph\/_min_spanning_tree.pyx:4\r\n.\/scipy\/spatial\/_qhull.pyx:241\r\n.\/scipy\/spatial\/_ckdtree.pyx:89\r\n.\/scipy\/spatial\/_voronoi.pyx:7\r\n.\/scipy\/spatial\/_hausdorff.pyx:8\r\n.\/scipy\/spatial\/transform\/_rotation.pyx:166\r\n.\/scipy\/signal\/_max_len_seq_inner.pyx:5\r\n.\/scipy\/signal\/_peak_finding_utils.pyx:9\r\n.\/scipy\/signal\/_upfirdn_apply.pyx:58\r\n.\/scipy\/signal\/_spectral.pyx:5\r\n.\/scipy\/signal\/_sosfilt.pyx:17\r\n.\/scipy\/stats\/_levy_stable\/levyst.pyx:11\r\n.\/scipy\/stats\/_ansari_swilk_statistics.pyx:23\r\n.\/scipy\/stats\/_rcont\/rcont.pyx:11\r\n.\/scipy\/stats\/_stats.pyx:101\r\n.\/scipy\/stats\/_qmc_cy.pyx:50\r\n.\/scipy\/stats\/_biasedurn.pyx:38\r\n.\/scipy\/stats\/_unuran\/unuran_wrapper.pyx:201\r\n.\/scipy\/stats\/_sobol.pyx:47\r\n```\r\n\r\nNote that this significantly __overestimates__ the work to be done, as we also include `cdef` statements inside the function","@ev-br I've got a question for you: I tried to change the argument ``a`` on this line [line](https:\/\/github.com\/scipy\/scipy\/blob\/9c98ee762a95cf46d8f91f1c1a5547701538d286\/scipy\/linalg\/_cythonized_array_utils.pyx#L19) to be const-qualified. However I get the following warnings:\r\n\r\n```\r\nscipy\/linalg\/_cythonized_array_utils.cpython-311-darwin.so.p\/_cythonized_array_utils.c:21402:79: warning: passing 'const float *' to parameter of type '__pyx_t_5scipy_6linalg_13cython_lapack_s *' (aka 'float *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    __pyx_f_5scipy_6linalg_13cython_lapack_sgetrf((&__pyx_v_n), (&__pyx_v_n), (&(*((float const  *) ( \/* dim=1 *\/ ((char *) (((float const  *) ( \/* dim=0 *\/ (__pyx_v_a.data + __pyx_t_3 * __pyx_v_a.strides[0]) )) + __pyx_t_4)) )))), (&__pyx_v_n), (&(__pyx_v_piv[0])), (&__pyx_v_info));\r\n                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nscipy\/linalg\/_cythonized_array_utils.cpython-311-darwin.so.p\/_cythonized_array_utils.c:21855:79: warning: passing 'const double *' to parameter of type '__pyx_t_5scipy_6linalg_13cython_lapack_d *' (aka 'double *') discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    __pyx_f_5scipy_6linalg_13cython_lapack_dgetrf((&__pyx_v_n), (&__pyx_v_n), (&(*((double const  *) ( \/* dim=1 *\/ ((char *) (((double const  *) ( \/* dim=0 *\/ (__pyx_v_a.data + __pyx_t_3 * __pyx_v_a.strides[0]) )) + __pyx_t_4)) )))), (&__pyx_v_n), (&(__pyx_v_piv[0])), (&__pyx_v_info));\r\n                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nscipy\/linalg\/_cythonized_array_utils.cpython-311-darwin.so.p\/_cythonized_array_utils.c:22310:79: warning: passing 'const __pyx_t_float_complex *' to parameter of type '__pyx_t_float_complex *' discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    __pyx_f_5scipy_6linalg_13cython_lapack_cgetrf((&__pyx_v_n), (&__pyx_v_n), (&(*((__pyx_t_float_complex const  *) ( \/* dim=1 *\/ ((char *) (((__pyx_t_float_complex const  *) ( \/* dim=0 *\/ (__pyx_v_a.data + __pyx_t_3 * __pyx_v_a.strides[0]) )) + __pyx_t_4)) )))), (&__pyx_v_n), (&(__pyx_v_piv[0])), (&__pyx_v_info));\r\n                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nscipy\/linalg\/_cythonized_array_utils.cpython-311-darwin.so.p\/_cythonized_array_utils.c:22769:79: warning: passing 'const __pyx_t_double_complex *' to parameter of type '__pyx_t_double_complex *' discards qualifiers [-Wincompatible-pointer-types-discards-qualifiers]\r\n    __pyx_f_5scipy_6linalg_13cython_lapack_zgetrf((&__pyx_v_n), (&__pyx_v_n), (&(*((__pyx_t_double_complex const  *) ( \/* dim=1 *\/ ((char *) (((__pyx_t_double_complex const  *) ( \/* dim=0 *\/ (__pyx_v_a.data + __pyx_t_3 * __pyx_v_a.strides[0]) )) + __pyx_t_4)) )))), (&__pyx_v_n), (&(__pyx_v_piv[0])), (&__pyx_v_info));\r\n                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n4 warnings generated.\r\n```\r\n\r\nI understand what the warnings mean. My question is whether we should also ``const`` qualify the relevant functions in the call to lapack?","Ouch. I don't know whether\r\n- cython does sensible things in `if lapack_t is float: ...` selections below https:\/\/github.com\/scipy\/scipy\/blob\/9c98ee762a95cf46d8f91f1c1a5547701538d286\/scipy\/linalg\/_cythonized_array_utils.pyx#L29\r\n-  lapack bindings are prepared to handle `const float*` arguments. there is no notion of const in F77.","Maybe @ilayn has an opinion here?","Probably you will need the `const` version of that fused type if you want to pass `const lapack_t`. This whole templating business we have is just a neverending chore in my opinion (say, if compared to Rust generics)\r\n\r\nSo I am tempted to rewrite them spelled out four times for these helper functions rather than dealing with these fake-C constructs of Cython.","Actually since @rgommers triggered me to write more C code instead of Cython in the `scipy.special` rewrites, I think it is better for us to have more C code than Cython for array manipulation funcs. We really don't benefit from Cython which is best for mostly quick bursts of typed arithmetic and for loops but instead we are extensively using arrays and low-level C access. That's both not useful for us and unfair to Cython. \r\n\r\nI still don't have a full grasp of NumPy C API but I think I got the gist of it lately. So feel free to modify things as you see fit. I will circle back to these at some point anyways.","Public Cython API we expose (`cython_blas`, `cython_lapack`, `cython_special`) is the exception to what is actionable in this issue. We tried adding `const` to signatures before, and found it was a backwards compatibility break. Cython considers any change in signature breaking, even if adding `const` in the equivalent C code would be perfectly compatible. So I suggest not touching public Cython API.","> We really don't benefit from Cython which is best for mostly quick bursts of typed arithmetic and for loops but instead we are extensively using arrays and low-level C access. That's both not useful for us and unfair to Cython\r\n\r\nWhile this starts to border OT for this issue, I'm going to go on record that I disagree with the statement above.\r\nWe definitely do benefit from both Cython and f2py, with all their shortcomings.\r\nThe value we get from boundschecking, from abstracting away pointer arithmetics and refcounting is huge.\r\nNote that scipy.special is indeed special: in there you basically have scalar kernels + ugly magic of generating the ufuncs. I cannot believe anybody would seriously suggest switching back to generating the python glue manually.\r\n\r\nSo realistically, there are IMO two things to sort out if we want to gradually have more C++ than Cython:\r\n\r\n- what's the best way to generate the python glue. FWIW, thin cython wrappers are easy to write, easy to read, not error-prone, and well understood. What are the shortcomings?\r\n- what's the best way to handle arrays in C++. Not via pointer arithmerics. No, definitely not via NpyIter :-).\r\n\r\nIf we don't sort these out, we're going to end up in a mess which is way worse than the status quo.\r\nDo I have my answers? Sort of. In-progress. In https:\/\/github.com\/scipy\/scipy\/pull\/19753.","> The value we get from boundschecking, from abstracting away pointer arithmetics and refcounting is huge.\r\n\r\nAh, I don't mean to use less Cython. I want to use it where it matters. Let me clarify. I mean there is no significant speed benefit to do array operations in Cython. NumPy is already pretty performant, for many array operations. In fact often, you don't even need to type the numpy arrays. They are already fast enough. \r\n\r\nHowever if you want to do certain things involving for loops and other ops there is crazy amount of performance that Cython offers. What I meant is that if we really want to get Cython benefits we probably should use it where it matters and not just for minute performance improvements. The function @Kai-Striega mentioned is a good example in my opinion. I wrote it when I did not have any other choice. However Cython's fused type handling is really not so robust and takes quite a bit of Python machinery around it to provide the right argument at the right contiguousness etc. It is getting better but quite not there yet in my opinion.\r\n\r\nI think we are on the same page with your comments anyways. Regarding the two bullet points you mentioned, \r\n\r\n - Cython is injecting too much code in between and binary size increase. They are really great for C-function binding just do `cimport ... extern ...` and we are good to go. So in that sense we should use more of it. Definitely much easier than `pybind11` or `nanobind`. \r\n - If we have to, I would side with using C and not C++. Then it is pretty mundane old-school flat memory with `A[ row*n + col ] = 5.` silliness with all unsafe stuff about C but at least very robust infrastructure\/compiler support and backwards compatible language not changing incredibly every 3-5 years. \r\n\r\n\r\n\r\n\r\n\r\n","> I mean there is no significant speed benefit to do array operations in Cython. NumPy is already pretty performant, for many array operations.\r\n\r\nYes, until you need a for loop. We all know it, and you write exactly this below. So no preaching to the choir. My point is that needing a for loop is not a fringe rarity, and it should not be made prohibitively difficult.\r\n\r\nNow, for cython producing too fat binaries, that's actually actionable (or at least researcheable). Cool!\r\n- what adds the most fat, what can an author of a cython module do to slim it down?\r\n- cython naturally encourages to mix pure python and typed C-like constructs. A pure guesswork is that it's this pure python which adds to the module size, is this correct?\r\n\r\nFlat memory: yes, as long as I can switch the boundschecking on :-). \r\n\r\nFor C\/C++ : oh beware. THere will be six opinions for every five devs :-). All the more important to formulate guidelines, on a common denominator, however informal. \r\n\r\nWe should probably take this discussion to a separate issue anyway.","Bringing the discussion back to this issue: I think I'm going to leave these functions till last, or at least until we have reached a consensus regarding our use of Cython\/C\/C++.","> what adds the most fat, what can an author of a cython module do to slim it down?\r\n\r\nSee:\r\n* https:\/\/cython.readthedocs.io\/en\/latest\/src\/userguide\/faq.html#how-do-i-reduce-the-size-of-the-binary-modules\r\n* https:\/\/github.com\/cython\/cython\/issues\/6045\r\n* https:\/\/github.com\/cython\/cython\/issues\/4425#issuecomment-1966330797\r\n* https:\/\/github.com\/cython\/cython\/issues\/6051"],"labels":["task","maintenance","Cython"]},{"title":"BUG:linalg:Fix cossin for nonsymmetric cases","body":"\u2026omposition correctly. If I run it for a 9x9 matrix with p, q as 3, 6 and 6,3 it causes errors.\r\n\r\n<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: If you apply cossin decomp with a 9x9 matrix and choose uneven parameters 3, 6 or 6,3 then it can give incorrect results (certain matrixes are just 0).-->\r\n\r\n#### What does this implement\/fix?\r\n<!--I fixed the implementation of where the Identities are implemented in the decomposed matrices-->\r\n\r\n#### Additional information\r\n<!--This issue only happens for uneven choice of p and q parameters.-->\r\n","comments":["Hi @Ritvik1sharma thank you for taking the time to fix an issue. Could you please elaborate what the issue was and what the solution fixes? \r\n\r\nFor instance, there is still #19365 open that I just realized I forgot to come back to. Is it possible to replicate the story for this issue too? \r\n\r\nIf you have a test case that was failing and now does not, we should add it to our test suite."],"labels":["defect","scipy.linalg"]},{"title":"ENH: sparse: first pass at `array_api` compat","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nTowards #18915\r\n\r\n#### What does this implement\/fix?\r\nThis begins the works towards an `array_api` compliant implementation.  I've take some first steps here just to see what works and what doesn't and document how to do this.  Of note is that most of the current errors arise from either indexing problems or lack of 1d support.   See also #20128\r\n\r\n#### Additional information\r\nIn terms of current failing tests on account of missing names\/functions (i.e., not 1D issues), we have a sampling here, although this is probably the majority:\r\n\r\n<details>\r\n\r\n```\r\nFAILED array_api_tests\/test_data_type_functions.py::test_isdtype - AssertionError: isdtype is not defined in scipy.array_api\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-cross] - AssertionError: scipy.array_api is missing the linalg extension function cross()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-diagonal] - AssertionError: scipy.array_api is missing the linalg extension function diagonal()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-matmul] - AssertionError: scipy.array_api is missing the linalg extension function matmul()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-matrix_norm] - AssertionError: scipy.array_api is missing the linalg extension function matrix_norm()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-matrix_power] - AssertionError: scipy.array_api is missing the linalg extension function matrix_power()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-matrix_rank] - AssertionError: scipy.array_api is missing the linalg extension function matrix_rank()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-matrix_transpose] - AssertionError: scipy.array_api is missing the linalg extension function matrix_transpose()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-outer] - AssertionError: scipy.array_api is missing the linalg extension function outer()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-slogdet] - AssertionError: scipy.array_api is missing the linalg extension function slogdet()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-tensordot] - AssertionError: scipy.array_api is missing the linalg extension function tensordot()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-trace] - AssertionError: scipy.array_api is missing the linalg extension function trace()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-vecdot] - AssertionError: scipy.array_api is missing the linalg extension function vecdot()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linalg-vector_norm] - AssertionError: scipy.array_api is missing the linalg extension function vector_norm()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[statistical-prod] - AssertionError: scipy.array_api is missing the statistical function prod()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[statistical-std] - AssertionError: scipy.array_api is missing the statistical function std()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[statistical-var] - AssertionError: scipy.array_api is missing the statistical function var()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[set-unique_all] - AssertionError: scipy.array_api is missing the set function unique_all()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[set-unique_counts] - AssertionError: scipy.array_api is missing the set function unique_counts()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[set-unique_inverse] - AssertionError: scipy.array_api is missing the set function unique_inverse()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[set-unique_values] - AssertionError: scipy.array_api is missing the set function unique_values()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[searching-nonzero] - AssertionError: scipy.array_api is missing the searching function nonzero()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[searching-where] - AssertionError: scipy.array_api is missing the searching function where()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-arange] - AssertionError: scipy.array_api is missing the creation function arange()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-from_dlpack] - AssertionError: scipy.array_api is missing the creation function from_dlpack()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-linspace] - AssertionError: scipy.array_api is missing the creation function linspace()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-meshgrid] - AssertionError: scipy.array_api is missing the creation function meshgrid()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-tril] - AssertionError: scipy.array_api is missing the creation function tril()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[creation-triu] - AssertionError: scipy.array_api is missing the creation function triu()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[utility-any] - AssertionError: scipy.array_api is missing the utility function any()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-broadcast_arrays] - AssertionError: scipy.array_api is missing the manipulation function broadcast_arrays()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-broadcast_to] - AssertionError: scipy.array_api is missing the manipulation function broadcast_to()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-concat] - AssertionError: scipy.array_api is missing the manipulation function concat()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-expand_dims] - AssertionError: scipy.array_api is missing the manipulation function expand_dims()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-flip] - AssertionError: scipy.array_api is missing the manipulation function flip()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-permute_dims] - AssertionError: scipy.array_api is missing the manipulation function permute_dims()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-roll] - AssertionError: scipy.array_api is missing the manipulation function roll()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-squeeze] - AssertionError: scipy.array_api is missing the manipulation function squeeze()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[manipulation-stack] - AssertionError: scipy.array_api is missing the manipulation function stack()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[sorting-argsort] - AssertionError: scipy.array_api is missing the sorting function argsort()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[sorting-sort] - AssertionError: scipy.array_api is missing the sorting function sort()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[data_type-astype] - AssertionError: scipy.array_api is missing the data_type function astype()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[data_type-isdtype] - AssertionError: scipy.array_api is missing the data_type function isdtype()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_and] - AssertionError: scipy.array_api is missing the elementwise function bitwise_and()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_left_shift] - AssertionError: scipy.array_api is missing the elementwise function bitwise_left_shift()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_invert] - AssertionError: scipy.array_api is missing the elementwise function bitwise_invert()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_or] - AssertionError: scipy.array_api is missing the elementwise function bitwise_or()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_right_shift] - AssertionError: scipy.array_api is missing the elementwise function bitwise_right_shift()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-bitwise_xor] - AssertionError: scipy.array_api is missing the elementwise function bitwise_xor()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-floor_divide] - AssertionError: scipy.array_api is missing the elementwise function floor_divide()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-greater] - AssertionError: scipy.array_api is missing the elementwise function greater()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-greater_equal] - AssertionError: scipy.array_api is missing the elementwise function greater_equal()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-less] - AssertionError: scipy.array_api is missing the elementwise function less()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-less_equal] - AssertionError: scipy.array_api is missing the elementwise function less_equal()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-logical_and] - AssertionError: scipy.array_api is missing the elementwise function logical_and()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-logical_not] - AssertionError: scipy.array_api is missing the elementwise function logical_not()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-logical_or] - AssertionError: scipy.array_api is missing the elementwise function logical_or()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-logical_xor] - AssertionError: scipy.array_api is missing the elementwise function logical_xor()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-multiply] - AssertionError: scipy.array_api is missing the elementwise function multiply()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-not_equal] - AssertionError: scipy.array_api is missing the elementwise function not_equal()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[elementwise-subtract] - AssertionError: scipy.array_api is missing the elementwise function subtract()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linear_algebra-matmul] - AssertionError: scipy.array_api is missing the linear_algebra function matmul()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linear_algebra-matrix_transpose] - AssertionError: scipy.array_api is missing the linear_algebra function matrix_transpose()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linear_algebra-tensordot] - AssertionError: scipy.array_api is missing the linear_algebra function tensordot()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[linear_algebra-vecdot] - AssertionError: scipy.array_api is missing the linear_algebra function vecdot()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__and__] - AssertionError: The scipy.array_api array object is missing the method __and__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__complex__] - AssertionError: The scipy.array_api array object is missing the method __complex__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__dlpack__] - AssertionError: The scipy.array_api array object is missing the method __dlpack__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__dlpack_device__] - AssertionError: The scipy.array_api array object is missing the method __dlpack_device__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__float__] - AssertionError: The scipy.array_api array object is missing the method __float__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__floordiv__] - AssertionError: The scipy.array_api array object is missing the method __floordiv__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__index__] - AssertionError: The scipy.array_api array object is missing the method __index__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__int__] - AssertionError: The scipy.array_api array object is missing the method __int__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__invert__] - AssertionError: The scipy.array_api array object is missing the method __invert__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__lshift__] - AssertionError: The scipy.array_api array object is missing the method __lshift__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__mod__] - AssertionError: The scipy.array_api array object is missing the method __mod__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__or__] - AssertionError: The scipy.array_api array object is missing the method __or__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__pos__] - AssertionError: The scipy.array_api array object is missing the method __pos__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__rshift__] - AssertionError: The scipy.array_api array object is missing the method __rshift__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-__xor__] - AssertionError: The scipy.array_api array object is missing the method __xor__()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_method-to_device] - AssertionError: The scipy.array_api array object is missing the method to_device()\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_attribute-device] - AssertionError: The scipy.array_api array object is missing the attribute device\r\nFAILED array_api_tests\/test_has_names.py::test_has_names[array_attribute-mT] - AssertionError: The scipy.array_api array object is missing the attribute mT\r\n```\r\n<\/details>\r\n\r\nAt the end of the day, I'm not sure if this is worth doing or not.  I will remember to come to the working group to discuss in two weeks!  That being said, a lot of the missing functions probably could be implemented and this might serve as a nice roadmap for new features that would make QOL better for many.  For example, one of the blockers for dask integration I found was the `keepdims` arg being passed through to `csr_matrix.sum` but if we could rely on the array api implementation here, we could probably ignore it or handle it better with the advent of 1D arrays.","comments":["I will continue working on this at some point but just wanted to post here since @ivirshup wanted to see as well.  There are also a few blockers like 1D and #20128.  Sorry for the spam, didn't know it would auto-request a review!","A first \"fix\" would probably be moving the folder under `sparse`?","> A first \"fix\" would probably be moving the folder under sparse?\r\n\r\nIndeed \ud83d\udc4d. SciPy is largely an array-consumer library, so having this at the top-level would be very confusing. Keeping this as close to the array-producer part of `sparse` as possible would be good.","@willow-ahrens, I believe you'd mention that you were working on getting [`finch-tensor`](https:\/\/github.com\/willow-ahrens\/finch-tensor) passing the array-api test suite as well. Is that work publicly available anywhere? It would be nice to see what you've done and compare what tests are being skipped.","We've been building it array api-compliant, but haven't begun testing with the Array API suite. I'll try to remember to update when we get to the test suite. ","Definitely interested @willow-ahrens - I left out operations that don't already have standard implementations.  It seems like certain operations like `logical_or` (one such op) have simple implementations but it seems out of scope to do that here.  I think its output could definitely still be \"sparse enough\" to warrant being a sparse operation (after all, you probably shouldn't be using sparse unless your data is _truly_ sparse and in that sense an operation on two sparse things should just give sparse as it does with addition).","@lucascolley tried to add CI but I don't think it'll run without approval?  Not sure it's worth really delving into this ATM unless we really think it'll help move things along, especially given the status of 1D arrays.  I also definitely don't think non-trivial implementations should go here since array-api-tests do not test for correctness (and even for things like `shape` and `dtype` correctness as properties of the output, they are often marked for failure).","sounds good. As I said, no rush on the CI check. FYI, we are able to skip certain CI workflows with tags in commit messages, see http:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping. That might be useful here, e.g. if you don't need to build the docs and would only like GitHub Actions to run.","maybe a next step tomorrow, thanks @lucascolley :)","First up: https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.where.html#where \r\n\r\nThis appears to be necessary for solving at least part of https:\/\/github.com\/dask\/dask\/issues\/10980#issuecomment-1985184881\r\n\r\nInterestingly, the array api version does not actually cover all the use-cases I'd be interested in (or rather the ones where numpy currently has coverage):\r\n\r\n```python\r\nfrom numpy import array_api as npa\r\nimport numpy as np\r\nX = np.random.randn(20, 20)\r\nnpa.where(X>0, 10, X) # errors out because 10 is not an array as specified\r\n```","> where(X>0, asarray(10), X)\r\n\r\nOh very cool!!!","@rgommers I changed the module structure but `_array_api` is still importable.   However, if I remove it from the `sparse` `meson.build`, I can no longer import from it within the package.  But maybe just the underscore is what you were going for with private (would seem so since `from scipy.sparse import tests` works for me)","Yes indeed, just adding the underscore is what I meant."],"labels":["enhancement","scipy.sparse","array types"]},{"title":"ENH: sparse.csgraph: Yen K-shortest paths","body":"Closes [#18080](https:\/\/github.com\/scipy\/scipy\/issues\/18080)\r\n\r\nImplements [Yen's](https:\/\/en.wikipedia.org\/wiki\/Yen%27s_algorithm) algorithm for K-shortest paths between two nodes in a graph.\r\n\r\n#### Additional information\r\n* Search iteratively for K shortest paths between the source node and the sink node.\r\n* Utilizes existing Dijkstra and Johnson's algorithms for single paths searches.\r\n* Correctness was tests against networkx [shortest_simple_paths](https:\/\/networkx.org\/documentation\/stable\/reference\/algorithms\/generated\/networkx.algorithms.simple_paths.shortest_simple_paths.html#networkx.algorithms.simple_paths.shortest_simple_paths) using a [script](https:\/\/github.com\/tsery-ns\/scipy\/blob\/sery\/compare_yen\/scipy\/sparse\/csgraph\/yen_compare_scipy_vs_nx.py) checking all the combinations of the features described below.\r\n* The main computation loop is compiled to C fully (no yellow lines in annotation).\r\n\r\nSupports the following features:\r\n1. Directed and Undirected graphs.\r\n2. Weighted and Unweighted graph.\r\n3. Graphs with negative weights (applying Johnson's algorithm as pre-process and post-process).\r\n\r\n\r\n","comments":["Thanks @tsery-ns, would you be able to make a post to the mailing list about this to gauge interest before we start a review","I should be able to review this, but I'm time poor at the moment. @tsery-ns could you please ping me if I haven't responded in a week?","Thank you @Kai-Striega and @j-bowhay for taking the time to review the PR.\r\nI believe I addressed all your comments, let me know if I missed anything.\r\n\r\nI was wondering whether I should add an `.. versionadded:: ` label in the docstring? If so, let me know which version to specify.","A quick question: in the Wikipedia [improvements](https:\/\/en.wikipedia.org\/wiki\/Yen%27s_algorithm#Improvements) section, states the following:\r\n\r\n> Yen's algorithm can be improved by using a heap to store B, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity\r\n\r\nBut I see no use of a heap in this algorithm. Have you considered trying the heap implementation?  We already have a heap implementation in csgraph, so it might be work looking into it. This might be a nice to have, but it's not expected in this PR. \r\n\r\n","> I was wondering whether I should add an .. versionadded:: label in the docstring? If so, let me know which version to specify.\r\n\r\nYes. We should have a ``.. versionaddedd::`` directive, please add it into the notes section. The current dev version is 1.13.","> A quick question: in the Wikipedia [improvements](https:\/\/en.wikipedia.org\/wiki\/Yen%27s_algorithm#Improvements) section, states the following:\r\n> \r\n> > Yen's algorithm can be improved by using a heap to store B, the set of potential k-shortest paths. Using a heap instead of a list will improve the performance of the algorithm, but not the complexity\r\n> \r\n> But I see no use of a heap in this algorithm. Have you considered trying the heap implementation? We already have a heap implementation in csgraph, so it might be work looking into it. This might be a nice to have, but it's not expected in this PR.\r\n\r\nI have considered that. However, the heap is implemented specifically for Dijkstra's algorithm. Refactoring the heap to support such operations is out of scope of this PR. Also, I don't think this improvement will have a significant affect as it saves a `O(K)` complexity in a `O(M+N logN)` operation (see [complexity](https:\/\/en.wikipedia.org\/wiki\/Yen%27s_algorithm#Features)).\r\n\r\nFurthermore, there are better improvements to Yen's algorithm (e.g., Lawler's modification) which I chose not to implement as I felt it's already quite a big PR as it is.","Sounds good. I'm not an expert on the algorithm, but the scope of the PR is good.\n\nWell done and thanks for taking the time to contribute!","New feature\/large diff--should we wait until after branching for SciPy `1.13.0` in a few days perhaps? I'll bump the milestone, but if folks feel strongly this is ready for a merge short-term just switch the milestone back.","Ping to reviewers and maintainers.\r\nAre you considering merging this PR anytime soon?","Sorry @tsery-ns. Yes, I think this can be merged soon. However I've been really busy with work so haven't had the time to review it.  If this PR doesn't move by next week, feel free to ping me personally and I'll give it a final review "],"labels":["enhancement","scipy.sparse","scipy.sparse.csgraph","Cython"]},{"title":"ENH: Add mask option to scipy.ndimage.generic_filter","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI need to apply a filter to an image, but only using its intensity values under a certain boolean mask ( front pixels of a binary image)\r\nIn my case it is the generic filter defined by numpy.std and a size 3 footprint\r\n\r\n### Describe the solution you'd like.\r\n\r\nA solution would be to add the optional \"mask:np.ndarray = None\" to the function's signature and to compute the trace of the mask in the footprint before passing  this trace as the where or mask argument of  \"function\" or raise an Error if \"function\" does not handle masks. \r\n\r\nBest,\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":[],"labels":["enhancement","scipy.ndimage"]},{"title":"BUG: CSR\/CSC Division Yields COO","body":"### Describe your issue.\r\n\r\nI would expect dividing a CSR or CSC matrix by an `ndarray` of the same size would yield a `CSR\/CSC` matrix since the operation should just scale `data` but instead we get `COO`.  I see there's [an in place row scale](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.sparsefuncs.inplace_row_scale.html) that should be similar except it doesn't need to broadcast.  But the problem then becomes one of just indexing the divisor, and then calling this?\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom scipy import sparse as sp\r\nimport numpy as np\r\n\r\ndividend = sp.random(1_000, 10_000, density=0.01, random_state=0, format=\"csr\")\r\ndivisor = np.ones((1_000, 10_000))\r\n\r\ndividend \/ divisor # coo\r\nsp.csr_array(dividend) \/ divisor # coo again\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nN\/A\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.3 sys.version_info(major=3, minor=11, micro=6, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-ewarq4oq\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-73jfxkej\/cp311-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["cc: @ivirshup","For example, could we do:\r\n\r\n```python\r\ndividend.data \/ divisor[dividend.nonzero()]\r\n```\r\n\r\nTo get the new `data`?","I would add:\r\n\r\n* `CSR \/ scalar -> CSR`, but `CSR \/ vector -> COO`\r\n* the  behaviour occurs for broadcast multiply, so I assume is using a common code path.\r\n\r\n```python\r\nfrom scipy import sparse\r\nimport numpy as np\r\n\r\ncsr = sparse.random_array((100, 100), format=\"csr\", density=0.01)\r\ntype(csr \/ 2)\r\n# scipy.sparse._csr.csr_array\r\ntype(csr \/ np.full(100, 2))\r\n# scipy.sparse._coo.coo_array\r\n\r\ntype(csr * 2)\r\n# scipy.sparse._csr.csr_array\r\ntype(csr * np.full(100, 2))\r\n# scipy.sparse._coo.coo_array\r\n```","The code path for the `CSR \/ vector` case does this (given inputs `csr` and `vector`):\r\n\r\n```python\r\nrecip = np.true_divide(1., vector)\r\nreturn csr.multiply(recip)\r\n```\r\n\r\nThen, in the handling of `multiply`, we do:\r\n\r\n```python\r\ncoo = csr.tocoo()\r\ncoo.data = np.multiply(coo.data, recip[coo.row, coo.col])\r\nreturn coo\r\n```\r\n\r\nThis strips away a bunch of branches for shape-checking and other stuff, but that's the core of the operation.","I wouldn't necessarily call this a correctness bug, because we don't generally guarantee that sparse formats will be preserved, but I can see a case for it being a performance bug in some situations.\r\n\r\nNote: using `csr.nonzero()` to index into the dense argument would be slower, because it's implemented using `tocoo()` under the hood.","I would also say more performance bug, and probably unexpected behaviours\r\n\r\n> because we don't generally guarantee that sparse formats will be preserved\r\n\r\nBased on that, would changing this be an acceptable change in behavior?\r\n\r\n----\r\n\r\nFor reference, here is how scikit-learn implements the similar `inplace_row_scale` functions:\r\n\r\n* [`inplace_csr_column_scale`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/5c4aa5d0d90ba66247d675d4c3fc2fdfba3c39ff\/sklearn\/utils\/sparsefuncs.py#L42) is`X.data *= scale.take(X.indices, mode=\"clip\")`\r\n* [`inplace_csr_row_scale`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/5c4aa5d0d90ba66247d675d4c3fc2fdfba3c39ff\/sklearn\/utils\/sparsefuncs.py#L83) is `X.data *= np.repeat(scale, np.diff(X.indptr))`\r\n\r\nSo pure python, though with allocations we could skip in compiled code. But can look like:\r\n\r\n```python\r\nfrom scipy import sparse\r\nimport numpy as np\r\nfrom operator import mul, truediv\r\n\r\ndef broadcast_csr_by_vec(X, vec, op, axis):\r\n    if axis == 0:\r\n        new_data = op(X.data, np.repeat(vec, np.diff(X.indptr)))\r\n    elif axis == 1:\r\n        new_data = op(X.data, vec.take(X.indices, mode=\"clip\"))\r\n    return X._with_data(new_data)\r\n```\r\n\r\nWith some promise for performance:\r\n\r\n```python\r\nIn [28]: X = sparse.random_array((10_000, 1_000), format=\"csr\", density=0.1)\r\n    ...: y = np.random.randn(10_000)\r\n\r\nIn [29]: broadcast_csr_by_vec(X, y, mul, axis=0) != (X * y[:, None])\r\nOut[29]: \r\n<10000x1000 sparse array of type '<class 'numpy.bool_'>'\r\n\twith 0 stored elements in Compressed Sparse Row format>\r\n\r\nIn [30]: %timeit broadcast_csr_by_vec(X, y, mul, axis=0)\r\n4.89 ms \u00b1 453 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [31]: %timeit (X * y[:, None])\r\n18.9 ms \u00b1 974 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n","Added bonus of the approach above: since we're not \"faking\" division the results are actually equivalent to numpy (which they currently aren't):\r\n\r\n```python\r\ndense_op = X.toarray() \/ y[:, None]\r\nsparse_op = (X \/ y[:, None]).toarray()\r\nnew_op = broadcast_csr_by_vec(X, y, truediv, axis=0).toarray()\r\n\r\nnp.testing.assert_array_equal(dense_op, new_op)\r\nassert not np.array_equal(dense_op, sparse_op)\r\n```","I think a PR to fix this may be a little bigger than expected due to the number of cases that currently return COO, though don't have to. For example, currently $CSR_{m,n} * Dense_{m, n} \\rightarrow COO_{m,n}$. This could (with fewer allocations) result in `CSR`"],"labels":["defect","scipy.sparse"]},{"title":"BUG: optimize: Fix constraint condition in inner loop of nnls","body":"This pull request fixes a bug in `_nnls` caused by a wrong check for constraint violation.\r\nThe check made was `s[P].min() <= tol`, which according to the paper this function is based on\r\n(https:\/\/analyticalsciencejournals.onlinelibrary.wiley.com\/doi\/abs\/10.1002\/%28sici%291099-128x%28199709\/10%2911%3A5%3C393%3A%3Aaid-cem483%3E3.0.co%3B2-l) should have been `s[P].min() <= 0`. \r\nThis behaviour could cause non-converging iterations for definitely well-posed problems. \r\nI have `s[P].min() <= 0`. \r\n","comments":["Hi @ezander do you have any examples where this actually causes any errors? Because hitting actual zeros are never feasible in floating point arithmetic hence we have to assume things zero if they are small enough. That's why a tol is used. If the inequality was strict indeed we would have used zero instead of tol.","> Hi @ezander do you have any examples where this actually causes any errors? \r\nYes, I do. I used nnls for fitting some piecewise linear function to some data using monotonicity constraints. And in very few of the cases the algorithm did not converge (or rather threw some RuntimeError), which I could trace back finally to the inner loop of `_nnls`. I could put that into a unit test, however, it is a bit messy and I was unable to extract some nice small MWE from it. \r\n\r\n> Because hitting actual zeros are never feasible in floating point arithmetic hence we have to assume things zero if they are small enough. That's why a tol is used. If the inequality was strict indeed we would have used zero instead of tol.\r\nSure, but we're not testing for equality to zero here. If it was `s[P].min()==0` and you would turn that into `-tol<s[P].min()<tol`, I'd be totally fine, but here the check is `s[P].min()<0`, which checks for an interval $(-\\infty,0)$ anyway, so there is no need for additional tolerance IMHO. \r\n\r\nAnd, BTW, I think the inequality should be strict, because it's purpose is to check for a constraint violation, which is only given if some `s` is negative, not when it's just zero.\r\n","Messy or not if we can get some testing for this function that would be actually great. And if this solves these edge cases that also very nice. The authors also mention \r\n\r\n>  In practice the NNLS algorithm seldom enters the inner loop C.\r\n\r\nbut apparently you managed to find those examples. The runtime warnings might be coming from the `solve` operations and indicates the illposedness of the problem anyways so if you can confirm that, that would be great too. \r\n\r\n\r\nThe strictness of the inequality is something I wondered while I was coding and in fact later they mention in the text\r\n\r\n> When a new variable has been included in the passive set P, there is a chance that in the unconstrained solution to the new least squares problem (step B4) some of the regression coefficients of the free variables will turn negative (step C1). \r\n\r\nSo they are indeed looking for negativity however having zero is the indicator that the constraint is still active hence it has to be nonstrict. In fact that's why all start by zeroing the set. Hence there is some confusion there algorithmically. \r\n","Also did you notice any performance benefits from this? If you have SciPy 1.11 lying around and if you have time for even a feeling based benchmark it would help us to judge whether there is any regression or not.","Okay. I'll try to set up a test case as unmessy as possible. But I can't do that before Sunday. \r\n\r\nSo far, I haven't seen a performance benefit. In my cases either the inner loop ran only once, or did not terminate at all (except of course by the max iterations criterion).","> Okay. I'll try to set up a test case as unmessy as possible. But I can't do that before Sunday.\r\n\r\nNo problem, please take your time. \r\n\r\n> So far, I haven't seen a performance benefit. In my cases either the inner loop ran only once, or did not terminate at all (except of course by the max iterations criterion).\r\n\r\nFor this function, even no news is good news hence that's already good enough sign.","I added a test case for the bug now. If you want to see it, just revert the `< 0` line back to `<= tol` and it should fail the test. "],"labels":["defect","scipy.optimize"]},{"title":"ENH: Adding parallelism to guassian kde logpdf ","body":"This function only uses a single thread. I haven't dived too far into the code, but I suspect you could run each data point in parallel? It's quite an expensive computation. \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/4edfcaa3ce8a387450b6efce968572def71be089\/scipy\/stats\/_kde.py#L618","comments":[],"labels":["scipy.stats","enhancement"]},{"title":"ENH: Sparse splu for non-square matrices using SuperLU","body":"### Is your feature request related to a problem? Please describe.\n\nI tried to LU factorize sparse matrices using scipy.sparse.linalg.splu (uses SuperLU). But it raised an error saying only supported for square sparse matrices.\r\nIt can be seen in official documentations that SuperLU routines do support non-square matrix LU decomposition.\r\nOfficial site line 3 : https:\/\/portal.nersc.gov\/project\/sparse\/superlu\/\r\nUser Manuel page 5 line 4 (section 1.1) : https:\/\/portal.nersc.gov\/project\/sparse\/superlu\/superlu_ug.pdf\n\n### Describe the solution you'd like.\n\nCurrently it is [restricted](https:\/\/github.com\/scipy\/scipy\/blob\/b8158952b43f9f93c7e77a24279d317aca42d07b\/scipy\/sparse\/linalg\/_dsolve\/linsolve.py#L424C1-L425C77) for input matrix to be square. We can remove that and [modify gstrf function call](https:\/\/github.com\/scipy\/scipy\/blob\/b8158952b43f9f93c7e77a24279d317aca42d07b\/scipy\/sparse\/linalg\/_dsolve\/linsolve.py#L438C5-L438C61) like gstrf(M, N, .....) to allow non square matrices.\r\n\r\nFew changes will be required too in [C implementation](https:\/\/github.com\/scipy\/scipy\/blob\/b8158952b43f9f93c7e77a24279d317aca42d07b\/scipy\/sparse\/linalg\/_dsolve\/_superluobject.c#L674) of interface between SuperLU and Python.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":[],"labels":["enhancement","scipy.sparse.linalg"]},{"title":"BUG: Inconsistent results in `signal.firls` between aarch & x86","body":"### Describe your issue.\r\n\r\nThe following snippet shows different results when executed in ARM & X86 for wheels downloaded from PyPi, building SciPy from a conda installation works as expected.\r\n\r\nSome of the very small values are different between both architectures. The small magnitude of the differing results (`e-14`) doesn't really indicate a problem per se in this specific case, but having the conda installation in aarch64 matching exactly x86 results while pypi doesn't may indicate that some build flags regarding FP ops precision are different for PyPi wheels and can cause problems in other routines. As a side note, CuPy results match x86 results 1:1 too.\r\n(Thanks to @leofang for figuring this out by comparing conda & PyPi installs!)\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport scipy as scp\r\n\r\nN = 11  # number of taps in the filter\r\na = 0.1  # width of the transition band\r\n# design a halfband symmetric low-pass filter\r\nh = scp.signal.firls(N, [0, a, 0.5-a, 0.5], [1, 1, 0, 0], fs=1.0)\r\nprint(h)\r\n```\r\n\r\n\r\n### Error message\r\n\r\naarch64 output\r\n```\r\n[ 9.20115457e-03  2.13215371e-14 -5.71460483e-02 -6.05624404e-14\r\n  2.98061198e-01  5.00000000e-01  2.98061198e-01 -6.05624404e-14\r\n -5.71460483e-02  2.13215371e-14  9.20115457e-03]\r\n```\r\n\r\nx86 output:\r\n```\r\n[ 9.20115457e-03  1.81577277e-14 -5.71460483e-02 -5.15707625e-14\r\n  2.98061198e-01  5.00000000e-01  2.98061198e-01 -5.15707625e-14\r\n -5.71460483e-02  1.81577277e-14  9.20115457e-03]\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\naarch64\r\n```\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= MAX_THREADS=80\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= MAX_THREADS=80\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-otvjb90d\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n\r\nx86\r\n```\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-oj5r1eje\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n","comments":["I wonder if this is related to https:\/\/github.com\/scipy\/scipy\/issues\/18533 \nCould you check if switching the lapack driver matters here? "],"labels":["defect","scipy.signal"]},{"title":"BUG: Inconsistent results in `ndimage.` filters between aarch64 & x86","body":"### Describe your issue.\r\n\r\nAfter running the following script in both ARM aarch64 & X86 we find that when using `cval=-1.0` and the dtype is `uint8` the results between ARM & x86 are different due to type casting\/overflow issues in several filters of `ndimage`.\r\n\r\nIn ARM `char` dtype is unsigned by default while in X86 is signed. I am not sure if this is exactly the case because we are dealing with `uint8` but it may be worth to be looked at.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy\r\nimport scipy\r\n\r\n\r\ndef main():\r\n    x = numpy.array(\r\n        [[[5, 7, 6, 5, 4],\r\n        [6, 4, 8, 9, 3],\r\n        [7, 5, 5, 9, 0],\r\n        [0, 0, 8, 7, 8]],\r\n       [[9, 7, 4, 7, 1],\r\n        [6, 1, 9, 5, 4],\r\n        [2, 7, 4, 5, 0],\r\n        [6, 6, 6, 9, 6]],\r\n       [[3, 4, 6, 0, 6],\r\n        [6, 2, 1, 3, 3],\r\n        [5, 4, 9, 1, 2],\r\n        [1, 6, 2, 4, 2]]], dtype=numpy.uint8\r\n    )\r\n    w = numpy.array([5,7,6], dtype=numpy.uint8)\r\n    print(scipy.ndimage.convolve1d(x, w, axis=0, mode=\"constant\", cval=-1.0))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\n### Error message\r\n\r\n\r\naarch64 output:\r\n```\r\n[[[ 74  78  56  64  27]\r\n  [ 66  27  95  82  35]\r\n  [ 53  64  49  82   0]\r\n  [ 24  24  80  88  80]]\r\n```\r\n\r\nx86 output:\r\n```\r\n[[[ 74  78  56  64  27]\r\n  [ 66  27  95  82  35]\r\n  [ 53  64  49  82 250]\r\n  [ 24  24  80  88  80]]\r\n```\r\n\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\naarch64\r\n```\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= MAX_THREADS=80\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= MAX_THREADS=80\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-otvjb90d\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n\r\nx86\r\n```\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-oj5r1eje\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n\r\n","comments":[],"labels":["defect","scipy.ndimage"]},{"title":"DOC: Inconsistent naming convention between docs and error messages within scipy.optimize.curve_fit","body":"### Issue with current documentation:\n\nWhen working with `scipy.optimize.curve_fit` I had a typo in my bounds, which led to the following ValueError being raised:\r\n\r\n```\r\nValueError: `x0` is infeasible.\r\n```\r\n\r\nReferring back to the documentation, the parameter `x0` is not mentioned, with the terminology used throughout the documentation page being `p0` instead: [Documentation](https:\/\/scipy.github.io\/devdocs\/reference\/generated\/scipy.optimize.curve_fit.html#scipy.optimize.curve_fit)\r\n\r\nLooking through the source code, it seems that `p0` is the preferred terminology within the `curve_fit` method, but `x0` is used in many other places in the code. As far as I can see, the message `x0 is infeasible.` only appears here: https:\/\/github.com\/scipy\/scipy\/blob\/9c4f6a91c5eb3e797d8b591e502d746128dc1ce2\/scipy\/optimize\/_lsq\/least_squares.py#L820\r\n\r\n\r\n\r\n\n\n### Idea or request for content:\n\nAssuming my understanding of the code is correct, adding a note to the documentation of `curve_fit` that informs a user that `x0` and `p0` are used interchangeably across the codebase would make the user experience a bit more streamlined.\r\n\r\nThis could be achieved by adding \r\n\r\n> Note that `x0` may be used interchangeably with `p0`\r\n\r\nto line 611 of https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/optimize\/_minpack_py.py\r\n\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Can I ask If I can work on this?\r\nIt seems like a good chance for a contribution for a newcomer like me.","Yes, feel free! I probably wouldn't have time to work on a PR for a few months, so if you want to take it on that's great \ud83d\ude80 "],"labels":["scipy.optimize","Documentation"]},{"title":"BUG: Inconsistent interpolation depending on data type","body":"### Describe your issue.\n\nDepending on the data type, interpolate gives different results.  The example code outputs two arrays which are pasted below. The first array end with a \"1\" while the seconds array, which is float32, has all NaNs. I was expecting each array to have a single NaN and a single numerical value.\n\n### Reproducing Code Example\n\n```python\nimport scipy as sp\r\nimport numpy as np\r\nxi = np.array([0, 6])\r\nyi = np.array([np.nan, 1])\r\nprint(sp.interpolate.interp1d(xi, yi, kind=\"linear\")(xi))\r\nprint(sp.interpolate.interp1d(xi, yi.astype(np.float32), kind=\"linear\")(xi))\n```\n\n\n### Error message\n\n```shell\nActual Output:\r\n[nan  1.]\r\n[nan nan]\r\n\r\nExpected Output\r\n[nan  1.]\r\n[nan  1.]\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.2 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-6ckgqyn6\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\n```\n","comments":["Unfortunately, as per https:\/\/docs.scipy.org\/doc\/scipy\/tutorial\/interpolate\/1D.html#missing-data we do not support missing data. Furthermore if you just want piecewise linear then I would recommend using ` np.interp` instead"],"labels":["scipy.interpolate","query"]},{"title":"ENH: support for \"fully sparse\" matrix operations when setting parts of a sparse matrix - COO proposal with code","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI am building the adjacency matrix of the union of a number of graphs. This implies adding to a set of columns and rows another matrix. And I need to do this with sparse matrices.\r\n\r\nIf I was working with dense matrices, I would just do\r\n\r\n```python3\r\ndef add_submatrix(big_m, small_m, rows, cols):\r\n    extraction = big_m[rows]\r\n    extraction[:, cols] += small_m\r\n    # Not even required, if the above is a view:\r\n    big_m[rows] = extraction\r\n```\r\n\r\nHowever, this to my understanding is unfeasible with sparse matrices in scipy, and the (first) problematic step is ``extraction[:, cols] = small_m``, because while scipy guarantees (e.g. starting from a LIL matrix) that ``extraction`` is never densified, it does densify ``small_m`` (which in my case is enough to raise a ``MemoryError`). This happens here:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/sparse\/_index.py#L290\r\n\r\n### Describe the solution you'd like.\r\n\r\nThere should be at least one format which allows doing the above without ever densifying anything, and in a vectorized way (that is, without explicitly iterating on the non-zero elements).\r\n\r\n### Describe alternatives you've considered.\r\n\r\nI created the required methods for COO (just because it's the simplest to manage). See here:\r\n\r\nhttps:\/\/gist.github.com\/toobaz\/e40ecfcf0e77ba158d8ac60a39faeaf9\r\n\r\nEssentially, the code above translates to:\r\n```python3\r\ndef add_submatrix(big_m, small_m, rows, cols):\r\n    extraction = coo_get_block(big_m, small_m, rows, cols)\r\n    extraction = coo_binary_op(extraction, small_m, op='sum')\r\n    coo_get_block(big_m, extraction, rows, cols)\r\n```\r\n\r\nI would be happy to create a PR once I have a generic acknowledgment that I didn't miss anything obvious and there is general interest in adding this feature. Clearly any feedback is welcome, e.g. I am sure the [catalog of possible operations I made](https:\/\/gist.github.com\/toobaz\/e40ecfcf0e77ba158d8ac60a39faeaf9#file-gistfile1-py-L94) sucks, and there's probably something similar already in numpy or scipy.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nNotice that all operations have linear cost in the number of elements _except_ the sorting that happens inside ``coo_binary_op``, via ``_alt_lexsort``. That could be linear too, because it sorts two arrays which are each already sorted, but my Python version is less efficient than my O(n*log(n)) vectorized version for any reasonable size.\r\n\r\nSee the following graph:\r\n![download](https:\/\/github.com\/scipy\/scipy\/assets\/1224492\/1234b2ab-b270-4dfc-acca-66b80000e569)\r\n\r\nIt compares several sorting functions on (the same) arrays of random positive integers, but all versions that end with `_h` work on an array where all items are sorted up to some point, and starting from that point (essentially, what is missing is the last step of a mergesort between different length arrays). The linear Python version is ``sort_py``, what I can get with ``np.lexsort`` is ``sort_lex_h``, and what I get with my alternative version is ``fake_lex_h``.\r\n\r\nNote that these sorting functions look at lexicographic ordering, just like ``np.lexsort``. The plot above also displays many cases where a simple array is been sorted, just to show that different sorting algorithms behave very differently when facing the case described above of an array composed by two sorted arrays - and ``stable`` beats them all, which is why I use it in my code.","comments":["By the way: ``_coo_base._sum_duplicates()`` relies on ``np.lexsort``:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/sparse\/_coo.py#L498\r\n\r\n... and could also use my version instead (at least after checking there are no overflow problems), with a gain in performance which is modest on random arrays, but very significant on partly ordered arrays, which are likely a common occurrence in handling COO matrices."],"labels":["enhancement","scipy.sparse"]},{"title":"BUG: incorrect nearest neighbor search using workers=-1","body":"### Describe your issue.\r\n\r\nIf I using `KDTree().query()` with the argument `workers` and the values `1` or `-1` I got different results. The result, using multiple workers, is wrong. The error only appears using many points in the query task (maybe you have to increase the number of points of `coo2` in my code example). Using fewer points, all is fine.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.spatial import KDTree\r\n\r\ncoo1 = np.random.rand(int(3e3), 2)\r\ncoo2 = np.random.rand(int(3e8), 2)\r\n\r\ntree = KDTree(coo1)\r\n_, idx_single = tree.query(coo2, k=1, workers=1)\r\nidx_single_bin = np.bincount(idx_single)\r\n\r\n_, idx_multi = tree.query(coo2, k=1, workers=-1)\r\nidx_multi_bin = np.bincount(idx_multi)\r\n\r\nif idx_single_bin[0] != idx_multi_bin[0]:\r\n    raise ValueError(\"result is not equal\")\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"d:\\studom\\CodeProjects\\voxel_compare\\issue-workers.py\", line 1\r\n5, in <module>\r\n    raise ValueError(\"result is not equal\")\r\nValueError: result is not equal\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.1 1.25.0 sys.version_info(major=3, minor=11, micro=4, releaselevel='final', serial=0)\r\nlapack_armpl_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n  NOT AVAILABLE\r\nopenblas_clapack_info:\r\n  NOT AVAILABLE\r\nflame_info:\r\n  NOT AVAILABLE\r\naccelerate_info:\r\n  NOT AVAILABLE\r\natlas_3_10_threads_info:\r\n  NOT AVAILABLE\r\natlas_3_10_info:\r\n  NOT AVAILABLE\r\natlas_threads_info:\r\n  NOT AVAILABLE\r\natlas_info:\r\n  NOT AVAILABLE\r\nlapack_info:\r\n    libraries = ['lapack', 'blas', 'lapack', 'blas']\r\n    library_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\lib']\r\n    language = f77\r\nblas_armpl_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblis_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n  NOT AVAILABLE\r\natlas_3_10_blas_threads_info:\r\n  NOT AVAILABLE\r\natlas_3_10_blas_info:\r\n  NOT AVAILABLE\r\natlas_blas_threads_info:\r\n  NOT AVAILABLE\r\natlas_blas_info:\r\n  NOT AVAILABLE\r\nblas_info:\r\n    libraries = ['cblas', 'blas', 'cblas', 'blas', 'cblas', 'blas']\r\n    library_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\lib']\r\n    include_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\include']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_opt_info:\r\n    define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]\r\n    libraries = ['cblas', 'blas', 'cblas', 'blas', 'cblas', 'blas']\r\n    library_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\lib']\r\n    include_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\include']\r\n    language = f77\r\nlapack_opt_info:\r\n    libraries = ['lapack', 'blas', 'lapack', 'blas', 'cblas', 'blas', 'cblas', 'blas', 'cblas', 'blas']\r\n    library_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\lib']\r\n    language = f77\r\n    define_macros = [('NO_ATLAS_INFO', 1), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['...\/Miniconda3\/envs\/env\\\\Library\\\\include']\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2,AVX512F,AVX512CD,AVX512_SKX\r\n    not found = AVX512_CLX,AVX512_CNL,AVX512_ICL\r\n```\r\n","comments":["cc @sturlamolden ","Several people have messed with the multithreaded code since I last layed my hands on it. It might be that someone has not understood it properly. \r\n\r\nBut before I start to debug, can you please make sure there are no ties in your data set? Ties in the context of a kd-tree are points with equal distance to the query point. In the presence of ties it might be that both queries in theory is correct, but if someone has made a race condition results cound be different. We should consider such a race condition a bug, but it is a different kind of bug than a simple logics error.\r\n\r\nI am tempted to move the multithreaded code into C++ (threads are now in the standard, just to make it more temper proof).","Also get rid of ```np.bincount```. I don\u2019t care for a wild goose chase that turns out to be an error in NumPy. Use e.g. ```np.array_equal``` \u2013 and also compare the single-threaded result with itself, as a negative control.","Thanks for the hints. I improved my Code Example. No similar distances are detected, but the error still exists:\r\n```python\r\nimport numpy as np\r\nfrom scipy.spatial import KDTree\r\n\r\n# initialize test data\r\ncoo1 = np.random.rand(int(3e3), 2)\r\ncoo2 = np.random.rand(int(3e8), 2)\r\n# initialize KDTree\r\ntree = KDTree(coo1)\r\n\r\n# single core neighbor search\r\ndist_single, idx_single = tree.query(coo2, k=1, workers=1)\r\n# check, if singe core result itself is equal\r\nif not np.array_equal(idx_single, idx_single):\r\n    raise ValueError(\"single itself is not equal\")\r\n\r\n# multi core neighbor search\r\ndist_multi, idx_multi = tree.query(coo2, k=1, workers=-1)\r\n# check, if multi core result itself is equal\r\nif not np.array_equal(idx_multi, idx_multi):\r\n    raise ValueError(\"multi itself is not equal\")\r\n\r\n# check for equal distances\r\ndist_single_u = np.unique(dist_single)\r\nif not len(dist_single_u) == len(dist_single):\r\n    print(\"similar distances detected\")\r\n\r\n# compare single vs multi core neighbor search\r\nif not np.array_equal(idx_single, idx_multi):\r\n    raise ValueError(\"results are not equal\")\r\n```","Which of the tests fail?","Your check for equal distances is wrong. You are taking the query results but you need to check all 3e3 x 3e8 distances against eachother. Basically you have ((3e3 x 3e8)**2 - 3e3x3e8)\/2 pairs to compare. Compute all 3e3 x 3e8 distances and push them into a set to verify that they are all unique.","> Which of the tests fail?\r\n\r\nOnly the last one, so I got the message: ValueError(\"results are not equal\")","> Your check for equal distances is wrong. You are taking the query results but you need to check all 3e3 x 3e8 distances against eachother. Basically you have ((3e3 x 3e8)**2 - 3e3x3e8)\/2 pairs to compare. Compute all 3e3 x 3e8 distances and push them into a set to verify that they are all unique.\r\n\r\nDid you agree with this code?\r\n```python\r\nfrom scipy.spatial import distance\r\n\r\n# check for equal distances\r\ndistances = distance.pdist(coo2)\r\nequal_distances = np.isclose(distances, 0)\r\nif np.any(equal_distances):\r\n    print(\"similar distances detected\")\r\n```\r\nBut the problem is: I can not compute it, because it needs too much memory \ud83d\ude1e .\r\n\r\nDid you have any idea, how to compute it, using less memory or do another check?","\r\n> Did you agree with this code?\r\n> \r\n> ```python\r\n> from scipy.spatial import distance\r\n> \r\n> # check for equal distances\r\n> distances = distance.pdist(coo2)\r\n> equal_distances = np.isclose(distances, 0)\r\n> if np.any(equal_distances):\r\n>     print(\"similar distances detected\")\r\n> ```\r\n\r\nNo, I do not.\r\n\r\n> But the problem is: I can not compute it, because it needs too much memory \ud83d\ude1e .\r\n> \r\n> Did you have any idea, how to compute it, using less memory or do another check?\r\n\r\nMemory is not the problem, a small loop in C will do this with almost zero memory overhead. \r\n\r\nThe real issue is time.\r\n\r\n"],"labels":["defect","scipy.spatial"]},{"title":"BUG: Disproportional drop in distance.mahalanobis processing speed at 96 dimensions","body":"### Describe your issue.\r\n\r\nThere seems to be a substantial and unusual drop in performance when calculating the Mahalanobis distance (MD) using the scipy distance function. I have tested this on a local machine and also in a colab environment, and it always seems to occur exactly when the dimension size becomes larger than 95. This phenomenon also seems to occur when manually calculating the MD, so could this be a limitation within Numpy itself?\r\n\r\n<img width=\"434\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/48124412\/b8eb5c5f-7c4d-42e4-a3e4-d076cbc4eeaa\">\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.spatial import distance\r\nfrom timeit import default_timer as timer\r\n\r\n# Create arbitrary list of dimension sizes for testing\r\ndims_list = [50, 75, 90, 93, 94, 95, 96, 97, 98, 99, 100]\r\nprocessing_times = []\r\n\r\n# Loop through each dimension size\r\nfor n_dims in dims_list:\r\n  # Create random inputs for Mahalanobis distance calc\r\n  mean = np.zeros(n_dims)\r\n  inv_cov = np.identity(n_dims)\r\n  y_i = np.random.randn(n_dims)\r\n\r\n  # Perform multiple repeats and get average processing time for current dimension size\r\n  agg_time = 0\r\n  n_repeats = 50\r\n  for j in range(n_repeats):\r\n    start_time = timer()\r\n    for i in range(1000):\r\n      md_i = distance.mahalanobis(u=y_i,\r\n                                  v=mean,\r\n                                  VI=inv_cov)\r\n    end_time = timer()\r\n    agg_time += (end_time - start_time)\r\n  processing_times.append(agg_time \/ n_repeats)\r\n\r\n# Display all processing times for each dimension\r\nplt.plot(dims_list, processing_times)\r\nplt.show()\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nN\/A\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.25.2 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-c6c8ru56\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n","comments":["I believe the core issue is indeed upstream of scipy, and in fact even upstream of numpy: OpenBLAS changes its threading behavior at dimension=96.  See a similar issue here: https:\/\/github.com\/numpy\/numpy\/issues\/22928\r\n\r\nA single `np.dot` call is sufficient to reproduce this slowdown:\r\n```\r\ndelta = np.random.randn(95)\r\nVI = np.identity(95)\r\n%timeit _ = np.dot(delta, VI)  # 2.52 \u00b5s \u00b1 245 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\ndelta = np.random.randn(96)\r\nVI = np.identity(96)\r\n%timeit _ = np.dot(delta, VI)  # 131 \u00b5s \u00b1 13.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n```\r\n\r\nAs mentioned in the linked numpy issue, `threadpoolctl` can be used to work around the OpenBLAS threading behavior:\r\n```\r\nfrom threadpoolctl import threadpool_limits\r\nwith threadpool_limits(limits=1, user_api=\"blas\"):\r\n    %timeit _ = np.dot(delta, VI)  # 3.22 \u00b5s \u00b1 315 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n```","Thanks @kandersolar. It looks like the upstream issue has been fixed: https:\/\/github.com\/OpenMathLib\/OpenBLAS\/issues\/2846. I'll leave this open for visibility, but please try again when OpenBLAS 0.3.27 is released @Glorfendal - I believe the problem should then be fixed"],"labels":["scipy.spatial","upstream bug"]},{"title":"ENH: stats.wasserstein_distance: add axis\/nan_policy support","body":"#### Reference issue\r\nToward gh-14651\r\n\r\n#### What does this implement\/fix?\r\nAdds axis \/ nan_policy \/ keepdims support to `scipy.stats.wasserstein_distance` and `scipy.stats.energy_distance`.\r\n\r\n#### Additional information\r\nI expect one test failure for each function. There is a test in which both positive and negative infinities appear in a sample. `_contains_nan` has always (or at least for as long as I've been looking) checked for NaNs by summing along `axis` and looking for NaN in the result. Adding the positive and negative infinity results in a NaN. In the end, the function returns NaN result instead of Inf.\r\n\r\nThere are many ways to address this. We could change `_contains_nan` to check individual elements for NaN by default, or we could add a pathway to ensure that the elements are checked individually for *these* functions. What do you think @tirthasheshpatel?","comments":["@tirthasheshpatel Re: the fact that `u_values`\/`u_weights` are paired and (separately) `v_values`\/`v_weights` are paired, maybe the best way to specify the pairings is to have a list of tuples of strings? Each tuple is independent of the other tuples, but arguments (identified by their keyword) within each tuple are paired.","> the fact that `u_values`\/`u_weights` are paired and (separately) `v_values`\/`v_weights` are paired, maybe the best way to specify the pairings is to have a list of tuples of strings?\r\n\r\nYeah. That's what I have in mind. Something like this `paired=[(a, b, ...), (g, h, ...), ...]` in the most general case. ","I suppose we'll add an option to the decorator that controls whether the \"summation\" method is used by `_contains_nan`, too."],"labels":["scipy.stats","enhancement"]},{"title":"scipy-openblas issues on macOS","body":"### Describe your issue.\n\nI'm having issues trying to get scipy-openblas to work properly for local development on macOS (as a prelude to using it in a CI entry).\r\n\r\nLocal usage\r\n------------\r\nSonoma 14.2.1, M3\r\nIf I use the latest version of scipy-openblas (scipy_openblas32-0.3.26.0.4-py3-none-macosx_11_0_arm64.whl) I get a whole load of `ImportError: dlopen(\/Users\/andrew\/Documents\/Andy\/programming\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/_flapack.cpython-311-darwin.so, 0x0002)` errors, and if I run `otool -L` on the dylib the openblas library is not listed. Using `nm -a` on the dylib indicates that a lot of the BLAS symbols are unresolved.\r\n\r\nIf I drop back to `scipy-openblas32<=0.3.23.293.2` then the build works, but there are a couple of test fails\r\n\r\n> FAILED scipy\/special\/tests\/test_cdflib.py::test_nctdtr_gh19896 - AssertionError: \r\nFAILED scipy\/special\/tests\/test_cdflib.py::test_nctdtrinc_gh19896 - AssertionError: \r\n\r\nIf I repeat the tests on my home machine I get similar behaviour, but a lot more test fails. This might be due to slightly different compiler versions (I have gfortran 13.2.0 and clang-1500.1.0.2.5 on my M3)\r\n\r\nCI usage\r\n---------\r\nIf I do a similar process (using the pinned scipy-openblas wheel, see [here](https:\/\/github.com\/andyfaff\/scipy\/pull\/62\/files) for run configuration) on a GHA CI using a `macos-14` image the build completes. However the tests reveal segfaults, e.g. [here](https:\/\/github.com\/andyfaff\/scipy\/actions\/runs\/8003061306\/job\/21857572355#step:4:7668). These segfaults appear to come from tests involving linalg (i.e. OpenBLAS). The specification for the Github actions M1 runners are [here](https:\/\/github.com\/actions\/runner-images\/blob\/main\/images\/macos\/macos-14-arm64-Readme.md).\r\n\r\nI'm wondering what the situation is for scipy-openblas w.r.t using it in CI? A pinned version seems to be fine for Linux.\r\n\r\n\r\n@mattip\r\n\r\n\r\n\r\n\r\n\n\n### Reproducing Code Example\n\n```python\nhttps:\/\/github.com\/andyfaff\/scipy\/pull\/62\/files\n```\n\n\n### Error message\n\n```shell\nN\/A\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nnumpy 1.26.4\r\npython 3.11\n```\n","comments":["The wheel requires a `scipy_` prefix on the OpenBLAS symbols, and I think some of the code does not use the proper macro to mangle the blas functions. xref #20074 (a different trial of the scipy-openblas wheels) and #19855 (which will use the proper macros)."],"labels":["defect"]},{"title":"BUG\/ENH?: scipy.linalg.norm: overflow\/underflow when `axis` is provided","body":"### Describe your issue.\r\n\r\nI'm working on adding an `axis` argument to `scipy.stats.pearsonr`. It uses `scipy.linalg.norm` instead of (e.g.) `np.linalg.norm` because `scipy.linalg.norm` tends to avoid premature overflow.\r\n\r\n```python3\r\nfrom scipy import linalg\r\nx = [-5e210, 5e210, 3e200, -3e200]\r\nlinalg.norm(x)\r\n# 7.071067811865475e+210\r\n```\r\n\r\nUnfortunately, that advantage is lost when `axis` is not `None`.\r\n\r\n```python3\r\nlinalg.norm(x, axis=0)  # warning and inf\r\n```\r\n\r\nSimilarly, premature underflow can occur when the argument has small magnitude elements and `axis` is not `None`.\r\n\r\nI can work around it if need be (e.g. manually scale), but can `linalg.norm` avoid premature under\/overflow regardless of `axis`?\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom scipy import linalg\r\nx = [-5e210, 5e210, 3e200, -3e200]\r\nlinalg.norm(x, axis=0)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nRuntimeWarning: overflow encountered in multiply s = (x.conj() * x).real\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nimport sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n1.13.0.dev0+1376.8360bac 1.26.0 sys.version_info(major=3, minor=11, micro=6, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.24\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.24\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib, -L\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib,\r\n      -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 15.0.7\r\n  c++:\r\n    args: -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib, -L\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib,\r\n      -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 15.0.7\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.4\r\n  fortran:\r\n    args: -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    commands: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib, -L\/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/lib,\r\n      -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/matthaberland\/miniforge3\/envs\/scipy-dev\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["`np.linalg.norm` is used whenever `axis` is provided: https:\/\/github.com\/scipy\/scipy\/blob\/fdf3b908eeb93203c1847c6fb4efd12e88c37ce5\/scipy\/linalg\/_misc.py#L177-L178","On top of what @lucascolley mentioned, NumPy does not use the safe norm from LAPACK but does its own unscaled dot product hence bypasses the safety guards. If you don't have too many cols\/rows in the axis a for loop is an easy way out without too much of python overhead. ","Should we fix 2-norm working with axis? I can cook up something in C or Cython and `nrm2` is not complicated. \r\n\r\nBased on what we discussed over kron deprecation, not sure what it entails in terms of array api though since this is a central function and unlike kron we should support it and make it as general as possible, in my opinion. However if I cook something up in C or Cython not sure if it is even possible to support it.","From what I understand, numpy should be able to use `nrm2` as well as it is BLAS, not LAPACK. In the long run, would it not be best if this was fixed in numpy itself on top of our own efforts in scipy? `norm` is definitely much less involved than many other linalg functions from numpy such as the matrix decompositions but a fundamentally important function in my opinion.","that sounds sensible - see https:\/\/github.com\/data-apis\/array-api\/issues\/213 for the array API perspective. We'd probably rewrite `scipy.linalg.norm` in terms of `xp.linalg.vector_norm` and `xp.linalg.matrix_norm` for the non-`np` codepath.\r\n\r\nIf we can upstream all efficiency gains from the SciPy implementation to those (new) NumPy functions, we'll be able to simplify it to just one codepath (well, there'll be a codepath for if the namespace doesn't implement the `linalg` extension, but that's orthogonal to the discussion here).","Oh my, I typed too fast probably: openblas does not support the safe scaling yet apparently: https:\/\/github.com\/OpenMathLib\/OpenBLAS\/pull\/4313. Until it is available there, probably difficult to use for both numpy and scipy but I would like to be convinced of the opposite by the BLAS gurus.\r\n\r\nXref numpy issue https:\/\/github.com\/numpy\/numpy\/issues\/19097","It doesn't have to use the BLAS code as all based on Anderson's work. We can write our own native version of nrm2 in C or Cython. Not sure if NumPy implements it since it is not using nrm2 even now. ","Overall judging by the discussions oat the array api, it seems to me the linalg parts are not well thought out. "],"labels":["enhancement","scipy.linalg"]},{"title":"BUG: `csr_array(int())` errors","body":"### Describe your issue.\n\nIt seems like the described behavior does no work while `np.array(int())` does seem to work.  This comes from the [array api test suite setup](https:\/\/github.com\/data-apis\/array-api-tests\/blob\/4f83bb3ee9146c30cf997f1527588b7a8b8ee6db\/array_api_tests\/dtype_helpers.py#L249), so presumably it should work and indeed does for `csr_matrix`.  I think the error is recent (or on `main`) because it does not error in `1.11.4`. I know there has been a lot of work around 1d so sorry if this is expected!  Happy to fix if it's a real bug!\n\n### Reproducing Code Example\n\n```python\nfrom scipy import sparse\r\nimport numpy as np\r\n\r\nsparse.csr_array(int()) # errors\r\nsparse.csr_matrix(int()) # does not error\r\nnp.array(int()) # does not error\n```\n\n\n### Error message\n\n```shell\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/ilangold\/Projects\/Theis\/scipy\/scipy\/sparse\/_compressed.py\", line 86, in __init__\r\n    self._coo_container(arg1, dtype=dtype)\r\n  File \"\/Users\/ilangold\/Projects\/Theis\/scipy\/scipy\/sparse\/_coo.py\", line 81, in __init__\r\n    self._shape = check_shape(M.shape, allow_1d=is_array)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilangold\/Projects\/Theis\/scipy\/scipy\/sparse\/_sputils.py\", line 317, in check_shape\r\n    raise TypeError(\"function missing 1 required positional argument: \"\r\nTypeError: function missing 1 required positional argument: 'shape'\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.13.0.dev0+1365.cd9d20b 1.26.4 sys.version_info(major=3, minor=11, micro=8, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/include\r\n    lib directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/include\r\n    lib directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/lib\/pkgconfig\r\n    version: 0.3.26\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2, -isystem,\r\n      \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -O2, -pipe, -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 16.0.6\r\n  c++:\r\n    args: -ftree-vectorize, -fPIC, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -O2, -pipe, -stdlib=libc++, -fvisibility-inlines-hidden, -fmessage-length=0,\r\n      -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2,\r\n      -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 16.0.6\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    args: -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    commands: \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    linker args: -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs, -Wl,-rpath,\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib,\r\n      -L\/Users\/ilangold\/micromamba\/envs\/scipy-dev\/lib, -march=armv8.3-a, -ftree-vectorize,\r\n      -fPIC, -fno-stack-protector, -O2, -pipe, -isystem, \/Users\/ilangold\/micromamba\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/micromamba\/envs\/scipy-dev-array-api\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/ilangold\/micromamba\/envs\/scipy-dev-array-api\/bin\/python3.11\r\n  version: '3.11'\n```\n","comments":["Just checked `1.12.0` as well and does not seem to be an error there either.","`sparse.csr_array(int())` is equivalent to `sparse.csr_array(np.array(0))`. So I suppose the question is whether that should be allowed.","We don't support 0-D sparse arrays. So I think this slipped through our shape checking changes. \r\nThat will still not make it conform to array_api, but it will be a different error message.\r\n\r\nI'm not sure what a sparse version of a 0-D array would look like. Probably raising is the right thing to do even if that breaks array_api. @ilan-gold what do you think should be returned in this case?","@dschult Let me do some fiddling and inspection.  This isn't even in one of the tests for the array api, it's just in the lead up to starting the tests.  So maybe it's not even in there (although I somewhat doubt this).\r\n\r\n> We don't support 0-D sparse arrays.\r\n\r\nIs there a design document or the like I could look at as a reference for this sort of thing?","Indexing a 2-D CSR array yields a numpy scalar:\r\n```python\r\n>>> csr_2d[1, 1]\r\nnp.float64(1.0)\r\n```\r\n\r\nThat is the same behavior as numpy, and given that numpy scalars duck type as 0-D arrays, this is okay. So there shouldn't be an urgent need for 0-D sparse arrays I'd think. \r\n\r\nThat said, if 1-D sparse arrays are a thing, I don't see why 0-D sparse arrays shouldn't be a thing also. It's probably more a \"is it worth it\" question.","In terms of \" is it worth it\" (since we will be getting 1D sparse arrays from what I understand), I would think maybe yes?  Beyond the goal of matching numpy's behavior (which is an array with shape `()`),  [this behavior](https:\/\/github.com\/data-apis\/array-api-tests\/blob\/4f83bb3ee9146c30cf997f1527588b7a8b8ee6db\/array_api_tests\/test_array_object.py#L85-L89) is \"officially\" tested in the array api.  However, I don't see anything officially in the spec.  Could you point me to it if it is there?\r\n\r\nI think the overriding factor should be the array api's demands and then secondarily can think about `numpy`.","> However, I don't see anything officially in the spec. Could you point me to it if it is there?\r\n\r\nFrom https:\/\/data-apis.org\/array-api\/draft\/API_specification\/array_object.html: _\"Furthermore, a conforming implementation of the array API standard must support, at minimum, array objects of rank (i.e., number of dimensions) 0, 1, 2, 3, and 4 and must explicitly document their maximum supported rank `N`. ... Conforming implementations must support zero-dimensional arrays.\"_\r\n\r\nIt is pretty weird indeed to return numpy scalars (i.e., a more-or-less 0-D array _from another library_) for a standard operation like indexing a 1-D or 2-D array.","Thanks! So it seems we do indeed need this behavior to not error for array api to work then.  And more generally, beyond being able to construct these arrays as the issue here specifies, we need the return type of scipy sparse arrays then to also be of this 0-d type.","This is somewhat related to https:\/\/github.com\/scipy\/scipy\/issues\/19919 as well.  The array api test suite actually provides a nice barrage of tests by which we could try to standardize some of this behavior.  I will do some more digging to formalize a list or at least some more concrete points of what exactly is needed\/missing.  \r\n\r\nBut that is separate from this issue, which I think is focused on construction, not return types.","> So it seems we do indeed need this behavior to not error for array api to work then\n\nNote the discussion in gh-18915, it seems unlikely that we'll reach a point where `scipy.sparse` arrays will \"work\" with the array API standard. (Maybe you just mean 'work, as far as we can with what is in scope')","@lucascolley I agree, but it just seemed that this case was special given that covering it is apparently needed just to get the tests to run at all.  Otherwise I don't think I would have posted it.\r\n\r\nOne thing I can see as a result of thinking about this would be some level of customization for hypothesis tests in the array api test suite.  This would allow us to pass tests we want to, while being clear about what we don't support.  I am not really sure it makes sense to have some sort of testing suite where _some_ things pass and others error, which we then have to track by hand somehow.  I imagine we will not be the only ones with this issue.   Although, I guess trakcing by hand isn't the worst thing, although it would not let us customize to only do 2D tests for example.  So we might want `__getitem__` to pass for 2D but not 3D.","> One thing I can see as a result of thinking about this would be some level of customization for hypothesis tests in the array api test suite. This would allow us to pass tests we want to, while being clear about what we don't support.\r\n\r\nHave you seen https:\/\/github.com\/numpy\/numpy\/blob\/main\/tools\/ci\/array-api-skips.txt?","Oh very cool @lucascolley thanks!  I had not!","Additional tests (and maybe tests of what we don't support) would naturally go into the `scipy.sparse` module `scipy\/sparse\/tests\/test_array_api.py`\r\n\r\n","NumPy just installs `array-api-tests` in a CI job and passes in the list of skips: https:\/\/github.com\/numpy\/numpy\/blob\/18c13458d544d9b935b63e7b495a346ec14f5468\/.github\/workflows\/linux.yml#L212-L249","I'm thinking about what a 0D sparse array would look like. I think we would just want to make numpy ndarray 0D arrays be the sparse array 0D arrays. They aren't identified as scipy.sparse in any way. But is that even needed? I guess what I mean is -- to support the array api we can combine array types with other libraries, right? (There is already at least one place where we return a dense nd array for n>2. I can't quite recall, but I think it is after multiplying a sparse array by a nd dense array.)"],"labels":["defect","scipy.sparse","array types"]},{"title":"ENH\/TST: Refactor refguide-check, take 3","body":"#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\nContinues and closes gh-16391, supersedes and closes gh-19242\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nContinue gh-19242 by @Sheila-nk : refactor the doctesting part of the refguide-check into a standalone tool, use pytest for running and orchestration. \r\n\r\nSo this PR has essentially two parts: \r\n- heavy lifting done by Sheila (see commit attributions) \r\n- additional fixes to docstring examples from gh-19242. A part with pytest ignores was suggested by Lucas in https:\/\/github.com\/scipy\/scipy\/pull\/19242#discussion_r1431636535, also commit-attributed.\r\n\r\nThe remaining TODOs follow https:\/\/github.com\/scipy\/scipy\/pull\/19242#issuecomment-1719964303. What's relevant now is\r\n\r\n- [ ] finish plumbing the external tool as git submodule\r\n- [x] activate checking the code in tutorials\r\n- [ ] drop the obsoleted part of `refguide-check`. I'd prefer to do it in a quick follow-up PR.\r\n\r\nOn the 'scpdt' tool side, we need to\r\n\r\n- [ ] get a better name (doctest_sp, doctest_scipy, scipy_doctest, something else?) --- `doctest-scipy` unless there are other suggestions\r\n- [ ] get out of my GH --- will propose to move it to the scipy GH org on the ML\r\n\r\nThe tool itself is in a bit of a flux at the moment, nothing blocking though. Here's the tracker which is being actively worked on: https:\/\/github.com\/ev-br\/scpdt\/issues\r\n\r\nThe pytest plugin is written by @Sheila-nk with inputs from @melissawm and myself. Thank you Sheila and Melissa!\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nI am starting to dislike tagging this all as _doctesting_. The goal is not ---never was and will never be --- to mix doctests into unit testing. The goal is to keep the examples in the docs current. If somebody comes up with a catchy name for this, I'm all ears :-).\r\n\r\n","comments":["Testing `dev.py` options:\r\n\r\n- [x] `$ python dev.py test --doctests -j 4` seems to give all workers all tests instead of distributing them? Needs checking, can be done in a follow-up (`refguide-check` never supported this). Is tracked at https:\/\/github.com\/ev-br\/scpdt\/issues\/116\r\n- [x]  `$ python dev.py test --doctests -t path\/to\/file` does not work. Can be done in a follow-up, as `refguide-check` never supported this.\r\n\r\nOne other bikeshed item is the name of the switch: `dev.py test --doctests` (current state) or `dev.py test --doctests-only`?\r\n\r\nWith regular pytest `pytest --doctests` means run _both_ unit tests _and_ doctests; Here `dev.py test --doctests` skips regular unit tests entirely. OTOH `--doctests-only` is more typing. Thoughts?","The commit attribution to me hasn't quite worked due to extra quotation marks, but no biggie :) FWIW I'm a fan of `smoke-doc`","Here's an update:\r\n\r\n- Following up on the ML discussion and Pamphile's suggestion (thanks @tupui), the dev.py plumbing is through a dedicated command: \r\n `$ python dev.py smoke-docs`. The command mirrors the `test` command, and accepts much the same arguments: `smoke-docs -s submodule`, `smoke-docs -t path\/to\/file::Klass::method` etc.\r\n\r\n- To smoke-test the tutorial, there's a separate command `$ python dev.py smoke-tutorials`\r\n\r\n- smoke-testing the tutorials now includes subfolders--- those in `doc\/source\/tutorial\/{stats, interpolate}` have never been checked by refguide-check.  This PR now includes several easy fixes and xfails for less obvious errors."],"labels":["enhancement","maintenance"]},{"title":"ENH: Add mode='valid' to morphology operators such as ndimage.grey_dilation, ndimage.grey_erosion etc.","body":"Mathematical morphology operators such as `ndimage.grey_dilation`, `ndimage.grey_erosion` only offer to pad the input with different modes `{\u2018reflect\u2019,\u2019constant\u2019,\u2019nearest\u2019,\u2019mirror\u2019, \u2018wrap\u2019}`. It would be nice to be able to do have a `valid` mode that doesn't do any padding, like in `signal.convolve`.\r\n\r\nAt the moment the only way to do a `valid` dilation is to do a padded dilation, and to crop the output to only keep the valid samples. However this is prone to errors, and this is highly inefficient when the structure size is relatively large compared to the input.\r\n\r\nThis issue is somewhat related to this other [existing issue](https:\/\/github.com\/scipy\/scipy\/issues\/12997) for ndimage.convolve.","comments":[],"labels":["enhancement","scipy.ndimage"]},{"title":"BUG: Fix stats.skewnorm.ppf for large skew values","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nCloses gh-20124\r\n\r\n#### What does this implement\/fix?\r\nFix an upstream bug in Boost that results in nonsensical results when computing PPF of skewnorm distributions that have large values of skew.\r\n\r\nI'm trying to verify my regression test on my fork https:\/\/github.com\/maresb\/scipy\/pull\/3.","comments":["x-ref gh-20122","Ah, it's a bit disappointing that upgrading Boost didn't automatically solve gh-20124. But at least the warning message is different.\r\n\r\nNow:\r\n```\r\nRuntimeWarning: Error in function boost::math::quantile(const skew_normal_distribution<d>&, %1%): Unable to locate solution in a reasonable time: either there is no answer to quantile or the answer is infinite.  Current best guess is %1%\r\n```\r\n\r\nPreviously:\r\n```\r\nRuntimeWarning: overflow encountered in _skewnorm_ppf\r\n```\r\n\r\nI may have to chase this one further upstream in Boost."],"labels":["defect","scipy.stats"]},{"title":"BUG: stats.skewnorm.ppf returns wrong values with moderately high skew parameters","body":"### Describe your issue.\n\nI'm trying to compute the inverse CDF (i.e. PPF) of several skewnorm distributions. I am getting nonsense answers when the skew parameter is moderately large.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.stats import halfnorm, skewnorm\r\n\r\n\r\n#  This is the correct ppf for a standard skewnorm with skew 500\r\nq = 0.012533469508013\r\n\r\n#  As confirmation of the above result, the skewnorm with skew 500 is\r\n#  extremely close to a halfnorm distribution.\r\n#  In fact, the ppf of the halfnorm agrees with q to 13 decimal places.\r\nhalfnorm.ppf(0.01, 0, 1)\r\n#  0.012533469508069276\r\n\r\n#  This should theoretically be 0.01, and it is very close.\r\nskewnorm.cdf(q, 500, 0, 1)\r\n#  0.009999999999999898\r\n\r\n#  This should theoretically be q, but it's not at all.\r\n#  This is the problem.\r\nskewnorm.ppf(0.01, 500, 0, 1)\r\n#  4.833181656374989e+142\n```\n\n\n### Error message\n\n```shell\n\/opt\/conda\/lib\/python3.11\/site-packages\/scipy\/stats\/_continuous_distns.py:9141: RuntimeWarning: overflow encountered in _skewnorm_ppf\r\n  return _boost._skewnorm_ppf(x, 0, 1, a)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.12.0 1.26.3 sys.version_info(major=3, minor=11, micro=7, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/conda\/include\r\n    lib directory: \/opt\/conda\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: \/opt\/conda\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/conda\/include\r\n    lib directory: \/opt\/conda\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: \/opt\/conda\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/opt\/conda\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/opt\/conda\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem,\r\n      \/opt\/conda\/include\r\n    commands: \/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib,\r\n      -Wl,-rpath-link,\/opt\/conda\/lib, -L\/opt\/conda\/lib, -Wl,-O2, -Wl,--sort-common,\r\n      -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now, -Wl,--disable-new-dtags, -Wl,--gc-sections,\r\n      -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib, -Wl,-rpath-link,\/opt\/conda\/lib,\r\n      -L\/opt\/conda\/lib, -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/opt\/conda\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem,\r\n      \/opt\/conda\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  c++:\r\n    args: -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -fvisibility-inlines-hidden,\r\n      -fmessage-length=0, -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC,\r\n      -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections, -pipe, -isystem,\r\n      \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/opt\/conda\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem,\r\n      \/opt\/conda\/include\r\n    commands: \/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib,\r\n      -Wl,-rpath-link,\/opt\/conda\/lib, -L\/opt\/conda\/lib, -Wl,-O2, -Wl,--sort-common,\r\n      -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now, -Wl,--disable-new-dtags, -Wl,--gc-sections,\r\n      -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib, -Wl,-rpath-link,\/opt\/conda\/lib,\r\n      -L\/opt\/conda\/lib, -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona,\r\n      -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt,\r\n      -O2, -ffunction-sections, -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -fvisibility-inlines-hidden,\r\n      -fmessage-length=0, -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC,\r\n      -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections, -pipe, -isystem,\r\n      \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/opt\/conda\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem,\r\n      \/opt\/conda\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.7\r\n  fortran:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix\r\n    commands: \/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib,\r\n      -Wl,-rpath-link,\/opt\/conda\/lib, -L\/opt\/conda\/lib, -Wl,-O2, -Wl,--sort-common,\r\n      -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now, -Wl,--disable-new-dtags, -Wl,--gc-sections,\r\n      -Wl,--allow-shlib-undefined, -Wl,-rpath,\/opt\/conda\/lib, -Wl,-rpath-link,\/opt\/conda\/lib,\r\n      -L\/opt\/conda\/lib, -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/opt\/conda\/include, -fdebug-prefix-map=\/home\/conda\/feedstock_root\/build_artifacts\/scipy-split_1705882420314\/work=\/usr\/local\/src\/conda\/scipy-split-1.12.0,\r\n      -fdebug-prefix-map=\/opt\/conda=\/usr\/local\/src\/conda-prefix\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/conda\/bin\/python\r\n  version: '3.11'\n```\n","comments":["Additional information:\r\n\r\nI have isolated the error to Boost. I found a [reported regression](https:\/\/github.com\/boostorg\/math\/issues\/184#issuecomment-1909917413) which has been fixed in https:\/\/github.com\/boostorg\/math\/pull\/1080. Thus I suspect that this can be solved by updating Boost.\r\n\r\n```python\r\nfrom scipy.stats._boost.skewnorm_ufunc import _skewnorm_ppf\r\n\r\n_skewnorm_ppf(0.01, 0, 1, 500)\r\n#  4.833181656374989e+142\r\n```","x-ref gh-19348 - looks like we want to try updating to Boost 1.85 once it's out"],"labels":["defect","scipy.stats"]},{"title":"ENH: sparse: Add indexing for 1D arrays","body":"Adds support for indexing with 1D arrays.\r\n\r\nBuilds on #19833 (csr-1d) to provide indexing for 1D CSR arrays, though this code also works for DOK with minor changes after #19715 (dok-1d) gets merged.\r\n\r\nThe diff will be much easier to read after #19833 is merged. Look at only the last commit to see the diff for just this PR. I will rebase as needed, but I think the CSR PR will be merged soon and the diff here will become cleaner.\r\n\r\n- adds `test_indexing1d.py`\r\n- refactors `IndexMixin._validate_indices` to use an ndim-independent approach for both 1d and 2d.\r\n- the supported formats will need methods `_get_int`, `_get_slice` and `_get_array`. These are dispatched to after processing the index.\r\n- this tries to incorporate all improvements from the recently merged #19957 (and doesn't change those tests). \r\n- the ndim-independent approach means some helper functions are no longer needed and\/or have been inlined. The two remaining helper functions are now methods so they have access to `self.ndim`. They are `_as_indices` and `_compatible_boolean_array`.\r\n\r\n","comments":["I've changed the tests to use np.testing `assert_equal` and `assert_allclose` in these new tests. I'll create a separate PR to change the new-but-merged-tests to use them too.\r\n\r\nI've also added indexing support for np.newaxis\/None (though it can't make 3D+). And that was an excuse to revamp some of the getitem\/setitem code to be easier to move to nd.  Quanstide's `ndindex` library would give almost everything we need for `validate_indices`, but it doesn't support boolean sparse arrays as indexes. Anyway, we can now index 2d and 1d sparse arrays.\r\n\r\nI believe the test failures are not related to this PR.\r\n\r\nNote: This PR does not change reduction operations to have them return 1D sparse arrays. That will be a separate PR.\r\n\r\nNote: Another separate PR: We need to decide how to handle 0D. `A[3,4]` should return a 0D object as per array_api. But should it return a numpy or scipy.sparse 0D object? In general, an array type \"should\" return it's own type when indexed. But sparse is not a standard array type -- we don't expect the entire array api to be implemented. But the answer to this question will impact this same code."],"labels":["enhancement","scipy.sparse"]},{"title":"ENH:  scipy.spatial.ConvexHull for many different polyhedra in parallel","body":"### Is your feature request related to a problem? Please describe.\n\nIn MR Imaging, we need to calculate for long lists of 2D or 3D points the volume of the voronoi cells around each point.\r\n\r\nThe vornoi tesselation is really fast, leading to a lists of length 10k of 6-40 `vertices `for each point. \r\n Using `dcf=[scipy.spatial.ConvexHull(v).volume for v in vertices]` is really slow.\r\nUsing a `ThreadPoolExecutor `seems to be limited by overhead \/  non-nogil regions (?) and uses only 200% CPU on a 64 Core machine\r\nUsing a _ProcessPoolExecutor_ is also just a factor of 2-3 faster than the list version, most likely due to overhead.\r\n\n\n### Describe the solution you'd like.\n\nIt would be nice to pass the vertices list to Convexhull, which then uses cython multihreading to call qhull.\r\nCalls to .volume etc would then return lists as well.\r\n\r\nFor the interface, this would be a bit similar to how scipy.spatial.transform.Rotation also handles arrays of Rotations\n\n### Describe alternatives you've considered.\n\nWe could also move this into a separate package if this does not fit into scipy.\r\nDo you have any suggestions why this does not parallelize nicely with a ThreadPoolExecutor?\r\nIt looked like most of the logic should be in a nogil block and should run in parallel threads..\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Can you clarify--you have a single Voronoi diagram and you want to calculate the area (2D) or volume (3D) of the Voronoi region around each point\/generator in that diagram? So you're calculating the convex hull of each Voronoi region just so you can access the `volume` attribute? Or is something else going on here?\r\n\r\nSince Voronoi regions are guaranteed to be convex, I'd already worry about efficiency if that's what you're doing, but maybe you mean more than one Voronoi region near each point?\r\n\r\nWe do have, for example, `calculate_areas` method for `SphericalVoronoi`. I hadn't thought about whether something similar might be generalized to `Voronoi` proper.","Yes, this seems unnecessarily complex considering the voronoi cells will always be convex. \r\nSo directly having a calculate_volume on Vornoi would be the best for us.","I was looking at this a bit today. Pauli noted that the convex hull of each region is indeed the recommended way: https:\/\/stackoverflow.com\/a\/19661317\/2942522. It is even mentioned in the Qhull docs: http:\/\/www.qhull.org\/html\/qh-faq.htm#volume\r\n\r\nApparently there's a lib that can do it faster, referenced in the Qhull docs: https:\/\/www.netlib.org\/voronoi\/hull.html\r\n\r\nAnyway, let me think about this a bit more. Pauli seemed to reach the same conclusion I did, the direct \"volume of vor region\" is not exposed directly in Qhull. We could probably provide something fairly fast under the hood though.","We might be able to lift the ideas from the netlib hull lib, assuming this is license compatible:\r\n\r\n```\r\n\/* hull.c : \"combinatorial\" functions for hull computation *\/\r\n\r\n\/*\r\n * Ken Clarkson wrote this.  Copyright (c) 1995 by AT&T..\r\n * Permission to use, copy, modify, and distribute this software for any\r\n * purpose without fee is hereby granted, provided that this entire notice\r\n * is included in all copies of any software which is or includes a copy\r\n * or modification of this software and in all copies of the supporting\r\n * documentation for such software.\r\n * THIS SOFTWARE IS BEING PROVIDED \"AS IS\", WITHOUT ANY EXPRESS OR IMPLIED\r\n * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY\r\n * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY\r\n * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.\r\n *\/\r\n```\r\n\r\n","FWIW, this has the [same annoying documentation clause](https:\/\/github.com\/scipy\/unuran\/pull\/14) of the function that we just removed from our UNU.RAN fork."],"labels":["enhancement","scipy.spatial"]},{"title":"BUG: instability in `optimize.milp` when using big numbers?","body":"### Describe your issue.\r\n\r\nThe below code gives infeasibility, however one solution would be x1=0, x2=0.\r\n\r\nNote that using `h=10**11` or `h=10**13` gives feasibility.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nh = 10**12\r\nA = np.array([\r\n    [100.4534, h],\r\n    [100.4534, -h]\r\n    ])\r\nb = np.array([h, 0])\r\nconstraints = LinearConstraint(A=A, ub=b)\r\nbounds = Bounds([0,0],[1,1])\r\nc = np.array([0, 0])\r\n\r\nres = milp(c=c, \r\n           constraints=constraints,\r\n           bounds=bounds,\r\n           integrality=1)\r\n```\r\n\r\n\r\n### Optimization result\r\n\r\n```shell\r\nmessage: The problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is At lower\/fixed bound)\r\n        success: False\r\n         status: 2\r\n            fun: None\r\n              x: None\r\n mip_node_count: None\r\n mip_dual_bound: None\r\n        mip_gap: None\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nPython version 3.11.5\r\nscipy==1.12.0\r\nnumpy==1.26.4\r\n```\r\n","comments":["Could anyone have a look at this? Or was the issue badly described?","Not an expert in the area but your constraint matrix is very poorly conditioned so wouldn't surprise me if that was the issue","@HaoZeke are you able to try this problem out with pure HiGHS code (no SciPy wrapper) and (assuming the problem persists) submit an upstream bug report?","> Not an expert in the area but your constraint matrix is very poorly conditioned so wouldn't surprise me if that was the issue\r\n\r\n@j-bowhay the matrix is conditioned that way because it essentially replicates the behavior of indicator functions. Thus, I do expect to have a different result if one of the variable slightly changes (e.g. from x1=0 to x1=1)."],"labels":["defect","scipy.optimize"]},{"title":"BUG: Scipy.io mmread does not respect variants of coordinate format","body":"### Describe your issue.\r\n\r\nHey!\r\n\r\nBased on this file format provided by MathNist (https:\/\/math.nist.gov\/MatrixMarket\/formats.html), `mmread` does not respect the pattern variant of coordinate format. \r\n\r\nThe file format says:\r\n```\r\nas well as for those in which only the position of the nonzero entries is prescribed (pattern matrices).\r\n```\r\n\r\nThat essentially means that I **do not** have to provide _nonzero_ entries as\r\n```\r\ni j value\r\n```\r\n\r\nAnd that leads to the error in the reproducible code:\r\n```\r\nValueError: Header line not of length 3: 2 1\r\n```\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport urllib\r\nimport zipfile\r\nfrom scipy.io import mmread\r\n\r\ndef download_and_unzip_dataset(url: str, file_name: str)->str:\r\n  extract_dir = \"output\"\r\n  zip_path, _ = urllib.request.urlretrieve(url)\r\n  with zipfile.ZipFile(zip_path, \"r\") as f:\r\n    f.extractall(extract_dir)\r\n  return f\"{extract_dir}\/{file_name}\"\r\n\r\nurl = \"https:\/\/nrvis.com\/download\/data\/ca\/ca-CSphd.zip\"\r\nsave_path = download_and_unzip_dataset(url, 'ca-CSphd.mtx')\r\n\r\ndata = mmread(save_path)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_mmio.py in mmread(source)\r\n    127            [0., 0., 0., 0., 0.]])\r\n    128     \"\"\"\r\n--> 129     return MMFile().read(source)\r\n    130 \r\n    131 # -----------------------------------------------------------------------------\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_mmio.py in read(self, source)\r\n    579 \r\n    580         try:\r\n--> 581             self._parse_header(stream)\r\n    582             return self._parse_body(stream)\r\n    583 \r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_mmio.py in _parse_header(self, stream)\r\n    643     def _parse_header(self, stream):\r\n    644         rows, cols, entries, format, field, symmetry = \\\r\n--> 645             self.__class__.info(stream)\r\n    646         self._init_attrs(rows=rows, cols=cols, entries=entries, format=format,\r\n    647                          field=field, symmetry=symmetry)\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_mmio.py in info(self, source)\r\n    406             else:\r\n    407                 if not len(split_line) == 3:\r\n--> 408                     raise ValueError(\"Header line not of length 3: \" +\r\n    409                                      line.decode('ascii'))\r\n    410                 rows, cols, entries = map(int, split_line)\r\n\r\nValueError: Header line not of length 3: 2 1\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.25.2 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-c6c8ru56\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n","comments":["Is this related to https:\/\/github.com\/scipy\/scipy\/issues\/9426?","> Is this related to #9426?\r\n\r\nI do not think so,  the pattern variant does not need us to parse the coordinates as a 3 length line. It should also accept `x1 y1` kind of coordinates too as the format allows it. `mmread` gives an error as it expects coordinates to be of length 3 which is not the case.\r\n\r\n"],"labels":["defect","scipy.io"]},{"title":"ENH: Compute envelope of a real- or complex-valued signal.","body":"This PR proposes a new function for `scipy.signal`, which computes the envelope of a real- or complex-valued signal after applying an optional bandpass filter.\r\n\r\nThe discussion in #19814 revealed that literature is a bit convoluted regarding on how to define envelopes for signals. Hence, this PR also tries to concisely document the approach utilized here.\r\n\r\nThis PR differs from #19814 in that it provides a general solution, whereas #19814  does not produce correct results (regarding the definitions of a envelope given here) for all possible signals.\r\n\r\n\r\n","comments":["The rendered version can be found [here](https:\/\/output.circle-artifacts.com\/output\/job\/a625a4d5-4ecb-4a5a-aae3-d7ad83b874c5\/artifacts\/0\/html\/reference\/generated\/scipy.signal.envelope.html).\r\n\r\nI am not sure what to make of the error message:\r\n```\r\nscipy\/signal\/_signaltools.py:2467: in <module>\r\n    def envelope(z: np.ndarray, bp_in: tuple[int | None, int | None] = (1, None), *,\r\nE   TypeError: unsupported operand type(s) for |: 'type' and 'NoneType'\r\n```\r\nThis is the only error I found in the failing tests which could be related this PR.","That syntax is not supported in Python < 3.10. `from __future__ import annotations` at the top of the file might work. Keeping the typing in a stub `.pyi` file may be easier to work with for now. Alternatively, you can use `Union`, however, I think the linter may complain about that (not sure).","Thanks @lucascolley, for the prompt answer. It could have been that I asked this question before :sweat_smile: \r\nUnfortunately, `python dev.py lint` does not complain...","@yagizolmez would you be interested in reviewing this given your heavy involvement in previous discussions?","@j-bowhay I will leave a review in the upcoming days. I want to note that this is NOT a competing PR with #19814. Following the discussion we had there, I decided to break up the envelope function I proposed and not implement the analytical envelope.\r\n\r\n@DietBru I  have a quick question before I leave a more detailed review. Comparing this function with `hilbert`, major additions seem to be:\r\n\r\n1. Support for complex-valued signals.\r\n2. The option to upsample the envelope.\r\n3. The option to apply a bandpass filter.\r\n\r\nHave you considered extending `hilbert` rather than adding a new function?","> Have you considered extending hilbert rather than adding a new function?\r\n\r\nI did, but I came to the conclusion that that would only lead to confusion:\r\n\r\nThe [analytic signal](https:\/\/en.wikipedia.org\/wiki\/Analytic_signal#Complex_envelope\/No) is a concept based on  real-valued signals. Rightfully, [hilbert()](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.hilbert.html) raises a `ValueError` for complex-valued inputs. Also, changing that behavior could break existing implementations.\r\n\r\n The envelope on the other hand is a concept that is inherent to complex-valued signals with real-valued signals being a special case. Note that the treatment of  real-valued signals depends on the underlying assumptions of what the signal represents. This is illustrated in the second example of the `envelope()`'s doc string. \r\n\r\n\r\n   ","@DietBru I am sorry for the delay. I have been very busy these days, so I am not yet able to provide a detailed review. One thing catched my eye. Under certain circumstances, this function must give the same result with `hilbert()`, i.e. when `z` is real and `bp_in` is selected appropriately. Have you considered writing some tests and benchmarks covering these situations?","> One thing catched my eye. Under certain circumstances, this function must give the same result with `hilbert()`, ...\r\n\r\nGood catch, thank you. I found an edge case where the results differ. Hence, I did some refactoring to ensure that `envelope()` and `hilbert()` always behave identically. I think it should ready for complete review now."],"labels":["enhancement","scipy.signal"]},{"title":"ENH: explicit fit for generalized Pareto distribution (genpareto)","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI come across this when browsing through general methods for fitting the location parameter of a distribution. SciPy already contains explicit `fit` methods for `pareto` (gh-12545) and `powerlaw` (gh-13053) distributions. It would be nice to \u201ccomplete the puzzle\u201d by implementing `fit` for the `genpareto` distribution. This would also tick a box in gh-17832.\r\n\r\n### Describe the solution you'd like.\r\n\r\nThis can be broken down into a few steps:\r\n\r\n1. Improve the documentation regarding the relation between `genpareto`, `pareto`, and `powerlaw`. The three actually share the same density function, but with different domain of the shape parameter. As a result, once the fit method of the latter two are implemented, the first one is ready almost \u201cfor free\u201d.\r\n\r\n2. Decide what to do when the _global_ maximum of the likelihood function is unbounded. At the moment this is handled differently between `pareto` and `powerlaw`: in the former, a _local_ maximum is sought and returned; in the latter, some artificial value \u201cjust\u201d within bound is returned. Literature appears to suggest to use local MLE when it exists, and to conclude that MLE is not appropriate when no local maximum exists and the global maximum is unbounded. I prefer this approach as it is more prudent than returning some spurious value \u201cjust\u201d within bound.\r\n\r\n3. Review the fit method of `pareto` and `powerlaw`. \r\n- When location or scale is known, explicit formulas are used and there\u2019s nothing more to do. \r\n- When location and\/or scale is unknown but shape is known, a small improvement could be made to the current implementation to bound the root of the first derivative of the log likelihood function by explicit formulas rather than trial and error.\r\n- When location, shape and scale are all unknown, a method is described in [Grimshaw (1993)](https:\/\/www.tandfonline.com\/doi\/abs\/10.1080\/00401706.1993.10485040). It\u2019s essentially the same as what `pareto.fit` does (local MLE), but with more elaborate discussion on the property of the likelihood function and its roots (existence and multiplicity). I need to look more closely to see whether the current method is adequate in the (not so corner) corner cases.\r\n\r\n4. Implement `genpareto.fit` reusing the code of `pareto.fit` and `powerlaw.fit`.\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["The relationship between `genpareto`, `pareto`, and `powerlaw` is as follows. A `genpareto` distribution with shape $c \\ne 0$, location $m \\in \\mathbb{R}$, and scale $s  > 0$ has density\r\n\r\n$$\r\nf(x;c,m,s)=\\frac{1}{s}\\left[1+c\\left(\\frac{x-m}{s}\\right)\\right]^{-\\frac{1}{c}-1}\r\n$$\r\n\r\nsubject to $x \\ge m$ if $c > 0$, and $m\\le x \\le m-s\/c$ if $c < 0$.\r\n\r\nA `pareto` distribution with shape $b > 0$, location $m_1 \\in \\mathbb{R}$, and scale $s_1 > 0$ has support $x \\ge m_1+s_1$ and density\r\n\r\n$$\r\n\\begin{align}\r\nf_1(x;b,m_1,s_1)&=\\frac{b}{s_1}\\left(\\frac{x-m_1}{s_1}\\right)^{-b-1}\\\\\r\n&=\\frac{1}{s_1b^{-1}}\\left[1+b^{-1}\\left(\\frac{x-(m_1+s_1)}{s_1b^{-1}}\\right)\\right]^{-\\frac{1}{b^{-1}}-1}\\\\\r\n&=f(x;b^{-1}, m_1+s_1, sb^{-1})\r\n\\end{align}\r\n$$\r\n\r\nThis shows `pareto` is a reparameterization of `genpareto` with positive shape parameter.\r\n\r\nA `powerlaw` distribution with shape $a > 0$, location $m_2 \\in \\mathbb{R}$, and scale $s_2 > 0$ has support $m_2 \\le x \\le m_2+s_2$ and density\r\n\r\n$$\r\n\\begin{align}\r\nf(x;a,m_2,s_2) &=\\frac{a}{s_2}\\left(\\frac{x-m_2}{s_2}\\right)^{a-1}\\\\\r\n&=\\frac{1}{s_2 a^{-1}} \\left[1+(-a^{-1})\\frac{(m_2+s_2)-x}{s_2 a^{-1}}\\right]^{-\\frac{1}{-a^{-1}}-1}\\\\\r\n&=f(-x;-a^{-1},-m_2-s_2,s_2a^{-1})\r\n\\end{align}\r\n$$\r\n\r\nThis shows that if $X \\sim \\mathrm{powerlaw}(a,m_2,s_2)$, then $-X\\sim \\mathrm{genpareto}(-a^{-1}, -m_2-s_2, s_2a^{-1})$.","This sounds like a sensible plan. Adding a fit method that basically reuses an existing one should be doable witout too much effort. Extending beyond what is done for `powerlaw` I would not advise at the moment. Reliably fitting distributions is just hard in principle and has drained a lot of time for both contributors and maintainers in the past (see for example the original powerlaw override: #13053). I have been bitten by that too, improving `vonmises.fit` took four PRs overall.\r\n\r\nAt this point, I would like to advertise also the still relatively new [`stats.fit`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.fit.html) function. It uses a global optimizer, so is typically more reliable than the `fit` methods of many distributions but slower.","Thanks for the comment @dschmitz89. I didn\u2019t know that `stats.fit` contains the \u201cmaximum product of spacing\u201d method. It\u2019s an interesting one and worth exploring.\r\n\r\nAs far as MLE is concerned, a global optimizer doesn\u2019t help for the genpareto case because we actually don\u2019t want a global maximum but a local one, because the global maximum tends to the boundary and is inconsistent, while the local maximum has the usual desirable properties of MLE.\r\n\r\nAnd I agree it requires a lot of work to improve the fit for a particular distribution. But that\u2019s the exact scenario that\u2019s suitable for a library to provide, like the tedious special math functions. Once you work on a few of them you get more experience and can do the others quicker and better :-) In addition, your code and method can be a valuable reference for others.\r\n\r\nOn the other hand, since the maintenance also requires effort, it\u2019s best to spend the effort in widely used distributions and where MLE is useful. `genpareto` is a good candidate here because it\u2019s popular and MLE, where it exists, is consistent and efficient.\r\n\r\nBtw, I took a quick look at `vonmise.fit` and it seems a small improvement could be made to bound the root of kappa. Let me revisit that in a separate ticket."],"labels":["scipy.stats","enhancement"]},{"title":"ENH: Add parameter estimation to scipy.stats.dirichlet","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI encountered the need to do in some data analysis scenarios, where you use the Dirichlet distribution as a Bayesian prior and want a way to set it's parameters reasonably based on data. (E.g. you use the dirichlet distribution as a prior to a categorical distribution describing the shop where a product will be sold. You need to use the prior to compensate for lack of data when the product is new. But you may want to estimate the prior parameters based on where similar products were selling.)\r\n\r\n### Describe the solution you'd like.\r\n\r\nA relatively simple implementation is here https:\/\/gist.github.com\/dongwookim-ml\/b890fd8f213701e0c693\r\n\r\nA paper describing the theory: http:\/\/jonathan-huang.org\/research\/dirichlet\/dirichlet.pdf\r\n\r\n### Describe alternatives you've considered.\r\n\r\nI use a code adapted from the one linked above to do this. But in my opinion it's within the scope of scipy to have this functionality.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nMany other common probability distributions already support this via the fit() method","comments":["Thanks for the suggestion and the literature review @jachymb. In principle this is in scope and a useful feature. Currently, only two multivariate distributions can be fitted: multivariate normal and von Mises. I had a quick look at the reference which looks quite good. One potentially helpful bit for the inverse digamma function already exists in scipy: https:\/\/github.com\/scipy\/scipy\/blob\/561815a1ac890bdc40dbad4ca18abcb724e752fb\/scipy\/stats\/_continuous_distns.py#L3238\r\n\r\nBe aware that reliably fitting distributions for a wide parameter range often turns out more difficult than expected. See for example these past PRs: #13053 , #16782.\r\n\r\nWould you be interested in creating a PR? In case, implementing the fix point method from the reference is likely the most robust and easiest way to get it into SciPy as it has the least numerical issues according to the summary."],"labels":["scipy.stats","enhancement"]},{"title":"BUG: `mmwrite` order guarantee?","body":"### Describe your issue.\r\n\r\nThis is a unique use case, but I am trying to sort the coordinates triplets (row, col, data) in mtx files. I am solely using scipy.sparse and io to \r\n1. handle IO with `*.mtx` files\r\n2. load .mtx file into a COO format via Scipy Sparse\r\n3. sort by row, then by col\r\n4. then save back to a new `*_sorted.mtx` file\r\n\r\nHowever, I'm finding that step 4 saves properly, but the ordering is wrong. I made sure that the indices are sorted in the SciPy Sparse COO Object before saving, but seems like there is no ordering guarantee? \r\n\r\nThis is not blocking issue for me, but I just manually save the mtx file by directly writing with Python's File IO. This solution works for me.\r\n\r\nI cannot recall if you internally sort during any operations performed on sparse matrices, but I have to do it because I'm benchmarking a bunch of vendor sparse libraries (Intel MKL Sparse, cuSPARSE, etc) in a C\/C++ codebase. I'm finding that there are some requirements that a matrix needs to sorted for a certain sparse format from the vendor library. Rather than sorting them every time I load in memory, I'd rather save a lot more time pre-sorting the mtx files.\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.sparse as sp\r\nfrom scipy.io import mmread, mmwrite\r\n\r\nnew_dataset = []\r\n\r\ndef sort_coo(m):\r\n    tuples = zip(m.row, m.col, m.data)\r\n    return sorted(tuples, key=lambda x: (x[0], x[1]))\r\n\r\ndef sort_mtx_file(file_path):    \r\n    # Load the matrix file\r\n    matrix = mmread(file_path)\r\n    \r\n    # Convert to COO format\r\n    coo = matrix.tocoo()\r\n    mdata = sort_coo(coo)\r\n    \r\n    sorted_row = np.array([e[0] for e in mdata])\r\n    sorted_col = np.array([e[1] for e in mdata])\r\n    sorted_data = np.array([e[2] for e in mdata])\r\n    \r\n    \r\n    # Create a new COO matrix with the sorted data\r\n    sorted_matrix = sp.coo_matrix((sorted_data, (sorted_row, sorted_col)), shape=coo.shape)\r\n    \r\n    # print first 10 entries of the sorted matrix\r\n    print(sorted_matrix.row[:40])\r\n    print(sorted_matrix.col[:40])\r\n    \r\n    # Write the sorted matrix back to the file, preserving metadata\r\n    # write to a new file to avoid overwriting the original\r\n    file_path = file_path.replace('.mtx', '_sorted.mtx')\r\n    new_dataset.append(file_path)\r\n    mmwrite(file_path, sorted_matrix)\r\n    print(f\"Saved sorted matrix to {file_path}\")\r\n\r\n# Read the list of MTX file paths\r\nwith open('dataset.txt', 'r') as f:\r\n    mtx_files = f.read().splitlines()\r\n\r\n# Sort each MTX file\r\nfor file_path in mtx_files:\r\n    sort_mtx_file(file_path)\r\n    print(f\"Sorted {file_path}\")\r\n\r\n# save new_dataset to a file called new_dataset.txt\r\nwith open('new_dataset.txt', 'w') as f:\r\n    for file_path in new_dataset:\r\n        f.write(file_path + '\\n')\r\n\r\n\r\nprint(\"All files have been sorted and saved.\")\r\n```\r\n\r\n\r\n### Error message\r\n\r\nI encountered this with `ex21_sorted.mtx` from SuiteSparse Matrix Collection \r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.10.0 1.24.1 sys.version_info(major=3, minor=8, micro=13, releaselevel='final', serial=0)\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblis_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\n```\r\n","comments":["CC @alugowski , looks like there might be a fast matrix market related regression. Would you have time to take a look?"],"labels":["defect","scipy.io"]},{"title":"ENH: Translate complex valued hyp2f1 to C++ and make improvements","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nCloses #8083\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nThis PR translates `hyp2f1` into CUDA compatible C++, including both the parts from written in Cython in `hyp2f1.pxd` and the parts written in Fortran in `specfun.f`. Improvements have been made by \r\n\r\n- Choosing better domains for using the different linear transformations as suggested by @FormerPhysicist in #8151 (though I've not used the same domains he used).\r\n- Using tolerance `EPS = 1e-15 throughout` and increasing the maximum number of iterations. (Zhang and Jin used a lower tolerance of `EPS = 1e-8` for cases their implementation could not handle well. The improvements made here make this unnecessary.\r\n- Fix the case where `a - b` is an integer and the smaller of `c - a` or `c - b` is a positive integer. A different limiting formula is needed in this case due to a singularity in a digamma function appearing in the terms of a series.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nHere is a table comparing percentiles for relative error in this PR and main over 3,003,855 test cases generated by the script https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/special\/_precompute\/hyp2f1_data.py, using default arguments, with the exception that parameter group 9 is excluded, and excluding cases where the true result is $\\infty$ which both `main` and this PR branch get right in all cases. These tests were run with gcc 11.4 on Linux.\r\n\r\nThe improvement is substantial, although note that there are still some cases where the new implementation fails. Fixing these cases will require implementing argument reduction of `a`, `b`, and `c` using [Miller's Recurrence Algorithm](https:\/\/en.wikipedia.org\/wiki\/Miller%27s_recurrence_algorithm). \r\n\r\n\r\n\r\n| -         |        30% |         40% |          50%|         60% |         70% |         80% |         90% |        max |\r\n|---------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\r\n| main | 1.06e-15  | 3.70e-15 | 9.02e-13 | 8.02e-11 | 8.69e-10 | 5.20e-09 | 3.99e-08 | inf         |\r\n| PR      | 5.25e-16 | 8.06e-16 | 1.27e-15 | 2.25e-15 | 5.19e-15 | 2.73e-14  | 2.81e-12  |2.02+44 |\r\n\r\nHere's a breakdown of the highest percentiles.\r\n\r\n| -|         91% |         92% |         93% |         94% |         95% |         96% |         97% |         98% |       99% |\r\n|------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-----------|\r\n| main       | 5.94e-08 | 9.49e-08 | 1.72e-07 | 3.66e-07 | 9.61e-07 | 3.74e-06 | 3.27e-05 | 0.0016  | 15.74   |\r\n| PR | 6.75e-12 | 1.83e-11 | 5.77e-11 | 2.23e-10  | 1.16e-09 | 9.40e-09 | 1.85e-07  | 2.47e-05 |  0.22 |\r\n\r\n**Update**: I've rounded to two significant figures and cut out some of the percentiles in order to get the tables to fit properly. The results for the 10th and 20th percentiles are comparable between main and this PR. (slightly better for this PR).\r\n\r\nThe example `hyp2f1(1, 1, 4, 3 + 4j)` from https:\/\/github.com\/scipy\/scipy\/issues\/1561 is now computed accurately.\r\n\r\n```python\r\nfrom mpmath import mp\r\nfrom scipy.special import hyp2f1\r\n\r\nhyp2f1(1, 1, 4, 3 + 4j)\r\n# (0.49234384000963594+0.6051340616612398j)\r\n\r\nmp.hyp2f1(1, 1, 4, 3 + 4j)\r\n# mpc(real='0.49234384000963544', imag='0.60513406166123973')\r\n```\r\nand has been added to the tests. This case was broken in Zhang and Jin because they did not account for the need to use a a different limiting formula in the case where both `a - b` is an integer and the smaller of `c - a` and `c - b` is a positive integer. The need for this was noted by @gertingold in https:\/\/github.com\/scipy\/scipy\/issues\/1561. I don't think this this is enough to close that issue, since other inaccuracies are mentioned there. Closing that will have to wait until argument reduction is implemented.\r\n\r\nI have implemented series evaluators in `scipy\/special\/special\/tools.h` which are used in this implementation. These evaluators take generators for the terms of a series as input, and using them has streamlined the implementation. I've confirmed that everything I've done here works on CuPy.\r\n\r\nI'm pretty confident in the mathematical correctness due to the large set of test cases in https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/special\/_precompute\/hyp2f1_data.py, and the extreme care I've taken.\r\n\r\ncc @izaid","comments":["Huh, those tables looked nice in the preview, but are unreadable in the actual post.","closing the 4th oldest open issue - not bad \ud83d\ude09","Things are finally coming together \ud83d\ude03 ","The failing test case is new to this PR and just needs a tolerance bump. It\u2019s not a regression. \r\n\r\nUpdate: OK, there actually are some new failures on Mac OS, and arm64\/aarch64. In aggregate, results have dramatically improved, so I think it's fine to just bump those tolerances.","I've also translated the real valued `hyp2f1` from cephes into C++, which I can either add here, or do in a follow-up. I think that will need to go in before we can add `hyp2f1` to CuPy.\r\n\r\nhttps:\/\/github.com\/steppi\/scipy\/blob\/hyp2f1_cephes\/scipy\/special\/special\/cephes\/hyp2f1.h","@izaid, I think I've addressed everything. Thanks for the suggestions!","Thanks @steppi! Looks good to me. ","> Thanks @steppi! Looks good to me.\r\n\r\nThanks @izaid!"],"labels":["enhancement","scipy.special","C\/C++","Cython"]},{"title":"ENH: array types: add JAX support","body":"#### Reference issue\r\nTowards gh-18867\r\n\r\n#### What does this implement\/fix?\r\nFirst steps on JAX support. To-do:\r\n\r\n- Check device support (GPU\/TPU)\r\n- Get existing tests to pass \/ skip where needed\r\n- Add to existing tests where needed\r\n- Update the docs\r\n\r\n#### Additional information\r\nJAX is complaining locally about trying to modify immutable arrays. Other than that it has already helped us catch some importability in `cluster`.\r\n\r\nCan do the same for `dask.array` once the problems are fixed over at https:\/\/github.com\/data-apis\/array-api-compat\/pull\/89.","comments":["(The reason I decided to comment over on the DLPack issue is that I recall a conversation about how portability could be increased if we replace occurrences of `np.asarray` with `{array_api_compat.numpy, np>=2.0}.from_dlpack}`. Clearly, portability past libraries which are coercible by `np.asarray` is very low prio at the minute, but something to consider long-term. Also, DLPack being the idiomatic way to do library-interchange, rather than relying on the array-creation function `asarray`)","Thanks for working on this Lucas. JAX support will be very nice. And a third library with CPU support (after NumPy and PyTorch) will also be good for testing how generic our array API standard support actually is. \r\n\r\nOkay, related to the read-only question, it looks like this is the problem you were seeing:\r\n```\r\nscipy\/cluster\/hierarchy.py:1038: in linkage\r\n    result = _hierarchy.mst_single_linkage(y, n)\r\n        method     = 'single'\r\n        method_code = 0\r\n        metric     = 'euclidean'\r\n        n          = 6\r\n        optimal_ordering = False\r\n        xp         = <module 'jax.experimental.array_api' from '\/home\/rgommers\/mambaforge\/envs\/scipy-dev-jax\/lib\/python3.11\/site-packages\/jax\/experimental\/array_api\/__init__.py'>\r\n        y          = array([1.48660687, 2.23606798, 1.41421356, 1.41421356, 1.41421356,\r\n       2.28254244, 0.1       , 1.48660687, 1.48660687, 2.23606798,\r\n       1.        , 1.        , 1.41421356, 1.41421356, 0.        ])\r\n_hierarchy.pyx:1015: in scipy.cluster._hierarchy.mst_single_linkage\r\n    ???\r\n<stringsource>:663: in View.MemoryView.memoryview_cwrapper\r\n    ???\r\n<stringsource>:353: in View.MemoryView.memoryview.__cinit__\r\n    ???\r\nE   ValueError: buffer source array is read-only\r\n```\r\n\r\nThe problem is that Cython doesn't accept read-only arrays when the signature is a regular memoryview. There's a long discussion about this topic in https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10624. Now that we have Cython 3 though, the fix is simple:\r\n```diff\r\ndiff --git a\/scipy\/cluster\/_hierarchy.pyx b\/scipy\/cluster\/_hierarchy.pyx\r\nindex 814051df2..c59b3de6a 100644\r\n--- a\/scipy\/cluster\/_hierarchy.pyx\r\n+++ b\/scipy\/cluster\/_hierarchy.pyx\r\n@@ -1012,7 +1012,7 @@ def nn_chain(double[:] dists, int n, int method):\r\n     return Z_arr\r\n \r\n \r\n-def mst_single_linkage(double[:] dists, int n):\r\n+def mst_single_linkage(const double[:] dists, int n):\r\n     \"\"\"Perform hierarchy clustering using MST algorithm for single linkage.\r\n \r\n     Parameters\r\n```\r\n\r\nThis makes the tests pass (at least for this issue, I tried with the `dendrogram` tests only). The `dists` input to `mst_single_linkage` isn't modified in-place, so once we tell Cython that by adding `const`, things are happy. ","thanks! I've removed the copies and added some `const`s to the Cython file to get the tests to pass. Still some failures for in-place assignments with indexing but we can circle back to those once we get integration with the test skip infra.","> One question I have here, which is probably a question more broadly for the array API: as written, much of the JAX support added here will not work under `jax.jit`, because it requires converting array objects to host-side buffers,\r\n\r\nDo you mean for testing purposes, or for library code? For the latter: we should never do device transfers like GPU->host memory under the hood. The array API standard design was careful to not include that. It wasn't even possible at all until very recently, when a way was added to do it with DLPack (for testing purposes).\r\n\r\nIf you mean \"convert to `numpy.ndarray` before going into Cython\/C\/C++\/Fortran code inside SciPy, then yes that is happening. That's kinda not an array API standard issue, because it's leaving Python - and that's a very different problem. To avoid compiled code inside SciPy - which indeed won't work with any JIT compiler unless that JIT is specifically aware of the SciPy functionality being called - it'd be necessary to have either a pure Python path (slow) or a matching API inside JAX that can be called (`jax.scipy` has some that we should be deferring to here).\r\n\r\n> and this is not possible during tracing when the array objects are abstract. JAX has mechanisms for this (namely custom calls and\/or pure_callback) but the array API doesn't seem to have much consideration for this kind of library structure. Unfortunately, I think this will severely limit the usefulness of these kinds of implementations. I wonder if the array API could consider this kind of limitation?\r\n\r\nJIT compilers were explicitly considered, and nothing in the standard should be JIT-unfriendly, except for the few clearly marked as [data-dependent output shapes](https:\/\/data-apis.org\/array-api\/latest\/design_topics\/data_dependent_output_shapes.html) and the few dunder methods that are [also problematic for lazy arrays](https:\/\/data-apis.org\/array-api\/latest\/design_topics\/lazy_eager.html).","> Do you mean for testing purposes, or for library code? For the latter\r\n\r\nIf this is what you meant, x-ref the 'Dispatching Mechanism' section of gh-18286","I mean for actual user-level code: most of the work here will be more-or-less useless for JAX users because array conversions via dlpack cannot be done under JIT without some sort of callback mechanism.","Okay, I had a look at https:\/\/jax.readthedocs.io\/en\/latest\/tutorials\/external-callbacks.html and understand what you mean now. `jax.pure_callback` looks quite interesting indeed. I wasn't familiar with it, but it looks like that may actually solve an important puzzle in dealing with compiled code. It doesn't support GPU execution or auto-differentiation, but getting `jax.jit` and `jax.vmap` to work would be a significant step forward.\r\n\r\nIt looks fairly straightforward to support (disclaimer: I haven't tried it yet). It'd be taking this current code pattern:\r\n```python\r\n# inside some Python-level scipy function with array API standard support:\r\n\r\nx = np.asarray(x)\r\nresult = call_some_compiled_code(x)\r\nresult = xp.asarray(result)  # back to original array type\r\n```\r\n\r\nand replacing it with something like (untested):\r\n```python\r\ndef call_compiled_code_helper(x, xp):  # needs *args, *kwargs too\r\n    if is_jax(x):\r\n        result_shape_dtypes = ... # TODO: figure out how to construct the needed PyTree here\r\n        result = jax.pure_callback(call_some_compiled_code, result_shape_dtypes, x)\r\n    else:\r\n        x = np.asarray(x)\r\n        result = call_some_compiled_code(x)\r\n        result = xp.asarray(result)\r\n```\r\n\r\nUse of a utility function like `call_compiled_code_helper` may even make the code shorter and easier to understand. It seems feasible at first sight.\r\n\r\nIt's interesting that `jax.pure_callback` transforms JAX arrays to NumPy arrays under the hood already.","Yeah, something like that is what I had in mind, though `pure_callback` is probably not the right mechanism. JAX doesn't currently have an easy pure-callback-like mechanism for executing custom kernels on device, without the round-trip to host implied by `pure_callback`. I wonder if this kind of thing will be an issue for other array API libraries?",">  I wonder if this kind of thing will be an issue for other array API libraries?\r\n\r\nIt is (depending on your defintion of \"issue\") because there's no magic bullet that will do something like take some native function implemented in C\/Fortran\/Cython inside SciPy and make that run on GPU. \r\n\r\nThe basic state of things is:\r\n- functions implemented in pure Python are unproblematic, and with array API support get to run on GPU\/TPU, gain autograd support, etc.\r\n    - with a few exceptions: functions using `unique` and other data-dependent shapes, iterative algorithms with a stopping\/branching criterion that requires eager evaluation, functions using in-place operations.\r\n- as soon as you hit compiled code, things get harder. everything that worked before with numpy only will still work, but autograd and GPU execution won't\r\n\r\n> JAX doesn't currently have an easy pure-callback-like mechanism for executing custom kernels on device, without the round-trip to host implied by `pure_callback`.\r\n\r\nIn a generic library like SciPy it's almost impossible to support custom kernels on device. Our choices for arrays that don't live on host memory are:\r\n- find a matching function in the other library. e.g., we can explicitly defer to everything in `jax.scipy`, `cupyx.scipy` and `torch.fft\/linalg\/special`,\r\n- raise an exception\r\n- do an automatic to\/from host roundtrip (we haven't considered this a good idea before, since data transfers can be very expensive - but apparently that's what `pure_callback` prefers over raising)\r\n\r\n","I gave adding Dask another shot just now, but unfortunately things are missing from `dask.array` like `float64`, which makes most of our test code fail. Perhaps we will have to change to using the wrapped namespaces throughout the tests (this is awkward because we still need to imitate an array from the unwrapped namespace being input).\r\n\r\nx-ref https:\/\/github.com\/dask\/dask\/issues\/10387#issuecomment-1968929405","I'd suggest keeping this PR focused on JAX and getting that merged first. That makes it easier to see (also in the future) what had to be done only for JAX. And if we're going to experiment a bit with `jax.jit`, this PR may grow already.","Something is awry in the Array API CI job - `-b jax.numpy` works locally for me, but it isn't added to the dict of available backends in the CI job. This means that either\r\n1) `import jax.numpy` is raising an `ImportError` despite `python -m pip install jax` being included in the job (could be that I'm missing something in the job), or\r\n2) PyTest is somehow using the wrong `conftest.py` (some sort of caching issue, maybe??), or\r\n3) Something else that I haven't thought of.\r\n\r\nEDIT: I suspect a circular dependency problem as `pip install jax` is installing `scipy`.\r\n\r\nEDIT 2: never mind, `pip install \"jax[cpu]\"` worked!\r\n","Okay the CI failures should be real and total now. May be a few extra bits to add to `test_array_api`. Lots of failures in `fft` but it looks like `cluster` (with the added skips) and `special` are happy now!","It's exposed that how we handle creation functions like `fftfreq` still isn't quite right. We either need to:\r\n\r\n1) require that the namespace passed to the `xp` kwarg is array API compatible, or\r\n2) generate a compatible namespace in `fftfreq` based on the passed namespace.\r\n\r\nWe didn't catch this yet since `fft` isn't available in `array_api_compat`. The error is caused by calling `scipy.fft.fftfreq(n, xp=jax.numpy)`.","> It's exposed that how we handle creation functions like fftfreq still isn't quite right. We either need to:\r\n> 1. require that the namespace passed to the xp kwarg is array API compatible, or\r\n> 2. generate a compatible namespace in fftfreq based on the passed namespace.\r\n\r\nI've opted for (1), since that is what we want eventually. I figure it's better to have the temporary shim in the tests (where if it stays the inefficiency doesn't matter), rather than in the public function where it would have to be changed.\r\n\r\n"],"labels":["enhancement","scipy.cluster","scipy._lib","scipy.fft","Cython","array types"]},{"title":"BUG: Jensen shannon returns NaN (takes sqrt of small negative number)","body":"### Describe your issue.\n\nFor some cherry-picked pairs of distributions that are near one another, jensenshannon returns \"nan\".\r\n\r\nThe cause is that the code takes the sqrt of a small negative number [here](https:\/\/github.com\/scipy\/scipy\/blob\/9d08eb58205fb669ff72c60080b18be61b14d8e1\/scipy\/spatial\/distance.py#L1272).\r\n\r\nYou might fix it with\r\n\r\n`js = np.clip(js, a_min=0.0, a_max=None)`\r\n\r\nbefore the sqrt\r\n\r\nThanks for scipy!\r\n\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nfrom scipy.spatial.distance import jensenshannon\r\nimport struct\r\n\r\nc1 = np.array([0.027501475, 0.055202297], dtype='float32')\r\nc2 = np.array([0.027537677, 0.055334348], dtype='float32')\r\nprint(\"c1:\", c1)\r\nprint(\"c2:\", c2)\r\nprint(\"type:\", c1.dtype, \"shape:\", c1.shape)\r\n\r\n# returns nan!\r\nprint(\"first jsd:\", jensenshannon(c1, c2))\r\n\r\n# does not return nan\r\nc1[0]=c1[0]+0.000000001\r\nprint(\"second jsd:\", jensenshannon(c1, c2))\n```\n\n\n### Error message\n\n```shell\nc1: [0.02750148 0.0552023 ]\r\nc2: [0.02753768 0.05533435]\r\ntype: float32 shape: (2,)\r\n\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\/python3.12\/site-packages\/scipy\/spatial\/distance.py:1259: RuntimeWarning: invalid value encountered in sqrt\r\n  return np.sqrt(js \/ 2.0)\r\nfirst jsd: nan\r\nsecond jsd: 0.0001306777430188037\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nThe bug occurs on the new versions of scipy, numpy, python shown below, but it also happens for 1.10.1, 1.24.3, 3.8.15 respectively.\r\n\r\n\r\n1.11.4 1.26.3 sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\r\n\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\/python3.12\/site-packages\/scipy\/__config__.py:146: UserWarning: Install `pyyaml` for better output\r\n  warnings.warn(\"Install `pyyaml` for better output\", stacklevel=1)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"clang\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"14.0.6\",\r\n      \"commands\": \"x86_64-apple-darwin13.4.0-clang\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"0.29.36\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"clang\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"14.0.6\",\r\n      \"commands\": \"x86_64-apple-darwin13.4.0-clang++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"11.2.0\",\r\n      \"commands\": \"\/Users\/builder\/cbouss\/perseverance-python-buildout\/croot\/scipy_1701810578094\/_build_env\/bin\/x86_64-apple-darwin13.4.0-gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.13.1\",\r\n      \"include directory\": \"..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.12\/site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"darwin\"\r\n    },\r\n    \"build\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"darwin\"\r\n    },\r\n    \"cross-compiled\": false\r\n  },\r\n  \"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"mkl-sdl\",\r\n      \"found\": true,\r\n      \"version\": \"2023.1\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/include\",\r\n      \"lib directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\",\r\n      \"openblas configuration\": \"unknown\",\r\n      \"pc file directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"mkl-sdl\",\r\n      \"found\": true,\r\n      \"version\": \"2023.1\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/include\",\r\n      \"lib directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\",\r\n      \"openblas configuration\": \"unknown\",\r\n      \"pc file directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/lib\/pkgconfig\"\r\n    },\r\n    \"pybind11\": {\r\n      \"name\": \"pybind11\",\r\n      \"version\": \"2.10.4\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/include\"\r\n    }\r\n  },\r\n  \"Python Information\": {\r\n    \"path\": \"\/Users\/mep\/anaconda3\/envs\/scipy-bug\/bin\/python\",\r\n    \"version\": \"3.12\"\r\n  }\r\n}\n```\n","comments":["This may be related to gh-19436 and gh-19438.\r\n\r\nI think we're having issues a bit farther upstream in the control flow, and we probably should never end up with negative values anywhere, since JSD is lower bounded at `0` in the first place.","yes @tylerjereddy you're right, it's upstream.\r\n\r\nThe rel_entr should always be non-negative but if I add some print statements to the scipy.special.jensenshannon code:\r\n\r\n```\r\nfrom scipy.special import rel_entr\r\ndef jensenshannon(p, q, base=None, *, axis=0, keepdims=False):\r\n    p = np.asarray(p)\r\n    q = np.asarray(q)\r\n    p = p \/ np.sum(p, axis=axis, keepdims=True)\r\n    q = q \/ np.sum(q, axis=axis, keepdims=True)\r\n    m = (p + q) \/ 2.0\r\n    left = rel_entr(p, m)\r\n    right = rel_entr(q, m)\r\n    print(\"left\", left, \"right\", right)\r\n    left_sum = np.sum(left, axis=axis, keepdims=keepdims)\r\n    right_sum = np.sum(right, axis=axis, keepdims=keepdims)\r\n    print(\"left_sum\", left_sum, \"right_sum\", right_sum)\r\n    js = left_sum + right_sum\r\n    if base is not None:\r\n        js \/= np.log(base)\r\n    print(\"js\", js)\r\n    js = np.clip(js, a_min=0.0, a_max=None)\r\n    return np.sqrt(js \/ 2.0)\r\n```\r\n\r\nthen my example \r\n\r\n```\r\nc1 = np.array([0.027501475, 0.055202297], dtype='float32')\r\nc2 = np.array([0.027537677, 0.055334348], dtype='float32')\r\nprint(\"c1:\", c1)\r\nprint(\"c2:\", c2)\r\nprint(\"type:\", c1.dtype, \"shape:\", c1.shape)\r\n\r\nprint(\"first jsd:\", jensenshannon(c1, c2))\r\nc1[0]=c1[0]+0.000000001\r\nprint(\"second jsd:\", jensenshannon(c1, c2))\r\n```\r\n\r\nreturns negative numbers from rel_entr in left[1] and right[0] in this case:\r\n\r\n```\r\nc1: [0.02750148 0.0552023 ]\r\nc2: [0.02753768 0.05533435]\r\ntype: float32 shape: (2,)\r\nleft [ 0.00011914 -0.00011914] right [-0.00011913  0.0001191 ]\r\nleft_sum 2.1682354e-09 right_sum -2.7626811e-08\r\njs -2.5458576e-08\r\nfirst jsd: 0.0\r\nleft [ 0.00011914 -0.00011914] right [-0.00011913  0.00011916]\r\nleft_sum 2.1682354e-09 right_sum 3.198511e-08\r\njs 3.4153345e-08\r\nsecond jsd: 0.0001306777430188037\r\n```\r\n\r\n\r\n","Not a good solution, but a possible workaround is using `scipy.special.kl_div` instead of `rel_entr` in `jensenshannon` to avoid negative values. \r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/5fd8c5a6e3978cb4e7b1b4551fc09a6be7777bf6\/scipy\/spatial\/distance.py#L1263-L1268"],"labels":["defect","scipy.spatial"]},{"title":"ENH: io: Read and write wav files of size > 4GB","body":"Replacement for #8529 to avoid rebasing.\r\n\r\nAdds the ability to open a .wav file with the `RF64` format for files with a file size > 4GB along with two types of tests.\r\nTo save storage space, I modified two of the existing small test files to instead be in RF64 (even though they are so small they normally would use `RIFF`.\r\nThere is another round-trip test that writes and reads a large (just over 4GB) file. This is relatively slow, so the test is marked `xslow`   ","comments":["Lucas seems reasonably positive here, but there is a suggestion we get a \"domain expert\" review.\r\n\r\nSince it isn't critical for the NumPy 2.0.0 support focus of the 1.13.0 release branching soon, I'll bump the milestone."],"labels":["enhancement","scipy.io"]},{"title":"RFC:DEP:Deprecate linalg.kron?","body":"`linalg.kron` is a basic, data-movement intensive array operation. There is hardly any computation done other than scalar times array and quite some shuffling needed for higher order nd arrays. It is similar to a very special case of `np.einsum` in higher dimensions. Hence it doesn't make too much sense for us to carry around a suboptimal version of it since Numpy already supports batch array input https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.kron.html\r\n\r\nFor comparison we also have Khatri-Rao product which is a Kronecker-like product and it is specific enough for us to have it but kron is not in my opinion.\r\n\r\nSee #20072 and https:\/\/github.com\/pymc-devs\/pytensor\/issues\/640 for more context. \r\n\r\n","comments":["Why do we have scipy.linalg.kron at all? Burn it with fire, I'd say. ","I had been meaning to propose this for a while as the numpy version also supports higher dimensions.","Then should I leave it to you @j-bowhay ? I'm not sure how the deprecation tracker works so no problem for me if you take over. ","> Then should I leave it to you @j-bowhay ? I'm not sure how the deprecation tracker works so no problem for me if you take over.\r\n\r\nYes I am happy to tackle this","This does not look like the right approach to me, for a few reasons:\r\n\r\n1. our `kron` isn't broken, just doesn't have as many features as the numpy version of it. It's also not in the wrong place or has a bad API. So in principle, the deprecation is just churn for users.\r\n2. `scipy.linalg` is kinda designed to be a large superset of `numpy.linalg`. And in principle can have higher performance (although that's not relevant here). So users who have SciPy as a dependency should be able to use just `scipy.linalg` rather than mix and match depending on the function\r\n3. we may want the SciPy versions to be generic over array types in the end, meaning our `kron` may end up providing a superset of functionality compared to the numpy version of it\r\n\r\nSignatures look compatible, so why not simply alias our `kron` to `numpy.linalg.kron` for now? That's less disruptive for users, easier to do that a deprecation trajectory, and keeps goals (2) and (3) alive.\r\n\r\nMinor procedural side note: please don't close deprecation proposal issues within a day before they're addressed. ","This also seems fine to me but I think we should then consider the linalg roadmap item \"Reduce duplication of functions with numpy.linalg\" which seems contradictory to 2). My initial support for this was under the premise of working towards this item.","Hmm good point. That was written a long time ago, and I don't think is correct. We basically have two modules, `fft` and `linalg`, that are designed as compatible supersets. Modulo a few hard-to-resolve and unintentional differences like the `qr` signature\/behavior.","This functionality is hardly a linalg thing. So I don't agree that it has to stay here. We need to separate array manipulation functions from actual linalg stuff. Transpose is also an array op but has no place in scipy to have its own copy here, we already removed tril and triu because this needs really C level manipulation otherwise would be very slow and messy. This distinction will help us to weed out algorithms from memory ops in the long run in my opinion. \r\n\r\n> please don't close deprecation proposal issues within a day before they're addressed.\r\n\r\nThis was more or less after folks agreed upon it. \r\n","I think this issue is gives a good point to step back and ask: what actually is `scipy.linalg`?\r\n\r\n1. The historic description was along the lines of \"provides LAPACK-accelerated versions of numpy.linalg functions\". Is it still relevant, is numpy using lapack-lite?\r\n2. A home for additional\/more advanced linalg primitives (qr_update etc)\r\n3. (new) An umbrella dispatcher to various backends (numpy, cupy, pytorch, jax etc) via Array API.\r\n\r\nI sort of sense a potential tension between 2  and 3, but that's separate from `kron`.\r\n\r\nNow, what is `scipy.linalg.kron`? It shares the name with a `numpy.linalg` function, but is not an accelerated or otherwise improved version of it. \r\n\r\nDoes it provide something new and useful? I don't see much --- a fourth slightly incompatible version of <s>`multiply.outer`<\/s> `np.outer`, `np.linalg.kron` and `scipy.linalg.khatri_rao` does not sound too convincing IMO.\r\n\r\nCan it be replicated on a user side? Yes, easily, the whole implementation is three LOC: https:\/\/github.com\/scipy\/scipy\/blob\/v1.12.0\/scipy\/linalg\/_special_matrices.py#L448-L486\r\n\r\nSo, yes, deprecating it is a backcompat break. However I think it's a good change, especially if made together with some fleshing out of where we want `scipy.linalg` to evolve.","These days I have been guilty of typing things a bit too terse and sound a bit pedestrian, so let me flesh out sentences a bit what I meant. \r\n\r\nEven though numpy and scipy has the namespace identical is probably a bit unfortunate because that gives the impression that NumPy and SciPy has the same goal but with different scope when it comes to linalg. It might also be explained as \"not as comprehensive as scipy but better than nothing\". I don't share this view but I can see how it can be articulated. \r\n\r\nIn my opinion NumPy is and should be the place that provides best-of-class array manipulations and memory tricks and SIMD and so on. If the operation has a mathy name, this should not be a block or denounce it from adding to NumPy. \r\n\r\nNow the question is, how would the user know whether it is a memory trick or a real full algorithm like LU or SVD. That is, I think, a very valid point and I don't have a conclusive answer to. However borrowing NumPy funcs into the scipy namespace with an alias does not seem like a good long term solution. We are removing them one by one almost exclusively so far. \r\n\r\nIn this particular case, we are actually a bit lucky because it lives under `numpy` namespace and not in `numpy.linalg` hence the confusion is less dramatic. \r\n\r\nLike I mentioned above, array tricks like `tile`, `fill` or `concatenate` and other things are rightly inside NumPy and I think they should be there. In my limited linalg view `kron` is just like tile but has a mathy name for it. Hence probably should better live in NumPy namespace.\r\n","It's difficult to weigh up the balance of the code-churn (which is clearly 'unnecessary' in at least some reasonable sense), and trying to slim `scipy.linalg` down (given that the duplication between SciPy and NumPy is definitely a point of confusion for users).\r\n\r\nWithout my array API hat on, I think the arguments given for why `linalg.kron` does _not_ belong in SciPy are strong. I think the code-churn could be justified by reducing confusion and improving maintainability long-term.\r\n\r\n---\r\n\r\n_However_, I think Evgeni's 3rd point is important. Sure, it is only 3 LOC, but not completely trivial, and (probably?) we will want a (somewhat) 'standard' array-agnostic implementation to live somewhere long-term. If that isn't SciPy, where else?\r\n\r\n1. The array API standard (main namespace? unlikely. `linalg` extension? perhaps?)\r\n2. A different existing package (which?)\r\n3. A new package, specifically for building array-agnostic functions _on top of_ the standard. Perhaps targeted at things between the array library and SciPy levels (e.g. `xp.cov`), which could then be candidates for additions to the standard.\r\n\r\nIf there are concrete plans for any of options 1-3, then I'd probably be slightly in favour of the deprecation.\r\n\r\nHowever, if there is a risk that we will deprecate, but then in future wish to _return_ `kron` to `scipy.linalg` (albeit serving the slightly different purpose of accepting as input any array library), that would almost certainly be a bad and confusing amount of code-churn. What do we think?","> If that isn't SciPy, where else?\r\n\r\nWhy not NumPy? It is already there. We are not removing from NumPy too. Only from `scipy.linalg`.\r\n\r\n> However, if there is a risk that we will deprecate, but then in future wish to return kron to scipy.linalg (albeit serving a slightly different purpose), that would almost certainly be a bad and confusing amount of code-churn. What do we think?\r\n\r\nI don't see how a kron operator would gain a different purpose. It has a very well defined operator definition just like `numpy.outer`. I would have made the same proposal if numpy had [block_diag](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.linalg.block_diag.html) but unfortunately it doesn't so scipy.linalg it is. Though it should have been NumPy all along.","Sorry, probably best to avoid a rapid back-and-forth here but just to clarify:\n\n> Why not NumPy?\n\nI'm talking specifically about the function being available regardless of which array library is used. Perfectly reasonable to say that every library should have it, but in that case we'd want a plan for it to be added to the standard.\n\n> I don't see how a kron operator would gain a different purpose.\n\nSorry, I phrased this poorly. I meant just the ability to take any array library as input, not just NumPy.",">  but in that case we'd want a plan for it to be added to the standard.\r\n\r\nI think I don't follow this part. It is already in NumPy namespace. Why should we the arbiter of what array api standard should have? We have not been active in the `linalg` parts of it so far. Also I think it is not in the standard anyways \r\n\r\nIn this case, we are removing things that are not suitable for us. What happens afterwards are the packages' prerogative as not every NumPy function is available in the other namespaces anyways.","> I think this issue is gives a good point to step back and ask: what actually is `scipy.linalg`?\r\n\r\nYes, that is a useful exercise indeed. Probably better in a fresh issue? This one is very specific.\r\n\r\n> So, yes, deprecating it is a backcompat break. However I think it's a good change, especially if made together with some fleshing out of where we want `scipy.linalg` to evolve.\r\n\r\nFirst deprecating and then thinking about the bigger picture and whether we need it seems clearly the wrong way around to me. Why not just make it an alias right now, and if someone actually has energy to work on defining the plan for `scipy.linalg` and pushing it forward, then it can either be deprecated or not.","> I think I don't follow this part. It is already in NumPy namespace\r\n\r\nPlease let's not do another round of this discussion right now? We've already been through this I'm sure. The tl;dr is: numpy is an array library, it will not be deferring to other array libraries. SciPy can support multiple array libraries.","OK I am definitely not following. How is array api relevant here? I am not even trying to start any discussion let alone that one in particular. It is already confusing enough. This function is not in the standard plus as I argued above it is not even suitable for us. This is equivalent to having `scipy.linalg.tile` in our codebase. \r\n\r\nI would like to understand why you want to keep it in return","There seems to be agreement that a `kron` which works only for NumPy arrays belongs in NumPy.\r\n\r\n---\r\n\r\nThe question is where a `kron` which works with _any_ array library belongs.\r\n\r\nThis may well not be SciPy - for many of the same reasons given in this thread for why it belongs on (or at least closer to) the array library level. When considering the following options:\r\n\r\n1. In the array libraries (i.e. `xp.kron` or `xp.linalg.kron`, in the standard),\r\n2. In SciPy, or\r\n3. In a different library (see my point (3) above),\r\n\r\nmy preference would be for option 1 or 3.\r\n\r\n_However_, we must acknowledge that option 2 is the closest to reality. Such a library as option 3 does not exist, and additions to the standard (and their implementations) are an involved process. Either one involves work. Option 2 would simply be a ~10 line PR.\r\n\r\n---\r\n\r\nThe point is _not_ that SciPy should be the arbiter of what belongs in the standard, nor that SciPy should be responsible for providing implementations that would otherwise be out of scope. Rather, at a time where we are asking \"what actually is `scipy.linalg`?\", support for multiple array libraries is a part of the conversation.\r\n\r\nI don't think anyone has made an argument for `scipy.linalg.kron` staying long-term. However, perhaps it would be best to figure out a plan based on the question \"what actually is `scipy.linalg`?\" (_in a separate issue_), and how (if at all) array-agnostic functions like this fit into it (in the short- or long-term), before we start causing code-churn. If we don't want functions like `kron` in SciPy, it would be nice to find another home for them[^1] first. Finding that home is not SciPy's responsibility, but we might be doing whoever puts in that work a favour by being patient.\r\n\r\n[^1]: The reason discussion of this is difficult to get right is the difference between implementation and API. While we have already found another home for the current SciPy implementation (NumPy), the API for an array-agnostic implementation does not have a home yet. SciPy's existing `kron` API is the most immediate (though not certainly the best) candidate for such a home.","Yes that's perfectly fine and I get the argument. The argument I am failing to convey is that kron does not have to be in every package. This is not a core linalg functionality. \r\n\r\nIf we remove it, we are not failing anyone so why can't we just deprecate it is still unanswered. That's why I don't see the connection why we are considering all these 3 options. Just like we removed tril and triu we can remove this one too. So I need some clarification why kron is more important than those two.","`tril` and `triu` are in the Array API. `kron` is not. There is an opportunity for `scipy` to provide an Array API-agnostic, fully-featured `kron` implementation so no one else has to.","We do use `kron` elsewhere in `scipy`, so to make those parts of `scipy` Array API-agnostic, we'd need an Array API-agnostic version of `kron` anyways, so to move that forward, we wouldn't want to rely on using `np.kron`.","Ok let me try another way, kron was until now never meant to be in the array api right? So just because historically we had a bad version of it should not make it eligible to an addition to array api. That doesn't make any sense. I really don't know how else I can put this. \r\n\r\nWe are also using extensively triu and tril **and** concatenate and tile and many others that are not ever considered for array api. No argument is done on them because those also equally would make no sense. Because there is no historical luggage on them. This one has and I am strongly -1 on making it our responsibility just because we happen to have a historical overlap.\r\n\r\n> we wouldn't want to rely on using np.kron\r\n\r\nQuite the other way, we would like to solely depend on np.kron\r\n","Put differently this function has no place in scipy.linalg because it is not a linalg function. It is an array creation function.","[`tril`](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.tril.html) and [`triu`](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.triu.html) _are_ in the Array API. Therefore, `scipy` does not need to provide copies of them because we can rely on the Array API implementation to provide them.\r\n\r\nNo one is proposing to add `kron` to the Array API. However, `scipy` does use `kron` in various places. If we want to make those places Array API-_agnostic_ (not implementations of the Array API but just code that can work with whatever Array API implementation objects it is given), then we need an Array API-agnostic version of `kron`. To the extent that `np.kron()` will not preserve the Array API implementation of its inputs and instead convert them to `np.ndarray`s and return an `np.ndarray`, we need an implementation that does preserve the implementation of the inputs. This is not the same as adding `kron` to the Array API. To the contrary, if it _were_ added to the Array API, then we could easily go forward with the deprecation of `scipy.linalg.kron`.\r\n\r\n> > we wouldn't want to rely on using np.kron\r\n\r\n> Quite the other way, we would like to solely depend on np.kron\r\n\r\nPlease be careful with snipping out phrases. I start with \"to move that [Array API agnosticization] effort forward, we wouldn't ...\". And that is true. If you rely on `np.kron`, that's a blocker for moving that effort forward. If you don't consider that effort, then yes, the balance would favor relying on `np.kron`. This issue started without considering that effort, and was reopened in part to consider that effort.\r\n\r\n> Put differently this function has no place in scipy.linalg because it is not a linalg function. It is an array creation function.\r\n\r\nIt's a special matrix. We have a [whole module in `scipy.linalg` for creating such special matrices](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/linalg\/_special_matrices.py).","OK I am about to give up. First thing, we already agreed initially this function does not make sense here. \r\n\r\nThen Ralf comes in he says no, that's a bit out of place but fine. Then we start discussing this function should support array api. \r\n\r\nLet's focus on this instance. Forget about anything comes after this. This function should not support array API because it doesn't have a place in scipy.linalg. \r\n\r\n1- it is not a special matrix. There is no such thing called Kronecker matrix. This is a misnomer. \r\n2- it is an array creation routine and it is already in the numpy namespace for this particular reason. You supply arrays and you generate a new one just like outer or einsum or many others in numpy.\r\n\r\nOn top of it this function is very slow if you do it on Python level because you need to allocate a new array, place the existing inputs by scalar multiples. Just like einsum this goes much faster if there is a numpy implementation done on the C level. Anything else is just a historical convenience and should not have been there. \r\n\r\nIf I can't make this point across I guess I have to concede this argument however I have to confess so far counter arguments for not deprecating is not put forward and still stuck on array api. \r\n\r\n","In principle, a `kron` function which works when given a CuPy array, a Dask array or a NumPy array (etc.) is desirable to _use_ in many projects. SciPy happens to be one of these projects, as Robert pointed out. It can live anywhere, as long as a project like SciPy can use it. Point taken about a Python implementation being slow, but the ability to handle various array types holds value in spite of performance concerns[^1].\n\nYou have made clear that you believe that this function[^2] doesn't belong in `scipy.linalg`. I'm inclined to agree with you. However, there are no obvious other places for it right now. Given that it is plausible (if undesirable) that this will find its home (perhaps temporarily) in `scipy.linalg`, it would be awkward to deprecate now. Hence my suggestion to go ahead with the deprecation of the current `scipy.linalg.kron`, but only once we can be sure that this array-agnostic function will live somewhere else, so that we (and other libraries) can use it. Since that requires work from others, we can be patient, alias `np.kron`, and live with the unfortunate naming for a little longer.\n\n[^1]: In SciPy we could use `np.kron` to be performant with NumPy arrays, and use this 'array-agnostic' function for other types.\n\n[^2]: To be crystal clear, 'this function' refers to the `kron` that works with any array API compatible array, throughout.","> However, there are no obvious other places for it right now\r\n\r\nThat's circling the argument. If a array library needs it, they can implement it in their native code. 99% of the users of kron will switch to NumPy. It does not need a common place. Let's not go back to the beginning again. \r\n\r\nWhy do you want it to work with different arrays, could you elaborate? If a function drops out of SciPy why do we need to find a new home for it? ","> Why do you want it to work with different arrays, could you elaborate? If a function drops out of SciPy why do we need to find a new home for it?\r\n\r\nWe use `kron` within `solve_discrete_lyapunov`. `solve_discrete_lyapunov` is not in the standard so we would have to provide an implementation that works will other array rather than just dispatching to a library's implementation. Hence as `kron` is used within `solve_discrete_lyapunov` and is also not in the standard we also would need an implementation of `kron` that works with other array types than just NumPy.","Yes exactly, so then `kron` has to go into the standard if solve_are or solve_lyapunov should support the standard. Or it has to fall back to NumPy arrays. \r\n\r\nWe discussed this and just as above I would really prefer not to start that discussion again. Every function in SciPy cannot support array API because otherwise SciPy is shackled by array API forever and cannot move since every internal function it uses has to support array API. I am sure that is not what we want and moreover, that is exactly what we have established that it won't happen in the first place when we started doing this. Instead it has to add piecemeal support for things that it can support. \r\n\r\nOur API is not BLAS-like and still cleaning up lots of things in the meantime. So I hope we will not have this discussion everytime we have to deprecate something.","> If a array library needs it, they can implement it in their native code. 99% of the users of kron will switch to NumPy. It does not need a common place. \n\nI am not talking about end-user code. I am talking about consumer libraries like SciPy, which have the potential to become array-agnostic, that currently use `np.kron` or `torch.kron` or `scipy.linalg.kron` or `cupy.kron` etc. Rather than every one of these consumer projects providing its own new `kron` implementation[^1], it probably makes sense to have a single implementation which everyone can use. I'm _not_ arguing for the view that this single implementation _must exist_, nor that it _must exist in SciPy_. I'm arguing for the view that it may be helpful to the ecosystem for this single implementation to exist, and SciPy is plausibly at least a temporary home for it.\n\n[^1]: Of course this would be possible given that it is three lines of code, but having to move away from a single function to three nontrivial lines many times over is still not ideal.","@lucascolley that is still again circling back to the starting point, we already, I hope, established that this is an array function. So it cannot be array agnostic. That is the reason why we want to deprecate it in the first place. \r\n\r\nSee my comment above. We cannot maintain a library if there is an external dependency that blocks us by its own state.\r\n\r\nCan we stop talking about array api for this please now? I don't know how different can I make the same point that array api does not apply to this function. ","> we already, I hope, established that this is an array function. So it cannot be array agnostic.\r\n\r\nI don't think we've established that, since no one commented on the implementation beyond simply asserting this. And it does not seem true. If you look at the numpy implementation, then you'll see (modulo some `np.matrix`-related cruft that we don't need) that it is implemented in pure Python and can be done so with the primitives: `asarray`, `max`, `reshape`, `expand_dims` and `+`\/`=`. So it looks to me like we are able to write a `kron` that's generic over array types, while being as performant as the numpy version.\r\n\r\nHow about I just write that implementation? That seems like a more effective use of time than more comments here.","I would really need to understand why we are trying to keep kron here. What is the concrete downside of deprecating other than a hypothetical array agnosticism (which is currently already not there)?\r\n\r\nUnless you want to write C code for it, no matter what you write here won't make any difference in terms of performance as this has to happen in the low level if we want a decent implementation. Here are some random data points from the interwebz why this is fundamentally a memory manipulation function\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/60383660\/why-is-numpys-kron-so-fast\r\nhttps:\/\/discourse.julialang.org\/t\/kron-vs-scalar-product-speed-difference-python-code-faster\/1460\/42\r\nhttps:\/\/nl.mathworks.com\/matlabcentral\/answers\/111918-is-it-possible-to-improve-performance-of-the-kron-operator-by-making-it-a-built-in-function-rather-t\r\n\r\nin fact first one happens to demonstrate the einsum connection I mentioned above. I hope we are not going to take over einsum too. So concrete question, why are we so willing to commit to maintaining a less performant version of kron? Array libraries can do this much more conveniently in their own native format be it distributed or otherwise."],"labels":["scipy.linalg","deprecated","RFC"]},{"title":"MAINT: Remove `openblas_support.py`","body":"Builds on numpy\/numpy#25505 to refactor wheel building\r\n- remove `tools\/openblas_support`, use wheels instead\r\n- refactor to use a single cibw_before_build.sh script\r\n- move many of the CIBW env variables set in CI into pyproject.toml, which makes it easier to run cibuildwheel outside of CI and reduces some duplication\r\n- upgrade OpenBLAS from `v0.3.20-571` (in the openblas_support script) and `v0.3.23.293` (in the ci test runs) to v0.3.26, and add a single source of truth in a `requirements\/openblas_requirements.txt` file\r\n\r\ncc@ [matthewfeickert](https:\/\/github.com\/matthewfeickert)","comments":["Thanks Matti. Some non-existing `pip` and undefined symbols - probably worth working on a fork to get one wheel build job per OS to pass?","Right, it is a shame to burn so much CI. I can reproduce the failures locally. \r\n\r\nAfter the build starts to work, I am seeing\r\n```\r\nImportError: <path>_fblas.cpython-311-x86_64-linux-gnu.so: undefined symbol: dznrm2_\r\n```\r\n\r\nThis should be `scipy_dznrm2_`. I think there is some missing handling of `BLAS_SYMBOL_PREFIX` around `scipy.linalg.get_blas_funcs`","> This should be `scipy_dznrm2_`. I think there is some missing handling of `BLAS_SYMBOL_PREFIX` around `scipy.linalg.get_blas_funcs`\r\n\r\nThat wouldn't be surprising, since we've never used a prefix for anything. I'm fairly sure that it'll be missing in more places. I think this is going to intersect substantially with the re-introduction of ILP64 support, which is ongoing in other branches. So we should probably pause this until ILP64 support is back.","OK"],"labels":["Build issues","maintenance","CI"]},{"title":"ENH: fix premature overflow in boxcox","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/19604#issuecomment-1833218049\r\nTowards #19016\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nFix premature overflow of the following modules.\r\n`special.boxcox`, `special.inv_boxcox`, `special.boxcox1p`, `special.inv_boxcox1p` \r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":["Thanks @xuefeng-xu! Since this is modifying the formulation for all values (not just in cases of overflow), I'd like a `special` regular to take a look. Did you benchmark the accuracy across the full ranges of $\\lambda$ and $x$ values as [suggested](https:\/\/github.com\/scipy\/scipy\/issues\/19016#issuecomment-1933527651)? We want to make sure this does no harm. Looks like it does, currently - `genpareto` is implemented in terms of these `special` functions, and those test failures look real. Let's get those resolved, and then we can ask someone to take a look.","The modified formulation loses precision when:\r\n\r\n1. $\\lambda\\approx0$\r\n<details>\r\n\r\n```python\r\nimport mpmath\r\nimport numpy as np\r\nfrom scipy.special._mptestutils import (\r\n    Arg, assert_mpmath_equal, exception_to_nan)\r\n\r\n\r\nnp.seterr(over='ignore')\r\n\r\n\r\ndef boxcox(x, lmbda):\r\n    if abs(lmbda) < 1e-14: # change 1e-19 to 1e-14\r\n        return np.log(x)\r\n    else:\r\n        # return np.expm1(lmbda * np.log(x)) \/ lmbda\r\n        return np.sign(lmbda) * np.exp(lmbda * np.log(x) - np.log(abs(lmbda))) - 1 \/ lmbda\r\n\r\n\r\ndef test_boxcox():\r\n\r\n    def mp_boxcox(x, lmbda):\r\n        x = mpmath.mp.mpf(x)\r\n        lmbda = mpmath.mp.mpf(lmbda)\r\n        if lmbda == 0:\r\n            return mpmath.mp.log(x)\r\n        else:\r\n            return mpmath.mp.powm1(x, lmbda) \/ lmbda\r\n\r\n    assert_mpmath_equal(\r\n        boxcox,\r\n        exception_to_nan(mp_boxcox),\r\n        [Arg(a=0, inclusive_a=False), Arg()],\r\n        n=1000,\r\n        dps=100,\r\n        rtol=1e-13,\r\n    )\r\n```\r\n```\r\n==================================================== test session starts ====================================================\r\nplatform darwin -- Python 3.11.6, pytest-7.4.3, pluggy-1.3.0\r\nrootdir: \/Users\/xxf\/code\r\nplugins: anyio-3.5.0, timeout-2.2.0, xdist-3.4.0, cov-4.1.0, hypothesis-6.89.0\r\ncollected 1 item                                                                                                            \r\n\r\nmpboxcox.py F                                                                                                         [100%]\r\n\r\n========================================================= FAILURES ==========================================================\r\n________________________________________________________ test_boxcox ________________________________________________________\r\n\r\n    def test_boxcox():\r\n    \r\n        def mp_boxcox(x, lmbda):\r\n            x = mpmath.mp.mpf(x)\r\n            lmbda = mpmath.mp.mpf(lmbda)\r\n            if lmbda == 0:\r\n                return mpmath.mp.log(x)\r\n            else:\r\n                return mpmath.mp.powm1(x, lmbda) \/ lmbda\r\n    \r\n>       assert_mpmath_equal(\r\n            boxcox,\r\n            exception_to_nan(mp_boxcox),\r\n            [Arg(a=0, inclusive_a=False), Arg()],\r\n            n=1000,\r\n            dps=100,\r\n            rtol=1e-13,\r\n        )\r\n\r\nmpboxcox.py:28: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nscipy\/scipy\/special\/_mptestutils.py:295: in assert_mpmath_equal\r\n    d.check()\r\nscipy\/scipy\/special\/_mptestutils.py:282: in check\r\n    raise value\r\nscipy\/scipy\/special\/_mptestutils.py:263: in check\r\n    assert_func_equal(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nfunc = <function boxcox at 0x10624ca40>, results = <function MpmathData.check.<locals>.<lambda> at 0x1201414e0>\r\npoints = array([[ 1.00000000e-030, -8.98846567e+307],\r\n       [ 1.00000000e-030, -4.32305117e+205],\r\n       [ 1.00000000e-030, -2...567e+307,  2.07919484e+103],\r\n       [ 8.98846567e+307,  4.32305117e+205],\r\n       [ 8.98846567e+307,  8.98846567e+307]])\r\nrtol = 1e-13, atol = 1e-300, param_filter = None, knownfailure = None, vectorized = False, dtype = None, nan_ok = True\r\nignore_inf_sign = False, distinguish_nan_and_inf = True\r\n\r\n    def assert_func_equal(func, results, points, rtol=None, atol=None,\r\n                          param_filter=None, knownfailure=None,\r\n                          vectorized=True, dtype=None, nan_ok=False,\r\n                          ignore_inf_sign=False, distinguish_nan_and_inf=True):\r\n        if hasattr(points, 'next'):\r\n            # it's a generator\r\n            points = list(points)\r\n    \r\n        points = np.asarray(points)\r\n        if points.ndim == 1:\r\n            points = points[:,None]\r\n        nparams = points.shape[1]\r\n    \r\n        if hasattr(results, '__name__'):\r\n            # function\r\n            data = points\r\n            result_columns = None\r\n            result_func = results\r\n        else:\r\n            # dataset\r\n            data = np.c_[points, results]\r\n            result_columns = list(range(nparams, data.shape[1]))\r\n            result_func = None\r\n    \r\n        fdata = FuncData(func, data, list(range(nparams)),\r\n                         result_columns=result_columns, result_func=result_func,\r\n                         rtol=rtol, atol=atol, param_filter=param_filter,\r\n                         knownfailure=knownfailure, nan_ok=nan_ok, vectorized=vectorized,\r\n                         ignore_inf_sign=ignore_inf_sign,\r\n                         distinguish_nan_and_inf=distinguish_nan_and_inf)\r\n>       fdata.check()\r\nE       AssertionError: \r\nE       Max |adiff|: 7.36617e-08\r\nE       Max |rdiff|: 3.7072e-07\r\nE       Bad results (76 out of 1024) for the following points (in output 0):\r\nE                               1.e-30         -3.4223003202678016e-08 =>             -69.07763443514705 !=             -69.07763444097992  (rdiff          8.443939614851335e-11)\r\nE                               1.e-30            3.73837195305305e-08 =>              -69.0774636156857 !=             -69.07746359779577  (rdiff         2.5898361610806074e-10)\r\nE               5.2625215202708194e-27         -3.4223003202678016e-08 =>             -60.50924983993173 !=            -60.509249873916964  (rdiff          5.616535952096334e-10)\r\nE               5.2625215202708194e-27            3.73837195305305e-08 =>            -60.509118776768446 !=             -60.50911878486791  (rdiff         1.3385523697463226e-10)\r\nE                 2.76941327513135e-23         -3.4223003202678016e-08 =>             -51.94086775556207 !=             -51.94086781940562  (rdiff         1.2291583404492937e-09)\r\nE                 2.76941327513135e-23            3.73837195305305e-08 =>             -51.94077119231224 !=            -51.940771227349884  (rdiff          6.745691885085158e-10)\r\nE               1.4574096958902303e-19         -3.4223003202678016e-08 =>             -43.37248829007149 !=            -43.372488277445164  (rdiff           2.91113643685989e-10)\r\nE               1.4574096958902303e-19            3.73837195305305e-08 =>             -43.37242095917463 !=             -43.37242092524083  (rdiff          7.823820027240829e-10)\r\nE                7.669649888473719e-16         -3.4223003202678016e-08 =>             -34.80411123111844 !=             -34.80411124803483  (rdiff          4.860456906829046e-10)\r\nE                7.669649888473719e-16            3.73837195305305e-08 =>             -34.80406788736582 !=             -34.80406787853985  (rdiff         2.5359021525611324e-10)\r\nE                4.036169759103547e-12         -3.4223003202678016e-08 =>            -26.235736686736345 !=            -26.235736731173912  (rdiff         1.6937800135262329e-09)\r\nE                4.036169759103547e-12            3.73837195305305e-08 =>            -26.235712070018053 !=            -26.235712087246075  (rdiff           6.56663010274995e-10)\r\nE               2.1240430216748705e-08         -3.4223003202678016e-08 =>             -17.66736465319991 !=            -17.667364726861653  (rdiff          4.169367799561624e-09)\r\nE               2.1240430216748705e-08            3.73837195305305e-08 =>            -17.667353603988886 !=            -17.667353551358623  (rdiff         2.9789556574815125e-09)\r\nE                 0.000111778221115451         -3.4223003202678016e-08 =>             -9.098995238542557 !=             -9.098995235097325  (rdiff          3.786386921305008e-10)\r\nE                 0.000111778221115451            3.73837195305305e-08 =>             -9.098992295563221 !=             -9.098992270876613  (rdiff         2.7131145491938895e-09)\r\nE                   0.5882352941176471         -3.4223003202678016e-08 =>            -0.5306282304227352 !=            -0.5306282558801932  (rdiff         4.7976069401507286e-08)\r\nE                   0.5882352941176471            3.73837195305305e-08 =>            -0.5306282453238964 !=            -0.5306282457991719  (rdiff          8.956844823031987e-10)\r\nE                   1.1764705882352942         -3.4223003202678016e-08 =>            0.16251898929476738 !=            0.16251892904581908  (rdiff          3.707195749831828e-07)\r\nE                   1.1764705882352942            3.73837195305305e-08 =>             0.1625189520418644 !=            0.16251892999147188  (rdiff         1.3567891760710356e-07)\r\nE                   1.7647058823529411         -3.4223003202678016e-08 =>             0.5679840371012688 !=             0.5679840320856685  (rdiff          8.830530411261046e-09)\r\nE                   1.7647058823529411            3.73837195305305e-08 =>             0.5679840035736561 !=             0.5679840436360429  (rdiff          7.053435268343966e-08)\r\nE                   2.3529411764705883         -3.4223003202678016e-08 =>             0.8556660898029804 !=             0.8556660975292865  (rdiff          9.029580652676584e-09)\r\nE                   2.3529411764705883            3.73837195305305e-08 =>             0.8556660749018192 !=             0.8556661237432364  (rdiff         5.7079993910757565e-08)\r\nE                   2.9411764705882355         -3.4223003202678016e-08 =>             1.0788096599280834 !=             1.0788096414570465  (rdiff         1.7121683207484437e-08)\r\nE                   2.9411764705882355            3.73837195305305e-08 =>             1.0788097083568573 !=             1.0788096831260827  (rdiff          2.338760485316419e-08)\r\nE                   3.5294117647058822         -3.4223003202678016e-08 =>             1.2611312307417393 !=             1.2611311909508638  (rdiff          3.155173368874185e-08)\r\nE                   3.5294117647058822            3.73837195305305e-08 =>             1.2611312307417393 !=               1.26113124789439  (rdiff         1.3601003592462826e-08)\r\nE                     4.11764705882353         -3.4223003202678016e-08 =>             1.4152818992733955 !=             1.4152818637184148  (rdiff          2.512219060304968e-08)\r\nE                     4.11764705882353            3.73837195305305e-08 =>             1.4152819067239761 !=              1.415281935433366  (rdiff         2.0285279586798455e-08)\r\nE                    4.705882352941177         -3.4223003202678016e-08 =>             1.5488132759928703 !=             1.5488132495702094  (rdiff          1.705993989619612e-08)\r\nE                    4.705882352941177            3.73837195305305e-08 =>              1.548813309520483 !=             1.5488133354561222  (rdiff          1.674549062699351e-08)\r\nE                    5.294117647058823         -3.4223003202678016e-08 =>             1.6665962599217892 !=              1.666596278746113  (rdiff         1.1295071348422783e-08)\r\nE                    5.294117647058823            3.73837195305305e-08 =>             1.6665963903069496 !=             1.6665963781915003  (rdiff          7.269576166728482e-09)\r\nE                    5.882352941176471         -3.4223003202678016e-08 =>             1.7719568386673927 !=             1.7719567882046523  (rdiff         2.8478538961367388e-08)\r\nE                    5.882352941176471            3.73837195305305e-08 =>              1.771956853568554 !=             1.7719569006211584  (rdiff         2.6554034391498504e-08)\r\nE                    6.470588235294118         -3.4223003202678016e-08 =>             1.8672669865190983 !=              1.867266962073766  (rdiff         1.3091503625045225e-08)\r\nE                    6.470588235294118            3.73837195305305e-08 =>              1.867267120629549 !=               1.86726708690885  (rdiff         1.8058851547428588e-08)\r\nE                   7.0588235294117645         -3.4223003202678016e-08 =>             1.9542784057557583 !=              1.954278333373515  (rdiff         3.7037837484100626e-08)\r\nE                   7.0588235294117645            3.73837195305305e-08 =>              1.954278476536274 !=             1.9542784701138582  (rdiff         3.2863360102343935e-09)\r\nE                    7.647058823529412         -3.4223003202678016e-08 =>             2.0343210138380527 !=             2.0343210355840626  (rdiff         1.0689566445667878e-08)\r\nE                    7.647058823529412            3.73837195305305e-08 =>             2.0343211963772774 !=             2.0343211837549267  (rdiff          6.204699051943513e-09)\r\nE                     8.23529411764706         -3.4223003202678016e-08 =>             2.1084290742874146 !=              2.108429002484369  (rdiff          3.405523527080793e-08)\r\nE                     8.23529411764706            3.73837195305305e-08 =>              2.108429156243801 !=             2.1084291616472517  (rdiff         2.5627849747855775e-09)\r\nE                    8.823529411764707         -3.4223003202678016e-08 =>             2.1774218641221523 !=              2.177421868911566  (rdiff         2.1995800735087972e-09)\r\nE                    8.823529411764707            3.73837195305305e-08 =>              2.177422020584345 !=             2.1774220386612586  (rdiff           8.30197978283715e-09)\r\nE                    9.411764705882353         -3.4223003202678016e-08 =>              2.241960447281599 !=             2.2419603851685883  (rdiff         2.7704776206429715e-08)\r\nE                    9.411764705882353            3.73837195305305e-08 =>             2.2419605627655983 !=               2.24196056513013  (rdiff         1.0546713971426796e-09)\r\nE                                  10.         -3.4223003202678016e-08 =>              2.302585057914257 !=               2.30258500227061  (rdiff         2.4165729807369755e-08)\r\nE                                  10.            3.73837195305305e-08 =>             2.3025851398706436 !=             2.3025851920963847  (rdiff         2.2681350181691292e-08)\r\nE               7.0880457030678306e+44         -3.4223003202678016e-08 =>              103.2719712741673 !=             103.27197125666322  (rdiff         1.6949495563902615e-10)\r\nE               7.0880457030678306e+44            3.73837195305305e-08 =>              103.2723530754447 !=             103.27235310448533  (rdiff          2.812043486259432e-10)\r\nE                5.024039188877834e+88         -3.4223003202678016e-08 =>             204.24100859835744 !=             204.24100861377485  (rdiff          7.548637381191282e-11)\r\nE                5.024039188877834e+88         -1.0540925533894595e-15 =>             204.24172241294545 !=             204.24172241292348  (rdiff         1.0756852777261997e-13)\r\nE                5.024039188877834e+88          1.1180339887498947e-15 =>             204.24172241294545 !=             204.24172241296878  (rdiff         1.1424807412846235e-13)\r\nE                5.024039188877834e+88            3.73837195305305e-08 =>             204.24250213429332 !=              204.2425021399006  (rdiff          2.745408312699838e-11)\r\nE               3.561061938476992e+132         -3.4223003202678016e-08 =>              305.2096971273422 !=              305.2096970748111  (rdiff         1.7211480198446786e-10)\r\nE               3.561061938476992e+132         -1.0540925533894595e-15 =>             305.21129107292114 !=              305.2112910728721  (rdiff         1.6072757434509447e-13)\r\nE               3.561061938476992e+132          1.1180339887498947e-15 =>             305.21129107292114 !=             305.21129107297327  (rdiff          1.707846879193543e-13)\r\nE               3.561061938476992e+132            3.73837195305305e-08 =>              305.2130323201418 !=              305.2130322997808  (rdiff          6.671068866885632e-11)\r\nE              2.5240969771380244e+176         -3.4223003202678016e-08 =>              406.1780366562307 !=             406.17803664097755  (rdiff          3.755283217569957e-11)\r\nE              2.5240969771380244e+176         -1.0540925533894595e-15 =>             406.18085973289686 !=              406.1808597328099  (rdiff         2.1411750153428288e-13)\r\nE              2.5240969771380244e+176          1.1180339887498947e-15 =>             406.18085973289686 !=              406.1808597329891  (rdiff         2.2713248692156064e-13)\r\nE              2.5240969771380244e+176            3.73837195305305e-08 =>              406.1839435324073 !=              406.1839435855646  (rdiff          1.308700024149117e-10)\r\nE              1.7890914732929676e+220         -3.4223003202678016e-08 =>             507.14602729678154 !=             507.14602731347986  (rdiff           3.29260643389193e-11)\r\nE              1.7890914732929676e+220         -1.0540925533894595e-15 =>              507.1504283928726 !=               507.150428392737  (rdiff         2.6732020006900317e-13)\r\nE              1.7890914732929676e+220          1.1180339887498947e-15 =>              507.1504283928726 !=             507.15042839301634  (rdiff         2.8346028762018307e-13)\r\nE              1.7890914732929676e+220            3.73837195305305e-08 =>             507.15523597225547 !=              507.1552359986905  (rdiff         5.2124142796559876e-11)\r\nE              1.2681162129669514e+264         -3.4223003202678016e-08 =>               608.113669142127 !=              608.1136690935235  (rdiff          7.992504201070365e-11)\r\nE              1.2681162129669514e+264         -1.0540925533894595e-15 =>              608.1199970528482 !=              608.1199970526534  (rdiff         3.2042892981527494e-13)\r\nE              1.2681162129669514e+264          1.1180339887498947e-15 =>              608.1199970528482 !=               608.119997053055  (rdiff          3.400584733568262e-13)\r\nE              1.2681162129669514e+264            3.73837195305305e-08 =>              608.1269095353782 !=              608.1269095405971  (rdiff          8.581938885735336e-12)\r\nE                8.98846567431105e+307         -3.4223003202678016e-08 =>              709.0809619985521 !=              709.0809619823142  (rdiff          2.289991115596109e-11)\r\nE                8.98846567431105e+307         -1.0540925533894595e-15 =>               709.089565712824 !=               709.089565712559  (rdiff         3.7372432418010035e-13)\r\nE                8.98846567431105e+307          1.1180339887498947e-15 =>               709.089565712824 !=              709.0895657131051  (rdiff         3.9649088532675946e-13)\r\nE                8.98846567431105e+307            3.73837195305305e-08 =>              709.0989642329514 !=              709.0989642127232  (rdiff          2.852660400008869e-11)\r\n\r\nscipy\/scipy\/special\/_testutils.py:85: AssertionError\r\n================================================== short test summary info ==================================================\r\nFAILED mpboxcox.py::test_boxcox - AssertionError: \r\n===================================================== 1 failed in 0.64s =====================================================\r\n```\r\n<\/details>\r\n\r\n2. $x=1$\r\n```python\r\nprint(boxcox(1, 10)) # -2.7755575615628914e-17 (not 0)\r\n```","Maybe we can use the same formula as the mpmath implementation using [scipy's `powm1`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.powm1.html) ?\r\n\r\nAnother possibility would be to use the old formulation for small values and the new one for large ones.","Thanks @mdhaber @dschmitz89 !\r\nI use the new formulation for large values, test passed."],"labels":["enhancement","scipy.special","Cython"]},{"title":"ENH:linalg: kron does not support nd arrays with n>2","body":"Is this a feature?\n\n> Re. `scipy.linalg.kron`, I'm not sure whether it might not be better to just delegate to `numpy.kron`, unless there are any advantages to `scipy.linalg.kron`. `numpy.kron` works with empty array arguments and also handles n-dimensional arrays, whereas `scipy.linalg.kron` gives some funny results when the number of dimensions is not two:\r\n>\r\n>```python\r\n>linalg.kron(np.empty((1,2,3)), >np.empty((4,5,6))).shape\r\n>(3, 8, 5, 6)  # should be (4, 10, 18)\r\n>```\n\n_Originally posted by @JozsefKutas in https:\/\/github.com\/scipy\/scipy\/pull\/18673#issuecomment-1590329028_","comments":["Hi @ricardoV94, you can continue the discussion in the linked issue. But honestly this is not an actionable query for us. Is yes an answer or no is? nD is not supported or should give an error. ","> But honestly this is not an actionable query for us. Is yes an answer or no is?\n\nI can't parse this sentence. What do you mean?\n\nI opened an issue to ask whether the behavior is a feature (which I can't quite grasp) or a bug. \n\nThe reason for that is if it's a bug we don't need to worry about supporting the behavior in PyTensor: https:\/\/github.com\/pymc-devs\/pytensor\/pull\/622#issuecomment-1937710505","I also don't understand your query. The subject says it is broken then text has a question if it is a feature\r\n\r\nWe don't have any support for nd arrays yet for [`linalg.kron`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.linalg.kron.html). It should probably error out until then.\r\n","Neither the excerpt nor the (lack) of documentation were clear to confirm the behavior was indeed a bug.\n\nIf it's a bug, shouldn't you leave the issue opened until is resolved? That PR is addressing something else and AFAICT it's not fixing the issue.","The docs are mentioning that the inputs are expecting 2D arrays with clearly stated dims. If you think it is still not clear then PR is welcome or if you wish to expand to nd arrays too.","Thanks, I just wanted to double check it's a bug\/undefined behavior. \n\nI don't need anything else, although I would suggest leaving an open issue for visibility.\n\n","Done","> The docs are mentioning that the inputs are expecting 2D arrays with clearly stated dims. If you think it is still not clear then PR is welcome\r\n\r\n~Whereabouts in the docs is this mentioned? I think https:\/\/scipy.github.io\/devdocs\/reference\/generated\/scipy.linalg.kron.html#scipy.linalg.kron could do with a note to say that the inputs must be 2D.~ ah, ignore me, I just now understand the `(M, N)` syntax \ud83d\udc4d sorry for the noise!\r\n\r\n> I also don't understand your query. The subject says it is broken then text has a question if it is a feature\r\n\r\nI think this was an unfortunate misunderstanding, Jake edited the original title.","@ricardoV94 for future reference, this syntax means that, for example, the shape of the input to `a` must be `M` by `N`, hence it must be 2D. I think the documentation is clear enough once you understand this.\r\n\r\n<img width=\"196\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/51488791\/75e2c667-edf7-40dd-a625-9f4074893ed2\">\r\n","cross posting @ricardoV94 's comment from https:\/\/github.com\/numpy\/numpydoc\/pull\/423\r\n\r\n> For context, that was not in fact the source of confusion. The confusion was that it was not mentioned that batch dimensions are not supported, and the function silently returning nonsense for >2D cases.\r\n\r\n> The syntax like (..., M, N) usually clarifies core vs batch cases, but the ommission of ellipsis to specify batch dimensions are not allowed\/undefined behavior is imo too subtle\/insufficient. I think this should always be stated explicitly if not checked by the function, given the widespread expectation that numpy-like code will handle batch dimensions correctly.\r\n\r\nWould you like to suggest a change to the documentation @ricardoV94 ?","> Would you like to suggest a change to the documentation @ricardoV94 ?\n\nMentioning the function is restricted to 2D inputs in the docs would have been helpful.\n\nAn explicit ValueError\/NotImplementedError for ndim != 2 may also be worth discussing but I don't know what's scipy policy regarding checks\/undefined behavior of that kind.\n\n"],"labels":["enhancement","scipy.linalg"]},{"title":"ENH: interpolate: improve calculation of Pade approximants with SVD based method","body":"The current method implemented for calculating Pade approximants involves solving a Toeplitz system which is often highly ill-conditioned. There are multiple ways to help with this, for example, iterative refinement. However, I would like to propose we replace our current implementation with an implementation based on the Trefethen paper [Robust Pade Approximation via SVD](https:\/\/people.maths.ox.ac.uk\/trefethen\/publication\/PDF\/2011_144.pdf). This method is used in the popular Chebfun (and license-compatible) package.\r\n\r\nI can provide more details if required however in my use case the current SciPy method produces a spurious pole whereas my translation of the code from Chebfun provides the correct approximation.\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/60778417\/ea4402f2-b5e3-446a-88da-023ea1c1e7e4)\r\n","comments":["Definitely a big +1 from me. What we have currently is historical, and a robust method from Trefethen and co is very much welcome as a replacement. I think it's easiest to add a new function and deprecate the current one.\r\nAlso cross-ref https:\/\/github.com\/scipy\/scipy\/pull\/14933 for a previous discussion, which unfortunately stalled.\r\n"],"labels":["enhancement","scipy.interpolate"]},{"title":"ENH: Custom padding in `map coordinates`","body":"### Is your feature request related to a problem? Please describe.\n\nHi,\r\n\r\nI have just stumbled across a problem when using padding mode in `map coordinates`. The padding appended to each spatial dimension is of the same nature. However, when working with 360 images, the horizontal padding mode is different than the vertical padding. The horizontal dimension follows a `grid-warp` fashion whereas the vertical is done by replicating the most external row but swapping the halved, i.e, the right most half is appended above the top most half and viceversa (for the first row in the image).\r\n\r\nThe code looks something like this\r\n\r\n```\r\ndef erp_padding(erp_image):\r\n    H, W, C = erp_image.shape\r\n    padded_erp_image = np.zeros((H + 2, W + 2, C))\r\n    padded_erp_image[1:-1, 1:-1] = erp_image\r\n    padded_erp_image[1:-1, 0] = erp_image[:, -1]\r\n    padded_erp_image[1:-1, -1] = erp_image[:, 0]\r\n    padded_erp_image[0, 1:W\/\/2 + 1] = erp_image[0, W\/\/2:]\r\n    padded_erp_image[0, W\/\/2 + 1:-1] = erp_image[0, :W\/\/2]\r\n    padded_erp_image[-1, 1:W\/\/2 + 1] = erp_image[-1, W\/\/2:]\r\n    padded_erp_image[-1, W\/\/2 + 1:-1] = erp_image[-1, :W\/\/2]\r\n    return padded_erp_image\r\n```\n\n### Describe the solution you'd like.\n\nIs it possible to feed `map coordinates` with the padding version of the input 360 image? The coordinates I use are defined on the image without padding. i.e, `(0,0)` would belong to `(1,1)` in the padded image? However, this doesn\u00b4t work. The final result is one pixel shifted wrt to the no-padding version. \n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":[],"labels":["enhancement","scipy.ndimage"]},{"title":"DOC:signal: Suggest remedies for slow speed in `resample` when the number of samples is prime and large.","body":"The documentation currently mentions that if the number of samples in the input or output is large and prime, then `resample` can be very slow. However, it does not provide any remedies. This PR gives two suggestions to the user:\r\n\r\n1. Consider using `resample_poly`.\r\n2. Consider using `decimate` before applying `resample`.","comments":["@DietBru would you be interested in reviewing this?","@j-bowhay I have no idea why the docs don't render. They render on my PC. Do you have an idea what might be the reason?","I have closed reopened to hopefully flush the ci","@j-bowhay The issue persists.","I don't see the problem, but it probably isn't a coincidence that the doc build stops with `resample`. To be sure, you could merge the latest `main` into this PR. CircleCI doesn't do that automatically, and sometimes that is the source of trouble. I'll approve running the rest of CI after CircleCI is fixed.","@j-bowhay I have followed @mdhaber suggestion of merging main here, but the problem is not fixed. Locally, this builds perfectly on my PC.\r\n\r\nThe dev.py output:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/c462f190-f0c9-468f-8bf3-c3f7371ccd66)\r\n\r\nRendered docs:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/9b0c3d4d-07bf-4ea3-8167-ebb4b76b5da5)\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/f23ee0e1-0854-4fd3-a94b-f45d2fe87d5d)\r\n\r\nI don't know how to debug this. The error message is very criptic:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/bf7b67ac-5bf6-4506-8ab2-66f7f23b6e6d)\r\n\r\nDo you have an idea on what might cause this? Should I open a separate issue to address this?\r\n\r\nEDIT: I think the prime number I have here is too large. I will test with a smaller one."],"labels":["scipy.signal","Documentation"]},{"title":"BUG: Kruskal-Wallis test silently produces NaNs as test statistics and p-value if there are NaNs present in the input","body":"### Description\r\n\r\nPython version: 3.11.5\r\nSciPy version: 1.11.3\r\n\r\nWhen there are NaNs in the input, the test silently produces NaN as an output (i.e., resulting statistics and p-value). It should at least give a warning suggesting that the NaNs in the input can be responsible. Otherwise the user may think that the p-value\/stats is p=0, or p=1. Or, in general, it can be cumbersome to debug the issue without a feedback.\r\n\r\nSee other people also struggling with this issue: https:\/\/stackoverflow.com\/questions\/77087907\/kruskal-wallis-test-always-gives-nan-values\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.stats import kruskal\r\n\r\n# Sample data: scores from three different teaching methods\r\ngroup1 = np.array([20, 21, 19, 22, 24, 23, 21, 20, 21, 19, 20])\r\ngroup2 = np.array([22, 24, 25, 23, 23, 25, 26, 24, 21, 22, 30])\r\n\r\n# Perform the Kruskal-Wallis H-test\r\nstat, p = kruskal(group1, group2)\r\n\r\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\r\n\r\n# Sample data: scores from three different teaching methods\r\ngroup1 = np.array([20, 21, 19, 22, 24, 23, 21, 20, 21, 19, np.nan])\r\ngroup2 = np.array([22, 24, 25, 23, 23, 25, 26, 24, 21, 22, 30])\r\n\r\n# Perform the Kruskal-Wallis H-test\r\nstat, p = kruskal(group1, group2)\r\n\r\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nStatistics=9.678, p=0.002\r\nStatistics=nan, p=nan\r\n\r\n# Well, it is not an error per se, but a bug from the usability point of view.\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nPython version: 3.11.5\r\nSciPy version: 1.11.3\r\n\r\n$lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 22.04.3 LTS\r\nRelease:\t22.04\r\nCodename:\tjammy\r\n\r\n\r\n\r\n1.11.3 1.24.3 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory:\r\n    lib directory:\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory:\r\n    version: '2023.1'\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory:\r\n    lib directory:\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory:\r\n    version: '2023.1'\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory:\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path:\r\n  version: '3.11'\r\n```\r\n","comments":["Or, also, the same behaviour is observed (NaNs in the output), if one of the vectors (input) is empty. Which is not the same as some values missing\/being NaNs -- the output is the same in both cases stats=nan and p=nan","The same behavior is observed in most `stats` reducing functions, all NumPy reducing functions like `np.sum`, all (to my knowledge) elementwise functions in `scipy.special` and NumPy, and much more, so this is expected behavior, not a defect. If you want to make a case that NaNs in input should produce warnings for all similar functions, I'd suggest contacting the [mailing list](https:\/\/projects.scipy.org\/scipylib\/mailing-lists.html).","Thank you for the suggestion, I will do so.","Note that there is a `nan_policy='raise'` option if you want NaNs to raise errors, and `nan_policy='omit'` can be thought of as removing NaNs from the input before performing the calculation. The default is `nan_policy='propagate'` which returns the result that would be produced if NaNs were to propagate through the calculation (usually NaN, as you observed).","Interesting, why is `nan_policy` not `raise` by default? Seems like that's much safer behavior. And could be adjusted through a deprecation cycle? (I'd argue that, in the context of SciPy, raising on nans makes a lot more sense than in NumPy, since here you know what they user is trying to compute.)"],"labels":["scipy.stats","query"]},{"title":"MAINT: constants: reorganize codata constants data","body":"I\u00b4ll try to break down my draft #17577 into smaller bits, starting by this pull request.\r\n\r\n#### Reference issue\r\ngh-17577\r\n\r\n#### What does this implement\/fix?\r\nIt slims down the `_codata.py` file by moving the constants data in their own file. This makes its editing easier.\r\n\r\n#### Additional information\r\nThis pull request should not break anything as the changes are only internal.\r\n","comments":["Could anyone give me a hint why the file `scipy\/constants\/codata_constants_2002.txt` can't be found?","Try adding it to the list in `meson.build`, https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/constants\/meson.build","relevant CI error seems to be\r\n\r\n```\r\nscipy\/constants\/_codata.py:106: in parse_constants_2002to2014\r\n    uncert = float(line[77:99].replace(' ', '').replace('(exact)', '0'))\r\nE   ValueError: could not convert string to float: '0.00000029e'\r\n        constants  = {'alpha particle-electron mass ratio': (7294.2995361, '', 2.9e-06), '{220} lattice spacing of silicon': (1.920155714e-10, '2          m', 3.2e-07)}\r\n        d          = '\\n             2010 Fundamental Physical Constants --- Complete Listing\\n\\n\\n  From:  [http:\/\/physics.nist.gov\/constan...1\\nWien](http:\/\/physics.nist.gov\/constan...1\/nWien) wavelength displacement law constant                   2.897 7721 e-3           0.000 0026 e-3           m K\\n'\r\n        line       = 'alpha particle mass                                         6.644 656 75 e-27        0.000 000 29 e-27        kg'\r\n        name       = 'alpha particle mass'\r\n        uncert     = 2.9e-06\r\n        units      = ''\r\n        val        = 6.64465675e-27\r\n```","For the parsing of the 2002 file I came up with the following parsing function but it contains loads of regular expressions:\r\n\r\n```\r\ndef parse_constants_2002to2006(d: str) -> dict[str, tuple[float, str, float]]:\r\n    constants = {}\r\n    for line in d.split('\\n'):\r\n        if line == \"\":\r\n            continue\r\n        if line[1] == \" \":\r\n            continue\r\n        if line[1] == \"-\":\r\n            continue        \r\n        name = line[1:61].rstrip()\r\n\tsignificant = re.search(r'\\d*.\\d*', line[62:95].replace(\" \",\"\")).group()\r\n\texponent = re.search(r'e-*\\d*', line[62:95].replace(\" \",\"\")).group()\r\n\tval = float(significant+exponent)\r\n\tuncert_non_zeros = re.sub(r'[()]', '',  re.search(r'\\(\\d*\\)', line[62:95]).group())\r\n \tuncert_zeros = re.sub(r'\\d', '0', significant).group()\r\n\tuncert = float(uncert_zeros.replace(uncert_zeros[-len(uncert_non_zeros):], uncert_non_zeros) + exponent)\r\n\tunits = line[96:].rstrip()\r\n        constants[name] = (val, units, uncert)\r\n\treturn constants\r\n```\r\n\r\nI have to look at it again another time and perhaps it\u00b4s way to complicated.","This sounds like a good idea. While we're at it - what do you think about saving the data into a single `.npz` file at build time and installing that instead of the `.txt` files? That will make the wheel and on-disk installed sizes smaller; the text files are almost 200 kb; the binary equivalent is going to be a lot smaller.\r\n\r\nWe're doing this already in `scipy.special`. It should be pretty straightforward, the only thing to think about is to write the file to the build dir rather than in-tree. This can be done with a simplified version of https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/special\/utils\/makenpz.py"],"labels":["scipy.constants","maintenance"]},{"title":"CI: CircleCI seems to use branch config file on manual merge","body":"Studying the doc build failure in gh-20033 (https:\/\/app.circleci.com\/pipelines\/github\/scipy\/scipy\/24759\/workflows\/231eede0-469d-4160-891a-05395f2be7e2\/jobs\/83510), it looks like we do correctly get a merge of the feature branch used for the PR with `main` (we do this \"manually\" as a step beforehand), but CircleCI seems to use the exact `.circleci\/config.yml` on the submitted branch, since the old `python dev.py --no-build doc -j2` is still used there, triggering warnings about parallel build support with the newer sphinx theme\/setup.\r\n\r\nIntuitively, I can see how the chicken and egg type problem might happen depending on how they are setup, so I'm not sure this is worth much energy, maybe one documentation line for maintainers checking failures I suppose, or just refer back to this issue in the rare cases it matters. It could get messy if that file is getting changed a lot on `main` AND gets changed in a PR as well, but that is rare I think.","comments":[],"labels":["Documentation","CI"]},{"title":"BUG: fitpack pardeu derivative evaluation limits","body":"### Describe your issue.\r\n\r\n@ev-br \r\nIt feels like the partial derivative should be allowed up to the degree of the bspline.\r\nCurrently there is a limit to the degree - 1.\r\nBut, for example, the derivative of order 1 of a bspline of degree 1 exists, it is a bspline of degree 0.\r\nIt is defined everywhere except at the nodes, but it remains useful to be able to evaluate it everywhere else (for example to compute integrals, or for visualization). Also, it can be extended by continuity at the nodes using semi-open intervals.\r\nWhy this limit ?\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/5929562\/343772ae-4798-4d77-9c73-50d9743f6886)\r\n\r\n\r\nWhen one tries to call a with nu = deg, one gets the following error msg:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/5929562\/6dd4b463-1e01-4ed4-ae4f-0eb3f472c089)\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\ncall a scipy.interpolate.BivariateSpline(x, y, dx=1, dy=0, grid=False) where the degree of the bspline itself is 1.\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nValueError: Error code returned by pardeu: 10\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\npython 3.9.12\r\nnumpy 1.21.5\r\nscipy 1.7.3\r\n\r\non a windows laptop, with an Anaconda distribution, in Spyder\r\n```\r\n","comments":["This line has been there forever (tm):\r\n\r\n```\r\n$ git blame -M -C -C scipy\/interpolate\/fitpack\/pardeu.f\r\n<snip>\r\neda24252c92 Lib\/interpolate\/fitpack\/parder.f   (Travis Oliphant 2001-06-20 06:23:55 +0000  82)       if(nux.lt.0 .or. nux.ge.kx) go to 400\r\n```\r\n\r\n1D splines also require `k >= 1`: https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/fitpack\/curfit.f#L226\r\n\r\nSo it looks like FITPACK just does not support k=0 splines, in any dimension.\r\n\r\nAdding this is an enhancement. I wouldn't be surprised if the assumption that `k >= 1` is baked rather deep into FITPACK: it works by minimizing the k-th derivative jumps at knots, then what is it for piecewise-constant functions, the jumps themselves?\r\n\r\n-------------------------------------------------------------------\r\n\r\nA way around this limitation depends on what you are really after:\r\n- if you want k=0 spline (why would you, but OK), `make_interp_spline` accepts k=0 in 1D\r\n- `splder` can happily compute derivatives of the order k, again in 1D:\r\n\r\n```\r\nIn [14]: from scipy.interpolate import splder, splrep\r\n\r\nIn [15]: x = np.linspace(1, 4, 11)\r\n\r\nIn [16]: y = x**3 + 1\/(1+x)\r\n\r\nIn [17]: tck = splrep(x, y, k=3)\r\n\r\nIn [18]: splder(tck, 3)\r\nOut[18]: \r\n(array([1. , 1.6, 1.9, 2.2, 2.5, 2.8, 3.1, 3.4, 4. ]),\r\n array([5.81191949, 5.90245998, 5.92952647, 5.9531363 , 5.96625086,\r\n        5.97555179, 5.98149946, 5.98671657, 0.        ]),\r\n 0)\r\n```\r\n\r\n- I guess what `splder` is doing can be generalized to 2D. This needs a champion though :-).\r\n- If what you want is to compute derivatives of a tensor product spline interpolant, take a look at `RegularGridInterpolator` (you want version 1.13dev)"],"labels":["enhancement","query"]},{"title":"ENH: Adding zgemm3m to scipy.linalg.cython_blas","body":"### Is your feature request related to a problem? Please describe.\n\nI have plenty of complex-complex matrix multiplications in my cython code, and I currently use `scipy.linalg.cython_blas.zgemm` to treat them in a nogil environment. I've heard of `zgemm3m` being sometimes able to provide a substantial performance gain, sometimes at the expense of numerical stability.\r\n\r\nWas any thought put into including `zgemm3m` in `scipy.linalg.cython_blas` ?\r\nThe only reference I could find was this post from 2015 : https:\/\/github.com\/scipy\/scipy\/issues\/5266\n\n### Describe the solution you'd like.\n\nprovide an interface to  `zgemm3m` in `scipy.linalg.cython_blas` similar to `scipy.linalg.cython_blas.zgemm`\n\n### Describe alternatives you've considered.\n\nLive with the dread of performance being left on the table\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Hi @gabrielfougeron, thanks for the suggestion. I'm not sure we can do anything with this, since `zgemm3m` is not a standard BLAS function but an extension. MKL and OpenBLAS have it, but reference BLAS and Accelerate don't. So I don't see how we can make it available in a reasonable way."],"labels":["enhancement","scipy.linalg"]},{"title":"BUG: Output of logser.cdf is not larger than 0.9999999999999998 on some platforms","body":"I am encountering a problem when calculating the inverse of the cumulative distribution function (CDF) for the log-series distribution using SciPy's logser function. Specifically, when I run the following code:\r\n\r\n`import scipy.stats as st\r\nprint(st.logser(0.25739239425774363).ppf(0.9999999999999999)) # infinite loop begins\r\n`\r\n\r\nThe issue only arises in a Conda installation; other setups work without any problem. Upon preliminary investigation, it seems that an infinite loop is occurring within the _drv2_ppfsingle function.\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/8436511\/18c3d5a5-23a3-4573-b9e0-5d1f3b53df54)\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport scipy.stats as st\r\nprint(st.logser(0.25739239425774363).ppf(0.9999999999999999)) # infinite loop begins\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nThe error message is only because I interrupted command in the middle (first works, third works; second loops forever).\r\n\r\n(\/home\/jovyan\/conda_py312) jovyan@runner30-0:~$ python -c 'import scipy.stats as st;print(st.logser(0.25739239425774363).ppf(0.999999999999999))'\r\n24.0\r\n\r\n\r\n(\/home\/jovyan\/conda_py312) jovyan@runner30-0:~$ python -c 'import scipy.stats as st;print(st.logser(0.25739239425774363).ppf(0.9999999999999999))'\r\n^CTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 485, in ppf\r\n    return self.dist.ppf(q, *self.args, **self.kwds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 3625, in ppf\r\n    place(output, cond, self._ppf(*goodargs) + loc)\r\n                        ^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 1010, in _ppf\r\n    return self._ppfvec(q, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2372, in __call__\r\n    return self._call_as_normal(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2365, in _call_as_normal\r\n    return self._vectorize_call(func=func, args=vargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2455, in _vectorize_call\r\n    outputs = ufunc(*inputs)\r\n              ^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 2977, in _drv2_ppfsingle\r\n    qb = self._cdf(b, *args)\r\n         ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 3316, in _cdf\r\n    return self._cdfvec(k, *args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2372, in __call__\r\n    return self._call_as_normal(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2365, in _call_as_normal\r\n    return self._vectorize_call(func=func, args=vargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/numpy\/lib\/function_base.py\", line 2455, in _vectorize_call\r\n    outputs = ufunc(*inputs)\r\n              ^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_distn_infrastructure.py\", line 3312, in _cdf_single\r\n    return np.sum(self._pmf(m, *args), axis=0)\r\n                  ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/stats\/_discrete_distns.py\", line 899, in _pmf\r\n    return -np.power(p, k) * 1.0 \/ k \/ special.log1p(-p)\r\n            ^^^^^^^^^^^^^^\r\nKeyboardInterrupt\r\n\r\n\r\ncode below works\r\n(\/home\/jovyan\/conda_py312) jovyan@runner30-0:~$ python -c 'import scipy.stats as st;print(st.logser(0.25739239425774363).ppf(0.99999999999999999))'\r\ninf\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.4 sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\r\n\/home\/jovyan\/conda_py312\/lib\/python3.12\/site-packages\/scipy\/__config__.py:154: UserWarning: Install `pyyaml` for better output\r\n  warnings.warn(\"Install `pyyaml` for better output\", stacklevel=1)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"cc\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"3.0.8\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"c++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.15.0\",\r\n      \"include directory\": \"..\/..\/tmp\/pip-build-env-tfqmvy7q\/overlay\/lib\/python3.12\/site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n```\r\n","comments":["Seems to work on main for me locally:\r\n\r\n```\r\nIn [3]: st.logser(0.25739239425774363).ppf(0.9999999999999999)\r\nOut[3]: 99.0\r\n\r\nIn [4]: import scipy\r\n\r\nIn [5]: scipy.__version__\r\nOut[5]: '1.13.0.dev0+1282.c87635c'\r\n```","Yes, it's platform specific.\r\nI found possible root cause.\r\n\r\nWhen I'm executing this code on setup where bug doesn't arise, I have:\r\n```\r\nimport scipy.stats as st\r\nst.logser(0.25739239425774363).cdf(100) == 0.9999999999999999\r\n```\r\n\r\nBut on problematic machine\/setup\/conda (I don't know what exactly is the reason):\r\n\r\n```\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(100)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(1000)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(10000)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(100000)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(1000000)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(10000000)\r\n0.9999999999999998\r\n>>> import scipy.stats as st; st.logser(0.25739239425774363).cdf(100000000)\r\n0.9999999999999998\r\n```\r\nNo value is giving something bigger than argument which is passed to self.ppf().\r\n\r\nI will walk around this issue by restricting values passed to .ppf() (to not allow something too close to one).\r\nBut as you can see there are some platforms at which some unlucky floating point arithmetic is bounding cdf result.\r\nSo it seems that it's not _drv2_ppfsingle function problem which assumes monotonicity of cdf, but cdf() which is not strictly increasing from some point.",">  it seems that it's not _drv2_ppfsingle function problem which assumes monotonicity of cdf, but cdf() which is not strictly increasing from some point.\r\n\r\nmakes sense, would you mind editing the title of the issue to reflect this?","cross-ref https:\/\/github.com\/scipy\/scipy\/issues\/3890","> > it seems that it's not _drv2_ppfsingle function problem which assumes monotonicity of cdf, but cdf() which is not strictly increasing from some point.\r\n> \r\n> makes sense, would you mind editing the title of the issue to reflect this?\r\n\r\nok, I've edited title.","Maybe worth coding an explicit CDF and PPF using formulas from Wikipedia."],"labels":["defect","scipy.stats"]},{"title":"DOC: add comparison of optimizers in optimization guide","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nresolved DOC: comparison of optimizers in optimization guide #17404\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nAdded optimization solvers reference tables and its description.","comments":["Looks like doc build is still failing--this small diff fixes it for me locally:\r\n\r\n<details>\r\n\r\n```diff\r\n--- a\/doc\/source\/tutorial\/optimize.rst\r\n+++ b\/doc\/source\/tutorial\/optimize.rst\r\n@@ -1269,6 +1269,7 @@ which is especially important when the objective function has multiple local min\r\n      - \r\n      - \r\n      - \r\n+\r\n (\u2713) = Depending on the chosen local minimizer\r\n```\r\n\r\n<\/details>\r\n\r\nFor the expert reviewers, a sample render of the tables in this PR is below the fold, in the absence of the passing doc build in CI.\r\n\r\n<details>\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/7903078\/2df9ce3b-2b50-4ec4-bfdf-dfc8d5ac4ead)\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/7903078\/30e7a09f-1c3b-4101-bb10-11f49e8dbfe7)\r\n\r\n<\/details>\r\n\r\nAnd for reference, the relevant table in the SciPy `1.0` paper is here: https:\/\/www.nature.com\/articles\/s41592-019-0686-2\/tables\/1\r\n\r\nI'll leave most of the inspection to the experts, but one question I have--why does Newton-CG have a \"global\" checkmark in the published paper, but is listed under local here? The paper does note:\r\n\r\n> this is not a guarantee of convergence to a global minimum.\r\n\r\nIs that the distinction here, or? Anyway, if the difference confuses me a little, maybe worth noting, but I wouldn't change much until you get feedback from the `optimize` gurus.","> Looks like doc build is still failing--this small diff fixes it for me locally:\r\n> \r\n> For the expert reviewers, a sample render of the tables in this PR is below the fold, in the absence of the passing doc build in CI.\r\n> \r\n> And for reference, the relevant table in the SciPy `1.0` paper is here: https:\/\/www.nature.com\/articles\/s41592-019-0686-2\/tables\/1\r\n> \r\n> I'll leave most of the inspection to the experts, but one question I have--why does Newton-CG have a \"global\" checkmark in the published paper, but is listed under local here? The paper does note:\r\n> \r\n> > this is not a guarantee of convergence to a global minimum.\r\n> \r\n> Is that the distinction here, or? Anyway, if the difference confuses me a little, maybe worth noting, but I wouldn't change much until you get feedback from the `optimize` gurus.\r\n\r\nI am not an expert in this field so I don't know the details, but it is based on the spreadsheet presented by @dschmitz89  in DOC: comparison of optimizers in optimization guide #17404.","As described in the footnote, \"global convergence\" is a very specific property mostly unrelated to being a \"global optimizer\". None of the methods in that table in the paper are global optimizers.","The tables look great, thank you!\r\n\r\nMy only concern is where they should be placed. I would insert them in the beginning of the local or global optimization tutorials each. Another option would be to place them together at the top of the whole tutorial page. Opinions?","> The tables look great, thank you!\r\n> \r\n> My only concern is where they should be placed. I would insert them in the beginning of the local or global optimization tutorials each. Another option would be to place them together at the top of the whole tutorial page. Opinions?\r\n\r\nI think it is a good idea to list it at the top.","> > The tables look great, thank you!\r\n> > My only concern is where they should be placed. I would insert them in the beginning of the local or global optimization tutorials each. Another option would be to place them together at the top of the whole tutorial page. Opinions?\r\n> \r\n> I think it is a good idea to list it at the top.\r\n\r\nWhat changes should I make?","Since we are \r\n\r\n> > > The tables look great, thank you!\r\n> > > My only concern is where they should be placed. I would insert them in the beginning of the local or global optimization tutorials each. Another option would be to place them together at the top of the whole tutorial page. Opinions?\r\n> > \r\n> > \r\n> > I think it is a good idea to list it at the top.\r\n> \r\n> What changes should I make?\r\n\r\nSince we are both happy with moving it to the top, let's do so :).","Sorry for the back and forth but I am having second thoughts looking at the optimization user guide again. For users of linear programming or MILP for example, it would be confusing to be greeted in the second paragraph with tables of nonlinear optimizers. What about moving the tables to the dedicated sections for local and global optimization?","> For users of linear programming or MILP for example, it would be confusing to be greeted in the second paragraph with tables of nonlinear optimizers.\r\n\r\nMaybe there would be some advice about choosing between LP, NLP, global, curve fitting, root finding, etc... and linking to the relevant sections of the guide? But that opens a larger can of worms, so I'd understand if it's not something you want to tackle here.\r\n\r\nTable looks good, though. Great to finally have it, since it's been an idea for a long time. What do you think about having a different mark for *requiring* the user to specify the derivatives (like some of the trust methods)?","> Sorry for the back and forth but I am having second thoughts looking at the optimization user guide again. For users of linear programming or MILP for example, it would be confusing to be greeted in the second paragraph with tables of nonlinear optimizers. What about moving the tables to the dedicated sections for local and global optimization?\r\n\r\nYes. That might be easier to understand.","> > For users of linear programming or MILP for example, it would be confusing to be greeted in the second paragraph with tables of nonlinear optimizers.\r\n> \r\n> Maybe there would be some advice about choosing between LP, NLP, global, curve fitting, root finding, etc... and linking to the relevant sections of the guide? But that opens a larger can of worms, so I'd understand if it's not something you want to tackle here.\r\n> \r\n> Table looks good, though. Great to finally have it, since it's been an idea for a long time. What do you think about having a different mark for _requiring_ the user to specify the derivatives (like some of the trust methods)?\r\n\r\nGood idea. If necessary, you can request a code change and I can reflect it.","> > For users of linear programming or MILP for example, it would be confusing to be greeted in the second paragraph with tables of nonlinear optimizers.\r\n> \r\n> Maybe there would be some advice about choosing between LP, NLP, global, curve fitting, root finding, etc... and linking to the relevant sections of the guide? But that opens a larger can of worms, so I'd understand if it's not something you want to tackle here.\r\n\r\nYep, exactly, that would quickly spiral this PR out of control.\r\n\r\n> Table looks good, though. Great to finally have it, since it's been an idea for a long time. What do you think about having a different mark for _requiring_ the user to specify the derivatives (like some of the trust methods)?\r\n\r\nGood idea. I have never used those trust methods though myself, will have to dig to see which ones require the Hessian. I had always assumed that all our optimizers can fall back to finite differences.",">What do you think about having a different mark for requiring the user to specify the derivatives\r\n\r\nAs soon as this is implemented, it would be good to Merge this PR and start a new issue.\r\n\r\n>Maybe there would be some advice about choosing between LP, NLP, global, curve fitting, root finding, etc... and linking to the relevant sections of the guide\r\n\r\nThis idea is very good, but I think it is best discussed in a new Issue or PR.","Is there anything else we need to do for this PR to be merged?","@koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.","> @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n\r\nPreviously, I made a change to place it at the top for clarity.Will I change the placement again?","> > @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n> \r\n> Previously, I made a change to place it at the top for clarity.Will I change the placement again?\r\n\r\nSorry for this back and fourth. It unfortunately happens sometimes in reviews and I understand that it can be annoying.\r\n\r\nI am now convinced that placing the tables in the beginnings of the respective sections is the better choice, so I am not gonna ask to move them again once they are there.","> > > @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n> > \r\n> > \r\n> > Previously, I made a change to place it at the top for clarity.Will I change the placement again?\r\n> \r\n> Sorry for this back and fourth. It unfortunately happens sometimes in reviews and I understand that it can be annoying.\r\n> \r\n> I am now convinced that placing the tables in the beginnings of the respective sections is the better choice, so I am not gonna ask to move them again once they are there.\r\n\r\nI agree with putting a table at the beginning of each section. However, as far as the current tutorial is concerned, there is a section for global optimizations, but nothing for local optimizations. If you want to add a section for local optimization, you should consider where you want to add it.","> > > > @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n> > > \r\n> > > \r\n> > > Previously, I made a change to place it at the top for clarity.Will I change the placement again?\r\n> > \r\n> > \r\n> > Sorry for this back and fourth. It unfortunately happens sometimes in reviews and I understand that it can be annoying.\r\n> > I am now convinced that placing the tables in the beginnings of the respective sections is the better choice, so I am not gonna ask to move them again once they are there.\r\n> \r\n> I agree with putting a table at the beginning of each section. However, as far as the current tutorial is concerned, there is a section for global optimizations, but nothing for local optimizations. If you want to add a section for local optimization, you should consider where you want to add it.\r\n\r\nRight, I did not look in the details there. How about we add the first three main points in the user guide under a new heading \"Local optimization\" and put the table in the beginning?","> > > > > @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n> > > > \r\n> > > > \r\n> > > > Previously, I made a change to place it at the top for clarity.Will I change the placement again?\r\n> > > \r\n> > > \r\n> > > Sorry for this back and fourth. It unfortunately happens sometimes in reviews and I understand that it can be annoying.\r\n> > > I am now convinced that placing the tables in the beginnings of the respective sections is the better choice, so I am not gonna ask to move them again once they are there.\r\n> > \r\n> > \r\n> > I agree with putting a table at the beginning of each section. However, as far as the current tutorial is concerned, there is a section for global optimizations, but nothing for local optimizations. If you want to add a section for local optimization, you should consider where you want to add it.\r\n> \r\n> Right, I did not look in the details there. How about we add the first three main points in the user guide under a new heading \"Local optimization\" and put the table in the beginning?\r\n\r\nagree. I think it is possible to add each table to a section, but there is also the issue of where to put the usage instructions. Another option is to refer to the instructions above for proper use of each section.","Usage instruc\r\n\r\n> > > > > > @koko1928 : sorry for the late reply. Could you move the tables to the beginning of the local and global optimization sections respectively? After that, I will make a pass over the description and merge.\r\n> > > > > \r\n> > > > > \r\n> > > > > Previously, I made a change to place it at the top for clarity.Will I change the placement again?\r\n> > > > \r\n> > > > \r\n> > > > Sorry for this back and fourth. It unfortunately happens sometimes in reviews and I understand that it can be annoying.\r\n> > > > I am now convinced that placing the tables in the beginnings of the respective sections is the better choice, so I am not gonna ask to move them again once they are there.\r\n> > > \r\n> > > \r\n> > > I agree with putting a table at the beginning of each section. However, as far as the current tutorial is concerned, there is a section for global optimizations, but nothing for local optimizations. If you want to add a section for local optimization, you should consider where you want to add it.\r\n> > \r\n> > \r\n> > Right, I did not look in the details there. How about we add the first three main points in the user guide under a new heading \"Local optimization\" and put the table in the beginning?\r\n> \r\n> agree. I think it is possible to add each table to a section, but there is also the issue of where to put the usage instructions. Another option is to refer to the instructions above for proper use of each section.\r\n\r\nFor a first mergeable version, let's just leave out the usage instructions then? To get this in without too much more discussion, that's the most pragmatic way I think.","I'm getting an error like the one below. How should I do it?\r\n\r\n>ERROR: No matching distribution found for jupyterlite-sphinx>=0.13.0","> I'm getting an error like the one below. How should I do it?\n> \n> >ERROR: No matching distribution found for jupyterlite-sphinx>=0.13.0\n\nThis was fixed in main by now. If you merge main into your branch, CI should become green.","> > I'm getting an error like the one below. How should I do it?\r\n> > > ERROR: No matching distribution found for jupyterlite-sphinx>=0.13.0\r\n> \r\n> This was fixed in main by now. If you merge main into your branch, CI should become green.\r\n\r\nI got it. thank you.\r\nThe problem has been fixed and it works correctly."],"labels":["scipy.optimize","Documentation"]},{"title":"BUG: `ndinterp.affine_transform()` creates an image with black border, in each direction","body":"### Describe your issue.\r\n\r\nI am trying to upscale an image by 2x using `ndinterp.affine_transform()` and I am comparing the result with a \"ground truth\" obtained using napari. What I get is almost correct: the output image is upscaled and correctly aligned, but I get a black border, around direction of the image.\r\n\r\nI don't know if I am not using the function correctly or if I stumbled upon a bug of `affine_transform()`.\r\n\r\nI show 4 screenshots of napari.\r\n\r\n# original image\r\n<img width=\"2056\" alt=\"Screenshot 2024-02-06 at 23 26 12\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/2664412\/35a893cb-4839-44b0-bdb8-c1752c569195\">\r\n\r\n# \"ground truth\" upscaled imaged\r\noverlayed to the original image (the small, semi-transparent one). Notice how the center of the (0, 0) pixels are the same location for both the small and large image. This is how napari operates.\r\n<img width=\"2056\" alt=\"Screenshot 2024-02-06 at 23 26 20\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/2664412\/ef1d1454-0f57-4d3b-abbf-21827428c40a\">\r\n\r\n# transformed image\r\n<img width=\"2056\" alt=\"Screenshot 2024-02-06 at 23 26 39\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/2664412\/7ad8bfb5-25cc-48ae-8ab2-cf1489d3c8e9\">\r\n\r\n\r\n# transformed image\r\noverlayed to the \"ground truth\" image; notice that the transformed image is correctly aligned\r\n<img width=\"2056\" alt=\"Screenshot 2024-02-06 at 23 26 46\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/2664412\/65815927-9c6d-490a-b802-b4ea01dfee48\">\r\n\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport napari\r\nfrom scipy.ndimage import affine_transform\r\n\r\nviewer = napari.Viewer()\r\n\r\n# create image\r\ny = 2\r\nx = 2\r\nc = 3\r\nimage = np.random.rand(y, x, c)\r\n\r\n# add raw image to napari\r\nviewer.add_image(image, name=\"original\")\r\n\r\n# scale factor\r\nk = 2\r\n# add raw image to napari, with an affine scale transformation\r\nscale = np.array(\r\n    [\r\n        [k, 0, 0],\r\n        [0, k, 0],\r\n        [0, 0, 1],\r\n    ]\r\n)\r\nviewer.add_image(image, name=\"scaled\", affine=scale)\r\n\r\n# half-pixel offset, since the origin is at the center of the pixel\r\npre_scale = np.array(\r\n    [\r\n        [1, 0, 0, 0.5],\r\n        [0, 1, 0, 0.5],\r\n        [0, 0, 1, 0],\r\n        [0, 0, 0, 1],\r\n    ]\r\n)\r\ninverse_scale = np.array(\r\n    [\r\n        [1 \/ k, 0, 0, 0],\r\n        [0, 1 \/ k, 0, 0],\r\n        [0, 0, 1, 0],\r\n        [0, 0, 0, 1],\r\n    ]\r\n)\r\npost_scale = np.array(\r\n    [\r\n        [1, 0, 0, -0.5],\r\n        [0, 1, 0, -0.5],\r\n        [0, 0, 1, 0],\r\n        [0, 0, 0, 1],\r\n    ]\r\n)\r\naffine = post_scale @ inverse_scale @ pre_scale\r\n\r\ntransformed = affine_transform(image, matrix=affine, output_shape=[y * 2, x * 2, c], order=0, prefilter=False)\r\n# we need to add a translation to napari, since we want that the center of the (0, 0) pixel to coincide \r\n# for both the original and transformed image, and the pixel sizes are different\r\noffset_for_transformed = np.array(\r\n    [\r\n        [1, 0, -0.5 * k + 0.5],\r\n        [0, 1, -0.5 * k + 0.5],\r\n        [0, 0, 1],\r\n    ]\r\n)\r\nviewer.add_image(transformed, name=\"transformed\", affine=offset_for_transformed)\r\n\r\nnapari.run()\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nNo error message\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.26.3 sys.version_info(major=3, minor=10, micro=13, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/macbook\/miniconda3\/envs\/ome\/include\r\n    lib directory: \/Users\/macbook\/miniconda3\/envs\/ome\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/macbook\/miniconda3\/envs\/ome\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/macbook\/miniconda3\/envs\/ome\/include\r\n    lib directory: \/Users\/macbook\/miniconda3\/envs\/ome\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/macbook\/miniconda3\/envs\/ome\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/macbook\/miniconda3\/envs\/ome\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 16.0.6\r\n  c++:\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 16.0.6\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/Users\/runner\/miniforge3\/conda-bld\/scipy-split_1700812700233\/_build_env\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/_build_env\/venv\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: arm64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/macbook\/miniconda3\/envs\/ome\/bin\/python\r\n  version: '3.10'\r\n```\r\n","comments":[],"labels":["defect","scipy.ndimage"]},{"title":"BUG: raise on non-feasible half space","body":"* Fixes #19865\r\n\r\n* Rather than returning a problematic half space intersection polytope, we raise an error for incremental processing when a new half space is added for which the interior point is no longer clearly interior\r\n\r\n[skip cirrus]","comments":["I updated the test to include a 1D half space, and fixed the code to support that. I added Richard as a co-author.\r\n\r\nI haven't managed to \"vectorize\" the code though, the suggestion above didn't quite pass the full suite yet.","I checked out your branch and added vectorization like this:\r\n\r\n```python\r\n        halfspaces = np.atleast_2d(halfspaces)\r\n        dims = halfspaces.shape[1] - 1\r\n        dists = np.dot(halfspaces[:, :dims], self.interior_point[:dims]) + halfspaces[:, -1]\r\n        if (dists > 0).any():\r\n            msg = f\"feasible point is not clearly inside halfspaces. Signed distances {dists}\"\r\n            raise QhullError(msg)\r\n\r\n```\r\n\r\nThat passes `python dev.py test -s spatial`, but the full suite does not pass:\r\n\r\n```\r\n======================================== short test summary info ========================================\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_sum[coo_array] - ValueError: not enough values to unpack (expected 2, got 1)\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_sum_invalid_params[coo_array] - AssertionError: Regex pattern did not match.\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_numpy_sum[coo_array] - ValueError: not enough values to unpack (expected 2, got 1)\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_mean[coo_array] - IndexError: tuple index out of range\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_sum_dtype[coo_array] - ValueError: not enough values to unpack (expected 2, got 1)\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_mean_dtype[coo_array] - IndexError: tuple index out of range\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_mean_out[coo_array] - IndexError: tuple index out of range\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_numpy_mean[coo_array] - IndexError: tuple index out of range\r\nFAILED scipy\/sparse\/tests\/test_common1d.py::TestCommon1D::test_matvec[coo_array] - assert (1, 1) == ()\r\n= 9 failed...\r\n```\r\n\r\nIs that what you're seeing? This is unexpected for me, because that file was [added very recently](https:\/\/github.com\/scipy\/scipy\/commit\/1e3e8c603126f1bf2e8febd567cfd374d169283b) and in fact `scipy\/sparse\/tests\/test_common1d.py` doesn't seem to exist in this PR's branch! (I am guessing that `dev.py` does some magic to temporarily merge in the latest before running tests?).","I saw a real `spatial` failure actually, but it looks like you changed the code a bit from `>=` to `>` (which is more consistent with what I did originally, so makes sense).\r\n\r\nI did see one other issue--the index in the original error message always seemed to be `1` even if there was a single halfspace involved, so that gave me some pause.\r\n\r\nYour other failures don't look related.","> I saw a real `spatial` failure actually, but it looks like you changed the code a bit from `>=` to `>` (which is more consistent with what I did originally, so makes sense).\r\n\r\nOh, good catch! I just rewrote the code this morning instead of copy\/pasting the previous suggestion. \r\n\r\nBut I think actually `>=` is correct -- a feasible point needs to be on the strict interior of the polytope, and if the signed distance is indeed exactly 0 then the feasible point is on the boundary.\r\n\r\nWhen I change to `>=` the tests pass for me:\r\n\r\n```\r\n(scipy-dev) richard@donnie:~\/external_repos\/scipy$ python dev.py test -s spatial\r\n\ud83d\udcbb  ninja -C \/home\/richard\/external_repos\/scipy\/build -j16\r\nninja: Entering directory `\/home\/richard\/external_repos\/scipy\/build'\r\n[7\/7] Linking target scipy\/spatial\/_qhull.cpython-310-x86_64-linux-gnu.so\r\nBuild OK\r\n\ud83d\udcbb  meson install -C build --only-changed\r\nInstalling, see meson-install.log...\r\nInstallation OK\r\nSciPy from development installed path at: \/home\/richard\/external_repos\/scipy\/build-install\/lib\/python3\/dist-packages\r\nRunning tests for scipy version:1.13.0.dev0+1296.cbfb96c, installed at:\/home\/richard\/external_repos\/scipy\/build-install\/lib\/python3\/dist-packages\/scipy\r\n================================================================================================ test session starts =================================================================================================\r\nplatform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0\r\nrootdir: \/home\/richard\/external_repos\/scipy\r\nconfigfile: pytest.ini\r\nplugins: hypothesis-6.98.3\r\ncollected 1538 items \/ 20 deselected \/ 1518 selected                                                                                                                                                                 \r\n\r\nscipy\/spatial\/tests\/test__plotutils.py sss                                                                                                                                                                     [  0%]\r\nscipy\/spatial\/tests\/test__procrustes.py ......                                                                                                                                                                 [  0%]\r\nscipy\/spatial\/tests\/test_distance.py ...................................................................................................................................................................ss.... [ 11%]\r\n........................ss..........................................................ss........................................................................................................................ [ 25%]\r\n.........................................................................................                                                                                                                      [ 31%]\r\nscipy\/spatial\/tests\/test_hausdorff.py ..............s                                                                                                                                                          [ 32%]\r\nscipy\/spatial\/tests\/test_kdtree.py ........................................................................................................................................................................... [ 43%]\r\n................................................................                                                                                                                                               [ 47%]\r\nscipy\/spatial\/tests\/test_qhull.py ............................................................................................................................................................................ [ 58%]\r\n.......                                                                                                                                                                                                        [ 59%]\r\nscipy\/spatial\/tests\/test_slerp.py ...................................................................................                                                                                          [ 64%]\r\nscipy\/spatial\/tests\/test_spherical_voronoi.py .......................................................................................................................                                          [ 72%]\r\nscipy\/spatial\/transform\/tests\/test_rotation.py ............................................................................................................................................................... [ 83%]\r\n................                                                                                                                                                                                               [ 84%]\r\nscipy\/spatial\/transform\/tests\/test_rotation_groups.py ........................................................................................................................................................ [ 94%]\r\n................................................................................                                                                                                                               [ 99%]\r\nscipy\/spatial\/transform\/tests\/test_rotation_spline.py .......                                                                                                                                                  [100%]\r\n\r\n================================================================================== 1508 passed, 10 skipped, 20 deselected in 10.12s ==================================================================================\r\n```\r\n\r\n> \r\n> I did see one other issue--the index in the original error message always seemed to be `1` even if there was a single halfspace involved, so that gave me some pause.\r\n\r\nProbably my `np.where` usage was wrong (easy to do!). But the native `qHull` error message includes the distances too, so maybe we just print the distances?\r\n\r\n> \r\n> Your other failures don't look related.\r\n\r\n","I probably need a little more time to think about this before branching for `1.13.0` in a few days, so I'll bump the milestone.","Ok! LMK if there's anything I can do to help!"],"labels":["defect","scipy.spatial","Cython"]},{"title":"ENH: support an array for `proportiontocut` in scipy.stats.trim_mean","body":"### Is your feature request related to a problem? Please describe.\n\nWhen calculating multiple different trimmed means of a large sample, the function spends a lot of time re-sorting the sample over and over again.\n\n### Describe the solution you'd like.\n\nIt would be nice if the `scipy.stats.trim_mean` supported an array-like object for the `proportiontocut` argument, and return multiple different trimmed means from a single sort. Something similar to how the [`numpy.quantile`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.quantile.html) works with the `q` argument.\n\n### Describe alternatives you've considered.\n\nSort the array myself and calculate the different means myself?\n\n### Additional context (e.g. screenshots, GIFs)\n\nI'm using the trimmed means to bootstrap confidence intervals of large samples (6GB arrays of floating point numbers) so doing the sort three times instead of once when calculating three different trimmed means is rather significant.","comments":["~~If I\u2019m not mistaken, an O(n) instead of O(nlnn) algorithm is possible to compute the trimmed mean?~~\r\n\r\nscipy is already using an O(n) algorithm (by calling `numpy.partition`). Some overhead exists if called repeatedly on the same array due to extra copying the array.\r\n\r\nNote: The [documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.trim_mean.html) incorrectly states \u201cThe input is sorted before slicing.\u201d","This is something I've considered adding to similar functions before. @tirthasheshpatel and I will be working on other aspects of `trim_mean` shortly and we will consider this again at that time. The interface for `proportion_to_cut` would probably follow that of `popmean` in `ttest_1samp` than `q` in `quantile`.","@mdhaber could you maybe update the documenation of `popmean`? I may have missed it, but I don't see an explanation on what happens to the result if it's array-like: https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.ttest_1samp.html\r\n\r\n> Note: The [documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.trim_mean.html) incorrectly states \u201cThe input is sorted before slicing.\u201d\r\n\r\nit's correct in that it describes an equivalent algorithm. The important part is that it creates a copy of the array and operates on it, which for large arrays and multiple calls is throwing away reusable temporary results.","Re: ttest_1samp: it isn't very explicit because it follows the same broadcasting behavior of other stats functions, but about `popmean` it says;\r\n\r\n> If array_like, then its length along `axis` must equal 1, and it must otherwise be broadcastable with a.\r\n\r\nThe *shape* of the result is just like it would be with `ttest_ind` if one of the samples were an array of length 1.\r\n\r\nAt some point there will be a nice tutorial about this stuff so that it doesn't need to go explicitly in every function, and we'll link to it. ","While following established convention is good for users familiar with the library, expecting users to know about it implicitly isn't exactly nice to new users. I mean, I have been programming in python for over a decade and \"broadcastable\" has no meaning to me...","> it's correct in that it describes an equivalent algorithm. The important part is that it creates a copy of the array and operates on it, which for large arrays and multiple calls is throwing away reusable temporary results.\r\n\r\nIf the array were indeed sorted, the performance you observed would be 20x worse :-)","> This is something I've considered adding to similar functions before. @tirthasheshpatel and I will be working on other aspects of `trim_mean` shortly and we will consider this again at that time. The interface for `proportion_to_cut` would probably follow that of `popmean` in `ttest_1samp` than `q` in `quantile`.\r\n\r\nAn additional \u201coptimization\u201d would be to check if the array is already sorted and omit the partition if it is."],"labels":["scipy.stats","enhancement"]},{"title":"BUG: Issue passing options to local minimizer in optimize.shgo","body":"### Describe your issue.\n\nWhen passing an `options` dictionary in the `minimizer-kwargs` argument of `shgo`, it seems that these options are not taken account by the local minimizer.\r\n\r\nHere is an example with the cattle-feed example and trying to limit the local minimizer to only 2 iterations.\r\nThe output shows that the first local minimization reached 8 iterations.\n\n### Reproducing Code Example\n\n```python\nimport scipy\r\nimport numpy as np\r\n\r\ndef f(x):  # (cattle-feed)\r\n\r\n    return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\r\n\r\n\r\ndef g1(x):\r\n\r\n    return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\r\n\r\n\r\ndef g2(x):\r\n\r\n    return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\r\n\r\n            - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\r\n\r\n                            + 20.5*x[2]**2 + 0.62*x[3]**2)\r\n\r\n            ) # >=0\r\n\r\n\r\ndef h1(x):\r\n\r\n    return x[0] + x[1] + x[2] + x[3] - 1  # == 0\r\n\r\n\r\ncons = ({'type': 'ineq', 'fun': g1},\r\n\r\n        {'type': 'ineq', 'fun': g2},\r\n\r\n        {'type': 'eq', 'fun': h1})\r\n\r\nbounds = [(0, 1.0),]*4\r\n\r\nres = scipy.optimize.shgo(f, bounds, n=150, constraints=cons,\r\n           minimizer_kwargs = {'options':{'maxiter':2}},\r\n           options={'disp':True})\n```\n\n\n### Error message\n\n```shell\nSplitting first generation\r\nStarting minimization at [1.000 1.000 0.000 0.000]...\r\nbounds in kwarg:\r\n[[0.5, 1.0], [0.5, 1.0], [0.0, 0.5], [0.0, 0.5]]\r\nlres =  message: Positive directional derivative for linesearch\r\n success: False\r\n  status: 8\r\n     fun: 33.64749052456115\r\n       x: [ 5.000e-01  5.000e-01  8.055e-02  1.199e-01]\r\n     nit: 8\r\n     jac: [ 2.455e+01  2.675e+01  3.900e+01  4.050e+01]\r\n    nfev: 20\r\n    njev: 4\r\nStarting minimization at [0.000 0.000 1.000 0.000]...\r\nbounds in kwarg:\r\n[[0.0, 0.5], [0.0, 0.5], [0.5, 1.0], [0.0, 0.5]]\r\nlres =  message: Optimization terminated successfully\r\n success: True\r\n  status: 0\r\n     fun: 31.77500000000491\r\n       x: [ 5.000e-01  1.835e-13  5.000e-01  0.000e+00]\r\n     nit: 2\r\n     jac: [ 2.455e+01  2.675e+01  3.900e+01  4.050e+01]\r\n    nfev: 10\r\n    njev: 2\r\nSuccessfully completed construction of complex.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.10.1 1.20.3 sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: cmake\r\n    found: true\r\n    include directory: unknown\r\n    lib directory: unknown\r\n    name: OpenBLAS\r\n    openblas configuration: unknown\r\n    pc file directory: unknown\r\n    version: 0.3.18\r\n  lapack:\r\n    detection method: cmake\r\n    found: true\r\n    include directory: unknown\r\n    lib directory: unknown\r\n    name: OpenBLAS\r\n    openblas configuration: unknown\r\n    pc file directory: unknown\r\n    version: 0.3.18\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.33\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: \/tmp\/pip-build-env-q2fwe5jt\/overlay\/lib\/python3.8\/site-packages\/pythran\r\n    version: 0.12.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp38-cp38\/bin\/python\r\n  version: '3.8'\n```\n","comments":["Thank you for reporting this issue @giovanni-farwind, I will look into this unexpected behaviour."],"labels":["defect","scipy.optimize"]},{"title":"BUG: `sparse.random` returns transposed array in 1.12","body":"### Describe your issue.\n\nRef: https:\/\/github.com\/osqp\/osqp-python\/issues\/121#issuecomment-1925563879\n\n### Reproducing Code Example\n\n```python\n$ pip install \"scipy<1.12\"\r\nSuccessfully installed scipy-1.11.4\r\n$ python -c \"from scipy import sparse; x = sparse.random(3, 3, density=0.7, random_state=1); print(type(x)); print(x)\"\r\n<class 'scipy.sparse._coo.coo_matrix'>\r\n  (2, 2)\t0.23608897695197606\r\n  (2, 0)\t0.3965807272960261\r\n  (0, 2)\t0.3879107411620074\r\n  (1, 2)\t0.66974603680348\r\n  (1, 0)\t0.9355390708060318\r\n  (0, 0)\t0.8463109166860171\r\n\r\n$ pip install scipy --upgrade\r\nSuccessfully installed scipy-1.12.0\r\n$ python -c \"from scipy import sparse; x = sparse.random(3, 3, density=0.7, random_state=1); print(type(x)); print(x)\"\r\n<class 'scipy.sparse._coo.coo_matrix'>\r\n  (2, 2)\t0.23608897695197606\r\n  (0, 2)\t0.3965807272960261\r\n  (2, 0)\t0.3879107411620074\r\n  (2, 1)\t0.66974603680348\r\n  (0, 1)\t0.9355390708060318\r\n  (0, 0)\t0.8463109166860171\n```\n\n\n### Error message\n\n```shell\nSee above\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nsee above\n```\n","comments":["cc @perimosocordiae @dschult , looks like it would be good to get this closed for 1.13.0","We missed the 1.13.0 cutoff unfortunately, but I have a PR to address this that we can backport if desired."],"labels":["defect","scipy.sparse"]},{"title":"ENH: 1D  rank filter speed up","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nDuring working on the [Hampel filter](https:\/\/github.com\/scipy\/scipy\/issues\/12809), I implemented a median filter and tested it vs the current implementation. Obviously, there are some tests that I did not implement. Nevertheless, seems like for any window bigger than 5, the new implementation is faster. I checked vs the faster implementation [scipy.ndimage.median_filter](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.ndimage.median_filter.html) rather than the slower [signal.medfilt](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.medfilt.html)\r\n\r\nI wonder if I am missing something. Otherwise, is in the community's interest that I will continue the development and replace the current implementation?\r\nWhy do I think that the current implementation is slower? I think that they did not use the median heap. Obviously, in case I am right, it might lead to new implementations of rank filter too.\r\n\r\nThe testing code and the implementation are [here](https:\/\/github.com\/ggkogan\/hampel\/blob\/main\/median_filter\/test_median.py):\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/94892341\/51de0743-7a89-4bfb-9d67-2ace795696a5)\r\n\r\n\r\n### Describe the solution you'd like.\r\n\r\n_No response_\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["There certainly is interest, I'd say!\r\n\r\nIdeally the perf improvement happens in ndimage --- the duplication between scipy.ndimage and scipy.signal filtering routines is mostly historical, and would be nice to reduce where reasonable (see e.g. [1]).  This is not set in stone, however, so if there's a reason to keep duplicating things, let's hear them.\r\n\r\n[1] https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/Z3Y6CZHGJ2G5GVA3EKLGATDCJX6PDW5P\/#T53S74F46GCNEJMSUSAAC4BCR4VQFO55","Well, for both rank filter and median filter, a reasonable 1D use case in signal processing can have a 21 window length. Meaning, that for each step we will replace 1 element from 21. I am not familiar with the image processing but it seems to me that for a window of 6x6=36, every step will have new 6 elements. Therefore a ratio between the new and the current number of elements is smaller and the current code is reasonable (in my tests the advantage os from a widow size of 7 which is larger than 6). \r\nPractiaclly, I was working in the direction of Hampel filter and there is no big effort for me to cover the median and the rank filter for the 1d case. 2D case is kind of changing the direction. I prefer to do 1 step a time :) ","@ev-br , in your opinion, assuming I am covering all the boundary conditions (symetric, anti-symetric, zero-pad, constant, etc), what are the additional tests required for the function?","1 step at a time is good. So the story along the lines of \"scipy.signal.medfilt does better in 1D and delegates to ndimage otherwise\" sounds reasonable. As a first step at least :-).\r\n\r\nRe tests. Good question. Both ndimage and scipy.signal versions are documented to operate on general N-D arrays, so ideally there are tests to clarify and fix the behavior. \r\nHaving kernels incommensurate with the array lengths would be great, I guess this is going to be covered in what you call boundary conditions.\r\nAlso from a cursory glance at the scipy.signal code, it looks like it matters whether the array and\/or kernel size is even or odd. Would be great to test all combinations, too.\r\n\r\nAlso the question of backwards compatibility is going to come up, so you'll need to be able to show it's preserved (or clearly show where it is not). I'm not entirely sure the current coverage is stellar, so maybe think of how to help in this.\r\nOff the cuff, maybe structure the commits into first tests which pass on main, then tests which may not (are any? hopefully it's all transparent for a user?), then your new implementation.","Seems like the median filters in both modules are running on top of the rank filter. I suggest renaming the issue to rank filter speed up and handle this function for the 1D case by integrating to the source code and show improvement in performance. Is it ok with you @ev-br ?\r\n\r\n> Also from a cursory glance at the scipy.signal code, it looks like it matters whether the array and\/or kernel size is even or odd. Would be great to test all combinations, too.\r\n\r\nI figured out that though it though the [documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.medfilt.html) says that the kernal should be odd, it accepts even values too. Assuming we move to handling the rank filter it will not bother us and I will modify the code to consider it.\r\n\r\n> Also the question of backwards compatibility is going to come up, so you'll need to be able to show it's preserved (or clearly show where it is not). I'm not entirely sure the current coverage is stellar, so maybe think of how to help in this.\r\n> Off the cuff, maybe structure the commits into first tests which pass on main, then tests which may not (are any? hopefully it's all transparent for a user?), then your new implementation.\r\n\r\nAssuming we are moving to handle the background functions only, it will be transparent to the user, ok?","PS: a second thought about when it should be more efficient in 2D: I assume that it will outperform the current implementation in the cases where ratio of the computation time is higher than the width of the kernel. For example: all the kernels with a width of 2 and a total kernel size larger than 11 should be beneficial. Or all the kernels with a width of 3 and a total kernel size larger than 19.","Finished handling all the modes\/boundary coditions and implemented all the required modifications to handle the rank_filter. Current results are attached. Now I will start moving from ctypes API to Python\/C API. \r\nI don't have an experience with this API, meaning that you will probably have to go over my code and make some corrections at the end of the process :( \r\n\r\n[implementation and testing code](https:\/\/github.com\/ggkogan\/hampel\/tree\/main\/rank_filter)\r\n\r\n![time_ratio_rank_filter](https:\/\/github.com\/scipy\/scipy\/assets\/94892341\/3d5bead3-adfa-4c58-909e-b01df5a8eec1)\r\n![time_ratio_median_filter](https:\/\/github.com\/scipy\/scipy\/assets\/94892341\/af979c14-44d5-438c-8a25-ecd886a1475a)\r\n","A couple of quick superficial comments:\r\n\r\n1. For the build, you'll need to use meson, see `ndimage\/meson.build` file.\r\n2. It may (or may not be) easier to use cython for the python <-> C glue. Or maybe if it's a single function which has no python user API, a manual C extension is easier. My point is I guess, not everything which ndimage does needs replicating.\r\n3. At some point we'll ask for an ASV benchmark to show the speedup. See https:\/\/github.com\/scipy\/scipy\/tree\/main\/benchmarks\/benchmarks\r\n4. Would be nice to document the source of https:\/\/github.com\/ggkogan\/hampel\/blob\/main\/rank_filter\/_rank_filter_1d.c"],"labels":["enhancement","scipy.ndimage"]},{"title":"M1 runners via Github Actions available","body":"### Is your feature request related to a problem? Please describe.\n\nM1 runners are now publicly available: https:\/\/github.blog\/changelog\/2024-01-30-github-actions-introducing-the-new-m1-macos-runner-available-to-open-source\/\r\n\r\nWould it make sense to migrate some of our Cirrus CI pipelines to GA? I am not really up to speed with our CI infrastructure\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["It would help those who build scipy on M1 Mac (like me). For me the build was smooth other than some linker warnings (about unsupported switches) which I safely ignored. `dev.py doc` had some warnings; some of these fixed by #19978; one other related to doc string raising a DivideByZero in np.linalg on M1 chip which I didn\u2019t bother to report). ","I'm intending to look into it, doing a similar thing for numpy at the moment. \r\nDebugging on a local Mac is actually one of the most powerful platforms that one can use, https:\/\/github.com\/numpy\/numpy\/wiki\/Debugging-CI-guidelines"],"labels":["enhancement","CI"]},{"title":"WIP: ENH: `stats.multivariate_t`: support `covariance` class and unify handling of infinite degrees of freedom","body":"#### Reference issue\r\nCloses #19069\r\nCloses #19930\r\n\r\n#### What does this implement\/fix?\r\n* Adds support for the `covariance class` in `multivariate_t` \r\n* Fix different behavior between the frozen and the regular distribution for `df=inf`. The frozen distribution directly returns a multivariate normal here while the regular distribution does not call multivariate normal methods in the main branch.\r\n \r\n#### Additional information\r\nThis turned out to be a minefield due to the interplay between `multivariate_t` and `multivariate_normal`. There is one test failure still where the frozen distribution does not return the same CDF when using the `covariance` class.","comments":[],"labels":["scipy.stats","enhancement"]},{"title":"BUG: linalg: lapack functions silently discard unused elements of input array","body":"### Describe your issue.\r\n\r\nWhen calling an `lapack_func` with an (illegal) array argument, only the first element is used, and the rest is silently discarded. It should instead throw an error.\r\n\r\nWhen supplying an array of size 1, it at least yields the warning \r\n\r\n```\r\nDeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n```\r\n\r\nwhich is how I discovered the issue in the first place.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom scipy.linalg import lapack\r\nimport numpy as np\r\n\r\nlartg = lapack.get_lapack_funcs(\"lartg\", dtype=np.float64)\r\n\r\nprint(lartg(1.2, 3.4))\r\n\r\n# no error?!\r\nprint(lartg(np.array([1.2, 2.3]), np.array([3.4, 4.5])))\r\n```\r\n```\r\n(0.3328201177351375, 0.9429903335828895, 3.605551275463989)\r\n(0.3328201177351375, 0.9429903335828895, 3.605551275463989)\r\n```\r\n\r\n### Error message\r\n\r\n```shell\r\nNone; that's the problem.\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.3 sys.version_info(major=3, minor=11, micro=6, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-wkrrblds\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp311-cp311\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["This is due to how Fortran works because everything is pass by reference. Hence it only consumes what it needs from that \"pointer\" and does not bother with the rest. \r\n\r\nWe can meticulously follow which argument is a scalar and which is not. Historically it wasn't done and nobody needed until now. Hence it is not a bug but indeed a rather careless wrapper. ","Bumpted the milestone: the fix is unlikely to materialize in time for https:\/\/github.com\/scipy\/scipy\/pull\/19880#issuecomment-1992010395"],"labels":["defect","scipy.linalg"]},{"title":"ENH: linking LAPACK from C\/C++","body":"### Is your feature request related to a problem? Please describe.\r\n\r\n`cython_{blas,lapack}` does a great job to make using LAPACK from Cython easy: just\r\n\r\n```\r\nfrom scipy.linalg.cython_lapack cimport dgeev\r\n```\r\n\r\nand you're up and running.\r\n\r\nUsing it from C or C++ should be as easy, but it currently isn't. \r\n\r\nThe ugly part is dealing with Fortran name mangling, which `cython_lapack`  hides. It actually does the job internally:\r\n\r\n```\r\n$ ll build\/scipy\/linalg\/_lapack_subroutine*\r\n-rw-rw-r-- 1 br br 246836 \u044f\u043d\u0432 25 21:59 build\/scipy\/linalg\/_lapack_subroutines.h\r\n-rw-rw-r-- 1 br br  34384 \u044f\u043d\u0432 25 21:59 build\/scipy\/linalg\/_lapack_subroutine_wrappers.f\r\n```\r\n\r\nHere `_lapack_subroutines.h` exports all the correctly named pointers:\r\n\r\n```\r\n$ less build\/scipy\/linalg\/_lapack_subroutines.h\r\n\/* This file was generated by _generate_pyx.py. *\/\r\n\/* Do not edit this file directly. *\/\r\n\r\n#ifndef SCIPY_LINALG_LAPACK_FORTRAN_WRAPPERS_H\r\n#define SCIPY_LINALG_LAPACK_FORTRAN_WRAPPERS_H\r\n#include \"fortran_defs.h\"\r\n#include \"numpy\/arrayobject.h\"\r\n\r\n<snip>\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif\r\n\r\nvoid F_FUNC(chla_transtypewrp, CHLA_TRANSTYPEWRP)(char *ret, int *trans);\r\nvoid F_FUNC(cladivwrp, CLADIVWRP)(npy_complex64 *ret, npy_complex64 *x, npy_complex64 *y);\r\nvoid F_FUNC(clangbwrp, CLANGBWRP)(float *ret, char *norm, int *n, int *kl, int *ku, npy_complex64 *ab, int *ldab, float *work);\r\n\r\n...\r\n\r\n```\r\n\r\nso that `#including`  `_lapack_signatures` looks like the solution for user code, and it even _almost_ works. \r\nUsing e.g. \r\n\r\n```patch\r\n$ git diff\r\ndiff --git a\/scipy\/interpolate\/meson.build b\/scipy\/interpolate\/meson.build\r\nindex b9b3ed852b..eaa5e68c33 100644\r\n--- a\/scipy\/interpolate\/meson.build\r\n+++ b\/scipy\/interpolate\/meson.build\r\n@@ -124,7 +124,7 @@ _bspl = py3.extension_module('_bspl',\r\n   cython_gen.process('_bspl.pyx'),\r\n   c_args: cython_c_args,\r\n   include_directories: 'src\/',\r\n-  dependencies: np_dep,\r\n+  dependencies: [lapack, np_dep],\r\n   link_args: version_link_args,\r\n   install: true,\r\n   subdir: 'scipy\/interpolate'\r\ndiff --git a\/scipy\/interpolate\/src\/__fitpack.h b\/scipy\/interpolate\/src\/__fitpack.h\r\nindex 97ac44a950..e70b081140 100644\r\n--- a\/scipy\/interpolate\/src\/__fitpack.h\r\n+++ b\/scipy\/interpolate\/src\/__fitpack.h\r\n@@ -1,3 +1,22 @@\r\n+\r\n+#include \"..\/linalg\/_lapack_subroutines.h\"\r\n+\r\n```\r\n\r\nfails to build with\r\n\r\n```\r\n$ python dev.py build\r\n<snip>\r\nFound ninja-1.11.1 at \/home\/br\/mambaforge\/envs\/scipy-dev\/bin\/ninja\r\nCleaning... 0 files.\r\n[32\/43] Compiling C object scipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p\/src__fitpackmodule.c.o\r\nFAILED: scipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p\/src__fitpackmodule.c.o \r\n\/home\/br\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc -Iscipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p -Iscipy\/interpolate -I..\/scipy\/interpolate -I..\/scipy\/interpolate\/src -I..\/..\/..\/..\/mambaforge\/envs\/scipy-dev\/lib\/python3.10\/site-packages\/numpy\/core\/include -I\/home\/br\/mambaforge\/envs\/scipy-dev\/include\/python3.10 -fvisibility=hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c99 -O2 -g -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/br\/mambaforge\/envs\/scipy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/br\/mambaforge\/envs\/scipy-dev\/include -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -MD -MQ scipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p\/src__fitpackmodule.c.o -MF scipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p\/src__fitpackmodule.c.o.d -o scipy\/interpolate\/_fitpack.cpython-310-x86_64-linux-gnu.so.p\/src__fitpackmodule.c.o -c ..\/scipy\/interpolate\/src\/_fitpackmodule.c\r\nIn file included from ..\/scipy\/interpolate\/src\/__fitpack.h:2,\r\n                 from ..\/scipy\/interpolate\/src\/_fitpackmodule.c:7:\r\nscipy\/interpolate\/..\/linalg\/_lapack_subroutines.h:6:10: fatal error: fortran_defs.h: No such file or directory\r\n    6 | #include \"fortran_defs.h\"\r\n      |          ^~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n[33\/43] Compiling C object scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/meson-generated__bspl.c.o\r\nFAILED: scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/meson-generated__bspl.c.o \r\n\/home\/br\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc -Iscipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p -Iscipy\/interpolate -I..\/scipy\/interpolate -I..\/scipy\/interpolate\/src -I..\/..\/..\/..\/mambaforge\/envs\/scipy-dev\/lib\/python3.10\/site-packages\/numpy\/core\/include -I\/home\/br\/mambaforge\/envs\/scipy-dev\/include -I\/home\/br\/mambaforge\/envs\/scipy-dev\/include\/python3.10 -fvisibility=hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c99 -O2 -g -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/br\/mambaforge\/envs\/scipy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/br\/mambaforge\/envs\/scipy-dev\/include -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -DCYTHON_CCOMPLEX=0 -MD -MQ scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/meson-generated__bspl.c.o -MF scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/meson-generated__bspl.c.o.d -o scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/meson-generated__bspl.c.o -c scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/_bspl.c\r\nIn file included from ..\/scipy\/interpolate\/src\/__fitpack.h:2,\r\n                 from scipy\/interpolate\/_bspl.cpython-310-x86_64-linux-gnu.so.p\/_bspl.c:1198:\r\nscipy\/interpolate\/..\/linalg\/_lapack_subroutines.h:6:10: fatal error: fortran_defs.h: No such file or directory\r\n    6 | #include \"fortran_defs.h\"\r\n      |          ^~~~~~~~~~~~~~~~\r\n```\r\n\r\n\r\n### Describe the solution you'd like.\r\n\r\nHave a canonical, single header file to include. \r\n\r\n### Describe alternatives you've considered.\r\n\r\nOf course replicating the name mangling manually at include sites works:\r\n\r\n```diff\r\ndiff --git a\/scipy\/interpolate\/src\/__fitpack.h b\/scipy\/interpolate\/src\/__fitpack.h\r\nindex 97ac44a950..e70b081140 100644\r\n--- a\/scipy\/interpolate\/src\/__fitpack.h\r\n+++ b\/scipy\/interpolate\/src\/__fitpack.h\r\n@@ -1,3 +1,22 @@\r\n+#include \"..\/linalg\/fortran_defs.h\"\r\n+\r\n+#define DLARTG F_FUNC(dlartg, DLARTG)\r\n+\r\n+#ifdef __cplusplus\r\n+extern \"C\" {\r\n+#endif\r\n+void DLARTG(double *f, double *g, double *cs, double *sn, double *r);\r\n+#ifdef __cplusplus\r\n+}\r\n+#endif\r\n```\r\n\r\nThis is what we had here and there before `cython_lapack.pxd`, and I'd much prefer to not have these beauties sprinkled across the codebase :-).\r\n\r\nI don't know if forward slashes in the include path are a problem on Windows. If they are, no biggie, can copy-paste the `F_FUNC` macro. But the point is it's ugly, it's brittle, it's error-prone, and it's reinventing a small wheel at each include site.  \r\n\r\n\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nI'm mostly interested in internal usage in scipy. Whether we want to make this a part of the public SciPy contract (as cython_lapack is) is a separate question.","comments":["Did you try to link directly to our vendored OpenBLAS? It should be a static library in my limited understanding. Meson has some magic and probably would be possible to link to it with `extern` machinery. I didn't try myself though so lots of blue-eyed optimism involved \r\n\r\n```C\r\n#include \"stdlib.h\"\r\n...\r\nextern void dgemm_(char*, char*, int*, int*,int*, double*, double*, int*, double*, int*, double*, double*, int*);\r\n\r\n```\r\n\r\nthen some \r\n```\r\ngcc .....  -L\/..\/.libs -lopenblas\r\n```\r\nlike argument \"should work\". I think it is high time that we use OpenBLAS\/MKL\/... as static libraries instead of going through these hoops but I have no idea where to start for the generic discussion. ","Linking to various LAPACK variants is IIUC abstracted away with `+  dependencies: [lapack, np_dep],` in `meson.build`.\r\n\r\nWhat's left is \r\n\r\n```\r\n$ cat scipy\/linalg\/fortran_defs.h \r\n\/*\r\n * Handle different Fortran conventions.\r\n *\/\r\n\r\n#if defined(NO_APPEND_FORTRAN)\r\n#if defined(UPPERCASE_FORTRAN)\r\n#define F_FUNC(f,F) F\r\n#else\r\n#define F_FUNC(f,F) f\r\n#endif\r\n#else\r\n#if defined(UPPERCASE_FORTRAN)\r\n#define F_FUNC(f,F) F##_\r\n#else\r\n#define F_FUNC(f,F) f##_\r\n#endif\r\n#endif\r\n```\r\n\r\nSo it may be `dgemm_`  or `dgemm` or `DGEMM_` or `DGEMM` depending on who knows what. And if what you wrote works today for you or me, it breaks for some packager the day after the release :-).\r\n\r\nBasically, this works for me today, so I personally am good for now:\r\n\r\n```\r\n#include \"..\/linalg\/fortran_defs.h\"\r\n\r\n#define DLARTG F_FUNC(dlartg, DLARTG)\r\n\r\nextern \"C\" {\r\nvoid DLARTG(double *f, double *g, double *cs, double *sn, double *r);\r\n}\r\n```\r\n\r\nand I know this header file is C++ so no need to guard the `extern \"C\"`. \r\n\r\nThis is not new, we carry these things since forever (cf https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/src\/_fitpackmodule.c#L127)\r\n\r\nMy point is this all is best hidden away for reuse. And `_lapack_subroutines.h` almost does it, just needs tweak to make the include path for `scipy\/linalg\/fortran_defs.h` to be a bit more robust (so that it works outside of scipy\/linalg).\r\n\r\n","> Linking to various LAPACK variants is IIUC abstracted away with `+ dependencies: [lapack, np_dep],` in `meson.build`.\r\n\r\nThis is correct indeed.\r\n\r\n\r\n\r\n> What's left is\r\n> \r\n> ```\r\n> $ cat scipy\/linalg\/fortran_defs.h \r\n> \/*\r\n>  * Handle different Fortran conventions.\r\n>  *\/\r\n> \r\n> #if defined(NO_APPEND_FORTRAN)\r\n> ```\r\n\r\nThis is not great to have multiple copies of indeed. I think the correct thing to do is to use [scipy\/_build_utils\/src\/npy_cblas.h](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/_build_utils\/src\/npy_cblas.h) for Fortran mangling handling in a central place. This is enough for all BLAS functions, and also already used for LAPACK functions in several places - and that's pretty clean then, e.g.:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/ca0acabbf30ca70a6fae20f039e034b2ba01feed\/scipy\/special\/lapack_defs.h#L5-L11","I think I am missing some details. \r\n\r\nOnce we have the dll or the a file, why are we still dealing with fortran headers? Symbol names and name mangling hell is indeed a problem but we can deal with that and treat the library. Isn't it why we are vendoring our own OpenBLAS copy? If we vendor our own copy then what is the difficulty of using it as a static lib? Same with MKL since they also have a stable ABI as far as I know. It doesn't matter if it came via Fortran or any other source if it is a static lib no? \r\n\r\nI would appreciate an explanation about this. ","> I think the correct thing to do is to use scipy\/_build_utils\/src\/npy_cblas.h for Fortran mangling handling in a central place.\r\n\r\nSo if this header needs to be used in constructing `cython_lapack` :-), which otherwise uses `linalg\/fortran_defs.h` for name mangling. And the OP still stands: it would be best to have all blas\/lapack prototypes in a central location  for all submodules to link to as needed.","> And the OP still stands: it would be best to have all blas\/lapack prototypes in a central location for all submodules to link to as needed.\r\n\r\nThat is a good goal, but it's not as obvious or straightforward as you may think I suspect to have one rather than two locations. A bit of history\/context:\r\n- `linalg\/fortran_defs.h` was introduced as part of the initial creation of the `cython_blas`\/`cython_lapack` API (PR gh-4021),\r\n- Some of the LAPACK wrapping, like `special\/lapack_defs.h` predates that,\r\n- The [paper on the Cython APIs](https:\/\/conference.scipy.org\/proceedings\/scipy2015\/pdfs\/ian_henriksen.pdf) from @insertinterestingnamehere describes the process, trade-offs, and potential follow-up work,\r\n   - I think that process (e.g. the role of `f2py` and moving away from it) explains why the codegen for the Cython API is completely separate today,\r\n- It's important to note that the Cython API is public API, and cannot easily be changed; we have [docs on that](http:\/\/scipy.github.io\/devdocs\/dev\/contributor\/public_cython_api.html), \r\n- While the Cython API is LP64 (32-bit) only, we later added support for ILP64 as well - this uses `_build_utils\/npy_cblas.h`. I also have a branch to reintroduce that and extend it to MKL\/Accelerate ILP64, which should be ready somewhere in the next couple of months,\r\n- One consequence of the above two bullets is that we cannot use an ILP64-only library; we _also_ need to link LP32 to not break the public Cython API even if internally every BLAS\/LAPACK call is to the ILP64 interface.\r\n\r\nThere are a limited number of LAPACK functions we use from C\/C++, so it seems very much feasible to add hand-written wrappers a la the `special\/lapack_defs.h` as we need them. This will be much easier to do than merging the Cython codegen and the headers it's using with the direct C\/C++ usage. I think we can quite easily centralize and document that in a \"how to use BLAS and LAPACK from C\/C++ within SciPy\", and that answers the goal you have here.\r\n\r\n> Isn't it why we are vendoring our own OpenBLAS copy? If we vendor our own copy then what is the difficulty of using it as a static lib? Same with MKL since they also have a stable ABI as far as I know. It doesn't matter if it came via Fortran or any other source if it is a static lib no?\r\n\r\nNo, the only reason we are vendoring OpenBLAS in the wheels on PyPI is because we have to to make wheels work. It's an anti-pattern basically that is specific to PyPI because of how wheels were designed; no other distro does it like that and everywhere else regular dynamic linking is used. And that `libopenblas.so` we vendor cannot be used as a static library (we've tried fairly recently, I forgot the exact details but IIRC it had to do with symbol renaming in the OpenBLAS Makefiles). MKL & co are also used as dynamic libraries; MKL is several hundred MB large, so not very reasonable to do as a static library. Dynamic libraries are better supported and required in many places; requiring static-only is not an option.\r\n\r\n> why are we still dealing with fortran headers? Symbol names and name mangling hell is indeed a problem but we can deal with that and treat the library\r\n\r\n\"treat the library for name mangling and symbol names\" is exactly what is being done in `npy_cblas.h`. That already works today.","> which should be ready somewhere in the next couple of months,\r\n\r\nOh, and I shouldn't have forgotten to include the great work by @thalassemia in gh-19816 which overlaps and in several respects is ahead of what I was working on. That PR also extends `npy_cblas.h` and moves `linalg\/fortran_defs.h` right next to it.","> There are a limited number of LAPACK functions we use from C\/C++, so it seems very much feasible to add hand-written wrappers a la the special\/lapack_defs.h as we need them.\r\n\r\nSure, it's possible. Weird though that we already have them all in the autogenerated `_lapack_subroutines.h`, but it is not usable because of `#include \"fortran_defs.h\"`.\r\n\r\n> While the Cython API is LP64 (32-bit) only, we later added support for ILP64 as well \r\n\r\nHaving ILP64 back would be great, actually. But once it's back, it's wanted in other scipy submodules, too. Both in cython (via `cython_lapack`) and from C++. \r\n","> No, the only reason we are vendoring OpenBLAS in the wheels on PyPI is because we have to to make wheels work. It's an anti-pattern basically that is specific to PyPI because of how wheels were designed; no other distro does it like that and everywhere else regular dynamic linking is used. \r\n\r\nThis is good to know, I was missing this piece. \r\n\r\n> And that libopenblas.so we vendor cannot be used as a static library (we've tried fairly recently, I forgot the exact details but IIRC it had to do with symbol renaming in the OpenBLAS Makefiles).\r\n\r\nIf this is the only problem, we should work on solving that. Actually, this library is\/should be fundamentally standalone code. BLAS and LAPACK in theory should use as little as possible given that it is older than everything we use. Plus it's core BLAS is not fortran anymore. I know the common practice is to put everything in the same basket but BLAS can be swapped out. LAPACK is typically the fortran parts. It is definitely not easy but certainly possible. If I understand correctly BLIS is doing just that.\r\n\r\n> MKL & co are also used as dynamic libraries; MKL is several hundred MB large, so not very reasonable to do as a static library. Dynamic libraries are better supported and required in many places; requiring static-only is not an option.\r\n\r\nProbably yes, but that is because BLAS\/LAPACK seems like sitting in even a bigger basket. I don't know what Intel would say but it would be really nice to have a modular parts of it. Or that I would speculate. I don't really know much about the MKL side.","> Having ILP64 back would be great, actually. But once it's back, it's wanted in other scipy submodules, too. Both in cython (via `cython_lapack`) and from C++.\r\n\r\nOnce you have it available from C\/C++, you can use it from Cython too - in the same way as from C\/C++ code (with `cdef extern from`, not via `cython_lapack`). The main limitation is that Cython's support for optional features is not great ([`IF` is deprecated](https:\/\/cython.readthedocs.io\/en\/latest\/src\/userguide\/migrating_to_cy30.html#deprecation-of-def-if)), so it'll require code generation to actually use this from Cython."],"labels":["enhancement","Build issues","maintenance","C\/C++"]},{"title":"ENH: add cobyqa to scipy.optimize.","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nCloses #19918.\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nThis PR adds the COBYQA method to the `scipy.optimize` module.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nThe initial results of the benchmarks are posted in https:\/\/github.com\/scipy\/scipy\/issues\/19918#issuecomment-1903708120.\r\n","comments":["Thanks for the PR @ragonneau , this was really quick! Just a quick lunch break review regarding the API.\r\n\r\nIf you edit your message above to \"Closes #19918\", github will automatically link the issue. Regarding lint failures: locally, these checks can be run via `python dev.py lint`.\r\n\r\nIt looks like COBYQA uses different variable names for the tolerances than scipy's solvers. Is `tol` something similar to scipy's `ftol` or `xtol` (see [here](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.minimize-powell.html#optimize-minimize-powell) for the classic Powell algorithm)?\r\n\r\nI did not see a test where the objective uses additional user supplied arguments. Does COBYQA already support that?","Hi @dschmitz89,\r\n\r\n> If you edit your message above to \"Closes #19918\", github will automatically link the issue. Regarding lint failures: locally, these checks can be run via `python dev.py lint`.\r\n\r\nI will check the lint results and modify COBYQA accordingly. I first focused on the primary unit tests and the benchmarks.\r\n\r\n> It looks like COBYQA uses different variable names for the tolerances than scipy's solvers. Is `tol` something similar to scipy's `ftol` or `xtol` (see [here](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.minimize-powell.html#optimize-minimize-powell) for the classic Powell algorithm)?\r\n\r\nYes, the option that controls the main stopping criterion tolerance is called `radius_final` in COBYQA. I did this to indicate clearly that this \"tolerance\" is a final trust-region radius. In `_minimize_cobyqa`, however, I renamed this options as `tol`, to match the SciPy standards. Did I misunderstand what was expected here?\r\n\r\nThanks,\r\nTom.\r\n","In the past `tol` has taken on various meanings for various optimisers. I'd like to work towards more uniformity. Here are some tols that one could use:\r\n\r\n- fatol, Absolute error in func(xopt) between iterations that is acceptable for convergence.\r\n- frtol, relative change in evaluated function value\r\n- xatol, Absolute error in xopt between iterations that is acceptable for convergence.\r\n- xrtol, relative change in parameter value\r\n- gatol, absolute change in gradient norm\r\n- grtol, relative change in gradient norm\r\n\r\nI don't think it's a good idea to use `tol` for keyword, it doesn't tell us by name what it does.\r\n\r\n w.r.t all the other options can you peruse the other optimisers to try and achieve convergence. e.g. there is a `radius_init` keyword, this appears to do the same as `initial_trust_radius` for [`'trust-exact'`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.minimize-trustexact.html#optimize-minimize-trustexact). (Although `trust-constr` uses `initial_tr_radius`.\r\n","> Regarding lint failures: locally, these checks can be run via `python dev.py lint`.\r\n\r\nI just took care of those in the COBYQA repo and updated the submodule here.","> In the past `tol` has taken on various meanings for various optimisers. I'd like to work towards more uniformity. Here are some tols that one could use:\r\n> \r\n>     * fatol, Absolute error in func(xopt) between iterations that is acceptable for convergence.\r\n> \r\n>     * frtol, relative change in evaluated function value\r\n> \r\n>     * xatol, Absolute error in xopt between iterations that is acceptable for convergence.\r\n> \r\n>     * xrtol, relative change in parameter value\r\n> \r\n>     * gatol, absolute change in gradient norm\r\n> \r\n>     * grtol, relative change in gradient norm\r\n\r\nGreat, thanks for the list.\r\n\r\n> I don't think it's a good idea to use `tol` for keyword, it doesn't tell us by name what it does.\r\n\r\nI totally agree, this is why I did not use it in the code of COBYQA.\r\n\r\n> w.r.t all the other options can you peruse the other optimisers to try and achieve convergence. e.g. there is a `radius_init` keyword, this appears to do the same as `initial_trust_radius` for [`'trust-exact'`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.minimize-trustexact.html#optimize-minimize-trustexact). (Although `trust-constr` uses `initial_tr_radius`.\r\n\r\nI agree that uniformity is preferred. However, I believe that `initial_tr_radius` is much better than `initial_trust_radius`, as the latter does not really make sense mathematically. I could definitely change `radius_init` and `tol` in `_minimize_cobyqa` to `initial_tr_radius` and `final_tr_radius` (of course the code in `_minimize.py` needs to be updated correspondingly). What do you think @andyfaff?\r\n\r\nCheers,\r\nTom.","Hi @andyfaff,\r\n\r\nIn the updates of this PR, I proposed `initial_tr_radius` and `final_tr_radius` as the option names for COBYQA. I updated `scipy\/optimize\/_minimize.py` to set the default value of `options['final_tr_radius']` to `tol`. What do you think?\r\n\r\nI really believe that `initial_trust_radius` is not a good name, as it does not make much sense mathematically. I understand that `initial_tr_radius` has its flaws (in particular, the abbreviations used are not consistent), but I believe it is directly understandable by any engineer\/researcher familiar with trust-region methods.","Hi @dschmitz89,\r\n\r\n> I did not see a test where the objective uses additional user supplied arguments. Does COBYQA already support that?\r\n\r\nIn the updates of this PR, I improved the `scipy\/optimize\/tests\/test_cobyqa.py`. In particular, the argument `args` of the `minimize` function is tested. What do you think of the new tests?\r\n","> Hi @dschmitz89,\r\n> \r\n> > I did not see a test where the objective uses additional user supplied arguments. Does COBYQA already support that?\r\n> \r\n> In the updates of this PR, I improved the `scipy\/optimize\/tests\/test_cobyqa.py`. In particular, the argument `args` of the `minimize` function is tested. What do you think of the new tests?\r\n\r\nThe new tests looks good, thank you!\r\n\r\nThere will likely be more detailed reviews in the coming weeks still.","Hi @andyfaff and @rgommers,\r\n\r\nI am unsure how to solve the \"Linux Meson tests\" issue. I did not install the test files of COBYQA in Meson because I do not want them to be detected. Running the full suite takes a long time. How should we proceed?\r\n\r\nCheers,\r\nTom.","Are there basic but nontrivial problems for the testing that won't take too much time? Alternatively, we can create our own simple test suite by just taking a subset of yours and mark them `slow` so they only run in some of the test actions. At least the input validations should be present to catch strange API edge cases.\r\n\r\nI will probably need to open a PR on your side to change all printing to logging since we are slowly trying to leave that historical behavior to rest. Logging is a much better and powerful system to generate output and it would be nice if we do this from the get go instead of this olden way of displaying stuff. ","> I did not install the test files of COBYQA in Meson because I do not want them to be detected. Running the full suite takes a long time. How should we proceed?\r\n\r\nAside from what @ilayn has said regarding the tests we want for SciPy, you can resolve the CI error by adding the paths to https:\/\/github.com\/scipy\/scipy\/blob\/1e3e8c603126f1bf2e8febd567cfd374d169283b\/tools\/check_installation.py#L37-L43","Hi @dschmitz89 and @ragonneau,\n\n> What is called target here is called f_min for SHGO and DIRECT. For consistency let us stick with the same parameter name here.\n\nIndeed, I would suggest the opposite. As a person working in optimization for more than 15 years, I have never heard anyone meant target value by f_min, which exclusively mean the minimum of f. To be frank, this is a wrong name in the current context.\n\nIn contrast, ftarget or f_target is the standard term adopted in popular optimization packages. @ragonneau please put some references here.\n\nThanks.\n\nZaikun","Hi @zaikunzhang,\r\n\r\n> In contrast, ftarget or f_target is the standard term adopted in popular optimization packages. @ragonneau please put some references here.\r\n\r\nI suppose you are referring to the following.\r\n1. BFO (https:\/\/sites.google.com\/site\/bfocode) from Ph. L. Toint and M. Porcelli.\r\n2. A. Friedlander and J. M. Mart\u00ednez. Convergence and complexity in nonlinear optimization. Course dispensed at Unicamp, S\u00e3o Paulo, Brazil. 2017. url: https:\/\/www.ic.fcen.uba.ar\/elavio\/friedlandermartinez.pdf.\r\n3. R. Tanabe. \"Benchmarking feature-based algorithm selection systems for black-box numerical optimization.\" IEEE Trans. Evol. Comput. 26.6 (2022), pp. 1321\u20131335. doi: [10.1109\/TEVC.2022.3169770](https:\/\/doi.org\/10.1109\/TEVC.2022.3169770).\r\n\r\nNote that they referred to it as `ftarget` and not `f_target`. It is clear that `ftarget` here is not possible as it does not really comply with the Python naming standard [PEP8](https:\/\/peps.python.org\/pep-0008\/).\r\n\r\nCheers,\r\nTom.","> Hi @dschmitz89 and @ragonneau,\r\n> \r\n> > What is called target here is called f_min for SHGO and DIRECT. For consistency let us stick with the same parameter name here.\r\n> \r\n> Indeed, I would suggest the opposite. As a person working in optimization for more than 15 years, I have never heard anyone meant target value by f_min, which exclusively mean the minimum of f. To be frank, this is a wrong name in the current context.\r\n> \r\n> In contrast, ftarget or f_target is the standard term adopted in popular optimization packages. @ragonneau please put some references here.\r\n> \r\n> Thanks.\r\n> \r\n> Zaikun\r\n\r\nI agree that `f_min` is not a very catchy\/easy to grasp parameter name. `nlopt` for example calls this parameter `stopval`: https:\/\/nlopt.readthedocs.io\/en\/latest\/NLopt_Reference\/#stopping-criteria\r\n\r\nWe cannot change it for the existing optimizers without a deprecation cycle though. It would also be great to have this stopping criterion for all optimizers in principle.\r\n\r\nAs a library, we have to find a compromise between internal consistency and \"managed evolution\". In my view, we should try to unify this parameter name in one go (one release) to avoid user confusion. Open to other opinions though.","I'd prefer to use f_target for this PR and deprecate the rest","> I'd prefer to use f_target for this PR and deprecate the rest\r\n\r\nHi @andyfaff,\r\n\r\nThe most recent update of this PR renames `f_min` to `f_target`. Do you see anything else I can do right now?\r\n\r\nCheers,\r\nTom.","> I will probably need to open a PR on your side to change all printing to logging since we are slowly trying to leave that historical behavior to rest. Logging is a much better and powerful system to generate output and it would be nice if we do this from the get go instead of this olden way of displaying stuff.\r\n\r\nHi @ilayn,\r\n\r\nI understand. The `disp` option in COBYQA not only displays the returned `message` but also the objective and constraint function evaluations and information about the progress of the optimization procedure. Is this the expected behavior of the `disp` option in SciPy?\r\n\r\nCheers,\r\nTom.","Could we leave the `stdout -> logging` update for a follow up PR? This PR looks in good shape to me otherwise. I will run workflows once more.","Yes of course, I'll circle back to logging in the original source. We probably leave this merge to 1.14 though just to go through things a bit more thoroughly. ","> I will run workflows once more.\r\n\r\nThat just crossed paths with a breaking change in numpy - I retriggered the jobs with build failures.","> The benchmarks in CI didn't actually finish - I restarted the run. There are results posted at [#19918 (comment)](https:\/\/github.com\/scipy\/scipy\/issues\/19918#issuecomment-1903708120) - it'd be useful to edit the PR description and cross-link those, and maybe the main takeaway(s) for the folks who just look at this PR.\r\n\r\nI put the link you provided in the description of this PR.\r\n\r\nCheers,\r\nTom.","Hi everyone,\r\n\r\nI have discussed with Tom about the PR, and everything looks green to me. Thank everyone, especially those from the SciPy community, for the suggestions and help. \r\n\r\nWe believe that COBYQA will turn out useful to SciPy users, and the feedback will in turn help Tom and me improve COBYQA and develop other optimization packages, including but not limited to [PRIMA](http:\/\/www.libprima.net). \r\n\r\nThanks again. \r\n\r\nBest regards,\r\nZaikun","Hi @rgommers\r\n\r\nThe most recent update of this PR should patch the issue occurring in the tests.\r\n\r\nOne CI will, however, not pass since the test files of the [cobyqa](https:\/\/github.com\/cobyqa\/cobyqa) repo are not included in the `meson.build` files. How should we handle this?\r\n\r\nCheers,\r\nTom.\r\n","The commit right before the last one had failing CI jobs from `TestOptimizeSimple.test_nan_values[cobyqa]`. I restarted CI now.\r\n\r\n@ragonneau if you want to submit a trivial PR with some minor rewording of docs or a typo fix (doesn't matter which one), then we can merge that PR straight away, after which new pushes to this PR should no longer need maintainer approval for CI to run.","> One CI will, however, not pass since the test files of the [cobyqa](https:\/\/github.com\/cobyqa\/cobyqa) repo are not included in the `meson.build` files. How should we handle this?\r\n\r\nThe commit I just pushed fixed the problem.","Hi @rgommers,\r\n\r\n> The commit I just pushed fixed the problem.\r\n\r\nGreat, thank you very much! There is one more set of tests:\r\n```\r\ncobyqa\/cobyqa\/utils\/tests\/test_exceptions.py\r\ncobyqa\/cobyqa\/utils\/tests\/test_math.py\r\n```\r\n\r\nThe CI should succeed after this modification. I just included them in `check_installation.py` on my fork, but it did not sync with this PR. I am not sure what is happening...\r\n\r\nCheers,\r\nTom.","Hi @rgommers,\r\n\r\nCould you look at the most recent version of `check_installation.py`? I am not sure why the CI fails, as `_lib\/cobyqa\/cobyqa\/tests\/test_models.py` is included in the blacklisted files...\r\n\r\nThanks for your help,\r\nCheers,\r\nTom.","Sorry about that - should be fixed now. I improved the error messages while I was at it, so in case it fails again with a future submodule update, it's clearer how to resolve it.","> The integration looks pretty clean. I only have a few minor comments on how that is done.\r\n\r\nMy comments were addressed, this LGTM now. I didn't look at the algorithmic parts, so I'll leave the merge to @andyfaff or @dschmitz89 ","> Sorry about that - should be fixed now. I improved the error messages while I was at it, so in case it fails again with a future submodule update, it's clearer how to resolve it.\r\n\r\nNo trouble at all. I simply thought the `_lib\/` prefix was necessary, as it seems to be employed by `array_api_compat`. Let's see the outcomes of the CI now. Thank you very much for the help!\r\n\r\nCheers,\r\nTom.","Hi @andyfaff @dschmitz89,\r\n\r\nDo you see anything else I could improve for this PR?\r\n\r\nCheers,\r\nTom.","@ragonneau  I'll give this one two more weeks for comments\/objections, then I will merge. Currently, the NumPy 2.0 release is drawing a lot of attention from many maintainers, so less time for regular reviews.","> I've spent an hour or two on this. The code is probably in a good state, but there are some important things that need to be addressed before merge.\r\n\r\nThank you very much for taking the time. I will respond to each of your comments.\r\n\r\nCheers,\r\nTom.","I don't have the right to restart CircleCI's jobs. They should now pass, as `jupyterlite-sphinx` 0.13.1 is now released. Can someone restart them to confirm that this PR is in good shape?\r\n\r\nThanks,\r\nTom."],"labels":["enhancement","scipy.optimize"]},{"title":"BUG: Scipy Optimize with Nelder-Mead method has issues when specifying initial guess as upper bound value","body":"### Describe your issue.\n\nHi,\r\n      I ran across the following peculiar issue with scipy optimize using Nelder Mead with bounds . I believe a code snippet will be more useful here:\r\n\r\n\r\nNow we know that minimum is indeed when x = 0 . If we initialize Nelder Mead method above, with max bound value = initial guess, the algorithm returns 4 as the optimum value. However , if we change either the max bound value or initial guess then it correctly converges. I am not sure why we are getting this peculiar case and why it seems to be specific to Nelder_Mead case. This can ,be depending on context, a benign to a fairly insidious error (in my case as I am trying to optimize wear on a wall lining and hence initial value would be equal to max known wall thickness)\r\n\r\nPlease let me know if you can reproduce the above error and whether it can be fixed readily.\r\nThe work around for me rn is to just increase the bounds by a small epsilon\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nfrom scipy.optimize import minimize\r\ndef d(x):\r\n    return x**2-3\r\n\r\n\r\nw = minimize(d,np.array([4]),bounds=[[0,4]],method=\"Nelder-Mead\")\n```\n\n\n### Error message\n\n```shell\nN\/A as  it's a logical bug\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.1 1.24.1 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.35\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-gwuhujd5\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-t4t9eut3\\cp310-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.10'\n```\n","comments":[],"labels":["defect","scipy.optimize"]},{"title":"MAINT: factorial clean-ups","body":"Preparation for complex support in factorial functions (see #18409), which needs some preparations in the tests, as well as an auxiliary function `_is_subdtype` to be able to sanely express the necessary subtype conditions.\r\n\r\nNo functional changes in this PR, aside from changing the text of some warning messages. Also tightens some tests that were using the woefully inadequate default of `rtol=1e-7` in `assert_allclose`\r\n\r\nPreview of what this PR is preparing for can be found in #19988","comments":["The failures here are just minor tolerance violations (because I tightened things from the very loose default of `1e-7` of `assert_allclose`) - I'll bump the tolerance as necessary once there are any review requests.","Just saw https:\/\/github.com\/numpy\/numpy\/pull\/24680 & https:\/\/github.com\/numpy\/numpy\/pull\/24770 in the numpy 2.0 release notes. That's even nicer (and IMO enough if we're testing this on numpy 2.0 only). Thanks a lot @mdhaber! \ud83d\ude4f "],"labels":["scipy.special","maintenance"]},{"title":"WIP: Preview of upcoming factorial changes","body":"This PR is not meant to be merged for now, it combines ~4 planned PRs that follow #19716. It mainly serves as a preview, as well as for discussion purposes vis-\u00e0-vis #19987\r\n\r\n```\r\n174e7722db (HEAD -> factorial_unify) MAINT: unify implementations for factorial{,2,k}           # PR Nr. 4\r\nf502fbcbe1 MAINT: add short-cut to _range_prod and unify calling pattern also for factorial\r\nc2110ff456 (factorial_complex) MAINT: also test infinities for factorial{,2,k}                  # PR Nr. 3\r\n5590a50fd3 ENH: extend factorial{,2,k} to allow complex inputs\r\n042980c569 MAINT: remove helpmsg from factorialk errors; will reach parity with factorial{,2}\r\nc69bf03484 MAINT: prepare tests for `extend=` kwarg of factorial{,2,k}\r\ned245ece05 (factorial_types) MAINT: also test factorial{,2,k} with complex NaNs                 # PR Nr. 2\r\n70027abe73 MAINT: be consistent about dtype in factorial{,2,k} also for empty\/None.\r\n963d26e691 (factorial_cleanup) MAINT: minor test refactors & clean-ups for factorial functions  # PR Nr. 1 == #19989\r\nca2f900ba4 MAINT: ensure assert_allclose uses useful tolerance for factorial{,2,k} tests\r\nedc0f8ce26 MAINT: homogenise cases in test_factorialk_array_corner_cases\r\n8cb063e391 MAINT: refactor test_factorial*_corner_cases and test_factorialx_nan\r\nbcbcb13aea MAINT: harmonise factorial{,2,k} error message for unsupported types\r\nd91751f7f0 MAINT: break out test for `factorialk(..., exact=None)`\r\n2b4d56b3ef MAINT: break out assert_really_equal utility, teach it rtol\r\nc9a5bf7e32 MAINT: introduce subtype helper function for factorial{,2,k}\r\nee4429d0ed MAINT: improve testing of correct dtypes in test_factorial_array_corner_cases\r\n```","comments":[],"labels":["scipy.special"]},{"title":"MAINT: Rewrite factorial implementation as ufunc","body":"Besides being more efficient, I think it would actually be simpler from a maintenance perspective just to write a scalar kernel and use the existing ufunc infrastructure. I think the switch-over point is when you begin needing some form of control flow. I can submit a nested PR to your PR branch if you'd like, giving a simple C implementation.\r\n\r\n_Originally posted by @steppi in https:\/\/github.com\/scipy\/scipy\/pull\/19716#discussion_r1465446775_\r\n\r\nFollow-up from #19716","comments":["For figuring out how we want to set up the ufunc, here's a draft preview of the factorial changes that I already have in the pipeline: #19988 (split into 2 clean-ups, adding complex support, and finally unifying the implementations).","@steppi, given #19983 \/ #19988, how would you like to tackle the ufunc-for-factorial question? Should we prioritize implementing this now? Or wait until #19988 (incl. its predecessors) have landed? Do you think any\/all of that should land for 1.13?"],"labels":["scipy.special","maintenance"]},{"title":"ENH: Providing an interface for symbolic and numerical factorization of sparse matrices","body":"### Is your feature request related to a problem? Please describe.\n\nIn certain applications, symbolic factorizations of a sparse matrix can be re-used and only numerical factorization redone. Since the former is more expensive than the latter, it would be nice to be able to cache the symbolic factorization and be able to re-use it for the numerical step.\n\n### Describe the solution you'd like.\n\nHave an interface for sparse matrix solvers in scipy s.t. one can a) call a symbolic_factorization(A) which generates the symbolic factorization and caches it. and then numerical_solve(A) which does the numerical factorization with a cached symbolic factorization.\n\n### Describe alternatives you've considered.\n\numfpack\n\n### Additional context (e.g. screenshots, GIFs)\n\nsample application: https:\/\/pubs.aip.org\/aip\/apl\/article-abstract\/123\/22\/221106\/2925198\/Computational-scaling-in-inverse-photonic-design?redirectedFrom=fulltext","comments":[],"labels":["enhancement","scipy.sparse"]},{"title":"WIP: ENH: add Stirling numbers of the first kind","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nNA\r\n\r\n#### What does this implement\/fix?\r\nWIP PR for implementation of Stirling numbers of the first kind. \r\n\r\n#### Additional information\r\nThis is WIP CR. Having some issues building on laptop so pulling this into AWS ubuntu host to see if I can debug build there \r\n","comments":[],"labels":["enhancement","scipy.special","C\/C++"]},{"title":"BUG: Slow addition of CSC (sparse) arrays","body":"### Describe your issue.\r\n\r\nAdding two CSC arrays should be very fast for the small amount of elements. However, this is not the case. Summing two 2^30 arrays with a total of 3 non-zero elements takes around 5 seconds. Moreover, for larger arrays, it just segfaults.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\na = csc_array((2 ** 30, 1), dtype=np.complex128)\r\nb = dok_array((2 ** 30, 1), dtype=np.complex128)\r\nb[42949672, 0] = b[40257126, 0] = b[18785238, 0] = 0.1j\r\nc = b.tocsc()\r\na + c\r\n```\r\n\r\n\r\n\r\n### Error message\r\nNone\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.12.0 1.26.3 sys.version_info(major=3, minor=11, micro=7, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-ewarq4oq\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-73jfxkej\/cp311-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["I can verify the slow response. It goes away if you sort the indices of the  csc_array. That is, if the csc_array is in canonical format (sorted indices and no duplicate entries) it can use the faster code path. The general code path is apparently slow for all binary operations (e.g. `a < c` is also slow).\r\n\r\nWorkaround: try using `c.sort_indices()` before the addition.\r\nIf you have duplicate entries, `c.sum_duplicates()` is also needed."],"labels":["defect","scipy.sparse"]},{"title":"BUG: special: test_precompute_gammainc.py::test_gammaincc fails on main","body":"```\r\n__________________________________________________________________________________________________ test_gammaincc ___________________________________________________________________________________________________\r\n[gw10] linux -- Python 3.10.13 \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/python\r\nscipy\/special\/tests\/test_precompute_gammainc.py:99: in test_gammaincc\r\n    assert_mpmath_equal(lambda a, x: gammaincc(a, x, dps=1000),\r\nscipy\/special\/_mptestutils.py:295: in assert_mpmath_equal\r\n    d.check()\r\n        a          = (<function test_gammaincc.<locals>.<lambda> at 0x7f3cb01b1d80>, <function test_gammaincc.<locals>.<lambda> at 0x7f3cb0...[<scipy.special._mptestutils.Arg object at 0x7f3cb02041f0>, <scipy.special._mptestutils.Arg object at 0x7f3cb02041c0>])\r\n        d          = <MpmathData: <lambda>>\r\n        kw         = {'dps': 1000, 'n': 50, 'nan_ok': False, 'rtol': 1e-17}\r\nscipy\/special\/_mptestutils.py:282: in check\r\n    raise value\r\n        argarr     = array([[ 20.        ,  20.        ],\r\n       [ 20.        ,  25.16997901],\r\n       [ 20.        ,  31.67639218],\r\n       ...69106],\r\n       [100.        ,  63.13850356],\r\n       [100.        ,  79.45974047],\r\n       [100.        , 100.        ]])\r\n        dps        = 1000\r\n        dps_list   = [1000]\r\n        j          = 0\r\n        mptype     = <function MpmathData.check.<locals>.mptype at 0x7f3cb01b1f30>\r\n        old_dps    = 15\r\n        old_prec   = 53\r\n        pytype     = <function MpmathData.check.<locals>.pytype at 0x7f3cb01b1fc0>\r\n        self       = <MpmathData: <lambda>>\r\n        tb         = <traceback object at 0x7f3c2a9db580>\r\n        tp         = <class 'AssertionError'>\r\n        value      = AssertionError('\\nMax |adiff|: 4.02841e-15\\nMax |rdiff|: 9.04088\\nBad results (10 out of 64) for the following points ...     100. =>          3.207917587318186e-05 !=           3.20791758763931e-05  (rdiff         1.0010370019945959e-10)')\r\nscipy\/special\/_mptestutils.py:263: in check\r\n    assert_func_equal(\r\n        argarr     = array([[ 20.        ,  20.        ],\r\n       [ 20.        ,  25.16997901],\r\n       [ 20.        ,  31.67639218],\r\n       ...69106],\r\n       [100.        ,  63.13850356],\r\n       [100.        ,  79.45974047],\r\n       [100.        , 100.        ]])\r\n        dps        = 1000\r\n        dps_list   = [1000]\r\n        j          = 0\r\n        mptype     = <function MpmathData.check.<locals>.mptype at 0x7f3cb01b1f30>\r\n        old_dps    = 15\r\n        old_prec   = 53\r\n        pytype     = <function MpmathData.check.<locals>.pytype at 0x7f3cb01b1fc0>\r\n        self       = <MpmathData: <lambda>>\r\n        tb         = <traceback object at 0x7f3c2a9db580>\r\n        tp         = <class 'AssertionError'>\r\n        value      = AssertionError('\\nMax |adiff|: 4.02841e-15\\nMax |rdiff|: 9.04088\\nBad results (10 out of 64) for the following points ...     100. =>          3.207917587318186e-05 !=           3.20791758763931e-05  (rdiff         1.0010370019945959e-10)')\r\nscipy\/special\/_testutils.py:85: in assert_func_equal\r\n    fdata.check()\r\nE   AssertionError: \r\nE   Max |adiff|: 4.02841e-15\r\nE   Max |rdiff|: 9.04088\r\nE   Bad results (10 out of 64) for the following points (in output 0):\r\nE               31.676392175331582              25.169979012836535 =>             0.8821758376668161 !=              0.882175837666816  (rdiff         1.2585053650545238e-16)\r\nE               31.676392175331582              31.676392175331582 =>             0.4763684762586634 !=             0.4763684762586632  (rdiff           4.66119434831081e-16)\r\nE               31.676392175331582               39.86470631277378 =>            0.08020914302223373 !=            0.08020914302223293  (rdiff          9.862141337505123e-15)\r\nE               31.676392175331582              50.169691062270374 =>           0.002122219704622387 !=           0.002122219704620808  (rdiff          7.438430430655876e-13)\r\nE               31.676392175331582               63.13850355589194 =>          4.493859708366356e-06 !=          4.493859705971622e-06  (rdiff          5.328902649973653e-10)\r\nE               31.676392175331582               79.45974047018522 =>         3.6016673922928156e-10 !=         3.6016352765703096e-10  (rdiff          8.916983547711702e-06)\r\nE               31.676392175331582                            100. =>          4.473988644247102e-15 !=          4.455775242189288e-16  (rdiff              9.040875944292166)\r\nE                63.13850355589194               63.13850355589194 =>             0.4832629649118442 !=            0.48326296491184445  (rdiff          5.743369062161014e-16)\r\nE                63.13850355589194               79.45974047018522 =>           0.026138861275767616 !=           0.026138861275769205  (rdiff          6.079096894200853e-14)\r\nE                63.13850355589194                            100. =>          3.207917587318186e-05 !=           3.20791758763931e-05  (rdiff         1.0010370019945959e-10)\r\n        atol       = 1e-300\r\n        data       = array([[ 20.        ,  20.        ],\r\n       [ 20.        ,  25.16997901],\r\n       [ 20.        ,  31.67639218],\r\n       ...69106],\r\n       [100.        ,  63.13850356],\r\n       [100.        ,  79.45974047],\r\n       [100.        , 100.        ]])\r\n        distinguish_nan_and_inf = True\r\n        dtype      = None\r\n        fdata      = <Data for <lambda>>\r\n        func       = <function test_gammaincc.<locals>.<lambda> at 0x7f3cb01b1d80>\r\n        ignore_inf_sign = False\r\n        knownfailure = None\r\n        nan_ok     = False\r\n        nparams    = 2\r\n        param_filter = None\r\n        points     = array([[ 20.        ,  20.        ],\r\n       [ 20.        ,  25.16997901],\r\n       [ 20.        ,  31.67639218],\r\n       ...69106],\r\n       [100.        ,  63.13850356],\r\n       [100.        ,  79.45974047],\r\n       [100.        , 100.        ]])\r\n        result_columns = None\r\n        result_func = <function MpmathData.check.<locals>.<lambda> at 0x7f3cb01b1e10>\r\n        results    = <function MpmathData.check.<locals>.<lambda> at 0x7f3cb01b1e10>\r\n        rtol       = 1e-17\r\n        vectorized = False\r\n============================================================================================== short test summary info ==============================================================================================\r\nFAILED scipy\/special\/tests\/test_precompute_gammainc.py::test_gammaincc - AssertionError: \r\n======================================================================== 1 failed, 3190 passed, 53 skipped, 43 xfailed, 2 xpassed in 15.27s =========================================================================\r\n```\r\n```\r\n>>> import sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n1.13.0.dev0+1257.a927b24 1.26.2 sys.version_info(major=3, minor=10, micro=13, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    pc file directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.25\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    pc file directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.25\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include,\r\n      -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include,\r\n      -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  c++:\r\n    args: -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.7\r\n  fortran:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/miniconda3\/envs\/scipy-dev\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/python3.10\r\n  version: '3.10'\r\n```","comments":[],"labels":["defect","scipy.special"]},{"title":"ENH: interpolate: replicate `splrep` and `splprep` in Python","body":"#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\nImplement a clone of `splrep` (== UnivariateSpline)  and `splprep` in python, without FITPACK.\r\n\r\nThree objects are added:\r\n\r\n- `scipy.interpolate.make_splrep` : a clone of `splrep`\r\n- `scipy.interpolate.make_splprep` : a close of `splprep`\r\n\r\nThese two functions have signatures consistent with their FITPACK counterparts, and return a BSpline object, which is consistent with `make_interp_spline`.\r\n\r\n- `scipy.interpolate.generate_knots`: this is a generator which constructs knot vectors of increasing length for spline fitting. The core logic replicates that of FITPACK and the code closely follows the Fortran source.\r\n\r\nThe use of a generator was suggested in https:\/\/github.com\/scipy\/scipy\/issues\/2579 as alternative to FITPACK's black box routine (see the issue for detailed critique of the existing API).\r\n\r\nThe `generate_knots` API is a bit lower level: it generates knot vectors only, not smoothing splines. This way, we decouple the knot selection from spline construction, and power users can mix and match various strategies for constructing the spline itself given knots.\r\n\r\nOne other argument for exposing a generator is that it steps around task={-1, 0, 1} arguments completely (https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.interpolate.splrep.html).\r\n\r\nThe generator has two stopping conditions:\r\n\r\n- via the smoothing factor s, or\r\n- via the maximum number of knots, nest (the name follows that of FITPACK, maybe worth renaming if user-facing).\r\n\r\nThe latter is available in the Fortran FITPACK, but effectively disabled in the Fortran-to-python glue layer: https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/src\/fitpack.pyf#L277 or https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/src\/fitpack.pyf#L310.\r\n\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\nImplementation closely follows that of FITPACK, mainly https:\/\/github.com\/scipy\/scipy\/blob\/maintenance\/1.11.x\/scipy\/interpolate\/fitpack\/fpcurf.f and its dependencies.","comments":["Thanks Tyler, Jake! Indeed, the plan is to move parts of this from python to C. I'm doing it piecemeal over at gh-19753, with an intent to sync this one when ready. How much to move to C is still in a bit of a flux: on one hand it's better to keep in python everything which does not strictly need to be in C; on the other hand it gets a bit messy with a mix of C, python and Cython in between. I'll work on this a bit more I guess, to see how this pans out.  \r\n\r\nSpeaking of reviews --- I'd be much obliged if you could review this one. Please ping me on anything you'd want a non-expert review on!\r\nWhat would you actually prefer to review --- a three-PR stack or a single one or some other way?\r\n","I think this is ready from my side. \r\n\r\nSeveral cleanups are probably needed for the cpp side. At least \r\n- how to best document cpp routines; is the current way OK, or do we want to go the full doxygen way (hopefully not),\r\n- the current version probably inlines things too aggressively,\r\n\r\nThat said, I'm going to leave this PR as is, and first take a look at parametric version, `splPrep` instead. \r\nFITPACK uses two separate routines (https:\/\/github.com\/scipy\/scipy\/blob\/maintenance\/1.11.x\/scipy\/interpolate\/fitpack\/fpcurf.f and https:\/\/github.com\/scipy\/scipy\/blob\/maintenance\/1.11.x\/scipy\/interpolate\/fitpack\/fppara.f), and I'm wondering how much of the infra could be shared and what really needs to be separate.\r\n\r\nSo unless there's a review preference, the plan is to defer cleanups to a follow-up, together with the parametric stuff.\r\nMeanwhile this PR can be considered as is, and reviews would be much appreciated.\r\n\r\nEDIT: forgot to push a commit with tests requested by reviewers. Pushed now.","> I'm going to leave this PR as is, and first take a look at parametric version, splPrep instead.\r\n\r\nHuh. Implementing splprep turned out to be a great of a dogfooding exercise. The actual change is a dozen of LOC (+ docstrings, tests) _after_ a bit of cleanup of python side internals. So I took an opportunity to do this cleanup and push it here."],"labels":["enhancement","scipy.interpolate","Fortran","Cython"]},{"title":"DOC: Add examples to `scipy.interpolate.spalde`","body":"#### Reference issue\r\nDOC: Add \"Examples\" to docstrings https:\/\/github.com\/scipy\/scipy\/issues\/7168 (partial)\r\n\r\n#### What does this implement\/fix?\r\nAdds example for interpolate.spalde and unifies descriptions along the docstrings in the file.\r\nFixes previous attemps (#19434) to pass doc build and refgude-check.\r\n\r\n[skip actions] [skip cirrus]\r\n","comments":[],"labels":["scipy.interpolate","Documentation"]},{"title":"DOC: Colors\/design with PyData Sphinx Theme > 0.15","body":"### Issue with current documentation:\r\n\r\nThe new versions of the PyData Sphinx Theme have renamed\/refactores a few css classes, meaning that some of the colors in our documentation were overriden:\r\n\r\n![Screenshot_20240124_151656](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/5beb4613-edb6-4808-8c33-cb542ea0a41e)\r\n\r\n\r\n### Idea or request for content:\r\n\r\nDo we want to use some color from our usual palette? Note that the scipy.org website (which uses the [scientific python hugo theme](https:\/\/github.com\/scientific-python\/scientific-python-hugo-theme)) has incorporated some of the purple\/violet (?) highlights:\r\n\r\n![Screenshot_20240124_151948](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/45cbe4ed-3071-43de-bf45-88ca1a28ad4d)\r\n\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nPurely cosmetic question but I thought I'd ask \ud83d\ude04 ","comments":["The icons on the home page don't look great on dark mode:\r\n\r\n<img width=\"883\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/51488791\/0d4e5171-d941-4b5c-86a6-7ae8e19e3111\">\r\n","I would personally not change anything as the colors are supposedly better from an accessibility point of view. Unless we would have a team to design an accessible (like NumPy did I believe), I would forward questions or else to the theme itself","@tupui that is a good point, although currently we have a mix of colors from the theme and some pre-defined by us, so we would need to check for contrast and accessibility anyway. For reference, I did a quick simulation with colors inspired by NumPy. \r\n\r\nFirst, the current NumPy palette: https:\/\/github.com\/numpy\/numpy.org#user-experience-ux\r\n\r\n![Screenshot_20240125_211338](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/b225cc15-5cca-4045-86ca-180449fd223d)\r\n\r\nThis is how it looks like on a page: (including the pink that is not on the official palette)\r\n\r\n![Screenshot_20240125_210956](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/abf03f3f-9f26-4691-9618-fa3db667bd52)\r\n\r\nNote that for the homepage, however, we have a contrast problem for the footer:\r\n\r\n![Screenshot_20240125_211930](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/3d1898aa-ba1e-4a62-b04e-63341e3139bb)\r\n\r\nQuick check gives me the following:\r\n\r\n![Screenshot_20240125_213308](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/b6a977f0-3b11-417a-8f78-064660c1ab29)\r\n\r\n\r\nNote that the PyData Sphinx Theme color is chosen based on their own palette, not the NumPy \"blue\".\r\n\r\nSciPy's current palette, including the new violet from the PyData Sphinx Theme:\r\n![Screenshot_20240125_211731](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/a92c569b-3597-4626-81fa-7e6b7ecad932)\r\n\r\nFor good measure, I tested with the NumPy \"yellow\" just to see how that looks like:\r\n![Screenshot_20240125_212045](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/125ab280-8d5c-435a-a902-098612f9d113)\r\n\r\nI am of course not a designer \ud83d\ude04 but I'd be happy to check for contrast on all those at a minimum.\r\n","To clarify, I would either go with no color tweak on our side, i.e. removing all our customisation as we have been slowly doing since we introduced the theme. Or invest some resources to design our own identity. I would love to go with option 2 and we would even have some of our funds for that.","By the way, I am happy to see that the sidebar is actually fixed for the dev docs! Seems like we only have issues for PR previews which is totally ok to me then.\r\n\r\ncc @drammock (also I don't want to create an issue for that, but I am seeing in small screens that we have for the version button `margin-bottom: 1em` and this should be 0)","> Note that the scipy.org website (which uses the [scientific python hugo theme](https:\/\/github.com\/scientific-python\/scientific-python-hugo-theme)) has incorporated some of the purple\/violet (?) highlights\r\n\r\nIndeed, we've been working to unify several elements of the scientific python hugo theme and the pydata-sphinx-theme. See https:\/\/github.com\/scientific-python\/scientific-python-hugo-theme\/issues\/302\r\n\r\n> To clarify, I would either go with no color tweak on our side, i.e. removing all our customisation as we have been slowly doing since we introduced the theme. Or invest some resources to design our own identity. I would love to go with option 2 and we would even have some of our funds for that.\r\n\r\n+1.  The color system was pretty carefully crafted with usability and accessibility in mind; overriding just one or two colors risks doing more harm than good. That said, I'm happy to have some conversations if there are just a few specific colors \/ aspects of the theme that are particularly troublesome for SciPy; we might be able to find a solution that avoids a full identity redesign.","Sounds good! I can give it a go removing the scipy customizations and if everyone's happy with it, it's one less thing we need to maintain \ud83d\ude04 ","The version switcher could do with being vertically centred (on mobile Safari at least).\n\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/51488791\/93fac2cf-c5f7-49a4-a45a-6eb652669fc0)","Hey folks\n\n\nI am of course biased but I'd suggest removing non essential customisations to benefit from all the recent accessibility improvements in PST. \n\nIn addition, we could work towards some adjustments to the NumPy colours to keep some of the visual identity without hindering accessibility or usability. (For example, it will be unlikely we can keep the bright yellow for light\/dark theme conformance but could find an alternative). \nWithout needing a full identity overhaul like @drammock already mentioned. "],"labels":["Documentation"]},{"title":"MAINT: Translate wright_bessel function to C++","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/19404\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nThis PR translates the `wright_bessel` function into CUDA compatible C++ along with its dependencies. Following up on previous work like https:\/\/github.com\/scipy\/scipy\/pull\/19834.\r\n\r\n#### Additional information\r\nThis branches off of the digamma PR, https:\/\/github.com\/scipy\/scipy\/pull\/19834, which itself branches off of the `loggamma` PR https:\/\/github.com\/scipy\/scipy\/pull\/19496. These previous PRs should be merged first before this goes in. I'm just making this now so @izaid can have a chance to review while we wait for the other PRs to get a final look.\r\n<!--Any additional information you think is important.-->\r\n","comments":["The failures on 32 bit is because it compares `wright_bessel(a, b, 0)` against `rgamma(b)`, and `wright_bessel` no longer uses the C implementation from `scipy\/special\/cephes` for this case, but instead the C++ implementation from `scipy\/special\/special\/cephes`. `rgamma` is still using the C implementation, so there's therefore no reason to expect these to be equal any longer.\r\n\r\n`mpmath` agrees with the C++ implementation of `rgamma` so I think I'll just swap that one out too.","> (1) it looks like `_evalpoly.pxd` is removed, did this already have a corresponding `_evalpoly.h` file in a different migration PR?\r\n\r\nYes, this was removed in the PR translating the `lambertw` function, https:\/\/github.com\/scipy\/scipy\/pull\/19435\/files, but we had to wait until all Cython files using `_evalpoly.pxd` were removed before removing it.\r\n\r\n> (2) For my own edification where is `SPECFUN_HOST_DEVICE` getting set in the build?\r\n\r\nThis gets set here https:\/\/github.com\/scipy\/scipy\/blob\/62d2af2e13280d29781585aa39a3c5a5dfdfba17\/scipy\/special\/special\/config.h#L3-L4, conditionally on whether or not the code is running on CUDA. That's a good file to read to understand how we make it so the headers can be used on CPU and also GPU with CUDA.\r\n\r\n> The doc build is failing in gh-actions, do we need to add any docstrings for thjese changes?\r\n\r\nI don't think we've touched docs, I thought it was unrelated. I'll rebase on main and push again and see if it still happens."],"labels":["scipy.special","maintenance","C\/C++","Cython"]},{"title":"TST: We should add missing tests for cdflib","body":"Since merging https:\/\/github.com\/scipy\/scipy\/pull\/19560,  @ilayn's translation of cdflib into cython, several bugs have been brought to our attention by statsmodels maintainer @bashtage, https:\/\/github.com\/scipy\/scipy\/issues\/19896, https:\/\/github.com\/scipy\/scipy\/pull\/19933, https:\/\/github.com\/scipy\/scipy\/pull\/19944, who kindly made PRs fixing several of them.\r\n\r\nCurrently `scipy\/special\/tests\/test_cdflib.py` is missing tests for a number of functions\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/11ce9d798116092189a25d3799a6bcecfdedbc53\/scipy\/special\/tests\/test_cdflib.py#L4-L19\r\n\r\nI think there are two primary types of tests we can add. Tests against mpmath, (which will require writing arbitrary precision implementations of these functions using mpmath), and tests against the Fortran version (which would be faster, but it won't catch bugs in the Fortran version).\r\n\r\n@ilayn offered to add tests if I came up the parameter domains which in which the functions should be tested. I started making such a list, and realized that such special care around hot spots probability isn't necessary just yet. I think we should just use the classes `Arg`, `IntArg`, and `ProbArg` used in `test_cdflib.py` over the domains of definition of the functions. When mpmath is used, we should restrict the domains for cases where mpmath cannot compute a correct answer for sufficiently large values.\r\n\r\nWhile going through all of these, I found that the functions for the non-Central F distribution, `ncfdtr` and friends are documented to require the non-centrality parameter `nc` to in $(0, 1e^4)$. The comment in `_cdflib.pyx` refers to the bound too. I've found the bound is missing from the code though. @ilayn, do you remember if there was a reason to take the bound out? I can't remember if that came up in the review. We'll either need to put the bound back or change the docs.\r\n\r\nDomains below.\r\n\r\n### Non-central F distribution\r\n* `ncfdtr`, `ncfdtri`, `ncfdtridfn`, `ncfdtridfd`, and `ncfdtrinc`.\r\n    * `dfn`: `Arg(0, np.inf, inclusive_a=False)`\r\n    * `dfd`: `Arg(0, np.inf, inclusive_a=False)`\r\n    * `nc`: `Arg(0, 1e4)` when it is searched otherwise `np.inf`\r\n    * `f`: `Arg(0, np.inf)`\r\n    * `p`: `ProbArg()`.\r\n\r\n### Negative binomial distribution\r\n* `nbdtrik` and `nbdtrin`\r\n    * `k`: `IntArg(1, 1000)`\r\n    * `y`: `ProbArg()`\r\n    * `n`: `IntArg(1, 1000)`\r\n    * `p`: `ProbArg()`\r\n\r\n### Normal distribution\r\n* `nrdtrimn` and `nrdtrimsd`\r\n    * `p`: `ProbArg()`\r\n    * `x`: `Arg(-np.inf, np.inf)`\r\n    * `std`: `Arg(0, np.inf, inclusive_a=False)`\r\n    * `mn`: `arg(-np.inf, np.inf, inclusive_a=False, inclusive_b=False)`\r\n\r\n### Poisson distribution\r\n* `pdtrik`\r\n    * `p`: `ProbArg()`\r\n    * `m`: `Arg(0, np.inf, inclusive_b=False)`\r\n\r\n### Non-central T distribution\r\n* `ncdtr`, `nctdtrit`, `nctdtridf`, and `nctdrinc`.\r\n    * `df`: Arg(0, np.inf, inclusive_a=False)\r\n    * `nc`: `Arg(-1e6, 1e6)`\r\n    * `t`: `Arg(-np.inf, np.inf)`\r\n    * `p`: `ProbArg()`\r\n\r\n\r\n\r\n\r\n","comments":["If there is such a bound then it should be in the wrappers as fortran code does not check it \r\n\r\nThis is the old wrapper \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/1d3a067c2ccd0a6efddeb3194163aa9a3879d26e\/scipy\/special\/cdf_wrappers.c#L243-L252\r\n\r\nThis is the fortran code where it uses the 10^4 because it is an inverse search \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/1d3a067c2ccd0a6efddeb3194163aa9a3879d26e\/scipy\/special\/cdflib\/cdffnc.f#L261-L285\r\n\r\nSo it is only the `which=5` that uses that bound which makes sense because it is a bound of search failure. And that failure is in the code\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/11ce9d798116092189a25d3799a6bcecfdedbc53\/scipy\/special\/_cdflib.pyx#L2539\r\n\r\nHence that's not missing anywhere.","Got it. I think it's a docs problem. The docs mention the bound for all of the non-central f functions, making it seem like that should be the accepted domain.\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/1953382\/eaeabddd-9f28-45d1-bf19-a10be3a4b145)\r\n","Yeah, this bound should only appear on `ncfdtrinc` as an algorithmic shortcoming but it doesn't https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.ncfdtrinc.html#scipy.special.ncfdtrinc","I would like to help out with this issue and started looking into the normal distribution related functions `nrdtrimn` and `nrdtrisd`.\r\n\r\nI am probably misunderstanding the function signature as for me a very basic test already fails, returning `nan` for `nrdtrimn` both with the old Fortran and the new C version:\r\n\r\n```python\r\nfrom scipy.stats import norm\r\nfrom scipy.special import nrdtrimn\r\n\r\nx, mu, sigma = 0, 0, 0.1\r\nq = norm.cdf(x, loc=mu, scale=sigma)  # 0.5\r\n\r\nnrdtrimn(q, x, sigma) -> NaN\r\n``` \r\n\r\nI expected the original value for `mu` as the return value. Am I missing something?","Yes this is a very funky way of wrapping things for me too. So typically what I do is first get the function name to be tested, from the test file. Then I search for it in the `function.json` that gives me the wrapper name in the `cdflib_wrappers` to search for. Then I try to mimic the input to the output to test the function. It is indeed very cumbersome process overall probably for historical reasons. The naming scheme of these functions are very confusing for me. ","Thanks for the hint. This solved the mystery to me: the [docstring](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.nrdtrimn.html#scipy.special.nrdtrimn) is currently wrong, it lists the arguments in the wrong order. It must be `nrdtrimn(p, std, x)` instead of  `nrdtrimn(p, x, std)` . Will send a fix for that and a test soon.\r\n\r\nEdit: `ndtrisd` is also wrongly documented.","OK in #20069 we tried \r\n\r\n```python\r\ndef _normal_cdf(sigma, x, mu):\r\n    x = mpmath.mp.mpf(x)\r\n    mu = mpmath.mp.mpf(mu)\r\n    sigma = mpmath.mp.mpf(sigma)\r\n    q = mpmath.mp.ncdf(x, mu, sigma)\r\n    return q\r\n\r\n[snipped]\r\nclass\r\n    def test_nrdtrimn(self):\r\n        _assert_inverts(\r\n            sp.nrdtrimn,\r\n            _normal_cdf,\r\n            1, [ProbArg(), Arg(-30, 30), Arg(0, 10, inclusive_a=False)],\r\n            2, [ProbArg(), Arg(0, np.inf, inclusive_a=False),\r\n                Arg(-np.inf, np.inf)],\r\n            n=1000, rtol=1e-5)\r\n```\r\n\r\nSomehow couldn't make it work because the comparison was being done on the wrong argument although the result was correct. If we have an understanding why then we can roll out most of these in a single PR. ","I managed to make this work but the main issue is the closed form mpmath or some other credible source to compare for the other ones. For now, I'm just comparing the CDF with the inverse functions at least to stay consistent. \r\n\r\nAlso the testing facility is not too flexible to have logical ranges as noted in #20069 hence I'm limiting the domains quite a bit . But it can be augmented later on. ","For the `mpmath` baselines, I think the best thing to do is use mpmath's [findroot](https:\/\/mpmath.org\/doc\/current\/calculus\/optimization.html#root-finding-findroot)  to invert an mpmath implementation of the cdf function and retry with a higher `dps` if the tolerance isn't met. "],"labels":["scipy.special","task","maintenance"]},{"title":"BUG: `scipy.io.mmread` failure starting from `1.12.0` with precision `>= 22` during write","body":"### Describe your issue.\r\n\r\nStarting from `1.12.0`, `scipy.io.mm*` are implemented in `C++` (which is good).\r\n\r\nHowever, there seems to be a bug when using `scipy.io.mmread` for a matrix written in a file with `scipy.io.mmwrite` and a precision larger or equal to 22. The problem does not appear for a precision lower than 22.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nFROM python:3.12\r\n\r\nRUN pip install scipy==1.12.0\r\n\r\nCOPY <<EOF failure.py\r\n\r\nimport scipy\r\n\r\nprint(\"scipy version:\", scipy.__version__)\r\n\r\nmatrix_shape = (42,42)\r\nmatrix       = scipy.sparse.random(\r\n    m = matrix_shape[0], n = matrix_shape[1], density = 0.25, format = 'coo'\r\n)\r\npath = \"dontcare.mtx\"\r\nscipy.io.mmwrite(\r\n    target = path, a = matrix,\r\n    comment = \"2D sparse matrix generated for shownig the failure\",\r\n    # precision = 21, # Doesn't fail\r\n    precision = 22, # Fails with this precision given when reading !\r\n)\r\n\r\nscipy.io.mmread(path)\r\n\r\nprint(\"TEST PASSED\")\r\nEOF\r\n\r\nRUN python3 failure.py\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x00007f6bfd873000 (most recent call first):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_fast_matrix_market\/__init__.py\", line 149 in _read_body_coo\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/scipy\/io\/_fast_matrix_market\/__init__.py\", line 363 in mmread\r\n  File \"\/workspaces\/HELM\/toolbox\/utils\/load.py\", line 37 in loadMatrix\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nVersions are provided by the Docker file above.\r\n```\r\n","comments":["cc @alugowski ","Looking into it.","hey @alugowski, have you had a chance to look into this? Branching for 1.13.0 is planned for this Sunday"],"labels":["defect","scipy.io"]},{"title":"ENH: add rv_interior_continuous to scipy.stats","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI\u2019m working on fitting some data to the [non-central chi-square distribution with zero degrees of freedom](https:\/\/www.jstor.org\/stable\/2335674). This is almost a continuous distribution except that there\u2019s a probability mass at zero (the left boundary of the support). It is a special case of the Tweedie distribution (gh-11291). In R, this distribution is supported as a special case of the [non-central chi-square distribution](https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/Chisquare.html).\r\n\r\nCurrently scipy.stats doesn\u2019t support mixed continuous\/discrete distribution. I implemented what I need (PDF, CDF, MLE) by subclassing `rv_continuous` with a few hacks, but it would be nice if scipy natively supports this kind of distributions (continuous with discrete boundary).\r\n\r\n### Describe the solution you'd like.\r\n\r\nProposal: add `rv_interior_continuous` class. It should do something similar to `rv_continuous`, except that it allows probability mass at the left and\/or the right boundary of the support.\r\n\r\nSuch distributions are used in e.g. Poisson mixture models (Tweedie) and [zero-inflated models](https:\/\/en.m.wikipedia.org\/wiki\/Zero-inflated_model).\r\n\r\n### Describe alternatives you've considered.\r\n\r\nI have considered a more general class supporting discrete jumps of the CDF at not only the support boundaries but also its interior, but find the added complexity of implementation is not worth the added benefit at this stage, as I have not come across a use case for discontinuous CDF in the interior of the support.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["There is some related discussion here:\r\nhttps:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/ELRAAVQ367HTCO4BXEWLZXQB5VKM4YEU\/#ELRAAVQ367HTCO4BXEWLZXQB5VKM4YEU","A few design decisions has to be made with a mixed discrete\/continuous distribution:\r\n\r\npdf() at boundary: let the derived class decide what to return. The base class implementation should work regardless of what the derived class returns.\r\n\r\nppf(p): should return inf_x CDF(x)>=p, though ppf(0)=-inf might look unexpected if the support is bounded on the left.\r\n\r\nBoundary comparison: because floating point equality comparison is tricky, to ensure consistent result, checking whether x is at the boundary should always use the same formula, e.g. x == a*s+m where a is the boundary, s is scale and m is location. Exact equality should be used because for censored type of models the censoring threshold is expected to appear exactly in data.\r\n\r\nMost implementation should be straightforward. The only part that is a bit inconvenient is that `rv_generic.nnlf` standardizes the data before computing the log likelihood, and then add back the log scales. This is incorrect for mixed discrete\/continuous distribution and needs to be refactored. ","> There is some related discussion here:\r\n> https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/ELRAAVQ367HTCO4BXEWLZXQB5VKM4YEU\/#ELRAAVQ367HTCO4BXEWLZXQB5VKM4YEU\r\n\r\nThanks for the reference. That thread discusses about a concrete censored wrapper of discrete distributions. It is straightforward to implement by subclassing rv_discrete.\r\n\r\nMy proposal is different in that (1) it is censoring of continuous distribution, (2) the \u201cuncensored\u201d distribution is not fully defined (only the part that \u201csurvives\u201d censoring is defined), and (3) it cannot be implemented by subclassing the existing rv_* base classes without overriding many of the methods with minor tweaks.","I renamed the class to be clearer and avoid confusion.\r\n\r\nA potential (conceptual) class inheritance hierarchy could be\r\n\r\n- rv_generic\r\n  - rv_interior_continuous\r\n    - rv_continuous\r\n  - rv_discrete\r\n  \r\nwith the appropriate boilerplate code supplied at each level to make concrete distribution implementations simple."],"labels":["enhancement"]},{"title":"BUG: `optimize.approx_fprime` squeezes dimensions","body":"### Describe your issue.\n\nI was using `optimize.approx_fprime` to approximate Jacobians of various functions. In particular, I investigated functions defined on spaces with various dimensions `n`. My tests all worked for `n > 1`, but failed for `n == 1` since this case is returns an array with `ndim == 1` instead of `ndim == 2`, which would be expected generically.\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nfrom scipy import optimize\r\n\r\nfor n in [3, 2, 1]:\r\n    J = optimize.approx_fprime(np.ones(n), lambda x: x)\r\n    np.testing.assert_allclose(J, np.eye(n))\n```\n\n\n### Error message\n\n```shell\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nCell In[68], line 6\r\n      4 for n in [3, 2, 1]:\r\n      5     J = optimize.approx_fprime(np.ones(n), lambda x: x)\r\n----> 6     np.testing.assert_allclose(J, np.eye(n))\r\n\r\n    [... skipping hidden 1 frame]\r\n\r\nFile ~\/user\/miniconda3\/envs\/py3\/lib\/python3.9\/contextlib.py:79, in ContextDecorator.__call__.<locals>.inner(*args, **kwds)\r\n     76 @wraps(func)\r\n     77 def inner(*args, **kwds):\r\n     78     with self._recreate_cm():\r\n---> 79         return func(*args, **kwds)\r\n\r\nFile ~\/user\/miniconda3\/envs\/py3\/lib\/python3.9\/site-packages\/numpy\/testing\/_private\/utils.py:713, in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\r\n    707         reason = f'\\n(dtypes {x.dtype}, {y.dtype} mismatch)'\r\n    708     msg = build_err_msg([x, y],\r\n    709                         err_msg\r\n    710                         + reason,\r\n    711                         verbose=verbose, header=header,\r\n    712                         names=('x', 'y'), precision=precision)\r\n--> 713     raise AssertionError(msg)\r\n    715 flagged = bool_(False)\r\n    716 if isnumber(x) and isnumber(y):\r\n\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0\r\n\r\n(shapes (1,), (1, 1) mismatch)\r\n x: array([1.])\r\n y: array([[1.]])\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.2 sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/include\r\n    lib directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/include\r\n    lib directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: x86_64-apple-darwin13.4.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 16.0.6\r\n  c++:\r\n    commands: x86_64-apple-darwin13.4.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 16.0.6\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/Users\/runner\/miniforge3\/conda-bld\/scipy-split_1700812582701\/_build_env\/bin\/x86_64-apple-darwin13.4.0-gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/dzwicker\/user\/miniconda3\/envs\/py3\/bin\/python\r\n  version: '3.9'\n```\n","comments":["`approx_fprime` is used for calculating derivatives of functions that map from `R^n` to `R^m`, with the docstring saying that it should return an `(m, n)` matrix. However, it doesn't always return an `(m, n)` array.\r\n\r\n### Case 1 - differentiation of scalar function: \r\nIf `f` returns a scalar or a `(1,)` array then it's output is a 1D array of size `(n,)`.\r\n\r\n```python\r\ndef f1(x):\r\n    return np.sum(x)\r\n\r\nx1_1 = np.arange(3.0)\r\n\r\nprint(approx_fprime(x1_1)) # array([1., 1., 1.])\r\n```\r\n\r\nThis situation is that encountered in minimisation of scalar functions of one or more variables. I would argue that supplying an array of length 1 is a limiting example of this case.\r\n\r\ni.e. I would definitely expect this behaviour:\r\n\r\n```python\r\nx1_0 = np.array([1.0])\r\nassert approx_fprime(x1_0, f1).ndim == approx_fprime(x1_1, f1).ndim == 1\r\n```\r\n\r\nWith this case an `(n,)` is *always returned*.\r\n\r\n\r\n### Case 2 - differentiation of vector function\r\nIf `f` returns a vector function then its output is a 2D array of size `(m, n)`.\r\n\r\n```python\r\ndef f2(x):\r\n    return np.atleast_1d(x)\r\n\r\nprint(approx_fprime(x1_1, f2).shape) # (3, 3)\r\n```\r\n\r\nUnfortunately when an array of size `(1,)` to is supplied you only get a `(1,)` array back, when you expected a `(1, 1)`:\r\n```\r\nprint(approx_fprime(x1_0, f2).shape) # (1,)  expected a (1, 1)\r\n```\r\n\r\n### discussion\r\n\r\nUp until version `1.9.0` only case 1 functions, i.e. differentiation of scalar functions, were supported. Support for differentiating vector valued functions were added in `1.9.0`, and the docstring was updated. \r\n\r\nI don't think we can go down the road of amending the output for scalar functions because `approx_fprime` has *always* returned `(n,)` arrays for the gradient. Modifying it to return `(1, n)` arrays would break a lot of downstream code.\r\n\r\nUnfortunately this edge case was not dealt with when vector valued support was recently added. If a function is returning an array of length 1 it's not possible for `approx_fprime` to know if it's in the context of differentiating a scalar or vector function. I would argue `f2` is a vector valued function until it is supplied an array of length 1, when it should then be considered a scalar valued function.\r\n\r\nGiven this historical behaviour for scalar functions (returning `(n,)` instead of `(1, n)`), I don't actually think we can do anything about this behaviour.\r\n\r\nPerhaps the docstring could be modified? e.g.:\r\n\r\n> when vector valued functions (R^n --> R^m, where m > 1) are differentiated then the gradient has shape `(m, n)`. When scalar valued functions are supplied (R^n --> R^1), then the gradient always has shape `(n,)`. If a function normally returns a vector with m > 1, but can sometimes return a single value (m=1), then the dimensionality of the output array can change from 2 (m > 1) to 1 (m = 1).","I'm surprised that the cases where the function return arrays of shape `(1,)` and `()` are treated identically, which I find inconsistent. However, I agree that if this was the case historically, it should probably not be changed without a long time of deprecation. To allow a more general treatment of vector functions, would it be possible to introduce a keyword argument to force the output always to have shape `(m, n)`? This would be analogous to the `keepdims` argument that some numpy functions support.","> I'm surprised that the cases where the function return arrays of shape (1,) and () are treated identically, which I find inconsistent.\r\n\r\nIn general things like that were\/are an attempt to be tolerant of what user functions return for scalar valued functions:\r\n\r\n```\r\nlambda x: return 1\r\nlambda x: return np.array([1])\r\nlambda x: return np.array(1)\r\n```\r\nhave to be dealt with. If one isn't tolerant in this manner then one ends up with a whole heap of issues being created.\r\n\r\nA keyword argument might make sense. The most important things are to figure out what the keyword is called, and then the text of the docstring.\r\n\r\n"],"labels":["scipy.optimize"]},{"title":"BUG: multivariate t broken for `df=inf`","body":"### Describe your issue.\r\n\r\n`multivariate_t.pdf` and `multivariate_t.logpdf` do not work with `df=inf` currently. This case was not tested, so it went unnoticed when `multivariate_normal` was reworked to use the covariance class. Stumbled upon this looking into #19069.\r\n\r\nI will submit a fix including a simplifcation of the logpdf along https:\/\/github.com\/scipy\/scipy\/pull\/19792 soon.\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom scipy.stats import multivariate_t\r\nimport numpy as np\r\n\r\nmultivariate_t.logpdf(1, 1, 1, df=np.inf)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 multivariate_t.pdf(1, 1, 1, df=np.inf)\r\n\r\nFile ~\/scipy\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/stats\/_multivariate.py:4462, in multivariate_t_gen.pdf(self, x, loc, shape, df, allow_singular)\r\n   4460 x = self._process_quantiles(x, dim)\r\n   4461 shape_info = _PSD(shape, allow_singular=allow_singular)\r\n-> 4462 logpdf = self._logpdf(x, loc, shape_info.U, shape_info.log_pdet, df,\r\n   4463                       dim, shape_info.rank)\r\n   4464 return np.exp(logpdf)\r\n\r\nFile ~\/scipy\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/stats\/_multivariate.py:4530, in multivariate_t_gen._logpdf(self, x, loc, prec_U, log_pdet, df, dim, rank)\r\n   4502 \"\"\"Utility method `pdf`, `logpdf` for parameters.\r\n   4503\r\n   4504 Parameters\r\n   (...)\r\n   4527\r\n   4528 \"\"\"\r\n   4529 if df == np.inf:\r\n-> 4530     return multivariate_normal._logpdf(x, loc, prec_U, log_pdet, rank)\r\n   4532 dev = x - loc\r\n   4533 maha = np.square(np.dot(dev, prec_U)).sum(axis=-1)\r\n\r\nTypeError: multivariate_normal_gen._logpdf() takes 4 positional arguments but 6 were given\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\ncurrent main branch\r\n```\r\n","comments":[],"labels":["defect","scipy.stats"]},{"title":"ENH: stats.mannwhitneyu: replace exact p-value calculation","body":"#### Reference issue\r\ngh-4933\r\nCloses gh-17559\r\n\r\n#### What does this implement\/fix?\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/4933#issuecomment-1898082691 suggested a different algorithm for the `mannwhitneyu` exact p-value calculation that greatly improves performance (e.g. factor of 675 for $m \\approx n \\approx 100$) and reduces memory consumption from $O(m^2n^2)$ to $O(mn)$. This PR implements the suggestion.\r\n\r\n#### Additional information\r\n@AtsushiSakai let's wait for confirmation from @toobaz on use of the code before starting a detailed review. In the meantime, I thought you'd be interested in this:\r\n\r\n```python3\r\nimport numpy as np\r\nfrom scipy import stats\r\nfrom scipy.stats._mannwhitneyu import _mwu_state\r\nrng = np.random.default_rng(1305672164505329978)\r\nx = rng.random(100)\r\ny = rng.random(101)\r\n```\r\n\r\nBefore:\r\n```python3\r\nres = stats.mannwhitneyu(x, y, method='exact')  # 16.9 s \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\r\nres  # MannwhitneyuResult(statistic=4792.0, pvalue=0.5331021234219595)\r\n_mwu_state._fmnks.shape  #  (101, 102, 4793)\r\n```\r\n\r\nAfter:\r\n```python3\r\nres = stats.mannwhitneyu(x, y, method='exact')  # 25 ms \u00b1 0 ns per loop (mean \u00b1 std. dev. of 1 run, 1 loop each)\r\nres  # MannwhitneyuResult(statistic=4792.0, pvalue=0.5331021234219553)\r\n_mwu_state.configurations.shape  #  (4793,)\r\n```\r\n\r\n","comments":["Hey @AtsushiSakai thought I'd ping you again since you said you use `mannwhitneyu` and this gives a huge memory and speed upgrade. ","Pretty impressive speedup, yeah? BTW could you describe what you did to review?","Yeah. That is great.\r\nI just reviewed code readability and ran my static analyzer, if the mathematical collectiveness needs to be reviewed, maybe we need other specialists to review it.  ","I rarely consider review-by-test to be acceptable, but in this case I do. So if you're comfortable with it, feel free to merge.","Give me a week or so to test the new implementation using my data collections. Then I will merge it.","Apologies for not finding the time before to seriously look at this, but when I wrote this Python code I had also written a Cython version, and a few quick tests I just ran show a speed up between 4x and 5x asymptoticaly (and even larger for small values of U).\r\n\r\nIf there's interest, I should be able to put up a PR next week (in this case, do let me know if there is anything specific I must keep in mind to contribute cython code to scipy).","I would prefer to use Pythran. This is already a massive speedup, so i think we can merge this as is and move the loop into Pythran later. Thanks, though!","> I would prefer to use Pythran. This is already a massive speedup, so i think we can merge this as is and move the loop into Pythran later.\r\n\r\nOK. Just a double check: this code uses `scipy.special.comb`, written in cython, and hence to the best of my understanding not trivial to use from Pythran. (But the best of my understanding is really not much)","> OK. Just a double check: this code uses `scipy.special.comb`, written in cython, and hence to the best of my understanding not trivial to use from Pythran. (But the best of my understanding is really not much)\r\n\r\n`binom` is supported in Pythran so it should be fine (https:\/\/pythran.readthedocs.io\/en\/latest\/SUPPORT.html#special).","@mdhaber Sorry, I've tried compiling this PR branch on my MacBook environment several times, but the build for scipy is not passing. It seems to be the same issue related to clang as mentioned here, #18645\r\nbut I haven't been able to resolve it. Therefore, I would like to ask someone else to review it. I'm very sorry.","Hmm, our `cpp_std` is set to `17` in the top level meson file now so that specific issue shouldn't be here anymore... would you mind posting the error message over there or in a new issue?","@dschmitz89 It would be great to get this in 1.13.0 since it has huge speed and memory benefits for `mannwhitneyu`. Do you have time before branch to take a look? I don't even think it needs any stats-specific review; I'm treating the code as vendored and the tests that it is producing correct p-values are already pretty solid. ","> @dschmitz89 It would be great to get this in 1.13.0 since it has huge speed and memory benefits for `mannwhitneyu`. Do you have time before branch to take a look? I don't even think it needs any stats-specific review; I'm treating the code as vendored and the tests that it is producing correct p-values are already pretty solid.\r\n\r\nI am not familiar with this test but will see that I have a look in the next days.","bumped the milestone but feel free to move back if you get to this today @dschmitz89 "],"labels":["scipy.stats","enhancement"]},{"title":"BUG: Inconsistent Output from BenchGlobal Compared to BenchLeastSquares in Dev Benchmarks","body":"### Describe your issue.\r\n\r\nI've encountered an issue where the output from running the `BenchGlobal` benchmark is not formatted as a proper table, unlike the output from `BenchLeastSquares`. This inconsistency suggests there might be a bug in the [`BenchGlobal` class](https:\/\/github.com\/scipy\/scipy\/blob\/647d1cbd95d0407c7c0010e483c89ab77add76b4\/benchmarks\/benchmarks\/optimize.py#L486).\r\n\r\nRun the benchmark for `BenchLeastSquares` and `BenchGlobal` with the commands below.\r\n\r\nThe expected behavior should be a table:\r\n```\r\n[ 0.00%] \u00b7\u00b7 Benchmarking existing-py_Users_tessavanderheiden_.venvs_scipy-dev_bin_python\r\n[100.00%] \u00b7\u00b7\u00b7 optimize.BenchLeastSquares.track_all                  ok\r\n[100.00%] \u00b7\u00b7\u00b7 ====================== ============== ========================\r\n                     problem          result type                           \r\n              ---------------------- -------------- ------------------------\r\n                AlphaPineneDirect     average time    0.029906105995178223  \r\n                AlphaPineneDirect         nfev                 22           \r\n               .\r\n               .\r\n               .\r\n              ====================== ============== ========================\r\n```\r\n\r\n### Reproducing Code Example\r\n\r\n```shell\r\npython dev.py bench -t optimize.BenchLeastSquares\r\n```\r\n\r\n```shell\r\npython dev.py bench -t optimize.BenchGlobal\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n[ 0.00%] \u00b7\u00b7 Benchmarking existing-py_Users_tessavanderheiden_.pyenv_versions_3.9.14_bin_python\r\n[100.00%] \u00b7\u00b7\u00b7 Setting up optimize:580                               ok\r\n[100.00%] \u00b7\u00b7\u00b7 optimize.BenchGlobal.track_all                        ok\r\n[100.00%] \u00b7\u00b7\u00b7 ============================== ============= ========= =====\r\n                      test function           result type    solver       \r\n              ------------------------------ ------------- --------- -----\r\n                           AMGM                 success%       DE     n\/a \r\n                           AMGM                 success%    basinh.   n\/a \r\n                           AMGM                 success%       DA     n\/a \r\n                           AMGM                 success%     DIRECT   n\/a \r\n                           AMGM                 success%      SHGO    n\/a \r\n                           AMGM                  <nfev>        DE     n\/a \r\n                           AMGM                  <nfev>     basinh.   n\/a \r\n                           AMGM                  <nfev>        DA     n\/a \r\n                           AMGM                  <nfev>      DIRECT   n\/a \r\n                           AMGM                  <nfev>       SHGO    n\/a \r\n              .\r\n              .\r\n              .\r\n              ============================== ============= ========= =====\r\n\r\n[100.00%] \u00b7\u00b7\u00b7\u00b7 For parameters: 'AMGM', 'success%', 'DE'\r\n               asv: skipped: NotImplementedError('skipped')\r\n               \r\n               For parameters: 'AMGM', 'success%', 'basinh.'\r\n               asv: skipped: NotImplementedError('skipped')\r\n               .\r\n               .\r\n               .\r\n               For parameters: 'Zirilli', '<nfev>', 'DIRECT'\r\n               asv: skipped: NotImplementedError('skipped')\r\n               \r\n               For parameters: 'Zirilli', '<nfev>', 'SHGO'\r\n               asv: skipped: NotImplementedError('skipped')\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.13.0.dev0+1169.87f8428 1.26.3 sys.version_info(major=3, minor=9, micro=14, releaselevel='final', serial=0)\r\n\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/homebrew\/Cellar\/openblas\/0.3.25\/include\r\n    lib directory: \/opt\/homebrew\/Cellar\/openblas\/0.3.25\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=56\r\n    pc file directory: \/opt\/homebrew\/opt\/openblas\/lib\/pkgconfig\r\n    version: 0.3.25\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/homebrew\/Cellar\/openblas\/0.3.25\/include\r\n    lib directory: \/opt\/homebrew\/Cellar\/openblas\/0.3.25\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=56\r\n    pc file directory: \/opt\/homebrew\/opt\/openblas\/lib\/pkgconfig\r\n    version: 0.3.25\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.8\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 13.2.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/..\/private\/var\/folders\/s7\/rbrc7kq522bbv0pb_bwfrt7w0000gp\/T\/pip-build-env-bblq6l9m\/overlay\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.15.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/tessavanderheiden\/.venvs\/scipy-dev\/bin\/python\r\n  version: '3.9'\r\n```\r\n","comments":["When I comment out `def setup()` ([line](https:\/\/github.com\/scipy\/scipy\/blob\/fa9f13e6906e7d00510d593f7f982db30e4e4f14\/benchmarks\/benchmarks\/optimize.py#L527)) it seems to work:\r\n```\r\n    def setup(self, name, ret_value, solver):\r\n        if name not in self._enabled_functions:\r\n            raise NotImplementedError(\"skipped\")\r\n\r\n        # load json backing file\r\n        with open(self.dump_fn) as f:\r\n            self.results = json.load(f)\r\n```\r\nCan I make a PR?","> Can I make a PR?\r\n\r\nSure!"],"labels":["defect","scipy.optimize","Benchmarks"]},{"title":"BUG: Indexing with two empty boolean masks yields a `(1,0)` size sparse matrix","body":"### Describe your issue.\n\nI would expect something like `mat[idx, idx_other_axis]` where `idx.sum() == 0` and `idx_other_axis.sum() == 0` to yield a size `(0,0)` matrix but it does not.  Sorry if this is not a bug, but the behavior doesn't match `numpy`, which is a `(0,)` sized matrix.  As I said, I think the closest analogy here is `(0,0)` in `scipy` then.  I apologize if this has an explanation but I don't know what it is and couldn't find anything in the repo about it.\n\n### Reproducing Code Example\n\n```python\nfrom scipy import sparse\r\nimport numpy as np\r\n\r\nmat = sparse.random(1_000, 1_000, density=0.01, format=\"csc\")\r\nidx = np.zeros((1000,), dtype=bool)\r\nmat[idx, idx].shape # (1, 0)\r\nmat = np.random.randn(1_000, 1_000)\r\nmat_np[idx, idx].shape # (0,)\n```\n\n\n### Error message\n\n```shell\nN\/A\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.2 sys.version_info(major=3, minor=11, micro=6, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-xp5pbjk1\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-gft7gyfb\/cp311-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.11'\n```\n","comments":["This is independent of #19951 (see discussion there)\r\n\r\nThis error is controlled by [_index.py line 91](https:\/\/github.com\/scipy\/scipy\/blob\/e8b043df1d586d9df9fed96add1a536859f1cb8c\/scipy\/sparse\/_index.py#L91).\r\n`            return self.__class__(np.atleast_2d(row).shape, dtype=self.dtype)` when `row` is a size 0 array.\r\n\r\nWhen handling index validation, what do you do if the array index has size zero?\r\nThe code creates an empty sparse matrix of the same type being indexed. But does an empty matrix have shape (1,0) or (0,0) or (0,1)?  They are all empty arrays of size 0.\r\n\r\nScipy sparse matrix chooses (1,0) by using essentially `np.atleast_2d([])`. Thus shape is (1,0).\r\n\r\nNumpy has the flexibility to switch to 1d, stay at 2d or, in general make it nd. But they collapse any dimension that has length 0. I'm not sure why they decide to stick with 1d instead of going to 0d in this case. A 1d empty array has shape (0,).\r\n\r\nThis is the heart of this issue.  It presents this way for `csr, csc, dok, lil` formats for both sparse matrix and array -- basically anything that can be indexed has this difference between scipy-sparse and numpy.  \r\n\r\nI think the question is \"what should be returned from a blank mask?\". It looks like [the array_api for boolean indexing](https:\/\/data-apis.org\/array-api\/2022.12\/API_specification\/indexing.html#boolean-array-indexing) specifies boolean index must be a single array. Combining it with another index in another dimension is prohibited. But it also admits that this creates \"data dependent output\" so some libraries will not be able to handle boolean indexes this way. So, the array api does not allow the form of indexing used in this Issue.\r\n\r\nIt remains that numpy and scipy.sparse both support this form of indexing with scipy.sparse returning a size 0 matrix of shape (1,0), and numpy returning a size 0 array of shape (0,).\r\n\r\nPossible fixes:\r\n- leave it as is... a 2d-matrix when indexed should return 2d and (1,0) is a 2d shape that has size 0.\r\n- make it return a matrix with shape (0,0):   `self.__class__((0,0), dtype=self.dtype)`\r\n\r\nMore options exist for the new sparse arrays. There we can match numpy (produce a 1d array of size 0).\r\n`self.__class__((0,), dtype=self.dtype)` which works in my upcoming `indexing-1d` branch (not yet a PR). ","> I think the question is \"what should be returned from a blank mask?\". It looks like [the array_api for boolean indexing](https:\/\/data-apis.org\/array-api\/2022.12\/API_specification\/indexing.html#boolean-array-indexing) specifies boolean index must be a single array. Combining it with another index in another dimension is prohibited. But it also admits that this creates \"data dependent output\" so some libraries will not be able to handle boolean indexes this way. So, the array api does not allow the form of indexing used in this Issue.\r\n> ...\r\n>  Possible fixes:\r\n>\r\n>    - leave it as is... a 2d-matrix when indexed should return 2d and (1,0) is a 2d shape that has size 0.\r\n>    - make it return a matrix with shape (0,0): self.__class__((0,0), dtype=self.dtype)\r\n\r\n\r\nSo I would be in favor of the second option you mention because of the following (which is array-api legal):\r\n```python\r\nmask = np.zeros(2, dtype=bool)\r\nassert np.array([[1, 2], [3, 4]])[mask, :][:, mask].shape == (0, 0)\r\n```\r\n\r\nCould I make the PR for this if you approve?","Can you find where this option is described as array-api legal?\r\n\r\nThe Note in [the array_api for boolean indexing](https:\/\/data-apis.org\/array-api\/2022.12\/API_specification\/indexing.html#boolean-array-indexing) says:\r\n\r\n> An array must support indexing where the sole index is an M-dimensional boolean array B with shape S1 = (s1, ..., sM) according to the following rules. Let A be an N-dimensional array with shape S2 = (s1, ..., sM, ..., sN).\r\n\r\n> **Note:** The prohibition against combining boolean array indices with other single-axis indexing expressions includes the use of None. To expand dimensions of the returned array, use repeated invocation of [expand_dims()](https:\/\/data-apis.org\/array-api\/2022.12\/API_specification\/generated\/array_api.expand_dims.html#array_api.expand_dims).\r\n\r\nSo it looks like masks are not allowed to be combined with other indices as in `A[mask, :]`.  Am I reading this incorrectly? Or do i have the wrong array-api spec?","@dschult You're correct, but I was more using that as an example of why an all zeros shape is the correct way to go.\r\n```python\r\nmask = np.zeros((2,2), dtype=bool)\r\nnp.array([[1, 2], [3, 4]])[mask].shape # (0,)\r\n```\r\nwhich is different again since\r\n```python\r\nsparse.random(2, 2, density=0.01, format=\"csc\")[mask].shape # (1,0)\r\n```\r\nI understand this change is not going to happen here, though, since we don't have 1d arrays yet but I still think `(1,0)` is strange.  To that point:\r\n```python\r\nmask = np.zeros(2, dtype=bool)\r\nnp.array([[1, 2], [3, 4]])[mask].shape # (0,2)\r\n```\r\nThis to me, this makes sense even if I don't like the 2 lying around (I would collapse to an empty matrix like you mention).","I would note that this behaviour isn't specific to boolean indexing and also occurs when indexing with empty lists. E.g.\r\n\r\n```python\r\nsparse.csr_array(np.arange(50).reshape(5, 10))[[], []].shape\r\n# (1, 0)\r\n```\r\n\r\nAlso, this is the behaviour for `np.matrix`:\r\n\r\n```python\r\nnp.matrix(np.arange(25).reshape(5, 5))[[], []]\r\n# matrix([], shape=(1, 0), dtype=int64)\r\n```\r\n\r\nSo this may be intentional, at least for `sparse.spmatrix` subclasses. Though you can have an `np.matrix` with `shape = (0, 0)`\r\n\r\n```python\r\nnp.matrix(np.ones((0, 0)))\r\n# matrix([], shape=(0, 0), dtype=float64)\r\n```","Strange especially because your second example with boolean masks gives a different shape.  I am not sure what to do here and would be fine with nothing since this will hopefully be resolved via 1d arrays anyway and `numpy` is less consistent than I thought."],"labels":["defect","scipy.sparse"]},{"title":"ENH: Adding COBYQA to `scipy.optimize`?","body":"### Is your feature request related to a problem? Please describe.\n\nAs discussed in https:\/\/github.com\/scipy\/scipy\/issues\/18118, COBYLA is the only derivative-free optimization solver available in SciPy. COBYQA is a solver we developed with my colleague [Zaikun Zhang](https:\/\/www.zhangzk.net) from The Hong Kong Polytechnic University. It is designed to supersede COBYLA as a general solver.\n\n### Describe the solution you'd like.\n\nI want to propose the inclusion of the COBYQA solver in `scipy.optimize`. A Python implementation of COBYQA is available at https:\/\/github.com\/cobyqa\/cobyqa, and the related Python package is available on PyPI. Numerical experiments we conducted show the clear superiority of COBYQA over COBYLA in general.\r\n\r\nI would be happy to share our thoughts on this inclusion.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Thanks @ragonneau, can you post this on the mailing list as this is required for new features?\r\n\r\nThat being said I think I am +1","Pinging also @zaikunzhang ","Thanks @ragonneau for the proposal. Is there sign of general adoption yet?\n\nWe have strict academic criteria for inclusion in SciPy. Usually we don't follow trends and add new things but wait until there is a large consensus around adoption of new methods.\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/dev\/core-dev\/index.html#core-dev-guide\n\nBased on that alone I would be -1.","I'd like to understand the lineage and the status of this method. If it's completely new, I'd second Pamphile's opinion: it should mature as a separate package. However, is it a completely new method?\r\nThere is COBYLA by Powell; then there's PRIMA by Zhang & Powell, and now here's COBYQA by Ragonneau & Zhang.\r\n\r\nSo PRIMA is a streamlined and modernized implementation of COBYLA, and we sort of planned to replace the latter by the former. Now PRIMA is still in FORTRAN so https:\/\/github.com\/scipy\/scipy\/issues\/18566 applies.\r\n\r\nSo is COBYQA separate if related to COBYLA\/PRIMA, or can it be thought of an improved version of the same method? \r\nIf it is, we can I guess think of formulating a plan to skip PRIMA and go straight to COBYQA maybe.","Also cross-ref gh-1477 and gh-18118.","> I'd like to understand the lineage and the status of this method. If it's completely new, I'd second Pamphile's opinion: it should mature as a separate package. However, is it a completely new method? There is COBYLA by Powell; then there's PRIMA by Zhang & Powell, and now here's COBYQA by Ragonneau & Zhang.\r\n> \r\n> So PRIMA is a streamlined and modernized implementation of COBYLA, and we sort of planned to replace the latter by the former. Now PRIMA is still in FORTRAN so #18566 applies.\r\n\r\nNo, [PRIMA](http:\/\/www.libprima.net) is not in FORTRAN. \r\n\r\nIt is in Fortran, which I regard as a different language that conveniently benefits from the abundant libraries in FORTRAN without bearing the historical burden. \r\n\r\n> So is COBYQA separate if related to COBYLA\/PRIMA, or can it be thought of an improved version of the same method? If it is, we can I guess think of formulating a plan to skip PRIMA and go straight to COBYQA maybe.\r\n\r\n[COBYQA](www.cobyqa.com) is not COBYLA. COBYQA is a completely new and different algorithm, using more advanced techniques than COBYLA. This particularly includes SQP and derivative-free symmetric Broyden update.  Tom @ragonneau and I (Zaikun Zhang) developed COBYQA as part of [Tom's Ph.D. thesis](https:\/\/theses.lib.polyu.edu.hk\/handle\/200\/12294).\r\n","Thank you @zaikunzhang \r\n\r\n> It is in Fortran, which I regard as a different language that conveniently benefits from the abundant libraries in FORTRAN without bearing the historical burden.\r\n\r\nOK, let's keep the F77 vs _modern Fortran_ discussion in gh-18118 or elsewhere.\r\n\r\n> [COBYQA](https:\/\/github.com\/scipy\/scipy\/issues\/www.cobyqa.com) is not COBYLA. COBYQA is a completely new and different algorithm, using more advanced techniques than COBYLA. This particularly includes SQP and derivative-free symmetric Broyden update. Tom @ragonneau and I (Zaikun Zhang) developed COBYQA as part of [Tom's Ph.D. thesis](https:\/\/theses.lib.polyu.edu.hk\/handle\/200\/12294).\r\n\r\nThanks. So is COBYQA in some sense a superset of COBYLA? As in, can it be expected to be strictly better than COBYLA, would domain experts (you) recommend it over cobyla\/prima? ","`shgo` was also a very new solver when it was added to scipy so I would not directly reject `COBYQA`. How does COBYQA perform for example on the CUTEST benchmark compared to what scipy currently has to offer? Not sure about the quality of our benchmark suite there though.\r\n\r\nRegarding COBYLA I would strongly argue against removing it for COBYQA. In my experience, no optimizer is strictly better than another (no free lunch theorem etc. ..) and COBYLA is a famous and widely used algorithm.","> Thanks. So is COBYQA in some sense a superset of COBYLA? As in, can it be expected to be strictly better than COBYLA, would domain experts (you) recommend it over cobyla\/prima?\r\n\r\nNo, COBYQA is not a superset of COBYLA. They are different algorithms, just like GMRES is not a superset of CG or vice versa. \r\n\r\nFrom a mathematical perspective, the techniques used in COBYQA are more advanced (e.g., SQP and Powell's Broyden update). However, they are **mature and classical techniques** in numerical optimization, just less explored in derivative-free optimization (DFO), and even less available in readily usable DFO solvers.  \r\n\r\nWe have developed COBYQA as a successor of COBYLA. COBYQA is a package under development, but COBYLA has been stablized for almost 30 years, the last major change being the modernization I made. Note that \"under development\" does not mean that it is not ready for production usage, but means that new features and new improvements will continue to be made --- recall that the Linux kernel is also under development. Therefore, COBYQA has more potential. \r\n\r\nNo algorithm can always outperform another one on all problems. We, as algorithm developers, try to develop algorithms that perform better than others on most test problems we have. But this does not mean that our algorithm will outperform others on a particular problem. However, a particular user normally only cares about a single particular problem. \r\n\r\nConsequently, if a user asks my advice for solving a **nonlinearly constrained** optimization problem, I would suggest trying both COBYLA and COBYQA. Both have their advantages. \r\n\r\nFor unconstrained, bound-constrained, and linearly constrained problems, the solvers in PRIMA are state of the art, and the performance of COBYQA is comparable when the number of variables is at most 50. We have not tested COBYQA on larger problems systematically, which is due to the speed limitation of Python, although the PRIMA solvers are tested on bigger problems every day. (Note that it is improper to compare PRIMA with COBYQA, the former being a package consisting of five solvers while the latter being a particular solver. )\r\n\r\nCOBYQA has its advantages. For example, it respects the bound constraints (if any), meaning that it never evaluates the objective\/constraint functions at points outside the bounds. This is [the next improvement I will make to LINCOA and COBYLA](https:\/\/github.com\/libprima\/prima\/issues\/42).\r\n\r\nCOBYQA still has much potential to be fulfilled and explored. I believe it can still be improved to obtain high-precision solutions. \r\n\r\nDerivative-free optimization methods (aka, zeroth-order methods) are attracting more and more attention due to new applications arising from machine learning and AI. My suggestion is to make both PRIMA solvers (including COBYLA) and COBYLA available in SciPy --- this will be of great value to both SciPy, the community, and eventually the industry.  \r\n\r\nThank you. \r\n","> Regarding COBYLA I would strongly argue against removing it for COBYQA. In my experience, no optimizer is strictly better than another (no free lunch theorem etc. ..) and COBYLA is a famous and widely used algorithm.\r\n\r\nI hold the same opinion. ","> shgo was also a very new solver when it was added to scipy so I would not directly reject COBYQA. How does COBYQA perform for example on the CUTEST benchmark compared to what scipy currently has to offer? Not sure about the quality of our benchmark suite there though.\n\nWe have our own benchmark suite that one can run (see in the benchmark folder). If, as shgo, the method proves itself to be competitive, then by all means my -1 would become +1 \ud83d\ude03\n\nRunning the benchmark would in any case be the next step for me.","> Running the benchmark would in any case be the next step for me.\r\n\r\nAgreed, it would be great to see the result of those. @ragonneau you can find those in `benchmarks\/benchmarks\/optimize.py`. It would be very useful to see the results of those benchmarks that include COBYLA now (and other solvers), and add COBYQA to that.\r\n\r\n> We have our own benchmark suite that one can run (see in the benchmark folder). \r\n\r\nThose include benchmarks from CUTEST, which may be what @dschmitz89 meant.\r\n\r\n> A Python implementation of COBYQA is available at https:\/\/github.com\/cobyqa\/cobyqa,\r\n\r\nI had a quick browse of this package; the code generally looks in good shape. It's not so easy to immediately see how much of it would be included in SciPy, since there's models, problem sets, and some duplicated code from SciPy. But it looks like it'd be a few thousand lines of code. Those will presumably land in a single file in the SciPy implementation (e.g., `scipy\/optimize\/_cobyla.py`). You'll probably have to put that together anyway to be able to run the benchmarks. It'll be nice to look at that once it's ready.","> Those will presumably land in a single file in the SciPy implementation (e.g., `scipy\/optimize\/_cobyla.py`).\r\n\r\nThank you @rgommers, I suppose you meant `scipy\/optimize\/_cobyqa.py`. ","Yes indeed - that was a typo","> Agreed, it would be great to see the result of those. @ragonneau you can find those in `benchmarks\/benchmarks\/optimize.py`. It would be very useful to see the results of those benchmarks that include COBYLA now (and other solvers), and add COBYQA to that.\r\n\r\nI ran SciPy benchmarks locally after including COBYQA.\r\n\r\nThe results for `BenchDFO`:\r\n\r\n<details>\r\n\r\n```\r\n[ 2.27%] \u00b7\u00b7\u00b7 ============================== ============== =========== ========================\r\n             --                                                        result type             \r\n             --------------------------------------------- ------------------------------------\r\n              DFO benchmark problem number      solver      mean_nfev          min_obj         \r\n             ============================== ============== =========== ========================\r\n                           0                    COBYLA        182.0       36.00000001629636    \r\n                           0                    COBYQA         38.0              36.0          \r\n                           0                    SLSQP          21.0       35.99999999999999    \r\n                           0                    Powell        213.0       35.99999999999998    \r\n                           0                 nelder-mead      1390.0      36.000000004916636   \r\n                           0                   L-BFGS-B        30.0       36.00000000029089    \r\n                           0                     BFGS          40.0       36.00000000000747    \r\n                           0                 trust-constr      30.0       36.000000000065775   \r\n                           1                    COBYLA        239.0       36.00000002161123    \r\n                           1                    COBYQA         43.0       36.00000000000001    \r\n                           1                    SLSQP          21.0              36.0          \r\n                           1                    Powell        356.0       35.99999999999997    \r\n                           1                 nelder-mead      1096.0      36.00000001672908    \r\n                           1                   L-BFGS-B        50.0       36.00000000000111    \r\n                           1                     BFGS          70.0       36.00000000020062    \r\n                           1                 trust-constr      40.0       36.00000000004839    \r\n                           2                    COBYLA         93.0        8.38460155098156    \r\n                           2                    COBYQA        102.0       8.380281696617237    \r\n                           2                    SLSQP          23.0       8.380281690140846    \r\n                           2                    Powell        203.0       8.380281690140844    \r\n                           2                 nelder-mead      276.0       8.380314322959702    \r\n                           2                   L-BFGS-B        32.0       8.380281690152536    \r\n                           2                     BFGS         252.0       8.380281698131483    \r\n                           2                 trust-constr     128.0       8.380281690145555    \r\n                           3                    COBYLA        107.0       8.400780446868142    \r\n                           3                    COBYQA        121.0       8.380281704183599    \r\n                           3                    SLSQP          23.0       8.380284602260867    \r\n                           3                    Powell        182.0       8.380281690140844    \r\n                           3                 nelder-mead      320.0       8.380314069260285    \r\n                           3                   L-BFGS-B        40.0       8.380281690152602    \r\n                           3                     BFGS         372.0       8.380281690224777    \r\n                           3                 trust-constr     168.0       8.380281690143562    \r\n                           4                    COBYLA        103.0        9.89172925972701    \r\n                           4                    COBYQA        116.0       9.880597101883852    \r\n                           4                    SLSQP          23.0       9.880597014925373    \r\n                           4                    Powell        209.0       9.880597014925371    \r\n                           4                 nelder-mead      267.0       9.880630904843933    \r\n                           4                   L-BFGS-B       112.0       9.880597014925657    \r\n                           4                     BFGS         341.0       9.880597015302904    \r\n                           4                 trust-constr     136.0       9.880597014929531    \r\n                           5                    COBYLA        124.0       9.880673204658704    \r\n                           5                    COBYQA        109.0       9.880597794292857    \r\n                           5                    SLSQP          23.0       9.880597015430164    \r\n                           5                    Powell        139.0       9.880597014925371    \r\n                           5                 nelder-mead      310.0       9.880645653536046    \r\n                           5                   L-BFGS-B        40.0       9.880597014932862    \r\n                           5                     BFGS         380.0       9.880597014940303    \r\n                           5                 trust-constr     200.0       9.880597014935532    \r\n                           6                    COBYLA        1000.0      0.2761343670249867   \r\n                           6                    COBYQA        136.0     3.4577885926458176e-11 \r\n                           6                    SLSQP         109.0     3.534301293924672e-05  \r\n                           6                    Powell        607.0     1.7404243721438573e-26 \r\n                           6                 nelder-mead      159.0     8.177661197416674e-10  \r\n                           6                   L-BFGS-B       126.0     4.006329767353906e-07  \r\n                           6                     BFGS         117.0     3.403318352733089e-11  \r\n                           6                 trust-constr     177.0     8.189074233317919e-09  \r\n                           7                    COBYLA        1000.0      18.15444498891401    \r\n                           7                    COBYQA        342.0     1.6575428203999043e-08 \r\n                           7                    SLSQP         295.0     2.4772818584112955e-05 \r\n                           7                    Powell        950.0     1.1093356479670479e-31 \r\n                           7                 nelder-mead      325.0     4.3387935016943286e-10 \r\n                           7                   L-BFGS-B        33.0       18.35859455687261    \r\n                           7                     BFGS         390.0     2.1519670672414852e-11 \r\n                           7                 trust-constr     372.0     2.8716495252807545e-11 \r\n                           8                    COBYLA        1000.0    0.0063088149736903664  \r\n                           8                    COBYQA         87.0      4.72095533231052e-08  \r\n                           8                    SLSQP         112.0     2.467522680839185e-06  \r\n                           8                    Powell         60.0              0.0           \r\n                           8                 nelder-mead      142.0     0.0003575878654431348  \r\n                           8                   L-BFGS-B       120.0     1.2458244845337688e-08 \r\n                           8                     BFGS         136.0     3.645847385308141e-12  \r\n                           8                 trust-constr     128.0     4.664490098037049e-12  \r\n                           9                    COBYLA        1000.0      1.302791737849277    \r\n                           9                    COBYQA        171.0     2.6707378731301188e-08 \r\n                           9                    SLSQP         114.0     3.1523978560448592e-06 \r\n                           9                    Powell         54.0              0.0           \r\n                           9                 nelder-mead      110.0     0.0001185672468711772  \r\n                           9                   L-BFGS-B       140.0     1.0981915721898683e-05 \r\n                           9                     BFGS         160.0     4.3547185576347985e-12 \r\n                           9                 trust-constr     172.0     5.033947368965333e-11  \r\n                           10                   COBYLA        1000.0    0.0019940963089381952  \r\n                           10                   COBYQA        312.0     1.2655194279446566e-10 \r\n                           10                   SLSQP          99.0     9.231453061284653e-05  \r\n                           10                   Powell        908.0     1.692602785645291e-13  \r\n                           10                nelder-mead      305.0     1.3905860499424258e-06 \r\n                           10                  L-BFGS-B       100.0     3.5705209681058525e-05 \r\n                           10                    BFGS         165.0     3.266335996489636e-07  \r\n                           10                trust-constr     190.0     3.424058956351133e-07  \r\n                           11                   COBYLA        1000.0     0.007588035153884722  \r\n                           11                   COBYQA        232.0     1.9523309860714916e-08 \r\n                           11                   SLSQP         179.0      3.44391715908274e-05  \r\n                           11                   Powell        1577.0    3.013054708612935e-22  \r\n                           11                nelder-mead      380.0     1.2207288155937388e-06 \r\n                           11                  L-BFGS-B       165.0     2.5692617247172744e-05 \r\n                           11                    BFGS         270.0     1.7148717367764208e-07 \r\n                           11                trust-constr     355.0     2.2298540818792096e-07 \r\n                           12                   COBYLA        1000.0      63.35600768650094    \r\n                           12                   COBYQA         73.0       48.98425367984218    \r\n                           12                   SLSQP          28.0        48.9842541402506    \r\n                           12                   Powell        118.0       48.98425367924135    \r\n                           12                nelder-mead      120.0       48.98425367981376    \r\n                           12                  L-BFGS-B        54.0       48.98425585393734    \r\n                           12                    BFGS          27.0       48.98425367924926    \r\n                           12                trust-constr      60.0       48.98425367924055    \r\n                           13                   COBYLA        1000.0       52.5180366249726    \r\n                           13                   COBYQA         90.0       48.98425367987076    \r\n                           13                   SLSQP          47.0       48.984285104912686   \r\n                           13                   Powell        232.0       48.984253679248496   \r\n                           13                nelder-mead      119.0     1.7922698821282258e-09 \r\n                           13                  L-BFGS-B        90.0       48.984313597884025   \r\n                           13                    BFGS          75.0       48.98425367924157    \r\n                           13                trust-constr      87.0       48.98425367924004    \r\n                           14                   COBYLA        1000.0     0.010846968025621654  \r\n                           14                   COBYQA        104.0      0.008214877370595158  \r\n                           14                   SLSQP          36.0      0.010926876593972496  \r\n                           14                   Powell        435.0      0.008214880387696202  \r\n                           14                nelder-mead      226.0      0.008214877316410097  \r\n                           14                  L-BFGS-B        36.0      0.009974372219725124  \r\n                           14                    BFGS          92.0      0.008214877322487959  \r\n                           14                trust-constr      96.0      0.008214931318406598  \r\n                           15                   COBYLA        1000.0       8.3527643805625     \r\n                           15                   COBYQA        119.0      0.008214877325203946  \r\n                           15                   SLSQP          80.0      0.010852444332488186  \r\n                           15                   Powell        212.0       1.7749269756254313   \r\n                           15                nelder-mead      346.0      0.008214877318251797  \r\n                           15                  L-BFGS-B        92.0      0.010443321667281222  \r\n                           15                    BFGS         136.0      0.008214877306852426  \r\n                           15                trust-constr     1924.0     0.008214941243166055  \r\n                           16                   COBYLA        1000.0    0.00038308147268715565 \r\n                           16                   COBYQA        213.0     0.0003075056071085903  \r\n                           16                   SLSQP          22.0     0.0005011638818990982  \r\n                           16                   Powell        517.0     0.00030750560950841397 \r\n                           16                nelder-mead      260.0     0.00030750561104950664 \r\n                           16                  L-BFGS-B        45.0     0.0004998454317253484  \r\n                           16                    BFGS         165.0     0.00030750717208645955 \r\n                           16                trust-constr     170.0      0.000309855462194756  \r\n                           17                   COBYLA         49.0       7191620.061242092    \r\n                           17                   COBYQA        240.0       125887.6768230767    \r\n                           17                   SLSQP         113.0       1417869686.359142    \r\n                           17                   Powell        267.0       106966.8745748054    \r\n                           17                nelder-mead      600.0       6409.119264481873    \r\n                           17                  L-BFGS-B        16.0       6977375.029202724    \r\n                           17                    BFGS         1768.0      120.82601972635028   \r\n                           17                trust-constr     3376.0      22723.636975175406   \r\n                           18                   COBYLA        1000.0     0.010975991274795866  \r\n                           18                   COBYQA        631.0     0.0022876786037281716  \r\n                           18                   SLSQP         145.0     0.0031019739183846718  \r\n                           18                   Powell        1742.0    0.0022876970870087574  \r\n                           18                nelder-mead      1200.0     0.004794624522076027  \r\n                           18                  L-BFGS-B       133.0      0.009260117102741666  \r\n                           18                    BFGS         266.0      0.002287670084190521  \r\n                           18                trust-constr     371.0     0.0022877933510461667  \r\n                           19                   COBYLA        1000.0     0.01449258959907267   \r\n                           19                   COBYQA        1000.0     0.003041678435983581  \r\n                           19                   SLSQP         334.0      0.002387635461912692  \r\n                           19                   Powell        1840.0     0.017136978287569753  \r\n                           19                nelder-mead      1200.0     0.11412012392582763   \r\n                           19                  L-BFGS-B       224.0      0.01039282675002245   \r\n                           19                    BFGS         518.0      0.002287670112758827  \r\n                           19                trust-constr     539.0     0.0022876807830514287  \r\n                           20                   COBYLA        1000.0     0.012314747076097223  \r\n                           20                   COBYQA        1000.0    4.926796490034533e-05  \r\n                           20                   SLSQP         435.0     3.7403380991915257e-05 \r\n                           20                   Powell        3507.0    0.00013999662219714502 \r\n                           20                nelder-mead      1275.0    0.0037644989112963437  \r\n                           20                  L-BFGS-B       270.0     0.0028624144428293146  \r\n                           20                    BFGS         410.0     3.7976544143093916e-05 \r\n                           20                trust-constr     540.0     3.9115677318496754e-05 \r\n                           21                   COBYLA        1000.0     0.19021806725341522   \r\n                           21                   COBYQA        1000.0     0.12473258117608635   \r\n                           21                   SLSQP         975.0      0.005234314850263207  \r\n                           21                   Powell        5037.0     0.10605836851573695   \r\n                           21                nelder-mead      1800.0      0.5377026960946113   \r\n                           21                  L-BFGS-B       500.0     0.0002881772390133559  \r\n                           21                    BFGS         1000.0    2.1010434078081007e-06 \r\n                           21                trust-constr     1260.0    3.997821365625117e-05  \r\n                           22                   COBYLA        1000.0     0.10003177690766717   \r\n                           22                   COBYQA        1000.0    0.00013082934380459935 \r\n                           22                   SLSQP         613.0      0.007622482850835413  \r\n                           22                   Powell        9020.0    0.0009381552314367413  \r\n                           22                nelder-mead      2400.0     0.008256161684722775  \r\n                           22                  L-BFGS-B       715.0     0.00033578796198918936 \r\n                           22                    BFGS         936.0     1.8927701687374778e-06 \r\n                           22                trust-constr     1105.0    1.3321166924342236e-05 \r\n                           23                   COBYLA        1000.0      0.2362971492708929   \r\n                           23                   COBYQA        1000.0     0.10741551690232379   \r\n                           23                   SLSQP         1331.0     0.10106796888625674   \r\n                           23                   Powell        4778.0      6.567880194260053    \r\n                           23                nelder-mead      2400.0      5.580001849914321    \r\n                           23                  L-BFGS-B       793.0      0.011328602404693662  \r\n                           23                    BFGS         2041.0     0.000278772211425114  \r\n                           23                trust-constr     1612.0    1.3460798054702056e-05 \r\n                           24                   COBYLA        1000.0      0.0027178520030068   \r\n                           24                   COBYQA        177.0      6.38970543076227e-14  \r\n                           24                   SLSQP          85.0      0.039023423059962446  \r\n                           24                   Powell        1077.0     0.07583186991605789   \r\n                           24                nelder-mead      480.0      0.07558874075499751   \r\n                           24                  L-BFGS-B       100.0      0.000234110576993369  \r\n                           24                    BFGS         108.0     4.1709385349620864e-09 \r\n                           24                trust-constr     132.0     2.214996479723849e-06  \r\n                           25                   COBYLA         32.0       256.1647690452771    \r\n                           25                   COBYQA         73.0       124.36218407850306   \r\n                           25                   SLSQP          16.0             2020.0         \r\n                           25                   Powell        303.0       124.3621843304955    \r\n                           25                nelder-mead       72.0       124.3621848396066    \r\n                           25                  L-BFGS-B        72.0       214.29894954503868   \r\n                           25                    BFGS         147.0       124.36218235561685   \r\n                           25                trust-constr     180.0       124.36218235562204   \r\n                           26                   COBYLA        678.0       85822.20465825836    \r\n                           26                   COBYQA        161.0       85822.20162724197    \r\n                           26                   SLSQP         130.0       85822.20163050466    \r\n                           26                   Powell        270.0       85822.24229503315    \r\n                           26                nelder-mead      333.0       85822.20162974804    \r\n                           26                  L-BFGS-B        70.0       85822.28170932793    \r\n                           26                    BFGS         195.0       85822.20162635628    \r\n                           26                trust-constr     220.0        85822.2016263596    \r\n                           27                   COBYLA        928.0       85822.20748496873    \r\n                           27                   COBYQA        421.0       85822.20162810793    \r\n                           27                   SLSQP         252.0       85822.20162647964    \r\n                           27                   Powell        260.0       85822.29989083631    \r\n                           27                nelder-mead      397.0       85822.20164251165    \r\n                           27                  L-BFGS-B       155.0        85822.2134844625    \r\n                           27                    BFGS         435.0       85822.20162635644    \r\n                           27                trust-constr     390.0       85822.20162635874    \r\n                           28                   COBYLA        274.0     4.567933585235919e-06  \r\n                           28                   COBYQA        202.0     1.793038448544232e-09  \r\n                           28                   SLSQP          73.0     0.0001275291143854763  \r\n                           28                   Powell        2017.0     2.62902981284408e-22  \r\n                           28                nelder-mead      630.0     4.168900076322192e-09  \r\n                           28                  L-BFGS-B       112.0     9.673157105942231e-07  \r\n                           28                    BFGS         140.0     1.3774502648136339e-10 \r\n                           28                trust-constr     7000.0     0.019837220256553954  \r\n                           29                   COBYLA        169.0     7.228441475939319e-07  \r\n                           29                   COBYQA        173.0     1.2167995824458087e-08 \r\n                           29                   SLSQP          58.0     0.00017792262972978623 \r\n                           29                   Powell        2050.0    6.554533948298135e-23  \r\n                           29                nelder-mead      472.0     4.392761093121637e-07  \r\n                           29                  L-BFGS-B       112.0     2.0258935358234505e-05 \r\n                           29                    BFGS         176.0     6.121604659655201e-11  \r\n                           29                trust-constr     152.0     2.653738494450361e-10  \r\n                           30                   COBYLA        1000.0    0.0036544588617460697  \r\n                           30                   COBYQA        328.0     0.0035168812562413423  \r\n                           30                   SLSQP          72.0      0.006287589283329275  \r\n                           30                   Powell        2782.0    0.0035168774842111537  \r\n                           30                nelder-mead      813.0     0.0037430185611118747  \r\n                           30                  L-BFGS-B       180.0     0.0036331210839248406  \r\n                           30                    BFGS         261.0     0.0035168737405634758  \r\n                           30                trust-constr     333.0     0.0035168745891778416  \r\n                           31                   COBYLA        507.0     9.563260675050992e-06  \r\n                           31                   COBYQA        359.0     9.022403180824982e-08  \r\n                           31                   SLSQP         113.0     7.559376195674753e-05  \r\n                           31                   Powell        4760.0    1.4405961280325035e-21 \r\n                           31                nelder-mead      1008.0    1.993137661280346e-05  \r\n                           31                  L-BFGS-B       190.0     0.0001645285146463783  \r\n                           31                    BFGS         290.0     4.2026392909231023e-11 \r\n                           31                trust-constr     5000.0    4.6482458943424287e-10 \r\n                           32                   COBYLA        1000.0     0.004996093419091057  \r\n                           32                   COBYQA        428.0      0.004772756479272126  \r\n                           32                   SLSQP          76.0      0.00748298302440139   \r\n                           32                   Powell        2196.0     0.00654899679530417   \r\n                           32                nelder-mead      1142.0     0.004782384226390245  \r\n                           32                  L-BFGS-B       143.0      0.007512546418004671  \r\n                           32                    BFGS         363.0      0.006503957324546606  \r\n                           32                trust-constr    11000.0     0.03376312998006939   \r\n                           33                   COBYLA        1000.0     0.004790522744041185  \r\n                           33                   COBYQA        492.0      0.002800054880834453  \r\n                           33                   SLSQP          80.0      0.006136919804702198  \r\n                           33                   Powell        4493.0    0.0027998978593152275  \r\n                           33                nelder-mead      1799.0    0.0031707681652494613  \r\n                           33                  L-BFGS-B       312.0      0.002825355695283585  \r\n                           33                    BFGS         504.0     0.0027997619178927627  \r\n                           33                trust-constr    12000.0     0.026740538342589985  \r\n                           34                   COBYLA        1000.0     0.002477075965308623  \r\n                           34                   COBYQA        765.0     7.087734211593793e-08  \r\n                           34                   SLSQP          47.0     1.8202225661529484e-05 \r\n                           34                   Powell        2981.0    1.3549012557932144e-22 \r\n                           34                nelder-mead      2000.0      0.3406697389548668   \r\n                           34                  L-BFGS-B        55.0     2.991968759920641e-05  \r\n                           34                    BFGS         132.0     1.873957153532047e-13  \r\n                           34                trust-constr     154.0     1.7082620401841644e-08 \r\n                           35                   COBYLA        1000.0      2.775269864105493    \r\n                           35                   COBYQA        386.0       0.0660477478502633   \r\n                           35                   SLSQP          74.0       1.1056629223458196   \r\n                           35                   Powell        658.0      0.05064107286626231   \r\n                           35                nelder-mead      747.0      5.46489474798272e-05  \r\n                           35                  L-BFGS-B       108.0       1.1059653777113296   \r\n                           35                    BFGS         125.0       0.6345412274548067   \r\n                           35                trust-constr     198.0       1.105952334795245    \r\n                           36                   COBYLA        1000.0      0.3000350186116832   \r\n                           36                   COBYQA        1000.0     0.041631120736703206  \r\n                           36                   SLSQP         426.0      0.04140440668157865   \r\n                           36                   Powell        3143.0     0.040143302727005024  \r\n                           36                nelder-mead      2200.0     0.07609312836138543   \r\n                           36                  L-BFGS-B       684.0      0.042042079864747596  \r\n                           36                    BFGS         756.0      0.040137738738389564  \r\n                           36                trust-constr     1092.0      0.040137794964173    \r\n                           37                   COBYLA        298.0       1.7898166135669258   \r\n                           37                   COBYQA        232.0       1.7898135873783405   \r\n                           37                   SLSQP         228.0       1.789825242678684    \r\n                           37                   Powell        265.0       1.7898135868820382   \r\n                           37                nelder-mead      809.0       1.7898135880745636   \r\n                           37                  L-BFGS-B       396.0       1.7898136300054583   \r\n                           37                    BFGS         252.0       1.789813586881127    \r\n                           37                trust-constr     960.0       1.7898135869084242   \r\n                           38                   COBYLA        289.0       10.238983941422939   \r\n                           38                   COBYQA        424.0       10.238973723645511   \r\n                           38                   SLSQP         216.0       10.238994699907447   \r\n                           38                   Powell        494.0       10.238973850652153   \r\n                           38                nelder-mead      1058.0      10.238973442379468   \r\n                           38                  L-BFGS-B       135.0       10.239986312974672   \r\n                           38                    BFGS         396.0       10.23897342140286    \r\n                           38                trust-constr     405.0       10.238973421449707   \r\n                           39                   COBYLA        704.0       18.281166339083896   \r\n                           39                   COBYQA        379.0       18.281162006790357   \r\n                           39                   SLSQP         276.0       18.281172707363083   \r\n                           39                   Powell        603.0       18.281163008134268   \r\n                           39                nelder-mead      2000.0      18.281162945170095   \r\n                           39                  L-BFGS-B       165.0       18.28315259151096    \r\n                           39                    BFGS         517.0       18.28116175363231    \r\n                           39                trust-constr     638.0       18.281161753760827   \r\n                           40                   COBYLA        864.0       22.26060556377675    \r\n                           40                   COBYQA        560.0       22.260591789088142   \r\n                           40                   SLSQP         398.0        22.2606981682437    \r\n                           40                   Powell        682.0       22.260594329663107   \r\n                           40                nelder-mead      2200.0      22.260916512172354   \r\n                           40                  L-BFGS-B       180.0       22.26368109072446    \r\n                           40                    BFGS         564.0       22.260591734938853   \r\n                           40                trust-constr     792.0       22.260591734991067   \r\n                           41                   COBYLA        807.0        26.2727789431198    \r\n                           41                   COBYQA        622.0       26.272766516221093   \r\n                           41                   SLSQP         367.0       26.272774470672505   \r\n                           41                   Powell        715.0       26.272777250417114   \r\n                           41                nelder-mead      2400.0      26.27279966391945    \r\n                           41                  L-BFGS-B       195.0       26.27565349718534    \r\n                           41                    BFGS         611.0       26.27276639682053    \r\n                           41                trust-constr     949.0       26.272766396968294   \r\n                           42                   COBYLA        1000.0     0.07310066404205529   \r\n                           42                   COBYQA        1000.0    0.00014177578856341126 \r\n                           42                   SLSQP         337.0      0.002869140620444522  \r\n                           42                   Powell        472.0     0.0006099279648976476  \r\n                           42                nelder-mead      218.0      0.002941046450586494  \r\n                           42                  L-BFGS-B        66.0      0.027570873338690554  \r\n                           42                    BFGS         1680.0    1.2038440956761933e-07 \r\n                           42                trust-constr     2676.0    1.5612798805036442e-05 \r\n                           43                   COBYLA        1000.0     0.07978793620432727   \r\n                           43                   COBYQA        1000.0    0.00020598371863355666 \r\n                           43                   SLSQP         395.0      0.002744641508948131  \r\n                           43                   Powell        565.0     0.00026251356957190366 \r\n                           43                nelder-mead      326.0      0.005665898451296298  \r\n                           43                  L-BFGS-B       371.0     0.0021694202909624477  \r\n                           43                    BFGS         3715.0    8.540661203993601e-07  \r\n                           43                trust-constr     4347.0    2.5085999407170378e-05 \r\n                           44                   COBYLA        1000.0     0.08233794111808837   \r\n                           44                   COBYQA        1000.0    0.0006650365315197859  \r\n                           44                   SLSQP         535.0     0.0032067490009361457  \r\n                           44                   Powell        816.0     0.00026012887967490284 \r\n                           44                nelder-mead      723.0     8.272905977631895e-05  \r\n                           44                  L-BFGS-B       126.0      0.027542394225624045  \r\n                           44                    BFGS         6159.0     2.2593468907395e-06   \r\n                           44                trust-constr     4968.0    2.6883195826390666e-05 \r\n                           45                   COBYLA        149.0      0.02029939321743818   \r\n                           45                   COBYQA         42.0     0.00015000751307292886 \r\n                           45                   SLSQP          84.0     6.360763241040102e-07  \r\n                           45                   Powell        227.0     8.949262200449805e-22  \r\n                           45                nelder-mead      375.0     4.867800022545828e-05  \r\n                           45                  L-BFGS-B        42.0     2.400899932123255e-10  \r\n                           45                    BFGS         174.0      5.3673974914275e-10   \r\n                           45                trust-constr      78.0     2.454090575098533e-06  \r\n                           46                   COBYLA        1000.0      1567939963474.8965   \r\n                           46                   COBYQA        106.0     0.0007293992928959043  \r\n                           46                   SLSQP          84.0     9.996680902431176e-10  \r\n                           46                   Powell        278.0     1.1773407061309293e-20 \r\n                           46                nelder-mead      465.0     8.181505338496236e-05  \r\n                           46                  L-BFGS-B        66.0     2.7791623937188346e-10 \r\n                           46                    BFGS         210.0     5.368902177156296e-10  \r\n                           46                trust-constr     114.0     2.9953810817563102e-06 \r\n                           47                   COBYLA        203.0      0.01914426426703756   \r\n                           47                   COBYQA         55.0     5.4087818719220104e-05 \r\n                           47                   SLSQP         156.0     8.591197491407533e-10  \r\n                           47                   Powell        362.0     3.554425222840407e-20  \r\n                           47                nelder-mead      853.0     0.00010539806608652054 \r\n                           47                  L-BFGS-B        63.0     1.7100578981890751e-09 \r\n                           47                    BFGS         396.0     8.554467047410975e-10  \r\n                           47                trust-constr      81.0     4.0417068746088015e-06 \r\n                           48                   COBYLA        279.0       0.0331710012331183   \r\n                           48                   COBYQA         87.0      0.005073729292213983  \r\n                           48                   SLSQP         139.0      0.003427873430343299  \r\n                           48                   Powell        450.0     8.053464532704803e-20  \r\n                           48                nelder-mead      1117.0    0.00021983838347792528 \r\n                           48                  L-BFGS-B        88.0     4.803599008795718e-10  \r\n                           48                    BFGS         660.0     1.0683513286135924e-09 \r\n                           48                trust-constr      99.0     4.5226625371459055e-06 \r\n                           49                   COBYLA        369.0      0.03428531322541776   \r\n                           49                   COBYQA         95.0     0.0036452448361088343  \r\n                           49                   SLSQP         304.0     2.269282789883771e-06  \r\n                           49                   Powell        540.0     3.020724955433762e-19  \r\n                           49                nelder-mead      1411.0    0.00030925186830669545 \r\n                           49                  L-BFGS-B       104.0     6.510760200351575e-10  \r\n                           49                    BFGS         741.0     1.2849241618216414e-09 \r\n                           49                trust-constr     208.0     4.879304344714801e-06  \r\n                           50                   COBYLA        1000.0      4476072203419.262    \r\n                           50                   COBYQA        205.0      0.010356853034351174  \r\n                           50                   SLSQP         174.0       16090.611615482758   \r\n                           50                   Powell        562.0     2.8587659884100126e-19 \r\n                           50                nelder-mead      2400.0       9.64238414027846    \r\n                           50                  L-BFGS-B       156.0     1.885624896893773e-09  \r\n                           50                    BFGS         845.0     1.2851588507030082e-09 \r\n                           50                trust-constr     273.0     4.8786393752066874e-06 \r\n                           51                   COBYLA        1000.0     0.045532842697210224  \r\n                           51                   COBYQA        1000.0    0.0025772034067144288  \r\n                           51                   SLSQP         321.0      0.003924596822050221  \r\n                           51                   Powell        4594.0    1.0674322967281136e-24 \r\n                           51                nelder-mead      1600.0     0.001974655275796391  \r\n                           51                  L-BFGS-B       162.0      0.018002244158092647  \r\n                           51                    BFGS         540.0     2.5296977794668327e-10 \r\n                           51                trust-constr     837.0     5.2718565480438994e-08 \r\n                           52                   COBYLA        1000.0      894783.8530217626    \r\n                           52                   COBYQA        848.0       4.5059640773037355   \r\n                           52                   SLSQP         996.0       267.22972398525724   \r\n                           52                   Powell        5894.0      12.202607426064228   \r\n                           52                nelder-mead      1600.0      223.19648568816407   \r\n                           52                  L-BFGS-B       306.0       4.536639136942162    \r\n                           52                    BFGS         669.0       4.5348798664538466   \r\n                           52                trust-constr     432.0       4.544389712796571    \r\n             ============================== ============== =========== ========================\r\n```\r\n\r\n<\/details>\r\n\r\nThe results for `BenchSmoothUnbounded`:\r\n\r\n<details>\r\n\r\n```\r\n[ 6.82%] \u00b7\u00b7\u00b7 ====================== ============== ==================== ========================\r\n             --                                                     result type                 \r\n             ------------------------------------- ---------------------------------------------\r\n                 test function          solver          mean_nfev              mean_time        \r\n             ====================== ============== ==================== ========================\r\n                rosenbrock_slow         COBYLA            1000.0          0.10221092700958252   \r\n                rosenbrock_slow         COBYQA            152.8           0.41478211879730226   \r\n                rosenbrock_slow         Powell            887.2           0.09182374477386475   \r\n                rosenbrock_slow      nelder-mead          310.2           0.03188121318817139   \r\n                rosenbrock_slow        L-BFGS-B           148.0           0.017220544815063476  \r\n                rosenbrock_slow          BFGS             177.2           0.02521369457244873   \r\n                rosenbrock_slow           CG               n\/a                    n\/a           \r\n                rosenbrock_slow          TNC               n\/a                    n\/a           \r\n                rosenbrock_slow         SLSQP             155.1           0.022461724281311036  \r\n                rosenbrock_slow       Newton-CG            n\/a                    n\/a           \r\n                rosenbrock_slow         dogleg             n\/a                    n\/a           \r\n                rosenbrock_slow       trust-ncg            n\/a                    n\/a           \r\n                rosenbrock_slow      trust-exact           n\/a                    n\/a           \r\n                rosenbrock_slow      trust-krylov          n\/a                    n\/a           \r\n                rosenbrock_slow      trust-constr         218.0           0.08552446365356445   \r\n               rosenbrock_nograd        COBYLA            1000.0          0.019648361206054687  \r\n               rosenbrock_nograd        COBYQA            152.8           0.39280545711517334   \r\n               rosenbrock_nograd        Powell            887.2           0.016094064712524413  \r\n               rosenbrock_nograd     nelder-mead          310.2           0.007036185264587403  \r\n               rosenbrock_nograd       L-BFGS-B           148.0           0.005884408950805664  \r\n               rosenbrock_nograd         BFGS             177.2           0.009394955635070801  \r\n               rosenbrock_nograd          CG               n\/a                    n\/a           \r\n               rosenbrock_nograd         TNC               n\/a                    n\/a           \r\n               rosenbrock_nograd        SLSQP             155.1           0.007205891609191895  \r\n               rosenbrock_nograd      Newton-CG            n\/a                    n\/a           \r\n               rosenbrock_nograd        dogleg             n\/a                    n\/a           \r\n               rosenbrock_nograd      trust-ncg            n\/a                    n\/a           \r\n               rosenbrock_nograd     trust-exact           n\/a                    n\/a           \r\n               rosenbrock_nograd     trust-krylov          n\/a                    n\/a           \r\n               rosenbrock_nograd     trust-constr         218.0           0.07210166454315185   \r\n                   rosenbrock           COBYLA            1000.0          0.019016504287719727  \r\n                   rosenbrock           COBYQA            152.8            0.3881392240524292   \r\n                   rosenbrock           Powell            887.2           0.015932464599609376  \r\n                   rosenbrock        nelder-mead          310.2           0.006581926345825195  \r\n                   rosenbrock          L-BFGS-B           92.75          0.0037383198738098144  \r\n                   rosenbrock            BFGS             110.95          0.006753122806549073  \r\n                   rosenbrock             CG              110.0           0.008317852020263672  \r\n                   rosenbrock            TNC               84.3          0.0030642032623291017  \r\n                   rosenbrock           SLSQP             102.3           0.005086040496826172  \r\n                   rosenbrock         Newton-CG            64.5           0.007260346412658691  \r\n                   rosenbrock           dogleg             18.4             0.00196533203125    \r\n                   rosenbrock         trust-ncg            43.5          0.0044361591339111325  \r\n                   rosenbrock        trust-exact           17.5          0.0029834747314453126  \r\n                   rosenbrock        trust-krylov          35.7           0.004609107971191406  \r\n                   rosenbrock        trust-constr   98.16666666666667     0.04931776523590088   \r\n                rosenbrock_tight        COBYLA            1000.0          0.01909213066101074   \r\n                rosenbrock_tight        COBYQA            180.2            0.4743908166885376   \r\n                rosenbrock_tight        Powell            1070.1          0.019453907012939455  \r\n                rosenbrock_tight     nelder-mead          407.7           0.008971905708312989  \r\n                rosenbrock_tight       L-BFGS-B            99.2          0.0039985418319702145  \r\n                rosenbrock_tight         BFGS             160.9           0.009589815139770507  \r\n                rosenbrock_tight          CG              127.8           0.009342217445373535  \r\n                rosenbrock_tight         TNC               90.4           0.003626441955566406  \r\n                rosenbrock_tight        SLSQP             108.15          0.005452108383178711  \r\n                rosenbrock_tight      Newton-CG            71.6           0.00818331241607666   \r\n                rosenbrock_tight        dogleg             19.2           0.002088189125061035  \r\n                rosenbrock_tight      trust-ncg            44.9           0.004395413398742676  \r\n                rosenbrock_tight     trust-exact           18.4          0.0029824018478393556  \r\n                rosenbrock_tight     trust-krylov          93.7           0.008433985710144042  \r\n                rosenbrock_tight     trust-constr   106.53333333333333    0.052931682268778486  \r\n                simple_quadratic        COBYLA             54.8          0.0005712509155273438  \r\n                simple_quadratic        COBYQA             22.2           0.03898210525512695   \r\n                simple_quadratic        Powell             47.2          0.0005118846893310547  \r\n                simple_quadratic     nelder-mead          152.8          0.0022516965866088865  \r\n                simple_quadratic       L-BFGS-B            7.5           0.0003270387649536133  \r\n                simple_quadratic         BFGS              7.5           0.0003994584083557129  \r\n                simple_quadratic          CG               3.6           0.00027928352355957033 \r\n                simple_quadratic         TNC               7.0           0.00021626949310302735 \r\n                simple_quadratic        SLSQP              6.0           0.0003117680549621582  \r\n                simple_quadratic      Newton-CG            2.0           0.0002607107162475586  \r\n                simple_quadratic        dogleg             3.0           0.00029482841491699217 \r\n                simple_quadratic      trust-ncg            3.0           0.00023899078369140624 \r\n                simple_quadratic     trust-exact           3.0           0.00047221183776855467 \r\n                simple_quadratic     trust-krylov          3.0           0.0003118276596069336  \r\n                simple_quadratic     trust-constr          6.0           0.0033010323842366535  \r\n              asymmetric_quadratic      COBYLA             64.2          0.0006988286972045898  \r\n              asymmetric_quadratic      COBYQA             23.5           0.043817758560180664  \r\n              asymmetric_quadratic      Powell            134.6          0.0012090682983398437  \r\n              asymmetric_quadratic   nelder-mead          162.9          0.0022761344909667967  \r\n              asymmetric_quadratic     L-BFGS-B            7.5           0.00037877559661865235 \r\n              asymmetric_quadratic       BFGS              7.5           0.0003869175910949707  \r\n              asymmetric_quadratic        CG               3.1           0.00026428699493408203 \r\n              asymmetric_quadratic       TNC               12.5          0.0002912759780883789  \r\n              asymmetric_quadratic      SLSQP              6.0           0.00032651424407958984 \r\n              asymmetric_quadratic    Newton-CG            2.0           0.0002610206604003906  \r\n              asymmetric_quadratic      dogleg             2.9           0.00026187896728515627 \r\n              asymmetric_quadratic    trust-ncg            2.9           0.0002459287643432617  \r\n              asymmetric_quadratic   trust-exact           2.9           0.0004597187042236328  \r\n              asymmetric_quadratic   trust-krylov          2.9           0.0003061056137084961  \r\n              asymmetric_quadratic   trust-constr   5.966666666666667    0.0032143274943033854  \r\n                     sin_1d             COBYLA             24.7          0.0003074169158935547  \r\n                     sin_1d             COBYQA             13.7           0.02740190029144287   \r\n                     sin_1d             Powell             28.1          0.00037937164306640626 \r\n                     sin_1d          nelder-mead           36.8           0.00053558349609375   \r\n                     sin_1d            L-BFGS-B            9.45          0.00045931339263916016 \r\n                     sin_1d              BFGS              10.2          0.0007171988487243653  \r\n                     sin_1d               CG               5.5           0.00034372806549072267 \r\n                     sin_1d              TNC               10.2          0.0002444028854370117  \r\n                     sin_1d             SLSQP              6.95          0.00047876834869384763 \r\n                     sin_1d           Newton-CG            n\/a                    n\/a           \r\n                     sin_1d             dogleg             n\/a                    n\/a           \r\n                     sin_1d           trust-ncg            n\/a                    n\/a           \r\n                     sin_1d          trust-exact           n\/a                    n\/a           \r\n                     sin_1d          trust-krylov          n\/a                    n\/a           \r\n                     sin_1d          trust-constr          9.6            0.006979584693908691  \r\n                     booth              COBYLA             67.1          0.0008264064788818359  \r\n                     booth              COBYQA             30.9           0.06615271568298339   \r\n                     booth              Powell             63.4          0.0007641792297363281  \r\n                     booth           nelder-mead          122.9          0.0021659135818481445  \r\n                     booth             L-BFGS-B            11.6          0.0005254864692687989  \r\n                     booth               BFGS              15.4           0.001001286506652832  \r\n                     booth                CG               6.4           0.00041408538818359374 \r\n                     booth               TNC               6.8            0.000226593017578125  \r\n                     booth              SLSQP              12.3          0.0006320714950561523  \r\n                     booth            Newton-CG            n\/a                    n\/a           \r\n                     booth              dogleg             n\/a                    n\/a           \r\n                     booth            trust-ncg            n\/a                    n\/a           \r\n                     booth           trust-exact           n\/a                    n\/a           \r\n                     booth           trust-krylov          n\/a                    n\/a           \r\n                     booth           trust-constr          14.8           0.008171749114990235  \r\n                     beale              COBYLA            461.1           0.004681205749511719  \r\n                     beale              COBYQA            206.9           0.45821413993835447   \r\n                     beale              Powell            1821.9          0.019892191886901854  \r\n                     beale           nelder-mead          141.1           0.002144122123718262  \r\n                     beale             L-BFGS-B            60.7          0.0021002888679504395  \r\n                     beale               BFGS             311.6           0.018624258041381837  \r\n                     beale                CG               32.9          0.0018250226974487304  \r\n                     beale               TNC               24.1          0.0005386114120483398  \r\n                     beale              SLSQP              25.9          0.0011149048805236816  \r\n                     beale            Newton-CG            n\/a                    n\/a           \r\n                     beale              dogleg             n\/a                    n\/a           \r\n                     beale            trust-ncg            n\/a                    n\/a           \r\n                     beale           trust-exact           n\/a                    n\/a           \r\n                     beale           trust-krylov          n\/a                    n\/a           \r\n                     beale           trust-constr         476.35           0.2655073642730713   \r\n                       LJ               COBYLA            868.8           0.02984781265258789   \r\n                       LJ               COBYQA            696.3            4.859420919418335    \r\n                       LJ               Powell            1900.0          0.05867128372192383   \r\n                       LJ            nelder-mead          2281.8          0.08925626277923585   \r\n                       LJ              L-BFGS-B           278.55          0.011390721797943116  \r\n                       LJ                BFGS             722.75          0.03499552011489868   \r\n                       LJ                 CG              146.9           0.01970562934875488   \r\n                       LJ                TNC              115.6           0.011374115943908691  \r\n                       LJ               SLSQP             208.5           0.00989387035369873   \r\n                       LJ             Newton-CG            n\/a                    n\/a           \r\n                       LJ               dogleg             n\/a                    n\/a           \r\n                       LJ             trust-ncg            n\/a                    n\/a           \r\n                       LJ            trust-exact           n\/a                    n\/a           \r\n                       LJ            trust-krylov          n\/a                    n\/a           \r\n                       LJ            trust-constr        1083.05          0.30552700757980344   \r\n             ====================== ============== ==================== ========================\r\n```\r\n\r\n<\/details>","For the numbered problems, the objective function value from COBYQA is always better than that of COBYLA, and it beats Nelder-Mead on 44\/53 problems. The histogram of function evaluation counts of COBYQA looks better than that of both of the others. (Note that COBYLA and COBYA have a function evaluation limit of 1000; Nelder-Mead does not. It would be nice to re-run these with a higher function evaluation limit, although the results are already pretty convincing.) \r\n\r\n<img width=\"607\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/6570539\/d04f16ca-a3e0-4eb7-9c27-5f7d558597d5\">\r\n\r\nIt looks like something went awry with the named problems (`BenchSmoothUnbounded`)? COBQA's objective value is always _worse_. Or are the other algorithms really always better on smooth problems?\r\n\r\nIt might be nice to put it up against some of the gradient free global solvers, though. COBYLA and Nelder-Mead aren't very competitive, so beating them might not be enough to warrant inclusion.","> It looks like something went awry with the named problems (`BenchSmoothUnbounded`)? COBQA's objective value is always _worse_. Or are the other algorithms really always better on smooth problems?\r\n\r\nHi @mdhaber , if you are referring to the second table posted by Tom @ragonneau , you may note that the last column is `mean_time` rather than `min_obj`.  ","Ah. It's tough to compare the performance without knowing the objective function values. I can create an algorithm that runs pretty quickly but doesn't solve the problem : ) ~~But I think the first set is sufficient.~~ But I think that comparing against derivative-free global optimizers is more important than re-running this set.","> Ah. It's tough to compare the performance without knowing the objective function values. I can create an algorithm that runs pretty quickly but doesn't solve the problem : ) But I think the first set is sufficient.\r\n\r\nAgree. I do not know the SciPy benchmark, I did not run it, and I do not understand why the data in the two tables are different. Maybe Tom @ragonneau should check.  \r\n\r\n","> It looks like something went awry with the named problems (`BenchSmoothUnbounded`)? COBQA's objective value is always _worse_. Or are the other algorithms really always better on smooth problems?\r\n\r\nI reran the benchmark by adding `max_obj`, `min_obj`, and `mean_obj`. The results:\r\n\r\n<details>\r\n\r\n```\r\n[100.00%] \u00b7\u00b7\u00b7 ====================== ============== ============= ========================\r\n                  test function          solver      result type                          \r\n              ---------------------- -------------- ------------- ------------------------\r\n                 rosenbrock_slow         COBYLA       mean_nfev            1000.0         \r\n                 rosenbrock_slow         COBYLA       mean_time     0.10025284290313721   \r\n                 rosenbrock_slow         COBYLA        max_obj       6.480135564619959    \r\n                 rosenbrock_slow         COBYLA        min_obj      0.05985270052507147   \r\n                 rosenbrock_slow         COBYLA        mean_obj      1.040669274800954    \r\n                 rosenbrock_slow         COBYQA       mean_nfev            152.8          \r\n                 rosenbrock_slow         COBYQA       mean_time     0.45025100708007815   \r\n                 rosenbrock_slow         COBYQA        max_obj     2.627973375197527e-07  \r\n                 rosenbrock_slow         COBYQA        min_obj     9.551703764181064e-11  \r\n                 rosenbrock_slow         COBYQA        mean_obj    3.890437153553753e-08  \r\n                 rosenbrock_slow         Powell       mean_nfev            887.2          \r\n                 rosenbrock_slow         Powell       mean_time     0.08862974643707275   \r\n                 rosenbrock_slow         Powell        max_obj     1.1499616197377613e-22 \r\n                 rosenbrock_slow         Powell        min_obj     2.465190328815662e-31  \r\n                 rosenbrock_slow         Powell        mean_obj    1.577792702290088e-23  \r\n                 rosenbrock_slow      nelder-mead     mean_nfev            310.2          \r\n                 rosenbrock_slow      nelder-mead     mean_time     0.03394351005554199   \r\n                 rosenbrock_slow      nelder-mead      max_obj     7.893727233623956e-09  \r\n                 rosenbrock_slow      nelder-mead      min_obj     2.8705996346698776e-10 \r\n                 rosenbrock_slow      nelder-mead      mean_obj    1.948083288418953e-09  \r\n                 rosenbrock_slow        L-BFGS-B      mean_nfev            148.0          \r\n                 rosenbrock_slow        L-BFGS-B      mean_time      0.0187636137008667   \r\n                 rosenbrock_slow        L-BFGS-B       max_obj     6.102861843282039e-06  \r\n                 rosenbrock_slow        L-BFGS-B       min_obj     2.0019484847508766e-08 \r\n                 rosenbrock_slow        L-BFGS-B       mean_obj    2.395468489328531e-06  \r\n                 rosenbrock_slow          BFGS        mean_nfev            177.2          \r\n                 rosenbrock_slow          BFGS        mean_time     0.02675905227661133   \r\n                 rosenbrock_slow          BFGS         max_obj     9.367688138779361e-11  \r\n                 rosenbrock_slow          BFGS         min_obj     2.0222008422508898e-11 \r\n                 rosenbrock_slow          BFGS         mean_obj    4.675037804518409e-11  \r\n                 rosenbrock_slow           CG         mean_nfev             n\/a           \r\n                 rosenbrock_slow           CG         mean_time             n\/a           \r\n                 rosenbrock_slow           CG          max_obj              n\/a           \r\n                 rosenbrock_slow           CG          min_obj              n\/a           \r\n                 rosenbrock_slow           CG          mean_obj             n\/a           \r\n                 rosenbrock_slow          TNC         mean_nfev             n\/a           \r\n                 rosenbrock_slow          TNC         mean_time             n\/a           \r\n                 rosenbrock_slow          TNC          max_obj              n\/a           \r\n                 rosenbrock_slow          TNC          min_obj              n\/a           \r\n                 rosenbrock_slow          TNC          mean_obj             n\/a           \r\n                 rosenbrock_slow         SLSQP        mean_nfev            155.1          \r\n                 rosenbrock_slow         SLSQP        mean_time     0.019835877418518066  \r\n                 rosenbrock_slow         SLSQP         max_obj     5.024151064625316e-05  \r\n                 rosenbrock_slow         SLSQP         min_obj     4.4093823331410435e-07 \r\n                 rosenbrock_slow         SLSQP         mean_obj    1.0577238854081483e-05 \r\n                 rosenbrock_slow       Newton-CG      mean_nfev             n\/a           \r\n                 rosenbrock_slow       Newton-CG      mean_time             n\/a           \r\n                 rosenbrock_slow       Newton-CG       max_obj              n\/a           \r\n                 rosenbrock_slow       Newton-CG       min_obj              n\/a           \r\n                 rosenbrock_slow       Newton-CG       mean_obj             n\/a           \r\n                 rosenbrock_slow         dogleg       mean_nfev             n\/a           \r\n                 rosenbrock_slow         dogleg       mean_time             n\/a           \r\n                 rosenbrock_slow         dogleg        max_obj              n\/a           \r\n                 rosenbrock_slow         dogleg        min_obj              n\/a           \r\n                 rosenbrock_slow         dogleg        mean_obj             n\/a           \r\n                 rosenbrock_slow       trust-ncg      mean_nfev             n\/a           \r\n                 rosenbrock_slow       trust-ncg      mean_time             n\/a           \r\n                 rosenbrock_slow       trust-ncg       max_obj              n\/a           \r\n                 rosenbrock_slow       trust-ncg       min_obj              n\/a           \r\n                 rosenbrock_slow       trust-ncg       mean_obj             n\/a           \r\n                 rosenbrock_slow      trust-exact     mean_nfev             n\/a           \r\n                 rosenbrock_slow      trust-exact     mean_time             n\/a           \r\n                 rosenbrock_slow      trust-exact      max_obj              n\/a           \r\n                 rosenbrock_slow      trust-exact      min_obj              n\/a           \r\n                 rosenbrock_slow      trust-exact      mean_obj             n\/a           \r\n                 rosenbrock_slow      trust-krylov    mean_nfev             n\/a           \r\n                 rosenbrock_slow      trust-krylov    mean_time             n\/a           \r\n                 rosenbrock_slow      trust-krylov     max_obj              n\/a           \r\n                 rosenbrock_slow      trust-krylov     min_obj              n\/a           \r\n                 rosenbrock_slow      trust-krylov     mean_obj             n\/a           \r\n                 rosenbrock_slow      trust-constr    mean_nfev            218.0          \r\n                 rosenbrock_slow      trust-constr    mean_time     0.09591772556304931   \r\n                 rosenbrock_slow      trust-constr     max_obj     7.0143284069484044e-09 \r\n                 rosenbrock_slow      trust-constr     min_obj     3.946598530522198e-11  \r\n                 rosenbrock_slow      trust-constr     mean_obj     9.51616929722639e-10  \r\n                rosenbrock_nograd        COBYLA       mean_nfev            1000.0         \r\n                rosenbrock_nograd        COBYLA       mean_time     0.02318582534790039   \r\n                rosenbrock_nograd        COBYLA        max_obj       6.480135564619959    \r\n                rosenbrock_nograd        COBYLA        min_obj      0.05985270052507147   \r\n                rosenbrock_nograd        COBYLA        mean_obj      1.040669274800954    \r\n                rosenbrock_nograd        COBYQA       mean_nfev            152.8          \r\n                rosenbrock_nograd        COBYQA       mean_time       0.46280517578125    \r\n                rosenbrock_nograd        COBYQA        max_obj     2.627973375197527e-07  \r\n                rosenbrock_nograd        COBYQA        min_obj     9.551703764181064e-11  \r\n                rosenbrock_nograd        COBYQA        mean_obj    3.890437153553753e-08  \r\n                rosenbrock_nograd        Powell       mean_nfev            887.2          \r\n                rosenbrock_nograd        Powell       mean_time     0.017684626579284667  \r\n                rosenbrock_nograd        Powell        max_obj     1.1499616197377613e-22 \r\n                rosenbrock_nograd        Powell        min_obj     2.465190328815662e-31  \r\n                rosenbrock_nograd        Powell        mean_obj    1.577792702290088e-23  \r\n                rosenbrock_nograd     nelder-mead     mean_nfev            310.2          \r\n                rosenbrock_nograd     nelder-mead     mean_time     0.007095956802368164  \r\n                rosenbrock_nograd     nelder-mead      max_obj     7.893727233623956e-09  \r\n                rosenbrock_nograd     nelder-mead      min_obj     2.8705996346698776e-10 \r\n                rosenbrock_nograd     nelder-mead      mean_obj    1.948083288418953e-09  \r\n                rosenbrock_nograd       L-BFGS-B      mean_nfev            148.0          \r\n                rosenbrock_nograd       L-BFGS-B      mean_time     0.00701148509979248   \r\n                rosenbrock_nograd       L-BFGS-B       max_obj     6.102861843282039e-06  \r\n                rosenbrock_nograd       L-BFGS-B       min_obj     2.0019484847508766e-08 \r\n                rosenbrock_nograd       L-BFGS-B       mean_obj    2.395468489328531e-06  \r\n                rosenbrock_nograd         BFGS        mean_nfev            177.2          \r\n                rosenbrock_nograd         BFGS        mean_time     0.012914109230041503  \r\n                rosenbrock_nograd         BFGS         max_obj     9.367688138779361e-11  \r\n                rosenbrock_nograd         BFGS         min_obj     2.0222008422508898e-11 \r\n                rosenbrock_nograd         BFGS         mean_obj    4.675037804518409e-11  \r\n                rosenbrock_nograd          CG         mean_nfev             n\/a           \r\n                rosenbrock_nograd          CG         mean_time             n\/a           \r\n                rosenbrock_nograd          CG          max_obj              n\/a           \r\n                rosenbrock_nograd          CG          min_obj              n\/a           \r\n                rosenbrock_nograd          CG          mean_obj             n\/a           \r\n                rosenbrock_nograd         TNC         mean_nfev             n\/a           \r\n                rosenbrock_nograd         TNC         mean_time             n\/a           \r\n                rosenbrock_nograd         TNC          max_obj              n\/a           \r\n                rosenbrock_nograd         TNC          min_obj              n\/a           \r\n                rosenbrock_nograd         TNC          mean_obj             n\/a           \r\n                rosenbrock_nograd        SLSQP        mean_nfev            155.1          \r\n                rosenbrock_nograd        SLSQP        mean_time     0.007709956169128418  \r\n                rosenbrock_nograd        SLSQP         max_obj     5.024151064625316e-05  \r\n                rosenbrock_nograd        SLSQP         min_obj     4.4093823331410435e-07 \r\n                rosenbrock_nograd        SLSQP         mean_obj    1.0577238854081483e-05 \r\n                rosenbrock_nograd      Newton-CG      mean_nfev             n\/a           \r\n                rosenbrock_nograd      Newton-CG      mean_time             n\/a           \r\n                rosenbrock_nograd      Newton-CG       max_obj              n\/a           \r\n                rosenbrock_nograd      Newton-CG       min_obj              n\/a           \r\n                rosenbrock_nograd      Newton-CG       mean_obj             n\/a           \r\n                rosenbrock_nograd        dogleg       mean_nfev             n\/a           \r\n                rosenbrock_nograd        dogleg       mean_time             n\/a           \r\n                rosenbrock_nograd        dogleg        max_obj              n\/a           \r\n                rosenbrock_nograd        dogleg        min_obj              n\/a           \r\n                rosenbrock_nograd        dogleg        mean_obj             n\/a           \r\n                rosenbrock_nograd      trust-ncg      mean_nfev             n\/a           \r\n                rosenbrock_nograd      trust-ncg      mean_time             n\/a           \r\n                rosenbrock_nograd      trust-ncg       max_obj              n\/a           \r\n                rosenbrock_nograd      trust-ncg       min_obj              n\/a           \r\n                rosenbrock_nograd      trust-ncg       mean_obj             n\/a           \r\n                rosenbrock_nograd     trust-exact     mean_nfev             n\/a           \r\n                rosenbrock_nograd     trust-exact     mean_time             n\/a           \r\n                rosenbrock_nograd     trust-exact      max_obj              n\/a           \r\n                rosenbrock_nograd     trust-exact      min_obj              n\/a           \r\n                rosenbrock_nograd     trust-exact      mean_obj             n\/a           \r\n                rosenbrock_nograd     trust-krylov    mean_nfev             n\/a           \r\n                rosenbrock_nograd     trust-krylov    mean_time             n\/a           \r\n                rosenbrock_nograd     trust-krylov     max_obj              n\/a           \r\n                rosenbrock_nograd     trust-krylov     min_obj              n\/a           \r\n                rosenbrock_nograd     trust-krylov     mean_obj             n\/a           \r\n                rosenbrock_nograd     trust-constr    mean_nfev            218.0          \r\n                rosenbrock_nograd     trust-constr    mean_time     0.07255344390869141   \r\n                rosenbrock_nograd     trust-constr     max_obj     7.0143284069484044e-09 \r\n                rosenbrock_nograd     trust-constr     min_obj     3.946598530522198e-11  \r\n                rosenbrock_nograd     trust-constr     mean_obj     9.51616929722639e-10  \r\n                    rosenbrock           COBYLA       mean_nfev            1000.0         \r\n                    rosenbrock           COBYLA       mean_time     0.018932080268859862  \r\n                    rosenbrock           COBYLA        max_obj       6.480135564619959    \r\n                    rosenbrock           COBYLA        min_obj      0.05985270052507147   \r\n                    rosenbrock           COBYLA        mean_obj      1.040669274800954    \r\n                    rosenbrock           COBYQA       mean_nfev            152.8          \r\n                    rosenbrock           COBYQA       mean_time     0.42359488010406493   \r\n                    rosenbrock           COBYQA        max_obj     2.627973375197527e-07  \r\n                    rosenbrock           COBYQA        min_obj     9.551703764181064e-11  \r\n                    rosenbrock           COBYQA        mean_obj    3.890437153553753e-08  \r\n                    rosenbrock           Powell       mean_nfev            887.2          \r\n                    rosenbrock           Powell       mean_time     0.01701934337615967   \r\n                    rosenbrock           Powell        max_obj     1.1499616197377613e-22 \r\n                    rosenbrock           Powell        min_obj     2.465190328815662e-31  \r\n                    rosenbrock           Powell        mean_obj    1.577792702290088e-23  \r\n                    rosenbrock        nelder-mead     mean_nfev            310.2          \r\n                    rosenbrock        nelder-mead     mean_time     0.008095026016235352  \r\n                    rosenbrock        nelder-mead      max_obj     7.893727233623956e-09  \r\n                    rosenbrock        nelder-mead      min_obj     2.8705996346698776e-10 \r\n                    rosenbrock        nelder-mead      mean_obj    1.948083288418953e-09  \r\n                    rosenbrock          L-BFGS-B      mean_nfev            92.75          \r\n                    rosenbrock          L-BFGS-B      mean_time     0.00424497127532959   \r\n                    rosenbrock          L-BFGS-B       max_obj     6.102861843282039e-06  \r\n                    rosenbrock          L-BFGS-B       min_obj     1.932333213566105e-08  \r\n                    rosenbrock          L-BFGS-B       mean_obj    2.1047384326635483e-06 \r\n                    rosenbrock            BFGS        mean_nfev            110.95         \r\n                    rosenbrock            BFGS        mean_time     0.007572984695434571  \r\n                    rosenbrock            BFGS         max_obj     9.367688138779361e-11  \r\n                    rosenbrock            BFGS         min_obj     2.629601156323012e-13  \r\n                    rosenbrock            BFGS         mean_obj    2.5838983525240928e-11 \r\n                    rosenbrock             CG         mean_nfev            110.0          \r\n                    rosenbrock             CG         mean_time     0.008356165885925294  \r\n                    rosenbrock             CG          max_obj     8.124139813740547e-09  \r\n                    rosenbrock             CG          min_obj     1.2703890319045386e-14 \r\n                    rosenbrock             CG          mean_obj    2.184879891252899e-09  \r\n                    rosenbrock            TNC         mean_nfev             84.3          \r\n                    rosenbrock            TNC         mean_time    0.0037304162979125977  \r\n                    rosenbrock            TNC          max_obj       2.2122994099538835   \r\n                    rosenbrock            TNC          min_obj     5.117059169707981e-07  \r\n                    rosenbrock            TNC          mean_obj     0.41353307449601323   \r\n                    rosenbrock           SLSQP        mean_nfev            102.3          \r\n                    rosenbrock           SLSQP        mean_time     0.005245399475097656  \r\n                    rosenbrock           SLSQP         max_obj     5.092355413147263e-05  \r\n                    rosenbrock           SLSQP         min_obj     4.4093823331410435e-07 \r\n                    rosenbrock           SLSQP         mean_obj    1.2278847681581272e-05 \r\n                    rosenbrock         Newton-CG      mean_nfev             64.5          \r\n                    rosenbrock         Newton-CG      mean_time     0.007852506637573243  \r\n                    rosenbrock         Newton-CG       max_obj      0.001749938271370122  \r\n                    rosenbrock         Newton-CG       min_obj     3.2530627645985997e-06 \r\n                    rosenbrock         Newton-CG       mean_obj    0.00028386319748745415 \r\n                    rosenbrock           dogleg       mean_nfev             18.4          \r\n                    rosenbrock           dogleg       mean_time    0.0019324541091918946  \r\n                    rosenbrock           dogleg        max_obj     8.807022403281723e-12  \r\n                    rosenbrock           dogleg        min_obj     9.508766456687529e-22  \r\n                    rosenbrock           dogleg        mean_obj    1.0473748283949003e-12 \r\n                    rosenbrock         trust-ncg      mean_nfev             43.5          \r\n                    rosenbrock         trust-ncg      mean_time     0.005076169967651367  \r\n                    rosenbrock         trust-ncg       max_obj     4.448285444529859e-09  \r\n                    rosenbrock         trust-ncg       min_obj     1.5850914812888835e-14 \r\n                    rosenbrock         trust-ncg       mean_obj    9.096654341078131e-10  \r\n                    rosenbrock        trust-exact     mean_nfev             17.5          \r\n                    rosenbrock        trust-exact     mean_time    0.0030606985092163086  \r\n                    rosenbrock        trust-exact      max_obj     1.4567062252035646e-10 \r\n                    rosenbrock        trust-exact      min_obj     2.961486586346507e-22  \r\n                    rosenbrock        trust-exact      mean_obj    2.3688425209734387e-11 \r\n                    rosenbrock        trust-krylov    mean_nfev             35.7          \r\n                    rosenbrock        trust-krylov    mean_time     0.004921650886535645  \r\n                    rosenbrock        trust-krylov     max_obj     9.551148041084868e-09  \r\n                    rosenbrock        trust-krylov     min_obj     1.8117314229707773e-22 \r\n                    rosenbrock        trust-krylov     mean_obj    1.2080822443269492e-09 \r\n                    rosenbrock        trust-constr    mean_nfev      98.16666666666667    \r\n                    rosenbrock        trust-constr    mean_time     0.05261982282002767   \r\n                    rosenbrock        trust-constr     max_obj     7.0143284069484044e-09 \r\n                    rosenbrock        trust-constr     min_obj     1.3683069588567212e-14 \r\n                    rosenbrock        trust-constr     mean_obj    7.582424644998224e-10  \r\n                 rosenbrock_tight        COBYLA       mean_nfev            1000.0         \r\n                 rosenbrock_tight        COBYLA       mean_time     0.019109272956848146  \r\n                 rosenbrock_tight        COBYLA        max_obj       6.468444067117351    \r\n                 rosenbrock_tight        COBYLA        min_obj      0.05985270052507147   \r\n                 rosenbrock_tight        COBYLA        mean_obj      1.0395001250506932   \r\n                 rosenbrock_tight        COBYQA       mean_nfev            180.2          \r\n                 rosenbrock_tight        COBYQA       mean_time      0.5029502630233764   \r\n                 rosenbrock_tight        COBYQA        max_obj     4.6821658959029516e-15 \r\n                 rosenbrock_tight        COBYQA        min_obj     1.442329236888611e-17  \r\n                 rosenbrock_tight        COBYQA        mean_obj    1.0833609443328238e-15 \r\n                 rosenbrock_tight        Powell       mean_nfev            1070.1         \r\n                 rosenbrock_tight        Powell       mean_time     0.020065736770629884  \r\n                 rosenbrock_tight        Powell        max_obj     3.0211516529558574e-23 \r\n                 rosenbrock_tight        Powell        min_obj     2.465190328815662e-31  \r\n                 rosenbrock_tight        Powell        mean_obj    5.649905225233614e-24  \r\n                 rosenbrock_tight     nelder-mead     mean_nfev            407.7          \r\n                 rosenbrock_tight     nelder-mead     mean_time     0.008684158325195312  \r\n                 rosenbrock_tight     nelder-mead      max_obj      3.30913219217597e-17  \r\n                 rosenbrock_tight     nelder-mead      min_obj     6.533780628454954e-18  \r\n                 rosenbrock_tight     nelder-mead      mean_obj    1.7872342256810196e-17 \r\n                 rosenbrock_tight       L-BFGS-B      mean_nfev             99.2          \r\n                 rosenbrock_tight       L-BFGS-B      mean_time     0.004272651672363281  \r\n                 rosenbrock_tight       L-BFGS-B       max_obj      3.91876712597249e-09  \r\n                 rosenbrock_tight       L-BFGS-B       min_obj     3.9102181450584546e-16 \r\n                 rosenbrock_tight       L-BFGS-B       mean_obj    2.4807079357316176e-10 \r\n                 rosenbrock_tight         BFGS        mean_nfev            160.9          \r\n                 rosenbrock_tight         BFGS        mean_time     0.010002756118774414  \r\n                 rosenbrock_tight         BFGS         max_obj     3.609649916776654e-11  \r\n                 rosenbrock_tight         BFGS         min_obj     2.3627412588774485e-24 \r\n                 rosenbrock_tight         BFGS         mean_obj    1.6841960135232725e-11 \r\n                 rosenbrock_tight          CG         mean_nfev            127.8          \r\n                 rosenbrock_tight          CG         mean_time     0.010236239433288575  \r\n                 rosenbrock_tight          CG          max_obj     1.9318929334298802e-18 \r\n                 rosenbrock_tight          CG          min_obj     2.8848825536964484e-22 \r\n                 rosenbrock_tight          CG          mean_obj    5.959714896391864e-19  \r\n                 rosenbrock_tight         TNC         mean_nfev             90.4          \r\n                 rosenbrock_tight         TNC         mean_time    0.0034816741943359377  \r\n                 rosenbrock_tight         TNC          max_obj       2.2122994099538835   \r\n                 rosenbrock_tight         TNC          min_obj     9.203035045321898e-11  \r\n                 rosenbrock_tight         TNC          mean_obj     0.41043093540666387   \r\n                 rosenbrock_tight        SLSQP        mean_nfev            108.15         \r\n                 rosenbrock_tight        SLSQP        mean_time     0.005590105056762695  \r\n                 rosenbrock_tight        SLSQP         max_obj     2.623940609606607e-09  \r\n                 rosenbrock_tight        SLSQP         min_obj     4.4299895734851414e-12 \r\n                 rosenbrock_tight        SLSQP         mean_obj    5.646132513964207e-10  \r\n                 rosenbrock_tight      Newton-CG      mean_nfev             71.6          \r\n                 rosenbrock_tight      Newton-CG      mean_time     0.008595752716064452  \r\n                 rosenbrock_tight      Newton-CG       max_obj     4.602420867959876e-18  \r\n                 rosenbrock_tight      Newton-CG       min_obj     3.383562103372784e-24  \r\n                 rosenbrock_tight      Newton-CG       mean_obj    8.645626438121326e-19  \r\n                 rosenbrock_tight        dogleg       mean_nfev             19.2          \r\n                 rosenbrock_tight        dogleg       mean_time     0.00220029354095459   \r\n                 rosenbrock_tight        dogleg        max_obj     5.333478304546413e-21  \r\n                 rosenbrock_tight        dogleg        min_obj              0.0           \r\n                 rosenbrock_tight        dogleg        mean_obj    7.554726540980743e-22  \r\n                 rosenbrock_tight      trust-ncg      mean_nfev             44.9          \r\n                 rosenbrock_tight      trust-ncg      mean_time     0.004973936080932617  \r\n                 rosenbrock_tight      trust-ncg       max_obj     4.608518592049892e-23  \r\n                 rosenbrock_tight      trust-ncg       min_obj              0.0           \r\n                 rosenbrock_tight      trust-ncg       mean_obj    6.432714849941195e-24  \r\n                 rosenbrock_tight     trust-exact     mean_nfev             18.4          \r\n                 rosenbrock_tight     trust-exact     mean_time    0.0032602548599243164  \r\n                 rosenbrock_tight     trust-exact      max_obj     8.934399044739956e-19  \r\n                 rosenbrock_tight     trust-exact      min_obj              0.0           \r\n                 rosenbrock_tight     trust-exact      mean_obj    9.805261192705719e-20  \r\n                 rosenbrock_tight     trust-krylov    mean_nfev             93.7          \r\n                 rosenbrock_tight     trust-krylov    mean_time     0.008965826034545899  \r\n                 rosenbrock_tight     trust-krylov     max_obj     1.148537593305614e-16  \r\n                 rosenbrock_tight     trust-krylov     min_obj     2.216970314607213e-27  \r\n                 rosenbrock_tight     trust-krylov     mean_obj    1.227821735143473e-17  \r\n                 rosenbrock_tight     trust-constr    mean_nfev      106.53333333333333   \r\n                 rosenbrock_tight     trust-constr    mean_time      0.0547580083211263   \r\n                 rosenbrock_tight     trust-constr     max_obj     3.8458963755388384e-11 \r\n                 rosenbrock_tight     trust-constr     min_obj     4.992010415851715e-30  \r\n                 rosenbrock_tight     trust-constr     mean_obj    1.2124258427696534e-11 \r\n                 simple_quadratic        COBYLA       mean_nfev             54.8          \r\n                 simple_quadratic        COBYLA       mean_time    0.0007599115371704102  \r\n                 simple_quadratic        COBYLA        max_obj     2.203652512574382e-08  \r\n                 simple_quadratic        COBYLA        min_obj     3.2516329170669375e-09 \r\n                 simple_quadratic        COBYLA        mean_obj    9.518126940933508e-09  \r\n                 simple_quadratic        COBYQA       mean_nfev             22.2          \r\n                 simple_quadratic        COBYQA       mean_time     0.042428326606750486  \r\n                 simple_quadratic        COBYQA        max_obj     6.227518549300432e-30  \r\n                 simple_quadratic        COBYQA        min_obj              0.0           \r\n                 simple_quadratic        COBYQA        mean_obj    8.872232632322827e-31  \r\n                 simple_quadratic        Powell       mean_nfev             47.2          \r\n                 simple_quadratic        Powell       mean_time    0.0005606174468994141  \r\n                 simple_quadratic        Powell        max_obj     8.874685183736383e-31  \r\n                 simple_quadratic        Powell        min_obj              0.0           \r\n                 simple_quadratic        Powell        mean_obj    1.2855981106468597e-31 \r\n                 simple_quadratic     nelder-mead     mean_nfev            152.8          \r\n                 simple_quadratic     nelder-mead     mean_time    0.0022964000701904295  \r\n                 simple_quadratic     nelder-mead      max_obj     4.0526692995425695e-09 \r\n                 simple_quadratic     nelder-mead      min_obj     8.194357880792766e-10  \r\n                 simple_quadratic     nelder-mead      mean_obj    2.2619242867800513e-09 \r\n                 simple_quadratic       L-BFGS-B      mean_nfev             7.5           \r\n                 simple_quadratic       L-BFGS-B      mean_time    0.00033174753189086915 \r\n                 simple_quadratic       L-BFGS-B       max_obj     2.6721184548216788e-15 \r\n                 simple_quadratic       L-BFGS-B       min_obj              0.0           \r\n                 simple_quadratic       L-BFGS-B       mean_obj    2.4856928645318576e-16 \r\n                 simple_quadratic         BFGS        mean_nfev             7.5           \r\n                 simple_quadratic         BFGS        mean_time    0.00040994882583618165 \r\n                 simple_quadratic         BFGS         max_obj     2.7704509948525217e-14 \r\n                 simple_quadratic         BFGS         min_obj     1.7340892663330385e-33 \r\n                 simple_quadratic         BFGS         mean_obj    1.6821868699593401e-15 \r\n                 simple_quadratic          CG         mean_nfev             3.6           \r\n                 simple_quadratic          CG         mean_time    0.00026955604553222654 \r\n                 simple_quadratic          CG          max_obj     1.4789379928976038e-09 \r\n                 simple_quadratic          CG          min_obj              0.0           \r\n                 simple_quadratic          CG          mean_obj    3.154879136346819e-10  \r\n                 simple_quadratic         TNC         mean_nfev             7.0           \r\n                 simple_quadratic         TNC         mean_time    0.00022335052490234374 \r\n                 simple_quadratic         TNC          max_obj     2.0156105267122383e-06 \r\n                 simple_quadratic         TNC          min_obj      5.1464530111305e-20   \r\n                 simple_quadratic         TNC          mean_obj    2.0304535898501623e-07 \r\n                 simple_quadratic        SLSQP        mean_nfev             6.0           \r\n                 simple_quadratic        SLSQP        mean_time    0.00034669637680053713 \r\n                 simple_quadratic        SLSQP         max_obj     7.885667440551341e-16  \r\n                 simple_quadratic        SLSQP         min_obj              0.0           \r\n                 simple_quadratic        SLSQP         mean_obj    1.336048763662199e-16  \r\n                 simple_quadratic      Newton-CG      mean_nfev             2.0           \r\n                 simple_quadratic      Newton-CG      mean_time    0.00026035308837890625 \r\n                 simple_quadratic      Newton-CG       max_obj              0.0           \r\n                 simple_quadratic      Newton-CG       min_obj              0.0           \r\n                 simple_quadratic      Newton-CG       mean_obj             0.0           \r\n                 simple_quadratic        dogleg       mean_nfev             3.0           \r\n                 simple_quadratic        dogleg       mean_time    0.0002699136734008789  \r\n                 simple_quadratic        dogleg        max_obj     7.395570986446986e-32  \r\n                 simple_quadratic        dogleg        min_obj     1.7333369499485123e-33 \r\n                 simple_quadratic        dogleg        mean_obj     2.78107340254829e-32  \r\n                 simple_quadratic      trust-ncg      mean_nfev             3.0           \r\n                 simple_quadratic      trust-ncg      mean_time    0.00023586750030517577 \r\n                 simple_quadratic      trust-ncg       max_obj              0.0           \r\n                 simple_quadratic      trust-ncg       min_obj              0.0           \r\n                 simple_quadratic      trust-ncg       mean_obj             0.0           \r\n                 simple_quadratic     trust-exact     mean_nfev             3.0           \r\n                 simple_quadratic     trust-exact     mean_time    0.00048007965087890623 \r\n                 simple_quadratic     trust-exact      max_obj     7.395570986446986e-32  \r\n                 simple_quadratic     trust-exact      min_obj     1.7333369499485123e-33 \r\n                 simple_quadratic     trust-exact      mean_obj     2.76371887415049e-32  \r\n                 simple_quadratic     trust-krylov    mean_nfev             3.0           \r\n                 simple_quadratic     trust-krylov    mean_time     0.000313115119934082  \r\n                 simple_quadratic     trust-krylov     max_obj              0.0           \r\n                 simple_quadratic     trust-krylov     min_obj              0.0           \r\n                 simple_quadratic     trust-krylov     mean_obj             0.0           \r\n                 simple_quadratic     trust-constr    mean_nfev             6.0           \r\n                 simple_quadratic     trust-constr    mean_time    0.0032899141311645507  \r\n                 simple_quadratic     trust-constr     max_obj     1.7116592189687573e-15 \r\n                 simple_quadratic     trust-constr     min_obj              0.0           \r\n                 simple_quadratic     trust-constr     mean_obj    9.386350782614601e-17  \r\n               asymmetric_quadratic      COBYLA       mean_nfev             64.2          \r\n               asymmetric_quadratic      COBYLA       mean_time    0.0007475852966308594  \r\n               asymmetric_quadratic      COBYLA        max_obj      -0.24999997597739806  \r\n               asymmetric_quadratic      COBYLA        min_obj      -0.2499999973672839   \r\n               asymmetric_quadratic      COBYLA        mean_obj     -0.24999998850473912  \r\n               asymmetric_quadratic      COBYQA       mean_nfev             23.5          \r\n               asymmetric_quadratic      COBYQA       mean_time     0.05441782474517822   \r\n               asymmetric_quadratic      COBYQA        max_obj             -0.25          \r\n               asymmetric_quadratic      COBYQA        min_obj             -0.25          \r\n               asymmetric_quadratic      COBYQA        mean_obj            -0.25          \r\n               asymmetric_quadratic      Powell       mean_nfev            134.6          \r\n               asymmetric_quadratic      Powell       mean_time    0.0013516902923583984  \r\n               asymmetric_quadratic      Powell        max_obj             -0.25          \r\n               asymmetric_quadratic      Powell        min_obj             -0.25          \r\n               asymmetric_quadratic      Powell        mean_obj            -0.25          \r\n               asymmetric_quadratic   nelder-mead     mean_nfev            162.9          \r\n               asymmetric_quadratic   nelder-mead     mean_time    0.0025487422943115236  \r\n               asymmetric_quadratic   nelder-mead      max_obj      -0.2499999954228166   \r\n               asymmetric_quadratic   nelder-mead      min_obj      -0.24999999942034346  \r\n               asymmetric_quadratic   nelder-mead      mean_obj     -0.24999999810601786  \r\n               asymmetric_quadratic     L-BFGS-B      mean_nfev             7.5           \r\n               asymmetric_quadratic     L-BFGS-B      mean_time    0.00035222768783569334 \r\n               asymmetric_quadratic     L-BFGS-B       max_obj      -0.24999999999999323  \r\n               asymmetric_quadratic     L-BFGS-B       min_obj             -0.25          \r\n               asymmetric_quadratic     L-BFGS-B       mean_obj     -0.24999999999999883  \r\n               asymmetric_quadratic       BFGS        mean_nfev             7.5           \r\n               asymmetric_quadratic       BFGS        mean_time    0.00045205354690551757 \r\n               asymmetric_quadratic       BFGS         max_obj      -0.24999999999998002  \r\n               asymmetric_quadratic       BFGS         min_obj             -0.25          \r\n               asymmetric_quadratic       BFGS         mean_obj     -0.24999999999999784  \r\n               asymmetric_quadratic        CG         mean_nfev             3.1           \r\n               asymmetric_quadratic        CG         mean_time    0.00025806427001953127 \r\n               asymmetric_quadratic        CG          max_obj             -0.25          \r\n               asymmetric_quadratic        CG          min_obj             -0.25          \r\n               asymmetric_quadratic        CG          mean_obj            -0.25          \r\n               asymmetric_quadratic       TNC         mean_nfev             12.5          \r\n               asymmetric_quadratic       TNC         mean_time    0.0003543376922607422  \r\n               asymmetric_quadratic       TNC          max_obj      -0.24999000642369906  \r\n               asymmetric_quadratic       TNC          min_obj      -0.2499999999881751   \r\n               asymmetric_quadratic       TNC          mean_obj     -0.24999858212319187  \r\n               asymmetric_quadratic      SLSQP        mean_nfev             6.0           \r\n               asymmetric_quadratic      SLSQP        mean_time    0.0003188133239746094  \r\n               asymmetric_quadratic      SLSQP         max_obj      -0.2499999999999984   \r\n               asymmetric_quadratic      SLSQP         min_obj             -0.25          \r\n               asymmetric_quadratic      SLSQP         mean_obj     -0.24999999999999972  \r\n               asymmetric_quadratic    Newton-CG      mean_nfev             2.0           \r\n               asymmetric_quadratic    Newton-CG      mean_time    0.0002932548522949219  \r\n               asymmetric_quadratic    Newton-CG       max_obj             -0.25          \r\n               asymmetric_quadratic    Newton-CG       min_obj             -0.25          \r\n               asymmetric_quadratic    Newton-CG       mean_obj            -0.25          \r\n               asymmetric_quadratic      dogleg       mean_nfev             2.9           \r\n               asymmetric_quadratic      dogleg       mean_time    0.0002681970596313477  \r\n               asymmetric_quadratic      dogleg        max_obj             -0.25          \r\n               asymmetric_quadratic      dogleg        min_obj             -0.25          \r\n               asymmetric_quadratic      dogleg        mean_obj            -0.25          \r\n               asymmetric_quadratic    trust-ncg      mean_nfev             2.9           \r\n               asymmetric_quadratic    trust-ncg      mean_time     0.000263524055480957  \r\n               asymmetric_quadratic    trust-ncg       max_obj             -0.25          \r\n               asymmetric_quadratic    trust-ncg       min_obj             -0.25          \r\n               asymmetric_quadratic    trust-ncg       mean_obj            -0.25          \r\n               asymmetric_quadratic   trust-exact     mean_nfev             2.9           \r\n               asymmetric_quadratic   trust-exact     mean_time    0.0005069971084594727  \r\n               asymmetric_quadratic   trust-exact      max_obj             -0.25          \r\n               asymmetric_quadratic   trust-exact      min_obj             -0.25          \r\n               asymmetric_quadratic   trust-exact      mean_obj            -0.25          \r\n               asymmetric_quadratic   trust-krylov    mean_nfev             2.9           \r\n               asymmetric_quadratic   trust-krylov    mean_time    0.0002957820892333984  \r\n               asymmetric_quadratic   trust-krylov     max_obj             -0.25          \r\n               asymmetric_quadratic   trust-krylov     min_obj             -0.25          \r\n               asymmetric_quadratic   trust-krylov     mean_obj            -0.25          \r\n               asymmetric_quadratic   trust-constr    mean_nfev      5.966666666666667    \r\n               asymmetric_quadratic   trust-constr    mean_time    0.0037326574325561523  \r\n               asymmetric_quadratic   trust-constr     max_obj      -0.24999999999999856  \r\n               asymmetric_quadratic   trust-constr     min_obj             -0.25          \r\n               asymmetric_quadratic   trust-constr     mean_obj     -0.24999999999999986  \r\n                      sin_1d             COBYLA       mean_nfev             24.7          \r\n                      sin_1d             COBYLA       mean_time    0.00030689239501953126 \r\n                      sin_1d             COBYLA        max_obj       -0.999999991030651   \r\n                      sin_1d             COBYLA        min_obj      -0.9999999988115652   \r\n                      sin_1d             COBYLA        mean_obj     -0.9999999947997484   \r\n                      sin_1d             COBYQA       mean_nfev             13.7          \r\n                      sin_1d             COBYQA       mean_time     0.028106999397277833  \r\n                      sin_1d             COBYQA        max_obj      -0.9999999999074485   \r\n                      sin_1d             COBYQA        min_obj      -0.9999999999999999   \r\n                      sin_1d             COBYQA        mean_obj     -0.9999999999766593   \r\n                      sin_1d             Powell       mean_nfev             28.1          \r\n                      sin_1d             Powell       mean_time    0.00035395622253417967 \r\n                      sin_1d             Powell        max_obj      -0.9999999994277112   \r\n                      sin_1d             Powell        min_obj              -1.0          \r\n                      sin_1d             Powell        mean_obj     -0.9999999999215394   \r\n                      sin_1d          nelder-mead     mean_nfev             36.8          \r\n                      sin_1d          nelder-mead     mean_time    0.0006079673767089844  \r\n                      sin_1d          nelder-mead      max_obj      -0.9999999995689095   \r\n                      sin_1d          nelder-mead      min_obj      -0.9999999999999647   \r\n                      sin_1d          nelder-mead      mean_obj     -0.9999999998495948   \r\n                      sin_1d            L-BFGS-B      mean_nfev             9.45          \r\n                      sin_1d            L-BFGS-B      mean_time    0.00045709609985351564 \r\n                      sin_1d            L-BFGS-B       max_obj      -0.9999999733250989   \r\n                      sin_1d            L-BFGS-B       min_obj      -0.9999999999999969   \r\n                      sin_1d            L-BFGS-B       mean_obj     -0.9999999936365207   \r\n                      sin_1d              BFGS        mean_nfev             10.2          \r\n                      sin_1d              BFGS        mean_time    0.0007287979125976563  \r\n                      sin_1d              BFGS         max_obj      -0.9999999965829113   \r\n                      sin_1d              BFGS         min_obj              -1.0          \r\n                      sin_1d              BFGS         mean_obj     -0.9999999996362824   \r\n                      sin_1d               CG         mean_nfev             5.5           \r\n                      sin_1d               CG         mean_time    0.0003671407699584961  \r\n                      sin_1d               CG          max_obj      -0.9999999997706199   \r\n                      sin_1d               CG          min_obj              -1.0          \r\n                      sin_1d               CG          mean_obj     -0.9999999999451038   \r\n                      sin_1d              TNC         mean_nfev             10.2          \r\n                      sin_1d              TNC         mean_time     0.000263667106628418  \r\n                      sin_1d              TNC          max_obj      -0.9999999998840079   \r\n                      sin_1d              TNC          min_obj              -1.0          \r\n                      sin_1d              TNC          mean_obj     -0.9999999999816668   \r\n                      sin_1d             SLSQP        mean_nfev             6.95          \r\n                      sin_1d             SLSQP        mean_time    0.0005113840103149414  \r\n                      sin_1d             SLSQP         max_obj      -0.9999561001028866   \r\n                      sin_1d             SLSQP         min_obj      -0.9999999977056022   \r\n                      sin_1d             SLSQP         mean_obj     -0.9999874938865606   \r\n                      sin_1d           Newton-CG      mean_nfev             n\/a           \r\n                      sin_1d           Newton-CG      mean_time             n\/a           \r\n                      sin_1d           Newton-CG       max_obj              n\/a           \r\n                      sin_1d           Newton-CG       min_obj              n\/a           \r\n                      sin_1d           Newton-CG       mean_obj             n\/a           \r\n                      sin_1d             dogleg       mean_nfev             n\/a           \r\n                      sin_1d             dogleg       mean_time             n\/a           \r\n                      sin_1d             dogleg        max_obj              n\/a           \r\n                      sin_1d             dogleg        min_obj              n\/a           \r\n                      sin_1d             dogleg        mean_obj             n\/a           \r\n                      sin_1d           trust-ncg      mean_nfev             n\/a           \r\n                      sin_1d           trust-ncg      mean_time             n\/a           \r\n                      sin_1d           trust-ncg       max_obj              n\/a           \r\n                      sin_1d           trust-ncg       min_obj              n\/a           \r\n                      sin_1d           trust-ncg       mean_obj             n\/a           \r\n                      sin_1d          trust-exact     mean_nfev             n\/a           \r\n                      sin_1d          trust-exact     mean_time             n\/a           \r\n                      sin_1d          trust-exact      max_obj              n\/a           \r\n                      sin_1d          trust-exact      min_obj              n\/a           \r\n                      sin_1d          trust-exact      mean_obj             n\/a           \r\n                      sin_1d          trust-krylov    mean_nfev             n\/a           \r\n                      sin_1d          trust-krylov    mean_time             n\/a           \r\n                      sin_1d          trust-krylov     max_obj              n\/a           \r\n                      sin_1d          trust-krylov     min_obj              n\/a           \r\n                      sin_1d          trust-krylov     mean_obj             n\/a           \r\n                      sin_1d          trust-constr    mean_nfev             9.6           \r\n                      sin_1d          trust-constr    mean_time     0.008220541477203368  \r\n                      sin_1d          trust-constr     max_obj      -0.9999999953605566   \r\n                      sin_1d          trust-constr     min_obj              -1.0          \r\n                      sin_1d          trust-constr     mean_obj     -0.9999999995302697   \r\n                      booth              COBYLA       mean_nfev             67.1          \r\n                      booth              COBYLA       mean_time    0.0008814811706542968  \r\n                      booth              COBYLA        max_obj     1.7912081777512814e-07 \r\n                      booth              COBYLA        min_obj     1.715151073072473e-08  \r\n                      booth              COBYLA        mean_obj    9.727602527729466e-08  \r\n                      booth              COBYQA       mean_nfev             30.9          \r\n                      booth              COBYQA       mean_time     0.07166719436645508   \r\n                      booth              COBYQA        max_obj     1.0224462427977045e-08 \r\n                      booth              COBYQA        min_obj     3.981055866003684e-13  \r\n                      booth              COBYQA        mean_obj    1.1072140768523674e-09 \r\n                      booth              Powell       mean_nfev             63.4          \r\n                      booth              Powell       mean_time    0.0007703304290771484  \r\n                      booth              Powell        max_obj     7.651950780643815e-29  \r\n                      booth              Powell        min_obj              0.0           \r\n                      booth              Powell        mean_obj    2.1062586169401015e-29 \r\n                      booth           nelder-mead     mean_nfev            122.9          \r\n                      booth           nelder-mead     mean_time    0.0018064260482788086  \r\n                      booth           nelder-mead      max_obj       12.193805667342247   \r\n                      booth           nelder-mead      min_obj     5.171246278004901e-10  \r\n                      booth           nelder-mead      mean_obj      1.219380568865664    \r\n                      booth             L-BFGS-B      mean_nfev             11.6          \r\n                      booth             L-BFGS-B      mean_time    0.0005347371101379394  \r\n                      booth             L-BFGS-B       max_obj     2.1338676082762616e-07 \r\n                      booth             L-BFGS-B       min_obj     1.2027568162122083e-23 \r\n                      booth             L-BFGS-B       mean_obj    2.1341422667527374e-08 \r\n                      booth               BFGS        mean_nfev             15.4          \r\n                      booth               BFGS        mean_time    0.0009863972663879395  \r\n                      booth               BFGS         max_obj     7.552095014246231e-10  \r\n                      booth               BFGS         min_obj              0.0           \r\n                      booth               BFGS         mean_obj    1.4028567484760426e-10 \r\n                      booth                CG         mean_nfev             6.4           \r\n                      booth                CG         mean_time    0.0004721641540527344  \r\n                      booth                CG          max_obj     7.3837380728686705e-25 \r\n                      booth                CG          min_obj              0.0           \r\n                      booth                CG          mean_obj    8.662008283688798e-26  \r\n                      booth               TNC         mean_nfev             6.8           \r\n                      booth               TNC         mean_time     0.00023651123046875   \r\n                      booth               TNC          max_obj     4.8208367160805365e-14 \r\n                      booth               TNC          min_obj     1.5074779909037626e-21 \r\n                      booth               TNC          mean_obj    4.8330558571088694e-15 \r\n                      booth              SLSQP        mean_nfev             12.3          \r\n                      booth              SLSQP        mean_time    0.0006742238998413086  \r\n                      booth              SLSQP         max_obj     4.483137950070114e-07  \r\n                      booth              SLSQP         min_obj              0.0           \r\n                      booth              SLSQP         mean_obj    8.848768281252701e-08  \r\n                      booth            Newton-CG      mean_nfev             n\/a           \r\n                      booth            Newton-CG      mean_time             n\/a           \r\n                      booth            Newton-CG       max_obj              n\/a           \r\n                      booth            Newton-CG       min_obj              n\/a           \r\n                      booth            Newton-CG       mean_obj             n\/a           \r\n                      booth              dogleg       mean_nfev             n\/a           \r\n                      booth              dogleg       mean_time             n\/a           \r\n                      booth              dogleg        max_obj              n\/a           \r\n                      booth              dogleg        min_obj              n\/a           \r\n                      booth              dogleg        mean_obj             n\/a           \r\n                      booth            trust-ncg      mean_nfev             n\/a           \r\n                      booth            trust-ncg      mean_time             n\/a           \r\n                      booth            trust-ncg       max_obj              n\/a           \r\n                      booth            trust-ncg       min_obj              n\/a           \r\n                      booth            trust-ncg       mean_obj             n\/a           \r\n                      booth           trust-exact     mean_nfev             n\/a           \r\n                      booth           trust-exact     mean_time             n\/a           \r\n                      booth           trust-exact      max_obj              n\/a           \r\n                      booth           trust-exact      min_obj              n\/a           \r\n                      booth           trust-exact      mean_obj             n\/a           \r\n                      booth           trust-krylov    mean_nfev             n\/a           \r\n                      booth           trust-krylov    mean_time             n\/a           \r\n                      booth           trust-krylov     max_obj              n\/a           \r\n                      booth           trust-krylov     min_obj              n\/a           \r\n                      booth           trust-krylov     mean_obj             n\/a           \r\n                      booth           trust-constr    mean_nfev             14.8          \r\n                      booth           trust-constr    mean_time     0.008434629440307618  \r\n                      booth           trust-constr     max_obj     4.963419837080241e-10  \r\n                      booth           trust-constr     min_obj     1.6408306828597046e-28 \r\n                      booth           trust-constr     mean_obj    5.502755360622469e-11  \r\n                      beale              COBYLA       mean_nfev            461.1          \r\n                      beale              COBYLA       mean_time     0.005289626121520996  \r\n                      beale              COBYLA        max_obj       5.197476188271445    \r\n                      beale              COBYLA        min_obj     4.494183050729506e-06  \r\n                      beale              COBYLA        mean_obj      1.0645878833664317   \r\n                      beale              COBYQA       mean_nfev            206.9          \r\n                      beale              COBYQA       mean_time     0.48639354705810545   \r\n                      beale              COBYQA        max_obj       5.405226717701948    \r\n                      beale              COBYQA        min_obj     2.8020664537956077e-11 \r\n                      beale              COBYQA        mean_obj      1.2864101661603227   \r\n                      beale              Powell       mean_nfev            1821.9         \r\n                      beale              Powell       mean_time     0.019927501678466797  \r\n                      beale              Powell        max_obj       0.4612576155361058   \r\n                      beale              Powell        min_obj              0.0           \r\n                      beale              Powell        mean_obj     0.41411155100942254   \r\n                      beale           nelder-mead     mean_nfev            141.1          \r\n                      beale           nelder-mead     mean_time    0.0023077249526977537  \r\n                      beale           nelder-mead      max_obj     1.1428629175702085e-09 \r\n                      beale           nelder-mead      min_obj     9.284807062057851e-11  \r\n                      beale           nelder-mead      mean_obj    3.970084546666909e-10  \r\n                      beale             L-BFGS-B      mean_nfev             60.7          \r\n                      beale             L-BFGS-B      mean_time    0.0022241830825805663  \r\n                      beale             L-BFGS-B       max_obj        5.44664554527248    \r\n                      beale             L-BFGS-B       min_obj     3.055259460995235e-06  \r\n                      beale             L-BFGS-B       mean_obj      1.1948511188692428   \r\n                      beale               BFGS        mean_nfev            311.6          \r\n                      beale               BFGS        mean_time     0.019822537899017334  \r\n                      beale               BFGS         max_obj       0.4549434099996745   \r\n                      beale               BFGS         min_obj     3.459268421739041e-14  \r\n                      beale               BFGS         mean_obj     0.13604038140884195   \r\n                      beale                CG         mean_nfev             32.9          \r\n                      beale                CG         mean_time     0.001864790916442871  \r\n                      beale                CG          max_obj       15.004900346416202   \r\n                      beale                CG          min_obj     8.748070697720465e-14  \r\n                      beale                CG          mean_obj      1.5004900363633769   \r\n                      beale               TNC         mean_nfev             24.1          \r\n                      beale               TNC         mean_time    0.0005848169326782227  \r\n                      beale               TNC          max_obj      0.17820034878851157   \r\n                      beale               TNC          min_obj     1.0169060091826365e-09 \r\n                      beale               TNC          mean_obj     0.01830610047951784   \r\n                      beale              SLSQP        mean_nfev             25.9          \r\n                      beale              SLSQP        mean_time    0.0012875914573669434  \r\n                      beale              SLSQP         max_obj       5.205915323351862    \r\n                      beale              SLSQP         min_obj     0.00040931592971376295 \r\n                      beale              SLSQP         mean_obj      1.5104357514520017   \r\n                      beale            Newton-CG      mean_nfev             n\/a           \r\n                      beale            Newton-CG      mean_time             n\/a           \r\n                      beale            Newton-CG       max_obj              n\/a           \r\n                      beale            Newton-CG       min_obj              n\/a           \r\n                      beale            Newton-CG       mean_obj             n\/a           \r\n                      beale              dogleg       mean_nfev             n\/a           \r\n                      beale              dogleg       mean_time             n\/a           \r\n                      beale              dogleg        max_obj              n\/a           \r\n                      beale              dogleg        min_obj              n\/a           \r\n                      beale              dogleg        mean_obj             n\/a           \r\n                      beale            trust-ncg      mean_nfev             n\/a           \r\n                      beale            trust-ncg      mean_time             n\/a           \r\n                      beale            trust-ncg       max_obj              n\/a           \r\n                      beale            trust-ncg       min_obj              n\/a           \r\n                      beale            trust-ncg       mean_obj             n\/a           \r\n                      beale           trust-exact     mean_nfev             n\/a           \r\n                      beale           trust-exact     mean_time             n\/a           \r\n                      beale           trust-exact      max_obj              n\/a           \r\n                      beale           trust-exact      min_obj              n\/a           \r\n                      beale           trust-exact      mean_obj             n\/a           \r\n                      beale           trust-krylov    mean_nfev             n\/a           \r\n                      beale           trust-krylov    mean_time             n\/a           \r\n                      beale           trust-krylov     max_obj              n\/a           \r\n                      beale           trust-krylov     min_obj              n\/a           \r\n                      beale           trust-krylov     mean_obj             n\/a           \r\n                      beale           trust-constr    mean_nfev            476.35         \r\n                      beale           trust-constr    mean_time      0.2882970094680786   \r\n                      beale           trust-constr     max_obj       14.060403422086798   \r\n                      beale           trust-constr     min_obj     1.7342525463666807e-12 \r\n                      beale           trust-constr     mean_obj      1.4994513515047054   \r\n                        LJ               COBYLA       mean_nfev            868.8          \r\n                        LJ               COBYLA       mean_time     0.03003716468811035   \r\n                        LJ               COBYLA        max_obj       -2.005839480811375   \r\n                        LJ               COBYLA        min_obj       -5.999999612015638   \r\n                        LJ               COBYLA        mean_obj      -4.290638261230069   \r\n                        LJ               COBYQA       mean_nfev            696.3          \r\n                        LJ               COBYQA       mean_time      5.085029029846192    \r\n                        LJ               COBYQA        max_obj       -5.999999631288768   \r\n                        LJ               COBYQA        min_obj       -5.999999986291191   \r\n                        LJ               COBYQA        mean_obj      -5.999999907301164   \r\n                        LJ               Powell       mean_nfev            1900.0         \r\n                        LJ               Powell       mean_time     0.061473512649536134  \r\n                        LJ               Powell        max_obj      -5.0743693121621565   \r\n                        LJ               Powell        min_obj       -5.999999992795253   \r\n                        LJ               Powell        mean_obj     -5.7224243446285765   \r\n                        LJ            nelder-mead     mean_nfev            2281.8         \r\n                        LJ            nelder-mead     mean_time     0.10011553764343262   \r\n                        LJ            nelder-mead      max_obj      -2.0474063363496056   \r\n                        LJ            nelder-mead      min_obj       -5.999999991228395   \r\n                        LJ            nelder-mead      mean_obj     -3.8851133797531867   \r\n                        LJ              L-BFGS-B      mean_nfev            278.55         \r\n                        LJ              L-BFGS-B      mean_time     0.012547028064727784  \r\n                        LJ              L-BFGS-B       max_obj      -0.5858029766980063   \r\n                        LJ              L-BFGS-B       min_obj       -5.999997345609108   \r\n                        LJ              L-BFGS-B       mean_obj     -3.2094944991218455   \r\n                        LJ                BFGS        mean_nfev            722.75         \r\n                        LJ                BFGS        mean_time     0.03654147386550903   \r\n                        LJ                BFGS         max_obj       -5.999999999875672   \r\n                        LJ                BFGS         min_obj       -5.999999999999988   \r\n                        LJ                BFGS         mean_obj      -5.999999999973245   \r\n                        LJ                 CG         mean_nfev            146.9          \r\n                        LJ                 CG         mean_time     0.01984724998474121   \r\n                        LJ                 CG          max_obj      -5.9999999997497815   \r\n                        LJ                 CG          min_obj       -5.999999999999748   \r\n                        LJ                 CG          mean_obj      -5.999999999936375   \r\n                        LJ                TNC         mean_nfev            115.6          \r\n                        LJ                TNC         mean_time     0.011194372177124023  \r\n                        LJ                TNC          max_obj       -4.163230802726858   \r\n                        LJ                TNC          min_obj      -5.9999999788861755   \r\n                        LJ                TNC          mean_obj      -5.487077119375702   \r\n                        LJ               SLSQP        mean_nfev            208.5          \r\n                        LJ               SLSQP        mean_time     0.010371112823486328  \r\n                        LJ               SLSQP         max_obj     -6.101456492444751e-44 \r\n                        LJ               SLSQP         min_obj       -5.999992452851249   \r\n                        LJ               SLSQP         mean_obj     -1.9506977348879857   \r\n                        LJ             Newton-CG      mean_nfev             n\/a           \r\n                        LJ             Newton-CG      mean_time             n\/a           \r\n                        LJ             Newton-CG       max_obj              n\/a           \r\n                        LJ             Newton-CG       min_obj              n\/a           \r\n                        LJ             Newton-CG       mean_obj             n\/a           \r\n                        LJ               dogleg       mean_nfev             n\/a           \r\n                        LJ               dogleg       mean_time             n\/a           \r\n                        LJ               dogleg        max_obj              n\/a           \r\n                        LJ               dogleg        min_obj              n\/a           \r\n                        LJ               dogleg        mean_obj             n\/a           \r\n                        LJ             trust-ncg      mean_nfev             n\/a           \r\n                        LJ             trust-ncg      mean_time             n\/a           \r\n                        LJ             trust-ncg       max_obj              n\/a           \r\n                        LJ             trust-ncg       min_obj              n\/a           \r\n                        LJ             trust-ncg       mean_obj             n\/a           \r\n                        LJ            trust-exact     mean_nfev             n\/a           \r\n                        LJ            trust-exact     mean_time             n\/a           \r\n                        LJ            trust-exact      max_obj              n\/a           \r\n                        LJ            trust-exact      min_obj              n\/a           \r\n                        LJ            trust-exact      mean_obj             n\/a           \r\n                        LJ            trust-krylov    mean_nfev             n\/a           \r\n                        LJ            trust-krylov    mean_time             n\/a           \r\n                        LJ            trust-krylov     max_obj              n\/a           \r\n                        LJ            trust-krylov     min_obj              n\/a           \r\n                        LJ            trust-krylov     mean_obj             n\/a           \r\n                        LJ            trust-constr    mean_nfev           1083.05         \r\n                        LJ            trust-constr    mean_time      0.328438925743103    \r\n                        LJ            trust-constr     max_obj      -0.33285074468148407  \r\n                        LJ            trust-constr     min_obj      -5.9999999999820615   \r\n                        LJ            trust-constr     mean_obj      -4.104444644769407   \r\n              ====================== ============== ============= ========================\r\n```\r\n\r\n<\/details>\r\n","@ragonneau I suggest you edit your results. They are difficult to read. In addition, there seems no reason to use a table different from the other one. Thanks. ","> @ragonneau I suggest you edit your results. They are difficult to read. In addition, there seems no reason to use a table different from the other one. Thanks.\r\n\r\nI believe the layout of the table was made by `asv`. When the last column has more than two subcolumns, it creates a column that is difficult to read, as in my previous comment. If an `asv` expert is in the thread, I'd be happy to learn how to improve that.\r\n\r\nIn the meantime, I reran the experiment with only `mean_nfev` and `mean_obj`:\r\n\r\n<details>\r\n\r\n```\r\n[100.00%] \u00b7\u00b7\u00b7 ====================== ============== ==================== ========================\r\n              --                                                     result type                 \r\n              ------------------------------------- ---------------------------------------------\r\n                  test function          solver          mean_nfev               mean_obj        \r\n              ====================== ============== ==================== ========================\r\n                 rosenbrock_slow         COBYLA            1000.0           1.040669274800954    \r\n                 rosenbrock_slow         COBYQA            152.8          3.890437153553753e-08  \r\n                 rosenbrock_slow         Powell            887.2          1.577792702290088e-23  \r\n                 rosenbrock_slow      nelder-mead          310.2          1.948083288418953e-09  \r\n                 rosenbrock_slow        L-BFGS-B           148.0          2.395468489328531e-06  \r\n                 rosenbrock_slow          BFGS             177.2          4.675037804518409e-11  \r\n                 rosenbrock_slow           CG               n\/a                    n\/a           \r\n                 rosenbrock_slow          TNC               n\/a                    n\/a           \r\n                 rosenbrock_slow         SLSQP             155.1          1.0577238854081483e-05 \r\n                 rosenbrock_slow       Newton-CG            n\/a                    n\/a           \r\n                 rosenbrock_slow         dogleg             n\/a                    n\/a           \r\n                 rosenbrock_slow       trust-ncg            n\/a                    n\/a           \r\n                 rosenbrock_slow      trust-exact           n\/a                    n\/a           \r\n                 rosenbrock_slow      trust-krylov          n\/a                    n\/a           \r\n                 rosenbrock_slow      trust-constr         218.0           9.51616929722639e-10  \r\n                rosenbrock_nograd        COBYLA            1000.0           1.040669274800954    \r\n                rosenbrock_nograd        COBYQA            152.8          3.890437153553753e-08  \r\n                rosenbrock_nograd        Powell            887.2          1.577792702290088e-23  \r\n                rosenbrock_nograd     nelder-mead          310.2          1.948083288418953e-09  \r\n                rosenbrock_nograd       L-BFGS-B           148.0          2.395468489328531e-06  \r\n                rosenbrock_nograd         BFGS             177.2          4.675037804518409e-11  \r\n                rosenbrock_nograd          CG               n\/a                    n\/a           \r\n                rosenbrock_nograd         TNC               n\/a                    n\/a           \r\n                rosenbrock_nograd        SLSQP             155.1          1.0577238854081483e-05 \r\n                rosenbrock_nograd      Newton-CG            n\/a                    n\/a           \r\n                rosenbrock_nograd        dogleg             n\/a                    n\/a           \r\n                rosenbrock_nograd      trust-ncg            n\/a                    n\/a           \r\n                rosenbrock_nograd     trust-exact           n\/a                    n\/a           \r\n                rosenbrock_nograd     trust-krylov          n\/a                    n\/a           \r\n                rosenbrock_nograd     trust-constr         218.0           9.51616929722639e-10  \r\n                    rosenbrock           COBYLA            1000.0           1.040669274800954    \r\n                    rosenbrock           COBYQA            152.8          3.890437153553753e-08  \r\n                    rosenbrock           Powell            887.2          1.577792702290088e-23  \r\n                    rosenbrock        nelder-mead          310.2          1.948083288418953e-09  \r\n                    rosenbrock          L-BFGS-B           92.75          2.1047384326635483e-06 \r\n                    rosenbrock            BFGS             110.95         2.5838983525240928e-11 \r\n                    rosenbrock             CG              110.0          2.184879891252899e-09  \r\n                    rosenbrock            TNC               84.3           0.41353307449601323   \r\n                    rosenbrock           SLSQP             102.3          1.2278847681581272e-05 \r\n                    rosenbrock         Newton-CG            64.5          0.00028386319748745415 \r\n                    rosenbrock           dogleg             18.4          1.0473748283949003e-12 \r\n                    rosenbrock         trust-ncg            43.5          9.096654341078131e-10  \r\n                    rosenbrock        trust-exact           17.5          2.3688425209734387e-11 \r\n                    rosenbrock        trust-krylov          35.7          1.2080822443269492e-09 \r\n                    rosenbrock        trust-constr   98.16666666666667    7.582424644998224e-10  \r\n                 rosenbrock_tight        COBYLA            1000.0           1.0395001250506932   \r\n                 rosenbrock_tight        COBYQA            180.2          1.0833609443328238e-15 \r\n                 rosenbrock_tight        Powell            1070.1         5.649905225233614e-24  \r\n                 rosenbrock_tight     nelder-mead          407.7          1.7872342256810196e-17 \r\n                 rosenbrock_tight       L-BFGS-B            99.2          2.4807079357316176e-10 \r\n                 rosenbrock_tight         BFGS             160.9          1.6841960135232725e-11 \r\n                 rosenbrock_tight          CG              127.8          5.959714896391864e-19  \r\n                 rosenbrock_tight         TNC               90.4           0.41043093540666387   \r\n                 rosenbrock_tight        SLSQP             108.15         5.646132513964207e-10  \r\n                 rosenbrock_tight      Newton-CG            71.6          8.645626438121326e-19  \r\n                 rosenbrock_tight        dogleg             19.2          7.554726540980743e-22  \r\n                 rosenbrock_tight      trust-ncg            44.9          6.432714849941195e-24  \r\n                 rosenbrock_tight     trust-exact           18.4          9.805261192705719e-20  \r\n                 rosenbrock_tight     trust-krylov          93.7          1.227821735143473e-17  \r\n                 rosenbrock_tight     trust-constr   106.53333333333333   1.2124258427696534e-11 \r\n                 simple_quadratic        COBYLA             54.8          9.518126940933508e-09  \r\n                 simple_quadratic        COBYQA             22.2          8.872232632322827e-31  \r\n                 simple_quadratic        Powell             47.2          1.2855981106468597e-31 \r\n                 simple_quadratic     nelder-mead          152.8          2.2619242867800513e-09 \r\n                 simple_quadratic       L-BFGS-B            7.5           2.4856928645318576e-16 \r\n                 simple_quadratic         BFGS              7.5           1.6821868699593401e-15 \r\n                 simple_quadratic          CG               3.6           3.154879136346819e-10  \r\n                 simple_quadratic         TNC               7.0           2.0304535898501623e-07 \r\n                 simple_quadratic        SLSQP              6.0           1.336048763662199e-16  \r\n                 simple_quadratic      Newton-CG            2.0                    0.0           \r\n                 simple_quadratic        dogleg             3.0            2.78107340254829e-32  \r\n                 simple_quadratic      trust-ncg            3.0                    0.0           \r\n                 simple_quadratic     trust-exact           3.0            2.76371887415049e-32  \r\n                 simple_quadratic     trust-krylov          3.0                    0.0           \r\n                 simple_quadratic     trust-constr          6.0           9.386350782614601e-17  \r\n               asymmetric_quadratic      COBYLA             64.2           -0.24999998850473912  \r\n               asymmetric_quadratic      COBYQA             23.5                  -0.25          \r\n               asymmetric_quadratic      Powell            134.6                  -0.25          \r\n               asymmetric_quadratic   nelder-mead          162.9           -0.24999999810601786  \r\n               asymmetric_quadratic     L-BFGS-B            7.5            -0.24999999999999883  \r\n               asymmetric_quadratic       BFGS              7.5            -0.24999999999999784  \r\n               asymmetric_quadratic        CG               3.1                   -0.25          \r\n               asymmetric_quadratic       TNC               12.5           -0.24999858212319187  \r\n               asymmetric_quadratic      SLSQP              6.0            -0.24999999999999972  \r\n               asymmetric_quadratic    Newton-CG            2.0                   -0.25          \r\n               asymmetric_quadratic      dogleg             2.9                   -0.25          \r\n               asymmetric_quadratic    trust-ncg            2.9                   -0.25          \r\n               asymmetric_quadratic   trust-exact           2.9                   -0.25          \r\n               asymmetric_quadratic   trust-krylov          2.9                   -0.25          \r\n               asymmetric_quadratic   trust-constr   5.966666666666667     -0.24999999999999986  \r\n                      sin_1d             COBYLA             24.7           -0.9999999947997484   \r\n                      sin_1d             COBYQA             13.7           -0.9999999999766593   \r\n                      sin_1d             Powell             28.1           -0.9999999999215394   \r\n                      sin_1d          nelder-mead           36.8           -0.9999999998495948   \r\n                      sin_1d            L-BFGS-B            9.45           -0.9999999936365207   \r\n                      sin_1d              BFGS              10.2           -0.9999999996362824   \r\n                      sin_1d               CG               5.5            -0.9999999999451038   \r\n                      sin_1d              TNC               10.2           -0.9999999999816668   \r\n                      sin_1d             SLSQP              6.95           -0.9999874938865606   \r\n                      sin_1d           Newton-CG            n\/a                    n\/a           \r\n                      sin_1d             dogleg             n\/a                    n\/a           \r\n                      sin_1d           trust-ncg            n\/a                    n\/a           \r\n                      sin_1d          trust-exact           n\/a                    n\/a           \r\n                      sin_1d          trust-krylov          n\/a                    n\/a           \r\n                      sin_1d          trust-constr          9.6            -0.9999999995302697   \r\n                      booth              COBYLA             67.1          9.727602527729466e-08  \r\n                      booth              COBYQA             30.9          1.1072140768523674e-09 \r\n                      booth              Powell             63.4          2.1062586169401015e-29 \r\n                      booth           nelder-mead          122.9            1.219380568865664    \r\n                      booth             L-BFGS-B            11.6          2.1341422667527374e-08 \r\n                      booth               BFGS              15.4          1.4028567484760426e-10 \r\n                      booth                CG               6.4           8.662008283688798e-26  \r\n                      booth               TNC               6.8           4.8330558571088694e-15 \r\n                      booth              SLSQP              12.3          8.848768281252701e-08  \r\n                      booth            Newton-CG            n\/a                    n\/a           \r\n                      booth              dogleg             n\/a                    n\/a           \r\n                      booth            trust-ncg            n\/a                    n\/a           \r\n                      booth           trust-exact           n\/a                    n\/a           \r\n                      booth           trust-krylov          n\/a                    n\/a           \r\n                      booth           trust-constr          14.8          5.502755360622469e-11  \r\n                      beale              COBYLA            461.1            1.0645878833664317   \r\n                      beale              COBYQA            206.9            1.2864101661603227   \r\n                      beale              Powell            1821.9          0.41411155100942254   \r\n                      beale           nelder-mead          141.1          3.970084546666909e-10  \r\n                      beale             L-BFGS-B            60.7            1.1948511188692428   \r\n                      beale               BFGS             311.6           0.13604038140884195   \r\n                      beale                CG               32.9            1.5004900363633769   \r\n                      beale               TNC               24.1           0.01830610047951784   \r\n                      beale              SLSQP              25.9            1.5104357514520017   \r\n                      beale            Newton-CG            n\/a                    n\/a           \r\n                      beale              dogleg             n\/a                    n\/a           \r\n                      beale            trust-ncg            n\/a                    n\/a           \r\n                      beale           trust-exact           n\/a                    n\/a           \r\n                      beale           trust-krylov          n\/a                    n\/a           \r\n                      beale           trust-constr         476.35           1.4994513515047054   \r\n                        LJ               COBYLA            868.8            -4.290638261230069   \r\n                        LJ               COBYQA            696.3            -5.999999907301164   \r\n                        LJ               Powell            1900.0          -5.7224243446285765   \r\n                        LJ            nelder-mead          2281.8          -3.8851133797531867   \r\n                        LJ              L-BFGS-B           278.55          -3.2094944991218455   \r\n                        LJ                BFGS             722.75           -5.999999999973245   \r\n                        LJ                 CG              146.9            -5.999999999936375   \r\n                        LJ                TNC              115.6            -5.487077119375702   \r\n                        LJ               SLSQP             208.5           -1.9506977348879857   \r\n                        LJ             Newton-CG            n\/a                    n\/a           \r\n                        LJ               dogleg             n\/a                    n\/a           \r\n                        LJ             trust-ncg            n\/a                    n\/a           \r\n                        LJ            trust-exact           n\/a                    n\/a           \r\n                        LJ            trust-krylov          n\/a                    n\/a           \r\n                        LJ            trust-constr        1083.05           -4.104444644769407   \r\n              ====================== ============== ==================== ========================\r\n```\r\n\r\n<\/details>","Based on the benchmarks, I would consider COBYQA a great improvement over COBYLA and for our derivative-free solvers in general. Also it should be noted that currently COBYLA is our only derivative-free local solver that works with constraints. As it is implemented in Python, COBYQA should not cause extraordinary maintenance burden as well. On how to proceed: as the current trend seems to be git submodules, l would advocate for that here as well.\r\n\r\nBut let's wait for a few more opinions :).","> Based on the benchmarks, I would consider COBYQA a great improvement over COBYLA and for our derivative-free solvers in general. Also it should be noted that currently COBYLA is our only derivative-free local solver that works with constraints. As it is implemented in Python, COBYQA should not cause extraordinary maintenance burden as well. On how to proceed: as the current trend seems to be git submodules, l would advocate for that here as well.\r\n> \r\n> \r\n> \r\n> But let's wait for a few more opinions :).\r\n\r\nFor the moment, we use `setuptools` to create the package. Would it be helpful if we switch to `meson`?","> For the moment, we use `setuptools` to create the package. Would it be helpful if we switch to `meson`?\r\n\r\nIt's pure Python, so it really doesn't matter.\r\n\r\n> as the current trend seems to be git submodules, l would advocate for that here as well.\r\n\r\nIt depends a bit on plans I'd say. E.g., is the standalone package expected to continue to be developed for years into the future? Or once it's stable in SciPy, that will become the main implementation?","> It depends a bit on plans I'd say. E.g., is the standalone package expected to continue to be developed for years into the future? Or once it's stable in SciPy, that will become the main implementation?\r\n\r\nThe goal is to maintain `cobyqa` as a standalone package and to continue to develop it in the future, possibly to add new features, etc. I believe this implies adding it as a submodule in `scipy\/_lib`? I tried on my side; my `scipy\/optimize\/_cobyqa_py.py` looks like this:\r\n```python\r\nimport importlib.util\r\nimport sys\r\n\r\nimport numpy as np\r\n\r\nfrom ._optimize import _check_unknown_options\r\n\r\n__all__ = []\r\n\r\n\r\ndef _minimize_cobyqa(fun, x0, args=(), bounds=None, constraints=(),\r\n                     callback=None, disp=False, maxfev=1000, maxiter=1000,\r\n                     target=-np.inf, feasibility_tol=1e-8, radius_init=1.0,\r\n                     radius_final=1e-6, **unknown_options):\r\n    \"\"\"\r\n    Minimize a scalar function of one or more variables using the\r\n    Constrained Optimization BY Quadratic Approximations (COBYQA) algorithm.\r\n\r\n    Options\r\n    -------\r\n    disp : bool\r\n        Set to True to print information about the optimization procedure.\r\n    maxfev : int\r\n        Maximum number of function evaluations.\r\n    maxiter : int\r\n        Maximum number of iterations.\r\n    target : float\r\n        Target value for the objective function. The optimization procedure is\r\n        terminated when the objective function value of a nearly feasible point\r\n        is less than or equal to this target.\r\n    feasibility_tol : float\r\n        Tolerance for the constraint violation.\r\n    radius_init : float\r\n        Initial trust-region radius. Typically, this value should be in the\r\n        order of one tenth of the greatest expected change to the variables.\r\n    radius_final : float\r\n        Final trust-region radius. It should indicate the accuracy required in\r\n        the final values of the variables.\r\n    \"\"\"\r\n    # Import the cobyqa module.\r\n    cobyqa_spec = importlib.util.spec_from_file_location(\r\n        'cobyqa', '..\/_lib\/cobyqa\/cobyqa\/__init__.py')\r\n    cobyqa_module = importlib.util.module_from_spec(cobyqa_spec)\r\n    sys.modules['cobyqa'] = cobyqa_module\r\n    cobyqa_spec.loader.exec_module(cobyqa_module)\r\n\r\n    # Run the COBYQA method.\r\n    _check_unknown_options(unknown_options)\r\n    options = {\r\n        'disp': bool(disp),\r\n        'maxfev': int(maxfev),\r\n        'maxiter': int(maxiter),\r\n        'target': float(target),\r\n        'feasibility_tol': float(feasibility_tol),\r\n        'radius_init': float(radius_init),\r\n        'radius_final': float(radius_final),\r\n    }\r\n    return cobyqa_module.minimize(fun, x0, args, bounds, constraints, callback, options)\r\n```\r\n\r\nThe problem: I am unsure how to modify `meson.build` so that the Python files in `scipy\/_lib\/cobyqa\/cobyqa` are included. There should be a cleaner way.","I think we should try to go to some significant lengths to avoid poking around with `importlib` and `sys.modules` here. AFAICT, there are only a couple of fully-qualified absolute `from cobyqa.utils import ...` statements in your codebase. They could be written to be relative (`from ..utils import ...`), though the double `..` is often considered poor style. But then it wouldn't matter (as far as I can tell) whether its being imported as a top-level `cobyqa` or `scipy._lib.cobyqa`. If you really want to do the single-source-of-truth, `git submodule` route, I think that's the kind of compromise you'd have to make.","+1 to what Robert said on using relative imports and avoiding `importlib`. That's not even much of a compromise I'd say; relative imports within a package are idiomatic.\r\n\r\n> The problem: I am unsure how to modify `meson.build` so that the Python files in `scipy\/_lib\/cobyqa\/cobyqa` are included.\r\n\r\nI think it only requires something like this at the bottom of `scipy\/_lib\/meson.build`:\r\n```meson\r\npy3.install_sources(\r\n  [\r\n    'cobyqa\/__init__.py',\r\n    'cobyqa\/cobyqa.py',\r\n  ],\r\n  subdir: 'scipy\/_lib\/cobyqa',\r\n)\r\n```\r\nIf there's some kind of hiccup, then I'm happy to help on a PR or branch, please feel free to ping me.","> +1 to what Robert said on using relative imports and avoiding `importlib`. That's not even much of a compromise I'd say; relative imports within a package are idiomatic.\r\n\r\nI already pushed an update to the COBYQA repo, removing all the absolute imports.\r\n\r\n> I think it only requires something like this at the bottom of `scipy\/_lib\/meson.build`:\r\n> \r\n> ```meson\r\n> py3.install_sources(\r\n>   [\r\n>     'cobyqa\/__init__.py',\r\n>     'cobyqa\/cobyqa.py',\r\n>   ],\r\n>   subdir: 'scipy\/_lib\/cobyqa',\r\n> )\r\n> ```\r\n> \r\n> If there's some kind of hiccup, then I'm happy to help on a PR or branch, please feel free to ping me.\r\n\r\nGreat! I will work next week on a PR, I will contact you if I face any trouble by then. Thanks \ud83d\udc4d","I just created a [PR](https:\/\/github.com\/scipy\/scipy\/pull\/19994). I would be happy to have some feedback and improve it if necessary.","As a side note, on my challenging noisy function COBYQA showed systematically better convergence to global minimum vs COBYLA and all other optimization functions available in scipy. Looking forward to see it merged in!","> As a side note, on my challenging noisy function COBYQA showed systematically better convergence to global minimum vs COBYLA and all other optimization functions available in scipy. Looking forward to see it merged in!\n\nThat's great to hear! I'm delighted this work is useful!"],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: Specific logger for scipy.optimize.shgo","body":"#### Using the `logging` library\r\n\r\nI would like to commend the use of the `logging` library to log messages in SHGO. It seems to me a very good practice which allows the user much flexibilty and useful insight for development and debugging. I have implemented a few changes which enhance the flexibilty on the user's end.\r\n\r\n\r\n#### Specific SHGO logger\r\n\r\nSet up the `scipy.optimize.shgo` solver to log messages to a logger named \"scipy.optimize.shgo\" instead of the \"root\" logger. This provides the end user more flexibility in setting up a logging system in their application. When logging to the \"root\" logger, any changes to the logging level will affect all other loggers in the system as they all inherit settings from \"root\". Thus the loggers from different libraries, as well as the user's own code, may conflict.\r\n\r\nPreviously, all the log statements were hidden behind conditional statements using the 'disp' option passed as argument to the `shgo` function. Thus to see the log statements the user had to both set this option to `True` as well as set the logging level of the \"root\" logger. I have removed this conditional such that setting the logging level of the \"scipy.optimize.shgo\" logger is sufficient to configure the log messages. I have edited the docstring to illustrate the use.\r\nRemoving these conditional statements incur a performance cost which is remedied by a change in the formatting of the log messages.\r\n\r\n\r\n#### Formatting of log messages\r\n\r\nChange the log messages to use lazy formatting rather than f-strings. With the new formatting log messages will not be created unless they are actually going to be logged. This gives a performance improvement for messages that ultimately are not logged. (Running a simple test of the time required to log the elements of a NumPy array with five elements showed that formatting with f-strings takes 45-50 times as long as lazy formatting on my desktop.)\r\n\r\nThe combined changes in this PR result in practically indistinguishable execution times on my desktop for the benchmarking and test suites (the change is within 1% for the benchmarking suite; I see greater variation between individual runs caused by other processes on the computer).\r\n\r\n\r\n#### Logging in SciPy\r\n\r\nThis system of logging could well be extended to the rest of the Python code in SciPy, but I could not find discussions of this elsewhere. The loggers are arranged in a namespace such that \"scipy.optimize.shgo\" is the child of \"scipy.optimize\" which is the child of \"scipy\". Configurations made for a logger automatically affect its children. Thus loggers for different parts of the code base may be set up in the \"scipy\" namespace and be configured all together, in groups, or individually depending on the user's desires.","comments":["Thanks @VegardFalmaar this is indeed something we really needed. I have been doing a similar thing for SLSQP. So this is good timing. \r\n\r\nOne detail that we have to be really careful is how to collate the logs to the same logging mechanism if the users would like to provide their own. Often times this logging works great by itself but say I'm on a remote device and I also have other logs generating stuff and I want to use the same logger then it is an unnecessary pain to find all the relevant loggers and deal with them individually. \r\n\r\nStill having proper logging is much better than just `print`ing but while we are at it we should not paint ourselves into another corner. ","Thank you for the quick reply, @ilayn!\r\n\r\nI am not quite sure I understand what you mean by your last two paragraphs, so please do correct me if I am mistaken in the following. One always has the opportunity to use the same logging mechanism for all loggers by configuring the \"root\" logger since it is the parent of all other loggers. If a user configures the \"root\" logger to use a specific handler then logs from all loggers would be written to this. Alternatively they may cherry pick certain loggers if they need to, and add the same handler to them to collate their messages.\r\n\r\nIn any case, I am planning to continue working on the SHGO solver. Is there anything regarding log statements that I should do differently as I move forward?","> One always has the opportunity to use the same logging mechanism for all loggers by configuring the \"root\" logger since it is the parent of all other loggers.\r\n\r\nThe common practice in the industry is to not use root logger. Because you don't know what other loggers are running in other jobs hence you want to provide the algorithm your logger to say \"here is my logger use this one\", or if not use a standard one and not create a specific one because then you need to intercept this logger if running under other packages and becomes a big mess.","> The common practice in the industry is to not use root logger.\r\n\r\nHe's suggesting the common practice, which is to write your library modules such that they each log to a specific, named `Logger`, but in your `main()`, configure the logging on the anonymous root logger. By default, that configuration on the root logger will cascade down to all of the named `Logger`s (unless if specifically overridden).","Good idea @VegardFalmaar. In general it would be helpful to uniformize logging in the module. As its logs, I suppose there is not much harm experimenting with a given function first and then bringing the solution to other functions.\n\nOne thing to consider as well is the level of various messages. I suppose we could move most logs to debug.\n","> By default, that configuration on the root logger will cascade down to all of the named Loggers (unless if specifically overridden).\r\n\r\nThis is what you do only if you are in control of the script that you are running and if you are running scipy.optimize code yourself. But anyways, no need to turn this into a logger discussion. I don't have the capacity to continue it further. We can always modify things if they turn out to be problematic. Having loggers instead of printing things is the nice thing already. \r\n","Thank you for the feedback @tupui! I have moved almost all logging statements to debug in the latest commit. I have also tried to clean up the logs a little bit, e.g. by removing some duplicate logging statements.","Hi @VegardFalmaar , Mypy isn't too happy, here's an excerpt from the CI log:\r\n\r\n```\r\n\ud83d\udcbb  mypy.api.run --config-file \/home\/runner\/work\/scipy\/scipy\/mypy.ini scipy\r\nscipy\/optimize\/_shgo.py:665: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:666: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:667: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:669: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:670: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:793: note: By default the bodies of untyped functions are not checked, consider using --check-untyped-defs  [annotation-unchecked]\r\nscipy\/optimize\/_shgo.py:1096: error: Incompatible types in assignment (expression has type \"ndarray[Any, Any]\", variable has type \"List[ndarray[Any, dtype[signedinteger[_64Bit]]]]\")  [assignment]\r\nscipy\/optimize\/_shgo.py:1098: error: Too many arguments for \"tuple\"  [call-arg]\r\nscipy\/optimize\/_shgo.py:1099: error: Need type annotation for \"points\" (hint: \"points: Dict[<type>, <type>] = ...\")  [var-annotated]\r\nscipy\/optimize\/_shgo.py:1106: error: \"Tuple[Any, ...]\" has no attribute \"points\"  [attr-defined]\r\nscipy\/optimize\/_shgo.py:1106: error: \"Tuple[Any, ...]\" has no attribute \"simplices\"  [attr-defined]\r\nscipy\/optimize\/_shgo.py:1158: error: Incompatible types in assignment (expression has type \"ndarray[Any, dtype[Any]]\", variable has type \"List[Any]\")  [assignment]\r\nscipy\/optimize\/_shgo.py:1159: error: Incompatible types in assignment (expression has type \"ndarray[Any, dtype[Any]]\", variable has type \"List[Any]\")  [assignment]\r\nscipy\/optimize\/_shgo.py:1231: error: Incompatible types in assignment (expression has type \"ndarray[Any, dtype[Any]]\", variable has type \"List[Any]\")  [assignment]\r\n```\r\n\r\nMaybe try pushing a fix for these errors (you can check locally with `python dev.py mypy`) then we can see how the rest of the CI checks are looking."],"labels":["enhancement","scipy.optimize"]},{"title":"BUG: Performance Degradation in Creating CSR Matrices with Scalar Multiplication","body":"### Describe your issue.\n\nI have encountered a performance issue related to creating Compressed Sparse Row (CSR) matrices in SciPy when performing scalar multiplication. Specifically, when using the `sp.csr_matrix` constructor with scalar multiplication, there appears to be a significant slowdown compared to using the same constructor with a single-element array multiplication.\r\nThere is a notable performance slowdown when creating CSR matrices with scalar multiplication, which is unexpected.\r\nI believe this performance issue may impact users who work with large datasets and require efficient CSR matrix creation. Your attention to this matter would be greatly appreciated.\r\n\r\n\n\n### Reproducing Code Example\n\n```python\nimport timeit\r\n\r\n# Create test data\r\ndense_array = np.random.rand(1000, 1000)\r\nscalar = 10\r\nsingle_element_array = np.array([10])\r\n\r\n# Define functions to create sparse matrices\r\ndef create_sparse_matrix_scalar():\r\n    return sp.csr_matrix(dense_array * scalar)\r\n\r\ndef create_sparse_matrix_array():\r\n    return sp.csr_matrix(dense_array * single_element_array)\r\n\r\n# Test the execution time of the csr_matrix constructor\r\ntime_scalar = timeit.timeit(create_sparse_matrix_scalar, number=100)\r\ntime_array = timeit.timeit(create_sparse_matrix_array, number=100)\r\n\r\n# Print the results\r\nprint(f\"Time for creating csr_matrix with scalar multiplication: {time_scalar} seconds\")\r\nprint(f\"Time for creating csr_matrix with single element array multiplication: {time_array} seconds\")\r\n```\n```\n\n\n### Error message\n\n```shell\nTime for creating csr_matrix with scalar multiplication: 13.837873875163496 seconds\r\nTime for creating csr_matrix with single element array multiplication: 1.8720805700868368 seconds\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.3 sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/include\r\n    lib directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/include\r\n    lib directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/home\/yhw\/anaconda3\/envs\/LLMFP\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/home\/yhw\/anaconda3\/envs\/LLMFP\/bin\/python\r\n  version: '3.9'\n```\n","comments":["Hi @Alexia-I , just to check, did you close this because you don't think there's a problem anymore?","Thank you for your response. The test results showed varying performance differences, which is very strange, sometimes having much diff on time, sometimes not. I was wondering if anyone else has been able to reproduce the performance difference?","It looks like your timing functions do two things:\r\n1) multiply a dense array times a scalar-like using numpy. Notice that there is no scipy-sparse involved in this portion of the function.\r\n2) convert the resulting dense matrix to sparse using `csr_matrix`.\r\n\r\nIn both functions you are creating a 1000x1000 dense array and then converting it to sparse csr format. So I would not expect much difference due to the call to `csr_array`. The difference is more likely to be due to the multiplication which is happening in numpy.\r\n\r\nAs far as the variation in timing results, that is likely due to your computer being used for \"other tasks\" while the timing is going on. To avoid other processes taking time, you can remove background tasks, turn off network access while timing, or repeat the timing multiple times and take the minimum of the result.  There are probably other approaches as well. \r\n\r\nBut I suspect any difference in time to be due to the difference in time of the multiplication rather than the difference of converting a dense array to sparse (since that is the same operation for both cases."],"labels":["defect","scipy.sparse"]},{"title":"ENH: add method to rv_generic to return Fisher information matrix","body":"### Is your feature request related to a problem? Please describe.\n\nAfter fitting a distribution to data by MLE, I would like to know the (asymptotic) variance of the estimator to get a sense of its accuracy. This is given by the inverse of the Fisher (observed) information matrix. I don\u2019t find how to do that in scipy.\n\n### Describe the solution you'd like.\n\nAdd a `fisher` method to rv_generic to return the Fisher information or observed information matrix.\r\n\r\nThe signature could be the following:\r\n\r\n`def fisher(self, theta, data=None)` returns the Fisher information matrix evaluated at `theta` if data is None, or the observed information matrix if data is not None.\r\n\r\nThe generic implementation could do a numerical approximation of the Hessian for observed information, and raise an exception if data is None.\n\n### Describe alternatives you've considered.\n\nFor my particular task at hand, I approximated the Hessian by numerical difference. It\u2019s also possible to code the analytical formula for the distribution of interest if an analytical formula exists.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Adding Fisher information to the current distribution infrastructure would be hard to do. But I think it could be incorporated into the new one which is currently in the making (#15928)","This is part of why I wrote [`_differentiate`](https:\/\/github.com\/scipy\/scipy\/pull\/18811). It needs to be extended substantially (multiple dimensions and higher integrals), but the ball is rolling.","> This is part of why I wrote [`_differentiate`](https:\/\/github.com\/scipy\/scipy\/pull\/18811). It needs to be extended substantially (multiple dimensions and higher integrals), but the ball is rolling.\r\n\r\nNice stuff. Seems non-trivial to extend `_differentiate` to support second (partial) derivatives?","Well, trivial to extend it by numerically differentiating the numerical derivative, but non-trivial to do the right thing. But I familiarized myself with the theory and how to make it adaptive. So at some point this will happen. As @dschmitz89 mentioned, I'm knee deep in distribution infrastructure, and I'd like to create public interfaces for `_differentiate`, `_tanhsinh`, `_nsum`, `_chandrupatla`, `_chandrupatla_minimize`, `_bracket_root`, and `_bracket_minimize`, too, so it will take a little bit for this to make it to the top of the queue. But TBH this sort of thing is rarely held up by technical difficulties or lack of time to write the code but lack of reviewer bandwidth\/interest. See, e.g. gh-19475 and gh-19561 (also very useful but not much traffic). It's fine; it just takes time.\r\nIn the meantime, there is [numdifftools](https:\/\/pypi.org\/project\/numdifftools\/)."],"labels":["scipy.stats","enhancement"]},{"title":"BUG: open_memstream unavailable with glibc >= 2.10 + C99","body":"### Describe your issue.\n\nThe `open_memstream` function is not detected on systems with glibc >= 2.10.  The top-level meson.build file sets the C standard to C99, but as the `open_memstream` man page says, the function is available only if `_POSIX_C_SOURCE >= 200809L`.  Thanks to Florian Weimer for diagnosing the issue.\r\n\n\n### Reproducing Code Example\n\n```python\nN\/A\n```\n\n\n### Error message\n\n```shell\nChecking for function \"open_memstream\" : NO\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.2 sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/include\/flexiblas\r\n    lib directory: \/usr\/lib64\r\n    name: flexiblas\r\n    openblas configuration: unknown\r\n    pc file directory: \/usr\/lib64\/pkgconfig\r\n    version: 3.4.1\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/include\/flexiblas\r\n    lib directory: \/usr\/lib64\r\n    name: flexiblas\r\n    openblas configuration: unknown\r\n    pc file directory: \/usr\/lib64\/pkgconfig\r\n    version: 3.4.1\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/usr\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: gcc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 14.0.1\r\n  c++:\r\n    commands: g++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 14.0.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.6\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 14.0.1\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/..\/usr\/lib\/python3.12\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/usr\/bin\/python3\r\n  version: '3.12'\n```\n","comments":["Sounds like we could\/should just set `_GNU_SOURCE`? C.f. https:\/\/github.com\/mesonbuild\/meson\/issues\/2108 resp. [docs](https:\/\/www.man7.org\/linux\/man-pages\/man7\/feature_test_macros.7.html) & [implementation](https:\/\/github.com\/bminor\/glibc\/blob\/glibc-2.38\/include\/features.h#L55).\r\n\r\nIt [seems](https:\/\/stackoverflow.com\/questions\/50167494\/how-to-know-to-which-value-i-should-define-posix-c-source) that this is automatically used when setting `gnu99` instead of `c99`, but I wasn't able to verify this through godbolt just now.","`_DEFAULT_SOURCE` is the more conservative option. It does not alter the behavior of `strerror` and some other functions the way that `_GNU_SOURCE` does.","Thanks for the report. The more conservative define, and only defining it in the target that needs it rather than project-wide, seems like the way to go here. Otherwise it's too easy for other non-portable function\/macro usage to slip in."],"labels":["defect","Build issues"]},{"title":"DEP: extend deprecation of private namespaces also to fortran-generated modules","body":"While looking at cleaning up the deprecations following #18279 (fitting into the larger picture of #14360), I noticed that `scipy.interpolate.dfitpack` is not deprecated, which seems like an oversight - in particular because it contains several functions that `scipy.interpolate` itself contains:\r\n```python\r\n>>> import scipy.interpolate\r\n>>> import scipy.interpolate.dfitpack\r\n>>> dfitpack_funcs = set(scipy.interpolate.dfitpack.__dir__())\r\n>>> interpolate_funcs = set(scipy.interpolate.__dir__())\r\n>>> sorted([x for x in dfitpack_funcs & interpolate_funcs if not x.startswith(\"_\")])\r\n['spalde', 'splder', 'splev', 'splint', 'sproot']\r\n```\r\n\r\nBut that's only half the story, because we have a new interface to fitpack...\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/52732578a67e1f243b466e0668d99f6ec85d0d08\/scipy\/interpolate\/__init__.py#L170-L171\r\nand funnily enough that [file](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/_fitpack2.py) wraps _both_ `dfitpack` directly, as well as a separate wrapper [layer](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/_fitpack_impl.py), also around `dfitpack` (sidenote: looks like this [tweet](https:\/\/twitter.com\/sc_codeUM\/status\/1747684397745049964) I saw today turned out to be prophetic \ud83d\ude05 ). Suffice it to say, despite different names, a lot (all?) of the remaining functions in `dfitpack` then also get accounted for.\r\n\r\nIt's looks likely that this was overlooked because there's no actual python code, we just directly generate the functions from fortran:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/52732578a67e1f243b466e0668d99f6ec85d0d08\/scipy\/interpolate\/meson.build#L145-L162\r\n\r\nNot sure how many other such should-be-private-but-has-no-underscore fortran libraries are about.\r\n\r\nCC @rgommers @tupui @j-bowhay @ev-br ","comments":["The story here is a bit different.\r\n\r\nTL;DR: 1) `scipy.interpolate.dfitpack` is an implementation detail, not meant to be public; 2) any \"new\" comment is likely obsolete, can be removed if wanted; 3) cleanup is ongoing, help appreciated, the work is way beyond just deprecating accidentally public names.\r\n\r\nLong version. Let's start from the source files:\r\n\r\n```\r\n$ ll scipy\/interpolate\r\n<edited for clarity>\r\n\r\ndrwxrwxr-x  2 br br  4096 \u044f\u043d\u0432 17 15:20 fitpack\/    # FORTRAN sources\r\n-rw-rw-r--  1 br br 89172 \u044f\u043d\u0432  7 15:24 _fitpack2.py    # implementation details of `UnivariateSpline` etc\r\n-rw-rw-r--  1 br br   964 \u043d\u043e\u044f 22 19:48 fitpack2.py    # deprecated wrappers for _fitpack2.py\r\n-rw-rw-r--  1 br br 28669 \u044f\u043d\u0432  7 15:24 _fitpack_impl.py    # implementation details of `splrep` etc\r\n-rw-rw-r--  1 br br   716 \u043d\u043e\u044f 22 19:48 fitpack.py    # deprecated wrappers for fitpack.py\r\n-rw-rw-r--  1 br br 27528 \u044f\u043d\u0432 17 15:20 _fitpack_py.py    # implementation details for `splrep` etc, goes with `_fitpack_impl.py`\r\n\r\n(scipy-dev) br@gonzales:~\/repos\/scipy\/scipy$ ll scipy\/interpolate\/src\/\r\n-rw-rw-r-- 1 br br  1696 \u043d\u043e\u044f 21  2022 __fitpack.h               # is unrelated to FORTRAN\r\n-rw-rw-r-- 1 br br 19315 \u044f\u043d\u0432  6 13:11 _fitpackmodule.c          # sources for the `_fitpack.so` extension\r\n-rw-rw-r-- 1 br br 32669 \u043d\u043e\u044f 22 19:48 fitpack.pyf           # sources for the `dfitpack.so` extension\r\n```\r\n\r\nThe fortran sources in `scipy\/interpolate\/fitpack` have two sets of C wrapper to python. The extension modules are `_fitpack.so` and `dfitpack.so`. \r\n\r\nThe comment \"New interface to fitpack library\" is historical (remember book titles like \"modern analysis of ...\"?) is from 2003:\r\n\r\n```\r\n$ git blame -M -C -C scipy\/interpolate\/__init__.py\r\n...\r\n2715c638314 Lib\/interpolate\/__init__.py   (Pearu Peterson  2003-09-15 13:58:12 +0000 171) # New interface to fitpack library:\r\n23ef8d5c336 scipy\/interpolate\/__init__.py (Gagandeep Singh 2021-10-27 15:42:25 +0530 172) from ._fitpack2 import *\r\n...\r\n```\r\n\r\nWhy there are two extensions: historical reasons, I suppose. \r\nAtsushi Sakai did a lot of work deduplicating them recently, see https:\/\/github.com\/scipy\/scipy\/issues\/16729.\r\n\r\nNeither of these extensions needs to be public, anything they export is an implementation detail of either `splrep` et al or `UnivariateSpline` and friends. \r\nSome dfitpack functions share the name but differ from eponymous public functions scipy.interpolate: e.g. `interpolate.splder` does not use `dfitpack.splder`.\r\n\r\nWhether it is worth the hassle to make `dfitpack` raise a DeprecationWarning on import, I don't know. I personally am -0 (== am not going to do it myself; do not believe it is worth doing;  will not oppose if somebody think it's worth and does it).\r\n\r\nA more productive line of work is IMO to cook up a feature-complete alternative to FITPACK with an eye to remove the latter at some point. \r\nThe exact path to removal needs to be worked out. \r\nInitial steps are tracked in https:\/\/github.com\/users\/ev-br\/projects\/1\/views\/1, the currently open scipy PRs are \r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/19753, https:\/\/github.com\/scipy\/scipy\/pull\/19777 and https:\/\/github.com\/scipy\/scipy\/pull\/19873\r\nHelp is welcome and appreciated :-).\r\n\r\n\r\n\r\n \r\n"],"labels":["scipy.interpolate","deprecated"]},{"title":"DEP: remove deprecated imports from privatized modules","body":"This is a follow-up to #18279 resp. https:\/\/github.com\/scipy\/scipy\/commit\/0581177bfd763a2ef74dd952b302ea4dfffb736c where we've generally promised removal of _incidental_ imports from modules we've made private. The modules themselves are set to be removed with SciPy 2.0.\r\n\r\nI'm attempting this in #19904, but wanted to open an issue for discussing any eventual issues.","comments":["@j-bowhay, I was wondering about `approx_jacobian`, which wasn't touched in #18968, but _did_ appear in the `__all__` of the private module implementation:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/730010e77319606718b3d50a8fc402ba9cc629bc\/scipy\/optimize\/_slsqp_py.py#L16\r\n\r\nHowever, it is raising a warning currently, and I'm wondering if this is entirely intentional?\r\n```\r\n>>> from scipy.optimize.slsqp import approx_jacobian\r\n<stdin>:1: DeprecationWarning: `scipy.optimize.slsqp.approx_jacobian` is deprecated along with the `scipy.optimize.slsqp` namespace. `scipy.optimize.slsqp.approx_jacobian` will be removed in SciPy 1.13.0, and the `scipy.optimize.slsqp` namespace will be removed in SciPy 2.0.0.\r\n```\r\n\r\nIf I were to follow through on that warning (see #19904), then `approx_jacobian` would be deleted completely; there are no other [occurrences](https:\/\/github.com\/search?q=repo%3Ascipy%2Fscipy%20approx_jacobian&type=code).\r\n\r\nI ask because outright removal is obviously a different ballpark than privatizing a given module.","Another object that would cease to be \"officially\" importable is `scipy.io.mmio.MMFile`, while the methods for that (`scipy.io.mmio.{mminfo,mmread,mmwrite}`) have \"only\" moved to the top-level `io` module.\r\n```\r\n<stdin>:1: DeprecationWarning: Please import `mminfo` from the `scipy.io` namespace; the `scipy.io.mmio` namespace is deprecated and will be removed in SciPy 2.0.0.\r\n<stdin>:1: DeprecationWarning: Please import `mmread` from the `scipy.io` namespace; the `scipy.io.mmio` namespace is deprecated and will be removed in SciPy 2.0.0.\r\n<stdin>:1: DeprecationWarning: Please import `mmwrite` from the `scipy.io` namespace; the `scipy.io.mmio` namespace is deprecated and will be removed in SciPy 2.0.0.\r\n<stdin>:1: DeprecationWarning: `scipy.io.mmio.MMFile` is deprecated along with the `scipy.io.mmio` namespace. `scipy.io.mmio.MMFile` will be removed in SciPy 1.13.0, and the `scipy.io.mmio` namespace will be removed in SciPy 2.0.0.\r\n```\r\nThis is again in contrast to the `__all__` of the implementing module:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/730010e77319606718b3d50a8fc402ba9cc629bc\/scipy\/io\/_mmio.py#L21","For `approx_jacobian` I will try to dig up the issue but I seem to remember that the conclusion from the optimize regulars was this was fine, it was never meant to be a public function.\r\n\r\nFor the `MMFile` a [search of github](https:\/\/github.com\/search?q=MMFile+language%3APython++NOT+repo%3Ascipy%2Fscipy+NOT+repo%3Awilltryagain%2Fscipy&type=code) it appears there is no real use but it would be nice to get the opinion of an io regular. There is also https:\/\/github.com\/scipy\/scipy\/issues\/19223","> For the `MMFile` a [search of github](https:\/\/github.com\/search?q=MMFile+language%3APython++NOT+repo%3Ascipy%2Fscipy+NOT+repo%3Awilltryagain%2Fscipy&type=code) it appears there is no real use but it would be nice to get the opinion of an io regular.\r\n\r\nJust noticed that we have two identical implementations of `mminfo` etc.:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/io\/_mmio.py\r\nhttps:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy\/io\/_fast_matrix_market\r\n\r\nThe latter even says\r\n> The Python bindings are interchangeable with scipy.io._mmio methods to read and write Matrix Market files.\r\n\r\nI guess since https:\/\/github.com\/scipy\/scipy\/commit\/7e96eff33bb864a6413ea1475b3b1cb486e652b5 (and so on) is fresh in 1.12, we haven't yet had experience with it. However, the `scipy.io` init has already switched to the new implementation\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/52732578a67e1f243b466e0668d99f6ec85d0d08\/scipy\/io\/__init__.py#L105\r\nso assuming no-one screams after 1.12 is released, I'd say we can start deprecating the original implementation. CC @alugowski "],"labels":["deprecated"]},{"title":"MAINT: remove incidental imports from private modules","body":"This has now grown into a full-blown fix for #19905; while we're targetting 1.14 with this, I wanted to check if we're actually warning in all cases (after discovering some potential discrepancies).\r\n\r\n___\r\n_Previously:_\r\n\r\nSome more follow-ups from the work for https:\/\/github.com\/scipy\/scipy\/issues\/18279, same as #19893.\r\n\r\nI've tried & failed to import these on 1.11 resp 1.12; the imports are fully ignored AFAICT due to the presence of `__all__` in those private modules.\r\n\r\nNot sure what the intention there was @j-bowhay, but in any case, this should be good to remove IMO.","comments":["These imports were intentional; they were part of the plan to remove these unnecessary imports (and other pseudo-public attributes) in a backward compatible way. The current behavior is, e.g.:\r\n```python3\r\nfrom scipy.optimize.slsqp import exp\r\n```\r\n\r\nwhich produces the desired warning message.\r\n```\r\nDeprecationWarning: `scipy.optimize.slsqp.exp` is deprecated along with the `scipy.optimize.slsqp` namespace. `scipy.optimize.slsqp.exp` will be removed in SciPy 1.13.0, and the `scipy.optimize.slsqp` namespace will be removed in SciPy 2.0.0.\r\n```\r\nThe tests checking this behavior are failing. But I guess they can be removed now too since we said they'd be removed in SciPy 1.13.0.","> The current behavior is, e.g.:\r\n\r\nIn one of these [discussions](https:\/\/github.com\/scipy\/scipy\/pull\/18992#discussion_r1279034139) the reason given for keeping these imports was that they once were part of `__all__`. As it happens, that wasn't the case anymore for the import I removed in https:\/\/github.com\/scipy\/scipy\/pull\/19893, but still affects some of the functions here.\r\n\r\n> These imports were intentional and part of the plan to fix [gh-18279](https:\/\/github.com\/scipy\/scipy\/issues\/18279).\r\n\r\nIn any case, we've scheduled them for removal in 1.13, so the timing is still OK. \ud83d\ude43 ","So yeah, go ahead and remove the tests. It's fine with me to take all these out now, even if 1.13 is coming earlier than usual.","> In any case, we've scheduled them for removal in 1.13, so the timing is still OK. \ud83d\ude43\r\n\r\nActually, we've delayed this to 1.14 in #19892. So I'll wait until 1.13 is out.","Lint error is not due to this PR:\r\n```\r\nscipy\/optimize\/_minpack_py.py:875:89: E501 Line too long (89 > 88)\r\n```\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/65bb18c07795e887803fe65522ce26f4b9a0888f\/scipy\/optimize\/_minpack_py.py#L877-L881","This is ready for review now (even though my memory of why I did what I did is a bit hazy two months later \ud83d\ude05). We should also double-check the discussion from https:\/\/github.com\/scipy\/scipy\/issues\/19905 w.r.t. `scipy.optimize.slsqp.approx_jacobian` resp. `scipy.io.mmio.MMFile`.","@h-vetinari  The last two test failures are related. About `scipy.optimize.slsqp.approx_jacobian` and `scipy.io.mmio.MMFile`: no one objected to the removal, so let's get rid of them, I'd say. After fixing, this can be merged IMO.","> This is ready for review now (even though my memory of why I did what I did is a bit hazy two months later \ud83d\ude05). We should also double-check the discussion from #19905 w.r.t. `scipy.optimize.slsqp.approx_jacobian` resp. `scipy.io.mmio.MMFile`.\r\n\r\nWe could consider leaving them in for now? I don't quite have the capacity at the moment to investigate properly what to do with these\r\n\r\nEDIT: missed Daniel's comment; either is fine with me","There is also #19223 to consider","I'm not touching `MMFile` here, and `approx_jacobian` got removed partly because you had mentioned in https:\/\/github.com\/scipy\/scipy\/issues\/19905 that it was an accidental export as well. I don't mind deferring if you think it's worth delaying.","> I'm not touching `MMFile` here, and `approx_jacobian` got removed partly because you had mentioned in #19905 that it was an accidental export as well. I don't mind deferring if you think it's worth delaying.\r\n\r\nOk cool that's fine then, I `approx_jacobian` was fine to remove its just the `io` stuff I can't remember all the details on"],"labels":["maintenance"]},{"title":"MAINT: fft: remove `array_api_strict` skips","body":"#### Reference issue\r\nTowards gh-17278 and gh-19257\r\n\r\n#### What does this implement\/fix?\r\nnumpy\/numpy#25317 has allowed us to remove some skips.\r\n\r\n#### Additional information","comments":["I'm pretty sure we get new `fft` failures with numpy `main`, and the latest released `numpy` doesn't contain `array_api.fft` yet. So this seems a little premature.","Fair - with NumPy nightly all tests pass for `python dev.py test -s cluster -s fft -b all` with these skips removed (and a workaround for the `api_version=None` thing).\r\n\r\nEdit: let's keep this as draft until NumPy 2.0 is out.","Brainstorming how we should go about testing the `device` parameter. If we explicitly set `SCIPY_DEVICE`, then we're probably fine to just test with that, and the onus is on the person using the CLI \/ writing the workflow to use a device which the backends can use.\r\n\r\nThe question is what to do when `SCIPY_DEVICE` is not set. NumPy wants `'cpu'`, while `array-api-strict` ~wants `'CPU_DEVICE'`~ can only accept `None` I think. Maybe `SCIPY_DEVICE` should remain `None` if not explicitly set, then we handle that case in setting the default device.","> Brainstorming how we should go about testing the device parameter. If we explicitly set SCIPY_DEVICE, then we're probably fine to just test with that, and the onus is on the person using the CLI \/ writing the workflow to use a device which the backends can use.\r\n\r\nI'm trying to wrap my head around that. Maybe I'm missing something, but given that not all backends are guaranteed to have a mechanism for setting a global default device, how does the current diff here not pose a challenge to handling parametrized device scenarios across backends that do and do not support that mechanism? Or are we just deferring that until it actually happens with a backend we support?","the diff here is just deferring yes in a sense. It should probably be reverted, along with adding actual testing of the `device` parameter, once we figure out how to make it work. (The reason for removal for now is that array-api-strict is not happy with `device='cpu'`)","I'll return to this after gh-20085 is in"],"labels":["maintenance","scipy.fft","array types"]},{"title":"dtype instability in `stats.rankdata`","body":"Following up on the issue with `rankdata` in https:\/\/github.com\/scipy\/scipy\/pull\/19797#issuecomment-1888863426 - the immediate failing test issue is addressed, however the `rankdata` behavior doesn't look good. The dtype of the returned array should not be depending on both the value of the `method` keyword and the values in the array (it's always `float64` if `nan`s are present, and it may be integer if no `nan`s).\r\n\r\nThe docstring examples show the issue pretty clearly:\r\n```python\r\n>>> rankdata([0, 2, 3, 2])\r\narray([ 1. ,  2.5,  4. ,  2.5])\r\n>>> rankdata([0, 2, 3, 2], method='min')\r\narray([ 1,  2,  4,  2])\r\n>>> rankdata([0, 2, 3, 2], method='max')\r\narray([ 1,  3,  4,  3])\r\n>>> rankdata([0, 2, 3, 2], method='dense')\r\narray([ 1,  2,  3,  2])\r\n>>> rankdata([0, 2, 3, 2], method='ordinal')\r\narray([ 1,  2,  4,  3])\r\n>>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\r\narray([[1. , 2.5],\r\n      [4. , 2.5]])\r\n>>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\r\narray([[1. , 2.5, 2.5],\r\n       [2. , 1. , 3. ]])\r\n>>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\r\narray([nan, nan, nan, nan, nan, nan])\r\n>>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\r\narray([ 2.,  3.,  4., nan,  1., nan])\r\n```\r\n\r\nThis looks bad enough that it can be considered a bug I think (and it was definitely buggy until 1.12 when it didn't even adhere to the rules as shown by https:\/\/github.com\/scipy\/scipy\/pull\/19797#issuecomment-1888863426; that was improved in the recent rewrite in gh-19776). The easiest solution would be to always return a `float64` array. ","comments":["> The easiest solution would be to always return a float64 array.\r\n\r\nThat's fine with me. In the rewrite, I just tried to maintain the old dtype behavior. You want me to open a PR that ensures that the output is always `float64`?","Thanks, that would be helpful. Maybe let's give it a day at least to see if anyone else sees a problem? \r\n\r\nI'll note that I did quickly patch this already and checked that nothing failed in the test suite except from the tests that explicitly assert integer dtype.","I wouldn't expect it to. The consecutive integers that can be represented exactly in `float64` get quite large compared to typical array sizes. I think the bigger BC concern would be if someone were using the output in a way that depends on the dtype; e.g. for indexing. "],"labels":["scipy.stats"]},{"title":"ENH: linalg: Automatically Regularized Least Squares","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nAn old PR, gh-12755, proposed adding a package for Automatically Regularized Least Squares to SciPy. See https:\/\/github.com\/scipy\/scipy\/pull\/12755#issuecomment-708029562 for thoughts on inclusion at the time. Over the years some things changed; the package made its way to PyPI as `arls`, then (in the words of the author @rejones7):\r\n\r\n> After years of revamping this method I was able to develop a much much shorter version which is theoretically and practically better, but it is only on MATLAB right now\r\n\r\n[The MATLAB code can be found here](https:\/\/uk.mathworks.com\/matlabcentral\/fileexchange\/130259-arls-automatically-regularized-least-squares) and comes with its own BSD-3 license (hence I think it would be possible to include in SciPy). [A write up about the package can be found here](https:\/\/blogs.mathworks.com\/cleve\/2023\/06\/16\/arls-automatically-regularized-least-squares\/).\r\n\r\n### Describe the solution you'd like.\r\n\r\nIf someone wants to work on adding this functionality, it seems that the easiest route would be to translate the MATLAB source to Python.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nIt only makes sense to take on this work if there is interest in the functionality being added to SciPy, so an alternative is to do nothing if the demand is not there.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["The method looks interesting in principle but regularized linear models belong in my opinion into more dedicated machine learning or statistics packages such as scikit-learn. Given scikit-learn's prominence, I doubt that many users would anyway switch to \"yet another hyperparameter tuning algorithm\" for L2 regularized least squares if it does not work with all the niceties of sklearn's API such as cross validation etc.","I want to explain something before you all dismiss ARLS completely.\r\n\r\nI took a quick look at scikit-learn, and unless libraries like LAPACK, and routines like LSMR and NNLS are all going away in favor of AI solutions, SCIPY needs ARLS. SCIPY\u2019s users deserve better than LSMR and NNLS, and SCIPY users would benefit from the rich set of solvers in ARLS. Four points:\r\n\r\n  1.  When I started developing ARLS for Python in 2020 the only auto-regularized solver I could find in Scipy was LSMR. But LSMR has problems: sometimes it produces fabulous solutions; but too often it fails miserably. It is so unreliable that it is unusable unless a human is going to check every solution for suitability. This unreliability makes it impossible to build constrained solvers on top of LSMR. (I\u2019ve tried!)\r\n  2.  In contrast, ARLS (the main routine in the ARLS package) virtually never fails to provide a usable regularized solution. This makes it an ideal basis for constrained solvers. So the ARLS package provides ARLSEQ for equality constraints, ARLSGT for general inequality constraints, ARLSALL for both types of constraints, ARLSNN for nonnegativity, and ARLSHAPE to let users require up or down slope, or up or down concavity, or both. All these work great. And I believe they would be a substantial enhancement to Scipy\u2019s current linear algebra facilities. Please correct me if that is wrong.\r\n  3.  While developing ARLSNN,  I compared its performance to that of the classic NNLS by Lawson and Hanson. But NNLS has more problems than LSMR has, and those arose in normal testing of ARLS. First, NNLS has no guaranteed iteration limit, and instead simply fails after some number of iterations. NNLS does not provide a partial or default answer in that case. In contrast, ARLSNN guarantees completion, with a max of \u201ccolumn-size minus 1\u201d iterations.\r\n  4.  Less well known is the fact that NNLS can zero-out an unreasonable number of variables. In a bio-tech problem that I was consulted about during my development of ARLS, NNLS zeroed almost half the variables in the problem. In other words, it effectively deleted about half of the entire model! Scipy users deserve better performance than that.   In contrast, ARLSNN only had to delete ONE variable to achieve nonnegativity. (I don\u2019t claim that is typical!)  Please bear in mind I am not trying to insult Hanson\u2019s work. Dick Hanson passed away a few years ago, here in Albuquerque. But for many years Dick and his wife Karen were friends and co-workers of mine at Sandia Labs. He was also on my PhD advisory committee. I am sure he would not resent having algorithms by a student of his supplant some of his much earlier work.\r\n\r\nWould you please give ARLS another look? You do not need to update the core algorithm to that of MATLAB. You already have a fine version in your contributed library. You can use it immediately. On the other hand, if you choose to accept ARLS I will take a look at converting the core algorithm to the newer algorithm that is in MATLAB. But I am 80 years old and mostly now consider myself retired, so I might need a helpful collaborator -- such as one of you on this email \u2013 to accomplish it in a reasonable time.\r\n\r\n\r\n\r\n________________________________\r\nFrom: Daniel Schmitz ***@***.***>\r\nSent: Tuesday, January 16, 2024 10:15 AM\r\nTo: scipy\/scipy ***@***.***>\r\nCc: rejones7 ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [scipy\/scipy] ENH: linalg: Automatically Regularized Least Squares (Issue #19888)\r\n\r\n\r\nThe method looks interesting in principle but regularized linear models belong in my opinion into more dedicated machine learning or statistics packages such as scikit-learn. Given scikit-learn's prominence, I doubt that many users would anyway switch to \"yet another hyperparameter tuning algorithm\" for L2 regularized least squares.\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https:\/\/github.com\/scipy\/scipy\/issues\/19888#issuecomment-1894172684>, or unsubscribe<https:\/\/github.com\/notifications\/unsubscribe-auth\/APWVH7BF5DWLMYRGDIDWJYDYO2YSBAVCNFSM6AAAAABB5EYLTKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQOJUGE3TENRYGQ>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n","Lucas: I prepared a more full explanation of why ARLS would be good for Python, but I am not sure you were sent it,. I will repeat it below. After thinking about it more I realized that there really is no problem with promoting the existing ARLS in the Python contributed library now.\r\nThis explanation applies to the current ARLS available now. I would appreciate knowing your thoughts on this. Sorry for the small font.. I wrote this in Word.\r\n------------------------------------------------------------------------------------\r\n\r\nI want to explain something before you all dismiss ARLS completely.\r\n\r\nI took a quick look at scikit-learn, and unless libraries like LAPACK, and routines like LSMR and NNLS are all going away in favor of AI solutions, SCIPY needs ARLS. SCIPY\u2019s users deserve better than LSMR and NNLS, and SCIPY users would benefit from the rich set of solvers in ARLS. Four points:\r\n\r\n  1.  When I started developing ARLS for Python in 2020 the only auto-regularized solver I could find in Scipy was LSMR. But LSMR has problems: sometimes it produces fabulous solutions; but too often it fails miserably. It is so unreliable that it is unusable unless a human is going to check every solution for suitability. This unreliability makes it impossible to build constrained solvers on top of LSMR. (I\u2019ve tried!)\r\n  2.  In contrast, ARLS (the main routine in the ARLS package) virtually never fails to provide a usable regularized solution. This makes it an ideal basis for constrained solvers. So the ARLS package provides ARLSEQ for equality constraints, ARLSGT for general inequality constraints, ARLSALL for both types of constraints, ARLSNN for nonnegativity, and ARLSHAPE to let users require up or down slope, or up or down concavity, or both. All these work great. And I believe they would be a substantial enhancement to Scipy\u2019s current linear algebra facilities. Please correct me if that is wrong.\r\n  3.  While developing ARLSNN,  I compared its performance to that of the classic NNLS by Lawson and Hanson. But NNLS has more problems than LSMR has, and those arose in normal testing of ARLS. First, NNLS has no guaranteed iteration limit, and instead simply fails after some number of iterations. NNLS does not provide a partial or default answer in that case. In contrast, ARLSNN guarantees completion, with a max of \u201ccolumn-size minus 1\u201d iterations.\r\n  4.  Less well known is the fact that NNLS can zero-out an unreasonable number of variables. In a bio-tech problem that I was consulted about during my development of ARLS, NNLS zeroed almost half the variables in the problem. In other words, it effectively deleted about half of the entire model! Scipy users deserve better performance than that.   In contrast, ARLSNN only had to delete ONE variable to achieve nonnegativity. (I don\u2019t claim that is typical!)  Please bear in mind I am not trying to insult Hanson\u2019s work. Dick Hanson passed away a few years ago, here in Albuquerque. But for many years Dick and his wife Karen were friends and co-workers of mine at Sandia Labs. He was also on my PhD advisory committee. I am sure he would not resent having algorithms by a student of his supplant some of his much earlier work.\r\n\r\nWould you please give ARLS another look? You do not need to update the core algorithm to that of MATLAB. You already have a fine version in your contributed library. You can use it immediately. On the other hand, if you choose to accept ARLS I will take a look at converting the core algorithm to the newer algorithm that is in MATLAB. But I am 80 years old and mostly now consider myself retired, so I might need a helpful collaborator -- such as one of you on this email \u2013 to accomplish it in a reasonable time.\r\n\r\n________________________________\r\nFrom: Lucas Colley ***@***.***>\r\nSent: Tuesday, January 16, 2024 10:02 AM\r\nTo: scipy\/scipy ***@***.***>\r\nCc: rejones7 ***@***.***>; Mention ***@***.***>\r\nSubject: [scipy\/scipy] ENH: linalg: Automatically Regularized Least Squares (Issue #19888)\r\n\r\n\r\nIs your feature request related to a problem? Please describe.\r\n\r\nAn old PR, gh-12755<https:\/\/github.com\/scipy\/scipy\/pull\/12755>, proposed adding a package for Automatically Regularized Least Squares to SciPy. Over the years some things changed; the package made its way to PyPy as arls, then (in the words of the author @rejones7<https:\/\/github.com\/rejones7>):\r\n\r\nAfter years of revamping this method I was able to develop a much much shorter version which is theoretically and practically better, but it is only on MATLAB right now\r\n\r\nThe MATLAB code can be found here<https:\/\/uk.mathworks.com\/matlabcentral\/fileexchange\/130259-arls-automatically-regularized-least-squares> and comes with its own BSD-3 license (hence I think it would be possible to include in SciPy). A write up about the package can be found here<https:\/\/blogs.mathworks.com\/cleve\/2023\/06\/16\/arls-automatically-regularized-least-squares\/>.\r\n\r\nDescribe the solution you'd like.\r\n\r\nIf someone wants to work on adding this functionality, it seems that the easiest route would be to translate the MATLAB source to Python.\r\n\r\nDescribe alternatives you've considered.\r\n\r\nIt only makes sense to take on this work if there is interest in the functionality being added to SciPy, so an alternative is to do nothing if the demand is not there.\r\n\r\nAdditional context (e.g. screenshots, GIFs)\r\n\r\nNo response\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https:\/\/github.com\/scipy\/scipy\/issues\/19888>, or unsubscribe<https:\/\/github.com\/notifications\/unsubscribe-auth\/APWVH7AUUCFNO7E757GWS63YO2XBJAVCNFSM6AAAAABB5EYLTKVHI2DSMVQWIX3LMV43ASLTON2WKOZSGA4DINBUG4YTKNI>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n","Hi @rejones7 , unfortunately I don't have the required domain knowledge to give an opinion here. Hopefully someone with the required knowledge can get round to sharing their thoughts at some point, but there are no guarantees about how long it will take (we have over 1500 other open issues right now...). Thanks for providing all of the explanation you have so far, I'm sure it will be helpful when discussion occurs!","ARLS by @rejones7 is all about inverse methods with automatic regularization. And inverse methods are very useful methods in engineering, optimization, deconvolution, tomography and what else. The automatic regularization is the key point of ARLS. I once worked with it, but only on small problems - toy problems if you want - and I found it really useful.\r\nI'm quite sure there will be problems where ARLS is not better or even worse compared to other well known methods (no hammer fits it all) , but for in many cases  it could be very helpful (due to automatic regularization) and may outperform other methods.\r\n\r\nBut I am by no means an expert on this field, so it would be useful to hear from some else."],"labels":["enhancement","scipy.linalg"]},{"title":"RFC: ENH: stats: Extend `wasserstein_distance` to different `p` values. See ticket #19727.","body":"Closes #19727\r\n\r\nThis is still a draft and I am seeking opinions.\r\n\r\nThere has been a request to add a function to calculate the Wasserstein distance with $p=\\infty$, see #19727. I have figured two things:\r\n\r\ni) The implementation for the multidimensional case easy to generalize to an arbitrary $p \\geq 1$.\r\n\r\nii) Following the [reference](https:\/\/arxiv.org\/pdf\/1509.02237.pdf) in the docs, I have figured that there is an analytical result for $p = \\infty$ in the 1-D case. On page 10, it is stated that  \r\n\r\n$$W_\\infty(P,Q) = ||F^{-1}-G^{-1}||_\\infty$$ \r\n\r\nin the 1-D case, where $F^{-1}$ and $G^{-1}$ are the [quantile functions](https:\/\/en.wikipedia.org\/wiki\/Quantile_function) of $P$ and $Q$ respectively. Here, $|\\cdot|_\\infty$ is the [infinity norm](https:\/\/en.wikipedia.org\/wiki\/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)).\r\n\r\nI have extended the `wasserstein_distance` function to take a new input `p`. In the one dimensional case, if $p=1$ then the analytical result from `_cdf_distance` is used. If $p=\\infty$, then the analytical result from the newly added function `_wasserstein_infty` is used. For any other `p` value the 1-D array is turned into 2-D and the corresponding Wasserstein distance is calculated numerically.\r\n\r\nI have written some tests. It seems that as long as the distributions are 'nice enough', $W_p \\to W_\\infty$ as $p \\to \\infty$ as expected. Please see the tests.\r\n\r\nHowever there are some caveats:\r\n\r\n1. If `p` is too large ($p>50$) the algorithm blows up. \r\n2. The distributions in my tests are a little too nice.\r\n\r\nAs a result, while I am mostly confident that `_wasserstein_infty` works as intended (more tests are needed though), I am not confident with extending $p=\\infty$ to the 2-D case or supporting all `p` values. Small `p` values (such as `p=2`) are probably fine.\r\n\r\nMy questions are:\r\n1. What are your opinions on extending this metric?\r\n2. What other tests would you recommend me to do?\r\n\r\nPlease let me know what you think.","comments":["On stability with large p, would it help if you scale the distances `D` before `cost = np.power(D.ravel(), p)` (line 10408), by dividing by the largest `D`?  Then de-scale the result before returning it.  See [Handling Overflow condition in calculating p-Norm](https:\/\/rehanguha.github.io\/articles\/overflow-p-norm).","@lutefiskhotdish Thanks for the recommendation. I also figured that there is an analytical result for general `p` in the 1-D case. I will also implement that and compared the results with the numerical method.","It might be worthwhile to check\/compare the numerics against [`pot`](https:\/\/pythonot.github.io\/index.html) for the `p=oo` scenario to confirm that the values agree. The pot package specializes in optimal transport so they should have numerically stable solution if one exists here. ","@yagizolmez I am pleased to be heard from my issue. :raised_hands: \r\n1.) From my current point of view, metrics where p=1, 2 and $\\infty$ in 1-D are enough for me. So I have no further need for generalization.\r\n2.) I am currently busy with other stuff so I will have to check and test in a while, so I did not go deep inside the code to check the scope of the tests. So for me, they are fine.\r\n\r\nThank you and keep up the good work! :rocket: ","I know that `wasserstein_distance` has been reverted, but I am keeping the 2-D version for now for this draft PR. I still use the 2-D method for tests.\r\n\r\nFor the 1-D case with `p=1` and `p=2`, I have implemented the Proposition 1 [in the reference](https:\/\/arxiv.org\/pdf\/1509.02237.pdf):\r\n\r\n$$\r\nW_p^p (P,Q) = \\int_0^1 |F^{-1} (t) - G^{-1} (t) |^p \\text{d} t\r\n$$\r\n\r\nOn the same page, there is also a formula for $p=\\infty$ case:\r\n\r\n$$\r\nW_\\infty(P,Q) = ||F^{-1} - G^{-1}||_\\infty\r\n$$\r\n\r\nwhich is also implemented. I have written tests to compare the `p=1` case with `_cdf_distance` and `p=2` case with the optimization method. I have not written any tests for the $p=\\infty$ case yet. \r\n\r\n@mdhaber, @com3dian. Any recommendations would be appreciated. Especially on writing tests for the $p=\\infty$ case and on including the `axis` argument.","Please merge main here. If you need to use the linear programming backend that was removed from `wasserstein_distance`, just copy-paste it from the code that was removed. You can make it a private function in `_stats_py.py`, e.g. `_wasserstein_distance_nd`, for now; we'll make it public in a separate PR.","@mdhaber Done! I would mainly like you to go over my implementation of `_wasserstein_p`. I have implemented it similar to `_cdf_distance`. I think it is sound, efficient and gives consistent results with `_cdf_distance` and `_wasserstein_distance_nd`. It could become even more efficient if numpy adds a function to merge two already sorted arrays into a sorted array.","@yagizolmez Let's keep this one open, too, since it is conceptually distinct from adding the separate `wasserstein_distance_nd` function. Once that merges, we can consider making these changes to that function, but the initial version of `wasserstein_distance_nd` should match https:\/\/github.com\/scipy\/scipy\/pull\/17473 closely.","Looking at this, based on how I implemented the $p=2$ in the multivariate case, I conjecture that for the $p = \\infty$ case we should simply return `D_largest_abs`. I will implement this and compare with the analytical results from the 1D case. If that works, I will look for an analytical proof somewhere to make sure."],"labels":["scipy.stats","enhancement","RFC"]},{"title":"BLD:RfC: nanobind as a possible dependency?","body":"As part of our rewrite efforts, lately I have been visiting the C land and our C\/C++ code wrapping can use slightly less black magic or can avoid Cython .pxd route to surface the C functions to higher level APIs. At some point we have to tie this code back to Python API and there are multitude of methods, and opinions. Because the fortran bindings are typically hidden behind the `f2py` mechanism, we often forget how incredibly useful `f2py` is regardless of all downsides we know and love to hate. However we don't have such mechanisms when gluing C code because using a tool called `c2c` probably would be considered as blasphemy. \r\n\r\nWe also have some strange usage patterns in our codebase, for example, `specfun` Fortran77 module is both compiled as a static library and powers partially the ufuncs at C level but also is used as a user facing code through an extension module generated via f2py and `specfun.pyf` wrappers. \r\n\r\nFor C\/C++ code we don't have a generic f2py tooling (which is probably a good thing). We have been successfully using `pybind11` and so far I think the overall positive impression we have is that, it is a robust and relatively straightforward-to-use tool. However things progressed and even the author of pybind11 created the new tool nanobind. Obviously I'm not the right person to actually distinguish all the spices just by smelling it.\r\n\r\nHowever, there a couple of things I really want\/desire\/obsess; runtime performance and binary reduction. There are a lot of positive vibes online about nanobind but I never tried more than the tutorials online on a production code base. Since we now have, quite lengthy C code, coming from #19587, #19824 and when retranslated (again) to C, #19560, this is becoming an item we might consider. \r\n\r\nNanobind has been briefly discussed previously in #19404, in a different context. The reason why I'm bringing it up again is that hopefully the context here is more suitable. We already use `pybind11` and more importantly I'd like to get opinions about its usage for glue not for the concerns mentioned in #19404 \r\n\r\nI know @henryiii is often present here and has extensive knowledge on both ends and @mckib2 has been using it in SciPy already hence I'd love to hear their take (and others obviously) about it. \r\n\r\n","comments":["One particular con about nanobind is the cmake dependency which we don't need to discuss again why we don't prefer it. ","Is cython not the least magical? I'd suggest the default to be a thin cython wrapper, with either no functionality at all (or maybe handling memory allocations for out and work arrays). The pros:\n\n- well understood, \n- no magic, easy to control the behavior manually\n- no silent copies. \n\nCons:\n- binary size (likely to become irrelevant if the wrappers are thin?). \n\nISTM it's an easy baseline to compare possible alternatives against. \n","I'd also note that the wrapping story is special in scipy.special. what was in fortran and now is in c\/c++ are kernels, which are the wrapped into ufuncs and cython_special.pxd.\n","I can't wrap (excuse the pun) my head quite yet what is going on there but I'm just trying to supply sufficient information to meson and crossing fingers. ","(Edited for fat-finger errors) What I'm saying is that in scipy.special everything works as is for ufuncs and is easiest to make work in cython for cython_special IIUC. \r\nSo nanobind or whatnot refers to... what exactly? Linalg? Planned future work on other compiled modules? ","That is to wrap C\/C++ code wrap in a more lightweight and more performant fashion. Cython is indeed adding quite some beef into the binaries and I'm not sure about its performance. There are some benchmarks here https:\/\/nanobind.readthedocs.io\/en\/latest\/benchmark.html but obviously benchmarks are never a science. \r\n\r\nFunctions declared in this file \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/special\/specfun.pyf\r\n\r\nare also exposed through Python API and used in `basic.py` of special. So we need to wrap those.\r\n","Thanks Ilhan for the link. Binary size and build time improvements are nice indeed (one thing I miss from Fortran is build times).\r\nThat said, runtime performance in these benchmarks is not very interesting for scipy I think, we do not often need to expose scalar functions of scalar arguments.\r\n\r\nTo be specific, I think it'd be helpful to pick some non-trivial function with array arguments (not a scipy.special ufunc), try wrapping it two ways with cython and nanobind, and compare on\r\n\r\n- ease of wrapping\r\n- build times\r\n- integration with our build system\r\n- binary size\r\n- runtime performance\r\n\r\nDo you have a specific function in mind?","Is pybind11 not scratching the itch that nanobind is being being put forward for here?","If I take the benchmarks at the face value, build times and binary sizes are really appealing given how much cython wrapped code we have including against pybind11. \r\n\r\nI can imagine pushing some of the array crunching cython code to C level since except linalg we use float64 and complex128 as data types. \r\n\r\nAlso ufuncs are not necessarily quite fast for some reason as we are seeing in #19079. \r\n\r\nBut this is just to get some opinions whether there is any benefit. If not, we can leave it to rest.","> I can imagine pushing some of the array crunching cython code to C level since except linalg we use float64 and complex128 as data types.\r\n\r\n> But this is just to get some opinions whether there is any benefit. If not, we can leave it to rest.\r\n\r\nA big fat +1 from me for coming up with a canonical recommended way of interfacing with C and C++ instead of a mishmash we have now. Let's pick some function, I'd be willing to poke around. ","I think we should allow usage of `nanobind` only if it can fully replace our `pybind11` usage - we shouldn't have both, that would increase the surface area for dependencies\/regressions, without too much of a gain overall.\r\n\r\nFrom https:\/\/nanobind.readthedocs.io\/en\/latest\/porting.html#removed it's not entirely clear whether it can fully replace `pybind11`. I don't think we use positional\/keyword-only args yet, but that's an odd choice imho - it's regular Python function definition syntax that is useful. The most worrying one is probably _\"Compilation: workarounds for buggy or non-standard-compliant compilers were removed and will not be reintroduced.\"_. Hard to tell to what extent that includes common compilers like MSVC and OneAPI.",">  think we should allow usage of nanobind only if it can fully replace our pybind11 usage - we shouldn't have both\r\n\r\nTo look at a specific instance of this, the HiGHS rework is being done using `pybind11` and that's what appears will be supported upstream for the foreseeable future.  There's always the possibility of asking them to switch to `nanobind`, but it might take some politicking on our part or accepting use of both `pybind11` (within a submodule dependency) and `nanobind` (everything else)\r\n\r\nEither way, +1 for unifying the C++ --> python build process","Regarding binary size: pip installed scipy is already too large for a AWS Lambda layer which has a size limit of 50MB: https:\/\/github.com\/keithrozario\/Klayers\/issues\/360#issuecomment-1893066931"],"labels":["Build issues","C\/C++","Cython","Meson","RFC"]},{"title":"ENH: let `squareform` process diagonal","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nThe `squareform` function is very handy in converting between vector-form and square-form distance matrices. However, it does not take the diagonal line. Matrices with a meaningful non-zero diagonal line are frequent. For example, an amino acid substitution matrix has values in the diagonal line as well as triangles to represent the similarity of characters. Here is an example: https:\/\/ftp.ncbi.nlm.nih.gov\/blast\/matrices\/BLOSUM62 . It will be useful to keep the values in the diagonal line in the vector-form.\r\n\r\n### Describe the solution you'd like.\r\n\r\nAdd a boolean parameter `diagonal` to the `squareform` function. The default is `False`, which keeps the current behavior. When it is set to `True`, it will include the diagonal line during the conversion.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nHere is my homebrew code for reference. It considers the diagonal line.\r\n\r\n```python\r\ndef matrix_to_vector(mat):\r\n    return mat[np.triu_indices(len(mat))]\r\n\r\ndef vector_to_matrix(vec):\r\n    n = int((np.sqrt(1 + 8 * len(vec)) - 1) \/ 2)\r\n    mat = np.zeros((n, n))\r\n    mat[np.triu_indices(n)] = vec\r\n    return mat + np.triu(mat, k=1).T\r\n```\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":[],"labels":["enhancement","scipy.spatial"]},{"title":"BUG: error in calculation of p-values in sp.stats.wilcoxon when there are ties in ranked differences","body":"### Describe your issue.\n\nP-values for one-sided wilcoxon signed-rank tests are inconsistent depending on the ordering of the input for the same test in cases where there are ties in the ranked differences.\r\n\r\nSpecifically, testing for A>B and B<A should, in theory, give the same p-value for the one-sided tests, but do not give the same p-value in instances where there are ties in the ranked differences and the average rank is assigned (a decimal value).\r\n\r\nThis problem arises when calculating the 'exact' p-value as the null distribution for a given N is coded as an array requiring integer slicing to get the appropriate p-value. This results in A>B and B<A not actually being the same, despite being the same.\n\n### Reproducing Code Example\n\n```python\nvar1 = [62, 66, 61, 68, 74, 62, 68, 62, 55, 59]\r\nvar2 = [71, 71, 69, 61, 75, 71, 77, 72, 62, 65]\r\nprint(stats.wilcoxon(var1, var2, alternative = 'less'))\r\nprint(stats.wilcoxon(var2, var1, alternative = 'greater'))\n```\n\n\n### Error message\n\n```shell\nn\/a\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.1 1.24.3 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/sammichekroud\/anaconda3\/include\r\n    lib directory: \/Users\/sammichekroud\/anaconda3\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/sammichekroud\/anaconda3\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/sammichekroud\/anaconda3\/include\r\n    lib directory: \/Users\/sammichekroud\/anaconda3\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/sammichekroud\/anaconda3\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/sammichekroud\/anaconda3\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 12.0.0\r\n  c++:\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 12.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/var\/folders\/nz\/j6p8yfhx1mv_0grj5xl4650h0000gp\/T\/abs_66h0hq6x34\/croot\/scipy_1691606685645\/_build_env\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/..\/Users\/sammichekroud\/anaconda3\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/sammichekroud\/anaconda3\/bin\/python\r\n  version: '3.11'\n```\n","comments":["> This problem arises when calculating the 'exact' p-value as the null distribution for a given N is coded as an array requiring integer slicing to get the appropriate p-value.\r\n\r\nSpecifically:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/5e4a5e3785f79dd4e8930eed883da89958860db2\/scipy\/stats\/_morestats.py#L4166\r\nand\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/5e4a5e3785f79dd4e8930eed883da89958860db2\/scipy\/stats\/_morestats.py#L4175-L4176\r\n\r\n`r_plus` should not be rounded down before this calculation if it is legitimately greater than `int(r_plus)` because then the probability mass of `int(r_plus)` is included in the p-value even though the observed `r_plus` is strictly greater. \r\n\r\n(Well, it's tough to say precisely what *should* happen, since the null distribution `pmf` is calculated under the assumption that there are no ties. So the other way to look at this is that this alternative is calculated correctly, _conservatively_ including the probability mass at `int(r_plus)` since an assumption is violated, and the other alternative should include the extra probability mass in its calculation. But in any case, adjusting the rounding procedure for one of the alternatives is the way to fix this.)\r\n\r\nThere is a rewrite of `wilcoxon` in gh-19770 which can incorporate this fix.\r\n\r\n","yep, i also noticed that this is an issue of rounding by coercing to integer, but it makes two tests that *are* equivalent, non-equivalent - happening (at least in my case) because both R+ and R- are being rounded down. For true equivalences of the tests, `int(rplus) + int(rminus) = max_rank` should be forced to be true.\r\n\r\nAn alternative solution would be to allow individual control of the ranking method used - at the moment, `r = _stats_py.rankdata(abs(d))` doesn't allow any control over how ties are handled (it just uses the `'average'` default in `rankdata`. Being able to specify the method with which ties are handled would get over this too as the issue arises when the test statistic is not an integer"],"labels":["defect","scipy.stats"]},{"title":"BUG: HalfspaceIntersection.add_halfspaces() does not seem to work as expected","body":"### Describe your issue.\n\n`scipy` version `1.11.4`.\r\n\r\nIf I start with 3 halfspaces and a feasible point, then the resulting intersections form a triangle as expected:\r\n\r\n<img width=\"424\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/131701699\/5e578e33-bae1-458f-b03d-d0378ae38886\">\r\n\r\n(actual numbers in the code example below; initial feasible point shown as 'x', returned vertices shown as 'o').\r\n\r\nThe intersections there are: \r\n\r\n```\r\n[[ 0.355004   -0.45821925]\r\n [-0.24133901  0.46546184]\r\n [ 0.94809281  0.46546184]]\r\n```\r\n\r\nif I start with the square [-1,1] \u00d7 [-1,1] and incrementally add the halfspaces, I expect to get the same result (because that box strictly contains my triangle). Instead, I get out a pentagon:\r\n\r\n<img width=\"457\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/131701699\/ea9396ec-fb4b-4978-877c-77ada8fc83f8\">\r\n\r\nit seems like the +\/- sense of one of the halfspaces (blue line) is being reversed?\n\n### Reproducing Code Example\n\n```python\nhalfspaces = np.array([\r\n       [-0.70613882, -0.45589431,  0.04178256],\r\n       [ 0.70807342, -0.45464871, -0.45969769],\r\n       [ 0.        ,  0.76515026, -0.35614825]])\r\n\r\nfeasible_point = np.array([0.35425774, 0.14104826])\r\nvertices = HalfspaceIntersection(halfspaces, feasible_point).intersections\r\nprint(vertices) # prints the 3 vertices of the triangle\r\n\r\ninitial_square =  np.array(\r\n                    [[1, 0, -1], [0, 1, -1], [-1, 0, -1], [0, -1, -1]]\r\n                )\r\n\r\nincremental_intersector = HalfspaceIntersection(initial_square, np.zeros(2), incremental=True)\r\nfor h in halfspaces:\r\n    incremental_intersector.add_halfspaces(h[np.newaxis])\r\nincremental_intersector.close()\r\nresults = incremental_intersector.intersections\r\nprint(results) # prints the five vertices of the pentagon\n```\n\n\n### Error message\n\n```shell\n(none)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.23.5 sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-r1s2ogcy\/overlay\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-gft7gyfb\/cp39-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.9'\n```\n","comments":["@RichardBowenGM I've been looking at this a bit today. The first thing I did was plot the feasibility points--for the incremental case (black point), the feasibility point seems to lie outside the intersection polytope that you're expecting to preserve, while for the original case (red point), the feasibility point is indeed internal (see below). Am I misunderstanding something here? Wouldn't it be a problem if we started with a point that was not feasible\/interior to the final interior polytope?  \r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/7903078\/6c1a294e-c352-431a-af08-b13a9cb65c5f)\r\n\r\nI may need a bit more clarification here if you can help. One thing I did notice that seems weird is that even if use the same interior point for both incremental and non-incremental testing, there is still a subtle difference in the output. The test I wrote for that is below the fold. Basically two vertices get swapped in the incremental case if I use the same interior point as the non-incremental case. If I use the origin as the incremental interior point like you did, I indeed get the same issue with extra polytope vertices, though I'm somewhat concerned about the potential non-feasibility there?\r\n\r\n<details>\r\n<summary> Sample Code\/Test <\/summary>\r\n\r\n```diff\r\n--- a\/scipy\/spatial\/tests\/test_qhull.py\r\n+++ b\/scipy\/spatial\/tests\/test_qhull.py\r\n@@ -1176,3 +1176,33 @@ class Test_HalfspaceIntersection:\r\n             assert set(a) == set(b)  # facet orientation can differ\r\n \r\n         assert_allclose(hs.dual_points, qhalf_points)\r\n+\r\n+    @pytest.mark.parametrize(\"second_feasible\", [\r\n+            # these cases fails to match non-incremental\r\n+            # case for different reasons\r\n+            np.array([0.35425774, 0.14104826]),\r\n+            np.zeros(2),\r\n+    ])\r\n+    def test_gh_19865(self, second_feasible):\r\n+        halfspaces = np.array([\r\n+                       [-0.70613882, -0.45589431,  0.04178256],\r\n+                       [ 0.70807342, -0.45464871, -0.45969769],\r\n+                       [ 0.        ,  0.76515026, -0.35614825]])\r\n+        feasible_point = np.array([0.35425774, 0.14104826])\r\n+        initial_square =  np.array(\r\n+                    [[1, 0, -1], [0, 1, -1], [-1, 0, -1], [0, -1, -1]]\r\n+                )\r\n+        # check the non-incremental version for expected intersections\r\n+        # np.zeros(2) is not a valid feasible point here\r\n+        expected_intersector = qhull.HalfspaceIntersection(np.vstack((initial_square, halfspaces)),\r\n+                                                           feasible_point,\r\n+                                                           incremental=False)\r\n+        expected_intersections = expected_intersector.intersections\r\n+        incremental_intersector = qhull.HalfspaceIntersection(initial_square,\r\n+                                                              second_feasible,\r\n+                                                              incremental=True)\r\n+        for h in halfspaces:\r\n+            incremental_intersector.add_halfspaces(h[np.newaxis])\r\n+        incremental_intersector.close()\r\n+        actual_results = incremental_intersector.intersections\r\n+        assert_allclose(actual_results, expected_intersections)\r\n```\r\n\r\n<\/details>","> @RichardBowenGM I've been looking at this a bit today. The first thing I did was plot the feasibility points--for the incremental case (black point), the feasibility point seems to lie outside the intersection polytope that you're expecting to preserve, while for the original case (red point), the feasibility point is indeed internal (see below). Am I misunderstanding something here? Wouldn't it be a problem if we started with a point that was not feasible\/interior to the final interior polytope?\r\n\r\nAh, I see; the black point is feasible for the _initial_ polytope (the unit square), but it is not feasible for the _final_ one (the triangle). Good catch! I was expecting the method to also update the feasible point as I added more halfspaces. If that's not the intended behavior then it seems to me that `scipy.spatial.HalfspaceIntersection.add_halfspaces` should check and error out, much like the constructor does\r\n\r\n```python\r\nintersector = HalfspaceIntersection(initial_square, np.full((2,), 100))\r\n```\r\n\r\ngives\r\n\r\n```\r\nQhullError: QH6023 qhull input error: feasible point is not clearly inside halfspace\r\nfeasible point:    100    100 \r\n     halfspace:      1      0 \r\n     at offset:     -1  and distance:     99 \r\nThe halfspace was at index 0\r\n\r\nWhile executing:  | qhull H \r\nOptions selected for Qhull 2019.1.r 2019\/06\/21:\r\n  run-id 1970278628  Halfspace  _maxoutside  0\r\n```\r\n\r\n> \r\n> ![image](https:\/\/private-user-images.githubusercontent.com\/7903078\/302422276-6c1a294e-c352-431a-af08-b13a9cb65c5f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDcxNjY0NjksIm5iZiI6MTcwNzE2NjE2OSwicGF0aCI6Ii83OTAzMDc4LzMwMjQyMjI3Ni02YzFhMjk0ZS1jMzUyLTQzMWEtYWYwOC1iMTNhOWNiNjVjNWYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDIwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAyMDVUMjA0OTI5WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Y2U5ZDdhODA2OGZjZmRhYjllNmVkMWFjMzUyMmVhZDI2YzU0MzQxYTY3OTYyNTAxYjY0ZGRmM2EwNDM4OTI4NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.55bSUrwOhQ2lJ2pPiV8l4Lm8wBrE1rMq9Hf_Ls6Kt-M)\r\n> \r\n> I may need a bit more clarification here if you can help. One thing I did notice that seems weird is that even if use the same interior point for both incremental and non-incremental testing, there is still a subtle difference in the output. The test I wrote for that is below the fold. Basically two vertices get swapped in the incremental case if I use the same interior point as the non-incremental case. If I use the origin as the incremental interior point like you did, I indeed get the same issue with extra polytope vertices, though I'm somewhat concerned about the potential non-feasibility there?\r\n\r\nI think that the documentation makes no promises about the order of the returned vertices, so I wouldn't consider this permutation a bug (it would be a bug if the vertices moved, though).\r\n\r\n","> I was expecting the method to also update the feasible point as I added more halfspaces. If that's not the intended behavior then it seems to me that `scipy.spatial.HalfspaceIntersection.add_halfspaces` should check and error out, much like the constructor does\r\n\r\nI'd be happy to write a patch that does this if you like (unless it belongs upstream in qhull instead, I suppose).\r\n"],"labels":["defect","scipy.spatial"]},{"title":"MAINT: 1.12.x series tests still fail with pytest 8.0.0rc1","body":"I did backport some fixes for this, but here is a sample log for the 22 failures (it is also trivail to reproduce locally on x86_64 Linux on that branch of SciPy): https:\/\/github.com\/scipy\/scipy\/actions\/runs\/7465683723\/job\/20315409116?pr=19797\r\n\r\nNote that that pre-release job is also running with NumPy `1.26.3` by accident per gh-19846.\r\n\r\nSome warnings are suddenly appearing and\/or not appearing it seems, which sounds somewhat familiar. I've also been pinged to test various things over on the `pytest` issue tracker.\r\n\r\nI think for now what I'll probably do is proceed with RC2 for SciPy, since this is just a `pytest` pre-release, and even if the issue is on our end, we could patch for super-new `pytest` support in the `1.12.1` I think. That said, if it is determined to be crucial can consider an RC3 or similar.","comments":["> Note that that pre-release job is also running with NumPy `1.26.3` by accident per [gh-19846](https:\/\/github.com\/scipy\/scipy\/issues\/19846).\r\n\r\nThe logs you referenced use 2.0.0dev0:\r\n```\r\nInstalling collected packages: numpy\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.26.3\r\n    Uninstalling numpy-1.26.3:\r\n      Successfully uninstalled numpy-1.26.3\r\nSuccessfully installed numpy-2.0.0.dev0\r\n```\r\nWhich is almost certainly the reason for some warning mismatches (especially warnings that _aren't_ being raised anymore, because 2.0 executed some breaking changes).","> The logs you referenced use 2.0.0dev0\r\n\r\nIt gets uninstalled and then replaced by the stable release because of `matplotlib` if you grep through the log file. That's solved by gh-19849 now and I've also disabled the pre-release job in gh-19797.\r\n\r\nStill, I just checked with the latest version of the branch in gh-19797 locally on x86_64 Linux with NumPy `1.26.3` and I can still reproduce the failures with `pytest` `8.0.0rc1` and not with `pytest` stable release. It is the same 22 failures I reported yesterday.\r\n\r\nI don't think this is a blocker (although there is another blocker over there..), but just keeping the ticket open for reference in case we decide to fix.","I am reasonably confident that the issue you were seeing with pytest are gone in pytest 8.0.0rc2 that was released yesterday.\r\n\r\nFor example, h5py was having somewhat similar issues and they have disappeared with pytest 8.0.0rc2 see https:\/\/github.com\/h5py\/h5py\/pull\/2371\r\n\r\nFor reference, the pytest issues were mostly https:\/\/github.com\/pytest-dev\/pytest\/issues\/9765 (`assert mod not in mods`) but there was also https:\/\/github.com\/pytest-dev\/pytest\/issues\/11816 afterwards (conftest loaded but fixtures inside it were not defined)\r\n\r\n","@lesteve I see the same 22 failures locally with the `8.0.0rc2` version of `pytest` (same as the original log above). Perhaps we need another backport on the SciPy side though, since the latest `main` seems happy with that same RC version.\r\n\r\nI'll leave the ticket open for now--I may be asked to do a `1.12.1` to patch this, if the issue is indeed on our side.","~If the failures are just the extra warnings bubbling up then we just need to backport the fixes for those, right?~ ignore me, I was thinking of gh-19806 but that's already in.","Sorry I was not explicit enough, I was talking about the errors on Windows `assert mod not in mods` that were reported in https:\/\/github.com\/pytest-dev\/pytest\/issues\/9765#issuecomment-1879867556.\r\n\r\nThe Pytest issues that have been fixed are all Windows-related as far as I know. Not sure if you are on Windows or not."],"labels":["maintenance"]},{"title":"MAINT: investigate potential file descriptor leaks","body":"Following gh-19842 and gh-19844, it was reported in gh-19844 for there are still some potential file descriptor leaks found with the following hack:\r\n\r\n```\r\nfrom test.support.os_helper import fd_count\r\n\r\n@pytest.fixture(autouse=True)\r\ndef fd_leak():\r\n    before = fd_count()\r\n    yield\r\n    assert fd_count() == before\r\n```\r\n\r\nIt would be good to see whether any of these can be patched up, as it can cause issues when garbage collection is less aggressive.","comments":[],"labels":["task","maintenance"]},{"title":"How to deal with large ODE systems with solve_ivp","body":"Hi,\r\n\r\nI have an ODE system with 42 equations and 101 parameters. Solving this model with  \r\n**solve_ivp(model, time_span, y0, args, dense_output=True, data_point=250, rtol=1e-6, atol=1e-9)** cost around 20 seconds.\r\n\r\nAnother system has 70 equations and 161 parameters.  This model cannot be solved with either the above command or **solve_ivp(model, time_span, y0, args)**. The program had been running for more than 80 minutes so I stopped it. \r\n\r\nDuring the solving, no message or error was prompted. The models should be correct because they were transformed from a MATLAB program, and can be solved with a MATLAB APP. \r\n\r\nAny suggestions for dealing with large ODE systems with solve_ivp?","comments":["The first step is to profile the code to see where the time is spent. Now that you have a real world case, it'd be great if you could profile it, actually. ","Which MATLAB solver did you use? Is your problem stiff?","You need to know the system better to optimise your integration. Is the system stiff ? (Hint: which MATLAB integrator did you use previously ?) If so, is the Jacobian banded or dense ? Is the system singular at one point in the integration? Is the model correctly implemented ? Do you have non-vectorised loops that are very costly?\r\nWithout this information, not much can be advised.\r\n\r\nBy the way, this more a question for stackoverflow (or its computational science equivalent \"scicomp exchange), since this is most likely not an issue with Script itself.\r\n\r\n\"Data_point\" is not an argument recognised by solve_ivp.\r\nAlso a system of size 100 is by no means large, even for a Python-based solver ;)","Thanks @ev-br , I will go to profile my code later.\r\n\r\nThanks @j-bowhay and @laurent90git. The model I posted was sourced from a project file of MATLAB SimBiology APP  and the ODE solver is ode15s (stiff\/NDF) (https:\/\/www.mathworks.com\/help\/simulink\/gui\/solver.html). I think you are right, my model should be stiff. The ode15s (stiff\/NDF) is suitable for stiff equations, and if I use another solver not tailored for the stiffness, the model cannot be solved. \r\n\r\n> \"Data_point\" is not an argument recognised by solve_ivp.\r\nSorry, I made a typo here.","Which method in SciPy were you using? The default RK45 isn't well suited to stiff problems and generally will require a prohibitively small timestep. Try using LSODA\/BDF\/Radau. I have used these for 500+ dimension stiff problems with no issue"],"labels":["scipy.integrate","query"]},{"title":"ENH: sparse: Add csr 1d sparse arrays and test_arithmetic1d","body":"Adding sparse array support within the csr code allows us to use the 2d sparsetools routines for csr with 1d sparse arrays. While 1d arrays do not have a compressed index, we can construct `indptr = np.array([0 nnz])` and use the sparsetools functionality as if the 1d array were a 2d array. Reshaping the result where needed works well.  The other importance of csr format is that many features in other formats are funneled to csr because that handles most everything. So we should be able to start enabling some of the features for sparse arrays that are waiting for 1d support after this PR. I envision additional formats as a special feature rather than an essential part of the 1d support process. We are getting close to full functionality.\r\n\r\nThis PR also introduces tests of 1d arithmetic taken from the test_base.py arithmetic tests for 2d, converted to 1d and modernized to use fixtures and pytest parametrize rather than subclassing test classes for the different formats. This PR includes the tests of arithmetic without including the tests of various data dtypes, as those are presumably tested well in 2d and the code is essentially the same for 1d and for 1xN 2d. \r\n\r\nIndexing beyond integers (and tests of such) are not implemented for csr yet (or dok in #19715). \r\n\r\nIt is not clear whether it makes sense to use csc-1d. A 1d array is essentially a row. Using csc with this formulation means that indptr has N+1 elements. So, unless the indptr dtype is quit small, the array takes more RAM than a dense array. We could instead treat the csc-1d as a single column. But this breaks the usual symmetries between CSR and CSC. Converting between csr and csc would simply be a copy of the data and change of class. Another alternate implementation would be to make a 1d format that combines features of coo, csc, csr and leaves out support for 2d.\r\nBut I think this is likely to be the best way forward. So see what you think...\r\n\r\nThis PR is built on #19853 (base-1d). So it is independent of the dok-1d branch and the 3 PRs should be easier to review. Looking at it now will show files _base.py and test_common1d.py which are actually included in the base-1d PR. \r\n","comments":["`A1d @ A1d` in NumPy returns a scalar.  But the array-api says it should return a 0-dim array. \r\n\r\nWhich should sparse arrays return?\r\nI think they should match the array-api (and presumably numpy will eventually change to). I've made this PR return 0-dim arrays in that case. \r\n\r\nWhile adding tests, I found a broken behavior for this PRs 1d sparse arrays when they appear on the right of `@`.  Their shape_as_2d is `(N, 1)` and correctly indicates a single column, but they were still storing a row in csr_array format. I have corrected this PR to correctly handle the \"column\" by converting the \"row\" to 2d and transposing (which makes it a `csc_array`).  But that means we have to construct a csr_array for a column vector, which can explode memory for large column vectors as `indptr` will need `M+1` elements when \"@\" multiplying a CSR row by a CSC column.\r\n\r\nThis is a problem for the 2d case (spmatrix or sparray) also.  So I'm not going to fix that here. \r\n\r\nTo fix this we need a CSR @ CSC sparsetools function (we currently have CSR @ CSR). I have found an algorithm for that and will make a separate PR. We should be able to multiply sparse 1d arrays with length N while avoiding a csr column that requires an (N+1)-vector for `indptr`. Similar issues for 2d arrays that are very wide multiplying arrays that are very tall.","Array scalars kinda duck-type 0-D arrays. The discussion at https:\/\/github.com\/numpy\/numpy\/pull\/25542#issuecomment-1909047579 clarifies that returning array scalars therefore does not actually break array API compatibility. NumPy may or may not remove scalars in the future; it'd be a big operation and if this happens it'll be another major release, so at least several years away.\r\n\r\nI'd say 0-D arrays are preferred for new code. For existing code, we should pay attention to API consistency as well - if there's 10 functions\/methods and 9 already return scalars, then it's better for the 10th one to do the same."],"labels":["enhancement","scipy.sparse"]},{"title":"RFC: switch documentation to markdown, remove rst (over time)","body":"To start: I'm willing to write more detailed mailing list posts or proposals, but I first wanted to gauge the temperature so to speak, if this is something people would be interested in at all.\r\n\r\nTL;DR: Leaving aside the historical reasons for how things came to be, from my POV, reStructured Text (reST, RST) only has disadvantages over Markdown (more precisely, Markdown + MyST[^1], which we're already using in part). We should move to Markdown.\r\n\r\n## Disadvantages of RST\r\n\r\n### Formatting divergence from GH\r\n\r\nEven if we were to brush aside the following weaknesses of RST, one key point from a DX point of view is that SciPy lives on Github, where contributors constantly use (github-flavoured) Markdown in all issues & PRs.\r\n\r\nThis duplication of formatting rules creates an IMO massive overhead, in ensuring that links, code, etc. is formatted correctly. For example, people _constantly_ get confused between github's <code>\\`code\\`<\/code> and rst's <code>\\`\\`code\\`\\`<\/code>, and our docs & docstrings are littered with wrongly formatted strings due to that.\r\n\r\n### Horrible link format\r\n\r\nRST has a very problematic [format](https:\/\/www.sphinx-doc.org\/en\/master\/usage\/restructuredtext\/basics.html#external-links) for links\r\n```\r\n`text <url>`_\r\n     ^\r\n     significant whitespace\r\n```\r\nAside from being hard to remember (compared to markdown's `[text](url)`), this can break in various ways (especially where spaces are involved). It also _further_ overloads the backticks, and leads to anti-patterns like [`toolchain.rst`](https:\/\/github.com\/scipy\/scipy\/blob\/main\/doc\/source\/dev\/toolchain.rst) using footnotes for links, instead of simple hyperlinks in-text.\r\n\r\n### RST is indentation-heavy and git-history-unfriendly\r\n\r\nI wanted to hide some text in a page behind a [foldout](https:\/\/sphinx-design.readthedocs.io\/en\/latest\/dropdowns.html), and this would be how to do it in markdown (+sphinx-design)\r\n```diff\r\n+:::{dropdown} Dropdown title\r\n Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\r\n nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\r\n Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu\r\n fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in\r\n culpa qui officia deserunt mollit anim id est laborum.\r\n+:::\r\n```\r\nIn RST, I'd have to do:\r\n```diff\r\n-Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n-incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\r\n-nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\r\n-Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu\r\n-fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in\r\n-culpa qui officia deserunt mollit anim id est laborum.\r\n+.. dropdown:: Dropdown title\r\n+\r\n+    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n+    incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\r\n+    nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\r\n+    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu\r\n+    fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in\r\n+    culpa qui officia deserunt mollit anim id est laborum.\r\n```\r\nwhich would change the indentation of \"Lorem ...\", and thus bust the git history. Obviously this is highly undesirable, if we're only putting a paragraph or two behind a fold-out (and not rewriting anything of the content).\r\n\r\nThis is not an isolated but an illustrative example; RST generally uses indentation blocks (with special introductory markers), while markdown will add something surrounding (e.g. triple-backticks).\r\n\r\n### Saner syntax overall\r\n\r\nThis is obviously more subjective, but extends to various other bits (e.g. footnotes). This arguably leads to a lower barrier to entry (easier to grasp) and makes our contribution experience more inclusive. To make this a bit less subjective, here's a [quote](https:\/\/github.com\/pydata\/pydata-sphinx-theme\/issues\/1475#issuecomment-1737142031) from one of the pydata-sphinx-theme maintainers - as a hopefully more relevant example than my own opinion:\r\n> While I have been using `rst` for many years, I do have to recognise that the `md` is more intuitive (this is my own perception because despite having used rst for many years, I need to keep revisiting the cheatsheet all the time). Newer contributors seem to be more familiar with `md`, so the contribution barrier from this POV is lower.\r\n\r\n### Popularity\r\n\r\nWhile it's perhaps more of a reflection of my own biases than objective truth, I also have the impression that there are many people with a strong [dislike](https:\/\/stackoverflow.com\/questions\/2471804\/using-sphinx-with-markdown-instead-of-rest) of rst, and I don't see nearly as much as that aimed at markdown (except the zoo-of-flavours problem). I'm also aware of several projects wanting to move from rst to myst (example in the pydata universe: https:\/\/github.com\/pydata\/xarray\/issues\/7924).\r\n\r\n## Disadvantages of Markdown+MyST\r\n\r\n### Docstrings\r\n\r\nThe main problem is see is that markdown in docstrings is not yet as well-supported as rst, and unless we can overcome that, we're still stuck with two divergent formats, and all the headaches that entails. From what I can tell from e.g. https:\/\/github.com\/executablebooks\/MyST-Parser\/issues\/228, it's possible already though, and [documented](https:\/\/myst-parser.readthedocs.io\/en\/latest\/syntax\/code_and_apis.html#documenting-whole-apis) in myst as such.\r\n\r\n### XYZ?\r\n\r\nOther than that, I don't know of any big blockers (especially nowadays, and especially for the needs of SciPy), but that might be my ignorance - please correct me if I'm wrong. One of the biggest missing features AFAIU (cross-references) has been [fixed](https:\/\/myst-parser.readthedocs.io\/en\/latest\/syntax\/cross-referencing.html) by myst, and given the integration with all our other doc infra (sphinx etc.) things are very different than what they used to be in that respect.\r\n\r\n## Considerations for possible migration\r\n\r\nFor plain documentation:\r\n* sphinx, sphinx-design and pydata-sphinx-theme all support both formats\r\n* The generated HTML is equivalent\r\n* AFAIU it should be possible to use both `.rst` and `.md` in the same repo, enabling a gradual migration page-py-page (if we want)\r\n* There's [tooling](https:\/\/rst-to-myst.readthedocs.io\/) to do an automated conversion (if we want)\r\n\r\nFor docstrings:\r\n* TBD\r\n\r\nCC @tupui @rgommers \r\n\r\n[^1]: markedly structured text","comments":["Enthusiastic support from me. I can't take initiative toward it, but I can help with menial tasks.","The biggest problem with docs I think isn't reST, but Sphinx. If we could get rid of Sphinx in favor of (probably) MkDocs, that'd be an absolutely massive gain. I've investigated that before, and the only real problem is autodoc - the [mkdocstrings](https:\/\/github.com\/mkdocstrings\/mkdocstrings) parser couldn't handle dynamically generated docstrings last time I checked.\r\n\r\nFor narrative docs, switching to Markdown + MyST seems fine to me if someone wants to put in the effort. For docstrings I don't see the benefit, it'd be a ton of churn for zero real gain (and moving away from the well-established numpydoc format, so maybe a negative impact).","I'm +1 for converting the tutorials, conditional on clarifying several details on format, tooling and the transition plan.\r\n\r\nTooling:\r\n1. The main concern is the toolchain stability. Jupyter\/MyST MD ecosystem is still less established and is moving quite a bit faster than Sphinx. Are we aiming at a moving target? What are stability expectations in general? \r\n\r\nFormat:\r\n1. does the md\/myst-md handle cross-references? This includes both in-document cross-refs, md-to-md and md-to\/from-docstring cross-refs.\r\n3. does it support end notes or other form of references grouped at the end of a section or a document? \"just hyperrefs\" don't quite cut it, we need to be able to use a reference mutliple times in a document.\r\n4. math. I suppose TeX notation works, can we have numbered equations?\r\n\r\nTransition plan:\r\n1. How long do you think it would take to convert all of the tutorials?\r\n2. If we won't make it within a single release window, how do we handle a mix of rst and md?\r\n3. What are the release process changes?\r\n\r\nAnd of course, this needs a champion.  Are you volunteering to spearhead this @h-vetinari ? :-)). Like Matt, I'm ready to help with partial tasks.\r\n\r\n\r\n"],"labels":["Documentation","DX","RFC"]},{"title":"MAINT: follow-ups to gh-19724 (pytest pins, temporary CI changes, special ufunc type coercion)","body":"Some reminders after merging gh-19724, if only for myself...:\r\n\r\n- [ ] the hard pin of `pytest` version used by `cibuildwheel` should probably be reverted once the `pytest` 8 release candidate issues for Windows + Python < `3.11.0` + NumPy pre-release are resolved\r\n- [ ] we should try to figure out if there's an issue on our end for the above `pytest` RC + Windows + NumPy pre-release + Python < `3.11.0` matter, or if we just need to wait for a fix from https:\/\/github.com\/pytest-dev\/pytest\/issues\/9765 (or some combination of fixes on both ends)\r\n- [ ] when NumPy `2.0.0` is out, we can of course restore build isolation and turn off the pre-release package pull-ins for both the `sdist` CI job that was modified in above PR, and for our regular wheel builds\r\n- [ ] the `special` `ufunc` input `long` type shims may require a revisit (see comment from Albert in above PR), but were effectively considered \"ok\" in the short term; need to exercise caution","comments":[],"labels":["maintenance","CI"]},{"title":"BUG: In RPFInterpolator, wrong warning message if degree=-1","body":"### Describe your issue.\r\n\r\nIn the RBFInterpolator class, the optional parameter degree has this description:\r\n\r\n    degree : int, optional\r\n        Degree of the added polynomial. For some RBFs the interpolant may not\r\n        be well-posed if the polynomial degree is too small. Those RBFs and\r\n        their corresponding minimum degrees are\r\n\r\n            - 'multiquadric'      : 0\r\n            - 'linear'            : 0\r\n            - 'thin_plate_spline' : 1\r\n            - 'cubic'             : 1\r\n            - 'quintic'           : 2\r\n\r\n        The default value is the minimum degree for `kernel` or 0 if there is\r\n        no minimum degree. Set this to -1 for no added polynomial.\r\n\r\nSo, degree can be set to -1, to avoid the added polynomial.\r\nIn the code, inside __init__, a check on degree value is done:\r\n\r\n```\r\n        min_degree = _NAME_TO_MIN_DEGREE.get(kernel, -1)\r\n        if degree is None:\r\n            degree = max(min_degree, 0)\r\n        else:\r\n            degree = int(degree)\r\n            if degree < -1:\r\n                raise ValueError(\"`degree` must be at least -1.\")\r\n            elif degree < min_degree:\r\n                warnings.warn(\r\n                    f\"`degree` should not be below {min_degree} when `kernel` \"\r\n                    f\"is '{kernel}'. The interpolant may not be uniquely \"\r\n                    f\"solvable, and the smoothing parameter may have an \"\r\n                    f\"unintuitive effect.\",\r\n                    UserWarning, stacklevel=2\r\n                )\r\n```\r\n\r\nIf degree is set to -1, the warning message is always displayed, which is misleading.\r\nTo me, it should be displayed only if degree is not -1, i.e.:\r\n\r\n```\r\n        min_degree = _NAME_TO_MIN_DEGREE.get(kernel, -1)\r\n        if degree is None:\r\n            degree = max(min_degree, 0)\r\n        else:\r\n            degree = int(degree)\r\n            if degree < -1:\r\n                raise ValueError(\"`degree` must be at least -1.\")\r\n            elif -1 < degree < min_degree:\r\n                warnings.warn(\r\n                    f\"`degree` should not be below {min_degree} when `kernel` \"\r\n                    f\"is '{kernel}'. The interpolant may not be uniquely \"\r\n                    f\"solvable, and the smoothing parameter may have an \"\r\n                    f\"unintuitive effect.\",\r\n                    UserWarning, stacklevel=2\r\n                )\r\n\r\n```\r\n\r\nIs this correct? If so, can the code be fixed to avoid the spurious warning?\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\n# import matplotlib.pyplot as plt\r\nfrom scipy.interpolate import RBFInterpolator\r\nfrom scipy.stats.qmc import Halton\r\n\r\nrng = np.random.default_rng()\r\nxobs = 2*Halton(2, seed=rng).random(100) - 1\r\nyobs = np.sum(xobs, axis=1)*np.exp(-6*np.sum(xobs**2, axis=1))\r\n\r\n# xgrid = np.mgrid[-1:1:50j, -1:1:50j]\r\n# xflat = xgrid.reshape(2, -1).T\r\n\r\nrbfint = RBFInterpolator(xobs, yobs, kernel=\"linear\", degree=-1)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n.venv\/lib\/python3.9\/site-packages\/scipy\/interpolate\/_rbfinterp.py:346: UserWarning: `degree` should not be below 0 when `kernel` is 'linear'. The interpolant may not be uniquely solvable, and the smoothing parameter may have an unintuitive effect.\r\n  warnings.warn(\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.26.2 sys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-jptl773x\/overlay\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp39-cp39\/bin\/python\r\n  version: '3.9'\r\n```\r\n","comments":["Would you be interested in sending a pull request with your fix @fraf1?"],"labels":["defect","scipy.interpolate"]},{"title":"ENH: Changed LinearOperator that exception message","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nFixes: #18140 \r\n#### What does this implement\/fix?\r\nChanged exception message when multiplying LinearOperator from the right \r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":[],"labels":["scipy.sparse.linalg","scipy.sparse","maintenance"]},{"title":"BLD: Add Accelerate support for macOS 13.3+","body":"I added a script ([scipy\/_build_utils\/_generate_blas_wrapper.py](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-b9fb73b1113defdbecdd9276cd1a893eac534c6b47a53e3512195ee85a2f6203)) to generate C wrappers for Accelerate BLAS\/LAPACK. These wrappers are compiled into a static library that other modules can link against with no code modifications (e.g. a call to `dgemm` in a Fortran module will be routed through a wrapper and turned into a call to `dgemm$NEWLAPACK`). I also added Accelerate support to the script ([scipy\/linalg\/_generate_pyx.py](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-23487f1e98e4e3ec6489ff8fa3919ba87494ddfb3dea4af817b9c61de6c41c52)) that generates the Cython for `_fblas`\/`_flapack` using similar C wrappers. In the process of making these two changes, I included special logic to handle the G77 ABI wrappers, eliminating the need for additional Fortran wrappers.\r\n\r\nOther changes:\r\n- Functions in `_fblas`\/`_flapack` that have a return value were written as such (previously, f2py generated Fortran wrappers for these)\r\n  \r\n  - UPDATE: Reverting this change because the intermediate Fortran wrappers are needed to guarantee compatibility of complex return types, without which segfaults can occur on x86 machines (#19855)\r\n  - UPDATE 2: Brought this change back to avoid [erroneous F2PY boilerplate](https:\/\/github.com\/scipy\/scipy\/pull\/19816#issuecomment-1970338432) that referenced incorrect BLAS\/LAPACK symbols. Fortran subroutine wrappers were translated into C and appended to G77 ABI wrapper file (#20232)\r\n\r\n- Resolved missing prototype build warnings in [scipy\/integrate\/lsoda.pyf](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-bcfb240e4cf0b9546fdc187135f5e1711944e85a992c31e5b8057d62b3d10819) and [scipy\/integrate\/vode.pyf](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-3a625b9354fc6062bb9c6c0d28d527e376da21034e7cd296e8754afb63cf9051)\r\n\r\n  - UPDATE: Separated into #19821\r\n \r\n- Added test for `cython_blas.cladiv` and `cython_blas.dladiv`\r\n\r\n  - UPDATE: Separated into #19820\r\n \r\n- Declared separate `blas_dep` and `lapack_dep` with necessary compile arguments, leaving `blas` and `lapack` untouched for more useful information when calling `scipy.show_config()`\r\n- Resolved function with no return value build warnings and remove custom G77 wrappers in PROPACK (depends on [separate PR](https:\/\/github.com\/scipy\/PROPACK\/pull\/7) on the PROPACK repo)\r\n\r\n  - UPDATE: Separated into #19855\r\n\r\n#### Downsides\r\nThe wrapper functions are all in separate source files, allowing the linker to exclude unused symbols and keep binary sizes about the same as they currently are. Since there are ~1600 BLAS\/LAPACK functions, this increases build time by a noticeable amount. I tried writing everything to a single file and compiling with `-ffunction-sections -fdata-sections -Wl,-dead_strip`, but this still resulted in unused symbols and larger binaries.\r\n\r\nThere is no way to mix and match BLAS\/LAPACK libraries at the moment (also a challenge to support both LP64 and ILP64).\r\n\r\n#### Future\r\nThis wrapper generation framework makes it pretty easy to add ILP64 support. If this PR seems OK in principle, I can make a separate PR detailing some quirks I encountered trying to implement ILP64 support with this as a base.\r\n","comments":["Thanks @thalassemia this looks like a lot of effort so thank you for that. My initial impression is that this is way too Accelerate specific and treats the design as \"everything else vs Accelerate\". I think if we are to handle this we have to make Accelerate the special case. \r\n\r\nCould you please elaborate on exactly what Accelerate needs to work and what we are not providing? A mailing list post is also fine if you prefer to write there for a broader audience.\r\n\r\nAs a side note, I don't know what to think about Accelerate yet; we have been bitten by it before massively and I am really hesitant to re-enable it regardless of what Apple paints itself to be this time around. Last time it took a few years to arrive at a conclusion and here is the resulting discussion https:\/\/github.com\/scipy\/scipy\/wiki\/Dropping-support-for-Accelerate","Also relevant is that @rgommers is working on re-enabling Accelerate support. I believe most of the roadblocks that we were experiencing have been ameliorated on macOS >= Sonoma.X.\r\n\r\nHaving said that, the rest of Ilhan's points still apply.\r\n","We are missing a way to link the BLAS\/LAPACK calls scattered throughout the codebase to the non-standard symbols provided by Accelerate. These non-standard symbols are of the format `{lowercase name}$NEWLAPACK{$ILP64}`. This is problematic because many Fortran compilers mangle names by appending an underscore and converting everything to lowercase. This prevents us from simply using direct string substitutions in the relevant Fortran code. \r\n\r\nI considered writing Fortran interfaces with `bind(c)` and `iso_c_binding` but deemed it too much of a hassle since `cython_blas\/lapack_signatures.txt` lacks the dimensions of array arguments, something I would need to generate the interfaces. Additionally, SuperLU, a C library, also links against BLAS\/LAPACK, so I thought going with C wrappers was the better idea all around.\r\n\r\nIMO, this PR already treats Accelerate as an exception to the rule, setting extra flags in `meson.build` files that funnel the wrapper-generating scripts into specific `if` blocks when it is requested. The build simply skips much of the new code when a BLAS\/LAPACK library with standard symbols (e.g. LP64 OpenBLAS) is requested. Additionally, this extra logic to handle unconventional symbol names can be easily adapted to add ILP64 support across the board, providing a way to append `64_` to all BLAS\/LAPACK calls.\r\n\r\nThanks for the link to the Accelerate discussion. In light of Apple's recent overhaul of Accelerate to align with LAPACK 3.9.1 and include an ILP64 interface, I think that now is a good time to bring back support just as Numpy recently did. The performance advantage for certain operations is fairly compelling.","When we re-add Accelerate support any existing functionality will be available. Ralf will be able to chip in here, but I think his work is going to take care of all the name mangling issues.","> In light of Apple's recent overhaul of Accelerate to align with LAPACK 3.9.1 and include an ILP64 interface, I think that now is a good time to bring back support just as Numpy recently did. \r\n\r\nNote that 3.9.1 is from 3 three years ago already, and this precise reason got us into that previous mess. I don't want to commit to the Accelerate framework. Or put differently, we should not be the compatibility seeker and if things break for Accelerate users they should bother Apple and not us, (in English, it won't get fixed). This blocked SciPy for years already. I don't want this to hamper SciPy. It has already happened once and I see no reason for trusting Apple again. So if it works with Accelerate that's awesome but if it doesn't work then we should not be forced to *support* it. \r\n\r\n> The performance advantage for certain operations is fairly compelling.\r\n\r\nThe performance comes from BLAS and not LAPACK parts. LAPACK is pretty much high level code that uses BLAS ops. Hence if BLAS gets faster so do LAPACK funcs.\r\n\r\nIn the meanwhile, C++ standard accepted std::linalg for C++26, so, while there is so many things to complain about C++, I'll be the first one to hop on board to that train if it materializes.\r\n\r\nhttps:\/\/www.youtube.com\/watch?v=-UXHMlAMXNk (typical horrific naming choices but still, it's `stdlib`)\r\n\r\n","As a separate comment, PROPACK will be rewritten in Python parts for v.1.13 hence that won't be too much of a problem for BLAS\/LAPACK inclusions. ","Thanks a lot for this PR @thalassemia, this will be really helpful!\r\n\r\nLet me first address the below, and then do some testing and come back with more in-depth review comments.\r\n\r\n> Note that 3.9.1 is from 3 three years ago already, and this precise reason got us into that previous mess. I don't want to commit to the Accelerate framework. Or put differently, we should not be the compatibility seeker and if things break for Accelerate users they should bother Apple and not us, (in English, it won't get fixed). This blocked SciPy for years already. I don't want this to hamper SciPy. It has already happened once and I see no reason for trusting Apple again. So if it works with Accelerate that's awesome but if it doesn't work then we should not be forced to support it.\r\n\r\nI was just working on the exact same thing as @thalassemia, because:\r\n- Accelerate is way faster than OpenBLAS (see, e.g., benchmarks on https:\/\/github.com\/numpy\/numpy\/pull\/24053),\r\n- Accelerate would allow us to ship way smaller wheels for macOS >=14 (NumPy wheels got ~3x smaller, see https:\/\/github.com\/numpy\/numpy\/pull\/25255)\r\n- all known problems we had are solved,\r\n- importantly, we now get responses from Apple and even active engineering help (BLAS\/LAPACK, and also with compiler issues): https:\/\/github.com\/numpy\/numpy\/pulls?q=is%3Apr+developer-ecosystem-engineering+author%3Adeveloper-ecosystem-engineering+is%3Aclosed\r\n\r\nIn case we find ourselves in a situation like before again several years down the line, we can always drop support again. But right now there are only upsides to supporting Accelerate, and I don't see any reason not to.\r\n\r\nNote that this PR looks complex, but there isn't much in it that we don't also need in some form for ILP64 support.","After some initial testing it looks like everything works as advertised with Accelerate. The binary size looks fine (only a ~30 kb increase from the wrappers, while dropping vendoring of OpenBLAS gains ~7 MB).\r\n\r\nThe segfault on the 32-bit Linux CI job looks real.\r\n\r\n> I created union types to perform the conversion assuming that Numpy and Cython use the same memory structure for their complex types. I don't know whether it is safe to make this assumption or how else to handle this.\r\n\r\nYes, memory layout is the same and using unions is a common way to deal with complex types.\r\n\r\n> Resolved function with no return value build warnings and remove custom G77 wrappers in PROPACK (depends on [separate PR](https:\/\/github.com\/scipy\/PROPACK\/pull\/7) on the PROPACK repo)\r\n\r\nThis is really nice. I actually thought that it wasn't possible - but my memory is very hazy there, and if it works it works!\r\n\r\n\r\n\r\n> Functions in `_fblas`\/`_flapack` that have a return value were written as such (previously, f2py generated Fortran wrappers for these)\r\n> \r\n> Resolved missing prototype build warnings in [scipy\/integrate\/lsoda.pyf](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-bcfb240e4cf0b9546fdc187135f5e1711944e85a992c31e5b8057d62b3d10819) and [scipy\/integrate\/vode.pyf](https:\/\/github.com\/scipy\/scipy\/compare\/main...thalassemia:accelerate?expand=1#diff-3a625b9354fc6062bb9c6c0d28d527e376da21034e7cd296e8754afb63cf9051)\r\n> \r\n> Added test for `cython_blas.cladiv` and `cython_blas.dladiv`\r\n\r\nThese are all useful changes, thanks. Would you be able to submit these as separate PRs? Then we can get those merged straight away, which is helpful both in itself and to reduce the size of the diff in this PR. \r\n\r\nThe removal of the `wrap_g77_abi_*` wrappers could also be done independently of the code for BLAS symbol redirection, right? If so, that'd be great to do as a separate PR too. It can then still be tested merged with this one, and also with MKL (uses F77 ABI too).\r\n\r\n> There is no way to mix and match BLAS\/LAPACK libraries at the moment (also a challenge to support both LP64 and ILP64).\r\n\r\nI have a WIP branch with a set of CI jobs where this is possible - at least different BLAS and LAPACK libraries (e.g., there's a job with `blas=blis lapack=openblas`): https:\/\/github.com\/rgommers\/scipy\/tree\/ci-blas. \r\n\r\nWe had ILP64 support for OpenBLAS only in the `numpy.distutils` based build before. I was just in the process of resurrecting that support. The way it worked, unfortunately, is to link against both an LP64 and an ILP64 build of OpenBLAS. That is really ugly, and hard to even set up a test environment for. I'm not sure we should allow that again (it's unclear if anyone used that even). Both MKL and Accelerate provide a single library with two sets of symbols in them - that's much cleaner. OpenBLAS could be built like that as well in the future (reference BLAS\/LAPACK too).\r\n\r\nOne thing I noticed in this PR is that the wrapper libraries are also built if no symbol prefix\/suffix is needed. The way the old ILP64 support did that was a little different: it would write out a single header file (`blas64-prefix-defines.h`) and then rewrite all Fortran source files for build targets that depend on BLAS or LAPACK to include that header, if and only if a symbol suffix was needed. I was halfway through copying that approach into the Meson build. This approach seems to take about 20 seconds of extra build time on my macOS machine (2m 31s -> 2m 51s), so a ~10% increase. I suspect rewriting all the source files will be quite a bit faster, with fewer additional build targets.\r\n\r\nI'd be curious about your opinions on the two topics in the two paragraphs above.","> In case we find ourselves in a situation like before again several years down the line, we can always drop support again. But right now there are only upsides to supporting Accelerate, and I don't see any reason not to.\r\n\r\nTrue but keep in mind that we use LAPACK and NumPy does not and that's where the most of development happens. Reference BLAS has, for example, added many safety features as per Anderson's paper in 3.10 and I don't see any updates mentioned in any NumPy issues as far as I scrolled. So they have LAPACK 3.2.1 and 3.9.1 and who knows what now probably 3.20.1 next. \r\n\r\nHence it is not that optimistic in my opinion and dropping and acquiring Accelerate support is really not very productive as they release a new version. But like I mentioned above, if this is not going to be a dependency I'm +1. If we are going to stop and wait until Accelerate catches up for years then -1.","I took a look at the `numpy.distutils` approach. Making use of the preprocessor is a brilliant, virtually no-overhead way to handle the suffixes necessary for most ILP64 libraries. It's certainly far cleaner than this PR. \r\n\r\nFor Accelerate specifically, I worry that Fortran's lack of case sensitivity will complicate things. The symbols provided by the new Accelerate are mixed case, so the linker would not be able to find them. I found a compiler flag for `ifort\/ifx` (`-case as_is`) to force case sensitivity but not for modern `gfortran` (apparently the G77 version used to have `-fcase-preserve`). Adding this flag might break other things though.\r\n\r\nThanks for pointing out that the wrappers are being built whether they are needed or not. I'll change that so they are only built for Accelerate or, in the future, ILP64 builds.\r\n\r\nNow that I've thought about it slightly more, I think my approach should support mixing BLAS\/LAPACK as long as they share the same symbol names. Things might break when mixing ILP64 OpenBLAS and ILP64 MKL on certain systems given they use different compiler mangling conventions. \r\n\r\nI agree that it does not make much sense to support mixing LP64 and ILP64 libraries. In my testing, going all in on ILP64 by adding the Fortran compiler flags to make the default integer 64 bits only breaks `scipy.linalg._interpolative`, for which I needed to add a separate wrapper file. Surprisingly, the other Fortran modules just work with pretty minimal modifications (in fact, SuperLU, a C module, required the most tinkering to work).\r\n\r\nIt's great to hear PROPACK is getting converted to Python soon. Assuming the converted module accesses BLAS\/LAPACK through `get_blas\/lapack_funcs`, that should make my PR on that repo unnecessary and simplify ILP64 support in the future.\r\n\r\nI'll mull this over some more. In the meantime, I'll break the other changes out into their own PRs. The removal of the `wrap_g77_abi_*` wrappers only works because they are replaced with C wrappers generated by my new scripts so I'll leave that here for now.","I can't figure out what's causing the crash on the 32-bit build. After setting up the Docker container on my local machine, 0eac493 seemed to fix the segfault, but I also encountered test failures that did not occur here. It seems important not to mix Fortran and C wrappers when complex return values are involved. Also, when using C, the CBLAS ABI seems to be a safer bet than the Fortran one. It's confusing that issues only arise on a 32-bit OS.","> For Accelerate specifically, I worry that Fortran's lack of case sensitivity will complicate things\r\n\r\nHmm yes, that is a good point. The `blas64-prefix...` header generated all possible combinations of lower\/upper-case symbols and mapped them to all-lowercase:\r\n```C\r\n#define ccopy ccopy_64\r\n#define CCOPY ccopy_64\r\n#define cCOPY ccopy_64\r\n#define Ccopy ccopy_64\r\n#define ccOPY ccopy_64\r\n...\r\n```\r\nbut that won't help with Accelerate if the name is `ccopy_$NEWLAPACK`.\r\n\r\n> In my testing, going all in on ILP64 by adding the Fortran compiler flags to make the default integer 64 bits only breaks `scipy.linalg._interpolative`, for which I needed to add a separate wrapper file. Surprisingly, the other Fortran modules just work with pretty minimal modifications (in fact, SuperLU, a C module, required the most tinkering to work).\r\n\r\nThat's not too surprising, since they were the only two submodules that didn't support ILP64 before: https:\/\/github.com\/scipy\/scipy\/pull\/11193#issuecomment-568468741. We recently upgraded to SuperLU 6 though, and that's the first version that support ILP64 in the upstream build. I don't see any code changes for SuperLU in this PR though, other than adding the wrapper lib, so I'm curious - what was more difficult?","> It's great to hear PROPACK is getting converted to Python soon. Assuming the converted module accesses BLAS\/LAPACK through `get_blas\/lapack_funcs`, that should make my PR on that repo unnecessary and simplify ILP64 support in the future.\r\n\r\nThat may be months away, we should merge your PR and treat it as all the other Fortran code for now.","I think I've finally figured out the x86 segfault. It seems that on 32-bit machines, `_Complex` and `npy_complex64` cannot be used interchangeably as a return type. To avoid segfaulting, the Fortran modules (ARPACK, PROPACK, etc) require the standard C99 `_Complex`, but the Cython modules (`_fblas`, `_flapack`, etc) require `npy_complex64`. `npy_complex64` is probably `struct {float real,imag;}` because Cython uses a struct for its complex type (`-DCYTHON_CCOMPLEX=0`) and converting between the two with union seems to work fine.\r\n\r\nTo address this, I brought back a separate wrapper file for the BLAS\/LAPACK functions that return a complex value. By adding compile flags, I can change the complex type that is used for each build target.\r\n\r\nI also added a flag to only generate wrappers for each BLAS\/LAPACK function when `accelerate` is chosen, bringing the number of build targets and the build time back down for all other scenarios.\r\n\r\nEdit: Nevermind, now there are segfaults in other places. Hmm...\n\nEdit 2: Fixed now! Good thing I added that *ladiv test\ud83d\ude05","> We recently upgraded to SuperLU 6 though, and that's the first version that support ILP64 in the upstream build. I don't see any code changes for SuperLU in this PR though, other than adding the wrapper lib, so I'm curious - what was more difficult?\r\n\r\nUnfortunately, from what I can tell, the ILP64 support in SuperLU 6 is limited to their user-facing functions and internal array indexing. The underlying BLAS\/LAPACK calls were not updated to support ILP64. To use an ILP64 BLAS\/LAPACK, I wrote a Python script to replace all `int` and `unsigned int` across several files with `int64_t` and `uint64_t`, respectively. It worked on my machine, but this is almost certainly not a portable solution.\r\n\r\n","Since this now depends on #19855, I have made it into a draft. When that PR is ready for merging, a large chunk of the diff here should disappear, and we can proceed with iterating.","I noticed an issue with the `_fblas` and `_flapack` modules that are autogenerated by F2PY. The generated `PyInit` methods have code like the following for each `function` in the `*.pyf` signature file (example: `cdotu`):\r\n\r\n```c\r\nextern complex_float F_FUNC(cdotu,CDOTU)(void);\r\nPyObject* o = PyDict_GetItemString(d,\"cdotu\");\r\ntmp = F2PyCapsule_FromVoidPtr((void*)F_FUNC(cdotu,CDOTU),NULL);\r\nPyObject_SetAttrString(o,\"_cpointer\", tmp);\r\nPy_DECREF(tmp);\r\n```\r\n\r\n[Here is the relevant code](https:\/\/github.com\/numpy\/numpy\/blob\/7db1cc34e53e06ee2be0dab47d963789b075726f\/numpy\/f2py\/rules.py#L458) in F2PY. [According to the docs](https:\/\/numpy.org\/doc\/stable\/f2py\/python-usage.html#fortran-type-objects), the `_cpointer` attribute is meant to be used as a callback argument pointing to the wrapped Fortran\/C function.\r\n\r\nThe C code above is wrong because the wrapped function is not `F_FUNC(cdotu,CDOTU)` but `F_WRAPPEDFUNC(cdotu,CDOTU)`. This does not raise errors at runtime because Accelerate defines `F_FUNC(cdotu,CDOTU)`, a macro that evaluates to `cdotu_` (old Accelerate uses symbols of the form `name_`, new Accelerate uses `name$NEWLAPACK`). However, compiling with Accelerate results in `_fblas.cdotu._cpointer` pointing to `cdotu_` while `_fblas.cdotu` itself calls, in order:\r\n\r\n1. `F_WRAPPEDFUNC(cdotu,CDOTU)`: This macro evaluates to `f2pywrapcdotu_`, a Fortran wrapper function generated by F2PY ([`_fblas-f2pywrappers.f`](https:\/\/github.com\/scipy\/scipy\/blob\/51c5423f57d21ca193668b65287bcf00982a1aa7\/scipy\/linalg\/meson.build#L61)).\r\n2. `wcdotu_`: `f2pywrapcdotu_` calls the external function `wcdotu`, which is mangled into `wcdotu_`. `wcdotu_` is defined in the [G77 ABI wrapper](https:\/\/github.com\/scipy\/scipy\/blob\/51c5423f57d21ca193668b65287bcf00982a1aa7\/scipy\/_build_utils\/src\/wrap_g77_abi.c#L50).\r\n3. `cblas_cdotu_sub$NEWLAPACK`:  `wcdotu_` calls the C API for the new Accelerate. This is clearly different from `cdotu_`, a G77 ABI function from the old Accelerate.\r\n\r\n\r\nMaking things more complicated, `F_WRAPPEDFUNC(cdotu,CDOTU)` is not a traditional function with a return value. Instead, it returns `void` and takes its \"return\" value as its first argument. To make this work, we would need to edit the C code above to something like:\r\n\r\n```c\r\nstatic complex_float _cpointer_cdotu(F_INT* n, complex_float* x, F_INT* incx, complex_float* y, F_INT* incy) {\r\n    complex_float ret;\r\n    F_WRAPPEDFUNC(cdotu,CDOTU)(ret, x, incx, y, incy);\r\n    return ret;\r\n}\r\nPyObject* o = PyDict_GetItemString(d,\"cdotu\");\r\ntmp = F2PyCapsule_FromVoidPtr((void*)_cpointer_cdotu,NULL);\r\nPyObject_SetAttrString(o,\"_cpointer\", tmp);\r\nPy_DECREF(tmp);\r\n```\r\n\r\nI doubt that anyone is even using the `_cpointer` attribute. However, if this is not addressed, errors will be raised at runtime when we try to add support for BLAS libraries with symbol suffixes that do not also ship with suffix-free symbols (e.g. ILP64 libraries). Accelerate is an edge case that happens to build just fine.\r\n\r\nAll the comments by @h-vetinari still apply and are next on my to-do list. Thanks again for the detailed review here and on #19855!","I found a way to fix the `_cpointer` issue by using `intent(c)` statements to prevent F2PY from generating Fortran wrappers and the erroneous boilerplate quoted above (7e791fe). This does not address the underlying F2PY issue ([discussion from 2014](https:\/\/github.com\/scipy\/scipy\/pull\/4021#issuecomment-56930397)).\r\n\r\nRemoving the Fortran wrappers reintroduces the x86 segfaults caused by incompatibility between struct (e.g. `{float real, imag;}`) and C99 complex return types. Cython uses struct complex types due to `-DCYTHON_CCOMPLEX=0`, and F2PY uses struct complex types by default. However, our G77 wrappers and pure Fortran modules (e.g. PROPACK) use C99 complex types. To fix this, I wrote `wrap_g77_fortran_intermediate.f` to provide Fortran subroutine wrappers for the affected BLAS\/LAPACK functions. This file is bundled into a `*.a` with the G77 ABI wrappers (or dummy) and provides symbols of the form `{name}wrp_` for Cython\/F2PY modules (6404580).","I think that is an acceptable workaround. Frankly, I can't wait to convert PROPACK and be done with this saga. ","I've addressed @h-vetinari's comments by adding more inline documentation and refactoring both `_generate_blas_wrapper.py` and `_generate_pyx.py`. \r\n\r\nPreviously, to fix the x86 segfaults,  I let `_generate_pyx.py` and F2PY create Fortran subroutine wrappers around functions with complex return values. Unfortunately, I had to forgo the F2PY-generated wrappers to fix the `_cpointer` issue described in my last comment. In place of these wrappers, I've appended equivalent C wrappers to `wrap_g77_abi.c` and `wrap_dummy_g77_abi.c`. This let me remove all the Fortran wrapper generation code in `_generate_pyx.py` and link both Cython and F2PY modules against the G77 (or dummy) wrapper library.","Thanks again for the feedback @h-vetinari! I refactored a bit more and unified as much as I could in `_build_utils\/_wrappers_common.py`.","I can help with adding the CI entry. Does this PR successfully test locally?","Thanks @andyfaff. I think I have the CI entry set up the way I want now (did not realize that `macos-latest` was `macos-12` even though `macos-13` and `macos-14` are available). I have never run the full test suite locally before. On a macOS 13 x86 runner, there are 2 failures:\r\n\r\n- `TestInterpN.test_matrix_input[quintic]`: rel error 5.4e-7 > rel tolerance 1e-7\r\n- `TestLinprogIPSparse.test_bug_6139`: numerical issues I am not qualified to address\r\n\r\nThe Linux Meson test failures are unrelated. Not sure what's going on there.","I think it's probably better to test on `macos-14`, as eventually wheels will be deployed on macos>=14.","btw @thalassemia , feel free to save some CI resources by adding `[skip circle]` to skip the docs workflows as outlined in http:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping whilst iterating here - not a requirement at all, but we appreciate it!","I can reproduce the one macOS 14 M1 Accelerate test failure (`TestSqrtM.test_gh17918`) locally. The relevant code was recently modified in #20212. Reverting that commit fixes the issue.\r\n\r\nSince this failure is likely due to floating-point error introduced by Accelerate, I've marked this test as an xfail for now.","> I can reproduce the one macOS 14 M1 Accelerate test failure (TestSqrtM.test_gh17918) locally\n\ncc @ev-br, maybe a slight bump needed to accommodate for Accelerate?","Thanks for the ping Lucas.\r\n\r\nIf somebody has the right hardware + cycles to chase it down: \r\n\r\n```diff\r\n$ git diff\r\ndiff --git a\/scipy\/linalg\/_matfuncs_sqrtm.py b\/scipy\/linalg\/_matfuncs_sqrtm.py\r\nindex 9c84ca573b..5655726d07 100644\r\n--- a\/scipy\/linalg\/_matfuncs_sqrtm.py\r\n+++ b\/scipy\/linalg\/_matfuncs_sqrtm.py\r\n@@ -170,6 +170,7 @@ def sqrtm(A, disp=True, blocksize=64):\r\n     if blocksize < 1:\r\n         raise ValueError(\"The blocksize should be at least 1.\")\r\n     keep_it_real = np.isrealobj(A)\r\n+    breakpoint()\r\n     if keep_it_real:\r\n         T, Z = schur(A)\r\n         d0 = np.diagonal(T)\r\n\r\n```\r\n\r\nand then locally I see\r\n\r\n```\r\n173  \t    breakpoint()\r\n174  \t    if keep_it_real:\r\n175  \t        T, Z = schur(A)\r\n176  ->\t        d0 = np.diagonal(T)\r\n177  \t        d1 = np.diagonal(T, -1)\r\n178  \t        eps = np.finfo(T.dtype).eps\r\n179  \t        needs_conversion = abs(d1) > eps * (abs(d0[1:]) + abs(d0[:-1]))\r\n180  \t        if needs_conversion.any():\r\n181  \t            T, Z = rsf2csf(T, Z)\r\n(Pdb) n\r\n> \/home\/br\/repos\/scipy\/scipy\/build-install\/lib\/python3.10\/site-packages\/scipy\/linalg\/_matfuncs_sqrtm.py(177)sqrtm()\r\n-> d1 = np.diagonal(T, -1)\r\n(Pdb) n\r\n> \/home\/br\/repos\/scipy\/scipy\/build-install\/lib\/python3.10\/site-packages\/scipy\/linalg\/_matfuncs_sqrtm.py(178)sqrtm()\r\n-> eps = np.finfo(T.dtype).eps\r\n(Pdb) p d0\r\narray([ 0.06, 17.92,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06])\r\n(Pdb) p d1\r\narray([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -4.77048956e-18,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00])\r\n```\r\n\r\nThat said, I guess a conditional skip is fine if this otherwise distracts from the main push of this PR.\r\n","The `sqrtm` failure is not Accelerate-specific. Here it is failing in the wheel build jobs for macOS x86-64 with OpenBLAS 0.3.21: https:\/\/github.com\/scipy\/scipy\/actions\/runs\/8201161926\/job\/22429344365.\r\n\r\nIt's blocking rebuilding the nightlies, which we need. So I'll mark it as xfail in `main` now and will open a separate issue to fix it up.","@ev-br Here is the requested output from my M2 macOS 14 machine (0430674 from this branch):\r\n\r\n```bash\r\n> \/Users\/seancheah\/scipy\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/_matfuncs_sqrtm.py(179)sqrtm()\r\n-> needs_conversion = abs(d1) > eps * (abs(d0[1:]) + abs(d0[:-1]))\r\n(Pdb) d0\r\narray([17.92,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06])\r\n(Pdb) d1\r\narray([ 0.00000000e+00, -1.55245793e-15,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00])\r\n(Pdb) eps\r\n2.220446049250313e-16\r\n```\r\nI don't have the hardware (macOS 11, x86_64) to reproduce the failure observed in the wheel build on `main`.\r\n\r\nI also don't know anything about the underlying math, but as an idea, the following diff fixes the issue for me:\r\n```diff\r\n% git diff \r\ndiff --git a\/scipy\/linalg\/_matfuncs_sqrtm.py b\/scipy\/linalg\/_matfuncs_sqrtm.py\r\nindex 9c84ca573..e891faa44 100644\r\n--- a\/scipy\/linalg\/_matfuncs_sqrtm.py\r\n+++ b\/scipy\/linalg\/_matfuncs_sqrtm.py\r\n@@ -175,7 +175,7 @@ def sqrtm(A, disp=True, blocksize=64):\r\n         d0 = np.diagonal(T)\r\n         d1 = np.diagonal(T, -1)\r\n         eps = np.finfo(T.dtype).eps\r\n-        needs_conversion = abs(d1) > eps * (abs(d0[1:]) + abs(d0[:-1]))\r\n+        needs_conversion = abs(d1) > eps * (abs(d0[1:]) + abs(d0[:-1])).max()\r\n         if needs_conversion.any():\r\n             T, Z = rsf2csf(T, Z)\r\n```","Thanks a lot for the updates @thalassemia! I didn't go over it with a fine-toothed comb, and I don't doubt that there are details that could be polished if people feel find something they feel strongly about, but from my POV, this is now a very nice improvement to the generation infrastructure, completely aside from adding the capability to handle macOS 13.3+. \ud83d\ude4f ","In an ideal world, we'd split off the changes to the existing infrastructure into a separate PR and then add support for 13.3+ on top, but it's not a requirement from my side. It just makes history digging \/ bisecting etc. easier if we can split off refactoring from new feature additions. \ud83d\ude43 ","I'd like to branch for SciPy `1.13.0` around Sunday (17th). Let me know where this stands\/should stand with respect to merging by then vs. delaying to merge after branching.","@tylerjereddy This depends on gh-20232 but will have a much smaller diff once that is merged. I think it would be nice if @rgommers could review this in its final form before merging. To give more time for that, it would probably be safer to hold off on merging until after the branch.","I definitely want to get this into 1.13 (pending at least some review by @rgommers). ","It probably doesn't matter too much whether this gets into 1.13.0 or not, since we won't be able to ship Accelerate-based wheels in time, and conda-forge also doesn't have support yet for new Accelerate. But I'll try to re-review asap.","I rebased this onto gh-20232 and squashed everything down to two commits for clarity. The other commits were either iterative or split into separate PRs (all linked in the updated description).","Branching is imminent, so I've bumped the milestone. If there's a compelling benefit to having this in `1.13.0` we could discuss a backport, but it doesn't sound like there is."],"labels":["Build issues","3rd party binaries","CI","C\/C++","Cython","Meson"]},{"title":"DOC: Update toolchain roadmap","body":"It's been a while since https:\/\/github.com\/scipy\/scipy\/pull\/16589, and the toolchain roadmap has a bunch of things that should be updated. I'd also like to move much of the historical context into separate sections (or better: fold outs, if our docs support that).\r\n\r\n* [ ] Remove\/hide\/rewrite historical cruft\r\n* [ ] Note meson transition & requirement\r\n* [ ] Update relation to `oldest-supported-numpy` (about to be deprecated) and numpy 2.0\r\n* [ ] Update LLVM minimum version\r\n* [ ] Validate current images & compilers\r\n* [ ] Describe use\/role of openblas wheel\r\n* [ ] Update current compiler constraints (mostly C++ side)\r\n* [ ] Update fortran compiler constraints (gfortran should IMO match our GCC requirement; flang needs to distinguish llvm-flang)\r\n* [ ] Make Pythran required (c.f. https:\/\/github.com\/scipy\/scipy\/issues\/18683)\r\n* [ ] Note new Accelerate LAPACK as of MacOS 13.3\r\n* [x] Update doc dependencies (I doubt we're still compatible with matplotlib 2)\r\n* [ ] Update packaging section\r\n* [ ] Note plans to move windows builds to llvm-flang\r\n* [ ] Flesh out openmp section; refer to [pypackaging-native](https:\/\/pypackaging-native.github.io\/key-issues\/native-dependencies\/blas_openmp\/) and\/or [this](https:\/\/thomasjpfan.github.io\/parallelism-python-libraries-design\/)?\r\n* [ ] Note our submodules and relevant constraints\r\n  * For all, but boost especially\r\n* [ ] Note stdlib requirements (relevant for things like `aligned_alloc`, which we already have in use through pocketfft)?\r\n  * We have an effective glibc lower bound based which images in our CI (resp. the ones in conda-forge); glibc <2.17 had a bunch of issues in trig functions, for example\r\n  * Introduce policy for minimal macOS deployment target? (c.f. [this comment](https:\/\/github.com\/scipy\/scipy\/pull\/19811#issuecomment-1879476807))\r\n  * Windows does not support `aligned_alloc` in a sane manner (see [this comment](https:\/\/github.com\/scipy\/scipy\/issues\/19726#issuecomment-1869849094) resp. #19761), same for `<complex>` (e.g. we could document the undocumented `_CRT_USE_C_COMPLEX_H`; I have some relevant STL issue links).\r\n  * see also #18408\r\n  * Might want to write something about usage of various `atomic_*` things, where the ABI is a freaking mess everywhere.\r\n\r\nLet me know if there's something else you think should be fixed, or something above you disagree with.\r\n\r\nCC @rgommers, we might want to note some of the longer-term improvements you've been pursuing across the stack as well?","comments":["> Note new Accelerate LAPACK as of MacOS 13.3\n\nGood timing gh-19816","These all look like good topics to me, +1 for making this update.\r\n\r\n> We have an effective glibc lower bound based which images in our CI (resp. the ones in conda-forge); glibc <2.17 had a bunch of issues in trig functions, for example\r\n\r\nProbably not as strict a bound as you think - `libnpymath` fixes all the issues we know of with older `glibc` versions.\r\n\r\n> Note our submodules and relevant constraints \r\n\r\nAlso note that we plan to move them all under `subprojects` and enable linking to external versions if a distro has the right version packaged already.\r\n\r\n> Introduce policy for minimal macOS deployment target?\r\n\r\nJust a note that NumPy recently had an issue we fixed for MacPorts, which still supports as far back as macOS 10.6 PPC (yes, I know). Not worth a lot of effort obviously, but also no need to break things unnecessarily. \r\n\r\nWe can bump wheel lower bound to 10.13 or 10.15 though, if that is convenient.","> Probably not as strict a bound as you think - `libnpymath` fixes all the issues we know of with older `glibc` versions.\r\n\r\nI had forgotten about the workarounds there, but the point remains that anything below the lowest glibc version we're testing against is at a high risk of breaking, and we (IMO) shouldn't claim support below that. As of now, the lowest is glibc 2.17 through conda-forge. Actually, speaking of `libnpymath`, we should probably note https:\/\/github.com\/scipy\/scipy\/issues\/17498.\r\n\r\n> Also note that we plan to move them all under `subprojects` and enable linking to external versions if a distro has the right version packaged already.\r\n\r\nThat sounds awesome! I see now that I had missed #17751.\r\n\r\n> We can bump wheel lower bound to 10.13 or 10.15 though, if that is convenient.\r\n\r\nYeah, 10.15 would be convenient for `aligned_alloc` support. 10.14 might work as well. The DL numbers speak a pretty clear language though (it's <<1% for pillow; we could do the [analysis](https:\/\/discuss.python.org\/t\/moving-packaging-and-installers-to-macos-10-13-as-a-minimum\/31907\/5) for scipy as well of course).","> @h-vetinari: CC @rgommers, we might want to note some of the longer-term improvements you've been pursuing across the stack as well?\r\n\r\nA few other things that come to mind, WDYT?\r\n* meson support for various BLAS\/LAPACK implementations (https:\/\/github.com\/mesonbuild\/meson\/pull\/10921)\r\n* Stuff around ILP64 (and the rest of #16293)\r\n* getting all the LAPACK implementations to support side-by-side installation of those symbols\r\n* cross-compilation support (#14812)\r\n* #16352\r\n* LTO? #16098\r\n* SIMD things (#16984, #17111), or even runtime dispatch like numpy?\r\n* more platform support? ([pyodide](https:\/\/github.com\/scipy\/scipy\/issues\/17413) [risc-v](https:\/\/github.com\/scipy\/scipy\/issues\/19378), loongarch?)\r\n* removing old fortran bits (#18566)\r\n* switching in modern fortran libs (e.g. #18118)","> I'd also like to move much of the historical context into separate sections (or better: fold outs, if our docs support that).\r\n\r\n@tupui, since you were working on the docs a lot, could you let me know if there's a way to do this, e.g. something like the following:\r\n\r\n<details>\r\n<summary>A foldout<\/summary>\r\n\r\n```\r\nSurprise!\r\n```\r\n\r\n<\/details>\r\n\r\nThe only related mention I found is #18300, but I couldn't tell how this was done. I found a related sphinx [extension](https:\/\/sphinx-toolbox.readthedocs.io\/en\/stable\/extensions\/collapse.html), any blockers to using that?","What about this? Looks like what you are looking for? \nhttps:\/\/pydata-sphinx-theme.readthedocs.io\/en\/stable\/user_guide\/web-components.html#dropdowns","> What about this? Looks like what you are looking for?\r\n\r\nYeah, looks great, thanks a lot! I guess I'll wait for your pydata-spinx-theme PR to land (oldest theme docs are 0.11, so I don't know if dropdowns are supported in our currently-used 0.9)","> A few other things that come to mind, WDYT?\r\n\r\nI suggest being careful here in keeping the split between the regular roadmap and the toolchain roadmap. The latter is about dependencies and their versions. Some of the things you're suggesting now are \"things inside SciPy to work on\" and may already be covered in the top-level or detailed roadmap, or they belong there.\r\n\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/roadmap.html#support-for-more-hardware-platforms\r\n\r\n> removing old fortran bits\r\n\r\nThis seems to me to be important enough to become a new top-level item. The bullet below it, \"switching in ...\" is not a separate target, but part of removing F77 I'd say.\r\n\r\n> meson support for various BLAS\/LAPACK implementations [...]\r\n\r\nSplitting out BLAS and LAPACK from http:\/\/scipy.github.io\/devdocs\/dev\/toolchain.html#other-libraries into their own separate section and adding more detail there would be useful I think.\r\n\r\n> cross-compilation support [...], gh-16352, LTO\r\n\r\nI'd suggest one new section about further build system improvements near the top of the detailed roadmap.\r\n\r\n> SIMD things\r\n\r\nNo concrete plans for this one, not sure if we should add it. If we do, it should be on the detailed roadmap I'd say.","> removing old fortran bits\r\n\r\nWe have some of this in the detailed roadmap: https:\/\/docs.scipy.org\/doc\/scipy-1.11.4\/dev\/roadmap-detailed.html#use-of-venerable-fortran-libraries\r\n\r\nEDIT: tangentially related, detailed roadmap might benefit from an update, too, for things which are already done or in progress, e.g. active and expired deprecations."],"labels":["Documentation","Official binaries","3rd party binaries","DX"]},{"title":"ENH:signal: Add envelope extraction","body":"Closes #19634\r\n\r\nI am looking forward to your suggestions!","comments":["Please can you send an email to the mailing list to ensure there's sufficient support for adding this to scipy","@j-bowhay I have already done it, but most of the discussion occured in https:\/\/github.com\/scipy\/scipy\/issues\/19634. Should I send an email again?","There was already [a post](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/IXA3MFKJRRYOXYLDDAZPL6D6QB4SFGEI\/) which received no replies, but received activity on the issue","Questions from the issue:\r\n\r\n> 3. I have tested all three methods locally using the examples I have provided earlier. I have also written some unit tests and \r\n> added them to my commit. These tests only check for correct inputs, and they pass. Do you have any recommendations for any other tests?\r\n> \r\n> 4. In the documentation, I give the default value for the 'analytic' method as 'x.shape[0]' based on hilbert's default. Is this good practice? Should I change the code and set N = x.shape[0] for hilbert, so that my documentation would not be effected even if hilbert's default changes.\r\n> \r\n> 5. I am currently importing find_peaks within the envelope function. Doing this outside the function throws a circular import error. Is there a better way to handle circular imports? This is also something that I am not really familiar with.","Apologies for the duplicate comments but it looks like we have pointed out mostly the same things, aside from the problem of `N` being optional only sometimes.","I need help with dealing with circular imports. Importing find_peaks anywhere other than inside the function throws an error. Has anyone had this issue before? What is the best way to solve it? For efficiency, I could also import it if method = 'peak'.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/19814\/files\/aa3a2c485ebc5bb910f3a3bfeb44a1c860b327fb#r1443470920","Update: I have solved the circular import issue by moving my function from _signaltools to _peak_finding. \r\n\r\n@ilayn Do you think this is an acceptable solution? In the documentation, should I keep 'envelope' under filtering or move it under peak finding there as well? I will move the tests to test_peak_finding.","No, it shouldn't be in `_peak_finding`. We can probably resolve the cyclic import by moving around some of the imports in `scipy\/signal\/__init__.py` such that the import from `_signaltools` follows the import from `_peak_finding`, but that may cause new cyclic imports that would cause more fiddling until a solid order is established.\r\n\r\nIMO, the local import is a fine way to resolve the circular import, but it requires a comment explaining why its there (and possibly a `# noqa` comment to silence the code linter, I'm not sure.","@rkern 'python dev.py lint' had not raised any issues with the local import.\r\n\r\n@ilayn Please let me know what you think about local import.","@lagru Could we bother you with this feature and its relation to the peakfinding in case it interests you? ","@yagizolmez while we are waiting for a reviewer, if you could try to clean up all (\/most) of the minor issues pointed out above, that will make the PR much easier to review.","@lucascolley I have a version ready where my function is under _peak_finding (resolves the local import). I have been hesitant to push it because of @rkern objection. Do you think I should push it anyways? I felt like moving the function from _signaltools to _peak_finding, and then possibly back again would be confusing. Please let me know.","> IMO, the local import is a fine way to resolve the circular import, but it requires a comment explaining why its there (and possibly a # noqa comment to silence the code linter, I'm not sure.\r\n\r\nI suggest following Robert's suggestion here (adding a comment to explain that there is a circular-dependency issue). If the `noqa` is not needed, that's fine!","@lucascolley Thanks. I will do this within the next 24 hours.","@lucascolley Thanks for your edits, I have committed them. I do not understand your last suggestion. envelope is imported in test_signaltools.py, isn't it? I might be missing something.","> I do not understand your last suggestion. envelope is imported in test_signaltools.py, isn't it? I might be missing something.\r\n\r\nSorry, I was referring to my initial set of comments above. If you look up there (or in the \"Files changed\" tab), you will see that that comment is in the thread of a suggestion (just adding the correct whitespace).\r\n\r\nOnce that is done, you can hit \"resolve conversation\" on all of my above suggestions (however don't generally do this unless someone asks you to!)","Sorry for being bit late to the discussion. I compared the results of `envelope(x, None, 'analytic')` with `envelope(x, 1, 'rms')` which IMHO should produce identical results. I obtained:\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/12721170\/d55b0b92-4871-4450-a16d-db3824eabc16)\r\nThe reason for the deviation lies in the 'rms' method: Squaring a signal made up of two `cos()` components results in the difference and the sum of the respective frequencies. In my example plot the sum exceeds the Nyquist frequency, thus producing the incorrect result through aliasing.\r\n\r\nThe code for the plot:\r\n```python\r\nT, n = 1e-3, 1000  # sampling interval, number of samples\r\nt = np.arange(n) * T  # time stamps\r\nff = (150, 200)\r\nx = sum(np.cos(2*np.pi*f_*t) for f_ in ff)\r\nf_str = \"[\" + ', '.join(rf\"${f_}\\,$Hz\" for f_ in ff) + \"]\"\r\n\r\nfg0, ax0 = plt.subplots()\r\nax0.set(title=rf\"Envelope of $x(t)=\\sum_i \\cos(2\\pi f_i t)$, $f_i=$\" + f_str,\r\n        xlabel=rf\"Time $t$ \/ s ($\\Delta t={t[1]*1e3}\\,$ms)\", xlim=(0, 0.1),\r\n        ylabel=\"$x(t)$\")\r\nax0.plot(t, x, 'C0', alpha=.5, label=\"Signal\")\r\n\r\ne_args = [(None, 'analytic'), (1, 'rms')]\r\nfor c_, args_ in enumerate(e_args, start=1):\r\n    e_u, e_l = envelope(x, *args_)\r\n    ax0.plot(t, e_l, f'C{c_}--', alpha=.5, label=f\"envelope{('x',) + args_}\")\r\n    ax0.plot(t, e_u, f'C{c_}--', alpha=.5)\r\n\r\nax0.legend(loc=\"lower center\")\r\nax0.grid()\r\nplt.show()\r\n```","I pondered about the implementation a little bit more and noticed:\r\n\r\n- In signal processing downsampling usually delivers more consistent results than a sliding square window.\r\n- The [implementation](https:\/\/github.com\/scipy\/scipy\/blob\/5e4a5e3785f79dd4e8930eed883da89958860db2\/scipy\/signal\/_signaltools.py#L2390) of the utilized [hilbert](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.hilbert.html) function is FFT-based.\r\n\r\nBy implementing a custom version of the `hilbert()` function, it is straightforward to come up with a more versatile implementation for the envelope combining the `'analytic'` and `'rms'` mode with the following functionality:\r\n- providing optional band-pass filtering on the input as well as \r\n- providing the residual signal which was not used for the envelope calculation.\r\n- Optional up- or down-sampling of the output.\r\n\r\nA rudimentary implementation looks like this:\r\n```python\r\nfrom typing import Literal\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom scipy.fft import ifft, irfft, rfft\r\n\r\n\r\ndef envelope_bp(x: np.ndarray, l0_low: int = 1, l0_high: int | None = None,\r\n                n_out: int | None = None,\r\n                residual: Literal['low-pass', 'mean', 'all'] = 'low-pass') \\\r\n                -> tuple[np.ndarray, np.ndarray | float]:\r\n    \"\"\"Calculate envelope of real-valued 1-d signal.\r\n\r\n    Calculating the envelope consists of the following principal steps:\r\n    1. Apply a band-pass filter `[l0_low, l0_high]` to `x` resulting in `x_bp`.\r\n    2. Determine the analytical signal `z` of `x_bp`.\r\n    3. Calculate the squared envelope `e=|z|`.\r\n    5. Down- or up-sample the output to `n_out` samples (default `len(x)`).\r\n\r\n    Note that `f0_lo` and `f0_hi` can be also interpreted as the\r\n    frequency indexes of the array returned by `rfft(x)`.\r\n\r\n    Returns\r\n    -------\r\n    x_env :\r\n        The absolute value of the analytic signal of the band-pass filtered signal `x`.\r\n    x_res :\r\n        The residual of the band-pass filter depending on the parameter `residual`:\r\n            `low-pass`: The result of filtering the `x` with the low-pass filter\r\n                        `[0, l_low]` is returned.\r\n            `mean`: the mean of `x` is returned as a float.\r\n            'all': `x` with the band `[l_low, l_high]` filtered out.\r\n    \"\"\"\r\n    l_cutoff = (len(x) + 1) \/\/ 2  # highest allowed frequency, Nyquist freq. + 1\r\n    if l0_high is None:\r\n        l0_high = l_cutoff\r\n    if not (0 < l0_low < l0_high <= l_cutoff):  # always to remove mean\r\n        raise ValueError(f\"0 < {l0_low=} < {l0_high=} <= {l_cutoff} does not hold\")\r\n    if n_out is None:\r\n        n_out = len(x)\r\n\r\n    fak = n_out \/ len(x)  # scaling factor for resampling\r\n    X = rfft(x)  # Only the positive frequency components are needed for the analytical signal\r\n    # Envelope is invariant to frequency shifts of analytic signal:\r\n    x_env = np.abs(ifft(X[l0_low:l0_high], n=n_out)) * fak * 2\r\n    if residual == 'mean':\r\n        return x_env, np.mean(x)\r\n    elif residual == 'low-pass':\r\n        return x_env, irfft(X[:l0_low], n=n_out) * fak\r\n    elif residual == 'all':\r\n        X[l0_low:l0_high] = 0\r\n        return x_env, irfft(X, n=n_out) * fak\r\n    else:\r\n        raise ValueError(f\"{residual=} not in ['mean', 'low-pass', 'all']!\")\r\n\r\n\r\n# Create a signal made up of two cosines:\r\nT, n = 5e-4, 2000  # sampling interval, number of samples\r\nt = np.arange(n) * T  # time stamps\r\nff = (2, 10)  # cosine frequencies\r\ny = sum(np.cos(2*np.pi*f_*t) for f_ in ff)\r\nf_str = \"[\" + ', '.join(rf\"${f_}\\,$Hz\" for f_ in ff) + \"]\"\r\n\r\n# Determine the envelope with 3 Hz high-pass filter:\r\nn_e = n \/\/ 50  # down-sample by factor of 50\r\ny_e, y_res = envelope_bp(y, l0_low=3, n_out=n_e, residual='low-pass')\r\nt_e = np.arange(n_e) * T * n \/ n_e  # time stamps for envelope\r\n\r\n# Do the plotting:\r\nfg0, ax0 = plt.subplots()\r\nax0.set(title=rf\"Downsampled Envelope of $y(t)=\\sum_i \\cos(2\\pi f_i t)$, $f_i=$\" + f_str,\r\n        xlabel=\"Time $t$ \/ s\", ylabel=\"$y(t)$\")\r\nax0.plot(t, y, 'C0-', alpha=.5, label='Signal')\r\nax0.plot(t_e, y_res + y_e, 'C1.--', alpha=.5, label='Envelope')\r\nax0.plot(t_e, y_res - y_e, 'C1.--', alpha=.5)\r\nax0.legend()\r\nax0.grid()\r\n\r\nplt.show()\r\n```\r\nThe plot shows the envelope with a 3 Hz high pass filter resulting in ignoring the 2 Hz component for the envelope calculation (This is not possible with the current implementation):\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/12721170\/c7fac4f1-e597-48e8-a43f-2554e461e009)\r\n","@DietBru Thanks for pointing out the aliasing problem and providing a solution. I will think about whether envelope(x, None, 'analytic') and envelope(x, 1, 'rms') should give the same result. I think we could also do the following for the 'rms' method:\n\n1. Double the sampling rate of the signal by upsampling.\n2. Do the sliding window root-mean-square.\n3. Downsample the signal back to its original sampling frequency using 'decimate', which implements an anti-aliasing filter.\n\nI think this would be a more robust way to prevent aliasing. Your implementation seems to require a good knowledge of the underlying signal.\n\n","@DietBru I don't understand why you think that `envelope(x,None,'analytic')` and `envelope(x,1,'rms')` should give the same result. In the first example you provide, your signal is\r\n\r\n$$y(t) = \\cos(2 \\pi 150 t) + \\cos(2 \\pi 200 t)$$\r\n\r\nanalytical signal of which is\r\n\r\n$$a(t) =  \\cos(2 \\pi 150 t) + \\cos(2 \\pi 200 t) + i \\left( \\sin(2 \\pi 150 t) + \\sin(2 \\pi 200 t) \\right)$$\r\n\r\nTherefore the analytical envelope of $y(t)$ is\r\n\r\n$$\\begin{align}\r\ne_a (t) &= |a(t)| \\\\\r\n&= \\sqrt{ \\left( \\cos(2 \\pi 150 t) + \\cos(2 \\pi 200 t) \\right)^2 + \\left( \\sin(2 \\pi 150 t) + \\sin(2 \\pi 200 t) \\right)^2 }\r\n\\end{align}$$\r\n\r\nwhile the root-mean-square envelope with a sliding window size of 1 would be\r\n\r\n$$e_{rms} (t) = \\sqrt{ \\left( \\cos(2 \\pi 150 t) + \\cos(2 \\pi 200 t) \\right)^2 }$$\r\n\r\n$e_a (t)$ and $e_{rms} (t)$ are clearly not equal.\r\n\r\nAlso, to your point about aliasing. In this example your sampling rate is 1000 Hz, which corresponds to a Nyquist frequency of 500 Hz. I don't think that aliasing is an issue in this particular example because $150+200 = 350 < 500$.\r\n\r\nPlease let me know if I am missing something.","Your  are exactly right, @yagizolmez : $e_a(t)$ and $e_{rms}(t)$ are clearly not equal.\r\n\r\nThe docstring is ambiguous:  The `'rms'` method could also be interpreted as sliding a window over $|a(t)|^2$ compared to the implementation (sliding over $|x(t)|^2$) . The two variants will produce different results. I leaned  toward the $|a(t)|^2$ interpretation, since it allows to look at the `'rms'` method as a smoothed `'analytic'` method.\r\n\r\nSo how is a \"rms-envelope\" defined? Neither Matlab nor Wikipedia are very helpful here. Is anybody aware of any exact definitions in literature, which could be cited? Perhaps a citation for the standard (`'analytic'`) envelope, ideally providing some insights on how and why apply it to practical problems would not hurt either.\r\n\r\nMy personal 2 cents: Since a (rectangular) sliding window is the same thing as a low-pass filter with the frequency response of a [sinc](https:\/\/en.wikipedia.org\/wiki\/Sinc_function) function, in most cases using another low-pass filter is the better choice. Hence, for me the  \"rms-envelope\" envelope is a legacy which should replaced with low-pass filtering  (I am more than happy to be convinced otherwise). \r\n\r\n\r\n\r\n\r\n\r\n\r\n","Considering the aliasing: You are right @yagizolmez, in my example aliasing does not occur (but using e.g., 400 and 450 Hz would do the trick).\r\n\r\nTo compare the two methods, I think it is useful to rewrite $|e_{rms}|^2$ and $|e_a|^2$ with complex exponentials (assuming $f_0, f_1=150,200$):\r\n$$|y(t)|^2  = |e_{rms}|^2= \\frac{e^{4 i \\pi f_{0} t}}{4} + \\frac{e^{4 i \\pi f_{1} t}}{4} + \\frac{e^{2 i \\pi t \\left(- f_{0} + f_{1}\\right)}}{2} + \\frac{e^{2 i \\pi t \\left(f_{0} - f_{1}\\right)}}{2} + \\frac{e^{2 i \\pi t \\left(f_{0} + f_{1}\\right)}}{2} + \\frac{e^{- 2 i \\pi t \\left(f_{0} + f_{1}\\right)}}{2} + \\frac{e^{- 4 i \\pi f_{1} t}}{4} + \\frac{e^{- 4 i \\pi f_{0} t}}{4} +1$$\r\n$$|e_a|^2 = e^{2 i \\pi t \\left(- f_{0} + f_{1}\\right)} + e^{2 i \\pi t \\left(f_{0} - f_{1}\\right)} + 2\\ .$$\r\nComparing the two shows that the `'rms'` method produces additional frequency components, i.e., at $\\pm(f_0+f_1)$ and $\\pm2f_{0,1}$. In my opinion this makes interpreting results quite a bit harder.","@DietBru I have also not been able to find any concise resources explaining these methods. My understanding based on the MATLAB's documentation is as follows:\r\n\r\n1. `analytic` is the most versatile method that typically performs well for a wide range of signals. That's why it is the default selection.\r\n2. `rms` works well if the signal is very noisy. With this method you estimate the standard deviation of the underlying Gaussian using the samples within the window. Apparently this is commonly used in [EMG analysis](https:\/\/www1.udel.edu\/biology\/rosewc\/kaap686\/notes\/EMG%20analysis.pdf). Your point about similarity with low pass filtering is also made in these notes, but I don't think this similarity makes `rms` legacy.\r\n3. `peak` method is the only method that can yield asymmetric envelopes. [The example](https:\/\/github.com\/scipy\/scipy\/issues\/19634#issuecomment-1873537636) I gave in the discussion was using a human speech signal. Unfortunately, scipy does not have similar data in its dataset, so I haven't added an example for this method in docstrings. I would appreciate any recommendations on this.\r\n\r\nThanks for your comments. If you want to suggest any improvements to the docstrings, you can propose some edits and we can go over them together. I will commit them if we both agree. Also, please let me know if you have any other questions and\/or concerns.","Re https:\/\/github.com\/scipy\/scipy\/pull\/19814#issuecomment-1879762067, thanks for pinging me @ilayn and sorry for the somewhat late response. I don't think I will have much to add here. I'll keep an eye on this though.","> peak method is the only method that can yield asymmetric envelopes. [...] Unfortunately, scipy does not have similar data in its dataset, so I haven't added an example for this method in docstrings. I would appreciate any recommendations on this.\r\n\r\nWould [`scipy.datasets.electrocardiogram`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.datasets.electrocardiogram.html#scipy.datasets.electrocardiogram) help with demonstrating the asymmetric aspect?","Can someone help me on this? When I reply to reviews, there is a pending sign next to my name:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/7791057d-29b0-4aa8-a770-9dfd8bfb64d8)\r\n\r\nand when I open the page in Incognito mode, I don't see my own comments. I suspect that others don't see them either. What should I do?\r\n","@lagru Thank you for your comments and suggestions. I have just added an example for the `peak` method using the ecg data in line with your suggestion. I think it looks good:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/c3c88f44-435c-4c7f-b916-3fe29e979f27)\r\n\r\n@ilayn I would be happy, if you could inform us on this:\r\n\r\n> Not sure what SciPy's policy is on keyword-only arguments, but I personally like making optional arguments keyword-only.\r\n\r\nI second the suggestion by @lagru to break this function down to several functions, and possibly give more control to the user over the function parameters. I also suggest that if we take this route, we don't implement the `analytic` method since it would simply be a wrapper for `signal.hilbert`. With the separation, I can add a fourth widely used method. Which is called 'linear envelope'. It consists of rectifying the signal and lowpass filtering it. I will wait for a week to see if there are any objections to this proposal of introducing a separate function for each method, after which I will implement it.\r\n","Keyword-only arguments should be used where possible and appropriate as per https:\/\/docs.scipy.org\/doc\/scipy\/dev\/missing-bits.html","> Can someone help me on this? When I reply to reviews, there is a pending sign next to my name:\n\nThat means that they are review comments. You can send your review from the 'files changed' tab by submitting the review.","I mentioned this in my previous comment, but some people might have missed it. I would like to get opinions on this suggestion by @lagru:\r\n\r\n>Alternatively, what do you think about splitting this function into three: envelope_rms, envelope_analytic, envelope_peak? Looking at the docstring and code, there wouldn't be too much duplication and this would make each function easier to reason about and document.\r\n\r\nI think it makes sense.","Sorry @yagizolmez for the lagging response\u2014I had trouble finding the time to do my research on this topic.\r\n\r\nI would like to double down @largu's suggestion of splitting up this function: I believe the term \"envelope\" is often used to represent multiple concepts, which do no belong together. Let's pick those apart:\r\n\r\n#### Amplitude Bounds\r\nThe `'rms'` and `'peak'` method interpret the \"envelope\" as determining the upper and lower time dependant bounds where all \/ the majority of the amplitude points sit. A typical application might be representing a speech signal graphically for an audio recording software. This is only context where asymmetric bounds make sense.        \r\n\r\n#### Amplitude Demodulation\r\nFor a given baseband signal $s(t)$ and a carrier frequency $f_0$, the amplitude modulated signal is given by $x(t)=\\Re(s(t) \\exp(2j\\pi f_0 t))$. The ideal demodulator for $x(t)$ is the absolute value of the analytic signal, i.e., ``abs(hilbert(x))``. This is the classical interpretation in signal processing. Due to $x(t)$ being limited to a narrow frequency band a multitude of efficient algorithms exist for demodulation as discussed in this [paper](https:\/\/community.infineon.com\/gfawx74859\/attachments\/gfawx74859\/psoc135\/46469\/1\/R.%20Lyons_envelope_detection_v3.pdf). \r\n\r\n#### Enveloping a Stochastic Process \r\nThe envelope is characterized [mathematically](https:\/\/en.wikipedia.org\/wiki\/Envelope_(mathematics)) as a curve which touches a given set of curves tangentially. In signal processing stationary and cyclo-stationary stochastic processes are such sets curves where the envelope is of interest. An example is fault diagnostics for bearing (checkout Matlab's [envspectrum()](https:\/\/de.mathworks.com\/help\/signal\/ref\/envspectrum.html) function, especially the reference). It turns out that ``abs(hilbert(x))`` is also the correct solution here, though in practice $x(t)$ is usually bandpass filtered first.  Furthermore, the marginal of the spectral correlation density, which is requested in #14783, is the spectrum of the squared envelope.  \r\n\r\n## Summary\r\nI think it would be sensible to have two functions: \r\n\r\nThe `'rms'`, `'peak'` and similar methods could be implemented in a `amplitude_bounds()` function. It might make sense to reason about the provided methods and how to parametrize them a little more (which could entail providing separate functions). E.g., one could allow the degree of the spline for the interpolation to be parametrized or replace the '`rms`' method with a sliding window of arbitrary shape instead of allowing only a [boxcar](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.windows.boxcar.html) window. \r\n\r\nThe `envelope()` function (could be named differently) does the following steps (a slight extension to this [sketch](https:\/\/github.com\/scipy\/scipy\/pull\/19814#issuecomment-1880784457)):\r\n\r\n1. Calculates the two-sided FFT $X(f)$ from the input $x(t)$, which may be real or complex-valued to  accomodate for #19771.\r\n2. Apply a bandpass filter `Z = X[f_lo:f_hi]`. The default `X[f_lo:f_Nyquist]`only removes the DC-component. If `0<f_lo<f_hi` then `Z` is an analytic signal. Negative frequencies are also allowed, because the envelope is invariant to frequency shifts. \r\n3. Calculate the output `e = abs(irfft(Z, n_out))`. The `n_out` parameter allows to up- or down-sample the output.\r\n4. Optionally the unused part `r = ifft(X-Z)` can also be returned, which is useful to when $r(t)\\pm e(t)$ should be plotted.\r\n \r\nThis would address the following points:\r\n* Clearly separate the underlying concepts.\r\n* The second function tends to the classical signal processing definition of amplitude demodulation. Furthermore:\r\n  * The default parametrization would result in the `analytic` method. \r\n  * The bandpass filtering makes it more versatile than a simple wrapper to the `hilbert()` function.\r\n  * It would be conceptually compatible to a future implementation of a spectral correlation density, function, which is requested in #14783.\r\n  ","@DietBru Thank you for your comments.\r\n\r\n> The `rms` and `peak` method interpret the \"envelope\" as determining the upper and lower time dependant bounds where all \/ the majority of the amplitude points sit.\r\n\r\nI agree that this is the correct interpretation for the `peak` method. However, it is certainly not true for the `rms` method if you look at the example I provided in the [docs](https:\/\/output.circle-artifacts.com\/output\/job\/ba5557cc-ed6d-4df8-a59e-9044b2e5b379\/artifacts\/0\/html\/reference\/generated\/scipy.signal.envelope.html#scipy.signal.envelope). Mathematically, the `rms` method assumes that the underlying signal is white noise and it calculates the time-varying standard deviation.\r\n\r\n> The `rms`, `peak` and similar methods could be implemented in a amplitude_bounds() function.\r\n\r\nI disagree for the same reason above.\r\n\r\n> It might make sense to reason about the provided methods and how to parametrize them a little more (which could entail providing separate functions). E.g., one could allow the degree of the spline for the interpolation to be parametrized or replace the `rms` method with a sliding window of arbitrary shape instead of allowing only a [boxcar](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.windows.boxcar.html) window.\r\n\r\nHere, I agree with you on giving more control over the properties of the spline in the `peak` method. We could also give more control over the peak detection, in line with @lagru's suggestion. For the `rms` method, we could set the moving average (boxcar) as the default, and add an option to pass an arbitrary window as an array. I can imagine this being useful in some applications. **It would be nice, if you could try to find those applications so that we could add those to the documentation.**\r\n\r\n> which may be real or complex-valued to accomodate for\r\n\r\nWhy would someone need to calculate the envelope of a complex signal?\r\n\r\n> The default `X[f_lo:f_Nyquist]` only removes the DC-component.\r\n\r\nYou should not remove the DC component here.\r\n\r\n> The `n_out` parameter allows to up- or down-sample the output.\r\n\r\nIs there a way to add this to `hilbert`? I think it would be a useful addition. I would support adding a separate function `envelope()` the way you suggest it if:\r\n\r\n1. `hilbert` itself cannot be extended to have the parameter `n_out`, or\r\n2. You can demonstrate a case where custom bandpass filtering would be better than the lowpass filter as currently implemented by `hilbert`.\r\n\r\n> It would be conceptually compatible to a future implementation of a spectral correlation density\r\n\r\nHow does this relate to the spectral correlation density? I would be happy if you could elaborate.","@rkern If we break this function into several functions, would you be comfortable with moving `envelope_peak` to `_peak_finding` to resolve the circular import issue?","No, it would belong next to the other envelope functionality. That's the priority.\r\n\r\nWith multiple functions, you might want to consider moving them all together to their own `_envelope.py` module, anyways, instead of `_signaltools.py`. Then you can place the `from ._envelope import *` statement in `scipy\/stats\/__init__.py` appropriately so that there is no circularity.\r\n\r\nBut failing that, a properly-commented local import is _fine_.","> > The `rms` and `peak` method interpret the \"envelope\" as determining the upper and lower time dependant bounds where all \/ the majority of the amplitude points sit.\r\n> \r\n> I agree that this is the correct interpretation for the `peak` method. However, it is certainly not true for the `rms` method if you look at the example I provided in the [docs](https:\/\/output.circle-artifacts.com\/output\/job\/ba5557cc-ed6d-4df8-a59e-9044b2e5b379\/artifacts\/0\/html\/reference\/generated\/scipy.signal.envelope.html#scipy.signal.envelope). Mathematically, the `rms` method assumes that the underlying signal is white noise and it calculates the time-varying standard deviation.\r\n\r\nI think you mixed some things up here. That is not what is happening in the `rms` method. It is irrelevant if there is realization of a stochastic process involved nor is a standard deviation (or rms) calculated. What is implemented is a so-called square-law envelope detector (e.g., consult page 11 of this [PDF](https:\/\/user.eng.umd.edu\/~tretter\/commlab\/c6713slides\/ch5.pdf#page=11)). After removing the mean, it is implemented by the following line:\r\n```python\r\nzero_mean_envelope = np.sqrt(convolve(x_zero_mean ** 2, np.ones(N) \/ N,\r\n                                              mode='same', method='direct'))\r\n```\r\nIt consists of:\r\n1. Squaring the input\r\n2. Applying a lowpass filter (here by a direct convolution)\r\n3. Taking the square-root\r\n\r\nIt is designed to recover the baseband signal $b(t)$ from an AM signal $s(t)=A_c\\big(1+k_a m(t)\\big)\\cos(2\\pi f_c t)$, the utilized lowpass filter needs to be able to sufficiently separate $b(t)$ from\r\n$\\big(1+k_a m(t)\\big)^2\\cos(4\\pi f_c t)$. Note that the recovered signal has DC-offset and is scaled by $A_c\/4$. \r\n\r\nIn your [example](https:\/\/output.circle-artifacts.com\/output\/job\/ba5557cc-ed6d-4df8-a59e-9044b2e5b379\/artifacts\/0\/html\/reference\/generated\/scipy.signal.envelope.html#scipy.signal.envelope) there is not one carrier but a superposition of many. The reason being that in the discrete-time finite-duration domain zero-mean Gaussian noise my be interpreted as a Fourier series with the coefficients being uncorrelated zero-mean Gaussian random variables with the variance being proportional to (mean) energy of that noise signal.\r\nIf you grind through the formulas, you will notice that for the majority of the carriers the low-pass can separate the baseband signal to some degree but there a is also interference caused by the summation of the carriers. Hence, as the example shows, the recovery is not optimal. \r\n\r\n**Summary for the `rms` method:**\r\n* I had fun investigating the example, but I think it hides more about envelope analysis than it revals. The reason being that a firm understanding of [WSS processes](https:\/\/en.wikipedia.org\/wiki\/Stationary_process#Weak_or_wide-sense_stationarity) is required. To make this example useful to (novice) users, quite a lot more explaining would be required. References to literature where such classes of signals are discussed in detail would also be helpful.\r\n* I am not really convinced that there are relevant uses-cases where the `rms` method  is superior to a Hilbert transform. Square-law detectors and friends have their advantages in real-time processing of signal streams, which are obviously out of scope here. The already referenced [PDF](https:\/\/user.eng.umd.edu\/~tretter\/commlab\/c6713slides\/ch5.pdf) provides some insight in this regard. Again, further references would help.\r\n* As already noted, I am wondering why it makes sense to convolve with a square window as a lowpass filter when all this [filtering machienry](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/signal.html#filtering) is directly available at one's fingertips. \r\n* As already discussed [above](https:\/\/github.com\/scipy\/scipy\/pull\/19814#issuecomment-1880690861), the sampling rate needs to be doubled before squaring to avoid aliasing. ","\r\n> > which may be real or complex-valued to accomodate for\r\n> \r\n> Why would someone need to calculate the envelope of a complex signal?\r\n\r\nEnvelopes (at least if calculated with a hilbert transform) are invariant to modulation (i.e., frequency shifts). Modulation can only be done robustly on complex-valued signals. Furthermore, interpreting two-dimensional trajectories as complex-valued signals (as often is done vibration monitoring) allows straight forward spectral analysis. Hence, an FFT decomposes the 2D signal into forward and backward rotating circular trajectories with different frequencies. So does make sense here to honor #19771.\r\n \r\n> > The default `X[f_lo:f_Nyquist]` only removes the DC-component.\r\n> \r\n> You should not remove the DC component here.\r\n\r\nDue to being linear operations the order of those may be swapped. Hence it is okay to do here. \r\n\r\n> > The `n_out` parameter allows to up- or down-sample the output.\r\n> \r\n> Is there a way to add this to `hilbert`? I think it would be a useful addition. I would support adding a separate function `envelope()` the way you suggest it if:\r\n> \r\n>     1. `hilbert` itself cannot be extended to have the parameter `n_out`, or\r\n> \r\n>     2. You can demonstrate a case where custom bandpass filtering would be better than the lowpass filter as currently implemented by `hilbert`.\r\n\r\nIn vibration-based bearing diagnostics (e.g., checkout the literature of this [Matlab page](https:\/\/de.mathworks.com\/help\/predmaint\/ug\/Rolling-Element-Bearing-Fault-Diagnosis.html)),\r\nthe spectrum of an envelope is used to determine the average frequency of pulse trains with significant time-jitter. In practice, it only makes sense to use a frequency band (obtained by bandpass filtering), which is likely to not contain strong signals from other sound sources.    \r\n\r\n> > It would be conceptually compatible to a future implementation of a spectral correlation density\r\n> \r\n> How does this relate to the spectral correlation density? I would be happy if you could elaborate.\r\n\r\nAgain in bearing diagnostics a spectral correlation density can be utilized to chose an appropriate frequency band which contains the most energy of the jittered peaks. As noted above, the marginal of the spectral correlation density, is the spectrum of the squared envelope. ","> I think you mixed some things up here. That is not what is happening in the rms method. It is irrelevant if there is realization of a stochastic process involved nor is a standard deviation (or rms) calculated. What is implemented is a so-called square-law envelope detector\r\n\r\nIndeed, `rms` is a special case of square-law envelope detector. However, the thinking here is done solely in the time-domain. I have found some [lecture notes](https:\/\/www1.udel.edu\/biology\/rosewc\/kaap686\/notes\/EMG%20analysis.pdf) on this:\r\n\r\n>  I can think of at least two reasons for the popularity of RMS methods of analysis: 1. if the signal values are normal random deviates, then the RMS approach can be shown to be an optimal method for estimating the standard deviation of the underlying normal distribution. 2. If the signal were a voltage applied across a resistor, the mean square method correctly predicts the power (heat) that will be dissipated in the resistor - which has an intuitive appeal as a measure of \"strength of signal\".\r\n\r\nI like your AM example. It makes a lot of sense to me. However, I would want to keep `rms` as it is. I would like to incorporate your suggestion into a function like this:\r\n\r\n```python\r\ndef envelope_lp(x, b, a, method = 'rectify', *, upsampling_ratio=1, \r\n    filtfilt_padtype='odd', filtfilt_padlen=None, filtfilt_method='pad', \r\n    filtfilt_irlen=None)\r\n```\r\n\r\nHere `x` is the signal. `method` can take values `str{'rectify', 'square', 'half-wave'}`. `upsampling_ratio` parameter will give the user the option to upsample the signal to prevent aliasing in line with your previous suggestion. We should add a note stating that upsampling might be needed if the `method='square'`. All other parameters are the parameters for the [filtfilt](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.filtfilt.html), which will work in the backend. This function will essentially implement many of the algorithms in the [paper](https:\/\/community.infineon.com\/gfawx74859\/attachments\/gfawx74859\/psoc135\/46469\/1\/R.%20Lyons_envelope_detection_v3.pdf) you had sent previously.\r\n\r\nI will address your comments about the complex-valued envelopes in another day. I have been busy lately, sorry for the delay in responses.\r\n\r\n",">Indeed, rms is a special case of square-law envelope detector. However, the thinking here is done solely in the time-domain. I have found some [lecture notes](https:\/\/www1.udel.edu\/biology\/rosewc\/kaap686\/notes\/EMG%20analysis.pdf) on this:\r\n>\r\n>    I can think of at least two reasons for the popularity of RMS methods of analysis: 1. if the signal values are normal random deviates, then the RMS approach can be shown to be an optimal method for estimating the standard deviation of the underlying normal distribution. 2. If the signal were a voltage applied across a resistor, the mean square method correctly predicts the power (heat) that will be dissipated in the resistor - which has an intuitive appeal as a measure of \"strength of signal\".\r\n>\r\nRegarding tose lecture notes:\r\n1. The catch is that it is only optimal if the random distribution does not change over time (then it's equivalent to `np.std(x)`). The standard approach is to use [Welch's method](http:\/\/scipy.github.io\/devdocs\/tutorial\/signal.html#spectra-with-averaging), which also works if the signal contains additive colored noise (an easy mental model for that is to think of your signal as a Fourier series, where the Fourier coefficients are independent random variables \u2014 white noise has i.i.d Fourier coefficients). The 'analytic' method (with the same lowpass filter) would produce the same result. \r\n2. One paragraph down, the reader is advised to be \"wary of the RMS method is that if the signal values are NOT normally distributed\" \u2014 if you already know your signal is normally distributed, you do not need to calculate the envelope (just do `np.std(x)`).\r\n3. In signal processing the [power](http:\/\/scipy.github.io\/devdocs\/tutorial\/signal.html#spectral-analysis) of signal $x(t)$ is defined by the mean over $|x(t)|^2$. Thus lowpass filtering and taking the square root is what more or less what your [True-RMS Multimeter](https:\/\/en.wikipedia.org\/wiki\/True_RMS_converter) does. As noted using using a moving average filter produces a noisier signal than most other lowpass filters.\r\n\r\nMy hunch is that there is a different cause for the proclaimed popularity of the RMS method: One is able to reason about it to some degree without needing to understand what a Fourier transform, a complex-valued signal or a complex-valued stochastic process is. Furthermore, the over two decades old cited biology paper has limited relevance in gauging of today's popularity, IMHO.","> I would like to incorporate your suggestion into a function like this:\r\n> ```python\r\n> def envelope_lp(x, b, a, method = 'rectify', *, upsampling_ratio=1, \r\n>     filtfilt_padtype='odd', filtfilt_padlen=None, filtfilt_method='pad', \r\n>     filtfilt_irlen=None):\r\n> ```\r\nOff the bat, I have the following comments:\r\n* I am a bit uneasy about the option creep. I think is totally reasonable to only have one solid implementation of an envelope and point to references of alternative implementations. Here is why:\r\n  * Quite bit of experience is required to assess the options to assess the different options against each other. On other hand for many scenarios the customizability may not be enough. If someone has the experience he may take the function und reimplement to their own needs.\r\n  * If interpretation of options is not straightforward, the risk of futures bug reports about correct interpretations increase (I might be a bit biased here)\r\n* Note that [resample_poly](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.resample_poly.html#scipy.signal.resample_poly) has simpler interface. [firwin](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.firwin.html#scipy.signal.firwin) can be used to generate appropriate windows. I think it reasonable to only allow FIR filters like in `resample_poly()`. Note that  [lfilter](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.lfilter.html#scipy.signal.lfilter) is the more suitable for FIR filters than `filtfilt` is more suitable to use then.\r\n* Before implementing anything, I might be worthwhile to understand analytic signal method a little bit better. I prefer that method for good reasons, as you may have noticed :grinning: ","A good way to investigate the analytic signal method, is look at the following signal.\r\n\r\n$$ x(t) = a_0 + a_1 \\cos \\phi_1 + a_2 \\cos\\phi_2\\ \\text{  with } \r\n   \\phi_i := 2\\pi f_i t\\ ,$$\r\n\r\nwith duration of $\\tau=1$ s. Note that the continuous-time version of any sampled signal of duration $\\tau$ with constant sampling interval can be expressed by a Fourier series with frequency components $f_k=k\/\\tau$. Hence, $x(t)$ is an example of a simple Fourier series with frequencies $0, \\pm f_1, \\pm f_2$, due to $\\cos(\\phi)=(e^{j\\phi}+e^{-j\\phi})\/2$.  \r\n\r\nThe analytic signal $z(t)$ removes all negative frequency components from $x(t)$ and doubles the positive components. Hence, $\\cos(\\phi_k)$ is replaced with $\\exp(i\\phi_k)$, i.e.,\r\n\r\n$$ z(t) = a_0 + a_1\\,e^{i\\phi_1} + a_2\\,e^{i\\phi_2}\\ . $$\r\n\r\nRemembering that the 'analytic' envelope is $|z|=\\sqrt{|z|^2}=\\sqrt{z\\bar{z}}$, it makes sense to investigate the absolute square, which can be expressed as\r\n\r\n$$ |z(t)|^2 = \\sum_{k=0}^3 |a_k|^2 + 2a_0\\left( a_1\\cos\\phi_1 + a_2\\cos\\phi_2\\right) + 2a_1 a_2\\cos(\\phi_2-\\phi_1)\\ .$$\r\n\r\nThe squared un-smoothed square-law envelope $|x(t)|^2$ contains a few more terms, i.e.,\r\n\r\n$$|x(t)|^2 = a_0^2 + \\frac{a_1^2+a_2^2}{2} + 2a_0\\left( a_1\\cos\\phi_1 + a_2\\cos\\phi_2\\right)  + a_1a_2\\left(\\cos(\\phi_1+\\phi_2) + \\cos(\\phi_2-\\phi_1)\\right) \\\\ \r\n                    + \\frac{1}{2}a_1^2\\cos(2\\phi_1) + \\frac{1}{2} a_2^2\\cos(2\\phi_2) \r\n$$\r\n\r\nTypically, the lowpass filter for the square-law envelope is designed in way so terms at the frequencies $f_1,f_2, 2f_1, 2f_2, (f_1+f_2)$ are suppressed. Note that the structure is the same when having a signal with more frequencies: The terms of $|z(t)|^2$ are the permutations of all frequency differences (counting $0\\,$Hz as frequency as well) and a DC term. $|x(t)|^2$ additionally contains the permutations of the frequency sums and terms with doubled frequencies.\r\n\r\nThis little discussion gives us two key insights:\r\n* How well an envelope detector works strongly depends on the kind of input signal. Eg., for inputs with many frequencies it can happen that the difference components can interfere with the single frequency components.  \r\n* For generic applications the 'analytic' method is superior to the square-law method since it produces fewer interference terms. For the case of demodulating AM-modulated narrowband signals, both methods produce identical results, given that there's also a lowpass filter in the 'analytic' method. \r\n","> My hunch is that there is a different cause for the proclaimed popularity of the RMS method: One is able to reason about it to some degree without needing to understand what a Fourier transform, a complex-valued signal or a complex-valued stochastic process is. Furthermore, the over two decades old cited biology paper has limited relevance in gauging of today's popularity, IMHO.\r\n\r\nI think those are all the good reasons to have the `rms` method. It doesn't hurt to have a simple interpretation of your results. On this issue, we can agree to disagree for now and get more opinions from the community. @ilayn Would you (or someone you know) care to chime in?\r\n\r\n> Note that [resample_poly](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.resample_poly.html#scipy.signal.resample_poly) has simpler interface. [firwin](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.firwin.html#scipy.signal.firwin) can be used to generate appropriate windows.\r\n\r\nI am not sure, if this is a good idea. What if the user is only familiar with `butter` or `firls`? They would now have to study `firwin` for no reason. We can add tutorials on how to calculate the parameters `b` and `a`. I think it is an easier interface and it is more flexible.\r\n\r\n> Note that [lfilter](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.lfilter.html#scipy.signal.lfilter) is the more suitable for FIR filters than filtfilt is more suitable to use then.\r\n\r\nWouldn't `lfilter` introduce a phase shift?\r\n\r\n> Before implementing anything, I might be worthwhile to understand analytic signal method a little bit better. I prefer that method for good reasons, as you may have noticed.\r\n\r\nI agree that `analytic` method is superior for generic signals, but if we are breaking up the `envelope` method there is no point of implementing it. It is already implemented by `hilbert`. Please let me know if I am missing something.","Apologies for not attending to this issue as much as I would liked to. A classic too much to do-too little time is happening on my side. Just some comments in random order; but much appreciated the discussion going on here. Hence thank you all. \r\n\r\nIf we are adding something to SciPy, the goal is to first address all the expected functionality and then go to the special cases. Folks typically choose `rms` because the signal is noisy but they would like to get a hold of the trend of the signal inside that cloud as if the signal is a amplitude modulated mono-frequency sine. `analytic` is not very straightforward to explain to folks especially about the FIR length hence I am not sure matlab did the right thing here (it often does the wrong thing about APIs mostly because of [professional deformation](https:\/\/en.wikipedia.org\/wiki\/D%C3%A9formation_professionnelle)). I don't have a strong opinion about this.\r\n\r\n> Furthermore, interpreting two-dimensional trajectories as complex-valued signals (as often is done vibration monitoring) allows straight forward spectral analysis. Hence, an FFT decomposes the 2D signal into forward and backward rotating circular trajectories with different frequencies. So does make sense here to honor https:\/\/github.com\/scipy\/scipy\/issues\/19771.\r\n\r\nIt happens that I did predictive maintenance for a living a few years especially staring at SKF software for quite a long time. Nice to see another fellow bearing doctor. However, I would still say this is rather specific use case and does not necessarily warrants a common use case. Waterfall charts are certainly useful especially in spatio-temporal context, but complex signals and whether SciPy does this off-the-shelf is a tricky business as it is not very easy to come up with the code that does the right thing for all cases. That's why SKF still sells the same software since the 90s. While I understand the temptation, I would really like to see it in action first. Because nonstationary processes are quite specialized compared to the regular 1D array envelope function. We have a very long way to go to #19771 \r\n\r\n> My hunch is that there is a different cause for the proclaimed popularity of the RMS method: One is able to reason about it to some degree without needing to understand what a Fourier transform, a complex-valued signal or a complex-valued stochastic process is. Furthermore, the over two decades old cited biology paper has limited relevance in gauging of today's popularity, IMHO.\r\n\r\nRMS has a very strong time domain interpretation and indeed does not require any frequency domain relationship. But like I mentioned above this does not mean that it is of a lesser method. It just serves a different purpose. \r\n\r\n> ```python\r\n> def envelope_lp(x, b, a, method = 'rectify', *, upsampling_ratio=1, \r\n>     filtfilt_padtype='odd', filtfilt_padlen=None, filtfilt_method='pad', \r\n>     filtfilt_irlen=None)\r\n> ```\r\n> \r\n\r\nThis is a bit too crowded for a public function API, you can combine all that to a single kwarg, say, `filtering_options` that accepts a dict and pass it to whichever function is used for filtering. \r\n\r\nI would say, let's restrict the scope to real valued signal(s) and establish the basis for the API which does the common expected things. If that materializes, we would be more than happy to put more sound features to the envelope function in other PRs. Hence it doesn't have to be a single massive addition. Otherwise the discussion will spill over to many subjects that make the review more challenging. ","Thank you @ilayn for taking the time contributing to this discussion. \r\n> If we are adding something to SciPy, the goal is to first address all the expected functionality and then go to the special cases.\r\n\r\n\"Addressing all the expected functionality\" is the right call to make here. Thank you for that. The key aspect of the friendly discussion between @yagizolmez and me revolves around what an envelope actually is. Envelope properties were implicitly formulated, but not explicitly. But don't you think there should exist some kind of definition? Especially, since signal processing is basically just applied linear algebra? \r\n\r\nOur thread shows that looking at practical use cases did not really lead to satisfying results. As you noted, Matlab is not very helpful in that regard either. Also, during my time of working in predictive maintenance, I have seen my fair share of explanations how envelopes work, which rather deem me being a good fit for Harry Potter novels.  \r\n\r\nFrom the viewpoint of complex-valued signals being the norm and real-valued signals are a special case, everything becomes very simple and straightforward. In vibration analysis, it's also quite intuitive: just think of a two-channel sensor, which can measure horizontal and vertical accelerations at the same time (I am not talking about [time-frequency analysis](https:\/\/docs.scipy.org\/doc\/scipy-1.12.0\/tutorial\/signal.html#short-time-fourier-transform)).   \r\nInstead of writing another long explanation here, I decided it would be easier to just write a new PR.\r\n\r\n @ilayn, @yagizolmez please feel free to review and to comment, especially on how comprehensible the explanations in the documentation is. Concerning this PR, you can pursue it, if you feel that a square-law envelope detector (and its variants) would still be beneficial in SciPy or alternatively close it.","@DietBru Thanks. Since we have decided to break up the function, I was not planning to implement an envelope extraction function based on the analytical method. Therefore I don't think our PRs are competing. I will leave a review in the upcoming days."],"labels":["enhancement","scipy.signal"]},{"title":"ENH: Savitsky-Golay filter for non uniformly spaced data","body":"### Is your feature request related to a problem? Please describe.\n\nI've been using `scipy`'s Savitzky-Golay filter multiple times in the past. Now I have a task where I would like to apply the filter to non-uniformly spaced independent data.\r\n\r\nIn my application, the independent data are not entirely randomly spaced, but originate from lab data where the measurement interval is successively increased, e.g. 0,1 s interval for 1 hour, followed by 1 s interval for the rest of the day, followed by 10 s interval for some days, etc.\n\n### Describe the solution you'd like.\n\nI am by far not a signal nor mathematics expert, but I am still wondering whether it would be possible (or desired) to extent the `scipy.signal.savgol_filter` for the derivatives of non-uniformly spaced data. This could be triggered by setting the `delta` parameter not to a single value, but to an array of deltas, having the same size as the data.\r\n\r\nI found a [implementation on stackexchange](https:\/\/dsp.stackexchange.com\/a\/64313\/70801) and a [github repo based on the very same proposal](https:\/\/github.com\/aran159\/non-uniform-savgol).\r\n\r\nAs far as I understand, the current implementation is limited to uniformly spaced independent data (using `delta` and `deriv` parameters) [because this reduces to a single set of coefficients and very efficient folding operation](https:\/\/github.com\/scipy\/scipy\/issues\/5767#issuecomment-175720333). [There's also concerns that a significant increase in runtime by changing a single parameter (`delta`) could be at least surprising for users with large datasets.](https:\/\/github.com\/scipy\/scipy\/issues\/5767#issuecomment-176949005) Yet it thought it might still be worth discussing, whether any form of semi-fast implementation might be worth looking into.\r\n\r\nSo here are some ideas to speed things up:\r\nAt least for my desired application, the independent variable is not entirely random. Hence, a sort-of piece-wise savgol filter could be employed. Considering my example from above, for example. It would not be necessary to recalculate the `savgol_coeffs` for each window, as the `delta` varies only occasionally. This would result in a reduced set of deltas for which the coefficients would have to be calculated, e.g. for a window length of 5:\r\n```\r\n[0.1, 0.1, 0.1, 0.1, 0.1],\r\n[0.1, 0.1, 0.1, 0.1, 1.0],\r\n[0.1, 0.1, 0.1, 1.0, 1.0],\r\n[0.1, 0.1, 1.0, 1.0, 1.0],\r\n[0.1, 1.0, 1.0, 1.0, 1.0],\r\n[1.0, 1.0, 1.0, 1.0, 1.0],\r\n...\r\n```\r\nObviously it is strongly dependent on the level of non-uniformity of `delta`, how many sets of coefficients are needed.\r\n\r\nI also found a [reference ](https:\/\/mathematica.stackexchange.com\/questions\/85455\/nonuniform-savitzky-golay-filter-for-smoothing-and-differentiation ) to a [paper](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-75536-1_8) pointing towards polynomial something called \"similarity transformations\" in the same context. Unfortunately, I neither have access to the paper, nor do I fully understand the implications of this method.\r\n\r\nAny thoughts on this?\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["This is the kind of thing that I'd like to see developed in a separate package first, then adopted into scipy once completed (if it still makes sense to do so with the data gathered from real use) rather than developed inside of scipy.","Doing some research, I found a paper which I can access:\r\n\r\nGorry, P. A. (2002). \"General least-squares smoothing and differentiation of nonuniformly spaced data by the convolution method.\" Analytical Chemistry 63(5): 534-536. [1]\r\n\r\nI am in the process of implementing it. Preliminary results are promising. Odd-sized windows work, but even-sized windows are still a little off. Not sure how to fix that yet.\r\n\r\nA quick outlook (just an abritrary timeit comparison):\r\n```\r\nscipy: 0.03243189997738227 # original savgol_filter\r\npolyfit: 6.405896900017979 # fitting all polynomials individually\r\ngorry: 0.1077934000059031 # based on linked paper\r\n```\r\n\r\nI will publish the code, as soon as I find the time to clean it up a little.\r\n\r\n[1] https:\/\/pubs.acs.org\/doi\/abs\/10.1021\/ac00005a031\r\n","It looks good if gorry  is only 3 times slower.\r\n\r\nAFAICS, Gorry article is for unequal spaced points.\r\nAn alternative for piecewise constant spacing would be to stitch together SG on subsamples. The main work is then to smooth the connecting points which will require separate filters for the window length observations.\r\n\r\nAside:\r\nI have a stalled PR for statsmodels that includes binning to move from unequal spaced points to equal spaced points. That is a similar idea to what is used for fast 1-dim kernel density estimation (using fft convolution on binned, equal spaced data).\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/3492\r\n\r\nI don't remember any details (nor the references for the binning approach).\r\nThis would apply less to piecewise constant spacing where spacings might differ by a larger amount. \r\n","_A question:_\r\nIs there any recommended way to test the performance? Maybe even some standardized test signals available to use?\r\n\r\n> It looks good if gorry is only 3 times slower.\r\n\r\nTurns out this is not always the case. This was for a fairly small signal (1k points and `polyorder=2`). For larger signals it doesn't scale as good. Increasing the `polyorder` also reduces performance, as the Gorry approach recursively generates the coefficients in a loop over `polyorder`.\r\n\r\nFor now, I have focused on getting it to work. And after dealing with multiple edge-cases (even window, edge points), it now gives me identical results to `scipy`'s `savgol_filter`. While I am not a professional programmer, even I still see some room for improvements in my prototype. That's why I haven't uploaded it yet.\r\n\r\n\r\n\r\n> AFAICS, Gorry article is for unequal spaced points.\r\n> An alternative for piecewise constant spacing would be to stitch together SG on subsamples. The main work is then to smooth the connecting points which will require separate filters for the window length observations.\r\n\r\nThat's what I was thinking initially, but my application is not that performance hungry. Hence, I'd rather develop are more universal filter rather than dealing with my niche case and regret it the next time I have a truly nonuniform signal.\r\nIn the long run, however, a combination using piece-wise classic savgol on equally-spaced sections and Gorry for the stitching sections might be the way to go.","So here is finally a [draft of what I was able to come up with](https:\/\/github.com\/maximweb\/savgol_nonuniform). I am not entirely convinced, but at least I did learn a lot during the process.\r\n\r\nSome remarks regarding this implementation:\r\n* The Gorry algorithm relies on recursive calculation of weight factors, which I did not now how to do other than using a loop over `np.arange(0, polyorder+1)`.\r\n* Regarding the individual windows, I was able to do this with matrix formula, avoiding an outer loop. This might be memory intensive, but is faster than an outer loop.\r\n* I separated the start values for all recursively calculated parameters in separate conditions. This could be done in a more compressed manner in less conditions. For the sake of clarity, I kept it separated for now.\r\n* With the initial problem of partially uniform $x$ values in mind, I calculated $\\Delta x_{window} - x_{window, center}$ and ran `np.unique` on it, so that I only have to calculate parameters for uniquely spaced windows.\r\n* In an intermediate version, I did not use `np.unique` windows and did not center them to 0, but calculated every window regardless of possible repetition. There, I encountered weird spikes in the results when setting large `polyorder`. I am not entirely sure where they were coming from. The algorithm requires the calculation of powered values of x for each window in the form\r\n```math\r\n\\left( \\begin{matrix} x_0^0 & ... & x_{windowsize}^0\\\\ ... & ... & ... \\\\ x_0^{2*polyorder+1} & ... & x_{windowsize}^{2*polyorder+1}\\end{matrix} \\right)\r\n```\r\nSo I can only suspect, that I ran in some kind of `float` precision loss during calculation of $x_i^{2*polyorder+1}$. If it is in fact related to precision loss, this should also occur when have signals with very large $\\Delta x$. Unfortunately I was not able to reproduce this.\r\n\r\n# Performance\r\n\r\nRegarding performance, I did only some very basic tests. The basis was a sine signal with added artificial noise. Based on this, I created three different signals:\r\n1. with uniformly spaced $x$ -> `x-uniform`, solid lines\r\n2. with three uniformly spaced segments of $x$ -> `x-partial`, dashed lines\r\n3. with random noise added to all $x$ -> `x-nonuniform`, dotted lines\r\n\r\nThese I filtered with three `savgol` versions:\r\n1. `scipy.signal.savgol_filter` (blue)\r\n2. A reference implementation, which basically loops `np.polynomial.polynomial.polyfit` over all windows -> `polyfit` (green)\r\n3. The linked implementation of Gorry algorithm -> `nonunfirm` (red)\r\n\r\n## Signal length\r\n\r\n![signal_length](https:\/\/github.com\/scipy\/scipy\/assets\/6138142\/4a3e9408-ec66-4356-8c9f-22396c0f4629)\r\n\r\nRegarding signal length, the performance lies inbetween `scipy` version and referency `polyfit`. It does not scale as well as `scipy`-version. Calculating only uniquely spaced windows kicks in, for larger signal lengths as the uniformly spaced segments remain the same, whereas the ratio of unique transition windows relative to the total signal length decreases.\r\n\r\n## Window length\r\n\r\nTo be fully transparent, for a signal with $N_{points} = 10^5$, my implementation scales poorly, even exceeding the reference `polyfit`. Not sure whether this is my implementation or the algorithm itself...\r\n\r\n![window N1e5](https:\/\/github.com\/scipy\/scipy\/assets\/6138142\/5d3f4f39-3d99-4b4b-9367-32ef474270ef)\r\n\r\n# Conclusion\r\n\r\n* Scaling seems to be an issue with my implementation. For small signals its better than just fitting polynomials per window, but this seems to change for larger signals and window length.\r\n* Poor scaling with `polyorder` is intrinsic to the Gorry algorithm, as it requires recursive calculation of weight factors.\r\n* I will keep investigating, where the mentioned spikes came from, if I ever encounter them again.\r\n* For partially uniform signals, using uniquely spaced windows helps, but that strongly depends on the data.\r\n\r\nAfter seeing the performance tests, I think it is not yet suited for `scipy`. At least in it's current form. If you have any recommendations regarding my hobbyist-style implementation, I am open for suggestions."],"labels":["enhancement","scipy.signal"]},{"title":"BUG: filter out unhelpful & noisy numpy warnings in implementation, not in tests","body":"This was surfaced by #19804, causing me to take a look where those warnings were coming from. I was surprised that - in the example below at least - the warnings are duplicated between scipy & numpy (not caught by the new pytest 8 because the messages are identical).\r\n\r\nLooking [closer](https:\/\/github.com\/search?q=repo%3Ascipy%2Fscipy+%2FMean+of+empty+slice%5C.%7Cinvalid+value+encountered%2F&type=code), we're filtering out a lot of warnings in tests that should IMO really be filtered out in the implementations, rather than the tests.\r\n\r\nAs an example:\r\n```\r\n>>> from scipy import stats\r\n>>> stats.skew([])\r\n[...]\/scipy\/stats\/_stats_py.py:1175: RuntimeWarning: Mean of empty slice.\r\n  mean = a.mean(axis, keepdims=True)\r\n[...]\/numpy\/core\/_methods.py:121: RuntimeWarning: invalid value encountered in divide\r\n  ret = um.true_divide(\r\n[...]\/numpy\/core\/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\r\n  return _methods._mean(a, axis=axis, dtype=dtype,\r\n[...]\/numpy\/core\/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\r\n  ret = ret.dtype.type(ret \/ rcount)\r\nnan\r\n```\r\n\r\n4 warnings here are at least 3 too many, and we need to filter at least the ones from numpy; though IMO even \"Mean of empty slice\" is not helpful, and we could perhaps just return `np.nan` without a warning.","comments":["Having fiddled a little bit with special test suite recently, it is like a boeing 747. Wonderful to look at go, but almost impossible to do surgery on it or modernize. I still don't know what it does in certain places and hoping the code is correct enough. So not sure what the right way to go about this since there are also warnings coming up from the internals of the tests. ","> So not sure what the right way to go about this [...]\r\n\r\nNot everything might be clearcut, but I think as an aspirational goal, this should be uncontroversial?\r\n\r\n> [...] since there are also warnings coming up from the internals of the tests.\r\n\r\nIMO that just means we need to write more robust test helpers. \ud83e\udd37 ","What I mean is that it is nontrivial effort to catch and contain every error coming from the bowels of these tests because many special functions return nan's and inf's in the intermediate steps. So no controversy just tedious. ","> So no controversy just tedious.\r\n\r\nNot like QoI is every anything but tedium \ud83d\ude05 ","The main issue there is in numpy, with two warnings per `np.mean([])` call (and `np.median([])` too). So that seems like the best place to fix something first.\r\n\r\n> we could perhaps just return `np.nan` without a warning.\r\n\r\nMakes sense to fix this to yield only a single warning rather than two. Note though that if we do it with an early exit, care needs to be taken to preserve the correct output shape for empty arrays with multiple dimensions (which is often not tested). In this case it doesn't yield a warning:\r\n```python\r\n>>> np.ones((3,2,0))\r\narray([], shape=(3, 2, 0), dtype=float64)\r\n>>> stats.skew(np.ones((3,2,0)), axis=2)\r\narray([[nan, nan],\r\n       [nan, nan],\r\n       [nan, nan]])\r\n```\r\nbut often it will, since `np.mean` will yield the same warning."],"labels":["defect","scipy.stats","scipy.special"]},{"title":"ENH: improve stats.yeojohnson bounds","body":"#### Reference issue\r\nThis is a follow-up improvement to #18852, which addressed under- and overflow in the Yeo-Johnson transform.\r\n\r\n#### What does this implement\/fix?\r\nAs [suggested](https:\/\/github.com\/scipy\/scipy\/pull\/18852#pullrequestreview-1531164724) by @mdhaber, this PR implements an exact* formula to determine the lower and upper bounds on lambda in the Yeo-Johnson transform to avoid underflow and overflow (on the given input data, but also on unknown future input).\r\n\r\nImportant notes about this PR:\r\n1. This PR further increases the search space for lambda relative to the comparatively conservative bounds introduced by #18852.\r\n2. Overflow can now be avoided for input data up to 1e303 (which is close to double precision's maximum of 1e308), up from about 1e200.\r\n3. Underflow can unfortunately only be avoided for input data up to 1e-150, which is not an improvement over the current state. The reason for this is that other parts of Scipy, specifically the implementation of Brent's method in `scipy.optimize.fminbound`, are not equipped to deal with extremely small (or large) input data.\r\n4. Instead of optimising over lambda directly, this PR optimizes over $\u03bb' := \\textrm{tanh}(\u03bb)$. This is done because the increased search space for lambda (1) is sufficiently large that the implementation of Brent's method overflows. By optimising over the transformed lambda, we avoid that overflow.\r\n5. *Under- and overflow can appear in two places: as the result of the Yeo-Johnson transform itself ($((1 + x) ^ \u03bb - 1) \/ \u03bb$), and in its numerator ($(1 + x) ^ \u03bb - 1$). Overflow can be avoided by constraining lambda to an interval, but to avoid underflow a more complex constraint on lambda would need to be imposed in general. For that reason, underflow cannot be entirely avoided. Right now, the implementation accepts that underflow could in theory still occur in the numerator.","comments":["@lsorber happy new year! how does this relate to gh-19652? Please coordinate with @xuefeng-xu to create a unified PR. In the meantime, I need to finish reviewing gh-19631 to finish up with BC before returning to YJ. Please take a look at that PR, too, to see what we've done with BC. It would be good to make YJ as similar as possible."],"labels":["scipy.stats","enhancement"]},{"title":"RFC: Von Mises distribution (circular) moments","body":"### Describe your issue.\r\n\r\nCurrently, `vonmises.stats()` is very slow and does not compute the circular variance. While there are different definitions of circular kurtosis from a quick literature scan, the circular variance is pretty much standardized. The formula for the circular variance of the von Mises distribution is available: https:\/\/en.wikipedia.org\/wiki\/Von_Mises_distribution#Moments.\r\n\r\nFixing this would be easy to do (see below) but it breaks backwards compatability. Thinking of the general distribution infrastructure at the moment, I would like to gather feedback if this should be done now or later.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom scipy.stats import vonmises, circvar, circmean\r\nfrom scipy.special import i0e, i1e\r\n\r\nloc, kappa = 3, 10\r\n\r\nvonmises.stats(loc=loc, kappa=kappa, moments=\"mvs\")  # takes 2 sec on my machine\r\n# >>> (3.0, 0.10565504392764057, -1.75350482426206e-15)\r\n\r\nsamples = vonmises.rvs(loc=loc, kappa=kappa, size=100_000)\r\ncircmean(samples), circvar(samples)\r\n# >>> (2.9999484119189654, 0.05140615221071776)\r\n\r\n# using analytical formulas for the moments\r\nvar_analytic = 1 - i1e(kappa)\/i0e(kappa)  # takes 1 mu sec\r\nmean = loc\r\nskew = 0  # von mises distribution is symmetric\r\nmean, var_analytic, skew  \r\n# >>> (3, 0.05140017404515407, 0)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nN\/A\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nN\/A\r\n```\r\n","comments":[],"labels":["defect","scipy.stats","RFC"]},{"title":"BUG: scipy.linalg.schur sort ignores imaginary part of complex eigenvalues","body":"### Describe your issue.\n\nA real matrix can have complex (conjugate) eigenvalues.  scipy.linalg.schur provides the option to produce the **real** Schur decomposition of a real matrix and also the option for a user to provide a callable (a function of an eigenvalue) to be used in sorting the eigenvalues in a user-specified way.  The issue is that the callable receives only the real part of the eigenvalue from the schur routine, not the full complex-valued eigenvalue.  The accompanying code provides a small matrix that has complex conjugate eigenvalues; running it shows that the sort callable receives only the real part of the eigenvalues.  As a result, for this example the callable classifies the eigenvalues as inside the unit circle (because of the small real part) even though as shown in the output the eigenvalues have modulus of approximately 5.\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nfrom scipy import linalg\r\n\r\ndef ooouc(eval):\r\n    # Test whether an eigenvalue is on or outside the unit circle\r\n    ooouctf = abs(eval) >= 1.0\r\n    print(type(eval))\r\n    print('In ooouc',eval,eval.imag,ooouctf)\r\n    return ooouctf\r\n\r\na = np.array([[.15,5],[-5,.2]])\r\neigenvalues, eigenvectors = linalg.eig(a)\r\nprint(\"eigenvalues =\",eigenvalues)\r\nprint(\"modulus of eigenvalues =\",abs(eigenvalues))\r\n\r\nR,U,nbigroots = linalg.schur(a,output='real',sort=ooouc)\n```\n\n\n### Error message\n\n```shell\nRunning the code produces this output.\r\n\r\neigenvalues = [0.175+4.9999375j 0.175-4.9999375j]\r\nmodulus of eigenvalues = [5.0029991 5.0029991]\r\n<class 'float'>\r\nIn ooouc 0.17500000000000004 0.0 False\r\n<class 'float'>\r\nIn ooouc 0.17500000000000004 0.0 False\r\n<class 'float'>\r\nIn ooouc 0.17500000000000004 0.0 False\r\n<class 'float'>\r\nIn ooouc 0.17500000000000004 0.0 False\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.2 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-c6c8ru56\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\n```\n","comments":["I now believe this issue is not a bug but arises because of unclear documentation.  For the linalg.schur function, when output='real' is specified, I now see that the callable apparently must take two arguments--the first being the real part of the eigenvalue and the second being the imaginary part.  (In contrast, when output='complex', the callable takes a single argument--the complex eigenvalue.)\r\n\r\nThe linalg.schur documentation says \"A callable may be passed that, given a eigenvalue, returns a boolean denoting whether the eigenvalue should be sorted to the top-left (True).\"  In the output='real' case, this is potentially misleading.  Since complex numbers are available as a data type in linalg, one might assume, as I did, that \"given a eigenvalue\" means \"given an eigenvalue as a single complex number\" rather than what it evidently actually means in the output='real' case:  \"given the real part of the eigenvalue and the imaginary part as two separate arguments ...\"  The explanation in the documentation of the built-in string parameters (e.g. that for 'lhp') also leaves the impression that there must be a single complex argument passed to the callable, since (it only now seems clear to me) there are no built-in string parameters for the output='real' case.","Detaching the milestone: unlikely to get fixed in time for https:\/\/github.com\/scipy\/scipy\/pull\/19880#issuecomment-1992010395"],"labels":["scipy.linalg","Documentation"]},{"title":"MAINT\/BLD: resolve `scikit-umfpack` circular-dependency issue","body":"See https:\/\/github.com\/scipy\/scipy\/issues\/18912#issuecomment-1642418118.\r\n\r\nOnce the issue is resolved, the following lines should be removed, and `python tools\/generate_requirements.py` should be ran to update the requirements.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/ae3ca70a72ef04547fbc28501009246cef5ee6c8\/tools\/generate_requirements.py#L28-L31","comments":[],"labels":["Build issues","maintenance","DX"]},{"title":"MAINT: patch when `gmpy2` supports Python 3.12","body":"When `gmpy2` supports Python 3.12, the following lines should be removed, and `python tools\/generate_requirements.py` should be ran to update the requirements.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/ae3ca70a72ef04547fbc28501009246cef5ee6c8\/tools\/generate_requirements.py#L32-L35","comments":["Upstream issue: https:\/\/github.com\/aleaxit\/gmpy\/issues\/446"],"labels":["Build issues","maintenance","DX"]},{"title":"ENH: Better support for complex numbers in scipy.signal","body":"All the below is based off commit 35f3d3f01, which is after 1.12 release branch was made; 1.12 will likely serve as a reasonable baseline for the status described below.\r\n\r\n# Summary\r\n\r\nThis is a enhancement meta-proposal that I hope will argue the case that scipy.signal can achieve good support of complex-valued signals and complex-parameter systems with incremental changes.\r\n\r\nThe scope of what I propose is limited, and the following two goals are excluded:\r\n\r\n  - filter design of complex-coefficient systems\r\n  - conversion of complex-coefficient systems to second-order sections\r\n\r\nThis is a list of `scipy.signal` callables I think need changing, ordered approximately and subjectively from defect to enhancement.\r\n\r\n| callable        | comment                                                        |\r\n| --------------- | -------------------------------------------------------------- |\r\n| zpk2tf          | wrong result if z, p real and k complex; no tf2zpk round-trip  |\r\n| bilinear        | gh-13823; silent failure?                                      |\r\n| vectorstrength  | gives weird results with complex inputs                        |\r\n| filtfilt        | gh-17877                                                       |\r\n| sosfiltfilt     | check gh-17877 behaviour                                       |\r\n| coherence       | no complex unit test; needs a return\\_onesided argument        |\r\n| symiirorder1    | no unit tests. `#ifdef __GNUC__` only. See gh-18926            |\r\n| symiirorder2    | as symiirorder1                                                |\r\n| sepfir2d        | `#ifdef __GNUC__` only                                         |\r\n| savgol\\_filter  | complex values removed in output                               |\r\n| spline\\_filter  | forces complex to single precision; see e.g. cupy gh-7721      |\r\n| qspline1d\\_eval | perhaps a bug, given cspline1d\\_eval works?                    |\r\n| dlsim           | output set to float64 (np.zeros)                               |\r\n| lsim            | OK if params complex, but output type set to sys.A type        |\r\n| findfreqs       | return negative freqs for complex-coef sys                     |\r\n| freqresp        | OK, but should auto-choose negative freqs for complex-coef sys |\r\n| bode            | as freqresp                                                    |\r\n| dbode           | as freqresp                                                    |\r\n| dfreqresp       | as freqresp                                                    |\r\n| freqz           | auto \"whole=True\" for complex-coef                             |\r\n| freqz\\_zpk      | as freqz                                                       |\r\n| sosfreqz        | as freqz                                                       |\r\n| chirp           | gh-17318; perhaps a new function complex\\_chirp?               |\r\n| sweep\\_poly     | as chirp                                                       |\r\n| lombscargle     | assume possible; don't know how difficult                      |\r\n\r\nThere are four functions I haven't checked yet that may or may not need changes:\r\n\r\n  - cspline2d\r\n  - qspline2d\r\n  - check\\_COLA\r\n  - check\\_NOLA\r\n\r\n# Goals and constraints\r\n\r\nIn scipy.signal, support\r\n\r\n1.  Complex-valued signals and\r\n2.  Complex-parameter LTI systems (e.g., complex coef. TFs and complex SS matrices)\r\n\r\nscipy.signal should still support real-valued signals and real-parameter systems.\r\n\r\n## Motivation\r\n\r\nComplex-valued signals and systems are widely used in signal processing (see, e.g., <https:\/\/github.com\/scipy\/scipy\/pull\/18883#issuecomment-1636777541>).\r\n\r\nSuch systems are also used in specialized areas in control, e.g., 3-phase inverter control. For other control applications see <https:\/\/portal.research.lu.se\/portal\/files\/30365130\/2017ACC_0939_FI.pdf>\r\n\r\n## Backward compatibility\r\n\r\nMany, likely the great majority, of applications expect real-valued signals and real-parameter systems. Where necessary, support for complex numbers should be explicitly opt-in.\r\n\r\n## Non-goals\r\n\r\nTo be clear: these non-goals are **excluded** from the proposed changes.\r\n\r\n### Extending filter design for complex-coefficients\r\n\r\nThere are papers on this, but I haven't investigated in detail at all. We could add these later as new filter design functions.\r\n\r\nFor \"simple\" asymmetric design one can use normal (real-coefficient, zero-frequency-centred) low- or high-pass filters and frequency-shift the poles and zeros. See e.g. <https:\/\/www.mathworks.com\/help\/dsp\/ug\/complex-bandpass-filter-design.html>\r\n\r\n### Second-order section conversion\r\n\r\nConverting other forms (zpk, tf, \u2026) to second-order sections could probably be extended to support complex numbers, though it's not immediately clear how.\r\n\r\nArguably one only needs first-order sections if complex coefficients are allowed.\r\n\r\n### Conversion of complex-parameter to equivalent real-parameter systems\r\n\r\nSISO complex-parameter systems can be converted equivalent 2x2 MIMO real-parameter systems, but scipy.signal does not support MIMO systems.\r\n\r\n# scipy.signal complex-number support status\r\n\r\nPublic callables taken from docstring of scipy.signal.\\_\\_init\\_\\_ of commit 35f3d3f01.\r\n\r\nSupport status based on my reading of code and tests, and command-line experimentation. Please correct\\!\r\n\r\nMedian filters and peak finding need signal values to be ordered (that is, able-to-be-ordered; \"orderable?\"), and as I understand these cannot work with complex-valued signals.\r\n\r\n## Detail\r\n\r\nkey for \"status\"\r\n\r\n  - y = supported (possibly needs more unit tests)\r\n\r\n  - c = not supported or partially supported; change to support (needs change)\r\n\r\n  - i = impossible or unnecessary to support complex numbers (no change)\r\n\r\n  - n = could support, but leave (no change)\r\n\r\n  - d = deprecated (no change)\r\n\r\n  - (blank) = unknown\r\n    \r\n    | callable             | status | remark                                                                     |\r\n    | -------------------- | ------ | -------------------------------------------------------------------------- |\r\n    | convolve             | y      |                                                                            |\r\n    | correlate            | y      |                                                                            |\r\n    | fftconvolve          | y      |                                                                            |\r\n    | oaconvolve           | y      |                                                                            |\r\n    | convolve2d           | y      |                                                                            |\r\n    | correlate2d          | y      |                                                                            |\r\n    | sepfir2d             | c      | `#ifdef __GNUC__` only                                                     |\r\n    | choose\\_conv\\_method | i      |                                                                            |\r\n    | correlation\\_lags    | i      |                                                                            |\r\n    | bspline              | d      |                                                                            |\r\n    | cubic                | d      |                                                                            |\r\n    | quadratic            | d      |                                                                            |\r\n    | gauss\\_spline        | i      |                                                                            |\r\n    | cspline1d            | y      |                                                                            |\r\n    | qspline1d            | y      |                                                                            |\r\n    | cspline2d            |        | haven't figured out how to evaluate; probably should support complex?      |\r\n    | qspline2d            |        | as cspline2d                                                               |\r\n    | cspline1d\\_eval      | y      |                                                                            |\r\n    | qspline1d\\_eval      | c      | perhaps a bug, given cspline1d\\_eval works?                                |\r\n    | spline\\_filter       | c      | forces complex to single precision; see e.g. cupy gh-7721                  |\r\n    | order\\_filter        | i      | requires ordered signal values                                             |\r\n    | medfilt              | i      | requires ordered signal values                                             |\r\n    | medfilt2d            | i      | requires ordered signal values                                             |\r\n    | wiener               | y      | no complex test                                                            |\r\n    | symiirorder1         | c      | no unit tests. `#ifdef __GNUC__` only. See gh-18926                        |\r\n    | symiirorder2         | c      | as symiirorder1                                                            |\r\n    | lfilter              | y      |                                                                            |\r\n    | lfiltic              | y      | only error tests                                                           |\r\n    | lfilter\\_zi          | y      | only real tests                                                            |\r\n    | filtfilt             | c      | gh-17877                                                                   |\r\n    | savgol\\_filter       | c      | complex values removed in output                                           |\r\n    | deconvolve           | y      | only real tests, but quick check gives good results                        |\r\n    | sosfilt              | y      | complex type tested, but only with imag always 0                           |\r\n    | sosfilt\\_zi          | y      | as sosfilt                                                                 |\r\n    | sosfiltfilt          | c      | check gh-17877 behaviour                                                   |\r\n    | hilbert              | i      | inherent to the method?                                                    |\r\n    | hilbert2             | i      |                                                                            |\r\n    | decimate             | y      | gh-17877?                                                                  |\r\n    | detrend              | y      | only real tests                                                            |\r\n    | resample             | y      | tests complex type, imag=0?                                                |\r\n    | resample\\_poly       | y      |                                                                            |\r\n    | upfirdn              | y      |                                                                            |\r\n    | bilinear             | c      | gh-13823; silent failure?                                                  |\r\n    | bilinear\\_zpk        | y      | only tested with complex conj. pairs                                       |\r\n    | findfreqs            | c      | in principle could handle complex LTI                                      |\r\n    | firls                | n      | filter design                                                              |\r\n    | firwin               | n      | filter design                                                              |\r\n    | firwin2              | n      | filter design                                                              |\r\n    | freqs                | y      | uses findfreqs                                                             |\r\n    | freqs\\_zpk           | y      | uses findfreqs                                                             |\r\n    | freqz                | c      | auto \"whole=True\" for complex-coef                                         |\r\n    | freqz\\_zpk           | c      | as freqz                                                                   |\r\n    | sosfreqz             | c      | as freqz                                                                   |\r\n    | gammatone            | n      | filter design                                                              |\r\n    | group\\_delay         | n      | needs gh-19627                                                             |\r\n    | iirdesign            | n      | filter design                                                              |\r\n    | iirfilter            | n      | filter design                                                              |\r\n    | kaiser\\_atten        | n      |                                                                            |\r\n    | kaiser\\_beta         | n      |                                                                            |\r\n    | kaiserord            | n      |                                                                            |\r\n    | minimum\\_phase       | n      |                                                                            |\r\n    | savgol\\_coeffs       | n      |                                                                            |\r\n    | remez                | n      | filter design                                                              |\r\n    | unique\\_roots        | y      |                                                                            |\r\n    | residue              | y      |                                                                            |\r\n    | residuez             | y      |                                                                            |\r\n    | invres               | y      |                                                                            |\r\n    | invresz              | y      |                                                                            |\r\n    | BadCoefficients      | i      |                                                                            |\r\n    | abcd\\_normalize      | y      |                                                                            |\r\n    | band\\_stop\\_obj      | n      | filter design                                                              |\r\n    | besselap             | n      | filter design                                                              |\r\n    | buttap               | n      | filter design                                                              |\r\n    | cheb1ap              | n      | filter design                                                              |\r\n    | cheb2ap              | n      | filter design                                                              |\r\n    | cmplx\\_sort          | d      |                                                                            |\r\n    | ellipap              | d      | filter design                                                              |\r\n    | lp2bp                | d      | filter design                                                              |\r\n    | lp2bp\\_zpk           | d      | filter design                                                              |\r\n    | lp2bs                | d      | filter design                                                              |\r\n    | lp2bs\\_zpk           | d      | filter design                                                              |\r\n    | lp2hp                | d      | filter design                                                              |\r\n    | lp2hp\\_zpk           | d      | filter design                                                              |\r\n    | lp2lp                | d      | filter design                                                              |\r\n    | lp2lp\\_zpk           | d      | filter design                                                              |\r\n    | normalize            | y      |                                                                            |\r\n    | butter               | n      | filter design                                                              |\r\n    | buttord              | n      | filter design                                                              |\r\n    | cheby1               | n      | filter design                                                              |\r\n    | cheb1ord             | n      | filter design                                                              |\r\n    | cheby2               | n      | filter design                                                              |\r\n    | cheb2ord             | n      | filter design                                                              |\r\n    | ellip                | n      | filter design                                                              |\r\n    | ellipord             | n      | filter design                                                              |\r\n    | bessel               | n      | filter design                                                              |\r\n    | iirnotch             | n      | filter design                                                              |\r\n    | iirpeak              | n      | filter design                                                              |\r\n    | iircomb              | n      | filter design                                                              |\r\n    | lti                  |        |                                                                            |\r\n    | StateSpace           | y      |                                                                            |\r\n    | TransferFunction     | y      |                                                                            |\r\n    | ZerosPolesGain       | y      |                                                                            |\r\n    | lsim                 | c      | OK if params complex, but output type set to sys.A type                    |\r\n    | lsim2                | d      |                                                                            |\r\n    | impulse              | c      | depends on lsim                                                            |\r\n    | impulse2             | d      |                                                                            |\r\n    | step                 | c      | uses lsim; test\\_ltisys.TestStep.test\\_complex\\_input minimally tests step |\r\n    | step2                | d      |                                                                            |\r\n    | freqresp             | c      | OK, but should auto-choose negative freqs for complex-coef sys             |\r\n    | bode                 | c      | as freqresp                                                                |\r\n    | dlti                 |        |                                                                            |\r\n    | dlsim                | c      | output set to float64 (np.zeros)                                           |\r\n    | dimpulse             | c      |                                                                            |\r\n    | dstep                | c      |                                                                            |\r\n    | dfreqresp            | c      | as freqresp                                                                |\r\n    | dbode                | c      | as freqresp                                                                |\r\n    | tf2zpk               | y      | tested w\/ complex128, but tf coef imag part always (nearly\\!) zero         |\r\n    | tf2sos               | n      | \\*2sos tricky?                                                             |\r\n    | tf2ss                | y      | no complex tests                                                           |\r\n    | zpk2tf               | c      | wrong thing if k only complex                                              |\r\n    | zpk2sos              | n      | as tf2sos                                                                  |\r\n    | zpk2ss               | y      | no tests                                                                   |\r\n    | ss2tf                | y      |                                                                            |\r\n    | ss2zpk               | y      | ss2zpk not directly tested                                                 |\r\n    | sos2zpk              | y      |                                                                            |\r\n    | sos2tf               | y      |                                                                            |\r\n    | cont2discrete        | y      |                                                                            |\r\n    | place\\_poles         | n      |                                                                            |\r\n    | chirp                | c      | gh-17318; perhaps a new function complex\\_chirp?                           |\r\n    | gausspulse           | i      |                                                                            |\r\n    | max\\_len\\_seq        | i      |                                                                            |\r\n    | sawtooth             | i      |                                                                            |\r\n    | square               | i      |                                                                            |\r\n    | sweep\\_poly          | c      | as chirp                                                                   |\r\n    | unit\\_impulse        | i      |                                                                            |\r\n    | get\\_window          | i      |                                                                            |\r\n    | cascade              | d      |                                                                            |\r\n    | daub                 | d      |                                                                            |\r\n    | morlet               | d      |                                                                            |\r\n    | qmf                  | d      |                                                                            |\r\n    | ricker               | d      |                                                                            |\r\n    | morlet2              | d      |                                                                            |\r\n    | cwt                  | d      |                                                                            |\r\n    | argrelmin            | n      | requires ordered signal values                                             |\r\n    | argrelmax            | n      | as argrelmin                                                               |\r\n    | argrelextrema        | n      | as argrelmin                                                               |\r\n    | find\\_peaks          | n      | as argrelmin                                                               |\r\n    | find\\_peaks\\_cwt     | n      | as argrelmin                                                               |\r\n    | peak\\_prominences    | n      | as argrelmin                                                               |\r\n    | peak\\_widths         | n      | as argrelmin                                                               |\r\n    | periodogram          | y      |                                                                            |\r\n    | welch                | y      |                                                                            |\r\n    | csd                  | y      |                                                                            |\r\n    | coherence            | c      | no complex unit test; needs a return\\_onesided argument                    |\r\n    | spectrogram          | y      | has return\\_onesided. No complex tests.                                    |\r\n    | lombscargle          | c      | assume possible; don't know how difficult                                  |\r\n    | vectorstrength       | c      | gives weird results with complex inputs                                    |\r\n    | ShortTimeFFT         | y      |                                                                            |\r\n    | stft                 | d      |                                                                            |\r\n    | istft                | d      |                                                                            |\r\n    | check\\_COLA          |        | doesn't error on complex input; no idea on validity                        |\r\n    | check\\_NOLA          |        | doesn't error on complex input; no idea on validity                        |\r\n    | czt                  | y      | no explicit complex unit test                                              |\r\n    | zoom\\_fft            | y      | has complex unit test                                                      |\r\n    | CZT                  | y      | czt works                                                                  |\r\n    | ZoomFFT              | y      | zoom\\_fft works                                                            |\r\n    | czt\\_points          | y      |                                                                            |\r\n    \r\n\r\n# Related issues and PRs\r\n\r\n  - gh-4964 signal.dlsim cannot handle complex input\r\n  - gh-7228 scipy.signal.lsim fails if ZeroPolesGain to StateSpace produces complex matrix coefficients\r\n  - gh-13075 BUG: fix signal.dlsim for complex input\r\n  - gh-13823 scipy.signal.bilinear doesn't work for complex valued arrays\r\n  - gh-17318 ENH: Add functionality of Complex Chirp waveform to Scipy.signal\r\n  - gh-17877 BUG: filtfilt doesn't work with complex coefficient filters\r\n  - gh-18273 BUG: dstep doesn't work with dlti with complex coefficients.\r\n  - gh-18883 BUG: signal.dlsim: properly setting output dtype (\\#13075 & \\#4964)\r\n  - gh-19627 FIX: correctly compute group\\_delay for complex-coefficient TFs\r\n","comments":["Thanks @roryyorke for this great summary. It must be a substantial effort to hunt these down.\r\n\r\nWe have discussed the state of `scipy.signal` offline a few times among maintainers and indeed there is some work ahead of us. Before we start adding more features to the current state we have to make sure that `scipy.signal` is actually in good shape which is unfortunately not when it comes to LTI and dLTI parts and their system functions. The algorithms are rather the naive implementations (nothing should be news to you since you are neck deep in these issues on the python-control side) and fail almost all edge cases.\r\n\r\nWe discussed adding SLICOT directly a few years back and it was not received well. Back then, I was too naive to propose it and today I agree that we don't want anymore F77 code and the binary size goes up quite a bit. And it is not a walk in the park to compile it for all platforms due to its age. In fact we are unapologetically trying to remove our F77 dependence (#18566). I am not so far from the things I'd like to clean up and I would like to get to SLICOT eventually. Once the LTI parts stabilize then we can start adding delay systems and complex-coeff systems or other obscure control theory stuff. \r\n\r\nLong story short, we have to go through the LTI class first and bring it to a production code state and then it will not be a massive surgery to add features to `scipy.signal`. This is probably touch all the filter design functions too and that is the part that worries me the most.","Thanks @ilayn .\r\n\r\n> [LTI and dLTI algorithms] fail almost all edge cases\r\n\r\nDo you have examples, perhaps pointer to a discussion on this?\r\n\r\nOne algorithm that could be improved is zpk->ss, which currently goes via TF.\r\n\r\nDo you envision a low-level but public \"algorithm\" layer, operating on matrices, tf arrays, pole-zero arrays etc, and a higher-level system-focused layer?  Or only a high-level layer?\r\n\r\n> I would like to get to SLICOT\r\n\r\nWould this be a separate library, something like you discussed at https:\/\/github.com\/python-control\/python-control\/discussions\/928 ?\r\n\r\n> Long story short, we have to go through the LTI class first and bring it to a production code state\r\n\r\nOK, I understand.\r\n\r\n> This is probably touch all the filter design functions too and that is the part that worries me the most.\r\n\r\nWhy do the filter design functions need to change - do you want them to return LTI objects?\r\n\r\nOf the functions I listed, I think the following spectral analysis functions could be updated independently of the dLTI \/ LTI rework:\r\n\r\n  - coherence\r\n  - lombscargle\r\n  - vectorstrength\r\n\r\nsepfir2d could also be updated separately, though I wonder if that could be implemented via ndimage.convolve1d (I have no idea, maybe they are quite different).\r\n\r\nsymiiorder[1,2] and the spline functions have stalled PR gh-18926 to implement these i.t.o. lower-level functions, so I think it's not worth looking at complex-coefficient support for them separately.\r\n\r\nAlso independent of the dLTI and LTI changes would be adding complex forms of `chirp` and `sweep_poly`, though this is fairly esoteric."],"labels":["enhancement","scipy.signal"]},{"title":"BUG: `IndexError` when giving a zero distance matrix to `multiscale_graphcorr`","body":"### Describe your issue.\n\n### Error\r\nOn calling `scipy.stats.multiscale_graphcorr` with either its `x` or `y` parameter (as distance matrices, hence with `compute_distance=None`) contains only zero values, a `IndexError` is raised.\r\n\r\n### Investigation\r\nInvestigating the stack trace led me to the file `scipy\/stats\/_stats_py.py` lines 6471 to 6479*, where I see:\r\n```py\r\n    # calculate MGC map and optimal scale\r\n    stat_mgc_map = _local_correlations(distx, disty, global_corr='mgc')\r\n\r\n    n, m = stat_mgc_map.shape\r\n    if m == 1 or n == 1:\r\n        # the global scale at is the statistic calculated at maximial nearest\r\n        # neighbors. There is not enough local scale to search over, so\r\n        # default to global scale\r\n        stat = stat_mgc_map[m - 1][n - 1]\r\n```\r\n*these lines number are taken from the version of the file on the repo as of now ([ec98497](https:\/\/github.com\/scipy\/scipy\/commit\/ec98497595cd7eb39069ba5ab9117d306fe89579))\r\n\r\nIt appears to me as a mismatch between the definition, `n, m = stat_mgc_map.shape`, and its use as `stat_mgc_map[m - 1][n - 1]`, as it looks like that the arrays output by `_local_correlations` are not necessarily square.\r\n\r\nHowever I cannot say I 100% understood how MGC works, hence I wonder if this is actually intended.\r\n\r\n### Context\r\nFor information, I got this error through via `hyppo` package, see [this issue](https:\/\/github.com\/neurodata\/hyppo\/issues\/409).\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\n\r\nfrom scipy.stats import multiscale_graphcorr\r\nfrom sklearn.metrics import pairwise_distances\r\n\r\nn = 6\r\n\r\nx = np.arange(n).reshape(-1, 1)\r\ndistx = pairwise_distances(x, x)\r\ndisty = np.zeros((n, n))\r\nmgc = multiscale_graphcorr(disty, distx, compute_distance=None, reps=0)\n```\n\n\n### Error message\n\n```shell\n...\/.env\/lib\/python3.10\/site-packages\/scipy\/stats\/_stats_py.py:6477: RuntimeWarning: The number of replications is low (under 1000), and p-value calculations may be unreliable. Use the p-value result, with caution!\r\n  warnings.warn(msg, RuntimeWarning)\r\nTraceback (most recent call last):\r\n  File \"...\/problem-scipy.py\", line 15, in <module>\r\n    main()\r\n  File \"...\/problem-scipy.py\", line 12, in main\r\n    mgc = multiscale_graphcorr(disty, distx, compute_distance=None, reps=0)\r\n  File \"...\/.env\/lib\/python3.10\/site-packages\/scipy\/stats\/_stats_py.py\", line 6490, in multiscale_graphcorr\r\n    stat, stat_dict = _mgc_stat(x, y)\r\n  File \"...\/.env\/lib\/python3.10\/site-packages\/scipy\/stats\/_stats_py.py\", line 6545, in _mgc_stat\r\n    stat = stat_mgc_map[m - 1][n - 1]\r\nIndexError: index 5 is out of bounds for axis 0 with size 1\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.23.5 sys.version_info(major=3, minor=10, micro=13, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-6ckgqyn6\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\n```\n","comments":["@lucascolley I'm the author of both bits of code. Could I be assigned this issue?","Hi @sampan501 , go ahead! We don't assign people to issues, feel free to take on anything which nobody else is working on \ud83d\udc4d","@Rayerdyne Looking into the linked issue, I believe the issue relies more on the MGCX code in `hyppo` rather than the MGC code in `scipy`. The `_local_correlations` function will not output a square matrix each time when there are redundant rows or columns. That is a property of the algorithm.\r\n\r\nAs for the reproducing code example, I'm realizing now that the code should throw an error when the inputs have 0 variance (it doesn't make much sense to run an independence test in this setting)","@sampan501 I see, thanks!","> As for the reproducing code example, I'm realizing now that the code should throw an error when the inputs have 0 variance (it doesn't make much sense to run an independence test in this setting)\n\nWould either of you like to submit a PR for this?","I can do this"],"labels":["defect","scipy.stats"]},{"title":"BLD: Coalesce binops to cut `_sparsetools.so` library size in half","body":"#### Reference issue\r\nNo issue, just noticed this opportunity while working on another related project.\r\n\r\n#### What does this implement\/fix?\r\nCSR sparse matrix element-wise binary operators are implemented as templates. The binop operation is fairly large and instantiated many times, so binops account for a significant fraction of both compilation time and library size.\r\n\r\nBinops are largely memory bound. That means slightly increasing the cost of the op, even in the inner loop, has little visible performance impact. Therefore the less common operators can be grouped into one implementation with the operand chosen at runtime. This allows two template instantiations to cover all operands. Two because the comparators (greater, less, not_equal_to, etc) return bool while the other operators return T.\r\n\r\nPlus and Minus are left alone.\r\n\r\n### Build Impact\r\n\r\n`_sparsetools.so` goes from 4.0 MB to 2.2 MB.\r\n\r\nTotal `python build.py dev` runtime drops by about 5%, but the very large`csr.cxx.o` and `bsr.cxx.o` compilation time drops significantly (big red and purple blocks on the bottom rows):\r\n\r\nmain branch:\r\n![main timeline](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/88124bf1-6870-4487-bf5e-7ed439edf76f)\r\n\r\nthis PR:\r\n![smallcsr timeline](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/3d2e5e2f-d10b-4993-9e0a-66569535f3c6)\r\n\r\nThese may be the dominating steps for folks with 20+ core machines so those folks may see a noticeable speedup.\r\n\r\n### Runtime Impact\r\n\r\nI'm seeing up to 5% impact on elementwise multiply on very large (1M+ nnz) matrices, and less on smaller ones. Note that #19765 speeds up these methods by more than this amount.","comments":["Nice idea, thanks Adam!\r\n\r\n> I'm seeing up to 5% impact on elementwise multiply on very large (1M+ nnz) matrices, and less on smaller ones. Note that #19765 speeds up these methods by more than this amount.\r\n\r\nIt looks like the impact is larger on some benchmarks. It still sounds like a good idea to me though, given that the sum of this PR and gh-19765 halves binary size _and_ improves performance. \r\n\r\nOf course benchmarking results are always tricky to get right. Here what I did was remove the `--quick` flag from `cmd_compare` in `dev.py` (needed to get stable results in this case) and then run\r\n```\r\npython dev.py bench --compare main -t sparse.Arithmetic\r\n```\r\n\r\nWith the default matrix sizes in the benchmarks (`poisson2d(250)`) ~= 250,000 elements:\r\n\r\n```\r\n| Change   | Before [bee9b9ae] <main>   | After [b667240a] <smallcsr>   |   Ratio | Benchmark (Parameter)                                      |\r\n|----------|----------------------------|-------------------------------|---------|------------------------------------------------------------|\r\n| +        | 3.18\u00b10.04ms                | 4.30\u00b10.3ms                    |    1.35 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 'multiply') |\r\n| +        | 3.17\u00b10.05ms                | 4.12\u00b10.2ms                    |    1.3  | sparse.Arithmetic.time_arithmetic('csc', 'BB', 'multiply') |\r\n| +        | 3.24\u00b10.1ms                 | 4.11\u00b10.1ms                    |    1.27 | sparse.Arithmetic.time_arithmetic('coo', 'BB', 'multiply') |\r\n| +        | 3.16\u00b10.02ms                | 3.96\u00b10.1ms                    |    1.26 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 'multiply') |\r\n| +        | 1.32\u00b10.02ms                | 1.64\u00b10.01ms                   |    1.24 | sparse.Arithmetic.time_arithmetic('csc', 'AA', 'multiply') |\r\n| +        | 9.57\u00b10.1ms                 | 11.8\u00b12ms                      |    1.23 | sparse.Arithmetic.time_arithmetic('dia', 'AB', '__add__')  |\r\n| +        | 3.02\u00b10.07ms                | 3.58\u00b10.06ms                   |    1.18 | sparse.Arithmetic.time_arithmetic('csc', 'AB', 'multiply') |\r\n| +        | 1.21\u00b10.02ms                | 1.39\u00b10.02ms                   |    1.15 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 'multiply') |\r\n| +        | 9.54\u00b10.1ms                 | 10.8\u00b10.3ms                    |    1.13 | sparse.Arithmetic.time_arithmetic('dia', 'BA', 'multiply') |\r\n| +        | 3.09\u00b10.02ms                | 3.40\u00b10.1ms                    |    1.1  | sparse.Arithmetic.time_arithmetic('csc', 'BA', 'multiply') |\r\n| +        | 2.99\u00b10.03ms                | 3.30\u00b10.03ms                   |    1.1  | sparse.Arithmetic.time_arithmetic('csr', 'BA', 'multiply') |\r\n| +        | 6.92\u00b10.04ms                | 7.50\u00b10.4ms                    |    1.08 | sparse.Arithmetic.time_arithmetic('coo', 'AB', '__sub__')  |\r\n| +        | 7.03\u00b10.2ms                 | 7.43\u00b10.2ms                    |    1.06 | sparse.Arithmetic.time_arithmetic('coo', 'BA', 'multiply') |\r\n| +        | 9.30\u00b10.2ms                 | 9.86\u00b10.2ms                    |    1.06 | sparse.Arithmetic.time_arithmetic('dia', 'BA', '__add__')  |\r\n| -        | 986\u00b120\u03bcs                   | 929\u00b17\u03bcs                       |    0.94 | sparse.Arithmetic.time_arithmetic('csr', 'AA', '__sub__')  |\r\n```\r\n\r\nIncreasing the sizes to (`poisson2d(1000)`) ~= 4M elements (this takes a while to run, comment out all but `multiply` to make it run faster):\r\n```\r\n| Change   | Before [bee9b9ae] <main>   | After [b667240a] <smallcsr>   |   Ratio | Benchmark (Parameter)                                      |\r\n|----------|----------------------------|-------------------------------|---------|------------------------------------------------------------|\r\n| +        | 62.5\u00b13ms                   | 78.2\u00b12ms                      |    1.25 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 'multiply') |\r\n| +        | 72.7\u00b11ms                   | 85.3\u00b11ms                      |    1.17 | sparse.Arithmetic.time_arithmetic('csc', 'BB', 'multiply') |\r\n| +        | 68.4\u00b10.6ms                 | 80.3\u00b10.3ms                    |    1.17 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 'multiply') |\r\n| +        | 72.8\u00b11ms                   | 83.6\u00b11ms                      |    1.15 | sparse.Arithmetic.time_arithmetic('coo', 'BB', 'multiply') |\r\n| +        | 29.2\u00b10.9ms                 | 33.5\u00b10.5ms                    |    1.14 | sparse.Arithmetic.time_arithmetic('csc', 'AA', 'multiply') |\r\n| +        | 24.0\u00b10.3ms                 | 27.3\u00b10.5ms                    |    1.14 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 'multiply') |\r\n| +        | 65.6\u00b11ms                   | 72.1\u00b10.4ms                    |    1.1  | sparse.Arithmetic.time_arithmetic('csc', 'AB', 'multiply') |\r\n| +        | 57.1\u00b12ms                   | 62.7\u00b12ms                      |    1.1  | sparse.Arithmetic.time_arithmetic('csr', 'BA', 'multiply') |\r\n| +        | 65.9\u00b10.3ms                 | 70.4\u00b10.3ms                    |    1.07 | sparse.Arithmetic.time_arithmetic('csc', 'BA', 'multiply') |\r\n| -        | 15.6\u00b10.3ms                 | 14.6\u00b10.2ms                    |    0.94 | sparse.Arithmetic.time_arithmetic('csr', 'AA', '__sub__')  |\r\n```\r\nSo it looks like ~20% impact on average for element-wise multiplication here. This is with the default dev build settings on a x86-64 Linux machine; I didn't check whether release settings (`-O3` vs `-O2`, no debug info) make a difference here.","Thanks for the PR, @alugowski! This is a useful experiment, and I really appreciate the analysis here.\r\n\r\nMy current position is that it seems like the performance hit may not be worth the reduction in build size, but I would love to get more benchmark data if folks want to try this out on their local environments. There may also be a middle-ground option that does less runtime dispatch but still reduces the total number of template instantiations.","There's still a bit of debate about the tradeoffs here, and it certainly isn't crucial before I branch for `1.13.0` to support NumPy 2, so I'll just bump the milestone."],"labels":["scipy.sparse","Build issues","C\/C++"]},{"title":"ENH: single-pass CSR sparse matrix binops","body":"#### Reference issue\r\nNo issue, saw this optimization opportunity while working in this file.\r\n\r\n#### What does this implement\/fix?\r\n\r\nThe CSR elementwise binary operators have two implementations: one for canonical form (no duplicate elements, elements in order) and another for general.\r\n\r\nPrevious behavior first checks both operands then selects the appropriate implementation. However this check requires a full scan of both operands. The canonical form binop operation itself is also a single scan of both operands, so the form check adds an extra scan. Binops are a memory bandwidth-bound operation.\r\n\r\nThis PR merges the format check with the canonical form implementation. In other words, merges `csr_has_canonical_format()` into `csr_binop_csr_canonical()`.\r\n\r\nNew behavior is to optimistically execute the canonical implementation with a fallback to the general if the operands are not canonical, eliminating one full memory scan of both operands.\r\n\r\n#### Impact\r\nExpect a 25% to 45% speedup on large matrices.\r\n\r\nSee attached a synthetic `bench_sparse_add.py` script which benchmarks elementwise add runtime on random and Poisson 2d matrices of increasing nnz:\r\n\r\n|   | main | this PR | Speedup |\r\n|---|------|---------|---------|\r\n| random-50,000 | 0.00028791697695851326 | 0.00015637511387467384| 1.84    \r\n| poisson-49,600 | 0.0001479999627918005 | 0.00010145897977054119| 1.46    \r\n| random-500,000 | 0.001349416095763445 | 0.0009573330171406269| 1.41    \r\n| poisson-498,016 | 0.0014221249148249626 | 0.0010420831385999918| 1.36    \r\n| random-5,000,000 | 0.014241500059142709 | 0.011061792029067874| 1.29    \r\n| poisson-4,996,000 | 0.015750708989799023 | 0.010916250059381127| 1.44    \r\n| random-50,000,000 | 0.15272545884363353 | 0.11954350001178682| 1.28    \r\n| poisson-49,978,572 | 0.16468741605058312 | 0.1213894160464406| 1.36 \r\n\r\ntry yourself: `python dev.py python bench_sparse_add.py`\r\n\r\n[bench_sparse_add.py.txt](https:\/\/github.com\/scipy\/scipy\/files\/13774324\/bench_sparse_add.py.txt)\r\n","comments":["Thanks @alugowski!\r\n\r\nFirst benchmark run (same as on gh-19766) seems to show that things are indeed significantly faster, except for the DIA format. \r\n```\r\n| Change   | Before [bee9b9ae] <main>   | After [f427cf01] <fastbinop>   |   Ratio | Benchmark (Parameter)                                      |\r\n|----------|----------------------------|--------------------------------|---------|------------------------------------------------------------|\r\n| +        | 47.1\u00b10.7ms                 | 54.9\u00b15ms                       |    1.17 | sparse.Arithmetic.time_arithmetic('dia', 'BB', '__sub__')  |\r\n| +        | 67.1\u00b11ms                   | 77.9\u00b17ms                       |    1.16 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 'multiply') |\r\n| +        | 67.8\u00b10.2ms                 | 77.3\u00b17ms                       |    1.14 | sparse.Arithmetic.time_arithmetic('dia', 'BB', '__add__')  |\r\n| -        | 72.0\u00b17ms                   | 64.6\u00b11ms                       |    0.9  | sparse.Arithmetic.time_arithmetic('csr', 'AB', '__sub__')  |\r\n| -        | 162\u00b17ms                    | 143\u00b13ms                        |    0.88 | sparse.Arithmetic.time_arithmetic('coo', 'AB', 'multiply') |\r\n| -        | 174\u00b120ms                   | 145\u00b11ms                        |    0.84 | sparse.Arithmetic.time_arithmetic('coo', 'AB', '__sub__')  |\r\n| -        | 25.7\u00b12ms                   | 21.1\u00b10.09ms                    |    0.82 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 'multiply') |\r\n| -        | 23.3\u00b12ms                   | 18.9\u00b10.4ms                     |    0.81 | sparse.Arithmetic.time_arithmetic('csc', 'AA', '__sub__')  |\r\n| -        | 76.3\u00b18ms                   | 61.0\u00b12ms                       |    0.8  | sparse.Arithmetic.time_arithmetic('csr', 'AB', 'multiply') |\r\n| -        | 26.8\u00b13ms                   | 18.3\u00b10.2ms                     |    0.68 | sparse.Arithmetic.time_arithmetic('csr', 'AA', '__add__')  |\r\n```\r\n\r\nThese benchmarks need a bit of care, because the result is now going to depend strongly on whether the result is in canonical form or not. Here, it turns out that `A` is canonical while `B` is not, due to it being constructed as `A ** 2`.\r\n\r\nIf we insert something like this in the benchmark setup code to ensure both matrices are in canonical form:\r\n```python\r\n        if hasattr(x, 'sum_duplicates'):\r\n            x.sum_duplicates()\r\n        if hasattr(self.y, 'sum_duplicates'):\r\n            self.y.sum_duplicates()\r\n```\r\nwe get:\r\n```\r\n| Change   | Before [bee9b9ae] <main>   | After [f427cf01] <fastbinop>   |   Ratio | Benchmark (Parameter)                                      |\r\n|----------|----------------------------|--------------------------------|---------|------------------------------------------------------------|\r\n| +        | 28.4\u00b10.3ms                 | 40.1\u00b10.7ms                     |    1.41 | sparse.Arithmetic.time_arithmetic('csr', 'BB', '__sub__')  |\r\n| +        | 28.5\u00b10.4ms                 | 39.1\u00b10.3ms                     |    1.37 | sparse.Arithmetic.time_arithmetic('coo', 'BB', '__sub__')  |\r\n| +        | 28.3\u00b10.2ms                 | 38.9\u00b10.3ms                     |    1.37 | sparse.Arithmetic.time_arithmetic('dia', 'BB', '__sub__')  |\r\n| +        | 30.2\u00b11ms                   | 39.5\u00b10.4ms                     |    1.31 | sparse.Arithmetic.time_arithmetic('csc', 'BB', '__sub__')  |\r\n| +        | 50.0\u00b12ms                   | 64.6\u00b13ms                       |    1.29 | sparse.Arithmetic.time_arithmetic('csr', 'AB', '__sub__')  |\r\n| +        | 46.8\u00b12ms                   | 60.0\u00b12ms                       |    1.28 | sparse.Arithmetic.time_arithmetic('csr', 'AB', 'multiply') |\r\n| +        | 51.4\u00b10.1ms                 | 64.5\u00b11ms                       |    1.26 | sparse.Arithmetic.time_arithmetic('csc', 'BA', '__sub__')  |\r\n| +        | 51.3\u00b13ms                   | 64.8\u00b11ms                       |    1.26 | sparse.Arithmetic.time_arithmetic('csr', 'AB', '__add__')  |\r\n| +        | 52.7\u00b12ms                   | 66.0\u00b11ms                       |    1.25 | sparse.Arithmetic.time_arithmetic('csc', 'AB', '__sub__')  |\r\n| +        | 50.8\u00b12ms                   | 63.7\u00b12ms                       |    1.25 | sparse.Arithmetic.time_arithmetic('csr', 'BA', '__sub__')  |\r\n| +        | 54.7\u00b10.8ms                 | 67.7\u00b10.9ms                     |    1.24 | sparse.Arithmetic.time_arithmetic('csc', 'AB', 'multiply') |\r\n| +        | 62.5\u00b11ms                   | 76.9\u00b11ms                       |    1.23 | sparse.Arithmetic.time_arithmetic('coo', 'AB', '__add__')  |\r\n| +        | 54.3\u00b10.5ms                 | 66.8\u00b10.9ms                     |    1.23 | sparse.Arithmetic.time_arithmetic('csc', 'BA', 'multiply') |\r\n| +        | 48.2\u00b12ms                   | 58.7\u00b13ms                       |    1.22 | sparse.Arithmetic.time_arithmetic('csr', 'BA', 'multiply') |\r\n| +        | 60.3\u00b10.9ms                 | 73.0\u00b12ms                       |    1.21 | sparse.Arithmetic.time_arithmetic('coo', 'AB', 'multiply') |\r\n| +        | 52.3\u00b13ms                   | 63.4\u00b12ms                       |    1.21 | sparse.Arithmetic.time_arithmetic('csr', 'BA', '__add__')  |\r\n| +        | 62.3\u00b12ms                   | 74.6\u00b12ms                       |    1.2  | sparse.Arithmetic.time_arithmetic('coo', 'AB', '__sub__')  |\r\n| +        | 61.9\u00b11ms                   | 73.6\u00b12ms                       |    1.19 | sparse.Arithmetic.time_arithmetic('coo', 'BA', '__sub__')  |\r\n| +        | 54.2\u00b10.8ms                 | 64.5\u00b11ms                       |    1.19 | sparse.Arithmetic.time_arithmetic('csc', 'AB', '__add__')  |\r\n| +        | 54.0\u00b10.6ms                 | 64.0\u00b10.4ms                     |    1.19 | sparse.Arithmetic.time_arithmetic('csc', 'BA', '__add__')  |\r\n| +        | 65.3\u00b14ms                   | 75.8\u00b12ms                       |    1.16 | sparse.Arithmetic.time_arithmetic('coo', 'BA', 'multiply') |\r\n| +        | 210\u00b16ms                    | 233\u00b17ms                        |    1.11 | sparse.Arithmetic.time_arithmetic('dia', 'BA', '__sub__')  |\r\n| +        | 67.4\u00b12ms                   | 74.2\u00b12ms                       |    1.1  | sparse.Arithmetic.time_arithmetic('coo', 'BA', '__add__')  |\r\n| +        | 209\u00b15ms                    | 222\u00b18ms                        |    1.06 | sparse.Arithmetic.time_arithmetic('dia', 'AB', '__add__')  |\r\n| +        | 213\u00b16ms                    | 225\u00b17ms                        |    1.06 | sparse.Arithmetic.time_arithmetic('dia', 'BA', 'multiply') |\r\n| -        | 59.3\u00b12ms                   | 53.9\u00b10.6ms                     |    0.91 | sparse.Arithmetic.time_arithmetic('coo', 'BB', '__add__')  |\r\n| -        | 60.4\u00b10.9ms                 | 54.6\u00b10.6ms                     |    0.9  | sparse.Arithmetic.time_arithmetic('csc', 'BB', '__add__')  |\r\n| -        | 59.3\u00b10.5ms                 | 53.4\u00b10.1ms                     |    0.9  | sparse.Arithmetic.time_arithmetic('dia', 'BB', '__add__')  |\r\n| -        | 53.1\u00b12ms                   | 47.1\u00b12ms                       |    0.89 | sparse.Arithmetic.time_arithmetic('coo', 'AA', '__add__')  |\r\n| -        | 26.8\u00b10.6ms                 | 23.8\u00b10.2ms                     |    0.89 | sparse.Arithmetic.time_arithmetic('csc', 'AA', 'multiply') |\r\n| -        | 26.9\u00b10.3ms                 | 20.9\u00b10.2ms                     |    0.78 | sparse.Arithmetic.time_arithmetic('csc', 'AA', '__add__')  |\r\n| -        | 23.5\u00b10.3ms                 | 18.3\u00b10.2ms                     |    0.78 | sparse.Arithmetic.time_arithmetic('csr', 'AA', '__add__')  |\r\n```\r\n\r\n`B` should be in canonical form there, but the results hint at the new single-pass check for that failing for `__sub__` and `multiply`. At least, otherwise I can't explain why those results regress here. I'd also expect the `__sub__` and `__add__` results to be symmetric, so something isn't quite right yet it looks like.\r\n\r\nThere's also a all of `-Wmaybe-uninitialized` compiler warnings - it'd be good to address those, and perhaps that catches something.\r\n\r\nRunning the `bench_sparse_add.py` script from the PR description, the results improve but not as much as in the PR description, more like in the 0% - 10% range for me:\r\n\r\n<details>\r\n\r\n```\r\nrandom-50,000:  0.00027637499442789704\r\npoisson-49,600:         0.00023411099391523749\r\nrandom-500,000:         0.001635348002309911\r\npoisson-498,016:        0.0017717820010147989\r\nrandom-5,000,000:       0.023091215000022203\r\npoisson-4,996,000:      0.02406818899908103\r\nrandom-50,000,000:      0.22385444400424603\r\npoisson-49,978,572:     0.23443531700468156\r\n\r\n# main:\r\nrandom-50,000:  0.00038524300907738507\r\npoisson-49,600:         0.00021164999634493142\r\nrandom-500,000:         0.00149274300201796\r\npoisson-498,016:        0.0016441019979538396\r\nrandom-5,000,000:       0.022221983992494643\r\npoisson-4,996,000:      0.025533260995871387\r\nrandom-50,000,000:      0.21116333699319512\r\npoisson-49,978,572:     0.2499628399964422\r\n\r\n# main run 2:\r\nrandom-50,000:  0.00038719200529158115\r\npoisson-49,600:         0.00022731799981556833\r\nrandom-500,000:         0.001550414992379956\r\npoisson-498,016:        0.0016773520037531853\r\nrandom-5,000,000:       0.022551914007635787\r\npoisson-4,996,000:      0.02617731600184925\r\nrandom-50,000,000:      0.21719301299890503\r\npoisson-49,978,572:     0.2543543719948502\r\n\r\n# this PR, run 2:\r\nrandom-50,000:  0.0002743800141615793\r\npoisson-49,600:         0.00022587399871554226\r\nrandom-500,000:         0.0016353649989468977\r\npoisson-498,016:        0.0016516460018465295\r\nrandom-5,000,000:       0.02321807699627243\r\npoisson-4,996,000:      0.023670254988246597\r\nrandom-50,000,000:      0.2263502830028301\r\npoisson-49,978,572:     0.23783767299028113\r\n```\r\n\r\n<\/details>\r\n\r\nI'm not yet sure what to make of all this. Maybe you can run the same asv benchmark @alugowski and see how that looks for you?","Repeating the benchmark script on a macOS arm64 (M1) machine, I do see similar results as in the PR description:\r\n\r\n<details>\r\n\r\n```\r\n# this PR\r\nrandom-50,000:  0.00017312500131083652\r\npoisson-49,600:         0.00012250000145286322\r\nrandom-500,000:         0.0010954169993055984\r\npoisson-498,016:        0.0013311670045368373\r\nrandom-5,000,000:       0.01243333300226368\r\npoisson-4,996,000:      0.013911583999288268\r\nrandom-50,000,000:      0.12745545800135005\r\npoisson-49,978,572:     0.14511337500152877\r\n\r\n# run 2:\r\nrandom-50,000:  0.00017150000348920003\r\npoisson-49,600:         0.0001229170011356473\r\nrandom-500,000:         0.0010238329996354878\r\npoisson-498,016:        0.001206166998599656\r\nrandom-5,000,000:       0.013366833998588845\r\npoisson-4,996,000:      0.013843332999385893\r\nrandom-50,000,000:      0.12439233400073135\r\npoisson-49,978,572:     0.1430313750024652\r\n\r\n\r\n# main:\r\nrandom-50,000:  0.00029024999821558595\r\npoisson-49,600:         0.00013862500054528937\r\nrandom-500,000:         0.0014803340018261224\r\npoisson-498,016:        0.0016382500034524128\r\nrandom-5,000,000:       0.015349457993579563\r\npoisson-4,996,000:      0.015778542001498863\r\nrandom-50,000,000:      0.15929004100325983\r\npoisson-49,978,572:     0.16495433400268666\r\n\r\n# run 2:\r\nrandom-50,000:  0.00027474999660626054\r\npoisson-49,600:         0.0001384170027449727\r\nrandom-500,000:         0.0013510419958038256\r\npoisson-498,016:        0.0014166249966365285\r\nrandom-5,000,000:       0.016083499998785555\r\npoisson-4,996,000:      0.01596837499528192\r\nrandom-50,000,000:      0.15811200000462122\r\npoisson-49,978,572:     0.16117887500149664\r\n```\r\n\r\n<\/details>\r\n\r\nIt looks like the trade-off here depends pretty strongly on CPU and memory architecture."],"labels":["enhancement","scipy.sparse","C\/C++"]},{"title":"MAINT: stats: fix axis_nan_policy decorator non-broadcastable paired-sample statistics","body":"#### Reference issue\r\nFixes https:\/\/github.com\/scipy\/scipy\/pull\/19578#pullrequestreview-1766857165\r\n\r\n#### What does this implement\/fix?\r\nThe axis\/nan_policy decorator had edge case bugs with paired sample statistics when the length along `axis` caused the samples to be non-broadcastable:\r\n\r\n- When the arrays were not empty, there was an error (\"ValueError: unequal length arrays\"), but it was not the intended one (\"ValueError: Array shapes are incompatible for broadcasting.\").\r\n- When the arrays were empty, the function returned an array of NaNs that would have been appropriate for an independent sample statistic, but is not appropriate for a paired-sample statistic because the lengths along `axis` must be the same or `1`.\r\n\r\nThis fixes the bug(s) and adds tests.\r\n","comments":[],"labels":["scipy.stats","maintenance"]},{"title":"BUG: Bad result for stats.randint.pmf (edge case)","body":"### Describe your issue.\r\n\r\nEach time we execute `randint.pmf(x, a, b)` where\r\n- `a < 0`\r\n- b between a + 2\\**31 and 2**31 - 1\r\n\r\nWe should expect to get 1\/(2**31) when x is in {a, ..., b -1} but we get 0 instead. This behavior no longer appears when b is created with np.arange or when b is an array-like with a single value which does not satisfy as stated above.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom scipy.stats import randint\r\n\r\na = -354\r\nx = 347\r\nmax_range = abs(a)\r\n\r\nall_b_1 = [a + 2**31 + i for i in range(max_range)]\r\nprint((randint.pmf(325, a, all_b_1) == 0).all())  # output: True\r\n\r\n\r\nall_b_2 = [a + 2**31 + i for i in range(max_range + 1)]\r\nprint((randint.pmf(325, a, all_b_2) == 0).any())  # output: False\r\n\r\nall_b_3 = np.array(all_b_1)\r\nprint((randint.pmf(325, a, all_b_3) == 0).all())  # output: True\r\n\r\nall_b_4 = np.arange(a + 2**31, 2**31)\r\nprint((randint.pmf(325, a, all_b_4) == 0).any())  # output: False\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nNone\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.26.2 sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-vx80imvw\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: true\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-ojr8nxxo\\cp312-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.12'\r\n```\r\n","comments":[],"labels":["defect","scipy.stats"]},{"title":"scipy.sparse.linalg.eigsh() output eigenvector non-deterministic for multiple eigenvalues","body":"### Describe your issue.\n\nI utilize scipy.sparse.linalg.eigsh() to calculate eigenvalues and eigenvectors within my algorithm. I hope the outputs of scipy.sparse.linalg.eigsh() are same for the same input matrix. But I found not. Especially for the matrix with multiple eigenvalues, Within a 'For' loop, I got different eigenvectors for the same input matrix. How to solve it? Thx.\r\n\r\nFor example, I want to calculate the eigenvectors of a identity matrix, 'eigsh()' doesn't have a parameter, such as random_seed, to fix the output eigenvectors to be same.\r\n![Uploading image.png\u2026]()\r\n\n\n### Reproducing Code Example\n\n```python\nimport scipy.sparse.linalg\r\nimport numpy as np\r\nfrom scipy.sparse import csr_matrix\r\nA = csr_matrix(np.identity(3))\r\nfor i in range(10):\r\n    eigenvalue, eigenvector = scipy.sparse.linalg.eigsh(A,\r\n                              2,\r\n                              which= 'SM',\r\n                              ncv = 43,\r\n                              v0 = np.ones(A.shape[0]),\r\n                              maxiter = 13615)\r\n    print('\u7b2c', i, '\u8f6e')\r\n    print('\u7279\u5f81\u503c\uff1a', eigenvalue)\r\n    print('\u7279\u5f81\u5411\u91cf\uff1a', eigenvector)\n```\n\n\n### Error message\n\n```shell\n\u7b2c 0 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.55721834  0.59680347]\r\n [ 0.79545614  0.1841635 ]\r\n [-0.2382378  -0.78096698]]\r\n\u7b2c 1 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.32737847 -0.74799064]\r\n [-0.48408967  0.65751339]\r\n [ 0.81146813  0.09047725]]\r\n\u7b2c 2 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.41217449  0.70482541]\r\n [-0.40430947 -0.70936628]\r\n [ 0.81648395  0.00454087]]\r\n\u7b2c 3 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[ 0.14981898 -0.80263375]\r\n [-0.77001071  0.27156984]\r\n [ 0.62019173  0.53106392]]\r\n\u7b2c 4 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[ 0.78405184 -0.22788019]\r\n [-0.58937595 -0.56506872]\r\n [-0.19467589  0.7929489 ]]\r\n\u7b2c 5 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.39290435  0.71574635]\r\n [-0.42340235 -0.69813832]\r\n [ 0.8163067  -0.01760803]]\r\n\u7b2c 6 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[ 0.80576724  0.13193114]\r\n [-0.51713934  0.63184933]\r\n [-0.2886279  -0.76378047]]\r\n\u7b2c 7 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.77464117  0.25806534]\r\n [ 0.16382944 -0.7998916 ]\r\n [ 0.61081173  0.54182626]]\r\n\u7b2c 8 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[ 0.14187153 -0.57735027]\r\n [ 0.62541498 -0.57735027]\r\n [-0.7672865  -0.57735027]]\r\n\u7b2c 9 \u8f6e\r\n\u7279\u5f81\u503c\uff1a [1. 1.]\r\n\u7279\u5f81\u5411\u91cf\uff1a [[-0.69006985 -0.43642899]\r\n [ 0.72299352 -0.37940353]\r\n [-0.03292367  0.81583252]]\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.4 1.26.2 sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"cc\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"0.29.36\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"c++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.3.0\",\r\n      \"commands\": \"gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.14.0\",\r\n      \"include directory\": \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-v6zpuvvi\\\\overlay\\\\Lib\\\\site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"windows\"\r\n    },\r\n    \"build\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"windows\"\r\n    },\r\n    \"cross-compiled\": true\r\n  },\r\n  \"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/c\/opt\/64\/include\",\r\n      \"lib directory\": \"\/c\/opt\/64\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\",\r\n      \"pc file directory\": \"c:\/opt\/64\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/c\/opt\/64\/include\",\r\n      \"lib directory\": \"\/c\/opt\/64\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\",\r\n      \"pc file directory\": \"c:\/opt\/64\/lib\/pkgconfig\"\r\n    },\r\n    \"pybind11\": {\r\n      \"name\": \"pybind11\",\r\n      \"version\": \"2.11.0\",\r\n      \"detection method\": \"config-tool\",\r\n      \"include directory\": \"unknown\"\r\n    }\r\n  },\r\n  \"Python Information\": {\r\n    \"path\": \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-msxnhclq\\\\cp39-win_amd64\\\\build\\\\venv\\\\Scripts\\\\python.exe\",\r\n    \"version\": \"3.9\"\r\n  }\r\n}\n```\n","comments":[],"labels":["defect","scipy.sparse.linalg"]},{"title":"Build warnings from HiGHS","body":"Wall of build warnings from HiGHS, see scipy\/HiGHS#65 - from a brief look it looks like at least some of these have been fixed upstream, e.g. https:\/\/github.com\/ERGO-Code\/HiGHS\/issues\/989.\r\n\r\nEdit: this is fixed upstream, just need to decide when to pull in an updated version.","comments":["Okay great, I checked and gh-19255 will close this \ud83c\udf89 @HaoZeke IIUC that is pretty much ready, so we should just wait for that to merge? If so, you can add 'Closes gh-19734' over there \ud83d\udc4d\r\n\r\nedit: although there are still quite a few build warnings which I think you're aware of...\r\n\r\n<details>\r\n\r\n```\r\n[1447\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsInfo.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInfo.cpp:284:14: warning: unused variable 'md_file' [-Wunused-variable]\r\n  const bool md_file = file_type == HighsFileType::kMd;\r\n             ^\r\n1 warning generated.\r\n[1453\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsModelUtils.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsModelUtils.cpp:761:30: warning: unused variable 'status' [-Wunused-variable]\r\n      const HighsBasisStatus status = basis.row_status[iRow];\r\n                             ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsModelUtils.cpp:876:30: warning: unused variable 'status' [-Wunused-variable]\r\n      const HighsBasisStatus status = basis.col_status[iCol];\r\n                             ^\r\n2 warnings generated.\r\n[1454\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsInterface.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:615:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:614:15: warning: unused variable 'return_status' [-Wunused-variable]\r\n  HighsStatus return_status = HighsStatus::kOk;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:686:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:731:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:1118:12: warning: unused variable 'ekk_lp' [-Wunused-variable]\r\n  HighsLp& ekk_lp = ekk_instance_.lp_;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:1165:12: warning: unused variable 'num_col' [-Wunused-variable]\r\n  HighsInt num_col = lp.num_col_;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:1178:15: warning: unused variable 'scale' [-Wunused-variable]\r\n  HighsScale& scale = lp.scale_;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsInterface.cpp:1661:22: warning: unused variable 'status' [-Wunused-variable]\r\n    HighsBasisStatus status =\r\n                     ^\r\n8 warnings generated.\r\n[1455\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsSolution.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:231:12: warning: variable 'num_basic_var' set but not used [-Wunused-but-set-variable]\r\n  HighsInt num_basic_var = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:232:12: warning: variable 'num_non_basic_var' set but not used [-Wunused-but-set-variable]\r\n  HighsInt num_non_basic_var = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:458:16: warning: unused variable 'middle' [-Wunused-variable]\r\n  const double middle = (lower + upper) * 0.5;\r\n               ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:659:12: warning: unused variable 'lower' [-Wunused-variable]\r\n    double lower = lp.col_lower_[col];\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:660:12: warning: unused variable 'upper' [-Wunused-variable]\r\n    double upper = lp.col_upper_[col];\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:733:39: warning: unused variable 'residual' [-Wunused-variable]\r\n    double lower, upper, value, dual, residual;\r\n                                      ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:978:18: warning: unused variable 'upper' [-Wunused-variable]\r\n    const double upper = lp.col_upper_[col];\r\n                 ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:977:18: warning: unused variable 'lower' [-Wunused-variable]\r\n    const double lower = lp.col_lower_[col];\r\n                 ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolution.cpp:1223:12: warning: unused variable 'ekk_lp' [-Wunused-variable]\r\n  HighsLp& ekk_lp = ekk_instance.lp_;\r\n           ^\r\n9 warnings generated.\r\n[1456\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsRanging.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsRanging.cpp:231:20: warning: unused variable 'numerator' [-Wunused-variable]\r\n      const double numerator = (alpha < 0 ? dxi_inc[i] : dxi_dec[i]);\r\n                   ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsRanging.cpp:96:25: warning: unused variable 'row_scale' [-Wunused-variable]\r\n  const vector<double>& row_scale = use_lp.scale_.row;\r\n                        ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsRanging.cpp:95:25: warning: unused variable 'col_scale' [-Wunused-variable]\r\n  const vector<double>& col_scale = use_lp.scale_.col;\r\n                        ^\r\n3 warnings generated.\r\n[1459\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsSolve.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolve.cpp:48:10: warning: unused variable 'imprecise_solution' [-Wunused-variable]\r\n    bool imprecise_solution;\r\n         ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolve.cpp:169:8: warning: unused variable 'unbounded' [-Wunused-variable]\r\n  bool unbounded = false;\r\n       ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsSolve.cpp:168:8: warning: unused variable 'infeasible' [-Wunused-variable]\r\n  bool infeasible = false;\r\n       ^\r\n3 warnings generated.\r\n[1460\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsOptions.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsOptions.cpp:476:14: warning: unused variable 'found_digit' [-Wunused-variable]\r\n    HighsInt found_digit = value_trim.find_first_of(\"0123456789\");\r\n             ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsOptions.cpp:503:14: warning: unused variable 'found_digit' [-Wunused-variable]\r\n    HighsInt found_digit = value_trim.find_first_of(\"0123456789\");\r\n             ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsOptions.cpp:816:14: warning: unused variable 'md_file' [-Wunused-variable]\r\n  const bool md_file = file_type == HighsFileType::kMd;\r\n             ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsOptions.cpp:842:14: warning: unused variable 'html_file' [-Wunused-variable]\r\n  const bool html_file = file_type == HighsFileType::kHtml;\r\n             ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsOptions.cpp:843:14: warning: unused variable 'md_file' [-Wunused-variable]\r\n  const bool md_file = file_type == HighsFileType::kMd;\r\n             ^\r\n5 warnings generated.\r\n[1461\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_HighsLpUtils.cpp.o\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:135:12: warning: unused variable 'matrix_start_size' [-Wunused-variable]\r\n  HighsInt matrix_start_size = lp.a_matrix_.start_.size();\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:668:12: warning: unused variable 'num_modified_lower' [-Wunused-variable]\r\n  HighsInt num_modified_lower = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:833:8: warning: unused variable 'allow_cost_scaling' [-Wunused-variable]\r\n  bool allow_cost_scaling = options.allowed_cost_scale_factor > 0;\r\n       ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:1239:12: warning: unused variable 'simplex_scale_strategy' [-Wunused-variable]\r\n  HighsInt simplex_scale_strategy = use_scale_strategy;\r\n           ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:1457:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:1508:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:1558:15: warning: unused variable 'return_status' [-Wunused-variable]\r\n  HighsStatus return_status = HighsStatus::kOk;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/HighsLpUtils.cpp:2116:12: warning: unused variable 'status' [-Wunused-variable]\r\n  HighsInt status;\r\n           ^\r\n8 warnings generated.\r\n[1464\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/lp_data_Highs.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:30:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:1800:15: warning: unused variable 'return_status' [-Wunused-variable]\r\n  HighsStatus return_status = HighsStatus::kOk;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:1855:15: warning: unused variable 'return_status' [-Wunused-variable]\r\n  HighsStatus return_status = HighsStatus::kOk;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:1973:22: warning: unused variable 'active' [-Wunused-variable]\r\n  std::vector<bool>& active = this->callback_.active;\r\n                     ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:3210:15: warning: unused variable 'call_status' [-Wunused-variable]\r\n  HighsStatus call_status;\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:3307:15: warning: unused variable 'qpstatus' [-Wunused-variable]\r\n  QpAsmStatus qpstatus =\r\n              ^\r\n..\/subprojects\/highs\/src\/lp_data\/Highs.cpp:3485:20: warning: unused variable 'mip_max_infeasibility' [-Wunused-variable]\r\n      const double mip_max_infeasibility =\r\n                   ^\r\n7 warnings generated.\r\n[1465\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsSeparation.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSeparation.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSeparation.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1466\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsMipSolver.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolver.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolver.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1467\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsLpRelaxation.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsLpRelaxation.cpp:18:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsLpRelaxation.cpp:18:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsLpRelaxation.cpp:299:22: warning: unused variable 'dual' [-Wunused-variable]\r\n        const double dual = solution.row_dual[iRow];\r\n                     ^\r\n3 warnings generated.\r\n[1468\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsTableauSeparator.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsTableauSeparator.cpp:21:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsTableauSeparator.cpp:21:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsTableauSeparator.cpp:148:14: warning: unused variable 'i' [-Wunused-variable]\r\n    HighsInt i = fracvar.basisIndex;\r\n             ^\r\n3 warnings generated.\r\n[1469\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsDomain.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:153:14: warning: unused variable 'col' [-Wunused-variable]\r\n    HighsInt col = conflictEntries[i].column;\r\n             ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:234:41: warning: unused variable 'conflictEntries' [-Wunused-variable]\r\n  const std::vector<HighsDomainChange>& conflictEntries =\r\n                                        ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:255:41: warning: unused variable 'conflictEntries' [-Wunused-variable]\r\n  const std::vector<HighsDomainChange>& conflictEntries =\r\n                                        ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:294:12: warning: unused variable 'latestactive' [-Wunused-variable]\r\n  HighsInt latestactive[2];\r\n           ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:715:10: warning: unused variable 'lb' [-Wunused-variable]\r\n  double lb = numInfObjLower == 0\r\n         ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:2177:12: warning: unused variable 'nextBranchPos' [-Wunused-variable]\r\n  HighsInt nextBranchPos = -1;\r\n           ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:2421:16: warning: variable 'propnnz' set but not used [-Wunused-but-set-variable]\r\n      HighsInt propnnz = 0;\r\n               ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:2514:18: warning: variable 'propnnz' set but not used [-Wunused-but-set-variable]\r\n        HighsInt propnnz = 0;\r\n                 ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:3011:16: warning: variable 'numRelaxed' set but not used [-Wunused-but-set-variable]\r\n      HighsInt numRelaxed = 0;\r\n               ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:3012:16: warning: variable 'numDropped' set but not used [-Wunused-but-set-variable]\r\n      HighsInt numDropped = 0;\r\n               ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:3125:16: warning: variable 'numRelaxed' set but not used [-Wunused-but-set-variable]\r\n      HighsInt numRelaxed = 0;\r\n               ^\r\n..\/subprojects\/highs\/src\/mip\/HighsDomain.cpp:3126:16: warning: variable 'numDropped' set but not used [-Wunused-but-set-variable]\r\n      HighsInt numDropped = 0;\r\n               ^\r\n14 warnings generated.\r\n[1470\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsMipSolverData.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.cpp:11:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.cpp:11:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.cpp:598:20: warning: use of bitwise '&' with boolean operands [-Wbitwise-instead-of-logical]\r\n        numBin += ((mipsolver.model_->col_lower_[i] == 0.0) &\r\n                  ~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                                            &&\r\n..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.cpp:598:20: note: cast one or both operands to int to silence this warning\r\n..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.cpp:1582:12: warning: unused variable 'oldLimit' [-Wunused-variable]\r\n    double oldLimit = upper_limit;\r\n           ^\r\n4 warnings generated.\r\n[1471\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsPathSeparator.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPathSeparator.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPathSeparator.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1474\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsModkSeparator.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsModkSeparator.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsModkSeparator.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1475\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsCutGeneration.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCutGeneration.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCutGeneration.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsCutGeneration.cpp:61:17: warning: unused variable 'nodequeue' [-Wunused-variable]\r\n    const auto& nodequeue = lpRelaxation.getMipSolver().mipdata_->nodequeue;\r\n                ^\r\n3 warnings generated.\r\n[1478\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsCutPool.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCutPool.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCutPool.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1479\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsTransformedLp.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsTransformedLp.cpp:14:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsTransformedLp.cpp:14:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1480\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsSearch.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.cpp:11:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.cpp:18:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsSearch.cpp:250:29: warning: unused variable 'basisstart_threshold' [-Wunused-variable]\r\n  static constexpr HighsInt basisstart_threshold = 20;\r\n                            ^\r\n..\/subprojects\/highs\/src\/mip\/HighsSearch.cpp:942:17: warning: unused variable 'branchpos' [-Wunused-variable]\r\n    const auto& branchpos = localdom.getBranchingPositions();\r\n                ^\r\n4 warnings generated.\r\n[1482\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsPseudocost.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPseudocost.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPseudocost.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1483\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsRedcostFixing.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsRedcostFixing.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsRedcostFixing.cpp:13:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1484\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/model_HighsHessianUtils.cpp.o\r\n..\/subprojects\/highs\/src\/model\/HighsHessianUtils.cpp:224:12: warning: unused variable 'diagonal_value' [-Wunused-variable]\r\n    double diagonal_value = 0;\r\n           ^\r\n1 warning generated.\r\n[1485\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsObjectiveFunction.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsObjectiveFunction.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsObjectiveFunction.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1486\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsImplications.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsImplications.cpp:14:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsImplications.cpp:14:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsImplications.cpp:550:16: warning: unused variable 'oldNumEntries' [-Wunused-variable]\r\n      HighsInt oldNumEntries = mipsolver.mipdata_->cliquetable.getNumEntries();\r\n               ^\r\n3 warnings generated.\r\n[1487\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsNodeQueue.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsNodeQueue.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsNodeQueue.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n2 warnings generated.\r\n[1489\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/parallel_HighsTaskExecutor.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.cpp:1:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n1 warning generated.\r\n[1496\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/presolve_PresolveComponent.cpp.o\r\n..\/subprojects\/highs\/src\/presolve\/PresolveComponent.cpp:38:20: warning: unused variable 'status' [-Wunused-variable]\r\n  HighsModelStatus status = presolve.run(data_.postSolveStack);\r\n                   ^\r\n1 warning generated.\r\n[1497\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsCliqueTable.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:21:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:21:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:1614:18: warning: unused variable 'initSize' [-Wunused-variable]\r\n        HighsInt initSize = cliques[cliqueid].end - cliques[cliqueid].start;\r\n                 ^\r\n..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:1951:12: warning: unused variable 'node' [-Wunused-variable]\r\n  HighsInt node;\r\n           ^\r\n..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:2147:16: warning: variable 'numRemoved' set but not used [-Wunused-but-set-variable]\r\n      HighsInt numRemoved = 0;\r\n               ^\r\n..\/subprojects\/highs\/src\/mip\/HighsCliqueTable.cpp:2063:14: warning: unused variable 'node' [-Wunused-variable]\r\n    HighsInt node;\r\n             ^\r\n6 warnings generated.\r\n[1498\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/mip_HighsPrimalHeuristics.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPrimalHeuristics.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsPrimalHeuristics.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/mip\/HighsPrimalHeuristics.cpp:670:20: warning: unused variable 'oldNumBranched' [-Wunused-variable]\r\n          HighsInt oldNumBranched = numBranched;\r\n                   ^\r\n3 warnings generated.\r\n[1501\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/qpsolver_a_quass.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/qpsolver\/a_quass.cpp:4:\r\n..\/subprojects\/highs\/src\/qpsolver\/feasibility_highs.hpp:67:23: warning: unused variable 'internal_basis' [-Wunused-variable]\r\n    const HighsBasis& internal_basis = highs.getBasis();\r\n                      ^\r\n1 warning generated.\r\n[1505\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/presolve_HighsSymmetry.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:16:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:23:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:45:35: warning: unused variable 'oldCellStart' [-Wunused-variable]\r\n                         HighsInt oldCellStart = vertexToCell[vertex];\r\n                                  ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:182:15: warning: unused variable 'prevBounds' [-Wunused-variable]\r\n  const auto& prevBounds = localdom.getPreviousBounds();\r\n              ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:278:14: warning: unused variable 'fixVal' [-Wunused-variable]\r\n      double fixVal = domain.col_lower_[fixcol];\r\n             ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:818:14: warning: unused variable 'oldCellStart' [-Wunused-variable]\r\n    HighsInt oldCellStart = vertexToCell[vertex];\r\n             ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:1506:12: warning: unused variable 'numComponents' [-Wunused-variable]\r\n  HighsInt numComponents = componentData.componentStarts.size();\r\n           ^\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.cpp:1581:12: warning: unused variable 'orbitopeIndex' [-Wunused-variable]\r\n  HighsInt orbitopeIndex = symmetries.orbitopes.size();\r\n           ^\r\n8 warnings generated.\r\n[1506\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/qpsolver_quass.cpp.o\r\n..\/subprojects\/highs\/src\/qpsolver\/quass.cpp:343:14: warning: unused variable 'denominator' [-Wunused-variable]\r\n      double denominator = p * runtime.instance.Q.mat_vec(p, buffer_Qp);\r\n             ^\r\n..\/subprojects\/highs\/src\/qpsolver\/quass.cpp:422:17: warning: unused variable 'status' [-Wunused-variable]\r\n    BasisStatus status = basis.getstatus(e);\r\n                ^\r\n2 warnings generated.\r\n[1508\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HEkkDualRHS.cpp.o\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRHS.cpp:32:18: warning: unused variable 'numTot' [-Wunused-variable]\r\n  const HighsInt numTot =\r\n                 ^\r\n1 warning generated.\r\n[1510\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HEkkDual.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:118:10: warning: unused variable 'unperturbed_sum_infeasibilities' [-Wunused-variable]\r\n  double unperturbed_sum_infeasibilities = info.sum_dual_infeasibilities;\r\n         ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:116:12: warning: unused variable 'unperturbed_num_infeasibilities' [-Wunused-variable]\r\n  HighsInt unperturbed_num_infeasibilities = info.num_dual_infeasibilities;\r\n           ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:117:10: warning: unused variable 'unperturbed_max_infeasibility' [-Wunused-variable]\r\n  double unperturbed_max_infeasibility = info.max_dual_infeasibility;\r\n         ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:507:17: warning: unused variable 'Avalue' [-Wunused-variable]\r\n  const double* Avalue = a_matrix->value_.data();\r\n                ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:506:19: warning: unused variable 'Aindex' [-Wunused-variable]\r\n  const HighsInt* Aindex = a_matrix->index_.data();\r\n                  ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:2204:16: warning: unused variable 'shift' [-Wunused-variable]\r\n  const double shift = fabs(info.workShift_[iCol]);\r\n               ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDual.cpp:2289:14: warning: unused variable 'always_initialise_dual_steepest_edge_weights' [-Wunused-variable]\r\n  const bool always_initialise_dual_steepest_edge_weights = true;\r\n             ^\r\n8 warnings generated.\r\n[1512\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HEkkDualRow.cpp.o\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:337:14: warning: variable 'debug_loop_ln' set but not used [-Wunused-but-set-variable]\r\n    HighsInt debug_loop_ln = 0;\r\n             ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:332:12: warning: variable 'debug_num_loop' set but not used [-Wunused-but-set-variable]\r\n  HighsInt debug_num_loop = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:399:14: warning: variable 'debug_loop_ln' set but not used [-Wunused-but-set-variable]\r\n    HighsInt debug_loop_ln = 0;\r\n             ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:394:12: warning: variable 'debug_num_loop' set but not used [-Wunused-but-set-variable]\r\n  HighsInt debug_num_loop = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:491:16: warning: unused variable 'alt_workGroup_size' [-Wunused-variable]\r\n      HighsInt alt_workGroup_size = alt_workGroup.size();\r\n               ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:671:18: warning: unused variable 'delta' [-Wunused-variable]\r\n    const double delta = workData[i].second;\r\n                 ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkkDualRow.cpp:662:18: warning: unused variable 'move_out' [-Wunused-variable]\r\n  const HighsInt move_out = workDelta < 0 ? -1 : 1;\r\n                 ^\r\n7 warnings generated.\r\n[1513\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HEkkDualMulti.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/simplex\/HEkkDualMulti.cpp:22:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n1 warning generated.\r\n[1514\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HEkk.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:639:18: warning: unused variable 'lower' [-Wunused-variable]\r\n    const double lower = original_col_lower_[iCol];\r\n                 ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:635:12: warning: variable 'num_extra_col' set but not used [-Wunused-but-set-variable]\r\n  HighsInt num_extra_col = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:823:18: warning: unused variable 'cost' [-Wunused-variable]\r\n    const double cost = original_col_cost_[iCol];\r\n                 ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:791:19: warning: unused variable 'primal_work_value' [-Wunused-variable]\r\n  vector<double>& primal_work_value = info_.workValue_;\r\n                  ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:803:16: warning: unused variable 'inf' [-Wunused-variable]\r\n  const double inf = kHighsInf;\r\n               ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:790:19: warning: unused variable 'dual_work_dual' [-Wunused-variable]\r\n  vector<double>& dual_work_dual = info_.workDual_;\r\n                  ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:1157:18: warning: unused variable 'num_tot' [-Wunused-variable]\r\n  const HighsInt num_tot = num_col + num_row;\r\n                 ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:1227:12: warning: unused variable 'num_tot' [-Wunused-variable]\r\n  HighsInt num_tot = num_col + num_row;\r\n           ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:1575:21: warning: unused variable 'col_with_no_pivot' [-Wunused-variable]\r\n  vector<HighsInt>& col_with_no_pivot = factor.col_with_no_pivot;\r\n                    ^\r\n..\/subprojects\/highs\/src\/simplex\/HEkk.cpp:2547:18: warning: unused variable 'previous_cost' [-Wunused-variable]\r\n    const double previous_cost = info_.workCost_[i];\r\n                 ^\r\n11 warnings generated.\r\n[1517\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HSimplexNlaDebug.cpp.o\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNlaDebug.cpp:143:16: warning: unused variable 'report_level' [-Wunused-variable]\r\n  HighsLogType report_level;\r\n               ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNlaDebug.cpp:314:23: warning: unused variable 'options' [-Wunused-variable]\r\n  const HighsOptions* options = this->options_;\r\n                      ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNlaDebug.cpp:27:14: warning: unused variable 'kInverseExcessiveError' [-Wunused-const-variable]\r\nconst double kInverseExcessiveError = sqrt(kInverseLargeError);\r\n             ^\r\n3 warnings generated.\r\n[1519\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HSimplexNla.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/simplex\/HSimplexNla.cpp:20:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNla.cpp:180:25: warning: unused variable 'col_scale' [-Wunused-variable]\r\n  const vector<double>& col_scale = scale_->col;\r\n                        ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNla.cpp:289:25: warning: unused variable 'col_scale' [-Wunused-variable]\r\n  const vector<double>& col_scale = scale_->col;\r\n                        ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNla.cpp:320:25: warning: unused variable 'col_scale' [-Wunused-variable]\r\n  const vector<double>& col_scale = scale_->col;\r\n                        ^\r\n..\/subprojects\/highs\/src\/simplex\/HSimplexNla.cpp:452:18: warning: unused variable 'num_row' [-Wunused-variable]\r\n  const HighsInt num_row = lp_->num_row_;\r\n                 ^\r\n5 warnings generated.\r\n[1522\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/simplex_HighsSimplexAnalysis.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/simplex\/HighsSimplexAnalysis.cpp:19:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/simplex\/HighsSimplexAnalysis.cpp:1093:16: warning: unused variable 'lc_simplex_strategy' [-Wunused-variable]\r\n      HighsInt lc_simplex_strategy = lcAnIter.AnIterTrace_simplex_strategy;\r\n               ^\r\n..\/subprojects\/highs\/src\/simplex\/HighsSimplexAnalysis.cpp:1116:16: warning: unused variable 'local_simplex_strategy' [-Wunused-variable]\r\n      HighsInt local_simplex_strategy = lcAnIter.AnIterTrace_simplex_strategy;\r\n               ^\r\n3 warnings generated.\r\n[1532\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/util_HighsMatrixUtils.cpp.o\r\n..\/subprojects\/highs\/src\/util\/HighsMatrixUtils.cpp:85:14: warning: unused variable 'next_start' [-Wunused-variable]\r\n    HighsInt next_start = matrix_start[ix + 1];\r\n             ^\r\n1 warning generated.\r\n[1535\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/presolve_HPresolve.cpp.o\r\nIn file included from ..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:26:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:29:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsSearch.h:25:\r\n..\/subprojects\/highs\/src\/presolve\/HighsSymmetry.h:47:9: warning: unused variable 'color' [-Wunused-variable]\r\n    u32 color;\r\n        ^\r\nIn file included from ..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:26:\r\nIn file included from ..\/subprojects\/highs\/src\/mip\/HighsMipSolverData.h:31:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:2921:18: warning: unused variable 'colCoef' [-Wunused-variable]\r\n          double colCoef = Avalue[colhead[col]];\r\n                 ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:3493:18: warning: variable 'numTightened' set but not used [-Wunused-but-set-variable]\r\n        HighsInt numTightened = 0;\r\n                 ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:3520:18: warning: variable 'numTightened' set but not used [-Wunused-but-set-variable]\r\n        HighsInt numTightened = 0;\r\n                 ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:4602:12: warning: variable 'numsubst' set but not used [-Wunused-but-set-variable]\r\n  HighsInt numsubst = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:4603:12: warning: variable 'numsubstint' set but not used [-Wunused-but-set-variable]\r\n  HighsInt numsubstint = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:5408:12: warning: variable 'numRowBuckets' set but not used [-Wunused-but-set-variable]\r\n  HighsInt numRowBuckets = 0;\r\n           ^\r\n..\/subprojects\/highs\/src\/presolve\/HPresolve.cpp:5409:12: warning: variable 'numColBuckets' set but not used [-Wunused-but-set-variable]\r\n  HighsInt numColBuckets = 0;\r\n           ^\r\n9 warnings generated.\r\n[1536\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_basiclu_factorize.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/basiclu_factorize.c:29:20: warning: unused variable 'elapsed' [-Wunused-variable]\r\n    double tic[2], elapsed, factor_cost;\r\n                   ^\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/basiclu_factorize.c:29:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2], elapsed, factor_cost;\r\n           ^\r\n2 warnings generated.\r\n[1543\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_markowitz.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_markowitz.c:57:25: warning: unused variable 'tic' [-Wunused-variable]\r\n    double cmx, x, tol, tic[2];\r\n                        ^\r\n1 warning generated.\r\n[1545\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_solve_sparse.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_solve_sparse.c:46:20: warning: unused variable 'elapsed' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n                   ^\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_solve_sparse.c:46:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n           ^\r\n2 warnings generated.\r\n[1553\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/util_HighsUtils.cpp.o\r\n..\/subprojects\/highs\/src\/util\/HighsUtils.cpp:490:18: warning: unused variable 'PlusOneIx' [-Wunused-variable]\r\n  const HighsInt PlusOneIx = 0;\r\n                 ^\r\n..\/subprojects\/highs\/src\/util\/HighsUtils.cpp:485:14: warning: unused variable 'analyseValueList' [-Wunused-variable]\r\n  const bool analyseValueList = true;\r\n             ^\r\n..\/subprojects\/highs\/src\/util\/HighsUtils.cpp:491:18: warning: unused variable 'MinusOneIx' [-Wunused-variable]\r\n  const HighsInt MinusOneIx = 1;\r\n                 ^\r\n3 warnings generated.\r\n[1554\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_singletons.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_singletons.c:167:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2];\r\n           ^\r\n1 warning generated.\r\n[1559\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_update.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_update.c:469:20: warning: unused variable 'elapsed' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n                   ^\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_update.c:469:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n           ^\r\n2 warnings generated.\r\n[1563\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/util_HighsSparseMatrix.cpp.o\r\n..\/subprojects\/highs\/src\/util\/HighsSparseMatrix.cpp:174:8: warning: unused variable 'empty_matrix' [-Wunused-variable]\r\n  bool empty_matrix = num_col == 0 || num_row == 0;\r\n       ^\r\n..\/subprojects\/highs\/src\/util\/HighsSparseMatrix.cpp:906:12: warning: unused variable 'num_col' [-Wunused-variable]\r\n  HighsInt num_col = matrix.num_col_;\r\n           ^\r\n..\/subprojects\/highs\/src\/util\/HighsSparseMatrix.cpp:908:12: warning: unused variable 'num_nz' [-Wunused-variable]\r\n  HighsInt num_nz = matrix.numNz();\r\n           ^\r\n3 warnings generated.\r\n[1566\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_solve_for_update.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_solve_for_update.c:48:20: warning: unused variable 'elapsed' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n                   ^\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_solve_for_update.c:48:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2], elapsed;\r\n           ^\r\n2 warnings generated.\r\n[1567\/1606] Compiling C object subprojects\/highs\/src\/libhighs.a.p\/ipm_basiclu_lu_pivot.c.o\r\n..\/subprojects\/highs\/src\/ipm\/basiclu\/lu_pivot.c:82:12: warning: unused variable 'tic' [-Wunused-variable]\r\n    double tic[2];\r\n           ^\r\n1 warning generated.\r\n[1568\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/util_HFactor.cpp.o\r\n..\/subprojects\/highs\/src\/util\/HFactor.cpp:1277:18: warning: unused variable 'basic_index_rank_deficiency' [-Wunused-variable]\r\n  const HighsInt basic_index_rank_deficiency = rank_deficiency;\r\n                 ^\r\n1 warning generated.\r\n[1572\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/ipm_ipx_basiclu_wrapper.cc.o\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basiclu_wrapper.cc:135:14: warning: variable 'ncall' set but not used [-Wunused-but-set-variable]\r\n    for (Int ncall = 0; ; ncall++) {\r\n             ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basiclu_wrapper.cc:156:14: warning: variable 'ncall' set but not used [-Wunused-but-set-variable]\r\n    for (Int ncall = 0; ; ncall++) {\r\n             ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basiclu_wrapper.cc:176:14: warning: variable 'ncall' set but not used [-Wunused-but-set-variable]\r\n    for (Int ncall = 0; ; ncall++) {\r\n             ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basiclu_wrapper.cc:196:14: warning: variable 'ncall' set but not used [-Wunused-but-set-variable]\r\n    for (Int ncall = 0; ; ncall++) {\r\n             ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basiclu_wrapper.cc:217:14: warning: variable 'ncall' set but not used [-Wunused-but-set-variable]\r\n    for (Int ncall = 0; ; ncall++) {\r\n             ^\r\n5 warnings generated.\r\n[1574\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/ipm_ipx_control.cc.o\r\nIn file included from ..\/subprojects\/highs\/src\/ipm\/ipx\/control.cc:1:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsParallel.h:16:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsMutex.h:18:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsTaskExecutor.h:25:\r\nIn file included from ..\/subprojects\/highs\/src\/parallel\/HighsSplitDeque.h:28:\r\n..\/subprojects\/highs\/src\/parallel\/HighsTask.h:112:15: warning: unused variable 'state' [-Wunused-variable]\r\n    uintptr_t state =\r\n              ^\r\n1 warning generated.\r\n[1577\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/ipm_ipx_basis.cc.o\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basis.cc:356:15: warning: unused variable 'm' [-Wunused-variable]\r\n    const Int m = model_.rows();\r\n              ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/basis.cc:357:15: warning: unused variable 'n' [-Wunused-variable]\r\n    const Int n = model_.cols();\r\n              ^\r\n2 warnings generated.\r\n[1579\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/ipm_ipx_forrest_tomlin.cc.o\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/forrest_tomlin.cc:319:9: warning: unused variable 'nz' [-Wunused-variable]\r\n    Int nz = 0;\r\n        ^\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/forrest_tomlin.cc:351:9: warning: unused variable 'nz' [-Wunused-variable]\r\n    Int nz = 0;\r\n        ^\r\n2 warnings generated.\r\n[1583\/1606] Compiling C++ object subprojects\/highs\/src\/libhighs.a.p\/ipm_ipx_ipm.cc.o\r\n..\/subprojects\/highs\/src\/ipm\/ipx\/ipm.cc:385:18: warning: unused variable 'mu' [-Wunused-variable]\r\n    const double mu = iterate_->mu();\r\n                 ^\r\n1 warning generated.\r\n```\r\n\r\n<\/details>"],"labels":["Build issues","maintenance","DX"]},{"title":"ENH: Wasserstein distance with p=inf, i.e. bottleneck distance.","body":"### Is your feature request related to a problem? Please describe.\n\n_No response_\n\n### Describe the solution you'd like.\n\nI would like to calculate bottleneck distance between 2 vectors (i.e. distributions) which can be defined as Wasserstein p-distance $W_p$ with p = $\\infty$, i.e. $W_\\infty$ (sources: \r\nhttps:\/\/mtsch.github.io\/PersistenceDiagrams.jl\/v0.3\/generated\/distances\/ \r\nhttps:\/\/en.wikipedia.org\/wiki\/Wasserstein_metric#Higher_dimensions\r\nand https:\/\/en.wikipedia.org\/wiki\/Wasserstein_metric#Definition).\r\n\n\n### Describe alternatives you've considered.\n\nI tried with calculating limit of $W_p$ by private function `_cdf_distance` e.g. `_cdf_distance(600, u, v)` but as reported in issue: \r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/19712\r\nthis has problems (underflow?). So I would love to have this implemented as a public `bootleneck_distance` function.\r\n\r\n`persim` package inside of `scikit-tda` have bottleneck distance implemented between persistent diagrams, but I think it would be useful (at least for me) to have function between plain vectors also.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["btw: `_cdf_distance` and `wasserstein_distance` functions are located in `scipy.stats._stats_py`.","I don't think there is a need for a new function. We could add a new parameter `p` to `wasserstein_distance`. We could subsequently pass this parameter to line 10400. To be precise,\r\n\r\n```python\r\ndef wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None):\r\n```\r\n\r\nwould become\r\n\r\n```python\r\ndef wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None, p=2):\r\n```\r\n\r\nand\r\n\r\n```python\r\nD = distance_matrix(u_values, v_values, p=2)\r\n```\r\n\r\nwould become\r\n\r\n```python\r\nD = distance_matrix(u_values, v_values, p=p)\r\n```\r\n\r\nYour desired function would correspond to `p=np.inf`. If you want to, you could try doing this and see if it works. I could also implement this.","@B0Gec This will require a bit more work. I will work on this one and let you know.","Thanks! Much appreciated. It will certainly require a bit more work and unfortunately, I did not take enough time to dig deep enough into the definition and the corresponding `_cdf_distance` code.","@B0Gec if you want to help me, would you like to come up with some test cases for 1D Wasserstein distance? Are there any analytical results against which we can test our code?","I am not sure if I will have the time since I am still figuring out the definition.\r\nOne example would be e.g. dist_1 = [ 0.1, 0.9] vs. dist2 = [0.9, 0.1] or dist_1 vs dist_3 = [0.5, 0.5]. I am not sure if W_inf(dist_1, dist2) = 0 or not, given the obvious permutation.\r\n","Thanks, @yagizolmez! This generalization of $p$ would indeed be an interesting enhancement. However, there is a thing you might need to consider:\r\n1. The current Wasserstein distance function calls the _cdf_distance function to compute the answer. In the dev version, it also uses this method if the inputs are 1D; otherwise, linear programming is used. There could be cases where users pass 2D arrays that actually contain 1D data (e.g., `u1 = [[1], [2], [3]], v1 = [[1.5], [2.5], [3.5]]`, and `u2=[1, 2, 3], v2 = [1.5, 2.5, 3.5]`). For these input pairs, the backend algorithms used are different.\r\n3. I had some experiments\/tests to make sure both `_cdf_distance` and `wasserstein_distance` returns same value in the mentioned case (but I might forget to attach those tests under the `TestWassersteinDistance` class, so if you want to make an enhancement I suggest you to add a `testFlatten` method in tests). But if `p` goes very large, I don't know if the method used in the dev version will keep consistent with the `_cdf_distance`.","\r\n@com3dian The analytical result that `_cdf_distance` relies upon is valid only for `p=1`. Surprisingly, it does not depend on the norm.","@com3dian, @B0Gec I have just learned a very cool thing. Following the [reference](https:\/\/arxiv.org\/pdf\/1509.02237.pdf) in the docs, there is an analytical result for $p = \\infty$ in the 1-D case. On page 10, it states that\r\n\r\n$$ W_\\infty (P,Q) = || F^{-1} - G^{-1} ||_\\infty$$\r\n\r\nin the 1-D case, where $F^{-1}$ and $G^{-1}$ are the [quantile functions](https:\/\/en.wikipedia.org\/wiki\/Quantile_function) of $P$ and $Q$ respectively. Here, $\\| \\cdot \\|_\\infty$ is the [infinity norm](https:\/\/en.wikipedia.org\/wiki\/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)). I think this should be pretty straightforward to code. Once we have this, we can code for general `p` and test our result based on the analytical result.","@com3dian, @B0Gec. I have written the following function to calculate the bottleneck distance analytically. I got inspired by `_cdf_distance`. Please let me know what you think.\r\n\r\n```python\r\ndef _wasserstein_infty(u_values, v_values, u_weights=None, v_weights=None):\r\n    r\"\"\"\r\n    Calculate $W_\\infty$ distance for two 1-D distributions.\r\n    \"\"\"\r\n    u_values, u_weights = _validate_distribution(u_values, u_weights)\r\n    v_values, v_weights = _validate_distribution(v_values, v_weights)\r\n\r\n    u_sorter = np.argsort(u_values)\r\n    v_sorter = np.argsort(v_values)\r\n\r\n    u_values_sorted = u_values[u_sorter]\r\n    v_values_sorted = v_values[v_sorter]\r\n\r\n    # Calculate the CDFs of u and v using their weights, if specified.\r\n    if u_weights is None:\r\n        u_cdf = u_values_sorted \/ u_values.size\r\n    else:\r\n        u_sorted_cumweights = np.concatenate(([0],\r\n                                              np.cumsum(u_weights[u_sorter])))\r\n        u_cdf = u_sorted_cumweights[u_sorter] \/ u_sorted_cumweights[-1]\r\n\r\n    if v_weights is None:\r\n        v_cdf = v_values_sorted \/ v_values.size\r\n    else:\r\n        v_sorted_cumweights = np.concatenate(([0],\r\n                                              np.cumsum(v_weights[v_sorter])))\r\n        v_cdf = v_sorted_cumweights[v_sorter] \/ v_sorted_cumweights[-1]\r\n\r\n    # Calculate the quantile functions\r\n    all_cdf = np.concatenate((u_cdf, v_cdf))\r\n    all_cdf.sort(kind='mergesort')\r\n\r\n    u_quantile_indices = u_cdf.searchsorted(all_cdf[:-1], 'right')\r\n    v_quantile_indices = v_cdf.searchsorted(all_cdf[:-1], 'right')\r\n\r\n    u_values_sorted = np.append(u_values_sorted,[u_values_sorted[-1]])\r\n    v_values_sorted = np.append(v_values_sorted,[v_values_sorted[-1]])\r\n\r\n    u_quantile_values = u_values_sorted[u_quantile_indices]\r\n    v_quantile_values = v_values_sorted[v_quantile_indices]\r\n\r\n    return np.max(np.abs(u_quantile_values-v_quantile_values))\r\n```","@com3dian, @B0Gec I have opened a pull request on this. I am looking forward to your input.","is the `p=oo` case not already handled by python optimal transport package `pot` ? \r\nhttps:\/\/pythonot.github.io\/index.html\r\n","@rlucas7 I couldn't find it.","Is there any use case for $p \\neq 1, 2, \\infty$ ?  So instead of an open argument, you could enumerate valid options like [`np.linalg.norm`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.norm.html) does. ","@ilayn Probably not. I just want to implement the general case, so that I can test $p = \\infty$, since $W_p \\to W_\\infty$ as $p \\to \\infty$.","That's not really a good policy for a package like SciPy. Like I mentioned, the vector norm is also defined for general case but won't work reliably (even though we know the cases for any $p$) hence we don't provide norms for any $p$. Only the cases where we know closed form statements. Otherwise it would be pointless to provide a broken function. The general case is only meaningful for theoretical cases. \r\n\r\nInfinity norm for both the vector and the matrix cases are derived from taking the limit theoretically and then computing the resulting expression.","@ilayn Apparently, in the 1-D case, there is an analytical result for any $1< p< \\infty$. I could also implement that and check if $W_p \\to W_\\infty$ as $p \\to \\infty$.\r\n\r\nMy concern is that in the new version `1.12.0`, `wasserstein_distance` has been extended to 2-D distributions. In the 2-D case calculations are done using a linear programming algorithm. While I can provide analytical solutions for the 1-D case for any `p`, I cannot do that in 2-D. What do you think would be the right way to approach to this issue?","What I mean is that, we don't need to prove continuity in $p$ for this distance. If it is continuous or smooth nice, otherwise we should not care. That's not the goal of the numerical package. We want to provide robust computational tools. Not proof tools. As $p$ grows, you would need to deal with all kinds of overflows\/underflows and unnecessary nonsense just to make a point that can be shown theoretically. Hence my comment.\r\n\r\nIf nobody is using anything other than $p = 1, 2, \\infty$ then it is good enough to provide these cases.  ","@ilayn I understand, but then we cannot add $p = \\infty$ support for the 2-D case. I was planning to implement it using a large `p` value. Is that okay?","> then we cannot add $p = \\infty$ support for the 2-D case. I was planning to implement it using a large `p` value. Is that okay?\r\n\r\nPractically, probably, but someone will probably be upset that it is not theoretically sound.\r\n\r\n@com3dian originally you suggested making the multivariate Wasserstein distance a separate function, and I think I suggested combining them. I've started to regret that a bit; it would be easier for a separate 1D function to support an `axis` argument, and now there's this issue of support for $p=\\infty$. \r\n\r\nIf we split the function at this point, I think that multivariate Wasserstein would miss SciPy 1.12.0, but 1.13.0 will be released much sooner than our normal 6 month release cadence to support NumPy 2.0. Would you still support splitting them?","> Is that okay?\r\n\r\nThen probably, you don't need implement it, I don't think what you are after converges quickly to inf values (pretty much guessing by looking at the expression).  I'd say skip it if the user supplies `inf` for the 2D case. Like before, if it doesn't work in a robust fashion, you don't need to squeeze squares into circles. But I should not press on the point since I don't have all the details. \r\n\r\nIf the rest of the function works, then that's already good contribution I'd say and should not block it.  ","> @com3dian, @B0Gec I have just learned a very cool thing. Following the [reference](https:\/\/arxiv.org\/pdf\/1509.02237.pdf) in the docs, there is an analytical result for p=\u221e in the 1-D case. On page 10, it states that\r\n> \r\n> W\u221e(P,Q)=||F\u22121\u2212G\u22121||\u221e\r\n> \r\n> in the 1-D case, where F\u22121 and G\u22121 are the [quantile functions](https:\/\/en.wikipedia.org\/wiki\/Quantile_function) of P and Q respectively. Here, |\u22c5|\u221e is the [infinity norm](https:\/\/en.wikipedia.org\/wiki\/Norm_(mathematics)#Maximum_norm_(special_case_of:_infinity_norm,_uniform_norm,_or_supremum_norm)). I think this should be pretty straightforward to code. Once we have this, we can code for general `p` and test our result based on the analytical result.\r\n\r\n\r\n@yagizolmez Cool! \r\nI am currently busy with other stuff so I will have time to go through the literature and code. So for me, they are fine and I am happy with the existence of the new function about $\\infty$. I commented your pull request.\r\n\r\nThank you and keep up the good work! :rocket: "],"labels":["scipy.stats","enhancement"]},{"title":"ENH: optimize: add hessian to `_trustregion.py` callback","body":"fixes #19719\r\n","comments":["To prepare this PR for review, please:\r\n\r\n- add the gradient, too, so we don't skip straight to Hessian,\r\n- do this for all methods of `minimize` that support it,\r\n- check that the values are present and as expected in the [existing test](https:\/\/github.com\/scipy\/scipy\/blob\/309bfd5c257737cbf44ff1d6a0ac284db0c6f709\/scipy\/optimize\/tests\/test_optimize.py#L1602), and\r\n- mention it in the [documentation](https:\/\/github.com\/scipy\/scipy\/blob\/5e4a5e3785f79dd4e8930eed883da89958860db2\/scipy\/optimize\/_minimize.py#L208).","I have tried to find other minimization routines, but it seems I am not familiar enough with the scipy codebase. I have trouble finding the code for these functions:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/1104198\/f47e6ea5-7993-4587-9007-0484467100b8)\r\n\r\nTaking a look at the `_optimize.py`-file I could not find any definition of the supposedly imported functions. So I could need some help on that.\r\n\r\nOther than that I have now added the jacobian to `intermediate_results`, but so far only for the trust-region methods. If I understood it correctly the different trust-region methods only differ in how they approach the trust-region subproblem...","The functions are defined in `_optimize.py`. For example:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/e72de0714f0ad6e9b93c9107be84ee68ef70a5db\/scipy\/optimize\/_optimize.py#L2054\r\n\r\nForms the `intermediate_result` here:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/e72de0714f0ad6e9b93c9107be84ee68ef70a5db\/scipy\/optimize\/_optimize.py#L2230\r\n\r\nPerhaps you looked in `optmize.py` (no underscore) before?","I only looked at `__all__` in `_optimize.py` and ignored that the files is thousands of lines long... My bad.\r\n\r\nI hope I have addressed your suggestions well enough...","Ah, `__all__` is only used for the public methods.","Alright, first I need to apologize for my former commits, which were not really good and not up to my usual standard. Sorry for wasting your time and I highly appreciate your patience with me.\r\n\r\n1. I added the hessian as a return value to `newtoncg` as it was missing and also to the result fed into the callback.\r\n2. I disagree with the necessity for calling minimize without `g` and `h`, since all this would test is the quality of the gradient approximation, which depends on the local Lipschitz-behavior of `f`.\r\n3. I have adapted the docstring to account for the fact that now jacobians are always provided, no matter if approximated or not. Hessians are only provided, if present as callable.\r\n4. Tests are now passing locally, I hope it stays the same on the remote..","1. It looks like it wasn't added to `MINIMIZE_METHODS_HESS`, though?\r\n2. \r\n> since all this would test is the quality of the gradient approximation\r\n\r\nThat wasn't the intent. I don't know the internals of all these functions, and it's conceivable that they could use a Jacobian if it is provided and not use a Jacobian if it's not provided. For example, `newton` (a scalar root finder) defaulted to the secant method if no derivative was provided, so it would not estimate the derivative on its own. We want to know which is the case to provide useful documentation that avoids bug reports later on.","@j-bowhay do you have a moment to help bring this over the finish line? I can make the additions to the tests; I think we'd just need to work out how to document the methods\/conditions under which these new attributes are available.","I am quite busy preparing for university exams (unfortunately!) and have a few open prs that need some attention however I could probably get to this by the end of Jan.","Understood @j-bowhay! Good luck with exams!\r\n\r\nI'd like to try to finish it while it's still fresh in my mind to avoid starting from scratch mentally. @andyfaff @lucascolley do you have any bandwidth for this?\r\n\r\nWhen we added the `intermediate_result` interface, the intent was to pave the way for a lot of other information to be included. It made it a lot easier to do so technically, but the documentation and testing still needs the usual attention to detail. Can you help with that? We're adding `jac` and `hess` to all the methods of `minimize` (where available).","> @lucascolley do you have any bandwidth for this?\n\nUnfortunately, I'm in a similar position to Jake (my bandwidth for new PRs is going to be very low from now until summer really - need to catch up on some work I've neglected this past month or so).","Sorry, I'm out until 9th Jan onwards","@mdhaber I'd love to take a look once I'm back from my break on Jan 8. Is that reasonable? I may need some help getting into this code but I've been meaning to for a long time, so it might be a good opportunity. Cheers!","Sounds good @melissawm! Thanks, all.","Hi @melissawm, hope you've had a good break! I went ahead and re-opened this, and I think you can just push to the existing branch.","@melissawm were you still interested or shall I check in to see if the others have time now?"],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: Intermediate_result in trustregion could be richer","body":"### Is your feature request related to a problem? Please describe.\n\nI am currently using `_minimize_trust_region` in `scipy\/optimize\/_trustregion.py` to minimize a function of which I know the jacobian and Hessian. During the optimization, I want to determine some properties of the Hessian and possibly stop the iteration using the callback and raising `StopIteration` if needed.\r\n\r\nHowever, I realized that `_minimize_trust_region` only passes `m.fun` and `m.jac` to the callback via `intermediate_results`.\n\n### Describe the solution you'd like.\n\nI think it would be easy to add a similar check as for the final results, where the Hessian is added, if it is available.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\nI would be happy to do my own PR to add this, since it should be easy to do this.","comments":[],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: Parallelize CSR (sparse matrix) methods","body":"Apologies for the wall of text, I want to provide context and reasoning for the changes made here.\r\n\r\nMost CSR sparse matrix operations have an outer loop that iterates over the matrix rows. Often each iteration of this loop is, or can easily be made to be, independent. This PR parallelizes this loop.\r\n\r\nPR is fully implemented and working, but has some product-level questions remaining. See the tradeoffs section below. I'll update the PR to remove code for the rejected options.\r\n\r\n### Choice of parallelism framework\r\n\r\nOpenMP is explicitly off the table as per docs, so no `omp parallel for`. Common libraries like TBB or Taskflow seem too heavy for just this one application, correct me if I'm wrong. That leaves simple threads or a thread pool.\r\n\r\nI chose to use the interface of the [C++17 parallel algorithms](https:\/\/en.cppreference.com\/w\/cpp\/algorithm) to make the code future-proof and easy to understand for C++ programmers. I say *interface* because compiler support for those algorithms is not all there at the moment. Instead, I implemented the required algorithms on top of a thread pool using C++11 threads. This is fairly small and works on all platforms. It seems useful enough to be its own product, so I released it as [poolSTL](https:\/\/github.com\/alugowski\/poolSTL).\r\n\r\n### Parallel matmul\r\nOnly method that requires signature changes.\r\n\r\nThe core SMMP algorithm has two embarrassingly parallel phases. Phase 1 calculates the size of the result matrix then phase 2 fills it in.\r\n\r\nThe signature change is to the phase 1 implementation, `csr_matmat_maxnnz()`. Previously this method only calculated the result's nnz. A parallel phase 2 requires a pre-computed indptr (`Cp`), so the change is to have `csr_matmat_maxnnz()` return the row sizes as it computes the nnz. A cumsum of the row sizes is `Cp`. This is done in Python to handle the (rare) case where the output matrix has enough non-zeros to overflow the input matrices' index type.\r\n\r\nA small logic change is also required. The previous code would only count *computed values*, regardless whether those values are zero or not. Over-allocation is ok. The parallel code cannot do that as phase 2 is not allowed to emit explicit zeros. Therefore, phase 1 must also compute the values to test against zero to return an accurate nnz.\r\n\r\nUnfortunately this is extra work. Quick benchmarks suggest the single-threaded code is about 20-30% slower, but this is easily made up for in parallel.\r\n\r\n### Parallel binop\r\n\r\nI only parallelized the canonical version.\r\n\r\nBinop faces the same problem as matmul: the size and sparsity pattern of the output is needed for parallelism, but it's unknown. The sequential code would allocate for the worst case, then just construct the result. The parallel code uses matmul's approach: Phase 1 computes `Cp` in parallel, then phase 2 fills in the column indices and data in parallel. The sequential case remains the same single pass as before.\r\n\r\nOne weird observation I can't fully explain is that the sequential case now appears to be significantly faster than before, like often 2x. The only change I can identify is that the new code breaks some accidental inter-loop data dependencies, but that doesn't seem like it should be enough for the performance boost. Happy to be corrected.\r\n\r\n### Parallel Other methods\r\n\r\nMost obvious beneficiary is `csr_sort_indices`. Used everywhere and parallelizes very well. I also added an optimization to remove a temporary copy for the sort.\r\n\r\nOther embarrassingly parallel methods just replace their outer `for` loops with parallel `std::for_each`. Some methods that check the format instead use `std::all_of`.\r\n\r\n\r\n## Parallelism control\r\nNote: methods stay sequential unless the matrix is fairly large, current definition of \"large\" is 64k non-zeros. This may seem overly large, but sequential ops are *fast*. `pnumpy` also uses this threshold.\r\n\r\nParallelism is controlled with `threadpoolctl`:\r\n```python\r\nwith threadpoolctl.threadpool_limits(limits=4):\r\n    res = matrix1 @ matrix2\r\n```\r\n\r\nPR default is single threaded. One *must* use the code above to enable parallelism.\r\n\r\n## Benchmarks\r\nThe standard scipy benchmarks are too small to really exercise this PR. On my machine few take longer than a millisecond. I have a [branch](https:\/\/github.com\/alugowski\/scipy\/tree\/bigbench) with much bigger versions of the standard benchmarks: https:\/\/github.com\/scipy\/scipy\/commit\/6b78eacb75408faa2bacb0b4c29e949af6362822\r\n\r\nIf folks could try on their own benchmarks or their own workflows that would be very helpful.\r\n\r\n## Tests\r\n\r\nI configured the tests to run twice: once sequentially and once with parallelism forced on even for tiny problems. Happy to change this if it's excessive.\r\n\r\n## Product level decisions and tradeoffs\r\n\r\n### Is the worker count API ok?\r\n\r\nIs threadpoolctl enough?\r\n\r\nOther parallel SciPy C++ codes (fft, optimize) use explicit `worker` arguments. That would be difficult here as none of these methods are called directly by the user, and many are operators.\r\n\r\nAny other (or additional) options?\r\n\r\n### Thread pool vs Dedicated threads\r\nA common thread pool started on first use of a large op (Pros: threads are reused) vs starting and joining dedicated threads for each op (Pros: smaller library size, no idle worker threads left behind).\r\n\r\nControlled by `USE_POOL` constant in `scipy\/sparse\/sparsetools\/util_par.h`.\r\n\r\n### What ops to parallelize \r\nLibrary size is the main downside of this PR. Note that all the methods in `csr.h` are templates with quite a few instantiations each. The dedicated threads create about one more symbol for each method, while the thread pool requires several more (due to wrappers, vtables, etc). I've already more than halved the `.so` size from my first attempts, would appreciate further ideas from folks with stronger `nm` skills than mine. Also, I understand that the final release build does some symbol stripping so the following may be an exaggeration.\r\n\r\nTo further control library size this PR includes two more switches in `util_par.h`:\r\n * `PARALLEL_BINOP` whether to parallelize binops.\r\n * `PARALLEL_OTHERS` whether to parallelize methods other than matmul, sort_elements, binop. Currently off.\r\n\r\nIt should also be possible to parallelize just some binops (like plus, minus) and save library size on the others.\r\n\r\n\r\nFor context, rough sizes of `_sparsetools.cpython-310-darwin.so` as built by `python dev.py build` on my machine:\r\n\r\n* main branch: 4.0 MB\r\n* `USE_POOL=0`\/`PARALLEL_BINOP=0` : 4.8 MB\r\n* `USE_POOL=0`\/`PARALLEL_BINOP=1` : 8.0 MB\r\n* `USE_POOL=1`\/`PARALLEL_BINOP=0` : 5.4 MB\r\n* `USE_POOL=1`\/`PARALLEL_BINOP=1` : 13 MB\r\n\r\nIf the choice is to go dedicated threads (no pool) then we can rip just those methods from poolSTL and drop that file entirely. The pure threads functions are <100 lines total.","comments":["# Benchmarks\r\n\r\nI'm attaching the ASV benchmark compare of this PR against the main branch using the bigger benchmarks I mentioned ( https:\/\/github.com\/scipy\/scipy\/commit\/6b78eacb75408faa2bacb0b4c29e949af6362822 )\r\n\r\nRunning on M1 Macbook pro (6 + 2 cores)\r\n\r\nAll are single repetition because I couldn't get ASV to do repeats, hence noisy (ignore the noisy ~1ms runs). But the pattern is clear, the bigger benchmarks are significantly faster.\r\n\r\nRandom matrix multiply:\r\n```\r\n| Change   | Before [6b78eacb] <bigbench>   | After [a9acb88f] <parallel_csr>   |   Ratio | Benchmark (Parameter)                             |\r\n|----------|--------------------------------|-----------------------------------|---------|---------------------------------------------------|\r\n| +        | 1.03\u00b10ms                       | 2.62\u00b10ms                          |    2.55 | sparse.MatmulLargeCSRRand.time_large('10K,1M')    |\r\n| +        | 132\u00b10\u03bcs                        | 174\u00b10\u03bcs                           |    1.33 | sparse.MatmulLargeCSRRand.time_large('1K,1K')     |\r\n| -        | 1.90\u00b10ms                       | 1.31\u00b10ms                          |    0.69 | sparse.MatmulLargeCSRRand.time_large('100K,100K') |\r\n| -        | 4.29\u00b10ms                       | 1.92\u00b10ms                          |    0.45 | sparse.MatmulLargeCSRRand.time_large('1M,10K')    |\r\n| -        | 57.2\u00b10ms                       | 25.5\u00b10ms                          |    0.45 | sparse.MatmulLargeCSRRand.time_large('1M,1M')     |\r\n| -        | 310\u00b10ms                        | 87.7\u00b10ms                          |    0.28 | sparse.MatmulLargeCSRRand.time_large('10M,1M')    |\r\n| -        | 3.28\u00b10s                        | 824\u00b10ms                           |    0.25 | sparse.MatmulLargeCSRRand.time_large('10M,10M')   |\r\n| -        | 6.23\u00b10s                        | 1.50\u00b10s                           |    0.24 | sparse.MatmulLargeCSRRand.time_large('15M,15M')   |\r\n```\r\n\r\nSort elements:\r\n```\r\n| Change   | Before [6b78eacb] <bigbench>   | After [a9acb88f] <parallel_csr>   |   Ratio | Benchmark (Parameter)             |\r\n|----------|--------------------------------|-----------------------------------|---------|-----------------------------------|\r\n| +        | 799\u00b10\u03bcs                        | 953\u00b10\u03bcs                           |    1.19 | sparse.Sort.time_sort('Rand10')   |\r\n| -        | 767\u00b10\u03bcs                        | 283\u00b10\u03bcs                           |    0.37 | sparse.Sort.time_sort('Rand25')   |\r\n| -        | 1.45\u00b10ms                       | 404\u00b10\u03bcs                           |    0.28 | sparse.Sort.time_sort('Rand50')   |\r\n| -        | 3.14\u00b10ms                       | 805\u00b10\u03bcs                           |    0.26 | sparse.Sort.time_sort('Rand100')  |\r\n| -        | 50.0\u00b10ms                       | 12.4\u00b10ms                          |    0.25 | sparse.Sort.time_sort('Rand2000') |\r\n| -        | 5.67\u00b10ms                       | 1.29\u00b10ms                          |    0.23 | sparse.Sort.time_sort('Rand200')  |\r\n| -        | 26.4\u00b10ms                       | 4.94\u00b10ms                          |    0.19 | sparse.Sort.time_sort('Rand1000') |\r\n```\r\n\r\nLarge arithmetic ops:\r\n```\r\n| Change   | Before [6b78eacb] <bigbench>   | After [a9acb88f] <parallel_csr>   |   Ratio | Benchmark (Parameter)                                                   |\r\n|----------|--------------------------------|-----------------------------------|---------|-------------------------------------------------------------------------|\r\n\r\n*truncated for size, see Arithmetic.txt below*\r\n\r\n| -        | 176\u00b10ms                        | 91.6\u00b10ms                          |    0.52 | sparse.Arithmetic.time_arithmetic('csr', 'AB', 1000, True, '__mul__')   |\r\n| -        | 3.46\u00b10ms                       | 1.81\u00b10ms                          |    0.52 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 250, False, '__sub__')   |\r\n| -        | 11.0\u00b10ms                       | 5.47\u00b10ms                          |    0.5  | sparse.Arithmetic.time_arithmetic('csr', 'BA', 250, False, '__mul__')   |\r\n| -        | 5.67\u00b10ms                       | 2.73\u00b10ms                          |    0.48 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 250, True, '__mul__')    |\r\n| -        | 55.2\u00b10ms                       | 26.4\u00b10ms                          |    0.48 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 1000, False, '__sub__')  |\r\n| -        | 3.53\u00b10ms                       | 1.65\u00b10ms                          |    0.47 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 250, False, '__sub__')   |\r\n| -        | 3.62\u00b10ms                       | 1.67\u00b10ms                          |    0.46 | sparse.Arithmetic.time_arithmetic('coo', 'BB', 250, False, '__sub__')   |\r\n| -        | 91.8\u00b10ms                       | 39.4\u00b10ms                          |    0.43 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 1000, False, '__mul__')  |\r\n| -        | 23.6\u00b10ms                       | 10.0\u00b10ms                          |    0.43 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 250, False, '__mul__')   |\r\n| -        | 92.0\u00b10ms                       | 37.7\u00b10ms                          |    0.41 | sparse.Arithmetic.time_arithmetic('csr', 'AA', 1000, True, '__mul__')   |\r\n| -        | 190\u00b10ms                        | 74.5\u00b10ms                          |    0.39 | sparse.Arithmetic.time_arithmetic('csr', 'AB', 1000, False, '__mul__')  |\r\n| -        | 211\u00b10ms                        | 82.1\u00b10ms                          |    0.39 | sparse.Arithmetic.time_arithmetic('csr', 'BA', 1000, False, '__mul__')  |\r\n| -        | 22.9\u00b10ms                       | 8.94\u00b10ms                          |    0.39 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 250, False, '__mul__')   |\r\n| -        | 208\u00b10ms                        | 78.6\u00b10ms                          |    0.38 | sparse.Arithmetic.time_arithmetic('csr', 'BA', 1000, True, '__mul__')   |\r\n| -        | 23.5\u00b10ms                       | 8.05\u00b10ms                          |    0.34 | sparse.Arithmetic.time_arithmetic('coo', 'BB', 250, True, '__mul__')    |\r\n| -        | 23.6\u00b10ms                       | 8.13\u00b10ms                          |    0.34 | sparse.Arithmetic.time_arithmetic('dia', 'BB', 250, True, '__mul__')    |\r\n| -        | 23.2\u00b10ms                       | 7.65\u00b10ms                          |    0.33 | sparse.Arithmetic.time_arithmetic('coo', 'BB', 250, False, '__mul__')   |\r\n| -        | 23.5\u00b10ms                       | 7.71\u00b10ms                          |    0.33 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 250, True, '__mul__')    |\r\n| -        | 401\u00b10ms                        | 123\u00b10ms                           |    0.31 | sparse.Arithmetic.time_arithmetic('csr', 'BB', 1000, True, '__mul__')   |\r\n| -        | 395\u00b10ms                        | 117\u00b10ms                           |    0.3  | sparse.Arithmetic.time_arithmetic('csr', 'BB', 1000, False, '__mul__')  |\r\n```\r\nRaw output from `python dev.py bench --compare`:\r\n\r\n[compare-sparse.Arithmetic.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722445\/compare-sparse.Arithmetic.txt)\r\n[compare-sparse.Conversion.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722447\/compare-sparse.Conversion.txt)\r\n[compare-sparse.MatmulLargeCSRPoissonSquared.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722448\/compare-sparse.MatmulLargeCSRPoissonSquared.txt)\r\n[compare-sparse.MatmulLargeCSRRand.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722449\/compare-sparse.MatmulLargeCSRRand.txt)\r\n[compare-sparse.Matvec.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722450\/compare-sparse.Matvec.txt)\r\n[compare-sparse.NullSlice.time_getcol.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722451\/compare-sparse.NullSlice.time_getcol.txt)\r\n[compare-sparse.Random.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722452\/compare-sparse.Random.txt)\r\n[compare-sparse.Sort.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722453\/compare-sparse.Sort.txt)\r\n[compare-sparse_linalg_expm.Expm.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722454\/compare-sparse_linalg_expm.Expm.txt)\r\n[compare-sparse_linalg_solve.Bench.time_solve.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722455\/compare-sparse_linalg_solve.Bench.time_solve.txt)\r\n[compare-sparse_matrix_power.txt](https:\/\/github.com\/scipy\/scipy\/files\/13722456\/compare-sparse_matrix_power.txt)\r\n","CI errors seem to be `scipy\/optimize\/tests\/test_linprog.py::TestLinprogIPSparse::test_bug_6139`. That test passes for me locally, and I can't figure out how it relates to this change.  Any insight appreciated!","Thanks @alugowski, this looks very interesting! The first question that came to mind for me regarding benchmarks\/performance is how linear the scaling is with the requested number of threads. E.g., use an array of size 64k x 32, and plot time taken by parallelized operations as a function of the number of threads. The closer to linear the scaling the better. Did you try anything like that already?\r\n\r\n> CI errors seem to be `scipy\/optimize\/tests\/test_linprog.py::TestLinprogIPSparse::test_bug_6139`. That test passes for me locally, and I can't figure out how it relates to this change. Any insight appreciated!\r\n\r\nI can't reproduce it either, but from the CI log it looks to me like the solution only slightly exceeds the requested `atol=1e-8`, so this is unlikely to be a significant issue, more a minor tolerance thing for an inherently unstable test.\r\n\r\nThe CircleCI job shows a build error in `csr.cxx`, that seems more important to sort out.\r\n\r\n","Re API for parallelism: your use of `threadpoolctl` sounds like a good idea to me, and is probably enough - given your explanation of why `workers` isn't a great fit here, I'm not sure what else would help. Perhaps a global switch to switch between single-threaded and completely automatic using all available CPU tools, but that's a pretty blunt tool and we try to avoid global state.\r\n\r\nI don't have immediate ideas for further operations, probably best to complete the work on this first (or at least till we have a strategy for getting this merged).","I'll write some code for a speedup benchmark.\r\n\r\n> The CircleCI job shows a build error in `csr.cxx`, that seems more important to sort out.\r\n\r\nThat'll take some thought, there is no error just \"killed\". My only guess is a compile unit size issue, which relates to:\r\n\r\n> I don't have immediate ideas for further operations, probably best to complete the work on this first (or at least till we have a strategy for getting this merged).\r\n\r\nI need to clarify. I agree further non-CSR ops would be worth discussing later. I meant that we may have to make a decision on what to turn off *in this PR* for library size issues. Hence why I have those three groups:\r\n* always on: matmul and sort_elements. Relatively small compilation penalty and a large bang for the buck.\r\n* binops: There are many of these, hence parallelizing these nearly doubles `_sparsetools.so` size. On in the PR.\r\n* others: all the other embarrassingly parallel ops in `csr.h`, like `to_dense`, `scale_columns`, etc. The PR has parallel code for these, but is turned off in the PR as is only for library size reasons.\r\n\r\nI'm hoping someone has some ideas on how to not blow up the library size. My best guess is that having both the parallel and sequential code paths doubles the generated code size, but it's unclear why that would be as both call the same lambdas. Perhaps the compiler inlines too much.","Speedup of `tocsr`(mostly sort_elements), two binops and a matmul on a random matrix with 5M nonzeros:\r\n\r\n![random](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/4ffbd87d-638e-41d3-96c1-fd0042c19252)\r\n\r\nOf course the sparsity pattern matters a lot with any sparse matrix op. A Poisson 2D grid is often a difficult one. Here:\r\n\r\n![poisson2d](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/a3f6bea4-691c-446a-8e41-944b8976f511)\r\n\r\nPart of the reason for worse speedup is that the ops in general are significantly faster. The random matmul takes 9.8s, while the Poisson 2d matmul takes only 0.13s.\r\n\r\nThe binops are a difficult to decide on. In those plots I re-enabled the parallel versions of the \"is_canonical\" method, without that the speedup barely touches 1.5x at these sizes. That's still a nice speedup, but is it worth it?\r\n\r\nNote that parallel binops start to shine at much larger sizes:\r\n\r\n![random 50M](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/4355d9bc-82d3-41f5-8ed4-4ce98ef7bc06)\r\n\r\nHence the difficult tradeoff decision. Is the extra complexity worth it for only large matrices? I think if only plus\/minus were parallelized that might be ok, but I'd also be happy with sequential binops.\r\n\r\nAttaching the script I used. Command for first plot:\r\n` python dev.py python speedup.py 5_000_000 random plot tocsr add mul matmul`\r\n\r\n[speedup.py.txt](https:\/\/github.com\/scipy\/scipy\/files\/13736468\/speedup.py.txt)\r\n","I noticed the comment for M1 and thought it could cause some of the levelling off; I believe M1's always have 4 efficiency cores, not 2. However, I repeated the benchmarks on a machine with 12 physical cores and it doesn't look better:\r\n\r\n<img width=\"293\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/98330\/cc9a11c7-4cfd-459a-8b53-c455fcaeb864\">\r\n\r\nThis is with the default build settings in this PR. Something is wrong here though, these curves should be much closer to linear. A square root type leveling off indicates that there's communication overhead between threads where that is not expected (elementwise ops are the worst here, and they shouldn't have cross-thread data passing), or maybe the data chunk size is chosen too small, or something else like that. It may be worth profiling with something like `py-spy` to figure out what is going on. If we have C++-level threading without callbacks to Python, I think we should only offer a parallel API with expected (more-or-less linear) scaling up to a reasonable number of threads (e.g., 64).","> OpenMP is explicitly off the table as per docs, so no `omp parallel for`. Common libraries like TBB or Taskflow seem too heavy for just this one application, correct me if I'm wrong. That leaves simple threads or a thread pool.\r\n\r\nYes, we're not ready to revisit OpenMP I'm afraid. And yes, TBB & co are a bit too heavy for us.","> I noticed the comment for M1 and thought it could cause some of the levelling off; I believe M1's always have 4 efficiency cores, not 2.\r\n\r\nMine is 6 power + 2 efficiency.\r\n\r\n> However, I repeated the benchmarks on a machine with 12 physical cores and it doesn't look better:\r\n> <img alt=\"image\" width=\"293\" src=\"https:\/\/private-user-images.githubusercontent.com\/98330\/292228358-cc9a11c7-4cfd-459a-8b53-c455fcaeb864.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxODQ1NzcsIm5iZiI6MTcwMzE4NDI3NywicGF0aCI6Ii85ODMzMC8yOTIyMjgzNTgtY2M5YTExYzctNGNmZC00NTlhLThiNTMtYzQ1NWZjYWViODY0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDE4NDQzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWU4NDE0MjAyNTg1MTEwZTM5ZmZmZDM4ZWE1YzAzNjBiNDNmMTZhNzYxN2Y1NDY2ZWJlYzZmNTE3ZWJmOWZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.K8AxEbXAgdufSrrrXppUtLqavzfnGg3L5VsUP6k9fS4\">\r\n> \r\n> This is with the default build settings in this PR. Something is wrong here though, these curves should be much closer to linear. A square root type leveling off indicates that there's communication overhead between threads where that is not expected (elementwise ops are the worst here, and they shouldn't have cross-thread data passing), or maybe the data chunk size is chosen too small, or something else like that. It may be worth profiling with something like `py-spy` to figure out what is going on. If we have C++-level threading without callbacks to Python, I think we should only offer a parallel API with expected (more-or-less linear) scaling up to a reasonable number of threads (e.g., 64).\r\n\r\nIt's simpler than that. The binop is largely a memory scan, so it's dominated by memory bandwidth. One way to show that is to make the base operation more compute intensive so memory no longer dominates. To show that, in `csr_elmul_csr()` wrap the `multiplies<T>()` with `expensive_op(std::multiplies<T>())`:\r\n\r\n```c++\r\ntemplate <class Op>\r\nclass expensive_op {\r\npublic:\r\n    explicit expensive_op(Op op): op(op) {}\r\n    using T = typename Op::result_type;\r\n\r\n    T operator()(const T& a, const T& b) const {\r\n        int n = (int)op(a, b);\r\n        for (int i = 1; i < 500; ++i) {\r\n            n = (i % 2) ? (3*n + 1) : (n \/ 2);\r\n        }\r\n        if (n == 0) return 0;\r\n        return op(a, b);\r\n    }\r\n\r\n    Op op;\r\n};\r\n```\r\n\r\nNow `python dev.py python speedup.py 500_000 random plot add mul` shows that the expensified mul *does* scale as expected:\r\n![expensive_op](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/981548df-1c2c-4311-b659-551d2fbacaae)\r\n\r\nThe weird shape is because p>1 needs to do twice the work, hence no speedup for 2 threads then the slope of ~0.5 is expected. Without that p=1 optimization (add `true ||` to the `if` to force the parallel path) the scaling is nearly perfect:\r\n\r\n![expensive_op_p1](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/be0e8bf1-15b9-40f9-8faa-80162fccfdce)\r\n\r\nSo my claim is that the poor scaling is a natural property of the binop operation. The main improvements would be to only scale enough to not over subscribe memory bandwidth. To me it looks like my code works well on very large matrices, maybe 1M or larger. One option would be to limit the parallel version to that size. This code has no inter-thread communication at all (the 2 phases remove that need).\r\n\r\nThe other kink with binops is that the handler first checks both operands with `csr_has_canonical_format` (which is also just a memory-dependent scan). I've parallelized that op, but it's in the OTHERS group. I've enabled it in my previous comment, but it's disabled in the PR. Set `PARALLEL_OTHERS` to 1 in `util_par.h` to duplicate my plots.","Note about large p, like 32 or 64. Those sizes can uncover some non-obvious sequencing. A big one is allocators, the standard lib's allocator is thread safe but it can become a hotspot on large thread counts.  This PR creates as few splits as possible, which should suffer the least. Unfortunately any solutions (like arena allocators or other tricks) are in libraries like TBB.","I've implemented a way to parallelize only some binops, and enabled that for plus, minus, and multiply only. That reduces the compilation time and so I enabled the other parallel ops too. Library size is smaller than before and all ops are parallel. \r\n\r\nI also tweaked the parallelization thresholds a bit, now the memory-bound operations have to be larger to be parallelized, around 1M nonzeros. It should basically never be *worse* now. Sure 2x isn't amazing, but it's not nothing either. Either way, those single-scan methods are fairly fast in absolute terms anyway, so I wouldn't shed a tear if they were excluded.\r\n\r\nI think this is generally a good tradeoff. You should no longer see any differences when you run the speedup script. I also added a `matvec` bench to the script:\r\n![updated 5M](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/8d2e23b2-4aa5-4a5a-aa3f-d42974340943)\r\n\r\nJust for context and a sanity check, I've also added a `random2d` matrix type that creates an `ndarray`, to see how BLAS scales:\r\n\r\n![blas 5M](https:\/\/github.com\/scipy\/scipy\/assets\/2730364\/a38b6f5f-5ebc-47f1-b08a-9df8f0432f01)\r\n(OpenBLAS on macOS, default from numpy 1.26.2 wheels)\r\n\r\nMakes me feel a little better :)\r\n\r\n[speedup.py.txt](https:\/\/github.com\/scipy\/scipy\/files\/13749810\/speedup.py.txt)\r\n","I know I'm a broken record about library size, but even on the `main` branch binops account for half of `_sparsetools.so` size (~2MB worth), and about 5% of overall `python dev.py build` runtime. These things are important so I'm trying to be upfront about it.","I've simplified the PR based on feedback:\r\n * Parallelize only matmul, sort_elements, and matvec. These have obvious payoff.\r\n * No parallel binops. See #19765 for a sequential binop optimization.\r\n * Pure threads only, no thread pool\r\n\r\nThe parallelism utilities are broken down as:\r\n * `util_par_algorithms.h`: everything needed to parallelize any `for` loop. Just convert the loop to use `std::for_each` and use the provided `threaded` execution policy: `std::for_each(util_par::threaded(4), ...)`\r\n * `util_par_config.h`: configures the remembered worker count (set by `threadpoolctl` controller) and supplies a `par_if` method to decide between parallel and sequential based on datastructure size.\r\n\r\nThe PR looks big mostly because of the signature change in matmul that propagates through several layers. The \"changes\" in BSR are simply restoring the old pre-parallel CSR matmul_maxnnz that was reused by BSR.\r\n","Thanks for adding a lot of detail here @alugowski. Looks like you're right about the scaling and memory bandwidth indeed. \r\n\r\n> I've simplified the PR based on feedback:\r\n\r\nThese changes make sense I think, and should make it easier to get this merged since the wins are clearer without much of a downside (beyond a bit more code complexity of course).\r\n\r\nLooks like there's a crash in `_mul_sparse_matrix` in the Windows job that builds sdist\/wheel (so uses `-O3`, unlike the `dev.py` builds).","@alugowski one other question: did you consider fork-safety of the threadpool? The pocketfft threadpool implementation uses `pthread_atfork` shutdown\/restart to handle this (hat tip to @peterbell10 for pointing this out).\r\n\r\nTo be more precise - this is needed because `multiprocessing` is doing fork without exec, after threads may already be created. This is the reason why we can't use OpenMP (xref gh-10239). More CPython's fault than OpenMP's probably (see https:\/\/github.com\/python\/cpython\/issues\/84559), but it is what it is so we need to be robust against this.","> @alugowski one other question: did you consider fork-safety of the threadpool? The pocketfft threadpool implementation uses `pthread_atfork` shutdown\/restart to handle this (hat tip to @peterbell10 for pointing this out).\r\n> \r\n> To be more precise - this is needed because `multiprocessing` is doing fork without exec, after threads may already be created. This is the reason why we can't use OpenMP (xref [gh-10239](https:\/\/github.com\/scipy\/scipy\/issues\/10239)). More CPython's fault than OpenMP's probably (see [python\/cpython#84559](https:\/\/github.com\/python\/cpython\/issues\/84559)), but it is what it is so we need to be robust against this.\r\n\r\nGood question. There is no longer a thread pool, and no mutexes or other fork-unfriendly constructs. Since `fork()` only carries over the calling thread to the child, I believe there is nothing to handle.\r\n\r\nThe original PR had an *optional* thread pool, but had to be manually enabled with `USE_POOL=1` else the default fork-join model was used. Not `fork()`, just starting\/joining threads in each parallel function call. All the performance analytics we talked about above are with this model. I saw negligible performance benefits from the pool, just more code, so I removed that unused code about a week ago. \r\n\r\nAll threading logic is in this one function: https:\/\/github.com\/scipy\/scipy\/blob\/ff667f9d7b19618bf80d61ec377224cddc2610fc\/scipy\/sparse\/sparsetools\/util_par_algorithms.h#L51-L88","> Looks like there's a crash in `_mul_sparse_matrix` in the Windows job that builds sdist\/wheel (so uses `-O3`, unlike the `dev.py` builds).\r\n\r\nThe last two test fails were true headscratchers. Appologies it took so long to figure them out. The MSVC fail was caused by my use of `std::exclusive_scan` to compute `Cp` in `csr_tocsc()`, replacing a manual loop. I can't explain why this causes a crash there, but it's not a needed change so I just reverted it (made the PR slightly smaller, too).\r\n\r\nThe gcc8 workflow fail is even more weird. I spent a lot of time trying to figure out what's different about GCC 8, and why that one test which also didn't use any sparse methods at all. But the issue wasn't the compiler, but that pytest fixture in `test_base.py`. Again this makes no sense to me, since the tests are not related at all, `test_base` is run much later than the failing test, and solver success shouldn't be affected anyway. But moving the fixture solved the issue. The gcc8 workflow uses a different way to run tests (not using `python dev.py tests`). I'm using a pytest fixture to run all tests both threaded and sequentially.\r\n\r\nNow CircleCI is complaining about a missing `doc_requirements.txt`, which is something this PR does not touch.","> Now CircleCI is complaining about a missing doc_requirements.txt, which is something this PR does not touch.\n\nMerging main will fix this, the file was recently moved to `requirements\/doc.txt` instead.","If C++ parallel has a wait policy, it could end up stalling when switching between different parallel contexts: https:\/\/github.com\/OpenMathLib\/OpenBLAS\/issues\/3187 and https:\/\/github.com\/OpenMathLib\/OpenBLAS\/issues\/3187#issuecomment-940999630\r\n\r\nIn the issue, the example switches between OpenBLAS and OpenMP parallel contexts and a threadpool's wait policy causes the program to slow down.","> If C++ parallel has a wait policy, it could end up stalling when switching between different parallel contexts: [OpenMathLib\/OpenBLAS#3187](https:\/\/github.com\/OpenMathLib\/OpenBLAS\/issues\/3187) and [OpenMathLib\/OpenBLAS#3187 (comment)](https:\/\/github.com\/OpenMathLib\/OpenBLAS\/issues\/3187#issuecomment-940999630)\r\n> \r\n> In the issue, the example switches between OpenBLAS and OpenMP parallel contexts and a threadpool's wait policy causes the program to slow down.\r\n\r\nThe linked comment says the issue is that OpenMP uses active waiting in its own thread pool, meaning OpenMP worker threads spin waiting for work. This crowds out any other workloads. There's nothing to be done about that, besides using OpenMP yourself. The parallel code in this PR does not actively wait, and neither did the thread pool that was originally in the PR."],"labels":["enhancement","scipy.sparse","C\/C++"]},{"title":"ENH: Add half=True kwarg to minimum_phase","body":"It always bothered me that `scipy.signal.minimum_phase` gave a filter half the length with (roughly) half the effect on the magnitude spectrum. This PR adds a `half=True` backward-compatible kwarg to `scipy.signal.minimum_phase` that you can set to `half=False` to get a filter the same length as `h` with the same magnitude spectrum. This matches the reference case in Oppenheim and Shafer. Also fixes a small bug where previously we used `len(h)` rather than `n_fft` to decide where to truncate the cepstral filter, though the effects seem minimal.","comments":["[updated example output](https:\/\/output.circle-artifacts.com\/output\/job\/4e89adf4-2384-4a5b-affc-d3efaab6852a\/artifacts\/0\/html\/reference\/generated\/scipy.signal.minimum_phase.html#scipy.signal.minimum_phase) showing that the magnitude response is preserved with `half=False`:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/2365790\/fd531d24-228e-4ac0-9b24-09264ac31e99)\r\n","@e-q, @endolith, @WarrenWeckesser you were involved in the reviews of the initial implementation in #6497, pinging you in case you have time to look at this fix\/enhancement as well!","I bumped the milestone since Eric ping for a few more reviews, but if someone is confident to merge before I branch in a few days, just adjust the milestone back.","Maybe a good one for @DietBru ?"],"labels":["enhancement","scipy.signal"]},{"title":"Tolerance arguments for `scipy.sparse.linalg.{lsmr,lsqr}` do not match related functions","body":"There were a bunch of long-outstanding changes regarding `tol\/atol\/rtol` in `scipy.sparse.linalg.*`, c.f. #15738, #18934, #18943 and #19702.\r\n\r\nWhile preparing the latter (to try to make the imminent 1.12 more consistent), I fixed a couple outstanding functions, but noticed that `lsmr` and `lsqr` have a completely different setup for their tolerance arguments:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/c4ce0c4560bc635867512c4d2ea6db6f666d3eeb\/scipy\/sparse\/linalg\/_isolve\/lsmr.py#L57-L74\r\n\r\nWhat's worse, it uses `atol` -- a kwarg that's pretty universally used to mean _absolute_ tolerance -- as a _relative_ tolerance, which is IMO pretty bad as a footgun for users coming from any of the other `scipy.sparse.linalg.*` functions.\r\n\r\nI don't have a good suggestion how to fix this (much quickly enough for 1.12!), but perhaps we can find a way to harmonize this a bit better?\r\n\r\nFWIW, as of SciPy 1.14, the other functions in `scipy.sparse.linalg.*` should uniformly default to `rtol=1.e-5` as relative tolerance and `atol=0` as absolute tolerance (with any remaining `tol=` usages removed).\r\n\r\nCC @tupui who touched this in https:\/\/github.com\/scipy\/scipy\/commit\/3d53ac2972201267a415c343175418c2b4184932, and CC @mdhaber for the related #7219\r\n\r\nCC @ilayn @j-bowhay ","comments":[],"labels":["scipy.sparse.linalg","maintenance"]},{"title":"ENH: Optimize least_squares LM Method with Automatic Jacobian Scaling when scaling factor is not specified (Default diag=None). ","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nThis PR addresses the issue described in #19459 - \"BUG: optimize.least_squares giving poor result compared to optimize.leastsq and optimize.curve_fit\".\r\n\r\n#### What does this implement\/fix?\r\nThis enhancement modifies the behavior of optimize.least_squares when used with the method='lm' (Levenberg-Marquardt algorithm). Previously, unless x_scale was set to 'jac', the default behavior was to use diag=1.0 for scaling, leading to suboptimal fits in certain scenarios. This change proposes to set the default value of x_scale='jac' when method=='lm', aligning the behavior more closely with that of optimize.leastsq and optimize.curve_fit. This results in improved optimization performance and robustness.\r\nSummary of Changes\r\nSet default x_scale='jac' for least_squares with method='lm'.\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":["Can you add a unit test demonstrating that the change performs as intended?","> Can you add a unit test demonstrating that the change performs as intended?\r\n\r\nThanks for the comment, Updated the unit test and committed the change. Please review the PR and let me know. ","> I cannot find how to remove my review here unfortunately. Will take a closer look in some time.\r\n\r\nThanks for the email and suggested changes, I did them accordingly and committed them. Please let me know if you need any further updates from me :) ","Hi @Roshan-Velpula  , if you do the following the doc build check should work:\r\n\r\n```\r\ngit fetch --all\r\ngit merge upstream\/main\r\ngit push\r\n```","> Hi @Roshan-Velpula , if you do the following the doc build check should work:\r\n> \r\n> ```\r\n> git fetch --all\r\n> git merge upstream\/main\r\n> git push\r\n> ```\r\n\r\nHi, Thank you for the message! Pushed the changes and doc build check worked. :) "],"labels":["enhancement","scipy.optimize","needs-decision"]},{"title":"DOC\/DX: point to latest version of docs on contributor pages","body":"### Issue with current documentation:\n\nPeople (especially new contributors) can be led astray if they end up on an outdated version of the development docs (and don't realise it).\n\n### Idea or request for content:\n\n- Add a note to the dev sections of released docs to direct people to the devdocs (as this is almost always what they want).\r\n\r\nAn alternative would be to automatically redirect, but I think it makes sense to have the HTML pages available for old versions, even if their use would be rare.\n\n### Additional context (e.g. screenshots, GIFs)\n\n- Would probably be nice to get the version switcher fixed before this (gh-18031).","comments":["@tylerjereddy how could we go about this? A PR to maintenance branch + backports + a PR every release, or is there a more sophisticated way to hide notes from the devdocs?\n\n(Sidenote: I vaguely remember reading something about merging the two docs websites into one, I don't know how speculative that was though)","I think @tupui had thoughts on this kind of stuff. I'm not fully sure what you mean just yet though:\r\n\r\n> led astray if they end up on an outdated version of the development docs\r\n\r\nYou mean landing on a stable release that is older than current stable or? There is the red color for example on an older release:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/7903078\/bc155180-d4cb-4f54-bbc7-8d7387444027)\r\n\r\nAlthough it currently shows up as red for me on `1.11.4` as well, but there's also the version switcher issue. There may be a few different problems that need to be solved. Normally wouldn't want to backport just for a warning banner or something, sometimes we manually apply stuff right on the docs server but that's also risky without peer review, etc.","At least for me the dev link works from a stable release, no? (ignore current version switcher issue, so from `1.11.3`)","Sorry, I think the message was lost in the terminology here, let me elaborate.\r\n\r\nThis PR is just about the `\/dev` section of the docs (https:\/\/scipy.github.io\/devdocs\/dev\/index.html).\r\n\r\nThe specific problem is a user landing on e.g. https:\/\/docs.scipy.org\/doc\/scipy-1.11.4\/dev\/index.html (any released version, stable or old - I imagine the stable case is not uncommon). Sometimes changes are made to the `\/dev` section between releases, and it can be confusing if new contributors are reading outdated info (especially when there are changes to build instructions[^1]). The problem only gets worse the older the version of the docs the contributor has found themself on.\r\n\r\nThe utility of the `\/dev` section in released doc versions is very low IMO, as most readers want the most up-to-date info. As mentioned in the issue description, an alternative would be to always redirect to the `development` version, but it probably makes sense to keep the older versions accessible for reference\/historical reasons.\r\n\r\n---\r\n\r\nHopefully this makes sense now, but let me know if not! I assume that the +1s from Pamphile and Ralf were to what I was intending, but perhaps not.\r\n\r\n> At least for me the dev link works from a stable release, no? (ignore current version switcher issue, so from 1.11.3)\r\n\r\nYeah this is fine and orthogonal.\r\n\r\n[^1]: this issue was motivated by a discussion on the Scientific Python Discord where Pamphile was helping Brigitta with build issues","Ah ok, I took the other meaning of \"development docs,\" the clarification is very helpful, thanks.\r\n\r\nMaybe it would be helpful to have a small sample diff to show how intrusive\/minor the change would be. Then we could adopt a practice of adding the patch\/note to future stable release process, but as for backporting maybe we could indeed get away with manually patching what's already out there (don't really like that, but far less work if it can be done with high degree of safety, and perhaps only last N releases but not too far back).","> Maybe it would be helpful to have a small sample diff to show how intrusive\/minor the change would be.\n\nIndeed, and that's what I'm not sure of either. Ideally, I think it would be good to have a note on every page in the `\/dev` section.\n\nMy initial question was whether there would be some clever way to make a PR to `main` and just hide the notes from the `development` version, or whether we need the backporting \/ addition to release process.","We talked about this at the community meeting today and I just wanted to mention one aspect.\r\n \r\nIf we consider the option of just redirecting people to the latest version whenever the Development pages are visited, it may be confusing for users. Consider a user navigating the (say) API docs for version 1.11.0\r\n\r\nIf they navigate to the development docs, they would be immediately redirected to the latest version of the Development pages. If they now try to navigate back to the API docs, they will still be in the latest version of the pages - and may lose that context if they are not paying attention. To mitigate this, one option would be to use a version warning banner, which is a fairly simple change in the conf.py enabled in later versions of the pydata-sphinx-theme (note that this would depend on #16660 being resolved). Here's how that looks like (the red banner at the very top of the page):\r\n\r\n![Screenshot_20231227_175913](https:\/\/github.com\/scipy\/scipy\/assets\/3949932\/28fa37b8-bc59-4c6f-b25c-7d8413f09dde)\r\n\r\nThe backporting option would not be as elegant, but eliminates this kind of problem.","Older version of the dev section of the docs are regularly useful to me, so I'd like to keep them where they are. Aside from that: a prominent banner or note will be useful indeed, so +1 on that.","same issue in skimage: scikit-image\/scikit-image#7138","> If we consider the option of just redirecting people to the latest version whenever the Development pages are visited, it may be confusing for users. Consider a user navigating the (say) API docs for version 1.11.0\r\n\r\nThis is indeed not nice. What seems worse is that there is no way to get rid of the banner. I am using the dev docs only typically, and have to stare at this prominent red bar every time I use the docs now. It really needs an `x` to be able to close it, and that should be remembered for the duration of the browsing session.\r\n\r\nIn the absence of that, the cure here seems worse than the disease.","> It really needs an x to be able to close it, and that should be remembered for the duration of the browsing session.\n\nWould definitely be nice if the theme included this!\n\nI'm neutral on the issue of the banner being too prominent (hard to weigh up the benefits of it being there vs what it looks like), but I definitely think that this is overall negative if the banner shows on the contributor section of the docs. If we can't get that working soon, I'd be in favour of a revert.","Relevant issue: https:\/\/github.com\/pydata\/pydata-sphinx-theme\/issues\/1384","How about a compromise: remove the banner for \"latest\", but leave the banner for older versions of the docs. This would be a feature proposal for the PyData Sphinx theme, but I think it may be a reasonable feature. We could also try our own solution for that, I'm happy to investigate.","That sounds perfectly reasonable to me and a good idea, but what I really want is a banner that shows on the stable version of the contributor guide, pointing to the \"latest\" version. If we can do both of those that would be great.","@melissawm proposal SGTM and can probably be done now. The added feature @lucascolley proposes for only the contributor\/development section sounds good too, as long as it's easy to do (i.e., avoid custom fiddly JS code, that usually breaks sooner or later).","Now that gh-20184 is in I'll change the title to my remaining feature request"],"labels":["Documentation","DX"]},{"title":"Unknown option '-fvisibility=hidden' for fotran compiler while building scipy from source with meson","body":"### Describe your issue.\r\n\r\nI am trying to build `scipy` 1.11.4 from source with `meson` because I'd like to take advantage of Intel MKL linear algebra libraries that are installed on our system. I followed instructions [here](https:\/\/github.com\/scipy\/scipy\/issues\/16200#issuecomment-1615094519) for how to link with MKL libraries while building with `meson`. The build failed after some steps, and scrolling through the output the only errors I could see had to do with an unknown option `-fvisibility` passed to the fortran compiler. I'm using Intel's C\/C++ and fortran compilers, `icx` and `ifx`.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\npip3 install . --config-settings=setup-args=-Dblas=mkl-dynamic-lp64-seq --config-settings=setup-args=-Dlapack=mkl-dynamic-lp64-seq\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n      [430\/1622] Compiling Fortran object scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_det.f.o\r\n      FAILED: scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_det.f.o\r\n      \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin\/ifx -Iscipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/linalg -I..\/scipy\/linalg -I..\/..\/..\/..\/..\/..\/..\/..\/tmp\/pip-build-env-4s8_r9mu\/overlay\/lib\/python3.11\/site-packages\/numpy\/core\/include -I..\/..\/..\/..\/..\/..\/..\/..\/tmp\/pip-build-env-4s8_r9mu\/overlay\/lib\/python3.11\/site-packages\/numpy\/f2py\/src -Iscipy\/lib_fortranobject.a.p -I\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/latest\/lib\/pkgconfig\/..\/..\/include -I\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/latest\/lib\/pkgconfig\/..\/..\/linux\/compiler\/include -I\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/include\/python3.11 -fvisibility=hidden -DNDEBUG -D_FILE_OFFSET_BITS=64 -warn general -warn truncated_source -stand=none -O3 -fPIC -module scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p -gen-dep=scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_det.f.o -gen-depformat=make -o scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_det.f.o -c ..\/scipy\/linalg\/src\/det.f\r\n      xfortcom: Unknown command line argument '-fvisibility'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\n      xfortcom: Did you mean '--ddg-simplify'?\r\n      xfortcom: Unknown command line argument 'hidden'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\n      compilation aborted for ..\/scipy\/linalg\/src\/det.f (code 1)\r\n      [431\/1622] Compiling Fortran object scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_lu.f.o\r\n      FAILED: scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_lu.f.o\r\n      \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin\/ifx -Iscipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/linalg -I..\/scipy\/linalg -I..\/..\/..\/..\/..\/..\/..\/..\/tmp\/pip-build-env-4s8_r9mu\/overlay\/lib\/python3.11\/site-packages\/numpy\/core\/include -I..\/..\/..\/..\/..\/..\/..\/..\/tmp\/pip-build-env-4s8_r9mu\/overlay\/lib\/python3.11\/site-packages\/numpy\/f2py\/src -Iscipy\/lib_fortranobject.a.p -I\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/latest\/lib\/pkgconfig\/..\/..\/include -I\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/latest\/lib\/pkgconfig\/..\/..\/linux\/compiler\/include -I\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/include\/python3.11 -fvisibility=hidden -DNDEBUG -D_FILE_OFFSET_BITS=64 -warn general -warn truncated_source -stand=none -O3 -fPIC -module scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p -gen-dep=scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_lu.f.o -gen-depformat=make -o scipy\/linalg\/_flinalg.cpython-311-x86_64-linux-gnu.so.p\/src_lu.f.o -c ..\/scipy\/linalg\/src\/lu.f\r\n      xfortcom: Unknown command line argument '-fvisibility'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\n      xfortcom: Did you mean '--ddg-simplify'?\r\n      xfortcom: Unknown command line argument 'hidden'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\n      compilation aborted for ..\/scipy\/linalg\/src\/lu.f (code 1)\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n>>> import sys, numpy; print(numpy.__version__, sys.version_info); numpy.show_config()\r\n1.26.2 sys.version_info(major=3, minor=11, micro=6, releaselevel='final', serial=0)\r\nblas_armpl_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib\/intel64']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/include', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib']\r\nblas_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib\/intel64']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/include', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib']\r\nlapack_armpl_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib\/intel64']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/include', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib']\r\nlapack_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib\/intel64']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/include', '\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/2022.2.1\/lib']\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2,AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\r\n    not found = AVX512_KNL\r\n```\r\n","comments":["Please note that I'm not 100% sure that the error message I've pasted is the root cause for the failure. The way meson builds, it dumps everything on the screen on a failure without saving anything to a log file, and the last error message (from the bottom) may not be what triggered the failure. If someone can tell me how to save all the output in a log, I'll happily upload that here.","Hmm, I just gave it a try with the latest OneAPI (2024.0.0) and Meson releases with:\r\n```\r\nCC=icx CXX=icx FC=ifx python dev.py build -C-Dblas=mkl-sdl -C-Dlapack=mkl-sdl\r\n```\r\nand that builds fine for me. OneAPI 2022 is very buggy, can you try with a newer version?","Unfortunately, I can't try a different oneapi, because that's the only one installed with MKL on our cluster (beyond my control). I tried just now your `dev.py` route, and it fails with similar unknown options for fortran code:\r\n```shell\r\n[455\/1628] Compiling Fortran object scipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__dpropack-f2pywrappers.f.o\r\nFAILED: scipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__dpropack-f2pywrappers.f.o\r\nifx -Iscipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/sparse\/linalg\/_propack -I..\/scipy\/sparse\/linalg\/_propack -I..\/..\/..\/python\/3.11.6\/lib\/python3.11\/site-packages\/numpy\/core\/include -I..\/..\/..\/python\/3.11.6\/lib\/python3.11\/site-packages\/numpy\/f2py\/src -Iscipy\/lib_fortranobject.a.p -Iscipy\/sparse\/linalg\/_propack\/liblib__dpropack.a.p -I\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/latest\/lib\/pkgconfig\/..\/..\/include -I\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/include\/python3.11 -fvisibility=hidden -D_FILE_OFFSET_BITS=64 -warn general -warn truncated_source -stand=none -g -traceback -O2 -g -fPIC -module scipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p -gen-dep=scipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__dpropack-f2pywrappers.f.o -gen-depformat=make -o scipy\/sparse\/linalg\/_propack\/_dpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__dpropack-f2pywrappers.f.o -c scipy\/sparse\/linalg\/_propack\/_dpropack-f2pywrappers.f\r\nxfortcom: Unknown command line argument '-fvisibility'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\nxfortcom: Did you mean '--ddg-simplify'?\r\nxfortcom: Unknown command line argument 'hidden'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\ncompilation aborted for scipy\/sparse\/linalg\/_propack\/_dpropack-f2pywrappers.f (code 1)\r\n```\r\nsimilarly,\r\n```shell\r\n[460\/1628] Compiling Fortran object scipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__cpropack-f2pywrappers.f.o\r\nFAILED: scipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__cpropack-f2pywrappers.f.o\r\nifx -Iscipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/sparse\/linalg\/_propack -I..\/scipy\/sparse\/linalg\/_propack -I..\/..\/..\/python\/3.11.6\/lib\/python3.11\/site-packages\/numpy\/core\/include -I..\/..\/..\/python\/3.11.6\/lib\/python3.11\/site-packages\/numpy\/f2py\/src -Iscipy\/lib_fortranobject.a.p -Iscipy\/sparse\/linalg\/_propack\/liblib__cpropack.a.p -I\/apps\/spack-managed\/oneapi-2022.2.1\/intel-oneapi-mkl-2022.2.1-klrcilzymbsllrr6wmepfg2cfzem5ekd\/mkl\/latest\/lib\/pkgconfig\/..\/..\/include -I\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/include\/python3.11 -fvisibility=hidden -D_FILE_OFFSET_BITS=64 -warn general -warn truncated_source -stand=none -g -traceback -O2 -g -fPIC -module scipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p -gen-dep=scipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__cpropack-f2pywrappers.f.o -gen-depformat=make -o scipy\/sparse\/linalg\/_propack\/_cpropack.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__cpropack-f2pywrappers.f.o -c scipy\/sparse\/linalg\/_propack\/_cpropack-f2pywrappers.f\r\nxfortcom: Unknown command line argument '-fvisibility'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\nxfortcom: Did you mean '--ddg-simplify'?\r\nxfortcom: Unknown command line argument 'hidden'.  Try: '\/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2022.2.1-z2sjni66fcyqcsamnoccgb7c77mn37qj\/compiler\/2022.2.1\/linux\/bin-llvm\/xfortcom --help'\r\ncompilation aborted for scipy\/sparse\/linalg\/_propack\/_cpropack-f2pywrappers.f (code 1)\r\n```\r\nIs there a way to **not** pass that `-fvisibility=hidden` option? Is that necessary?","FYI, here's a config summary that's printed out before `ninja` starts building:\r\n```shell\r\nCC=icx CXX=icx FC=ifx python3 dev.py build -C-Dblas=mkl-dynamic-lp64-seq -C-Dlapack=mkl-dynamic-lp64-seq\r\n\ud83d\udcbb  meson setup \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build --prefix \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install -Dblas=mkl-dynamic-lp64-seq\r\n-Dlapack=mkl-dynamic-lp64-seq\r\nThe Meson build system\r\nVersion: 1.3.0\r\nSource dir: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\r\nBuild dir: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build\r\nBuild type: native build\r\nProject name: SciPy\r\nProject version: 1.11.4\r\nC compiler for the host machine: icx (intel-llvm 11.3.1 \"Intel(R) oneAPI DPC++\/C++ Compiler 2022.2.1 (2022.2.1.20221020)\")\r\nC linker for the host machine: icx ld.bfd 2.35.2-24\r\nC++ compiler for the host machine: icx (intel-llvm 11.3.1 \"Intel(R) oneAPI DPC++\/C++ Compiler 2022.2.1 (2022.2.1.20221020)\")\r\nC++ linker for the host machine: icx ld.bfd 2.35.2-24\r\nCython compiler for the host machine: cython (cython 3.0.6)\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram python3 found: YES (\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/python3.11)\r\nFound pkg-config: YES (\/usr\/bin\/pkg-config) 1.7.3\r\nRun-time dependency python found: YES 3.11\r\nProgram cython found: YES (\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/cython)\r\nCompiler for C supports arguments -Wno-unused-but-set-variable: NO\r\nCompiler for C supports arguments -Wno-unused-function: NO\r\nCompiler for C supports arguments -Wno-conversion: NO\r\nCompiler for C supports arguments -Wno-misleading-indentation: NO\r\nLibrary m found: YES\r\nFortran compiler for the host machine: ifx (intel-llvm 2022.2.1 \"ifx (IFORT) 2022.2.1 20221020\")\r\nFortran linker for the host machine: ifx ld.bfd 2.35.2-24\r\nCompiler for Fortran supports arguments -Wno-conversion: NO\r\nChecking if \"-Wl,--version-script\" : links: YES\r\nProgram pythran found: YES (\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/pythran)\r\nFound CMake: \/usr\/bin\/cmake (3.20.2)\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nRun-time dependency xsimd found: NO (tried pkgconfig and cmake)\r\nRun-time dependency threads found: YES\r\nLibrary npymath found: YES\r\nLibrary npyrandom found: YES\r\npybind11-config found: YES (\/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/pybind11-config) 2.11.1\r\nRun-time dependency pybind11 found: YES 2.11.1\r\nRun-time dependency mkl-dynamic-lp64-seq found: YES 2022.2\r\nDependency mkl-dynamic-lp64-seq found: YES 2022.2 (cached)\r\nCompiler for C supports arguments -Wno-maybe-uninitialized: NO\r\nCompiler for C supports arguments -Wno-discarded-qualifiers: NO\r\nCompiler for C supports arguments -Wno-empty-body: NO\r\nCompiler for C supports arguments -Wno-implicit-function-declaration: NO\r\nCompiler for C supports arguments -Wno-parentheses: NO\r\nCompiler for C supports arguments -Wno-switch: NO\r\nCompiler for C supports arguments -Wno-unused-label: NO\r\nCompiler for C supports arguments -Wno-unused-variable: NO\r\nCompiler for C supports arguments -Wno-incompatible-pointer-types: NO\r\nCompiler for C++ supports arguments -Wno-cpp: NO\r\nCompiler for C++ supports arguments -Wno-deprecated-declarations: NO\r\nCompiler for C++ supports arguments -Wno-class-memaccess: NO\r\nCompiler for C++ supports arguments -Wno-format-truncation: NO\r\nCompiler for C++ supports arguments -Wno-non-virtual-dtor: NO\r\nCompiler for C++ supports arguments -Wno-sign-compare: NO\r\nCompiler for C++ supports arguments -Wno-switch: NO\r\nCompiler for C++ supports arguments -Wno-terminate: NO\r\nCompiler for C++ supports arguments -Wno-unused-but-set-variable: NO\r\nCompiler for C++ supports arguments -Wno-unused-function: NO\r\nCompiler for C++ supports arguments -Wno-unused-local-typedefs: NO\r\nCompiler for C++ supports arguments -Wno-unused-variable: NO\r\nCompiler for C++ supports arguments -Wno-int-in-bool-context: NO\r\nCompiler for Fortran supports arguments -Wno-argument-mismatch: NO\r\nCompiler for Fortran supports arguments -Wno-conversion: NO (cached)\r\nCompiler for Fortran supports arguments -Wno-intrinsic-shadow: NO\r\nCompiler for Fortran supports arguments -Wno-maybe-uninitialized: NO\r\nCompiler for Fortran supports arguments -Wno-surprising: NO\r\nCompiler for Fortran supports arguments -Wno-uninitialized: NO\r\nCompiler for Fortran supports arguments -Wno-unused-dummy-argument: NO\r\nCompiler for Fortran supports arguments -Wno-unused-label: NO\r\nCompiler for Fortran supports arguments -Wno-unused-variable: NO\r\nCompiler for Fortran supports arguments -Wno-tabs: NO\r\nChecking if \"Check atomic builtins without -latomic\" : links: YES\r\nConfiguring __config__.py using configuration\r\nChecking for function \"open_memstream\" : NO\r\nConfiguring messagestream_config.h using configuration\r\nCompiler for Fortran supports arguments -w: YES\r\nChecking for size of \"void*\" : 8\r\nCompiler for Fortran supports arguments -w: YES (cached)\r\nChecking for size of \"void*\" : 8 (cached)\r\nBuild targets in project: 257\r\n\r\nSciPy 1.11.4\r\n\r\n  User defined options\r\n    prefix: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\r\n    blas  : mkl-dynamic-lp64-seq\r\n    lapack: mkl-dynamic-lp64-seq\r\n\r\nFound ninja-1.11.1.git.kitware.jobserver-1 at \/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/ninja\r\nMeson build setup OK\r\n```","> Is there a way to **not** pass that `-fvisibility=hidden` option? Is that necessary?\r\n\r\nIt's not necessary for Spack. The symbol hiding is needed to avoid conflicts for wheels that aren't all built with a consistent toolchain. The flag is added by Meson, not by SciPy. I believe removing these lines from your Meson install will be enough: https:\/\/github.com\/mesonbuild\/meson\/blob\/ebf5757c59d9bceb953dc531c757e07dcf83c081\/mesonbuild\/modules\/python.py#L235-L237\r\n\r\n> `Run-time dependency mkl-dynamic-lp64-seq found: YES 2022.2`\r\n\r\nThis reminded me that MKL's layered libraries are completely broken when used from Python extensions until 2023.0. You can replace `mkl-dynamic-lp64-seq` with `mkl-sdl` to use the Single Dynamic Library, that should work better.","I wish I could build with a different oneapi, and I've submitted a ticket to the admins to install a later version. Meanwhile, I'll have to do with what's installed, unfortunately.\r\n\r\nI followed your advice about commenting out the [lines](https:\/\/github.com\/mesonbuild\/meson\/blob\/ebf5757c59d9bceb953dc531c757e07dcf83c081\/mesonbuild\/modules\/python.py#L235-L237) you mentioned, and I could compile successfully. However, pretty much every test in `python dev.py test` failed with a similar error, e.g.:\r\n```shell\r\n_______________________________________ ERROR collecting build-install\/lib\/python3.11\/site-packages\/scipy\/optimize\/tests\/test_trustregion.py _______________________________________\r\nImportError while importing test module '\/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/optimize\/tests\/test_trustregion.py'.\r\nHint: make sure your test modules\/packages have valid Python names.\r\nTraceback:\r\n..\/..\/..\/..\/..\/..\/python\/3.11.6\/lib\/python3.11\/importlib\/__init__.py:126: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nscipy\/optimize\/__init__.py:409: in <module>\r\n    from ._optimize import *\r\nscipy\/optimize\/_optimize.py:34: in <module>\r\n    from scipy.sparse.linalg import LinearOperator\r\nscipy\/sparse\/__init__.py:274: in <module>\r\n    from ._csr import *\r\nscipy\/sparse\/_csr.py:11: in <module>\r\n    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\r\nE   ImportError: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/sparse\/_sparsetools.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZTISt9bad_alloc\r\n```\r\nIs this because of the lines I commented out, or is this unrelated?","It's unrelated. It looks like an issue in oneAPI packaging. It has a very weird new requirement (in 2024 I believe, 2023.0\/2 didn't have that IIRC) to have `g++` installed on your system, or the Linux installer complains. I really don't understand how they built that new oneAPI version. I've seen the same issue, but I'm on an unsupported Linux flavor (Arch Linux), so I chalked it up to that and didn't dig into it further yet.","Hmm, I do have `g++` on the same system. I just rebuilt with oneapi 2023.1.0 (the latest the admins were able to install), and I still get the errors as above:\r\n```shell\r\n_______________________________________ ERROR collecting build-install\/lib\/python3.11\/site-packages\/scipy\/optimize\/tests\/test_linesearch.py ________________________________________\r\nImportError while importing test module '\/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/optimize\/tests\/test_linesearch.py'.\r\nHint: make sure your test modules\/packages have valid Python names.\r\nTraceback:\r\n..\/..\/..\/..\/..\/..\/python\/3.11.6\/lib\/python3.11\/importlib\/__init__.py:126: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nscipy\/optimize\/__init__.py:409: in <module>\r\n    from ._optimize import *\r\nscipy\/optimize\/_optimize.py:34: in <module>\r\n    from scipy.sparse.linalg import LinearOperator\r\nscipy\/sparse\/__init__.py:274: in <module>\r\n    from ._csr import *\r\nscipy\/sparse\/_csr.py:11: in <module>\r\n    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\r\nE   ImportError: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/sparse\/_sparsetools.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZTISt9bad_alloc\r\n```","As for building against `mkl-sdl`, that doesn't seem to exist on our system. At least there are no `pkg-config` files for them:\r\n```shell\r\n$ ls \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-mkl-2022.2.1-ifnryw36ggh5z5fxmdn2w3n5syxhvnr5\/mkl\/2022.2.1\/lib\/pkgconfig\/\r\nmkl-dynamic-ilp64-iomp.pc  mkl-dynamic-lp64-iomp.pc  mkl-static-ilp64-iomp.pc  mkl-static-lp64-iomp.pc\r\nmkl-dynamic-ilp64-seq.pc   mkl-dynamic-lp64-seq.pc   mkl-static-ilp64-seq.pc   mkl-static-lp64-seq.pc\r\n```\r\nSo I need to build with one of the libraries we do have.","I rebuilt with oneapi 2023.1.0, and the good thing is that the build was successful even without removing the [lines](https:\/\/github.com\/mesonbuild\/meson\/blob\/ebf5757c59d9bceb953dc531c757e07dcf83c081\/mesonbuild\/modules\/python.py#L235-L237) in `meson`. However, `python3 dev.py test` still failed with these errors:\r\n```shell\r\n____________________________________________ ERROR collecting build-install\/lib\/python3.11\/site-packages\/scipy\/stats\/tests\/test_fit.py _____________________________________________\r\nImportError while importing test module '\/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/stats\/tests\/test_fit.py'.\r\nHint: make sure your test modules\/packages have valid Python names.\r\nTraceback:\r\n..\/..\/..\/..\/..\/..\/python\/3.11.6\/lib\/python3.11\/importlib\/__init__.py:126: in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nscipy\/stats\/__init__.py:608: in <module>\r\n    from ._stats_py import *\r\nscipy\/stats\/_stats_py.py:39: in <module>\r\n    from scipy.spatial.distance import cdist\r\nscipy\/spatial\/__init__.py:110: in <module>\r\n    from ._kdtree import *\r\nscipy\/spatial\/_kdtree.py:4: in <module>\r\n    from ._ckdtree import cKDTree, cKDTreeNode\r\nE   ImportError: \/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/spatial\/_ckdtree.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZTISt11logic_error\r\n```","Looks like for C++ code, a standard library link goes missing perhaps? When building with GCC I've got this:\r\n```\r\n$ ldd build\/scipy\/spatial\/_ckdtree.cpython-310-x86_64-linux-gnu.so\r\n        linux-vdso.so.1 (0x00007ffc93781000)\r\n        libstdc++.so.6 => \/home\/rgommers\/mambaforge\/envs\/scipy-dev\/lib\/libstdc++.so.6 (0x00007fab356a6000)\r\n        libm.so.6 => \/usr\/lib\/libm.so.6 (0x00007fab35596000)\r\n        libgcc_s.so.1 => \/home\/rgommers\/mambaforge\/envs\/scipy-dev\/lib\/libgcc_s.so.1 (0x00007fab3557d000)\r\n        libc.so.6 => \/usr\/lib\/libc.so.6 (0x00007fab3539b000)\r\n        \/usr\/lib64\/ld-linux-x86-64.so.2 (0x00007fab35904000)\r\n```","And this is what I get:\r\n```shell\r\n$ ldd build\/scipy\/spatial\/_ckdtree.cpython-311-x86_64-linux-gnu.so\r\n\tlinux-vdso.so.1 (0x00007ffe3f144000)\r\n\tlibsvml.so => \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2023.1.0-sb753366rvywq75zeg4ml5k5c72xgj72\/compiler\/2023.1.0\/linux\/compiler\/lib\/intel64_lin\/libsvml.so (0x000014fa0086f000)\r\n\tlibimf.so => \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2023.1.0-sb753366rvywq75zeg4ml5k5c72xgj72\/compiler\/2023.1.0\/linux\/compiler\/lib\/intel64_lin\/libimf.so (0x000014fa00478000)\r\n\tlibgcc_s.so.1 => \/usr\/lib64\/libgcc_s.so.1 (0x000014fa0045d000)\r\n\tlibintlc.so.5 => \/apps\/spack-managed\/gcc-11.3.1\/intel-oneapi-compilers-2023.1.0-sb753366rvywq75zeg4ml5k5c72xgj72\/compiler\/2023.1.0\/linux\/compiler\/lib\/intel64_lin\/libintlc.so.5 (0x000014fa003df000)\r\n\tlibc.so.6 => \/usr\/lib64\/libc.so.6 (0x000014fa001d6000)\r\n\t\/lib64\/ld-linux-x86-64.so.2 (0x000014fa01fd3000)\r\n```\r\nLooks like all the library links are resolved?","By the way, during the build stage I am getting a lot of messages such as\r\n```shell\r\nperformance hint: scipy\/special\/cython_special.pyx:3471:6: Exception check on 'yve' will always require the GIL to be acquired. Declare the function as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\r\n```\r\nCould this be related? I know it's not an error, but still... I didn't know there was an option to **not** acquire the GIL.","FYI, I'm building now with Intel oneapi 2023.1, also MKL 2023.1.","> Could this be related? I know it's not an error, but still... I didn't know there was an option to not acquire the GIL.\r\n\r\nThat's not related, but I'll see about resolving these - they've been bothering me for a little while.\r\n\r\n> Looks like all the library links are resolved?\r\n\r\nIt does, but none of those are a C++ standard library. So what continues to puzzle me is how that's supposed to work. Something should be linked, but it isn't. Could be a Meson bug in its OneAPI support perhaps, but it'd be helpful to know what this is supposed to look like. @oleksandr-pavlyk I hope I can bother you with this one - could you please tell us what the linkage to a C++ standard library is supposed to look like here on Linux? Should it link to the system's `libstdc++`, or is there a OneAPI compiler runtime that is missing from the `ldd` output above, or ...?","I am afraid I won't be able to look into this until after returning from holiday travels @rgommers . Answering your question though, oneAPI compilers build on GNU toolchain, so `ICPX` should be linking to toolchain's `libstdc++`.\r\n\r\nI would return Jan. 8, and if this is not resolved by then, I would like to reproduce it and provide a better info.","Thanks @oleksandr-pavlyk, that info is already very helpful. Enjoy your holiday!","Just pinging this thread again, since the issue has not been resolved. As before, I'm happy to perform any tests with intel oneapi 2023.1.","Regarding the original fvisibility issue, the discussion starting here is relevant: https:\/\/github.com\/mesonbuild\/meson\/pull\/10909#issuecomment-1277779755","The culprit was in setting `CXX=icx`. `icx` is compiler for C language (like `clang`). Please use `CXX=icpx` (like `clang++`) instead. \r\n\r\nHere are steps I performed, in more details:\r\n\r\n1. Create `icx_for_scipy.cfg` and `ifx_for_scipy.cfg` configuration files with the following content:\r\n\r\n```\r\n$ cat icx_for_scipy.cfg\r\n-fno-fast-math\r\n```\r\n\r\n```\r\n$ cat ifx_for_scipy.cfg\r\n-ffp-model=strict\r\n```\r\n\r\nIntel LLVM compiler use `fast-math` floating point mode by default, while SciPy requires strict IEEE-754 compliance, hence we must be setting these options for every invocation of the compiler. Using configuration files is the quickest way to accomplish that.\r\n\r\n2.  Run the build\r\n\r\n```bash\r\nCC=icx CXX=icpx FC=ifx ICXCFG=$(pwd)\/icx_for_scipy.cfg ICPXCFG=$(pwd)\/\/icx_for_scipy.cfg IFXCFG=$(pwd)\/ifx_for_scipy.cfg python dev.py build -C-Dblas=mkl-sdl -C-Dlapack=mkl-sdl\r\n```\r\n\r\nNote use of `CXX=icpx`.\r\n\r\n\r\nAfter build completes, `python dev.py test` discovers all test files correctly, and runs them without errors about unresolved symbol. Moreover, individual import of `scipy.sparse` works as well:\r\n\r\n```\r\n(build_scipy) opavlyk@opavlyk-mobl:~\/repos\/scipy\/temp_wd$ PYTHONPATH=~\/repos\/scipy\/build-install\/lib\/python3.11\/site-packages:$PYTHONPATH python -c \"import scipy.sparse as sp; print(sp.eye(4))\"\r\n  (0, 0)        1.0\r\n  (1, 1)        1.0\r\n  (2, 2)        1.0\r\n  (3, 3)        1.0\r\n```","I should also add that the configuration files may come in handy to inform IntelLLVM compiler about non-standard location of GNU toolchain, i.e. https:\/\/github.com\/IntelPython\/dpctl\/blob\/master\/conda-recipe\/build.sh#L7","@oleksandr-pavlyk Thanks for the detailed instructions, now `python dev.py test` proceeds further than before. There are no unresolved symbols. However, it has been stuck at this stage for the past couple of hours\r\n```\r\nscipy\/linalg\/tests\/test_matfuncs.py .F.FF\r\n```\r\nThe numbers on the right say that tests are 25% done. Should I be worried? How long are the tests supposed to take? I'm running this on an 80-core Intel Xeon 8380 node, so I'm thinking that something is wrong, and testing should not take this long.","The test itself is pretty fast on my machine (Laptop with Gen-12 Intel Core i7 CPU):\r\n\r\n```\r\n$ PYTHONPATH=~\/repos\/scipy\/build-install\/lib\/python3.11\/site-packages:$PYTHONPATH \/usr\/bin\/time -f \"%E %M %P\" python -m pytest --pyargs scipy.linalg.test\r\ns.test_matfuncs\r\n============================================================================================== test session starts ==============================================================================================\r\nplatform linux -- Python 3.11.7, pytest-7.4.4, pluggy-1.3.0\r\nrootdir: \/home\/opavlyk\/repos\/scipy\r\nconfigfile: pytest.ini\r\ncollected 63 items\r\n\r\n..\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py ...........................................X..........s........                                                         [100%]\r\n\r\n=================================================================================== 61 passed, 1 skipped, 1 xpassed in 1.23s ====================================================================================\r\n0:01.49 148612 190%\r\n```\r\n\r\nSo it took 2 seconds, and effectively used about 2 cores. Try setting `PYTHONPATH` and executing `python -m pytest -vv --pyargs scipy.linalg.test` to get an idea for which test it gets stuck on.","Hmm, for me, this is where it gets stuck:\r\n```\r\n$ PYTHONPATH=\/work2\/noaa\/co2\/sbasu\/packages\/sources\/scipy-1.11.4\/build-install\/lib\/python3.11\/site-packages:$PYTHONPATH time -f \"%E %M %P\" python -m pytest -vv --pyargs scipy.linalg.tests.test_matfuncs\r\n=============================================================================== test session starts ================================================================================\r\nplatform linux -- Python 3.11.6, pytest-7.4.3, pluggy-1.3.0 -- \/work2\/noaa\/co2\/sbasu\/packages\/python\/3.11.6\/bin\/python\r\ncachedir: .pytest_cache\r\nrootdir: \/work2\/noaa\/co2\/sbasu\/packages\/sources\r\ncollected 63 items\r\n\r\nscipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py::TestSignM::test_nils PASSED                                                     [  1%]\r\nscipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py::TestSignM::test_defective1 FAILED                                               [  3%]\r\nscipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py::TestSignM::test_defective2 PASSED                                               [  4%]\r\nscipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py::TestSignM::test_defective3 FAILED                                               [  6%]\r\nscipy-1.11.4\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py::TestLogM::test_nils\r\n```\r\nAfter 20 minutes, I hit `ctrl+c` to abort.","It is of course important to understand where it hangs for you. The `scipy\/linalg\/_matfuncs.py` is a Python script file which you can add print statements to, and rerun test file (making  sure to use `python -m pytest -s` to ensure that `pytest` dues not capture your debug printouts). \r\n\r\nYou could also use `MKL_VERBOSE` to gain some insight into call to MKL library. Here is what I see running that particular test:\r\n\r\n<details>\r\n<summary>\r\n<code>MKL_VERBOSE=1<\/code> output for successful run of  <code>test_matfuncs::TestLogM::test_nils<\/code>\r\n<\/summary>\r\n\r\n```\r\n(build_scipy) opavlyk@opavlyk-mobl:~\/repos\/scipy\/temp_wd$ MKL_VERBOSE=1 PYTHONPATH=~\/repos\/scipy\/build-install\/lib\/python3.11\/site-packages:$PYTHONPATH python -m pytest -s --pyargs scipy.linalg.tests.test_matfuncs::TestLogM::test_nils\r\n============================================================================================== test session starts ==============================================================================================\r\nplatform linux -- Python 3.11.7, pytest-7.4.4, pluggy-1.3.0\r\nrootdir: \/home\/opavlyk\/repos\/scipy\r\nconfigfile: pytest.ini\r\ncollected 1 item\r\n\r\n..\/build-install\/lib\/python3.11\/site-packages\/scipy\/linalg\/tests\/test_matfuncs.py MKL_VERBOSE oneMKL 2024.0 Product build 20231011 for Intel(R) 64 architecture Intel(R) Advanced Vector Extensions 512 (Intel(R) AVX-512) with support of Intel(R) Deep Learning Boost (Intel(R) DL Boost), EVEX-encoded AES and Carry-Less Multiplication Quadword instructions, Lnx 3.00GHz lp64 intel_thread\r\nMKL_VERBOSE DSBEVD(N,U,6,1,0x55af25afa240,2,0x55af25af5c10,0x55af252ee830,1,0x55af25afa2b0,12,0x55af25338880,1,0) 84.70us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af259fe060,7,0x55af257e2160,7,0) 270.45us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af259fb830,7,0x55af259fe060,7,0) 6.53us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af257e2160,7,0x55af259fb830,7,0) 2.42us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af259fe060,7,0x55af257e2160,7,0) 1.52us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af259fb830,7,0x55af259fe060,7,0) 2.71us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZTRTRS(L,T,N,7,7,0x55af257e2160,7,0x55af259fb830,7,0) 3.46us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 24.98us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 13.90us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 2.43us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153ed0,0x55af25f8ed10,7,0x55af25f8ed10,7,0x7fff6c153ea0,0x55af25f8f020,7) 27.68us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153ed0,0x55af25f8f020,7,0x55af25f8f020,7,0x7fff6c153ea0,0x55af25f8f330,7) 2.37us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153ed0,0x55af25f8f330,7,0x55af25f8f020,7,0x7fff6c153ea0,0x55af25f8f640,7) 1.01us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afde00,1) 554ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afde00,1) 484ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 959ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 705ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 750ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 674ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 882ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 727ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 858ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 3.15us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 851ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 637ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 836ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 558ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 527ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 743ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 692ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 839ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 695ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 893ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 589ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 661ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 433ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 488ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153ed0,0x55af25f8f330,7,0x55af25f8f330,7,0x7fff6c153ea0,0x55af25f8f950,7) 3.16us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afde00,1) 739ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 700ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 558ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 481ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 564ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 722ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 561ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 676ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 1.13us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 605ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 975ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 705ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 873ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 728ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 898ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 570ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 1.14us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 435ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 523ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153ed0,0x55af25f8f640,7,0x55af25f8f330,7,0x7fff6c153ea0,0x55af25f8f950,7) 3.11us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afde00,1) 604ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DSCAL(49,0x7fff6c153d60,0x55af24cb1cd0,1) 10.77us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 760ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 562ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 498ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 430ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 464ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 428ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 463ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 430ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 458ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 556ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 738ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 468ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 843ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 695ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DGEMV(C,7,7,0x7fff6c153d30,0x55af24cb1cd0,7,0x55af25afdd80,1,0x7fff6c153d00,0x55af25afe380,1) 739ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE DCOPY(7,0x55af25afe380,1,0x55af25afdd80,1) 673ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE IDAMAX(7,0x55af25afdd80,1) 794ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZCOPY(49,0x55af25f8f020,1,0x55af25f8e0f0,1) 8.61us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZSCAL(49,0x7fff6c153e50,0x55af25f8e0f0,1) 6.08us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e70,0x55af25f8f330,1,0x55af25f8e0f0,1) 8.29us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e90,0x55af25f8f640,1,0x55af25f8e0f0,1) 661ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZCOPY(49,0x55af25f8f020,1,0x55af25f8e410,1) 901ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZSCAL(49,0x7fff6c153de0,0x55af25f8e410,1) 1.03us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e00,0x55af25f8f330,1,0x55af25f8e410,1) 560ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e20,0x55af25f8f640,1,0x55af25f8e410,1) 942ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZCOPY(49,0x55af25f8f020,1,0x55af25f8fc70,1) 921ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZSCAL(49,0x7fff6c153e40,0x55af25f8fc70,1) 734ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e60,0x55af25f8f330,1,0x55af25f8fc70,1) 704ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e80,0x55af25f8f640,1,0x55af25f8fc70,1) 730ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZSCAL(49,0x7fff6c153df0,0x55af25f8f020,1) 595ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e10,0x55af25f8f330,1,0x55af25f8f020,1) 716ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153e30,0x55af25f8f640,1,0x55af25f8f020,1) 673ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153da0,0x55af25f8e0f0,7,0x55af25f8f640,7,0x7fff6c153da0,0x55af25f8f020,7) 22.65us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153da0,0x55af25f8f020,7,0x55af25f8ed10,7,0x7fff6c153d70,0x55af25f8e0f0,7) 22.04us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGEMM(N,N,7,7,7,0x7fff6c153da0,0x55af25f8fc70,7,0x55af25f8f640,7,0x7fff6c153da0,0x55af25f8e410,7) 1.72us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZAXPY(49,0x7fff6c153ea0,0x55af25f8e0f0,1,0x55af25f8e410,1) 720ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGETRF(7,7,0x55af25f8e410,7,0x55af24e10f70,0) 62.27us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZGETRS(T,7,7,0x55af25f8e410,7,0x55af24e10f70,0x55af25f8fc70,7,0) 12.76us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZDSCAL(49,0x7fff6c153eb0,0x55af25f8fc70,1) 18.63us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZLANGE(i,7,7,0x55af25ac8340,7,0x55af25a28980) 19.41us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\nMKL_VERBOSE ZLANGE(i,7,7,0x55af25ac2020,7,0x55af25a28980) 774ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:4\r\n.\r\n\r\n=============================================================================================== 1 passed in 0.70s ===============================================================================================\r\n``` \r\n<\/details>","So, what is hanging is the call `logm(m, disp=True)` inside `class TestLogM`, method `test_nils`. A print statement before that `logm` succeeds, while a print statement after that is never executed. If I set `MKL_VERBOSE=1`, I get never-ending printouts like the following:\r\n```\r\nMKL_VERBOSE DDOT(7,0x3482328,2,0x3482320,2) 654ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x35cc960,2,0x347b5e0,7,0x14e586c88228,0x35b10e0,2) 3.22us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x35cc960,2,0x347b5e0,7,0x14e586c88228,0x35b10e0,2) 1.20us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x2b8c170,2,0x347b5e0,7,0x14e586c88228,0x35cc960,2) 940ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,T,2,7,7,0x14e586c88238,0x2b8c170,2,0x347b5e0,7,0x14e586c88228,0x35b10e0,2) 2.01us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,T,2,7,7,0x14e586c88238,0x35cc960,2,0x347b5e0,7,0x14e586c88228,0x35b10e0,2) 1.10us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,T,2,7,7,0x14e586c88238,0x35a60b0,2,0x347b5e0,7,0x14e586c88228,0x35cc960,2) 968ns CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x35b10e0,2,0x347b5e0,7,0x14e586c88228,0x35a60b0,2) 2.13us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x35b10e0,2,0x347b5e0,7,0x14e586c88228,0x35a60b0,2) 1.10us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\nMKL_VERBOSE ZGEMM(N,N,2,7,7,0x14e586c88238,0x35cc960,2,0x347b5e0,7,0x14e586c88228,0x35b10e0,2) 1.05us CNR:OFF Dyn:1 FastMM:1 TID:0  NThr:1\r\n```\r\nThese printouts never stop!","Pinging this thread again, because the problem is still there. Does the above mean that for now, I'm out of luck trying to build `scipy` with MKL?","@AgilentGCMS Please make sure to use `icx_for_scipy.cfg` and `ifx_for_scipy.cfg` to set strict floating point mode. If that was done, it also remains to dig deeper into the algorithm with print statements to identify which convergence condition is not being met. \r\n\r\nI am afraid bugs may need to be filed based on your findings. ","@oleksandr-pavlyk thanks for circling back to this. `logm` does indeed have `while` loops that aren't protected by a `maxiter` condition:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/baefc12bf7e84be3ac78b190fb7d741dea7ea008\/scipy\/linalg\/_matfuncs_inv_ssq.py#L371-L417\r\n\r\nSo that could well be the cause of a hang, especially if strict floating point mode isn't enabled.","> @AgilentGCMS Please make sure to use `icx_for_scipy.cfg` and `ifx_for_scipy.cfg` to set strict floating point mode. If that was done, it also remains to dig deeper into the algorithm with print statements to identify which convergence condition is not being met.\r\n> \r\n> I am afraid bugs may need to be filed based on your findings.\r\n\r\n@oleksandr-pavlyk Based on your suggestion I I have \r\n```\r\n$ cat icx_for_scipy.cfg\r\n-fno-fast-math\r\n$ cat ifx_for_scipy.cfg\r\n-ffp-model=strict\r\n```\r\n\r\nAnd I am using (at least trying to use) those by prepending `CC=icx CXX=icpx FC=ifx ICXCFG=$(pwd)\/icx_for_scipy.cfg ICPXCFG=$(pwd)\/icx_for_scipy.cfg IFXCFG=$(pwd)\/ifx_for_scipy.cfg` before `python3 dev.py`. However, while the build finishes successfully, `scipy.test()` still hangs at `::TestLogM::test_nils`."],"labels":["Build issues","upstream bug","Meson"]},{"title":"ENH: Wiener filter padding","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nFollow-up on my comment under https:\/\/github.com\/scipy\/scipy\/issues\/11415#issuecomment-1851748997.\r\n\r\n#### What does this implement\/fix?\r\n\r\nCurrent implementation of Wiener filter uses zero padding. This creates undershot local mean around edges and from this lower local variance. When noise is not provided, the noise estimated from the input image is under-shot.\r\n\r\n\r\n#### Additional information\r\n\r\nLet me show the difference on following subsampled version of grayscale Lena.\r\n\r\n![lena_gray](https:\/\/github.com\/scipy\/scipy\/assets\/24460070\/97e2fee0-39be-4fb9-9728-3b26804d0c96)\r\n\r\nI process it with `scipy.signal.wiener()`, both old and new version:\r\n\r\n```python\r\nfx = scipy.signal.wiener(x, (3, 3))\r\n```\r\n\r\nThe results are as follows:\r\n\r\n![lena_wiener_original](https:\/\/github.com\/scipy\/scipy\/assets\/24460070\/6f577e5e-25ef-429e-9baa-4c9eb1b32dc6)\r\nOriginal (zero padding)\r\n\r\n![lena_wiener_symmetric](https:\/\/github.com\/scipy\/scipy\/assets\/24460070\/b3dd7323-2d1e-44b9-bfb9-977408e9ec56)\r\nProposed (symmetric padding)\r\n\r\nNote following:\r\n\r\n- old version has a \"frame\" around its edges, caused by zero padding\r\n- new version has deeper shadows\/contours, because the estimated noise is higher.\r\n","comments":["This breaks backwards compatibility so unless this is a bug this will probably need implementing in a different way","I changed the interface. Now, by default, it behaves the same as before.\r\n\r\n```python\r\nfx = scipy.signal.wiener(x, (3, 3))\r\nfx = scipy.signal.wiener(x, (3, 3), 0.2)\r\n```\r\n\r\nWhat is new is that now you can specify the padding method (and fill value).\r\nUser can choose to remove the artifact by using symmetric boundary padding.\r\n\r\n```python\r\nfx = scipy.signal.wiener(x, (3, 3), boundary='symm')\r\n```\r\n","This will also need test coverage for the new modes added"],"labels":["enhancement","scipy.signal"]},{"title":"DEP: signal: raise error using medfilt and order_filter with float128 and object dtypes","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\nfollow up to #18341\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\nIt has been two releases since float128 and object types were deprecated to deduplicate code, should be fine to raise an error now.\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n","comments":["I'm also a bit confused since gh-18341 said \"The goal here is to reduce duplication between ndimage and signal, and get rid of https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/signal\/_lfilter.c.in entirely (700 LOC of fairly obscure C code). Cannot just remove them, so deprecate first.\"","The point of the original deprecation was that we currently have two sets of C code for doing the same thing, one in `signal` and one in `ndimage`. The signal functions were modified to use the ndimage code under the hood so signal C code could be removed as it was decided the `ndimage` code was more maintainable. However `ndimage` did not support object or float128 dtypes hence the deprecation cycle before all the `signal` C code could be removed.","There are some relevant test failures that need resolving though","Ah okay, so does all of `_lfilter.c.in` need to stay?","> Ah okay, so does all of `_lfilter.c.in` need to stay?\r\n\r\nAs far as I could tell it was still needed but I also might have just got lost the maze. Maybe @ev-br could confirm?","> Ah okay, so does all of _lfilter.c.in need to stay?\r\n\r\nIndeed. Thank you Jake for chasing down where this dead code is despite the typo in the original PR which introduced the deprecation.","Updated to use `longdouble` and removed more dead code","I think we can wait for 1.14 with this? Unless you want to get this in urgently, I think we should give it a year as usual.","Closing until we branch for 1.14","Branch has happened, reopening."],"labels":["scipy.signal","deprecated"]},{"title":"ENH: Allow Boolean `dtype` for `in2` Array in `scipy.signal.convolve2d`","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nCurrently, the `scipy.signal.convolve2d` function does not support a boolean `dtype` for the `in2` input array. This limitation restricts the function's applicability in scenarios where a simple, binary convolution operation is desired, particularly in cases where speed and memory efficiency are important.\r\n\r\nFor example, in specific binary image processing tasks or signal processing scenarios where data is inherently boolean, the inability to use boolean arrays directly leads to unnecessary conversions and complexity.\r\n\r\n### Describe the solution you'd like.\r\n\r\nI propose to extend the `scipy.signal.convolve2d` function to accept a boolean `dtype` for the `in2` array. This enhancement would not only broaden the function's utility but also potentially improve performance in scenarios where boolean operations are sufficient.\r\n\r\nIf feasible, the implementation could ensure that when a boolean array is used, the function leverages optimized computational pathways, possibly including bitwise operations, to achieve faster execution times compared to equivalent operations with numerical arrays.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nAn alternative approach is to convert boolean arrays to numerical types before convolution. However, this workaround is less efficient, both in terms of computation and memory usage. It also adds additional steps to the processing pipeline, which could be streamlined with direct support for boolean arrays.\r\n\r\nAnother possibility is the creation of a separate function specifically for boolean convolutions. However, this might lead to redundancy and increased maintenance overhead in the SciPy codebase. Integrating boolean support into the existing `convolve2d` function is a more elegant and consolidated approach.\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nI was working on an optimized version of Conway's Game of Life model (https:\/\/github.com\/projectmesa\/mesa\/pull\/1898#issuecomment-1848962861), where I encountered an error when I wanted to specify `dtype=bool` in the in2 array. Switching it to `int` resolved the error, but I thought it would be nice if a boolean data type was allowed. This way you could use `convolve2d` as a mask of which the sum is taken (in this case the number of neighbours).","comments":["Would you expect `(bool, bool) -> bool` (so there's no summing, only True\/False), or `(bool, bool) -> int64`, or ....?","Thanks for getting back!\r\n\r\nThe latter `(bool, bool) -> int`"],"labels":["enhancement","scipy.signal"]},{"title":"ENH: Option subproblem_maxiter for tr-exact, gh-12513","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\ngh-12513\r\n\r\n#### What does this implement\/fix?\r\nAvoid infinite loop when using `method=\"tr-exact\"` in `optimize.minimize`.\r\n\r\n#### Additional information\r\nI have only a superficial understanding of these methods. So to the best of my knowledge this is an appropriate fix, but someone more familiar with this code and these methods might have another suggestion.\r\n","comments":["How do I fix the linting errors?\r\n\r\nI tried:\r\n```console\r\n$ tools\/lint.py --fix --files scipy\/optimize\r\n```\r\nbut it didn't fix it. Running\r\n```console\r\nblack scipy\/optimize\r\n```\r\nchanges too many files. Running `ruff` doesn't seem to do the right thing either...","Would be good to get a review from an `optimize` regular, I think I'll bump the milestone before branching for `1.13.0`, since that special release is focused primarily on NumPy 2 support.","Sure, that sounds reasonable. I keep carrying this patch locally, and it is working well for me so far. "],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: Add Inverse Distance Weighted Interpolation","body":"#### Reference issue\r\n\r\nAddresses: https:\/\/github.com\/scipy\/scipy\/issues\/2022 and expands ungridded interpolation techniques in scipy to include a well-known interpolation technique. \r\n\r\n#### What does this implement\/fix?\r\n\r\nInverse Distance Weighted interpolator. \r\n\r\nFrom wikipedia: \r\n<img width=\"1010\" alt=\"Screenshot 2023-12-09 at 8 16 34 AM\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/6124630\/faace1a6-3ab4-47d1-b400-b1e8108e078b\">\r\n\r\nKey features of this PR: \r\n- Implements IDW interpolation using scipy's cKDTree (similar to NearestNDNeighbor)\r\n- PR leaves the option open to the user to define their own weighting function \/ kernel instead of just the reciprocal^power distances used in original IDW. \r\n\r\nSimple example below (taken from [griddata](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.interpolate.griddata.html)) with different params: \r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/6124630\/2336bdd1-4c4d-4414-af3c-dcb103641c05)\r\n\r\nNow with some noise added to each the observed values.\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/6124630\/3a98b823-a2f2-403b-aac1-56abdd057e5f)\r\n\r\n\r\n#### Additional information\r\nn\/a\r\n","comments":["Thanks @harshilkamdar can you post to the mailing list explaining what you intend to add here and the motivation","@harshilkamdar you should be able to fix some of the CI errors by adding `_inverse_distance_weighted.py` [here in `scipy\/interpolate\/meson.build`](https:\/\/github.com\/scipy\/scipy\/blob\/e35edb58c6287ea566ab65fe996e6657d30487d6\/scipy\/interpolate\/meson.build#L185) and `test_idw.py` [here in `scipy\/interpolate\/tests\/meson.build`](https:\/\/github.com\/scipy\/scipy\/blob\/e35edb58c6287ea566ab65fe996e6657d30487d6\/scipy\/interpolate\/tests\/meson.build#L1)."],"labels":["enhancement","scipy.interpolate","needs-work"]},{"title":"Query:Cython, npy_math.h and complex.h inclusion","body":"As part of the Cythonization efforts, I have been hitting the common issue of getting access to complex versions of abs, sqrt, log and so on functions. This is problematic for a few reasons, \r\n\r\n- MSVC in windows is c99 incompatible and we all know the history etc. it is what it is\r\n- npy_math is on the pedestal with discussions such as https:\/\/github.com\/numpy\/numpy\/issues\/20880 and similar discussions elsewhere hence it is only going to be less useful in the future, in my limited understanding\r\n- Cython injects guard macros after the includes and we have to play with the `CCYTHON_COMPLEX` env var and do some custom bricklaying at the Cython compilation setup (if I am understanding correctly) Here is a better discussion of it https:\/\/stackoverflow.com\/questions\/57837255\/defining-dcomplex-externally-in-cython\r\n\r\nIn our legacy code we have a few avenues of \"hacks\", namely \r\n\r\n- the types switching back and forth between `_DComplex` , `double complex` and `npy_cdouble` types in unions or casting schemes, \r\n- The imports from `npy_math` as in https:\/\/github.com\/scipy\/scipy\/blob\/fa9f13e6906e7d00510d593f7f982db30e4e4f14\/scipy\/special\/_complexstuff.pxd#L12-L24\r\n- and some other redefinition clauses of `_Dcomplex` and so on\r\n\r\nNone of them seems to be the clean way but I'd like to ask our build residents what way we should pursue to have a sustainable code? \r\n\r\nI'll ping Cython, NumPy folks too, so that we don't stray too outwards @da-woods @lysnikolaou @rgommers @seberg\r\n\r\nI would really appreciate a mini code block that should run succesfully on a win or *nix system such as \r\n\r\n\r\n```python\r\n%%cython -f\r\nimport numpy as np\r\ncimport numpy as cnp\r\n\r\n# just to make sure that things real and complex funcs can coexist\r\nfrom libc.math cimport (abs, sqrt)\r\n\r\n<insert complex cimport magic here>\r\n\r\n\r\ncdef double test_func(double complex z):\r\n    cdef double absz, sqaz\r\n    absz = cabs(z)\r\n    sqaz = sqrt(absz)\r\n    return sqaz\r\n```\r\n\r\nIf you want to make functions run with `npy_cdouble` and `npy_cabs` that's also fine. I just need some clarification which way we should be going towards. \r\n\r\nWe can also disallow MSVC but I feel that it's a bit too much of a restriction.","comments":["> MSVC in windows is c99 incompatible\r\n\r\nIt's just the complex types that are differently named, but using C99 `cabs` in C or Cython should work fine AFAIK (MSVC should be fully C99 compliant modulo the mess with types).\r\n\r\nThis is always a hairy question; I don't quite have the bandwidth to write the requested code snippet here, hopefully @lysnikolaou will be able to do so.","In the meantime I convinced myself that it won't be possible but if @lysnikolaou has the answer that's great ","I haven't played too much with complex in *real* code, so a little unclear on the issues (although I totally believe they exist!)\r\n\r\nWith recent Cython (3.0+) on Linux I just have to add `from libc.complex cimport cabs` and your code compiles for me as written on Linux with GCC. My impression is that `CYTHON_CCOMPLEX` gets detected mostly correctly, and so the `complex double` types can get passed to suitable C functions. I imagine you don't have to go that far back for that not to be true though.\r\n\r\nThe alternative Numpy `npy_cdouble` types don't look like they'll automatically work though.\r\n\r\nSo it looks like the basic problem is: you've got a bunch of types that are very close to equivalent (same information, same memory layout, but different name). And perhaps what would be useful to do is have a way of declaring \"this is a complex type\" in Cython. Maybe assume that they're all compatible with the basic C or C++ standard library memory layout (which I think is true) and let Cython do the automatic conversions between them.\r\n\r\nThat's probably something we could support better in some future version of Cython, although doesn't help now.","> With recent Cython (3.0+) on Linux I just have to add from libc.complex cimport cabs and your code compiles for me as written on Linux with GCC. \r\n\r\nYes on windows it fails depending on clang or mscv pickup by the toolchain. \r\n\r\n> Maybe assume that they're all compatible with the basic C or C++ standard library memory layout (which I think is true) and let Cython do the automatic conversions between them.\r\n\r\nThis would be indeed a great improvement. Not sure how the internals work but instead of the user is doing the conversions in various codebases, a canonical Cython machinery would be quite useful. Currently I'm basically using a double boot machine and testing in both systems Win and Linux and whichever complains I'm adjusting the code and see what the other starts complaining until both does not complain. "],"labels":["Build issues","query","C\/C++","Cython"]},{"title":"ENH: Yeo-Johnson compute LLF in log-space and adjust lambda when transformation overflow","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\n<!--Example: Closes gh-WXYZ.-->\r\n\r\nTowards #19016\r\n\r\n#### What does this implement\/fix?\r\n<!--Please explain your changes.-->\r\n\r\n1. Compute LLF in the log-space\r\n  - Code is inspired by https:\/\/github.com\/scipy\/scipy\/pull\/18852#issuecomment-1657858886\r\n  - The variance formula can be simplyfied for pure positive or pure negative data\r\n    + Remove the offset and add the absolute value sign to the denominator\r\n    + Silimar tricks is applied in Box-Cox https:\/\/github.com\/scipy\/scipy\/pull\/19604#issue-2014219660\r\n  - For mixed positive and negative data, use the exact formula of YJ tranformation\r\n\r\n2. Overflow checks\r\nAssume `xmin` and `xmax` are the minimum and maximun of input data, `ymin` and `ymax` are the minimum and maximum of input dtype. Then there are three situations:\r\n  - 0 < `xmin` < `xmax`: check if `YJ(xmax)` > `ymax`\r\n  - `xmin` < `xmax` < 0: check if `YJ(xmin)` < `ymin`\r\n  - `xmin` < 0 < `xmax`: check if `YJ(xmax)` > `ymax` and `YJ(xmin)` < `ymin`\r\n\r\nWe can simplifies as following. For x>0, I think only `lmb`>1 can cause overflow. And for x<0, only `lmb`<1 can cause overflow. Therefore, the three checks become\r\n  - `xmax` > 0 and `lmb_opt` > 1: check if `YJ(xmax)` > `ymax`\r\n  - `xmin` < 0 and `lmb_opt` < 1: check if `YJ(xmin)` < `ymin`\r\n\r\n3. Use the exact approach to compute the constrained lambda\r\n  - For x>=0: https:\/\/www.wolframalpha.com\/input?i=solve+%28%28x%2B1%29%5El-1%29%2Fl+%3D+y+for+l\r\n  - For x<0: https:\/\/www.wolframalpha.com\/input?i=solve+-%28%28-x%2B1%29%5E%282-l%29-1%29%2F%282-l%29+%3D+y+for+l\r\n\r\n#### Additional information\r\n<!--Any additional information you think is important.-->\r\n\r\n4. Underflow issue\r\n\r\nI tested the data added in #18852, and didn't found underflow.\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy import stats\r\n\r\nx = np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0])\r\n# x = np.array([2003.0, 1950.0, 1997.0, 2000.0, 2009.0,\r\n#               2009.0, 1980.0, 1999.0, 2007.0, 1991.0])\r\nscale = [1, 1e-12, 1e-32, 1e-150, 1e32, 1e200]\r\nsign = [1, -1]\r\n\r\nnp.seterr(under='raise')\r\nfor sc in scale:\r\n    for si in sign:\r\n        stats.yeojohnson(sc * si * x)\r\n```\r\n\r\nIn theory, underflow happens when x>0 and `lmd` is a very small negtive value, or x<0 and `lmd` is a very large positive value. But I failed to find a data causes underflow.","comments":["Some useful lemma from the original YJ paper [1]\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/41134380\/1ad28179-9b5d-48a2-afc4-b4c224841025)\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/41134380\/e03a7fe1-9ea6-45d6-9424-c9e74b248536)\r\n\r\n[1] [A new family of power transformations to improve normality or symmetry](https:\/\/doi.org\/10.1093\/biomet\/87.4.954)","Hi @xuefeng-xu, I just submitted a follow-up PR #19802 to the original PR #18852 that adds exact bounds for lambda to avoid under- and overflow. @mdhaber asked me to coordinate with you as there is some overlap between that PR and this one. I hope it's OK that I share some thoughts on this PR with you here.\r\n\r\nI see that this PR improves two aspects of YJ:\r\n1. Computing the log likelihood in log space\r\n2. A backtracking-style overflow protection that limits lambda when overflow is detected\r\n\r\nHere are some thoughts that came to mind when reviewing this PR:\r\n1. Computing the log likelihood in log space would in theory allow to relax the bounds on lambda further, as we don't need to reserve room in floating point's exponent space to be able to compute the variance without overflow. That said, I would still continue to reserve half of the floating point exponent space because YJ users may want to compute the variance themselves afterwards.\r\n2. This implementation removes the safety factor applied to the maximum observed input data. This is important to keep because YJ users may want to apply the fitted YJ transform to new data, which may be larger or smaller than the training data.\r\n3. Because of the backtracking-style overflow protection, I suppose that Brent's method will still generate overflow. A user that calls YJ may be surprised to see that overflow warnings are generated. While this could be avoided by contextually ignoring overflow, that would also mask other unexpected sources of overflow.\r\n4. Just as with overflow, there are (in theory) at least two places that could generate underflow: the YJ transform as a whole, and its numerator. Examples of the former are difficult to find (though I wouldn't rule it out), while underflow in latter is pretty easy to trigger. The follow-up PR #19802 addresses the former, but not the latter (because it cannot be solved with a simple interval constraint on lambda).\r\n5. The formula that inverts YJ to produce lambda could be made more robust by computing it along the lines of `lambertw(-exp(logx), k)`.\r\n\r\nWith all of the above in mind, we should choose how to proceed from here. Two options:\r\n1. Discard #19802, and further improve the bounding of lambda in this PR.\r\n2. Rescope this PR to only the log likelihood in log space feature, and relegate improved bounding of lambda to #19802.\r\n\r\nI have no strong preference on which option to choose: #19802 is more thorough on improving the bounds, but may also be more complex than the backtracking solution. On the other hand, I do expect the complexity of the backtracking solution in this PR to grow as the above and other comments are addressed.","Marking this as a draft pending agreement in gh-19016. @xuefeng-xu please coordinate with the others to agree on a plan for the two functions. I don't need to be part of the discussions, and I will most likely support whatever you all agree on. Once there is a plan, break it into pieces, and let's worth through it step by step rather with mega-PRs (to the extent that this is possible)."],"labels":["scipy.stats","enhancement"]},{"title":"ENH: as_quat() and from_quat() seams to be reverse x,y,z,w vs w,x,y,z","body":"### Is your feature request related to a problem? Please describe.\n\nI was trying to use the quaternion in scipy, and I found out that the format of the quaternion in scipy seams to be in a different order than the wiki page.\r\n\r\nOn your page:\r\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.spatial.transform.Rotation.from_quat.html\r\nYou say:\r\n\"Each row is a (possibly non-unit norm) quaternion representing an active rotation, in scalar-last (x, y, z, w) format. Each quaternion will be normalized to unit norm.\"\r\nbut the example seams to have the scalar in front?\r\n```\r\nr = R.from_quat([1, 0, 0, 0])\r\nr.as_quat()\r\narray([1., 0., 0., 0.])\r\n```\r\n\r\nAt the wiki link that you provide on your doc web page, https:\/\/en.wikipedia.org\/wiki\/Quaternions_and_spatial_rotation\r\n, it also says that the format is w, x, y, z\r\nWikipedia page says:\r\n\"We can avoid this by using four Euclidean coordinates w,\u2009x,\u2009y,\u2009z, ...\"\r\n \r\nI am using a ABB robot and the documentation also says that the quaternion is in the format w, x, y, z.\r\nsee link: https:\/\/developercenter.robotstudio.com\/api\/robotstudio\/api\/ABB.Robotics.Math.Quaternion.html\r\nq1, q2, q3, q4\r\nq1 Scalar part: q1, Vector part: q2, q3, q4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Describe the solution you'd like.\n\nTo add a configuration for the quaternion.\r\nsame as Euler angle that have the possibility to be configure as 'XYZ' or 'ZYX'\r\n```\r\nRotation.from_quat(type cls, quat, conf = 'wxyz')\r\n```\r\nWithout conf, from_quat could have by default 'xyzw', and this could be backward compatible.\r\n\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\nEnglish is not my native language, I could have made a few typo here and there.\r\nThanks and sorry\r\n","comments":["To the enhancement request: this has 4 requests now, with a seemingly working implementation previously reverted and a +0 from a maintainer; seems worth another look?\r\n\r\nx-ref gh-14548, gh-15410,  gh-15431\r\nping @nmayorov @pmla\r\ncc @alspitz @jmw182 @RTnhN\r\n\r\nTo the supposed example inconsistency: I don't have the knowledge to confirm but I would be surprised if this went unnoticed for five years","There's no inconsistency in the example. You get the same quaternion that was put in with `x=1` and the rest 0s. The scalar `w` is simply `0` in both input and output. That's a valid quaternion.\r\n\r\nWhether `w` goes at the front or the back is just a matter of convention. Wikipedia's use of `wxyz` is not an indication that the `xyzw` convention doesn't exist or is wrong.","Thanks,,\r\n\r\nIf quaternion ordering wxyz, or xyzw is just a convention, Euler angle could be XYZ or ZYX is also\r\nbut in the as_euler, from_euler we have the possibility to tell scipy the ordering, but not in as_quat, from_quat\r\n\r\nWikipedia alone is not an indication, but this the site that you refer to in the documentation\r\n\r\nIn my research on the web about the quaternion ordering, I do find more reference to the wxyz than the xyzw\r\nYou are right that both convention exist, but it looks like the xyzw is less common in my opinion.\r\n\r\nFor example, in the github site: https:\/\/github.com\/clemense\/quaternion-conventions\r\n30 use wxyz and 21 use xyzw\r\n\r\nI can easilly swap the quaternion myself before or after i receive them from scipy, \r\nbut it would be a nice to have option directly in scipy\r\n\r\nThanks.\r\nPat\r\n","Hi,\r\nThis is what i have done to shift \/ rotate the quaternion\r\n\r\n```\r\ndef from_xyzw_to_wxyz(quat = [0, 0, 0, 1]):\r\n    return quat[-1:] + quat[:-1]\r\n\r\ndef from_wxyz_to_xyzw(quat=[1, 0, 0, 0]):\r\n    return quat[1:] + quat[:1]\r\n```\r\n\r\nThanks\r\nPat","Recommend closing this issue as won\u2019t-do. There are at least 16 permutations of different quaternion conventions (w location being one choice). Switching the ordering is a simple user operation, this can be such a complex topic that I think it\u2019s more important to stick to one convention and be clear about it than to give toggles for all the options. \r\n\r\nSee: https:\/\/arxiv.org\/pdf\/1801.07478.pdf, https:\/\/www.researchgate.net\/publication\/278619675_Quaternion_kinematics_for_the_error-state_KF","There are only two: w first or w last. scipy supports different Euler conventions, I don't see why it shouldn't support different quaternion conventions. Ingesting and outputting quaternions between different libraries of different conventions is quite common and having this option would be a great usability improvement.","My point about conventions was not just about ordering, but also several other binary convention choices that can be made (see table 1 of the second link). Quaternion convention mismatch is a  huge source of tricky errors when integrating different systems. Wikipedia for example mixes several different conventions on different pages, and is not a good resource to lean on.\r\n\r\nBut the comparison to the Euler angles is fair, I agree now that a `seq = {'xyzw', 'wxyz'}` argument similar to Euler angles would be an easy addition, and the existence of that option might help bring visibility to the default ordering as well. ","Adding arguments `scalar_last=True` (or `scalar_first=False`) to `from_quat` and `as_quat` is reasonable and seamless addition.\r\n\r\n@scottshambaugh would you make a PR for that?"],"labels":["enhancement","scipy.spatial"]},{"title":"ENH: Add F-test of equality of variances to scipy.stats","body":"### Is your feature request related to a problem? Please describe.\n\nHello everyone.\r\n\r\nI'd like to suggest to add [F-test of equality of variances](https:\/\/en.wikipedia.org\/wiki\/F-test_of_equality_of_variances) to `scipy.stats`. Dispite the test is sensitive to non-normality of distribution rather other implemented tests in `scipy` (i.e. Levene), the F-test is one of basic statistic test and used widely.\r\n\r\nThere are plenty search requests like \"how to perform an f-test in python\" in google and at common developers' platforms and related articles. I listed below some of them:\r\n- [stackoverflow.com](https:\/\/stackoverflow.com\/questions\/21494141\/how-do-i-do-a-f-test-in-python), viewed 91k times, has a solution with 58 likes.\r\n- [geeksforgeeks.org](https:\/\/www.geeksforgeeks.org\/how-to-perform-an-f-test-in-python\/)\r\n- [statology.org](https:\/\/www.statology.org\/f-test-python\/)\r\n\r\nAll these resources copy the same simple idea based on the scipy function.\r\n\r\nMoreover, the second common used statistic language - R has [a realisation of the test](https:\/\/www.rdocumentation.org\/packages\/stats\/versions\/3.6.2\/topics\/var.test)\n\n### Describe the solution you'd like.\n\nI propose to add function `f_equality_test`.\r\n\r\nDescription: Test the null-hypothesis that the two given samples have the same variance.\r\n\r\nParameters:\r\n- x, y, array-like. The sample data, possibly with different lengths. Only one-dimensional samples are accepted\r\n\r\nReturns:\r\n- statistic: float. Calculated f-statistic\r\n- pvalue: float. Calculated p-value for the test.\r\n\r\nThe implementation could be taken from materail of [Dr. Anoop Kumar Singh, Lucknow University ](https:\/\/www.lkouniv.ac.in\/site\/writereaddata\/siteContent\/202004261258144523Anoop_Applied_ANNOVA.pdf). As far as I know the method is originaly described in Statistical Methods, 8th Edition by George W. Snecdecor, William G. Cochran.\r\nThere are many links to author's book, unfortunately I didn't find it in the free access to check.\r\n\r\nShortly:\r\n- calculate s  as sum(x_i - mean (x))\/n-1 for the both samples\r\n- calculate degree of freedom for as n-1 for the both samples\r\n- calculate f = larger s \/ smaller s\r\n- calculate pvalue using scipy.stats.f.cdf\r\n\r\nNote:\r\nThe test has an assamption that both samples have normal distribution.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":[],"labels":["scipy.stats","enhancement"]},{"title":"ENH: Support of state change in `solve_ivp` via events.","body":"### Is your feature request related to a problem? Please describe.\n\nMultiple people is strugelling on implementing functions such \"Dirac Delta\" or \"dt\" based.\r\n - https:\/\/stackoverflow.com\/questions\/56989417\/how-to-solve-in-python-a-set-of-odes-where-one-of-them-contains-a-dirac-delta-fu\r\n - https:\/\/stackoverflow.com\/questions\/63443810\/how-to-repeatedly-change-y-when-solving-ode-dy-dt-ft-y-with-solve-ivp\r\n - https:\/\/stackoverflow.com\/questions\/63230568\/numerical-solution-to-a-differential-equation-containing-a-dirac-delta-function\r\n - and more\n\n### Describe the solution you'd like.\n\nI see the most popular way to solve this issue is to use `events` with `terminal` to stop the integration, change the state values and resume the integration.\r\n\r\nHowever, this is a bit anoying in cases where you have to stop, edit t_span and at the end, merge all the solutions into a unique array. A nice approach would be to use `events` to actually do this.\r\n\r\nInstead of using `.terminal=True` I would like to be able to fix an attribute `.set_state=fun(y,t) -> y_new`. Then when the solver finds a root to the event, the conditions would change, saving the user from all the work above.\n\n### Describe alternatives you've considered.\n\n - Capability to have dt in the integration function (long topic, no sense).\r\n - A set of functions such \"dirac_delta\" are provided and that the solver accuratelly solves them.\r\n - Use decorators or classes for complex equations.\n\n### Additional context (e.g. screenshots, GIFs)\n\nhttps:\/\/github.com\/scipy\/scipy\/issues\/18041\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/18039\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/17602\r\n\r\n","comments":[],"labels":["enhancement","scipy.integrate"]},{"title":"gcrotmk invalid value warning with complex r.h.s.","body":"On some platforms, `sparse.linalg.gcrotmk` emits a \r\n\r\n```\r\nscipy\/sparse\/linalg\/_isolve\/_gcrotmk.py:131: in _fgmres\r\n    alpha = 1\/hcur[-1]\r\nE   RuntimeWarning: invalid value encountered in cdouble_scalars\r\n```\r\n\r\nfor (even trivial) real-valued l.h.s. and complex-valued r.h.s.\r\n\r\nA stripped-down test case, which demonstrates the failure in a PR on my scipy fork (to minimize noise on the main repo) is https:\/\/github.com\/ev-br\/scipy\/pull\/10\/, and is reproduced under the fold.\r\n\r\n<details>\r\n\r\n```\r\n        n = 7**4\r\n        a = np.eye(n, dtype=float)\r\n\r\n        vals = np.asarray([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0])\r\n        vals = (vals[:, None, None, None] +\r\n                vals[None, :, None, None] * 10 +\r\n                vals[None, None, :, None] * 100 +\r\n                vals[None, None, None, :] * 1000)\r\n        vals = vals - 2j*vals\r\n        vals = vals.flatten()\r\n\r\n        import warnings\r\n        with warnings.catch_warnings():\r\n            warnings.simplefilter(\"error\")\r\n            res, info = gcrotmk(a, vals, atol=1e-5)\r\n\r\n        assert info == 0\r\n\r\n        res_re, _ = gcrotmk(a, vals.real, atol=1e-5)\r\n        res_im, _ = gcrotmk(a, vals.imag, atol=1e-5)\r\n\r\n        assert_allclose(res, res_re + 1j*res_im)\r\n```\r\n<\/details>\r\n\r\nThis specific repro is platform-dependent: does not repro locally or on CI with 64-bit linux, is only repro in the 32-bit CI run. Similar warnings were visible on some other platforms for less trivial systems in https:\/\/github.com\/scipy\/scipy\/pull\/19633, so I can try reconstructing them if it's helpful.\r\n\r\nOn a high level, I wonder if this case of `float_matrix @ unknowns = complex_vector` is supported at all, maybe it should fail early instead?\r\n\r\ncc @ilayn @zhaog6 as our linalg experts","comments":["Here's why I suspect the problem is more general than an obscure 32-bit linux:\r\n\r\nIn https:\/\/github.com\/scipy\/scipy\/pull\/19633, there are tests which basically check that\r\n```\r\nsolve(A, cmplx_b) == solve(A, b.real) + 1j * solve(A, b.imag)\r\n```\r\n(up to machine accuracy of course)\r\n\r\nThese tests fail and the results differ by non-negligible atol\/rtol even on bog-standard platforms. To trigger, apply the following patch to  https:\/\/github.com\/scipy\/scipy\/pull\/19633\r\n\r\n```diff\r\n$ git diff\r\n$ git diff\r\ndiff --git a\/scipy\/interpolate\/_ndbspline.py b\/scipy\/interpolate\/_ndbspline.py\r\nindex 14f8ab3ca6..0dc1cdaf10 100644\r\n--- a\/scipy\/interpolate\/_ndbspline.py\r\n+++ b\/scipy\/interpolate\/_ndbspline.py\r\n@@ -256,19 +256,6 @@ class NdBSpline:\r\n def _iter_solve(a, b, solver=ssl.gcrotmk, **solver_args):\r\n     # work around iterative solvers not accepting multiple r.h.s.\r\n \r\n-    # also work around a.dtype == float64 and b.dtype == complex128\r\n-    # gcrotmk has bad accuracy and\/or emits a warning\r\n-    #   scipy\/sparse\/linalg\/_isolve\/_gcrotmk.py:131: in _fgmres\r\n-    #      alpha = 1\/hcur[-1]\r\n-    #   E   RuntimeWarning: invalid value encountered in cdouble_scalars\r\n-    # on some linuxes (32-bit, ATLAS, some others, just not the default 64-bit\r\n-    # ubuntu with OpenBLAS). Note that that line has a comment\r\n-    # \"careful with denormals\".\r\n-    if np.issubdtype(b.dtype, np.complexfloating):\r\n-        real = _iter_solve(a, b.real, solver, **solver_args)\r\n-        imag = _iter_solve(a, b.imag, solver, **solver_args)\r\n-        return real + 1j*imag\r\n```\r\n\r\nand observe\r\n\r\n```\r\n____________________________________________________________________________________ TestInterpN.test_complex[quintic] ____________________________________________________________________________________\r\nscipy\/interpolate\/tests\/test_rgi.py:886: in test_complex\r\n    assert_allclose(v1, v2)\r\n        method     = 'quintic'\r\n        points     = (array([0.5, 2. , 3. , 4. , 5.5, 6. ]), array([0.5, 2. , 3. , 4. , 5.5, 6. ]))\r\n        sample     = array([[1. , 1. ],\r\n       [2.3, 3.3],\r\n       [5.3, 1.2],\r\n       [0.5, 4. ],\r\n       [3.3, 5. ],\r\n       [1.2, 1. ],\r\n       [3. , 3. ]])\r\n        self       = <scipy.interpolate.tests.test_rgi.TestInterpN object at 0x7f92c1c9b9d0>\r\n        v1         = array([7.86181788-15.72363576j, 1.96439131 -3.92878263j,\r\n       4.08375731 -8.16751462j, 2.00000317 -4.00000634j,\r\n       1.04843993 -2.09687986j, 7.77807923-15.55615846j,\r\n       3.00000039 -6.00000077j])\r\n        v2         = array([7.86181733-15.72363466j, 1.96439142 -3.92878283j,\r\n       4.08375696 -8.16751391j, 2.00000467 -4.00000933j,\r\n       1.0484406  -2.09688121j, 7.77807826-15.55615651j,\r\n       3.00000043 -6.00000086j])\r\n        v2i        = array([-15.72363466,  -3.92878283,  -8.16751391,  -4.00000933,\r\n        -2.09688121, -15.55615651,  -6.00000086])\r\n        v2r        = array([7.86181733, 1.96439142, 4.08375696, 2.00000467, 1.0484406 ,\r\n       7.77807826, 3.00000043])\r\n        values     = array([[1.-2.j, 2.-4.j, 1.-2.j, 2.-4.j, 1.-2.j, 1.-2.j],\r\n       [1.-2.j, 2.-4.j, 1.-2.j, 2.-4.j, 1.-2.j, 1.-2.j],\r\n    ....j],\r\n       [1.-2.j, 2.-4.j, 1.-2.j, 2.-4.j, 1.-2.j, 1.-2.j],\r\n       [1.-2.j, 2.-4.j, 2.-4.j, 2.-4.j, 1.-2.j, 1.-2.j]])\r\n        x          = array([0.5, 2. , 3. , 4. , 5.5, 6. ])\r\n        y          = array([0.5, 2. , 3. , 4. , 5.5, 6. ])\r\n\/home\/br\/mambaforge\/envs\/scipy-dev\/lib\/python3.10\/contextlib.py:79: in inner\r\n    return func(*args, **kwds)\r\nE   AssertionError: \r\nE   Not equal to tolerance rtol=1e-07, atol=0\r\nE   \r\nE   Mismatched elements: 3 \/ 7 (42.9%)\r\nE   Max absolute difference: 3.34631492e-06\r\nE   Max relative difference: 7.48257019e-07\r\nE    x: array([7.861818-15.723636j, 1.964391 -3.928783j, 4.083757 -8.167515j,\r\nE          2.000003 -4.000006j, 1.04844  -2.09688j , 7.778079-15.556158j,\r\nE          3.       -6.000001j])\r\nE    y: array([7.861817-15.723635j, 1.964391 -3.928783j, 4.083757 -8.167514j,\r\nE          2.000005 -4.000009j, 1.048441 -2.096881j, 7.778078-15.556157j,\r\nE          3.       -6.000001j])\r\n        args       = (<function assert_allclose.<locals>.compare at 0x7f92c18891b0>, array([7.86181788-15.72363576j, 1.96439131 -3.92878263...1j, 2.00000467 -4.00000933j,\r\n       1.0484406  -2.09688121j, 7.77807826-15.55615651j,\r\n       3.00000043 -6.00000086j]))\r\n        func       = <function assert_array_compare at 0x7f92c4718280>\r\n        kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=0', 'verbose': True}\r\n        self       = <contextlib._GeneratorContextManager object at 0x7f92c4705300>\r\n```\r\n\r\nAlso, the issue persists if I replace `gcrotmk` with `gmres`.","I don't know exactly how this happens yet, but firing just a quick remarks \r\n\r\n-  `float_matrix @ unknowns = complex_vector`; This is supported by the `makesystem` in the entrance of iterative solvers. Maybe it is going awry for this particular combo?\r\n- `E   RuntimeWarning: invalid value encountered in cdouble_scalars`; this happens when hcur[-1] is zero. I don't know why Pauli went that way but typically this is controlled before the division say in `gmres`\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/7f397e03e0a3554d8c02c8b477e8dc5e0ce7edc9\/scipy\/sparse\/linalg\/_isolve\/iterative.py#L783-L788\r\n\r\n\r\n- `solve(A, cmplx_b) == solve(A, b.real) + 1j * solve(A, b.imag)`;  I think this can only work when both `b.imag` and `b.real` are in comparable scales. \r\n\r\n\r\nVery long shot, this might be the case of `make_system` in the entrance is not figuring out things correctly and still rendering the `x` as `float64`. "],"labels":["defect","scipy.sparse.linalg"]},{"title":"MAINT: Vendor npymath as git submodule and vendor npyrandom directly","body":"Closes gh-17498\r\n","comments":["x-ref gh-17498","The CI failures are looking like:\r\n```\r\nstats\/_biasedurn.cpython-310d-x86_64-linux-gnu.so: undefined symbol: npy_log1p\r\n```\r\n\r\nThe problem here will be with statically linking `libnpyrandom`, since that expects the old `libnpymath` library and that is now replaced with the new version with renamed symbols. Hence the code for `random` can no longer find `npy_log1p`. That's why I included both libraries in gh-18030. I'm not sure whether that's the correct resolution here though. The design in numpy isn't great, it'd be much better to have a regular shared library interface for Cython. Let's discuss that on https:\/\/github.com\/numpy\/numpy\/issues\/20880.","If we look at what we're actually using from `libnpyrandom`, it's fairly minimal. There are two Cython files, `stats\/_biasedurn.pyx` and `stats\/_rcont\/rcont.pyx`, which need it. Both need the `bitgen_t` struct, which is a struct of pointers (see https:\/\/numpy.org\/doc\/stable\/reference\/random\/extending.html#new-bit-generators):\r\n```C\r\ntypedef struct bitgen {\r\n  void *state;\r\n  uint64_t (*next_uint64)(void *st);\r\n  uint32_t (*next_uint32)(void *st);\r\n  double (*next_double)(void *st);\r\n  uint64_t (*next_raw)(void *st);\r\n} bitgen_t;\r\n```\r\n\r\nAnd then `_biasedurn.pyx` needs the `random_normal` function. If you look at the sources for that, you'll find that that's pretty much all pure C as well, except for it using `npy_log1p`. Which is simply an alias for C99's `log1p`.","I've removed the libnpymath (it's not needed at all) and libnpyrandom (no need for a static library, just the sources) dependencies altogether and tested this against numpy\/numpy#25390, which in turn removes any dependency on numpy from npyrandom, and it seems like everything's okay. @rgommers Do you think this is a sensible way forward?","Those source files aren't shipped by NumPy yet, so that cannot work I think - as also indicated by CI failures:\r\n```\r\n scipy\/meson.build:80:30: ERROR: File ..\/test\/lib\/python3.9\/site-packages\/numpy\/core\/include\/..\/..\/random\/src\/distributions\/distributions.c does not exist.\r\n```\r\n\r\nWe are supporting ~4 numpy versions back, so rather than trying to ship the C sources as part of NumPy, they should be vendored somehow (either directly or via a submodule). I was suggesting above that what we need is so minimal (one struct, one function) that it may be possible to vendor the relevant bits directly.","I got successfully (locally) Windows 32-bit build with https:\/\/github.com\/lysnikolaou\/scipy\/commit\/4829d69cc3c143f62f5d6751c2f895e15c39fa3d out of the box!","> I got successfully (locally) Windows 32-bit build with [lysnikolaou@4829d69](https:\/\/github.com\/lysnikolaou\/scipy\/commit\/4829d69cc3c143f62f5d6751c2f895e15c39fa3d) out of the box!\r\n\r\nAh - excellent - but I guess with the same ndimage test failures as before?","The same failures unfortunately. Not related to this PR however.","This is now updated to use numpy\/libnpymath, instead of my test repo. A round of review would be nice, so that we can merge it.","The `cp311 (build sdist + wheel), full, no pythran` fail is unrelated.","So is the `Meson build (3.12, false)` as far as I can tell.","If you push another commit, could you include `[wheel build]` in the first line of your commit message? That way we run all wheel builds too, and ensure that's all green.","@lysnikolaou can you summarize for me the relationship (if any) between this change and NumPy 2.0.0? As I recall, Ralf wanted this merged a few release ago, but then things got tricky with various discussion upstream.\r\n\r\nI'm thinking of branching for `1.13.0` (NumPy 2.0.0 support release) ~Sunday the 17th.","> @lysnikolaou can you summarize for me the relationship (if any) between this change and NumPy 2.0.0? As I recall, Ralf wanted this merged a few release ago, but then things got tricky with various discussion upstream.\r\n> \r\n> I'm thinking of branching for `1.13.0` (NumPy 2.0.0 support release) ~Sunday the 17th.\r\n\r\n*TL;DR This is not needed for 2.0 compatibility.*\r\n\r\nThe bigger picture is that we want to eventually stop shipping static libraries with NumPy. We're working with the assumption that SciPy is one of the few, if not the only, downstream user of the static libraries, hence this PR that adds support for the new way of doing things.\r\n\r\nHowever, this change did not make it into 2.0 because of challenges with complex numbers. I'm actively working on a solution for these problems, and hope that everything will be ready for 2.1. In case that goes well, `npymath` will be moved out of the NumPy source tree and it will be maintained in a [separate repo](https:\/\/github.com\/numpy\/libnpymath). NumPy will still ship with the `npymath` static library for backwards-compatibility reasons, although we would like to eventually sunset that, and the best way to go forward for downstream would be to vendor `npymath` as a git submodule instead of relying on NumPy for it.\r\n\r\nFor `npyrandom`, the purpose of this PR is to remove any dependency on it, and instead vendor the few needed files directly. This way, we can start figuring out a plan on how to stop shipping the `npyrandom` static library as well.\r\n\r\n\r\n"],"labels":["maintenance"]},{"title":"ENH: Dedicated Function for Envelope Extraction","body":"### Is your feature request related to a problem? Please describe.\n\nIt has come to my attention that scipy.signal does not have a dedicated function for extracting the envelope of a signal. It would be nice to have a function scipy.signal.envelope that implements several envelope extraction methods. MATLAB Signal Processing Toolbox has this functionality:\r\n\r\nhttps:\/\/www.mathworks.com\/help\/signal\/ref\/envelope.html\r\n\r\n\n\n### Describe the solution you'd like.\n\nI could implement this. I do not have experience contributing to open source, but this would be a good start.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["I would like to follow up on this. Could you assign me this issue, if this is a function you would want to have?","Hi @yagizolmez, please see [the docs for contributing new code](https:\/\/docs.scipy.org\/doc\/scipy\/dev\/hacking.html#contributing-new-code). The first step is to put your proposal to [the mailing list](https:\/\/mail.python.org\/mailman3\/lists\/scipy-dev.python.org\/).","Seems like Matlab's function is a wrapper. Some of the functionalities that are suggested are implemented in signal.hilbert or pandas. @yagizolmez, are you suggesting to create a wrapper? ","Regardless of the algorithm, please make sure that you do not check matlab's sourcecode (not sure if it is even possible) but same with octave and other incompatible licensed sources. It is OK to have a similar function signature but we cannot accept a derivative of such codebase and\/or algorithm.","@lucascolley I shared it in the email group, but the discussion seems to be happening here.\r\n\r\n@gideonKogan I would be happy if you could share the functions in pandas. signal.hilbert indeed implements the 'analytic' option of the MATLAB function but does not implement the other methods ('rms', 'peak').\r\n\r\n@ilayn Thank you for the warning. I will not check MATLAB's codebase.\r\n\r\nPlease let me know what you think.\r\nHappy New Year to all of you and thanks for commenting.","```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfs = 1000\r\ntime = 10\r\nfreq = 1\r\nwindow = 50\r\nt = np.arange(time * fs) \/ fs\r\n\r\nx = pd.Series((np.sin(2 * np.pi * freq * t) + 1) * np.random.normal(size=t.size))\r\n\r\nax = x.plot() #original\r\n((x**2).rolling(window=window).mean()**0.5).plot(ax = ax) #rms\r\nx.rolling(window=window).max().plot(ax = ax, grid=True) #peak\r\n```\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/41887702\/554a4fed-fed0-40b4-b66b-4570d716d992)","@ggkogan Thanks for sharing this. Your solution looks sound to me, but as far as I know scipy does not have a dependency on pandas. Would people be OK to add such dependency for this one function? @ilayn \r\n\r\nYour code also needs to be slightly modified to support upper and lower envelopes.\r\n\r\nWe could also add additional envelope extraction methods that are not covered by the MATLAB function such as rectify and low pass filter (also known as linear envelope), which is commonly used in EMG analysis.","No, we will not be depending on pandas.","@ggkogan, @rkern I have written a function that depends only on numpy and scipy, that follows MATLAB's function signature, and seems to work well.\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.signal import hilbert,find_peaks\r\nfrom scipy.interpolate import UnivariateSpline\r\n\r\ndef envelope(x,N = None,method = 'analytic'):\r\n    \r\n    #Assert that x is a 1D numpy array\r\n    assert isinstance(x, np.ndarray) and x.ndim == 1, 'x must be a 1D NumPy array'\r\n    \r\n    if method == 'analytic':\r\n        \r\n        #Calculate the mean of x and remove it\r\n        x_mean = np.mean(x)\r\n        x_zero_mean = x-x_mean\r\n        \r\n        #Take the absolute value of the Hilbert transform\r\n        #to calculate the analytical envelope of the zero mean\r\n        #version of x\r\n        zero_mean_envelope = np.abs(hilbert(x_zero_mean,N = N))\r\n        \r\n        #Calculate and return upper and lower envelopes\r\n        return x_mean+zero_mean_envelope,x_mean-zero_mean_envelope\r\n    \r\n    elif method == 'rms':\r\n        \r\n        #Assert that N is specified and is an integer\r\n        #For the rms method N serves as window size\r\n        #for the moving average\r\n        assert N and isinstance(N,int), 'N must be an integer'\r\n        \r\n        #Calculate the mean of x and remove it\r\n        x_mean = np.mean(x)\r\n        x_zero_mean = x-x_mean\r\n        \r\n        #Calculate the RMS envelope of the zero mean version of x.\r\n        #Moving average is calculated using np.convolve\r\n        zero_mean_envelope = np.sqrt(np.convolve(x_zero_mean**2,np.ones(N)\/N,mode = 'same'))\r\n        \r\n        #Calculate and return upper and lower envelopes\r\n        return x_mean+zero_mean_envelope,x_mean-zero_mean_envelope\r\n    \r\n    elif method == 'peak':\r\n        \r\n        #Assert that N is specified and is an integer\r\n        #For the peak method N serves as the minimum number of\r\n        #samples that seperate local maxima\r\n        assert N and isinstance(N,int), 'N must be an integer'\r\n        \r\n        #Calculate local maxima and minima which will serve\r\n        #as peaks for upper and lower envelopes respectively\r\n        peaks_upper,_ = find_peaks(x,distance = N)\r\n        peaks_lower,_ = find_peaks(-x,distance = N)\r\n        \r\n        #Calculate upper and lower envelopes by interpolating\r\n        #peaks using a Univariate spline\r\n        upper_spline = UnivariateSpline(peaks_upper,x[peaks_upper])\r\n        lower_spline = UnivariateSpline(peaks_lower,x[peaks_lower])\r\n        upper_envelope = upper_spline(np.arange(x.shape[0]))\r\n        lower_envelope = lower_spline(np.arange(x.shape[0]))\r\n        \r\n        #Return the envelopes\r\n        return upper_envelope,lower_envelope\r\n    \r\n    else:\r\n        \r\n        raise ValueError('%s is not a valid method' %(method))\r\n```\r\n\r\nTest for analytical envelope extraction:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\n\r\nt = np.arange(0, 2, 1\/2000)\r\nsignal = (1+0.5*np.cos(2*np.pi*1*t))*np.cos(2*np.pi*10*t)+5\r\nupper,lower = envelope(signal)\r\n\r\nplt.plot(t, signal)\r\nplt.plot(t, upper)\r\nplt.plot(t, lower)\r\nplt.show()\r\n```\r\n\r\n![analytic](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/2ccc428d-6ab8-46f6-b4e4-82858b0d488c)\r\n\r\nTest for RMS envelope extraction:\r\n\r\n```python\r\nt = np.arange(0, 2, 1\/2000)\r\nsignal = (1+0.5*np.cos(2*np.pi*1*t))*np.random.normal(size = t.shape[0])+5\r\n\r\nupper,lower = envelope(signal,N = 150,method = 'rms')\r\n\r\nplt.plot(t, signal)\r\nplt.plot(t, upper)\r\nplt.plot(t, lower)\r\nplt.show()\r\n```\r\n![rms](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/f52e07df-572d-425f-8004-ad6c83044c27)\r\n\r\nTest for peak envelope extraction\r\n\r\n```python\r\nfrom scipy.io.wavfile import read\r\n\r\nfs,signal = read('FK61_01.wav')\r\nt = np.linspace(1,signal.shape[0],signal.shape[0])\/fs\r\n\r\nupper,lower = envelope(signal,N = 200,method = 'peak')\r\n\r\nplt.plot(t, signal)\r\nplt.plot(t, upper)\r\nplt.plot(t, lower)\r\nplt.show()\r\n```\r\n![peak](https:\/\/github.com\/scipy\/scipy\/assets\/57116432\/67ea278e-692e-4039-87ca-68854fd73612)\r\n\r\nPlease let me know what you think!","@yagizolmez your analytic filtering should be in time domain. There might appear other deviatios. I suggest you try to test for bit-exactness.\n\nAre we sure that we want to replicate Matlab's function? Why?\n\n I am not sure whether we should try to reduce running time by Cython implementation.\n\n","@ggkogan I do not understand your comment about the analytical filtering. I simply use signal.hilbert as you suggested in your first comment. I would be happy if you could be a bit more specific or provide some code.\r\n\r\nI think using similar function signature with MATLAB would make it easier for people to migrate their codes into scipy. MATLAB Signal Processing Toolbox is unfortunately still more efficient than scipy.signal and it remains more widely used especially in the sciences. Many functions in scipy.signal have the same signature as their MATLAB counter parts (hilbert,butter,filtfilt,chirp etc).\r\n\r\nAdmittedly, I do not have much experience with cython. However, I would like to note that the code I provided does not have any python level loops. It relies on other functions in numpy and scipy. Considering that numpy is already partially written in C, I don't know how much of an improvement cython would provide.","> I think using similar function signature with MATLAB would make it easier for people to migrate their codes into scipy\r\n\r\nThat is correct but we all are in a way, past matlab survivors. Being similar is good but if matlab signature is not good we don't insist on the affinity. This is particularly the case especially for matlab's legacy functions, wrong defaults, terrible option names and so on. \r\n\r\n> Admittedly, I do not have much experience with cython\r\n\r\nLet's get there when it is needed; we can help you with all that. A bit of benchmarking would go a long way, even if it is done on a jupyter notebook with `%timeit` magic.","@yagizolmez  signal.hilbert gives envelope but if we try to **reproduce** the envelope you presented, we should implement in time domain. I suspect that the original implementation was designed to maintain **causality**, which is not maintained by using signal.hilbert.","@gideonKogan Do you suggest that I should implement the 'analytical' option with a causal filter? Causal filters introduce lags, so the filtered envelope would be lagged. I don't think we should do that.\r\n\r\n@ilayn I personally like the MATLAB signature in this case, but I am open to suggestions. I will post the benchmarks shortly.","@ilayn I did some benchmarking on the code I have provided. %timeit gave the following outputs for the analytic, rms and peak respectively:\r\n\r\n202 \u00b5s \u00b1 16.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n210 \u00b5s \u00b1 3.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n122 ms \u00b1 18.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\nI have written the following code to benchmark the MATLAB function\r\n\r\n```MATLAB\r\n\r\n%analytic\r\n\r\n% Your code block here\r\nt = 0:1\/2000:2;\r\nsignal = (1 + 0.5 * cos(2 * pi * 1 * t)) .* cos(2 * pi * 10 * t) + 5;\r\n\r\n% Number of iterations\r\nnumIterations = 1000;\r\n\r\n% Measure execution time for multiple runs\r\ntotalTime = 0;\r\nfor i = 1:numIterations\r\n    tic;\r\n    upper = envelope(signal);\r\n    elapsedTime = toc;\r\n    totalTime = totalTime + elapsedTime;\r\nend\r\n\r\n% Calculate the average execution time\r\naverageTime = totalTime \/ numIterations;\r\n\r\n% Display the average execution time\r\nfprintf('Average execution time of analytic over %d iterations: %.2f microseconds\\n', numIterations, averageTime*10^6);\r\n\r\n%rms\r\n\r\nt = 0:1\/2000:2; % Create a time vector from 0 to 2 with a step of 1\/2000\r\nsignal = (1 + 0.5 * cos(2 * pi * 1 * t)) .* randn(size(t)) + 5; % Define the signal using random normal distribution\r\n\r\n% Measure execution time for multiple runs\r\ntotalTime = 0;\r\nfor i = 1:numIterations\r\n    tic;\r\n    upper = envelope(signal,150,'rms');\r\n    elapsedTime = toc;\r\n    totalTime = totalTime + elapsedTime;\r\nend\r\n\r\n% Calculate the average execution time\r\naverageTime = totalTime \/ numIterations;\r\n\r\n% Display the average execution time\r\nfprintf('Average execution time of rms over %d iterations: %.2f microseconds\\n', numIterations, averageTime*10^6);\r\n\r\n%peak\r\n\r\n% Read the WAV file\r\n[signal, fs] = audioread('FK61_01.wav');\r\n\r\n% Create a time vector\r\nt = (1:length(signal)) \/ fs;\r\n\r\n% Measure execution time for multiple runs\r\ntotalTime = 0;\r\nfor i = 1:numIterations\r\n    tic;\r\n    upper = envelope(signal,200,'peak');\r\n    elapsedTime = toc;\r\n    totalTime = totalTime + elapsedTime;\r\nend\r\n\r\n% Calculate the average execution time\r\naverageTime = totalTime \/ numIterations;\r\n\r\n% Display the average execution time\r\nfprintf('Average execution time of peak over %d iterations: %.2f milliseconds\\n', numIterations, averageTime*10^3);\r\n\r\n```\r\n\r\nThe output of this code was:\r\n\r\nAverage execution time of analytic over 1000 iterations: 572.22 microseconds\r\nAverage execution time of rms over 1000 iterations: 130.67 microseconds\r\nAverage execution time of peak over 1000 iterations: 40.49 milliseconds\r\n\r\nThe peak method seems to be 3x faster in MATLAB. Please let me know what you think. ","@yagizolmez , I argue that we should decide if we want Matlab to be our reference. If we do, we should aim to bit-accurate. If we do not, we should decide how the function should be and ignore Matlab's implementation. As far as I understand, your motivation is copying Matlab, right? In such a case, we should consider the deviation...","Compatibility with MATLAB, either in terms of naming, function arguments, or bit-accuracy is an explicit non-goal of the scipy project. Looking at MATLAB is _at most_ market research: we can infer from the presence of an envelope function with those three methods that having those three methods exposed is desirable. Once we get that nugget of information, we start from scratch and ignore the details of the MATLAB implementation. If warranted, we can sometimes use MATLAB to generate reference values for our tests, but bit-level accuracy is unnecessary (which is good, because it's unattainable in general).","I have worked on this a little, bit and I need some help. Please see the commit in my fork:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/compare\/main...yagizolmez:scipy:signal-envelope?expand=1\r\n\r\nI have not opened a pull request yet, because I have some problems:\r\n\r\n1. When I first ran 'python dev.py doc' in my local machine, I was able to generate the docs, but the documentation for my function was missing. When I tried again, I have got the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/events.py\", line 97, in emit\r\n    results.append(listener.handler(self.app, *args))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pydata_sphinx_theme\/__init__.py\", line 145, in update_templates\r\n    if theme_css_name in context[\"css_files\"]:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/html\/_assets.py\", line 40, in __eq__\r\n    warnings.warn('The str interface for _CascadingStyleSheet objects is deprecated. '\r\nsphinx.deprecation.RemovedInSphinx90Warning: The str interface for _CascadingStyleSheet objects is deprecated. Use css.filename instead.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/cmd\/build.py\", line 298, in build_main\r\n    app.build(args.force_all, args.filenames)\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/application.py\", line 355, in build\r\n    self.builder.build_update()\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 293, in build_update\r\n    self.build(to_build,\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 363, in build\r\n    self.write(docnames, list(updated_docnames), method)\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 571, in write\r\n    self._write_serial(sorted(docnames))\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 581, in _write_serial\r\n    self.write_doc(docname, doctree)\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/html\/__init__.py\", line 655, in write_doc\r\n    self.handle_page(docname, ctx, event_arg=doctree)\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/builders\/html\/__init__.py\", line 1107, in handle_page\r\n    newtmpl = self.app.emit_firstresult('html-page-context', pagename,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/application.py\", line 492, in emit_firstresult\r\n    return self.events.emit_firstresult(event, *args,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/events.py\", line 118, in emit_firstresult\r\n    for result in self.emit(name, *args, allowed_exceptions=allowed_exceptions):\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/sphinx\/events.py\", line 108, in emit\r\n    raise ExtensionError(__(\"Handler %r for event %r threw an exception\") %\r\nsphinx.errors.ExtensionError: Handler <function update_templates at 0x7fa2e5df1a80> for event 'html-page-context' threw an exception (exception: The str interface for _CascadingStyleSheet objects is deprecated. Use css.filename instead.)\r\n\r\nI am thinking that I initially failed to create documentation for my function due to lines 4702,4703:\r\n\r\n>>> signal = (1+0.5*np.cos(2*np.pi*1*t))*\r\n    np.random.normal(size = t.shape[0])+5\r\n\r\nLater I fixed this, but I am still getting the error. This is my first time working with docstrings, so any help would be appreciated.\r\n\r\n2. Is there any audio dataset in scipy that I can use for the documentation? I think the 'peak' method would be best demonstrated with audio.\r\n\r\n3. I have tested all three methods locally using the examples I have provided earlier. I have also written some unit tests and added them to my commit. These tests only check for correct inputs, and they pass. Do you have any recommendations for any other tests?\r\n\r\n4. In the documentation, I give the default value for the 'analytic' method as 'x.shape[0]' based on hilbert's default. Is this good practice? Should I change the code and set N = x.shape[0] for hilbert, so that my documentation would not be effected even if hilbert's default changes.\r\n\r\n5. I am currently importing find_peaks within the envelope function. Doing this outside the function throws a circular import error. Is there a better way to handle circular imports? This is also something that I am not really familiar with.\r\n\r\nThese are my questions for now. Sorry for the long post. I am looking forward to your responses.","1. I also saw an error like this last night so chances are it is unrelated to your changes. I think it should be fixed by upgrading version of the theme after gh-16660 (in the meantime, I think downgrading your version of `sphinx` _might_ work).\n\n2. I don't think so - see http:\/\/scipy.github.io\/devdocs\/reference\/datasets.html","@lucascolley Downgrading sphinx by one version did not work. I tried deleting doc\/build, and building the whole html again. This worked, but it is too time consuming. Also, the documentation still does not include the new envelope function I have added. Do you have any idea why this might be? My terminal's output is as follows:\r\n\r\n```bash\r\n\ud83d\udcbb  ninja -C \/home\/yagizolmez\/git-repos\/scipy\/build -j8\r\nninja: Entering directory `\/home\/yagizolmez\/git-repos\/scipy\/build'\r\n[4\/4] Generating scipy\/generate-version with a custom command\r\nBuild OK\r\n\ud83d\udcbb  meson install -C build --only-changed\r\nInstalling, see meson-install.log...\r\nInstallation OK\r\n# for testing\r\n# @echo installed scipy 58e580e matches git version 58e580e; exit 1\r\nmkdir -p build\/html build\/doctrees\r\nLANG=C \/home\/yagizolmez\/miniforge3\/envs\/scipy-dev\/bin\/python -msphinx -WT --keep-going  -b html -d build\/doctrees -j1  source build\/html \r\nRunning Sphinx v7.2.5\r\nSciPy (VERSION 1.13.0.dev)\r\n[autosummary] generating autosummary for: building\/blas_lapack.rst, building\/compilers_and_options.rst, building\/cross_compilation.rst, building\/distutils_equivalents.rst, building\/index.rst, building\/introspecting_a_build.rst, building\/redistributable_binaries.rst, building\/understanding_meson.rst, dev\/api-dev\/api-dev-toc.rst, dev\/api-dev\/array_api.rst, ..., tutorial\/stats\/discrete_zipf.rst, tutorial\/stats\/discrete_zipfian.rst, tutorial\/stats\/resampling.rst, tutorial\/stats\/sampling.rst, tutorial\/stats\/sampling_dau.rst, tutorial\/stats\/sampling_dgt.rst, tutorial\/stats\/sampling_hinv.rst, tutorial\/stats\/sampling_pinv.rst, tutorial\/stats\/sampling_srou.rst, tutorial\/stats\/sampling_tdr.rst\r\n[autosummary] generating autosummary for: \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.LowLevelCallable.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.optimize.LbfgsInvHessProduct.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.optimize.OptimizeResult.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.optimize.RootResults.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.bsr_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.bsr_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.coo_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.coo_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.csc_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.csc_matrix.rst, ..., \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.csr_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.dia_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.dia_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.dok_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.dok_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.lil_array.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.lil_matrix.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.sparse.linalg.LaplacianNd.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.stats._result_classes.PearsonRResult.rst, \/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/reference\/generated\/scipy.stats._result_classes.TtestResult.rst\r\nloading intersphinx inventory from https:\/\/docs.python.org\/3\/objects.inv...\r\nloading intersphinx inventory from https:\/\/numpy.org\/devdocs\/objects.inv...\r\nloading intersphinx inventory from https:\/\/numpy.org\/neps\/objects.inv...\r\nloading intersphinx inventory from https:\/\/matplotlib.org\/stable\/objects.inv...\r\nloading intersphinx inventory from https:\/\/asv.readthedocs.io\/en\/stable\/objects.inv...\r\nloading intersphinx inventory from https:\/\/www.statsmodels.org\/stable\/objects.inv...\r\nmyst v2.0.0: MdParserConfig(commonmark_only=False, gfm_only=False, enable_extensions=set(), disable_syntax=[], all_links_external=False, url_schemes=('http', 'https', 'mailto', 'ftp'), ref_domains=None, fence_as_directive=set(), number_code_blocks=[], title_to_header=False, heading_anchors=0, heading_slug_func=None, html_meta={}, footnote_transition=True, words_per_minute=200, substitutions={}, linkify_fuzzy_links=True, dmath_allow_labels=True, dmath_allow_space=True, dmath_allow_digits=True, dmath_double_inline=False, update_mathjax=True, mathjax_classes='tex2jax_process|mathjax_process|math|output_area', enable_checkboxes=False, suppress_warnings=[], highlight_code_blocks=True)\r\nmyst-nb v1.0.0: NbParserConfig(custom_formats={}, metadata_key='mystnb', cell_metadata_key='mystnb', kernel_rgx_aliases={}, eval_name_regex='^[a-zA-Z_][a-zA-Z0-9_]*$', execution_mode='auto', execution_cache_path='', execution_excludepatterns=(), execution_timeout=30, execution_in_temp=False, execution_allow_errors=False, execution_raise_on_error=False, execution_show_tb=False, merge_streams=False, render_plugin='default', remove_code_source=False, remove_code_outputs=False, code_prompt_show='Show code cell {type}', code_prompt_hide='Hide code cell {type}', number_source_lines=False, output_stderr='show', render_text_lexer='myst-ansi', render_error_lexer='ipythontb', render_image_options={}, render_figure_options={}, render_markdown_format='commonmark', output_folder='build', append_css=True, metadata_to_fm=False)\r\nUsing jupyter-cache at: \/home\/yagizolmez\/git-repos\/scipy\/doc\/build\/.jupyter_cache\r\nbuilding [mo]: targets for 0 po files that are out of date\r\nwriting output... \r\nbuilding [html]: targets for 4539 source files that are out of date\r\nupdating environment: [new config] 4539 added, 0 changed, 0 removed\r\n\/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/notebooks\/interp_transition_guide.md: Executing notebook using local CWD [mystnb]\r\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\r\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\r\n0.00s - to python to disable frozen modules.\r\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\r\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\r\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\r\n0.00s - to python to disable frozen modules.\r\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\r\n\/home\/yagizolmez\/git-repos\/scipy\/doc\/source\/notebooks\/interp_transition_guide.md: Executed notebook in 6.03 seconds [mystnb]\r\n<string>:46: DeprecationWarning: invalid escape sequence '\\,'dtr\r\n<string>:46: DeprecationWarning: invalid escape sequence '\\,'dtrc\r\n<string>:50: DeprecationWarning: invalid escape sequence '\\d'uber\r\n<string>:30: DeprecationWarning: invalid escape sequence '\\i'tairy\r\n<string>:25: DeprecationWarning: invalid escape sequence '\\i'ti0k0\r\n<string>:25: DeprecationWarning: invalid escape sequence '\\i'tj0y0\r\n<string>:42: DeprecationWarning: invalid escape sequence '\\c've\r\n<string>:42: DeprecationWarning: invalid escape sequence '\\c've\r\n<string>:22: DeprecationWarning: invalid escape sequence '\\P'dtr\r\n<string>:50: DeprecationWarning: invalid escape sequence '\\d'seudo_huber\r\n<string>:46: DeprecationWarning: invalid escape sequence '\\l'klmbda\r\n<string>:19: DeprecationWarning: invalid escape sequence '\\s'\r\nreading sources... [100%] tutorial\/stats\/sampling_tdr\r\nlooking for now-outdated files... none found\r\npickling environment... done\r\nchecking consistency... done\r\npreparing documents... done\r\ncopying assets... copying static files... done\r\ncopying extra files... done\r\ndone\r\nwriting output... [100%] tutorial\/stats\/sampling_tdr\r\ngenerating indices... genindex done\r\nwriting additional pages... search done\r\ncopying images... [100%] ..\/build\/plot_directive\/tutorial\/stats\/sampling_srou-1.png\r\ndumping search index in English (code: en)... done\r\ndumping object inventory... done\r\nbuild succeeded.\r\n\r\nThe HTML pages are in build\/html.\r\n```\r\n","> the documentation still does not include the new envelope function I have added. Do you have any idea why this might be?\r\n\r\nYou need to add your new function to the toc in `scipy\/signal\/__init__.py`","I have completed the implementation and opened a pull request. I would be happy if you could review my code!\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/19814","Enhancement request issues typically stay open until the enhancement has been merged (or decided against) \ud83d\udc4d"],"labels":["enhancement","scipy.signal"]},{"title":"ENH: multidimensional y0 ODE integration","body":"### Is your feature request related to a problem? Please describe.\n\nI'm trying to simulate a three-body problem using odeint, but it doesn't support multidimensional ys, forcing me to write a custom function to do it.\n\n### Describe the solution you'd like.\n\n_No response_\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["You can reshape y in your rhs function to be multidimensional and then flatten it before returning","@j-bowhay I figured it would be simple enough to accommodate. Would you be willing to review that or, if you'd prefer, would you like to take it over and me review?\r\n\r\n*Oops this asks for `odeint`, but I did `solve_ivp`. Since `solve_ivp` is the new interface, I think that's the only one I'd do this for.*","> @j-bowhay I figured it would be simple enough to accomodate. Would you be willing to review that or, if you'd prefer, would you like to take it over and me review?\r\n> \r\n> _Oops this asks for `odeint`, but I did `solve_ivp`. Since `solve_ivp` is the new interface, I think that's the only one I'd add to._\r\n\r\nHappy to review but too busy with uni to implement atm. Agree about only touching `solve_ivp`","FYI: [`odeintw`](https:\/\/github.com\/WarrenWeckesser\/odeintw).  Also available on PyPI (https:\/\/pypi.org\/project\/odeintw\/), so it can be installed with `pip`."],"labels":["enhancement","scipy.integrate"]},{"title":"BUG: linear system simulation yielding wrong results due to integer datatype.","body":"### Describe your issue.\n\nWhen simulating a continuous linear time invariant system with `lsim`, the states are inheriting the data type of the matrix A, which may cause the signal to be restricted to integers, for instance. This might yield wrong results when using `lsim`.\r\n\r\nThe error is related to the following code in \r\n\r\n```  \r\n  if X0 is None:\r\n        X0 = zeros(n_states, sys.A.dtype)\r\n  xout = np.empty((n_steps, n_states), sys.A.dtype)\r\n```\r\n\r\nIt happens whenever the matrix A sent by the user has integer datatype. \n\n### Reproducing Code Example\n\n```python\nfrom scipy.signal import lti, lsim\r\nfrom numpy import arange\r\n\r\n# System matrices\r\nA = [[0, -3],\r\n     [3,  0]]\r\n\r\nB = [[0], [0]]\r\nC = [0, 0]\r\nD = 0\r\n\r\n# Define the system\r\nSys = lti(A, B, C, D)\r\n\r\n# Initial condition\r\nX0 = [1.0, 0]\r\n\r\n# Define time\r\nt = arange(0, 3, 0.05)\r\n# Get response\r\nT, Y, X = lsim(Sys, 0, t, X0=X0)\n```\n\n\n### Error message\n\n```shell\nThe code example returns the states X as an almost zero array (only the first initial condition is nonzero). \r\n\r\nX.T =\r\narray([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\r\n\r\nThe right value should be:\r\n\r\narray([[ 1.        ,  0.98877108,  0.95533649,  0.9004471 ,  0.82533561,\r\n         0.73168887,  0.62160997,  0.49757105,  0.36235775,  0.21900669,\r\n         0.0707372 , -0.07912089, -0.22720209, -0.37018083, -0.5048461 ,\r\n        -0.62817362, -0.73739372, -0.83005354, -0.90407214, -0.95778724,\r\n        -0.9899925 , -0.99996466, -0.98747977, -0.95281821, -0.89675842,\r\n        -0.82055936, -0.7259323 , -0.61500238, -0.49026082, -0.35450907,\r\n        -0.2107958 , -0.06234851,  0.08749898,  0.23538144,  0.37797774,\r\n         0.51208548,  0.63469288,  0.74304644,  0.83471278,  0.90763328,\r\n         0.96017029,  0.99114394,  0.99985864,  0.98611866,  0.95023259,\r\n         0.89300634,  0.8157251 ,  0.72012443,  0.60835131,  0.48291594,\r\n         0.34663532,  0.20257001,  0.05395542, -0.09587089, -0.24354415,\r\n        -0.38574794, -0.51928865, -0.64116727, -0.74864665, -0.83931303],\r\n       [ 0.        ,  0.14943813,  0.29552021,  0.43496553,  0.56464247,\r\n         0.68163876,  0.78332691,  0.86742323,  0.93203909,  0.97572336,\r\n         0.99749499,  0.99686503,  0.97384763,  0.92895972,  0.86320937,\r\n         0.7780732 ,  0.67546318,  0.55768372,  0.42737988,  0.28747801,\r\n         0.14112001, -0.00840725, -0.15774569, -0.30354151, -0.44252044,\r\n        -0.57156132, -0.68776616, -0.78852525, -0.87157577, -0.93505258,\r\n        -0.97753012, -0.99805444, -0.99616461, -0.97190307, -0.92581468,\r\n        -0.85893449, -0.77276449, -0.66923986, -0.55068554, -0.41976402,\r\n        -0.2794155 , -0.13279191,  0.0168139 ,  0.16604211,  0.31154136,\r\n         0.45004407,  0.57843976,  0.69384494,  0.79366786,  0.87566671,\r\n         0.93799998,  0.97926778,  0.99854335,  0.99539378,  0.96988981,\r\n         0.92260421,  0.85459891,  0.76740116,  0.66296923,  0.54364844]])\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.10.1 1.24.3 sys.version_info(major=3, minor=8, micro=17, releaselevel='final', serial=0)\r\nlapack_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/include']\r\nlapack_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/include']\r\nblas_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/include']\r\nblas_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/home\/edumapu\/anaconda3\/envs\/dynet-id\/include']\n```\n","comments":["#18982 seems to describe the same problem.","A simple fix would be to cast everything to doubles.\r\n\r\nOn the other side, I am not sure we should accept all numeric types. No numerical linear algebra related code should be expected to work with integers. So I would opt to throw a `ValueError`. I have never used these functions mysell though so open for more opinions from `signal` people."],"labels":["defect","scipy.signal"]},{"title":"BUG: LinearNDInterpolator >5 times slower than equivalent MATLAB's scatteredInterpolant","body":"### Describe your issue.\r\n\r\nHi,\r\n\r\nI have been updating an old MATLAB source base into a python one but I have come across some difficulty with interpolation.\r\n\r\nWe simulate a signal (ZZ) on a 2D mesh (XX, YY). We then create a linear interpolator using scattered data to get the following inversion map (x, z) -> y.\r\nRoughly speaking, the ranges of X, Y, and Z are [10, 200], [100, 4095], and [-0.5, 0.5] respectively (important due to rescaling).\r\n\r\nWhile I can retrieve the exact results from our MATLAB test suite by\r\n- setting SciPy's `LinearNDInterpolator`'s `rescale` parameter to `True` (MATLAB does this automatically),\r\n- setting MATLAB's `scatteredInterpolant` extrapolation to `False` (MATLAB does this by default)\r\n\r\nCreating the interpolator takes about the same amount of time (< 20 seconds). I found that evaluating the `LinearNDInterpolator` object takes about 5 times longer than evaluating the `scatteredInterpolant` object when evaluating the object on my (somewhat large) input dataset.\r\n\r\nDisclaimer: I'm using the MATLAB software (R2023 Update 5) on Windows (31.7G RAM) and the same version on WSL2 (15.5G RAM), while I run the python code on WSL2 (15.5G RAM). Using the reproduction scripts below on 1 CPU in WSL2, I don't see any significant additional RAM usage during `LinearNDInterpolator`'s evaluation (<0.01G RAM). Using `scatteredInterpolant`, I see about 0.20G additional RAM being used during evaluation.\r\n\r\nThe input data, its data type etc. is all the same and I turned off the parallel pool in my MATLAB client to interference.\r\nStill, I get runtimes of under 2 minutes with MATLAB and runtimes of close to 10 minutes with SciPy using a 40+ millions 2D input dataset.\r\n\r\nI wondered if this was linked to a recent issue (#19547) but isn't QHull used when creating the Delaunay triangulation? Would it have an impact when evaluating the interpolant once the interpolator is created?\r\n\r\nI don't really know what could make it that SciPy's evaluation takes so much longer than that of MATLAB when the rest of the code has been made much faster with Python\/NumPy\/SciPy, so I thought it may be useful submitting this.\r\n\r\n---\r\n\r\nIn the meantime, I looked into ways to parallelize the evaluation of the interpolator using Python's `multiprocessing`'s `starmap` on 10 CPUs, taking #8856 into account (2018) and splitting the dataset in 100 batches. However, the RAM usage is perpetually increasing the evaluation of the 40+ millions points had not finished after 26 minutes when it started hitting the swap, so I terminated the script. I had checked the script finished when using a very small input dataset, so I'm not sure what's causing the RAM to increase like. Since it's not happening on the sequential version, I'm considering it may be more of a `multiprocessing` issue though, like not freeing everything upon transferring the results to the main process.\r\n\r\nI also looked into using `Numba` on the interpolator but these functions aren't supported yet. I'm considering looking into the `threading` library next, so I'll update once it's done.\r\n\r\n---\r\n\r\nNote: in the code block below, I reduced the number of 2D-points to evaluate the interpolant on down to 1 000 000. It took my computer 54 seconds using my WSL2 Python environment and only 4 seconds using my MATLAB environment (R2023a Update 5) in Windows and 3.5 seconds in WSL2. I'd have imagined the MATLAB speedup to be lower than x5 when using smaller datasets but this may be due to using a 1 million 2D points randomly generated dataset rather than our more behaved 40+ millions 2D points physical dataset. I could check tomorrow how the speedup scales using the randomly generated dataset.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\n## Python\r\nfrom time import time\r\nfrom scipy.interpolate import LinearNDInterpolator\r\nfrom numpy import arange, meshgrid, exp\r\nfrom numpy.random import randint, rand\r\n\r\nX = arange(1, 201, 1, dtype=int)\r\nY = arange(0, 4096, 1, dtype=int)\r\nXX, YY = meshgrid(X, Y)\r\nZZ = 1 \/ (1 + exp(-(XX-100)\/100)) - YY\/4095 - 0.5\r\n\r\nnew_X = randint(1, 200, size=1000000)\r\nnew_Z = rand(1000000) - 0.5\r\n\r\ninterpolator = LinearNDInterpolator(list(zip(XX.ravel(), ZZ.ravel())), YY.ravel(), rescale=True)\r\nstart = time()\r\nnew_Y = interpolator(new_X, new_Z)\r\nprint(f\"Elapsed time: {time() - start:.2f}\")\r\n```\r\n\r\n```matlab\r\n%% MATLAB\r\nX  = 1:200;\r\nY  = 0:4095;\r\n[XX, YY] = meshgrid(X, Y);\r\nZZ = 1 .\/ (1 + exp(-(XX-100)\/100)) - YY.\/4095 - 0.5;\r\n\r\nnew_X = randi([1 200], 1, 1000000);\r\nnew_Z = rand(1, 1000000) - 0.5;\r\n\r\ninterpolator = scatteredInterpolant(XX(:), ZZ(:), YY(:), 'linear', 'none');\r\ntic\r\nnew_Y = interpolator(new_X, new_Z);\r\ntoc\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.4 1.26.2 sys.version_info(major=3, minor=11, micro=0, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    version: 0.3.21\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    version: 0.3.21\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1701295040508\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  version: '3.11'\r\n```\r\n","comments":["If your data is on a meshgrid you should use scipy.interpolate.RegularGridInterpolator instead","> If your data is on a meshgrid you should use scipy.interpolate.RegularGridInterpolator instead\r\n\r\nI agree but\r\n- it does not explain why it's so much slower than the MATLAB equivalent which could mean there is an issue in the implementation;\r\n- I'm using meshgrid to compute the signal ZZ, with ZZ somewhat randomly distributed in [-0.5, 0.5], but then I'm using the interpolant to invert the map (XX, YY) -> ZZ into (XX, ZZ) -> YY and `RegularGridInterpolator` requires XX and ZZ to be vectors (so; using vector `x` and making ZZ into a vector of shape `y`, the vectors used to make the XX, YY meshgrid). However, ZZ is not the same vector in every column. And that's without considering it requires the values of the input vectors to be monotonously increasing or decreasing, since I haven't found a way to reduce ZZ into a vector of shape `y` without loss of information anyways.\r\n\r\nSo in another case I would use `RegularGridInterpolator` (and have before) but here I cannot, and I'm doing the same in MATLAB though many times faster there which is why I'm posting this issue (and I'll post on math stackoverflow sometimes later for a more elegant way to invert the map, but which will introduce a breaking change from our code base).","To add to what Jake said above:\r\n\r\n- We do not know, and have no way of knowing, what matlab does under the hood. \r\n\r\n- If you see a way of speeding up SciPy's LinearNDInterpolator, we are very interested indeed.\r\nThe code is https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/interpnd.pyx#L246 + Qhull for triangulation (the sources are in https:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy\/spatial, look for `qhull` in that folder).  \r\n\r\nAlternatively, if you can use GPUs, you may want to keep an eye on https:\/\/github.com\/cupy\/cupy\/pull\/7985. ","> To add to what Jake said above:\r\n> \r\n> * We do not know, and have no way of knowing, what matlab does under the hood.\r\n> * If you see a way of speeding up SciPy's LinearNDInterpolator, we are very interested indeed.\r\n>   The code is https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/interpnd.pyx#L246 + Qhull for triangulation (the sources are in https:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy\/spatial, look for `qhull` in that folder).\r\n> \r\n> Alternatively, if you can use GPUs, you may want to keep an eye on [cupy\/cupy#7985](https:\/\/github.com\/cupy\/cupy\/pull\/7985).\r\n\r\nYeah that's the issue with closed source... It's also why we're moving our code base to Python.\r\n\r\nThanks for letting me know where to find the relevant SciPy sources. I'll try to find some time to look at it, though I'm sure I won't be able to contribute much since you, @j-bowhay and the rest of the SciPy team have already well optimized everything beyond what little lambda users like me can provide.\r\n\r\nFor my short-term use, I'll also look into CuPy as you suggested. Thanks! I'll update this issue if I find anything."],"labels":["scipy.interpolate","query"]},{"title":"stats: Test failure of `test_plot_iv` because of a Python deprecation in Python 3.12","body":"With `'1.12.0.dev0+2038.f34c611'`, I get the following failure when I run the tests with Python 3.12 on Linux.  I haven't investigated further; this might be a problem in maplotlib or one of its dependencies that is out of our hands.\r\n\r\nShort summary:\r\n\r\n    FAILED scipy\/stats\/tests\/test_survival.py::TestSurvival::test_plot_iv - DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal i...\r\n\r\n<details>\r\n<summary>\r\nFull error report\r\n<\/summary>\r\n\r\n```\r\n______________________________________ TestSurvival.test_plot_iv ______________________________________\r\n[gw6] linux -- Python 3.12.0 \/home\/warren\/py3.12.0\/bin\/python3\r\nscipy\/stats\/tests\/test_survival.py:382: in test_plot_iv\r\n    import matplotlib.pyplot as plt  # noqa: F401\r\n        _          = array([False, False, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False,...False, False, False,\r\n       False, False,  True, False, False, False, False, False, False,\r\n       False, False, False])\r\n        n_unique   = 83\r\n        res        = ECDFResult(cdf=EmpiricalDistributionFunction(quantiles=array([0.01327326, 0.01406269, 0.0302765 , 0.03673028, 0.052658...85899,\r\n       0.10301113, 0.08716326, 0.07923933, 0.06933441, 0.04952458,\r\n       0.03961966, 0.02641311, 0.        ])))\r\n        rng        = Generator(PCG64) at 0x7FC434978820\r\n        sample     = CensoredData(uncensored=array([0.8165432 , 0.65078278, 0.26971902, 0.23666593, 0.85843935, 0.19714283, 0.27266022, 0.6...5842522, 0.0302765 , 0.36740916, 0.96576494, 0.39879276, 0.62778709]), interval=array([], shape=(0, 2), dtype=float64))\r\n        self       = <scipy.stats.tests.test_survival.TestSurvival object at 0x7fc33352d2b0>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/pyplot.py:66: in <module>\r\n    from matplotlib.figure import Figure, FigureBase, figaspect\r\n        AbstractContextManager = <class 'contextlib.AbstractContextManager'>\r\n        Enum       = <enum 'Enum'>\r\n        ExitStack  = <class 'contextlib.ExitStack'>\r\n        FigureCanvasBase = <class 'matplotlib.backend_bases.FigureCanvasBase'>\r\n        FigureManagerBase = <class 'matplotlib.backend_bases.FigureManagerBase'>\r\n        MouseButton = <enum 'MouseButton'>\r\n        __annotations__ = {}\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__pycache__\/pyplot.cpython-312.pyc'\r\n        __doc__    = '\\n`matplotlib.pyplot` is a state-based interface to matplotlib. It provides\\nan implicit,  MATLAB-like, way of plotti...y)\\n\\n\\nSee :ref:`api_interfaces` for an explanation of the tradeoffs between the\\nimplicit and explicit interfaces.\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/pyplot.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc336bc83e0>\r\n        __name__   = 'matplotlib.pyplot'\r\n        __package__ = 'matplotlib'\r\n        __spec__   = ModuleSpec(name='matplotlib.pyplot', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc336bc83e0>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/pyplot.py')\r\n        _api       = <module 'matplotlib._api' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/_api\/__init__.py'>\r\n        _docstring = <module 'matplotlib._docstring' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/_docstring.py'>\r\n        _pylab_helpers = <module 'matplotlib._pylab_helpers' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/_pylab_helpers.py'>\r\n        annotations = _Feature((3, 7, 0, 'beta', 1), None, 16777216)\r\n        cast       = <function cast at 0x7fc437e9cea0>\r\n        cbook      = <module 'matplotlib.cbook' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/cbook.py'>\r\n        cm         = <module 'matplotlib.cm' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/cm.py'>\r\n        cycler     = <function cycler at 0x7fc334af47c0>\r\n        functools  = <module 'functools' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/functools.py'>\r\n        get_backend = <function get_backend at 0x7fc33492e700>\r\n        importlib  = <module 'importlib' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/importlib\/__init__.py'>\r\n        inspect    = <module 'inspect' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/inspect.py'>\r\n        interactive = <function interactive at 0x7fc33492e7a0>\r\n        logging    = <module 'logging' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/logging\/__init__.py'>\r\n        matplotlib = <module 'matplotlib' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__init__.py'>\r\n        overload   = <function overload at 0x7fc437e9d580>\r\n        rcParams   = RcParams({'_internal.classic_mode': False,\r\n          'agg.path.chunksize': 0,\r\n          'animation.bitrate': -1,\r\n     ...ize': 2.0,\r\n          'ytick.minor.visible': False,\r\n          'ytick.minor.width': 0.6,\r\n          'ytick.right': False})\r\n        re         = <module 're' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/re\/__init__.py'>\r\n        style      = <module 'matplotlib.style' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/style\/__init__.py'>\r\n        sys        = <module 'sys' (built-in)>\r\n        threading  = <module 'threading' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/threading.py'>\r\n        time       = <module 'time' (built-in)>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/figure.py:43: in <module>\r\n    from matplotlib import _blocking_input, backend_bases, _docstring, projections\r\n        ExitStack  = <class 'contextlib.ExitStack'>\r\n        Integral   = <class 'numbers.Integral'>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__pycache__\/figure.cpython-312.pyc'\r\n        __doc__    = '\\n`matplotlib.figure` implements the following classes:\\n\\n`Figure`\\n    Top level `~matplotlib.artist.Artist`, which...f:`user_interfaces` for a\\nlist of examples) .  More information about Figures can be found at\\n:ref:`figure-intro`.\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/figure.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333600e0>\r\n        __name__   = 'matplotlib.figure'\r\n        __package__ = 'matplotlib'\r\n        __spec__   = ModuleSpec(name='matplotlib.figure', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333600e0>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/figure.py')\r\n        inspect    = <module 'inspect' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/inspect.py'>\r\n        itertools  = <module 'itertools' (built-in)>\r\n        logging    = <module 'logging' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/logging\/__init__.py'>\r\n        mpl        = <module 'matplotlib' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__init__.py'>\r\n        np         = <module 'numpy' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/numpy\/__init__.py'>\r\n        threading  = <module 'threading' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/threading.py'>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections\/__init__.py:55: in <module>\r\n    from .. import axes, _docstring\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections\/__pycache__\/__init__.cpython-312.pyc'\r\n        __doc__    = '\\nNon-separable transforms that map from data space to screen space.\\n\\nProjections are defined as `~.axes.Axes` subc...y\/misc\/custom_projection`.  The polar plot functionality in\\n`matplotlib.projections.polar` may also be of interest.\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a4680>\r\n        __name__   = 'matplotlib.projections'\r\n        __package__ = 'matplotlib.projections'\r\n        __path__   = ['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections']\r\n        __spec__   = ModuleSpec(name='matplotlib.projections', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a4680...__init__.py', submodule_search_locations=['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections'])\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/__init__.py:2: in <module>\r\n    from ._axes import *\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/__pycache__\/__init__.cpython-312.pyc'\r\n        __doc__    = None\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a43e0>\r\n        __name__   = 'matplotlib.axes'\r\n        __package__ = 'matplotlib.axes'\r\n        __path__   = ['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes']\r\n        __spec__   = ModuleSpec(name='matplotlib.axes', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a43e0>, orig...b\/axes\/__init__.py', submodule_search_locations=['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes'])\r\n        _base      = <module 'matplotlib.axes._base' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/_base.py'>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/_axes.py:11: in <module>\r\n    import matplotlib.category  # Register category unit converter as side effect.\r\n        Integral   = <class 'numbers.Integral'>\r\n        Number     = <class 'numbers.Number'>\r\n        Real       = <class 'numbers.Real'>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/__pycache__\/_axes.cpython-312.pyc'\r\n        __doc__    = None\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/_axes.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a4c20>\r\n        __name__   = 'matplotlib.axes._axes'\r\n        __package__ = 'matplotlib.axes'\r\n        __spec__   = ModuleSpec(name='matplotlib.axes._axes', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a4c20>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/_axes.py')\r\n        functools  = <module 'functools' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/functools.py'>\r\n        itertools  = <module 'itertools' (built-in)>\r\n        logging    = <module 'logging' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/logging\/__init__.py'>\r\n        ma         = <module 'numpy.ma' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/numpy\/ma\/__init__.py'>\r\n        math       = <module 'math' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/lib-dynload\/math.cpython-312-x86_64-linux-gnu.so'>\r\n        mpl        = <module 'matplotlib' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__init__.py'>\r\n        np         = <module 'numpy' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/numpy\/__init__.py'>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/category.py:14: in <module>\r\n    import dateutil.parser\r\n        OrderedDict = <class 'collections.OrderedDict'>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/__pycache__\/category.cpython-312.pyc'\r\n        __doc__    = '\\nPlotting of string \"category\" data: ``plot([\\'d\\', \\'f\\', \\'a\\'], [1, 2, 3])`` will\\nplot three points with x-axis ... a tick locator, a tick formatter, and the\\n`.UnitData` class that creates and stores the string-to-integer mapping.\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/category.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333dede0>\r\n        __name__   = 'matplotlib.category'\r\n        __package__ = 'matplotlib'\r\n        __spec__   = ModuleSpec(name='matplotlib.category', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333dede0>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/category.py')\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/__init__.py:2: in <module>\r\n    from ._parser import parse, parser, parserinfo, ParserError\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/__pycache__\/__init__.cpython-312.pyc'\r\n        __doc__    = None\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a6120>\r\n        __name__   = 'dateutil.parser'\r\n        __package__ = 'dateutil.parser'\r\n        __path__   = ['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser']\r\n        __spec__   = ModuleSpec(name='dateutil.parser', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a6120>, orig...parser\/__init__.py', submodule_search_locations=['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser'])\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/_parser.py:50: in <module>\r\n    from .. import tz\r\n        Decimal    = <class 'decimal.Decimal'>\r\n        StringIO   = <class '_io.StringIO'>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/__pycache__\/_parser.cpython-312.pyc'\r\n        __doc__    = '\\nThis module offers a generic date\/time string parser which is able to parse\\nmost known formats to represent a date...>`_\\n- `Java SimpleDateFormat Class\\n  <https:\/\/docs.oracle.com\/javase\/6\/docs\/api\/java\/text\/SimpleDateFormat.html>`_\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/_parser.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a6300>\r\n        __name__   = 'dateutil.parser._parser'\r\n        __package__ = 'dateutil.parser'\r\n        __spec__   = ModuleSpec(name='dateutil.parser._parser', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a6300>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/_parser.py')\r\n        datetime   = <module 'datetime' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/datetime.py'>\r\n        integer_types = (<class 'int'>,)\r\n        monthrange = <function monthrange at 0x7fc436e52e80>\r\n        re         = <module 're' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/re\/__init__.py'>\r\n        relativedelta = <module 'dateutil.relativedelta' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/relativedelta.py'>\r\n        six        = <module 'six' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/six.py'>\r\n        string     = <module 'string' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/string.py'>\r\n        text_type  = <class 'str'>\r\n        time       = <module 'time' (built-in)>\r\n        unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 2097152)\r\n        warn       = <built-in function warn>\r\n        warnings   = <module 'warnings' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/warnings.py'>\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/__init__.py:2: in <module>\r\n    from .tz import *\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/__pycache__\/__init__.cpython-312.pyc'\r\n        __doc__    = None\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a72f0>\r\n        __name__   = 'dateutil.tz'\r\n        __package__ = 'dateutil.tz'\r\n        __path__   = ['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz']\r\n        __spec__   = ModuleSpec(name='dateutil.tz', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a72f0>, origin='...ateutil\/tz\/__init__.py', submodule_search_locations=['\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz'])\r\n\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/tz.py:37: in <module>\r\n    EPOCH = datetime.datetime.utcfromtimestamp(0)\r\nE   DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\r\n        OrderedDict = <class 'collections.OrderedDict'>\r\n        ZERO       = datetime.timedelta(0)\r\n        _TzOffsetFactory = <class 'dateutil.tz._factories._TzOffsetFactory'>\r\n        _TzSingleton = <class 'dateutil.tz._factories._TzSingleton'>\r\n        _TzStrFactory = <class 'dateutil.tz._factories._TzStrFactory'>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/__pycache__\/tz.cpython-312.pyc'\r\n        __doc__    = '\\nThis module offers timezone implementations subclassing the abstract\\n:py:class:`datetime.tzinfo` type. There are c...), given ranges (with help\\nfrom relative deltas), local machine timezone, fixed offset timezone, and UTC\\ntimezone.\\n'\r\n        __file__   = '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/tz.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a7410>\r\n        __name__   = 'dateutil.tz.tz'\r\n        __package__ = 'dateutil.tz'\r\n        __spec__   = ModuleSpec(name='dateutil.tz.tz', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7fc3333a7410>, origin='\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/tz.py')\r\n        __warningregistry__ = {'version': 22181}\r\n        _thread    = <module '_thread' (built-in)>\r\n        _tzinfo    = <class 'dateutil.tz._common._tzinfo'>\r\n        _validate_fromutc_inputs = <function _validate_fromutc_inputs at 0x7fc334257c40>\r\n        bisect     = <module 'bisect' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/bisect.py'>\r\n        datetime   = <module 'datetime' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/datetime.py'>\r\n        enfold     = <function enfold at 0x7fc334257ba0>\r\n        os         = <module 'os' (frozen)>\r\n        six        = <module 'six' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/six.py'>\r\n        string_types = (<class 'str'>,)\r\n        struct     = <module 'struct' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/struct.py'>\r\n        sys        = <module 'sys' (built-in)>\r\n        time       = <module 'time' (built-in)>\r\n        tzname_in_python2 = <function tzname_in_python2 at 0x7fc334257b00>\r\n        tzrangebase = <class 'dateutil.tz._common.tzrangebase'>\r\n        tzwin      = None\r\n        tzwinlocal = None\r\n        warn       = <built-in function warn>\r\n        weakref    = <module 'weakref' from '\/home\/warren\/py3.12.0\/lib\/python3.12\/weakref.py'>\r\n\r\n```\r\n\r\n<\/details>\r\n","comments":["> this might be a problem in maplotlib or one of its dependencies that is out of our hands.\r\n\r\nLooks like it, since the offending line appears to be `import matplotlib.pyplot as plt`. Can you try just that import on the same system?","Right, the warning arises during the import of `matplotlib.pyplot`:\r\n\r\n```\r\n$ python3 -Werror -c \"import matplotlib.pyplot\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/pyplot.py\", line 66, in <module>\r\n    from matplotlib.figure import Figure, FigureBase, figaspect\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/figure.py\", line 43, in <module>\r\n    from matplotlib import _blocking_input, backend_bases, _docstring, projections\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/projections\/__init__.py\", line 55, in <module>\r\n    from .. import axes, _docstring\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/__init__.py\", line 2, in <module>\r\n    from ._axes import *\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/axes\/_axes.py\", line 11, in <module>\r\n    import matplotlib.category  # Register category unit converter as side effect.\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/matplotlib\/category.py\", line 14, in <module>\r\n    import dateutil.parser\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/__init__.py\", line 2, in <module>\r\n    from ._parser import parse, parser, parserinfo, ParserError\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/parser\/_parser.py\", line 50, in <module>\r\n    from .. import tz\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/__init__.py\", line 2, in <module>\r\n    from .tz import *\r\n  File \"\/home\/warren\/py3.12.0\/lib\/python3.12\/site-packages\/dateutil\/tz\/tz.py\", line 37, in <module>\r\n    EPOCH = datetime.datetime.utcfromtimestamp(0)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nDeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\r\n\r\n```\r\n\r\nThe root cause is in the `dateutil` module that is used by matplotlib; see https:\/\/github.com\/dateutil\/dateutil\/pull\/1285, and the request for an updated release of dateutil at https:\/\/github.com\/dateutil\/dateutil\/issues\/1314.","@WarrenWeckesser Usually this kind of issue can be worked around by adding this warning to the ignore list here https:\/\/github.com\/scipy\/scipy\/blob\/d664abf70dda7b1ba1322f6abe1d4b55db6bdfee\/pytest.ini#L18"],"labels":["scipy.stats","upstream bug"]},{"title":"MAINT,BUG: Start reworking flapack to use correct f2py character types","body":"Suggestions welcome\r\n\r\n(not ready for a full review, there are quite a few cases to cover)\r\n\r\n\r\n----------------------\r\nCloses https:\/\/github.com\/numpy\/numpy\/issues\/25286, with https:\/\/github.com\/numpy\/numpy\/pull\/25287.\r\n\r\nFor a while there was special casing through a BC hook (https:\/\/github.com\/numpy\/numpy\/pull\/19388#issuecomment-910646779).","comments":["Is this a NumPy 2.0 `f2py` change? Because otherwise we will be in trouble until then. ","> Is this a NumPy 2.0 `f2py` change? Because otherwise we will be in trouble until then.\r\n\r\nNo, this came in over at [v1.24.0](https:\/\/github.com\/numpy\/numpy\/releases\/tag\/v1.24.0) almost a year ago, though at the time a [BC hook was added](https:\/\/github.com\/numpy\/numpy\/pull\/19388#issuecomment-910646779), which uses more global state I'd like to avoid in 2.0.0 and also isn't playing nicely with the `meson` backend, so I thought I'd just fix the root cause, i.e. make `flapack` signatures match the (now correct) `f2py` types.","I see, that linked issue is something that took lapack world a bit off-guard indeed but then why does it started affecting the CI jobs only now? ","> I see, that linked issue is something that took lapack world a bit off-guard indeed but then why does it started affecting the CI jobs only now?\r\n\r\nWell, that does have to do with the `meson` backend (https:\/\/github.com\/numpy\/numpy\/issues\/25286), which isn't calling the hook reliably.... AFAIK `scipy` is the only project using these many `pyf` features, so I thought it would be better to fix here than continue the hook for keeping older (incorrect) behavior.","No I mean, did you already merge something on the NumPy side before we fixed anything here so that the CI is failing to build? ","> No I mean, did you already merge something on the NumPy side before we fixed anything here so that the CI is failing to build?\r\n\r\nYup, but we really can't revert that (see [this issue](https:\/\/github.com\/numpy\/numpy\/issues\/25263)). There are a host of other changes introduced with the `meson` backend, but most if not all of them are very necessary (from an `f2py` perspective at any rate).\r\n\r\nI will have this ready to go by the Sunday evening at the latest..\r\n\r\nThe only non-`scipy` \"fix\" I can think of would be to add back the parse to generate incorrect bindings but I'd really rather not do that. Is there a reason to prefer a change to `numpy` over fixing these here? @ilayn ","OK let me understand, we have the very famous char issue. But what you merged is not related to that one so I don't quite follow how things hit us in the end. I don't follow the relationship between https:\/\/github.com\/numpy\/numpy\/issues\/25263 and the char issue. \r\n\r\nSo clearly something is breaking change and why we cannot revert is the part I don't understand. \r\n","> OK let me understand, we have the very famous char issue. But what you merged is not related to that one so I don't quite follow how things hit us in the end. I don't follow the relationship between [numpy\/numpy#25263](https:\/\/github.com\/numpy\/numpy\/issues\/25263) and the char issue.\r\n> \r\n> So clearly something is breaking change and why we cannot revert is the part I don't understand.\r\n\r\nOK, sorry it is a bit convoluted, but essentially:\r\n- https:\/\/github.com\/numpy\/numpy\/pull\/25181 fixed a bunch of bugs within `f2py` (and has tests)\r\n- The `meson` backend expects the generated `.pyf` to be the same as the one compiled by `distutils` (this unfortunately was never true, see for example https:\/\/github.com\/numpy\/numpy\/pull\/25287#issuecomment-1835986872)\r\n- It seems that the only way `scipy` was building was basically luck (e.g. see the changes to the `#defines` in https:\/\/github.com\/numpy\/numpy\/pull\/25287 for example\r\n\r\nEssentially, the only time it was building was evidently when it left the rest of the numpy unit tests in a lurch. I understand `scipy` is a major downstream user, but we can't really keep valid bugfixes out of `f2py` because they'd require changes to `scipy`...\r\n\r\nAs far as I can tell, `scipy` builds even when `f2py` generates completely bogus bindings, which is a bad sign anyway.\r\n\r\nFWIW I spent almost all day bisecting `f2py` trying to narrow down the exact issue, but as best I can tell so far this seems to be the best option (other than reverting a bunch of valid, tested bugfixes in `f2py`).\r\n\r\nAs noted though a revert isn't an option, [restoring the global hook](https:\/\/github.com\/numpy\/numpy\/pull\/19388#issuecomment-910646779) based BC thing can be done, but I don't see why that would be better than fixing the bindings?","Is there a simple \"formula\" for fixing the bindings on our end, or does it require expert inspection on a case-by-case basis?","> Is there a simple \"formula\" for fixing the bindings on our end, or does it require expert inspection on a case-by-case basis?\r\n\r\nIt's fairly mechanical. As can be seen, its a matter of changing the incorrect `callprotoargument char*` values to `char` and also making similar changes to the `check` calls. It isn't wholly automated, because some of the checks can actually take `char*`, so I'm keeping the documentation open next to me. I'd expect `scipy`'s existing test suite to suffice once I'm done making changes.. ","OK I fixed the bugs in `numpy` https:\/\/github.com\/numpy\/numpy\/pull\/25287\r\nthough I still think this is worth working through, let me know if not :)",">  I understand scipy is a major downstream user, but we can't really keep valid bugfixes out of f2py because they'd require changes to scipy...\r\n\r\nTrue but we could have coordinated this better before breaking the tests and builds. So I'm still not understanding why we broke first and fixing it later. NumPy itself is not using `f2py` so SciPy is the test case here. Probably same with other users of f2py.\r\n\r\nI appreciate the ambition but there are millions of other things about SciPy itself that I want to do but can't because of backwards compatibility concerns. ","> >  I understand scipy is a major downstream user, but we can't really keep valid bugfixes out of f2py because they'd require changes to scipy...\n> \n> True but we could have coordinated this better before breaking the tests and builds. So I'm still not understanding why we broke first and fixing it later. NumPy itself is not using `f2py` so SciPy is the test case here. Probably same with other users of f2py.\n\nWell, yeah this was on me. I forgot to test scipy for some of the merged PRs. @tylerjereddy is super proactive in reporting changes. Thankfully this is an increasingly unlikely situation since the test suite in numpy is getting better. \n\nAlso in general as we ramp up to 2.0 we're less conservative about changes, so fixes to f2py, even breaking ones are more acceptable for a bit. \n\nI'd prefer more communication as well, eg scipy vendored the src templating which should have been (and now is) part of f2py. \n\n> I appreciate the ambition but there are millions of other things about SciPy itself that I want to do but can't because of backwards compatibility concerns. \n\nYeah but this is a very weird situation IMO. As noted scipy is one of the de facto test cases, but it's using outdated and incorrect bindings which were fixed over a year ago. From an f2py perspective \"fixing our test suite\" even if it exists in SciPy makes sense to me. \n","What's the timeline for 1.24.0 to be the lowest supported version? I could spread out this effort to match that. ","> Yeah but this is a very weird situation IMO. As noted scipy is one of the de facto test cases, but it's using outdated and incorrect bindings which were fixed over a year ago. \r\n\r\nStill don't follow. If this was the case, we should have seen it a year ago. Clearly something very recent broke the whole thing. So why don't we revert that change and then fix what we are doing here? \r\n\r\nThis is not really a good way of dealing with the issue because this is blocking the build. If it was failing a few tests it wouldn't matter but it fails to build so our CI job is broken. ","> > Yeah but this is a very weird situation IMO. As noted scipy is one of the de facto test cases, but it's using outdated and incorrect bindings which were fixed over a year ago. \n> \n> Still don't follow. If this was the case, we should have seen it a year ago. Clearly something very recent broke the whole thing. So why don't we revert that change and then fix what we are doing here? \n\nThere wasn't really anything to revert, just some additional handling for pyf files. \n\n> This is not really a good way of dealing with the issue because this is blocking the build. If it was failing a few tests it wouldn't matter but it fails to build so our CI job is broken. \n\nAh no I agree. The build should be green again anyway, I landed a patch for it..","> OK I fixed the bugs in `numpy` https:\/\/github.com\/numpy\/numpy\/pull\/25287\n> though I still think this is worth working through, let me know if not :)\n\n@ilayn this was merged to fix the build, @tylerjereddy mentioned it re established earlier build profiles. If it's still breaking on an f2py change could you open a new bug report? It was tested on Linux and macos (M1). ","I am talking about the CHECKSCALAR failures here \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/actions\/runs\/7063166455\/job\/19228549856?pr=19560","> I am talking about the CHECKSCALAR failures here \n> \n> https:\/\/github.com\/scipy\/scipy\/actions\/runs\/7063166455\/job\/19228549856?pr=19560\n\nThat job is from twelve hours ago. Could you try it with the current numpy main? After the PR mentioned?\n\nAlso the checkscalar fix is the one in the PR, has a regression test too. ","Let me try that but it will take some time to recreate that flow","After getting everything up to the latest greatest, the build is not blocked locally too. Thanks @HaoZeke "],"labels":["scipy.linalg","maintenance"]},{"title":"BUG: Linprog deprecated","body":"### Describe your issue.\n\nMethod linprog from scipy.optimize doesn't actually detect method = 'highs' and says I am using something else, which won't let me solve LP problems\n\n### Reproducing Code Example\n\n```python\ndef two_phase_method(c, A, b):\r\n\r\n    num_original_vars = len(c)\r\n    num_artificial_vars = len(b)\r\n    A_artificial = np.hstack((A, np.eye(num_artificial_vars)))\r\n    c_artificial = np.hstack((np.zeros(len(c)), np.ones(num_artificial_vars)))\r\n\r\n    res_phase1 = linprog(c_artificial, A_eq=A_artificial, b_eq=b, method='highs')\r\n    if not res_phase1.success:\r\n        raise ValueError(\"No feasible solution found in Phase 1\")\r\n\r\n    x0_original = res_phase1.x[:num_original_vars]\r\n\r\n    res_phase2 = linprog(c, A_eq=A, b_eq=b, method='highs', x0=x0_original)\r\n    return res_phase2\n```\n\n\n### Error message\n\n```shell\n`method='{meth}'` is deprecated and will be removed in SciPy 1.11.0. Please use one of the HiGHS solvers (e.g. `method='highs'`) in new code.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nScipy 1.11.4\r\nPython 3.12\r\nNumpy 1.26.1\n```\n","comments":["Hi @mariusmarin98 , I haven't been able to reproduce the error message. Please can you give us a single call to `linprog`, including the values of the arguments, which gives you this error? There's certainly something strange going on, as the f-string to fill the message with `meth` doesn't seem to be working.","Try copy-pasting `'highs'` from the error message into the code. (Suggesting from experience dealing with cyrillic\/cp1241\/co866 code pages symbols being mixed in). "],"labels":["defect","scipy.optimize"]},{"title":"BUG: Bug in `scipy.sparse.linalg.svds` for large sparse matrices with a specific size","body":"### Describe your issue.\n\nI found a bug in `scipy.sparse.linalg.svds` when using a (65536, 65536) sparse matrix. It incorrectly raises an \"empty matrix\" error.\r\n\r\nThe issue is in the `_iv` function called by `svds`. It uses `np.prod(A.shape)` to check if the matrix is empty. This value is supposed to represent the total number of entries. However, for a matrix of shape (65536, 65536), which has 2^32 entries, this causes a 32-bit integer overflow, making the value 0. This results in a false empty check.\r\n\r\nProblematic code: [GitHub Link](https:\/\/github.com\/scipy\/scipy\/blob\/5f17ac3ad97c7f41c18bd3286406e5b93f76e7d3\/scipy\/sparse\/linalg\/_eigen\/_svds.py#L41).\r\n\r\nWe can simply replace that part with `any(x == 0 for x in A.shape)` or other robust codes.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.sparse import csc_array\r\nfrom scipy.sparse.linalg import svds\r\n\r\nmat = csc_array((65536, 65536))\r\n_, s, _ = svds(mat)\r\nprint(s[0])\n```\n\n\n### Error message\n\n```shell\nValueError: `A` must not be empty.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.8.1 1.22.3 sys.version_info(major=3, minor=10, micro=13, releaselevel='final', serial=0)\r\n\r\n(Note that the problematic part also exists in the current main.)\n```\n","comments":[],"labels":["defect","scipy.sparse.linalg"]},{"title":"MAINT: potential ndimage follow-ups","body":"Following gh-19576:\r\n\r\n- [ ] there seem to be quite a few `ndimage` tests that use the old `assert_array_almost_equal` in cases where integer types are expected, if not guaranteed, and where i.e., the stricter `assert_array_equal` for exact matches on integer types would be appropriate to enforce\r\n- [ ] may be good to double check that there are no creative ways to avoid the wider integer type used in gh-19576 (could be more performant\/memory efficient, but anyway that PR fixes a bug and matches `scikit-image` behavior for watershed, so IMO performance\/memory footprint regression is allowed for bug fix)","comments":[],"labels":["scipy.ndimage","maintenance"]},{"title":"BUG: Bug in scipy.special.struve, returns nan","body":"### Describe your issue.\n\nWhen calculating the Struve function for 25.765368073883174 it returns a nan. It should be a zero (or close to that).\n\n### Reproducing Code Example\n\n```python\nIn [1]: import scipy.special as sp_special\r\n\r\nIn [2]: sp_special.struve(0, 25.765368073883174)\r\nOut[2]: nan\r\n\r\nIn [3]: sp_special.struve(0, 25.765368073883174+0.001)\r\nOut[3]: 0.00015522631565283182\r\n\r\nIn [4]: sp_special.struve(0, 25.765368073883174-0.001)\r\nOut[4]: -0.00015433122211300148\n```\n\n\n### Error message\n\n```shell\nIn [2]: sp_special.struve(0, 25.765368073883174)\r\nOut[2]: nan\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.1 1.24.3 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/include\r\n    lib directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/include\r\n    lib directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/lib\r\n    name: mkl-sdl\r\n    openblas configuration: unknown\r\n    pc file directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/lib\/pkgconfig\r\n    version: '2023.1'\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/opt\/LIBRARIES\/ANACONDA3_2023\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1691606680890\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1691606680890\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1691606680890\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/LIBRARIES\/ANACONDA3_2023\/bin\/python\r\n  version: '3.11'\n```\n","comments":["Hello, I would like to work on this issue.\r\nIs that possible?\r\n\r\nThanks in advance"],"labels":["defect","scipy.special"]},{"title":"BUG: test_doccer:test_decorator fails with Python 3.13 due to dedented docstring constants","body":"### Describe your issue.\n\nWhen I try to build scipy 1.11.3 in Fedora with Python 3.13.0a1+ I see the following test failure:\r\n\r\n```\r\n________________________________ test_decorator ________________________________\r\n[gw1] linux -- Python 3.13.0 \/usr\/bin\/python3\r\n\r\n    @pytest.mark.skipif(DOCSTRINGS_STRIPPED, reason=\"docstrings stripped\")\r\n    def test_decorator():\r\n        with suppress_warnings() as sup:\r\n            sup.filter(category=DeprecationWarning)\r\n            # with unindentation of parameters\r\n            decorator = doccer.filldoc(doc_dict, True)\r\n    \r\n            @decorator\r\n            def func():\r\n                \"\"\" Docstring\r\n                %(strtest3)s\r\n                \"\"\"\r\n>           assert_equal(func.__doc__, \"\"\" Docstring\r\n                Another test\r\n                   with some indent\r\n                \"\"\")\r\nE           AssertionError: \r\nE           Items are not equal:\r\nE            ACTUAL: 'Docstring\\nAnother test\\n   with some indent\\n'\r\nE            DESIRED: ' Docstring\\n            Another test\\n               with some indent\\n            '\r\n\r\nscipy\/misc\/tests\/test_doccer.py:86: AssertionError\r\n```\r\n\r\nThis is likely caused by https:\/\/github.com\/python\/cpython\/issues\/81283 -- *Automatically dedent docstring constants by default*.\r\n\r\nI tried to get around this by asserting the docstrings are equal to manual dostrings, e.g.:\r\n\r\n```python\r\n        @decorator\r\n        def func():\r\n            \"\"\" Docstring\r\n            %(strtest3)s\r\n            \"\"\"\r\n\r\n        def expected():\r\n            \"\"\" Docstring\r\n            Another test\r\n               with some indent\r\n            \"\"\"\r\n        assert_equal(func.__doc__, expected.__doc__)\r\n```\r\n\r\nBut I think the dosctring of the original function is dedented first and the substitution happens after; hence the results are not the same as dedented manually constructed docstrings.\r\n\r\nI could add if sys.version around the expected value, but I am unsure if that is the proper fix. Before I submit a PR that makes changes to the test, I need to understand how we want this to behave on Python 3.13.\n\n### Reproducing Code Example\n\n```python\nrun the tests\n```\n\n\n### Error message\n\n```shell\nsee above\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nI am no longer able to paste the command above. This is on scipy 1.11.3 with numpy 1.26.0 and Python built from the main branch.\n```\n","comments":["I wonder what happens to the doc build under python 3.13. `doccer` is mainly used for constructing docstrings in `stats` and `ndimage`, so I suppose the right fix is to keep the docstrings tight. Any chance you can build the docs locally with the dev python version @hroncok?\r\n\r\nWe can of course just skip\/xfail the test itself for the time being, until we happen to try building the docs on python 3.13, whenever this is going to be :-).","> Any chance you can build the docs locally with the dev python version @hroncok?\r\n\r\nSorry, I must have missed this question. Will check.","I seem to be blocked by greenlet and pyzmq not yet working with Python 3.13 and `make dist` in the doc directory trying to install them.\r\n\r\n```\r\npyzmq>=24 (from ipykernel->myst-nb->-r ..\/requirements\/doc.txt (line 9))\r\ngreenlet!=0.4.17 (from sqlalchemy<3,>=1.3.12->jupyter-cache>=0.5->myst-nb->-r ..\/requirements\/doc.txt (line 9))\r\n```","Cross-ref https:\/\/github.com\/scipy\/scipy\/pull\/20046 for myst-nb which we have to pin to <1.0.\n\nLooks like this is going to be a problem when python 3.13 is out, so about Oct 2024.","That still needs pyzmq and greenlet.","Absolutely. \nI suspect these are transitive dependencies of myst-nb. (Does bare sphinx need pyzmq?) \n\nAlso my comment above \"this will become a problem in October\" is poor wording. Meant to mean \"will become a release blocker level problem in October\", great to have a forewarning now. ","> I suspect these are transitive dependencies of myst-nb.\r\n\r\nYes. The dependency chain is in my comment.\r\n\r\n> Does bare sphinx need pyzmq?\r\n\r\nNo."],"labels":["defect","scipy.misc","maintenance"]},{"title":"ENH: Deal with integration with fixed samples when the boundaries are known","body":"### Is your feature request related to a problem? Please describe.\n\nSometimes you want to integrate a function for which you have fixed samples, but you want to integrate it over a domain that may be larger (or maybe also smaller) than the range of the sampled points. For example, you may know the values of the function at points $\\mathbf{t} = (0.1, 0.3, 0.4, 0.7, 0.8)$ but you want to integrate it over the interval $(0, 1)$. In this case you cannot use the functions that Scipy provides for integrating with a fixed number of samples, as they would assume that the integration domain is $(0.1, 0.8)$ in this case. \n\n### Describe the solution you'd like.\n\nAdd parameters `a` and `b`, similar to the ones in [`quad`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.integrate.quad.html#scipy.integrate.quad) to the functions that compute quadratures based on a fixed number of samples (such as [`simpson`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.integrate.simpson.html#scipy.integrate.simpson)). The quadrature should then be computed in the domain defined by these parameters, if present.\n\n### Describe alternatives you've considered.\n\nIt could be possible to estimate via extrapolation the values at the boundaries and concatenate them before calling the corresponding quadrature method. I do not know if this is a good choice, though, or if there are better alternatives.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["If this were fit into a one-step function, it would need other arguments to specify what assumptions should be made past the known endpoints. What did you have in mind for that part of the interface?\r\n\r\nI'd think that the most reliable result would come from a multi-step process: [interpolate with extrapolation](https:\/\/docs.scipy.org\/doc\/scipy\/tutorial\/interpolate\/extrapolation_examples.html), confirm that what is done past the edge of known data is as expected (adjusting as needed), then integrate the interpolant."],"labels":["enhancement","scipy.integrate"]},{"title":"ENH: Add optimization method \"Barycenter method\" to optimizers","body":"### Is your feature request related to a problem? Please describe.\n\nThis feature is not related to a problem, but an extension to current library. \n\n### Describe the solution you'd like.\n\nThe here-required optimization method is called the \"Barycenter Method\". His name originates from calculating the barycenter respective to some Oracle map based on evaluation points. Each point evaluates to a scalar quantity, and the barycenter weight is the exponential of the scaled negative of this value. The higher the value, the lower the weight. It has \r\na gradient behaviour, but it derivative-free. I would be glad to have this method incorporated to scipy, since I worked with on my Master thesis. \r\n\r\n[1] https:\/\/arxiv.org\/abs\/2102.10467 \n\n### Describe alternatives you've considered.\n\nCurrently, I have this python version: https:\/\/github.com\/asmove\/pybary . I would be very grateful for any feedback in this regard.\r\n\r\n Python\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Thanks @brunolnetto I would suggest posting this to the mailing list too before starting any work so as not to waste effort. Are you able to provide any benchmarks to show if this is able to outperform the existing methods in scipy on example problems? The method does not seem to be very widespread or highly cited so we would need some motivating examples I suspect","Thanks to the fast response.\n\nHow do I add it to mailing? :-S\n\nSome methods to compare are Nelder-mead, gradient-descent, Newton-Raphson, stochastic gradient descent. Every method has its hyperparameters, I can stablish some metrics like convergence speed and computation effort, but it would require comparable hyperparameter equivalence. I will see what I do. ","> How do I add it to mailing? :-S\r\n\r\nYou can start a new thread here https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/ :)\r\n\r\nYou can find our guidance for suggesting and implementing new features at [Contributing new code](https:\/\/docs.scipy.org\/doc\/scipy\/dev\/hacking.html#contributing-new-code).","We have a fairly extensive existing set of benchmarks\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/benchmarks\/benchmarks\/optimize.py#L265","I tried something like `from scipy.benchmarks.benchmarks.optimize import bench_run`, and got an error. I am not very familiar with the scipy pipelines and workflows. Would you mind writing down the proper code to run the benchmarks? Thanks!  \r\n\r\n","Oh, silly me, it is a class method \ud83d\ude05. It seems, however, that the underline before naming `_BenchOptimizers(Benchmark)` makes the class protected. I still need some guidelines on how to benchmark run here.\r\n\r\n","Maybe this guide to benchmarks will help https:\/\/docs.scipy.org\/doc\/scipy\/dev\/contributor\/benchmarking.html","@mattip It may be some miss-setup of my machine: the very first command run `python dev.py bench` stumbled on the below error. A fix to `console.print(f'[red bold] {msg}')` did not solve the issue.\r\n\r\n```\r\n  File \"dev.py\", line 210\r\n    console.print(\"[red bold] msg\")\r\n                ^\r\nSyntaxError: invalid syntax\r\n```","This may be due to some problem earlier in `dev.py`. Perhaps you have some inadvertent modifications of your sources: make sure `git status` does not show anything unintentional. If you can't solve it, please open another issue with a full reproducer: what version of python, what platform, how you installed the dependencies. ","I'm happy to help debug in a new issue if you would like to avoid creating noise here.","@lucascolley refer to issue [#19571](https:\/\/github.com\/scipy\/scipy\/issues\/19571). "],"labels":["enhancement","scipy.optimize"]},{"title":"Developer installation fails with poetry","body":"I followed the instructions from this guide [Building from source for SciPy development](http:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source) and everything worked well including the actual build via:\r\n```bash\r\npython dev.py build\r\n```\r\nNote that I did NOT install the doc and typing\/linting dependencies, as I only need the latest `main` branch to work inside a python environment for some testing regarding #14740 .\r\nThis is the terminal output directly after the build, which for me looks good:\r\n```bash\r\n[1606\/1606] Linking target scipy\/optimize\/_highs\/_highs_wrapper.cpython-310-x86_64-linux-gnu.so\r\nBuild OK\r\n\ud83d\udcbb  meson install -C build --only-changed\r\nInstalling, see meson-install.log...\r\nInstallation OK\r\n```\r\n\r\nTo get an interpreter with the latest SciPy source, I tried\r\n```bash\r\npython dev.py ipython\r\n```\r\nand also \r\n```\r\npython dev.py python\r\n```\r\nbut both fail with\r\n```bash\r\n ...\r\n  File \"\/home\/xxx\/scipy\/dev.py\", line 536, in install_project                                                              \r\n    raise RuntimeError(\"Can't install in non-empty directory: \"                                                            \r\nRuntimeError: Can't install in non-empty directory: '\/home\/xxx\/scipy\/build-install' \r\n```\r\nI was so bold and deleted the `build-install` directory, repeating `python dev.py ipython` then gives:\r\n```bash\r\n ...\r\n  File \"\/home\/xxx\/scipy\/dev.py\", line 1161, in run                                                                         \r\n    import IPython                                                                                                         \r\nModuleNotFoundError: No module named 'IPython'  \r\n```\r\nhowever `python dev.py python` then actually spawns an interpreter, but then I get\r\n```python\r\n>>> import scipy                                                                                                           \r\nTraceback (most recent call last):                                                                                         \r\n  File \"\/home\/xxx\/scipy\/__init__.py\", line 51, in <module>                                                                 \r\n    from scipy.__config__ import show as show_config                                                                       \r\nModuleNotFoundError: No module named 'scipy.__config__'                                                                    \r\n                                                                                                                           \r\nThe above exception was the direct cause of the following exception:                                                       \r\n                                                                                                                           \r\nTraceback (most recent call last):                                                                                         \r\n  File \"<console>\", line 1, in <module>                                                                                    \r\n  File \"\/home\/xxx\/scipy\/__init__.py\", line 56, in <module>                                                                 \r\n    raise ImportError(msg) from e                                                                                          \r\nImportError: Error importing SciPy: you cannot import SciPy while                                                          \r\n    being in scipy source directory; please exit the SciPy source                                                          \r\n    tree first and relaunch your Python interpreter.\r\n```\r\nSo here I am stuck, as executing\r\n```bash\r\npython -c \"import scipy; print(scipy.__version__)\"\r\n```\r\n~~outside the source directory gives me the correct `1.11.4`, but~~(this was due to an pre-installed binary of the latest scipy release)\r\n\r\n```bash\r\npython -c \"import scipy.signal; scipy.signal.ShortTimeFFT\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'scipy.signal' has no attribute 'ShortTimeFFT'\r\n```\r\nwhich should be there as it was merged with PR #17408 and I can also see it [here](https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/signal\/_short_time_fft.py)\r\n\r\nEDIT: So the problem is: build works, but installing ipython and getting an `import scipy` to work fails\r\n\r\n @lucascolley advised to fire up a `git clean -xdf`, then repeating `python dev.py ipython` lead to the exact same error as above (`ModuleNotFoundError: No module named 'IPython'`).\r\n\r\nMy system:\r\n- OS: Ubuntu 22.04\r\n- virtual env.: poetry\r\n","comments":["It looks like you have tried to build SciPy from source in an environment where you have already installed a SciPy binary.\r\n\r\nYou have installed SciPy `1.11.4` from binary and are then trying to build SciPy `1.12.0.dev0` (`main`) in the same environment. Please try creating a new development environment first, as stated in the guide, then follow the rest of the steps.","Ok, so my issue of not getting the lastest source in a Python environment was solved my simply installing from source, and not going for a developer install. As described in the [doc](http:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source), these 3 commands within a virtual env do the trick:\r\n```bash\r\ngit clone https:\/\/github.com\/scipy\/scipy.git\r\ngit submodule update --init\r\npip install .\r\n```\r\nHowever, I am not closing this as I think the section [Building from source for SciPy development](http:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source-for-scipy-development) needs some adjustments, as what it describes does not work exactly. At least mentioning to delete the `build-install` dir before firing up `python dev.py ipython` might be a good idea.\r\n\r\nYes, @lucascolley now I have `1.12.0` no idea where that binary came from, I will try again in a clean new environment!","Ok with a clean new environment I can now successfully get into ipython with `python dev.py ipython`. However `import scipy` still fails, so I still get stuck. I tried importing from the `\/build\/scipy` directory by firing up `python -c \"import scipy\"` inside `\/build` but this only gives me\r\n```  \r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/xxx\/scipy\/build\/scipy\/__init__.py\", line 63, in <module>\r\n    from . import _distributor_init\r\nImportError: cannot import name '_distributor_init' from partially initialized module 'scipy' (most likely due to a circular import) (\/home\/whir\/Nextcloud\/Documents\/ESI\/code\/SciPy\/scipy2\/scipy\/build\/scipy\/__init__.py)\r\n```\r\n\r\nBut yeah, this whole issue is now pretty convoluted, I am fine with closing.. maybe a fresh start would be better :slightly_smiling_face: \r\n\r\n","You should run `python dev.py ipython` _from the source directory_ and then `import scipy`. There is no need to import from the build directory - perhaps something could be added to the documentation to make this more clear.","I am running the `python dev.py ipython` from the source directory, otherwise the `dev.py` would not be found. But then I can't `import scipy` becaus it is the source directory like stated in the initial post here. That's why I tried to import with relative path in `\/build` but to no avail.","> But then I can't import scipy becaus it is the source directory like stated in the initial post here.\r\n\r\nI thought that this was because you were in the environment with the SciPy 1.11.4 install. I just did a fresh build in a clean environment and I can't reproduce the error. `import scipy` should work fine in the source directory when you have built correctly. Can you try again, making sure you're in the clean environment? If that still doesn't work, please can you give a sequence of commands so that I can reproduce the error?","Ok sure, here are the things I am doing:\r\n```bash\r\nmkdir dev-install\r\ncd dev-install\r\ngit clone https:\/\/github.com\/scipy\/scipy.git\r\ncd scipy # this one is missing in the doc, but Ok :)\r\ngit submodule update --init\r\n```\r\nNow I am setting up a virtual env via poetry by creating a bogus project:\r\n```bash\r\ncd ..\r\npoetry new venv-scipy\r\ncd venv-scipy\r\npoetry shell # this creates AND activates the virtual env\r\n```\r\n\r\nTo confirm we have a clean slate:\r\n\r\n```bash\r\nwhich python3\r\n\/home\/xxx\/.cache\/pypoetry\/virtualenvs\/venv-scipy-A-J8Dfol-py3.10\/bin\/python3\r\n```\r\nand no scipy:\r\n```bash\r\npython -c \"import scipy\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'scipy'\r\n```\r\nThen I install all additional dependencies via pip in that venv as in the doc:\r\n```bash\r\npython -m pip install numpy cython pythran pybind11 meson ninja pydevtool rich-click\r\npython -m pip install pytest pytest-xdist pytest-timeout pooch threadpoolctl asv gmpy2 mpmath hypothesis\r\npython -m pip install sphinx \"pydata-sphinx-theme==0.9.0\" sphinx-design matplotlib numpydoc jupytext myst-nb\r\npython -m pip install mypy typing_extensions types-psutil pycodestyle ruff cython-lint\r\n```\r\nAfter cd'ing back into the `scipy` source directory, I fire up \r\n```bash\r\npython dev.py build\r\n```\r\nwhich succeeds but then already:\r\n```bash\r\npython dev.py ipython\r\n```\r\nfails with `RuntimeError: Can't install in non-empty directory: `. So removing the `build-install` directory (which is not covered by the current [doc](http:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source-for-scipy-development) as mentioned) and then repeating `python dev.py ipython` I get a running interpreter, but then `import scipy` fails:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nFile ~\/dev-install\/scipy\/scipy\/__init__.py:51\r\n     50 try:\r\n---> 51     from scipy.__config__ import show as show_config\r\n     52 except ImportError as e:\r\n\r\nModuleNotFoundError: No module named 'scipy.__config__'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nImportError                               Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 import scipy\r\n\r\nFile ~\/dev-install\/scipy\/scipy\/__init__.py:56\r\n     52 except ImportError as e:\r\n     53     msg = \"\"\"Error importing SciPy: you cannot import SciPy while\r\n     54     being in scipy source directory; please exit the SciPy source\r\n     55     tree first and relaunch your Python interpreter.\"\"\"\r\n---> 56     raise ImportError(msg) from e\r\n     59 from scipy.version import version as __version__\r\n     62 # Allow distributors to run custom init code\r\n\r\nImportError: Error importing SciPy: you cannot import SciPy while\r\n    being in scipy source directory; please exit the SciPy source\r\n    tree first and relaunch your Python interpreter.\r\n```\r\n\r\n","Thanks for giving the steps. I can't immediately see where the problem is, hopefully someone else can chime in.\r\n\r\n> `cd scipy # this one is missing in the doc, but Ok :)` \r\n\r\nlooks like we should add this","Please don't delete the build-install directory, that's where the build places an importable package (provided that it's on the `PYTHONPATH`).\r\n\r\nThe following commands work for me:\r\n\r\n```shell\r\n# I'm already in the scipy directory\r\ngit clean -xdf\r\nconda create -n 19563\r\nconda activate 19563\r\npwd\r\nconda install python=3.10\r\npip install numpy cython pybind11 pythran jupyter meson ninja meson-python scipy-openblas32\r\npip install click pydevtool pip install rich-click\r\npython dev.py build --with-scipy-openblas\r\npython dev.py ipython\r\n```\r\n\r\nWhen the console starts I see:\r\n```\r\nIn [1]: import scipy\r\n\r\nIn [2]: scipy.__file__\r\nOut[2]: '\/Users\/andrew\/Documents\/Andy\/programming\/scipy\/build-install\/lib\/python3.10\/site-packages\/scipy\/__init__.py'\r\n```\r\n\r\nCan you try using `venv` or `conda` to create the virtual environment instead of `poetry`?","Well poetry is explicitly mentionend as a possible provider of virt. envs. in the scipy [doc](http:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source-for-scipy-development). So I am pretty sure it will work with conda, but then maybe poetry has to be removed from that list?!\r\n\r\nEDIT: would be interesting to dig in, and see why dev.py complains about `Can't install in non-empty directory` in a poetry env but not in a conda env","I didn't check to see if poetry worked. But you checking if conda\/venv works might make it clearer if poetry is the issue.","Yup, can confirm with `venv` it works as described, I end up in an ipython session and can successfully `import scipy`!","So there's something about everything works with `poetry` that's the issue.","@tensionhead would you mind editing the title to something like \"Build fails in poetry env\" so that people can see what the issue is about from a glance?","The build actually works, but getting the latest scipy imported (either in an interpreter or via scripting) fails with poetry."],"labels":["Build issues","query","DX"]},{"title":"ENH: stats.power: add function to simulate hypothesis test power","body":"#### Reference issue\r\nNA\r\n\r\n#### What does this implement\/fix?\r\n[`statsmodels` ](https:\/\/www.statsmodels.org\/devel\/stats.html#power-and-sample-size-calculations) and [`pingouin`](https:\/\/pingouin-stats.org\/build\/html\/api.html#power-analysis) offer analytical power calculation for a few of SciPy's hypothesis tests, but I thought it would be useful to offer a general function for simulating the power of essentially any SciPy hypothesis test. The same function can be used to study the effects of\r\n- sample size(s),\r\n- test parameters, and\r\n- alternative hypothesis parameters (e.g. effect size)\r\n\r\non the power, and it can also be used to verify that p-values produced under the null hypothesis are uniformly distributed. In an enhancement, a method of the result object could be used to solve for an unknown (e.g. sample size, effect size) to achieve a given power, perhaps by (inverse) interpolation or a root finding method that is robust against noise.\r\n\r\n[Documentation\/examples](https:\/\/output.circle-artifacts.com\/output\/job\/b12ecf2f-c374-420e-845f-e3a8d4e0bfd1\/artifacts\/0\/html\/reference\/generated\/scipy.stats.power.html#scipy.stats.power)\r\n\r\n#### Additional information\r\nI think this is ready to ready to review. Items not checked below could be left to follow-up work.\r\n- [x] ~~improve auto-vectorization when callable does not have `axis` argument~~ (improved documentation)\r\n- [x] improve the `args` interface (should it be `kwargs`? Do *all* `rvs` and `test` callables need to accept the `args`?)\r\n- [x] simplify `n_observations`\/`args` input validation (or at least review it... it was a bit rushed)\r\n- [x] add tests\r\n- [x] ~~generalize for other ways of assessing rejection of null hypothesis (e.g. observed value of statistic and critical value instead of p-value and significance threshhold)~~\r\n- [ ] research \/ think about whether considerations like [\"Permutation P-values should never be zero\"](https:\/\/arxiv.org\/pdf\/1603.05766) apply here\r\n- [ ] add `plot` method to result object?\r\n- [ ] add `solve` method to result object?\r\n- [x] support array-like `significance`\r\n- [x] support `test` returning array-like p-value\r\n","comments":["just one comment based on a brief look\r\n\r\nWhen I check the null distribution (and power) of a hypothesis test, then I usually compare several p-value thresholds, `significance` (`alphas`).\r\n\r\nIt's a useful function.\r\nI usually write it from scratch each time in a notebook. One reason is flexibility, very often I compute the rejection rate also for several methods. Many of the `stats` functions that I added to statsmodels, especially proportions and rates, have method options. In those cases I collect several p-values for each Monte Carlo replication.\r\n ","Thanks for taking a look! I don't imagine this function will replace all ad hoc power simulations, but these suggestions will help make it cover more cases.\r\n\r\n> When I check the null distribution (and power) of a hypothesis test, then I usually compare several p-value thresholds\r\n\r\nSure. It would be easy to allow `significance` to be an array. I considered it, but thought it might be enough to return all the p-values. Did you see the last [example in the documentation](https:\/\/output.circle-artifacts.com\/output\/job\/b12ecf2f-c374-420e-845f-e3a8d4e0bfd1\/artifacts\/0\/html\/reference\/generated\/scipy.stats.power.html#scipy.stats.power) in which I compare the ECDF of p-values under the null hypothesis against the uniform CDF?\r\n\r\n*Update: added support for array-like significance threshold.*\r\n\r\n> Many of the `stats` functions... have method options. In those cases I collect several p-values for each Monte Carlo replication.\r\n\r\nIt's possible to compute the power for multiple method options in a single call, but right now I'm only sure about doing it with independent Monte Carlo samples. I'll think about allowing the Monte Carlo samples to be shared - probably I'd just allow the user to return multiple p-values from the `test` callable.\r\n\r\n*Update: added support for `test` to return multiple p-values from same Monte Carlo samples.*","@raphaelvallat do you have a moment to comment on the utility of such a function?","@mdhaber I think this would be a great addition to scipy \ud83d\udc4d ","Nice stuff!\r\n\r\nSmall suggestion: To be extra clear for the user, the doc on the return value of `stats.power` could be more elaborate. For example, the \u201cpower\u201d attribute could be something like \u201cThe estimated power against the alternative. It is the equal to proportion of test statistics that correctly reject the null hypothesis (test p-value being less than `significance`) given samples from the alternative hypothesis.\u201d\r\n\r\nAs for supporting multiple significance levels, am I right to say that given the p-values, one can plot a graph of power vs significance? If that\u2019s the case, would it be more handy\/logical if `power` is a member function of the returned result object that takes significance as an argument? So, for example, if the user suddenly wants to try another significance level, they don\u2019t need to run the whole simulation again.","In addition, it\u2019s probably helpful to start the \u201cExamples\u201d section with an example on one-sample test. Personally I\u2019m interested to see for example a K-S goodness of fit test of normal null vs say t alternative.\r\n\r\nIt might be also helpful to add an informative comment in the doc regarding composite alternatives (such as \u201cthe distribution is not normal\u201d), which basically says such composite alternatives cannot be tested and one must pick some simple alternative to test the power.","Also the `kwargs` argument seems redundant, especially that it is passed to `test` and\/or `rvs` depending on inspection. It seems just easier to let the caller use a lambda if they want to achieve this.","just one remark\r\n\r\nI think the function name should indicate simulation, e.g. power_simulated, simulate_power (montecarlo_power, ...)","> I think the function name should indicate simulation, e.g. power_simulated, simulate_power (montecarlo_power, ...)\n\nI would not do that as it would add more jargon. The doc can explain the nuance for people who want to know what's behind the hood. I suspect that this won't matter for most users. So better keep it simple.","Well, you use a pretty general term and use it for a special method.\r\n\r\npower without simulations:\r\nhttps:\/\/www.statsmodels.org\/dev\/stats.html#power-and-sample-size-calculations\r\n\r\nBut if scipy uses `power` term only for the special method, then I don't have to worry much that you steal my users.\r\n:)\r\n\r\n","Why again this narrative about stealing users!? \ud83e\udd37\u200d\u2642\ufe0f That's just not true and both projects definitely don't serve the same audience. Like Pingouin still being relevant even if this goes in.","It\u2019s probably most important to define the signature at this point as the implementation can always be refined later. To choose the \u201cnicest\u201d signature it is helpful to postulate a few example use cases and see whether the user code would be clear and easily understandable  with the proposed signature.\r\n\r\nSome uses cases are:\r\n\r\nOne sample t test that the mean of normal variable X is equal to 2 against the alternative that its mean is 1 and variance is 1.\r\n\r\n```\r\nscipy.stats.monte_carlo_power(lambda x: scipy.stats.ttest_1samp(x,2), scipy.stats.norm(1,1).rvs, 100).power(0.05)\r\n```\r\n\r\n","I personally prefer Matt's proposal with passing the significance as a parameter (also vectorized to allow exploration). I am not sure what we would facilitate by having a method like you propose instead. What is your rationale here?\n\nFor kwargs I agree that it can be confusing and would prefer not to propose it first. Though I get that it is sort of needed if we want to vectirize on these. I am just wondering if people would really use that capability. Maybe for later if there is a demand for it.","I would add examples to the docs where the asymptotic distribution of the test statistic is or might not be a good approximation in small samples.\r\nFor example nonparametric tests or tests on variances or dispersion that are available in scipy.stats.\r\n\r\nt-test is very reliably in small samples unless the distribution is very far away from the normal distribution. (Explicit power and sample size computation for t-test is reliable enough  except for extreme cases. and much faster than simulations.)\r\n\r\nThe problem is that simulated power is mainly useful in difficult cases, but then we need a lot of flexibility in specifying the distribution under the alternative.\r\n\r\n@tupui I just wanted to point out that I don't think of simulations when reading `power`. Using the term for simulations will limit what other \"powers\" there might be. Fine with me. :)\r\n\r\n(\r\nIt looks like I wrote a simulation function for statsmodels for a difficult case\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.oneway.simulate_power_equivalence_oneway\r\nto compare with the asymptotic p-values\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.oneway.power_equivalence_oneway.html\r\n)\r\n","The last example in the doc, which uses the `power` function to simulate the [size](https:\/\/en.m.wikipedia.org\/wiki\/Size_(statistics)) of a test, feels like an anti-pattern and probably should not be advertised. It is like including an example for the `sin` function to say it can be used to compute cosine by shifting the argument by `0.5*pi`; one should use `cos` if they want to compute cosine.\r\n\r\nOn the other hand, this \u201cduality\u201d suggests that the function might be able to be generalized to serve both purposes. The right level of generality requires some careful design.","I am not sure I understand what you suggest then @fancidev ","> I just wanted to point out that I don't think of simulations when reading power. Using the term for simulations will limit what other \"powers\" there might be.\n\nThat's a fair point. The question to me is more if we were to add more strategies to compute the power, would we want to add new functions or just use a parameter to set the method. I don't expect a lot of users to tweak that. For power user, when that time comes, we can still have separate functions and keep a higher level API.","@fancidev \r\n\r\nChecking the size is important, but for users not because of checking whether it's \"implemented correctly\"\r\n\r\nI would add an example where a hypothesis test has not the right size.\r\n\r\nfor example, because\r\n\r\n- some underlying assumption does not hold, e.g. use equal-var t-test when variances in two samples differ. (unequal var Welch test would be correctly sized)\r\n- discreteness of the sample space like binomial counts, test cannot have size=alpha but could maintain size <= alpha. Fisher's  exact test maintains size but is conservative, size can be much smaller than alpha on average.\r\n- asymptotic, large sample distribution and p-values are not a good approximation for small samples. e.g. AFAIR, many variance\/dispersion tests can have incorrect size in small samples, especially if kurtosis is larger than in normal distribution.\r\n\r\nAside: Once upon a time I checked with simulations that scipy.stats hypothesis tests are \"implemented correctly\", but, AFAIR, only for large samples where asymptotic distribution works well.","> the doc on the return value of `stats.power` could be more elaborate\r\n\r\nI will probably add more information in the Notes. \r\n\r\n> If that\u2019s the case, would it be more handy\/logical\r\n\r\n\"handy\", perhaps; \"*more* logical\" is hard to judge (but perhaps that is just due to my limited powers of logic). In this case, I followed the precedent set by `bootstrap`, which returns the confidence interval as an array when it is arguable that it should be a method. I've regretted that choice sometimes, but I don't recall any reported complaints.\r\n\r\n> Personally I\u2019m interested to see for example a K-S goodness of fit test of normal null vs say t alternative.\r\n\r\nIf the purpose is to show the very low power of K-S compared to other normality tests, that sounds like a nice addition. We'll see whether the reviewing maintainer thinks it's essential or if the first example is simple enough for them. \r\n\r\n> It seems just easier to let the caller use a lambda if they want to achieve this.\r\n\r\nI have spoken against the inclusion of `args`\/`kwargs` parameters in the past (e.g. see e.g. gh-13258). I agree that they are almost always redundant and *usually* have little added value, so normally I'm against adding them, too. (Others have disagreed and recommended inclusion.) Certainly it is easier for us to leave them out, but adding support for them is a common feature request (e.g. gh-8352 or many Stack Overflow questions).\r\n\r\nIt has been several months since I wrote this, so I can't be sure what pushed me in the other direction here, but it was probably that I could think of very few uses of this function *without* the user having to write a wrapper. I think I remember considering that the examples looked a more complicated without it, and reviewers would not like that, either. \r\n\r\nThere is a balance to be struck here.\r\n\r\n> The last example in the doc, which uses the `power` function to simulate the size of a test, feels like an anti-pattern and probably should not be advertised.\r\n\r\nThe difference is that the `cos` function exists.\r\n\r\n> I would add an example where a hypothesis test has not the right size.\r\n\r\nThat's a good idea. I like that better than suggesting that our tests might not be implemented correctly (although I have caught problems in PRs by simulating their size). I can add that. Is that what you meant in https:\/\/github.com\/scipy\/scipy\/pull\/19561#issuecomment-1992867265?\r\n\r\n(Note that the accuracy of the distribution of the test statistic for small samples is already discussed extensively in the [Resampling and Monte Carlo Methods](https:\/\/docs.scipy.org\/doc\/scipy\/tutorial\/stats\/resampling.html) tutorials and the API documentation of several tests based on asymptotic approximations (e.g. `normaltest`) and resampling\/MC methods (e.g. `monte_carlo_test`) . \r\n\r\nAlso, if you wanted to this to compare against the power result produce by one of the `statsmodels` power calculations, please suggest an example. I'd be happy to include it (if we can use `statsmodels` in examples)."],"labels":["scipy.stats","enhancement"]},{"title":"ENH: Warning: Fortran 2018 deleted feature: Shared DO termination","body":"### Describe your issue.\r\n\r\nWhile writing a cpp wrapper for scipy, the following warning was shown due to the very old Fortran coding style used in the fitpack part in interpolation.\r\n```\r\ninterpolate\/fitpack\/*.f\r\n```\r\n\r\nWarning: Fortran 2018 deleted feature: Shared DO termination\r\n\r\nThis can easily be solved. E.g.:\r\n\r\n```\r\n      do 20 i=1,k1\r\n         t(i) = xb\r\n         t(j) = xe\r\n         j = j-1\r\n  20  continue\r\n  ```\r\n  \r\nmust be changed to \r\n\r\n```\r\n      do i=1,k1\r\n         t(i) = xb\r\n         t(j) = xe\r\n         j = j-1\r\n      end do\r\n```\r\n\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nnone\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nWarning: Warning: Fortran 2018 deleted feature: Shared DO termination\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.1 1.24.4 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.35\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-hy2eo89f\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n","comments":["This is in `interpolate\/fitpack\/curfit.f`.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/86ab60469451c3bfb8c3e5f150c862b3c50e4f42\/scipy\/interpolate\/fitpack\/curfit.f#L241C19-L245","The language feature is ugly indeed! That said, it is unlikely to be really removed from the language \/ compiler support. \r\nSo the warning is harmless is somewhat annoying. It's probably easiest to silence the warning during the build.\r\n\r\nWe can in principle review a PR which \"modernizes\" these do-enddo loops, but --- If you're up to looking into Fortran code carried by scipy, you're most welcome to join the effort in https:\/\/github.com\/scipy\/scipy\/issues\/18566; if you're up to looking into FITPACK specifically, there's https:\/\/github.com\/scipy\/scipy\/issues\/2579.\r\n\r\nBoth these issues track efforts where help is wanted and very much appreciated; both are likely to bring more bang for the effort.  ","The code snippet you've shown, and that @lucascolley has linked, has no shared termination.\r\n\r\nShared termination refers to the case where nested loops end with the same continue statement:\r\n\r\n```fortran\r\nC shared_do.f\r\nC\r\n      do 10 i = 1, 3\r\n      do 10 j = 1, 3\r\n      write(*,*) i, j\r\nc >>> SHARED TERMINATION <<<\r\n   10 continue\r\n\r\n      do 40 i = 1, 3\r\n      do 50 j = 1, 3\r\n      write(*,*) i, j\r\n   50 continue\r\n   40 continue\r\n      end   \r\n```\r\n\r\nOnly the first case will raise a warning:\r\n```\r\n$ gfortran shared_do.f \r\nshared_do.f:4:72:\r\n\r\n    4 |       do 10 j = 1, 3\r\n      |                                                                        1\r\nWarning: Fortran 2018 deleted feature: Shared DO termination label 10 at (1)\r\n```\r\n\r\nSo if the warning bothers you, you can fix it relatively easily by inserting a new continue statement with a unique label. The reason you would label these loops in increments of 10 or 100, is precisely to be able to insert new labels later, if needed.\r\n\r\nThe labelled do loop remains valid F2018 syntax (albeit marked as obsolescent).\r\n"],"labels":["scipy.interpolate","maintenance","Fortran"]},{"title":"ENH: improve `scipy.special.log_softmax` accuracy in edge cases by a factor of `2**126` to `2**1022`","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nFixes #19521\r\n\r\n#### What does this implement\/fix?\r\nBy taking advantage of the fact that `x - x_max` is going to be 0 at the maximum and that `exp(0)` is 1, we can use `log1p` instead of `log` to increase the accuracy of `log_softmax` at the maximum index by a factor of about `2**126` (for float32) or about `2**1022` (for float64) when there is a single maximum value that is much larger than other values.  Most values should not be impacted.\r\n\r\n#### Additional information\r\nI've done some manual testing locally, but I'm not sure how to actually run all the tests, since `python dev.py test -v` fails with\r\n<details><summary>fatal error: bits\/timesize.h: No such file or directory<\/summary>\r\n\r\n```\r\nFAILED: scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ -Iscipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/stats -I..\/scipy\/stats -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include -I\/usr\/include -I\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11 -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++14 -O2 -g -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -fPIC -DENABLE_PYTHON_MODULE -D__PYTHRAN__=3 -DPYTHRAN_BLAS_NONE -Wno-cpp -Wno-deprecated-declarations -Wno-unused-but-set-variable -Wno-unused-function -Wno-unused-variable -Wno-int-in-bool-context -MD -MQ scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -MF scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d -o scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -c scipy\/stats\/_stats_pythran.cpp\r\nIn file included from \/usr\/include\/features.h:392,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/x86_64-conda-linux-gnu\/bits\/os_defines.h:39,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/x86_64-conda-linux-gnu\/bits\/c++config.h:655,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/type_traits:38,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/include\/types\/assignable.hpp:4,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/types\/assignable.hpp:4,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/core.hpp:41,\r\n                 from scipy\/stats\/_stats_pythran.cpp:1:\r\n\/usr\/include\/features-time64.h:21:10: fatal error: bits\/timesize.h: No such file or directory\r\n   21 | #include <bits\/timesize.h>\r\n      |          ^~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n<\/details>\r\n\r\nSo I'm relying on CI to run the tests that I added.","comments":["<details><summary>I guess I need to fix the tests<\/summary>\r\n\r\n```\r\n__________________________ test_log_softmax_noneaxis ___________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\nscipy\/special\/tests\/test_log_softmax.py:58: in test_log_softmax_noneaxis\r\n    assert_allclose(sc.log_softmax(x), expected, rtol=1e-13)\r\n        expected   = array([[-3.4401897, -2.4401897],\r\n       [-1.4401897, -0.4401897]])\r\n        log_softmax_expected = array([-3.4401897, -2.4401897, -1.4401897, -0.4401897])\r\n        log_softmax_x = array([0, 1, 2, 3])\r\n        x          = array([[0, 1],\r\n       [2, 3]])\r\nscipy\/special\/_logsumexp.py:306: in log_softmax\r\n    x_max = x[x_argmax] if x.ndim > 0 else x\r\nE   IndexError: index 3 is out of bounds for axis 0 with size 2\r\n        axis       = None\r\n        x          = array([[0, 1],\r\n       [2, 3]])\r\n        x_argmax   = array([[3]])\r\n__________________________ test_axes[0-expected_2d0] ___________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\nscipy\/special\/tests\/test_log_softmax.py:67: in test_axes\r\n    sc.log_softmax([[1000, 1], [1000, 1]], axis=axis_2d),\r\n        axis_2d    = 0\r\n        expected_2d = array([[-0.69314718, -0.69314718],\r\n       [-0.69314718, -0.69314718]])\r\nscipy\/special\/_logsumexp.py:321: in log_softmax\r\n    exp_tmp[x_argmax[finite_max_mask]] = 0\r\nE   IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\r\n        axis       = 0\r\n        exp_tmp    = array([[[1., 1.],\r\n        [1., 1.]]])\r\n        finite_max_mask = array([[[ True,  True],\r\n        [ True,  True]]])\r\n        tmp        = array([[[0, 0],\r\n        [0, 0]]])\r\n        x          = array([[1000,    1],\r\n       [1000,    1]])\r\n        x_argmax   = array([[0, 0]])\r\n        x_max      = array([[[1000,    1],\r\n        [1000,    1]]])\r\n__________________________ test_axes[1-expected_2d1] ___________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\nscipy\/special\/tests\/test_log_softmax.py:67: in test_axes\r\n    sc.log_softmax([[1000, 1], [1000, 1]], axis=axis_2d),\r\n        axis_2d    = 1\r\n        expected_2d = array([[   0, -999],\r\n       [   0, -999]])\r\nscipy\/special\/_logsumexp.py:321: in log_softmax\r\n    exp_tmp[x_argmax[finite_max_mask]] = 0\r\nE   IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\r\n        axis       = 1\r\n        exp_tmp    = array([[[1., 1.],\r\n        [1., 1.]],\r\n\r\n       [[1., 1.],\r\n        [1., 1.]]])\r\n        finite_max_mask = array([[[ True,  True]],\r\n\r\n       [[ True,  True]]])\r\n        tmp        = array([[[0, 0],\r\n        [0, 0]],\r\n\r\n       [[0, 0],\r\n        [0, 0]]])\r\n        x          = array([[1000,    1],\r\n       [1000,    1]])\r\n        x_argmax   = array([[0],\r\n       [0]])\r\n        x_max      = array([[[1000,    1]],\r\n\r\n       [[1000,    1]]])\r\n__________________________ test_log_softmax_2d_axis1 ___________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\nscipy\/special\/tests\/test_log_softmax.py:97: in test_log_softmax_2d_axis1\r\n    assert_allclose(sc.log_softmax(x, axis=1), expected, rtol=1e-13)\r\n        expected   = array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],\r\n       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]])\r\n        log_softmax_2d_expected = array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],\r\n       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]])\r\n        log_softmax_2d_x = array([[0, 1, 2, 3],\r\n       [4, 5, 6, 7]])\r\n        x          = array([[0, 1, 2, 3],\r\n       [4, 5, 6, 7]])\r\nscipy\/special\/_logsumexp.py:306: in log_softmax\r\n    x_max = x[x_argmax] if x.ndim > 0 else x\r\nE   IndexError: index 3 is out of bounds for axis 0 with size 2\r\n        axis       = 1\r\n        x          = array([[0, 1, 2, 3],\r\n       [4, 5, 6, 7]])\r\n        x_argmax   = array([[3],\r\n       [3]])\r\n__________________________ test_log_softmax_2d_axis0 ___________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\nscipy\/special\/tests\/test_log_softmax.py:103: in test_log_softmax_2d_axis0\r\n    assert_allclose(sc.log_softmax(x, axis=0), expected, rtol=1e-13)\r\n        expected   = array([[-3.4401897, -3.4401897],\r\n       [-2.4401897, -2.4401897],\r\n       [-1.4401897, -1.4401897],\r\n       [-0.4401897, -0.4401897]])\r\n        log_softmax_2d_expected = array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],\r\n       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]])\r\n        log_softmax_2d_x = array([[0, 1, 2, 3],\r\n       [4, 5, 6, 7]])\r\n        x          = array([[0, 4],\r\n       [1, 5],\r\n       [2, 6],\r\n       [3, 7]])\r\nscipy\/special\/_logsumexp.py:315: in log_softmax\r\n    tmp = x - x_max\r\nE   ValueError: operands could not be broadcast together with shapes (4,2) (1,2,2)\r\n        axis       = 0\r\n        finite_max_mask = array([[[ True,  True],\r\n        [ True,  True]]])\r\n        x          = array([[0, 4],\r\n       [1, 5],\r\n       [2, 6],\r\n       [3, 7]])\r\n        x_argmax   = array([[3, 3]])\r\n        x_max      = array([[[3, 7],\r\n        [3, 7]]])\r\n_____________________________ test_log_softmax_3d ______________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:59: in _wrapfunc\r\n    return bound(*args, **kwds)\r\nE   TypeError: 'tuple' object cannot be interpreted as an integer\r\n        args       = ()\r\n        bound      = <built-in method argmax of numpy.ndarray object at 0x7f89cce5b450>\r\n        kwds       = {'axis': (1, 2), 'keepdims': True, 'out': None}\r\n        method     = 'argmax'\r\n        obj        = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\n\r\nDuring handling of the above exception, another exception occurred:\r\nscipy\/special\/tests\/test_log_softmax.py:110: in test_log_softmax_3d\r\n    assert_allclose(sc.log_softmax(x_3d, axis=(1, 2)), expected_3d, rtol=1e-13)\r\n        expected_3d = array([[[-3.4401897, -2.4401897],\r\n        [-1.4401897, -0.4401897]],\r\n\r\n       [[-3.4401897, -2.4401897],\r\n        [-1.4401897, -0.4401897]]])\r\n        log_softmax_2d_expected = array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],\r\n       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]])\r\n        log_softmax_2d_x = array([[0, 1, 2, 3],\r\n       [4, 5, 6, 7]])\r\n        x_3d       = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\nscipy\/special\/_logsumexp.py:305: in log_softmax\r\n    x_argmax = np.argmax(x, axis=axis, keepdims=True)\r\n        axis       = (1, 2)\r\n        x          = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:1229: in argmax\r\n    return _wrapfunc(a, 'argmax', axis=axis, out=out, **kwds)\r\n        a          = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\n        axis       = (1, 2)\r\n        keepdims   = True\r\n        kwds       = {'keepdims': True}\r\n        out        = None\r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:68: in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n        args       = ()\r\n        bound      = <built-in method argmax of numpy.ndarray object at 0x7f89cce5b450>\r\n        kwds       = {'axis': (1, 2), 'keepdims': True, 'out': None}\r\n        method     = 'argmax'\r\n        obj        = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/numpy\/core\/fromnumeric.py:45: in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nE   TypeError: 'tuple' object cannot be interpreted as an integer\r\n        args       = ()\r\n        kwds       = {'axis': (1, 2), 'keepdims': True, 'out': None}\r\n        method     = 'argmax'\r\n        obj        = array([[[0, 1],\r\n        [2, 3]],\r\n\r\n       [[4, 5],\r\n        [6, 7]]])\r\n        wrap       = <built-in method __array_wrap__ of numpy.ndarray object at 0x7f89cce5b450>\r\n=========================== short test summary info ============================\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_log_softmax_noneaxis - IndexError: index 3 is out of bounds for axis 0 with size 2\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_axes[0-expected_2d0] - IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_axes[1-expected_2d1] - IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_log_softmax_2d_axis1 - IndexError: index 3 is out of bounds for axis 0 with size 2\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_log_softmax_2d_axis0 - ValueError: operands could not be broadcast together with shapes (4,2) (1,2,2)\r\nFAILED scipy\/special\/tests\/test_log_softmax.py::test_log_softmax_3d - TypeError: 'tuple' object cannot be interpreted as an integer\r\n= 6 failed, 44926 passed, 2331 skipped, 148 xfailed, 13 xpassed in 335.15s (0:05:35)\r\n```\r\n<\/details> ","Is there a good way to test only this one file, in light of `python dev.py test -v` failing?","> Is there a good way to test only this one file, in light of `python dev.py test -v` failing?\r\n\r\nWe should try to get your local dev environment working correctly. The missing `<bits\/timesize.h>` suggests a missing or incompatible `glibc`. Can you check your glibc version with `ldd --version` while your environment is activated?","@JasonGross, did you build SciPy with `python dev.py build` successfully and it's only `python dev.py test -v` that fails? Did you follow all the steps [here](https:\/\/docs.scipy.org\/doc\/scipy\/building\/index.html) for building from source for development?","Sorry it's taken me so long to respond.\r\n\r\n> We should try to get your local dev environment working correctly. The missing `<bits\/timesize.h>` suggests a missing or incompatible `glibc`. Can you check your glibc version with `ldd --version` while your environment is activated?\r\n\r\n```\r\n$ ldd --version\r\nldd (Ubuntu GLIBC 2.35-0ubuntu3.6) 2.35\r\n```\r\n\r\n\r\n\r\n> @JasonGross, did you build SciPy with `python dev.py build` successfully and it's only `python dev.py test -v` that fails? Did you follow all the steps [here](https:\/\/docs.scipy.org\/doc\/scipy\/building\/index.html) for building from source for development?\r\n\r\nI get the same error with `python dev.py build`.\r\nI've run\r\n<details><summary><code>$ sudo apt install -y gcc g++ gfortran libopenblas-dev liblapack-dev pkg-config python3-pip python3-dev<\/code><\/summary>\r\n\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree... Done\r\nReading state information... Done\r\ng++ is already the newest version (4:11.2.0-1ubuntu1).\r\ngcc is already the newest version (4:11.2.0-1ubuntu1).\r\ngfortran is already the newest version (4:11.2.0-1ubuntu1).\r\nliblapack-dev is already the newest version (3.10.0-2ubuntu1).\r\npkg-config is already the newest version (0.29.2-1ubuntu3).\r\nlibopenblas-dev is already the newest version (0.3.20+ds-1).\r\npython3-dev is already the newest version (3.10.6-1~22.04).\r\npython3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.4).\r\n0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\r\n```\r\n<\/details>\r\n\r\n<details><summary><code>$ mamba env create -f environment.yml<\/code><\/summary>\r\n\r\n```\r\nRetrieving notices: ...working... done\r\nwarning  libmamba Cache file \"\/home\/jgross\/.local64\/mambaforge\/pkgs\/cache\/497deca9.json\" was modified by another program\r\nhuggingface\/linux-64                                          No change\r\nhuggingface\/noarch                                            No change\r\nconda-forge\/noarch                                  15.3MB @   4.3MB\/s  3.9s\r\nconda-forge\/linux-64                                37.4MB @   4.9MB\/s 10.4s\r\n\r\n\r\nLooking for: ['python', \"cython[version='>=3.0.4']\", 'compilers', 'meson', 'meson-python', 'ninja', 'numpy', 'openblas', 'pkg-config', 'libblas=[build=*openblas]', 'pybind11', 'pooch', 'pythran', 'pytest', 'pytest-cov', 'pytest-xdist', 'pytest-timeout', \"asv[version='>=0.6']\", 'hypothesis', 'mypy', 'typing_extensions', 'types-psutil', 'sphinx', 'numpydoc', 'ipython', \"setuptools[version='<67.3']\", 'matplotlib', 'pydata-sphinx-theme==0.9.0', 'sphinx-design', 'jupytext', 'myst-nb', 'mpmath', 'gmpy2', 'threadpoolctl', 'rich-click', 'click', \"doit[version='>=0.36.0']\", 'pydevtool', 'ruff', 'cython-lint']\r\n\r\n\r\nTransaction\r\n\r\n  Prefix: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\r\n\r\n  Updating specs:\r\n\r\n   - python\r\n   - cython[version='>=3.0.4']\r\n   - compilers\r\n   - meson\r\n   - meson-python\r\n   - ninja\r\n   - numpy\r\n   - openblas\r\n   - pkg-config\r\n   - libblas=*[build=*openblas]\r\n   - pybind11\r\n   - pooch\r\n   - pythran\r\n   - pytest\r\n   - pytest-cov\r\n   - pytest-xdist\r\n   - pytest-timeout\r\n   - asv[version='>=0.6']\r\n   - hypothesis\r\n   - mypy\r\n   - typing_extensions\r\n   - types-psutil\r\n   - sphinx\r\n   - numpydoc\r\n   - ipython\r\n   - setuptools[version='<67.3']\r\n   - matplotlib\r\n   - pydata-sphinx-theme==0.9.0\r\n   - sphinx-design\r\n   - jupytext\r\n   - myst-nb\r\n   - mpmath\r\n   - gmpy2\r\n   - threadpoolctl\r\n   - rich-click\r\n   - click\r\n   - doit[version='>=0.36.0']\r\n   - pydevtool\r\n   - ruff\r\n   - cython-lint\r\n\r\n\r\n  Package                               Version  Build                Channel                    Size\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n  Install:\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\n  + _libgcc_mutex                           0.1  conda_forge          conda-forge\/linux-64     Cached\r\n  + _openmp_mutex                           4.5  2_gnu                conda-forge\/linux-64     Cached\r\n  + alabaster                            0.7.16  pyhd8ed1ab_0         conda-forge\/noarch         18kB\r\n  + alsa-lib                             1.2.10  hd590300_0           conda-forge\/linux-64     Cached\r\n  + asttokens                             2.4.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + asv                                   0.6.1  py311hb755f60_1      conda-forge\/linux-64     Cached\r\n  + asv_runner                            0.1.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + attr                                  2.5.1  h166bdaf_1           conda-forge\/linux-64     Cached\r\n  + attrs                                23.2.0  pyh71513ae_0         conda-forge\/noarch         55kB\r\n  + babel                                2.14.0  pyhd8ed1ab_0         conda-forge\/noarch          8MB\r\n  + backports.zoneinfo                    0.2.1  py311h38be061_8      conda-forge\/linux-64     Cached\r\n  + beautifulsoup4                       4.12.3  pyha770c72_0         conda-forge\/noarch        118kB\r\n  + beniget                               0.4.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + binutils                               2.40  hdd6e379_0           conda-forge\/linux-64     Cached\r\n  + binutils_impl_linux-64                 2.40  hf600244_0           conda-forge\/linux-64     Cached\r\n  + binutils_linux-64                      2.40  hbdbef99_2           conda-forge\/linux-64     Cached\r\n  + brotli                                1.1.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + brotli-bin                            1.1.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + brotli-python                         1.1.0  py311hb755f60_1      conda-forge\/linux-64     Cached\r\n  + bzip2                                 1.0.8  hd590300_5           conda-forge\/linux-64     Cached\r\n  + c-compiler                            1.7.0  hd590300_0           conda-forge\/linux-64        6kB\r\n  + ca-certificates                  2023.11.17  hbcca054_0           conda-forge\/linux-64     Cached\r\n  + cairo                                1.18.0  h3faef2a_0           conda-forge\/linux-64     Cached\r\n  + certifi                          2023.11.17  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + charset-normalizer                    3.3.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + click                                 8.1.7  unix_pyh707e725_0    conda-forge\/noarch       Cached\r\n  + cloudpickle                           3.0.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + colorama                              0.4.6  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + colorlog                              6.8.0  py311h38be061_0      conda-forge\/linux-64       22kB\r\n  + comm                                  0.2.1  pyhd8ed1ab_0         conda-forge\/noarch         12kB\r\n  + compilers                             1.7.0  ha770c72_0           conda-forge\/linux-64        7kB\r\n  + contourpy                             1.2.0  py311h9547e67_0      conda-forge\/linux-64     Cached\r\n  + coverage                              7.4.0  py311h459d7ec_0      conda-forge\/linux-64      365kB\r\n  + cxx-compiler                          1.7.0  h00ab1b0_0           conda-forge\/linux-64        6kB\r\n  + cycler                               0.12.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + cython                                3.0.7  py311hb755f60_0      conda-forge\/linux-64        4MB\r\n  + cython-lint                          0.16.0  pyhd8ed1ab_0         conda-forge\/noarch         18kB\r\n  + dbus                                 1.13.6  h5008d03_3           conda-forge\/linux-64     Cached\r\n  + debugpy                               1.8.0  py311hb755f60_1      conda-forge\/linux-64     Cached\r\n  + decorator                             5.1.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + docutils                             0.20.1  py311h38be061_3      conda-forge\/linux-64      918kB\r\n  + doit                                 0.36.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + exceptiongroup                        1.2.0  pyhd8ed1ab_2         conda-forge\/noarch         21kB\r\n  + execnet                               2.0.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + executing                             2.0.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + expat                                 2.5.0  hcb278e6_1           conda-forge\/linux-64     Cached\r\n  + font-ttf-dejavu-sans-mono              2.37  hab24e00_0           conda-forge\/noarch       Cached\r\n  + font-ttf-inconsolata                  3.000  h77eed37_0           conda-forge\/noarch       Cached\r\n  + font-ttf-source-code-pro              2.038  h77eed37_0           conda-forge\/noarch       Cached\r\n  + font-ttf-ubuntu                        0.83  h77eed37_1           conda-forge\/noarch          2MB\r\n  + fontconfig                           2.14.2  h14ed4e7_0           conda-forge\/linux-64     Cached\r\n  + fonts-conda-ecosystem                     1  0                    conda-forge\/noarch       Cached\r\n  + fonts-conda-forge                         1  0                    conda-forge\/noarch       Cached\r\n  + fonttools                            4.47.2  py311h459d7ec_0      conda-forge\/linux-64        3MB\r\n  + fortran-compiler                      1.7.0  heb67821_0           conda-forge\/linux-64        6kB\r\n  + freetype                             2.12.1  h267a509_2           conda-forge\/linux-64     Cached\r\n  + gast                                  0.5.4  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + gcc                                  12.3.0  h8d2909c_2           conda-forge\/linux-64     Cached\r\n  + gcc_impl_linux-64                    12.3.0  he2b93b0_3           conda-forge\/linux-64     Cached\r\n  + gcc_linux-64                         12.3.0  h76fc315_2           conda-forge\/linux-64     Cached\r\n  + gettext                              0.21.1  h27087fc_0           conda-forge\/linux-64     Cached\r\n  + gfortran                             12.3.0  h499e0f7_2           conda-forge\/linux-64     Cached\r\n  + gfortran_impl_linux-64               12.3.0  hfcedea8_3           conda-forge\/linux-64     Cached\r\n  + gfortran_linux-64                    12.3.0  h7fe76b4_2           conda-forge\/linux-64     Cached\r\n  + glib                                 2.78.3  hfc55251_0           conda-forge\/linux-64      490kB\r\n  + glib-tools                           2.78.3  hfc55251_0           conda-forge\/linux-64      111kB\r\n  + gmp                                   6.3.0  h59595ed_0           conda-forge\/linux-64     Cached\r\n  + gmpy2                                 2.1.2  py311h6a5fa03_1      conda-forge\/linux-64     Cached\r\n  + graphite2                            1.3.13  h58526e2_1001        conda-forge\/linux-64     Cached\r\n  + greenlet                              3.0.3  py311hb755f60_0      conda-forge\/linux-64      236kB\r\n  + gst-plugins-base                     1.22.8  h8e1006c_1           conda-forge\/linux-64        3MB\r\n  + gstreamer                            1.22.8  h98fc4e7_1           conda-forge\/linux-64        2MB\r\n  + gxx                                  12.3.0  h8d2909c_2           conda-forge\/linux-64     Cached\r\n  + gxx_impl_linux-64                    12.3.0  he2b93b0_3           conda-forge\/linux-64     Cached\r\n  + gxx_linux-64                         12.3.0  h8a814eb_2           conda-forge\/linux-64     Cached\r\n  + harfbuzz                              8.3.0  h3d44ed6_0           conda-forge\/linux-64     Cached\r\n  + hypothesis                           6.96.1  pyha770c72_0         conda-forge\/noarch        311kB\r\n  + icu                                    73.2  h59595ed_0           conda-forge\/linux-64     Cached\r\n  + idna                                    3.6  pyhd8ed1ab_0         conda-forge\/noarch         50kB\r\n  + imagesize                             1.4.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + importlib-metadata                    7.0.1  pyha770c72_0         conda-forge\/noarch       Cached\r\n  + importlib_metadata                    7.0.1  hd8ed1ab_0           conda-forge\/noarch       Cached\r\n  + importlib_resources                   6.1.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + iniconfig                             2.0.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + ipykernel                            6.29.0  pyhd33586a_0         conda-forge\/noarch        118kB\r\n  + ipython                              8.20.0  pyh707e725_0         conda-forge\/noarch        591kB\r\n  + jedi                                 0.19.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + jinja2                                3.1.3  pyhd8ed1ab_0         conda-forge\/noarch        112kB\r\n  + json5                                0.9.14  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + jsonschema                           4.21.0  pyhd8ed1ab_0         conda-forge\/noarch         72kB\r\n  + jsonschema-specifications         2023.12.1  pyhd8ed1ab_0         conda-forge\/noarch         16kB\r\n  + jupyter-cache                         1.0.0  pyhd8ed1ab_0         conda-forge\/noarch         31kB\r\n  + jupyter_client                        8.6.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + jupyter_core                          5.7.1  py311h38be061_0      conda-forge\/linux-64       95kB\r\n  + jupytext                             1.16.1  pyhd8ed1ab_0         conda-forge\/noarch        103kB\r\n  + kernel-headers_linux-64              2.6.32  he073ed8_16          conda-forge\/noarch       Cached\r\n  + keyutils                              1.6.1  h166bdaf_0           conda-forge\/linux-64     Cached\r\n  + kiwisolver                            1.4.5  py311h9547e67_1      conda-forge\/linux-64     Cached\r\n  + krb5                                 1.21.2  h659d440_0           conda-forge\/linux-64     Cached\r\n  + lame                                  3.100  h166bdaf_1003        conda-forge\/linux-64     Cached\r\n  + lcms2                                  2.16  hb7c19ff_0           conda-forge\/linux-64      245kB\r\n  + ld_impl_linux-64                       2.40  h41732ed_0           conda-forge\/linux-64     Cached\r\n  + lerc                                  4.0.0  h27087fc_0           conda-forge\/linux-64     Cached\r\n  + libblas                               3.9.0  20_linux64_openblas  conda-forge\/linux-64       14kB\r\n  + libbrotlicommon                       1.1.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + libbrotlidec                          1.1.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + libbrotlienc                          1.1.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + libcap                                 2.69  h0f662aa_0           conda-forge\/linux-64     Cached\r\n  + libcblas                              3.9.0  20_linux64_openblas  conda-forge\/linux-64       14kB\r\n  + libclang                             15.0.7  default_hb11cfb5_4   conda-forge\/linux-64      133kB\r\n  + libclang13                           15.0.7  default_ha2b6cf4_4   conda-forge\/linux-64       10MB\r\n  + libcups                               2.3.3  h4637d8d_4           conda-forge\/linux-64     Cached\r\n  + libdeflate                             1.19  hd590300_0           conda-forge\/linux-64     Cached\r\n  + libedit                        3.1.20191231  he28a2e2_2           conda-forge\/linux-64     Cached\r\n  + libevent                             2.1.12  hf998b51_1           conda-forge\/linux-64     Cached\r\n  + libexpat                              2.5.0  hcb278e6_1           conda-forge\/linux-64     Cached\r\n  + libffi                                3.4.2  h7f98852_5           conda-forge\/linux-64     Cached\r\n  + libflac                               1.4.3  h59595ed_0           conda-forge\/linux-64     Cached\r\n  + libgcc-devel_linux-64                12.3.0  h8bca6fd_103         conda-forge\/noarch       Cached\r\n  + libgcc-ng                            13.2.0  h807b86a_3           conda-forge\/linux-64     Cached\r\n  + libgcrypt                            1.10.3  hd590300_0           conda-forge\/linux-64      635kB\r\n  + libgfortran-ng                       13.2.0  h69a702a_3           conda-forge\/linux-64     Cached\r\n  + libgfortran5                         13.2.0  ha4646dd_3           conda-forge\/linux-64     Cached\r\n  + libglib                              2.78.3  h783c2da_0           conda-forge\/linux-64        3MB\r\n  + libgomp                              13.2.0  h807b86a_3           conda-forge\/linux-64     Cached\r\n  + libgpg-error                           1.47  h71f35ed_0           conda-forge\/linux-64     Cached\r\n  + libiconv                               1.17  hd590300_2           conda-forge\/linux-64      706kB\r\n  + libjpeg-turbo                         3.0.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + liblapack                             3.9.0  20_linux64_openblas  conda-forge\/linux-64       14kB\r\n  + libllvm15                            15.0.7  hb3ce162_4           conda-forge\/linux-64       33MB\r\n  + libnsl                                2.0.1  hd590300_0           conda-forge\/linux-64     Cached\r\n  + libogg                                1.3.4  h7f98852_1           conda-forge\/linux-64     Cached\r\n  + libopenblas                          0.3.25  pthreads_h413a1c8_0  conda-forge\/linux-64        6MB\r\n  + libopus                               1.3.1  h7f98852_1           conda-forge\/linux-64     Cached\r\n  + libpng                               1.6.39  h753d276_0           conda-forge\/linux-64     Cached\r\n  + libpq                                  16.1  h33b98f1_7           conda-forge\/linux-64        2MB\r\n  + libsanitizer                         12.3.0  h0f45ef3_3           conda-forge\/linux-64     Cached\r\n  + libsndfile                            1.2.2  hc60ed4a_1           conda-forge\/linux-64     Cached\r\n  + libsodium                            1.0.18  h36c2ea0_1           conda-forge\/linux-64     Cached\r\n  + libsqlite                            3.44.2  h2797004_0           conda-forge\/linux-64     Cached\r\n  + libstdcxx-devel_linux-64             12.3.0  h8bca6fd_103         conda-forge\/noarch       Cached\r\n  + libstdcxx-ng                         13.2.0  h7e041cc_3           conda-forge\/linux-64     Cached\r\n  + libsystemd0                             255  h3516f8a_0           conda-forge\/linux-64      404kB\r\n  + libtiff                               4.6.0  ha9c0a0a_2           conda-forge\/linux-64     Cached\r\n  + libuuid                              2.38.1  h0b41bf4_0           conda-forge\/linux-64     Cached\r\n  + libvorbis                             1.3.7  h9c3ff4c_0           conda-forge\/linux-64     Cached\r\n  + libwebp-base                          1.3.2  hd590300_0           conda-forge\/linux-64     Cached\r\n  + libxcb                                 1.15  h0b41bf4_0           conda-forge\/linux-64     Cached\r\n  + libxcrypt                            4.4.36  hd590300_1           conda-forge\/linux-64      100kB\r\n  + libxkbcommon                          1.6.0  hd429924_1           conda-forge\/linux-64      575kB\r\n  + libxml2                              2.12.4  h232c23b_1           conda-forge\/linux-64      705kB\r\n  + libzlib                              1.2.13  hd590300_5           conda-forge\/linux-64     Cached\r\n  + lz4-c                                 1.9.4  hcb278e6_0           conda-forge\/linux-64     Cached\r\n  + markdown-it-py                        3.0.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + markupsafe                            2.1.3  py311h459d7ec_1      conda-forge\/linux-64     Cached\r\n  + matplotlib                            3.8.2  py311h38be061_0      conda-forge\/linux-64        8kB\r\n  + matplotlib-base                       3.8.2  py311h54ef318_0      conda-forge\/linux-64        8MB\r\n  + matplotlib-inline                     0.1.6  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + mdit-py-plugins                       0.4.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + mdurl                                 0.1.2  pyhd8ed1ab_0         conda-forge\/noarch         15kB\r\n  + meson                                 1.3.1  pyhd8ed1ab_0         conda-forge\/noarch        628kB\r\n  + meson-python                         0.15.0  pyh0c530f3_0         conda-forge\/noarch       Cached\r\n  + mpc                                   1.3.1  hfe3b2da_0           conda-forge\/linux-64     Cached\r\n  + mpfr                                  4.2.1  h9458935_0           conda-forge\/linux-64     Cached\r\n  + mpg123                               1.32.4  h59595ed_0           conda-forge\/linux-64      491kB\r\n  + mpmath                                1.3.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + munkres                               1.1.4  pyh9f0ad1d_0         conda-forge\/noarch       Cached\r\n  + mypy                                  1.8.0  py311h459d7ec_0      conda-forge\/linux-64       18MB\r\n  + mypy_extensions                       1.0.0  pyha770c72_0         conda-forge\/noarch       Cached\r\n  + mysql-common                         8.0.33  hf1915f5_6           conda-forge\/linux-64     Cached\r\n  + mysql-libs                           8.0.33  hca2cd23_6           conda-forge\/linux-64     Cached\r\n  + myst-nb                               1.0.0  pyhd8ed1ab_0         conda-forge\/noarch         63kB\r\n  + myst-parser                           2.0.0  pyhd8ed1ab_0         conda-forge\/noarch         67kB\r\n  + nbclient                              0.8.0  pyhd8ed1ab_0         conda-forge\/noarch         65kB\r\n  + nbformat                              5.9.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + ncurses                                 6.4  h59595ed_2           conda-forge\/linux-64     Cached\r\n  + nest-asyncio                          1.5.9  pyhd8ed1ab_0         conda-forge\/noarch         12kB\r\n  + ninja                                1.11.1  h924138e_0           conda-forge\/linux-64     Cached\r\n  + nspr                                   4.35  h27087fc_0           conda-forge\/linux-64     Cached\r\n  + nss                                    3.96  h1d7d5a4_0           conda-forge\/linux-64        2MB\r\n  + numpy                                1.26.3  py311h64a7726_0      conda-forge\/linux-64        8MB\r\n  + numpydoc                              1.6.0  pyhd8ed1ab_0         conda-forge\/noarch         57kB\r\n  + openblas                             0.3.25  pthreads_h7a3da1a_0  conda-forge\/linux-64        6MB\r\n  + openjpeg                              2.5.0  h488ebb8_3           conda-forge\/linux-64     Cached\r\n  + openssl                               3.2.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + packaging                              23.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + parso                                 0.8.3  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pcre2                                 10.42  hcad00b1_0           conda-forge\/linux-64     Cached\r\n  + pexpect                               4.8.0  pyh1a96a4e_2         conda-forge\/noarch       Cached\r\n  + pickleshare                           0.7.5  py_1003              conda-forge\/noarch       Cached\r\n  + pillow                               10.2.0  py311ha6c5da5_0      conda-forge\/linux-64       42MB\r\n  + pip                                  23.3.2  pyhd8ed1ab_0         conda-forge\/noarch          1MB\r\n  + pixman                               0.43.0  h59595ed_0           conda-forge\/linux-64      387kB\r\n  + pkg-config                           0.29.2  h36c2ea0_1008        conda-forge\/linux-64     Cached\r\n  + pkgutil-resolve-name                 1.3.10  pyhd8ed1ab_1         conda-forge\/noarch       Cached\r\n  + platformdirs                          4.1.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pluggy                                1.3.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + ply                                    3.11  py_1                 conda-forge\/noarch       Cached\r\n  + pooch                                 1.8.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + prompt-toolkit                       3.0.42  pyha770c72_0         conda-forge\/noarch       Cached\r\n  + psutil                                5.9.7  py311h459d7ec_0      conda-forge\/linux-64      501kB\r\n  + pthread-stubs                           0.4  h36c2ea0_1001        conda-forge\/linux-64     Cached\r\n  + ptyprocess                            0.7.0  pyhd3deb0d_0         conda-forge\/noarch       Cached\r\n  + pulseaudio-client                      16.1  hb77b528_5           conda-forge\/linux-64     Cached\r\n  + pure_eval                             0.2.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pybind11                             2.11.1  py311h9547e67_2      conda-forge\/linux-64     Cached\r\n  + pybind11-global                      2.11.1  py311h9547e67_2      conda-forge\/linux-64     Cached\r\n  + pycodestyle                          2.11.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pydata-sphinx-theme                   0.9.0  pyhd8ed1ab_1         conda-forge\/noarch       Cached\r\n  + pydevtool                             0.3.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pygments                             2.17.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pympler                               1.0.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pyparsing                             3.1.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pyproject-metadata                    0.7.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pyqt                                 5.15.9  py311hf0fb5b6_5      conda-forge\/linux-64     Cached\r\n  + pyqt5-sip                           12.12.2  py311hb755f60_5      conda-forge\/linux-64     Cached\r\n  + pysocks                               1.7.1  pyha2e5f31_6         conda-forge\/noarch       Cached\r\n  + pytest                                7.4.4  pyhd8ed1ab_0         conda-forge\/noarch        245kB\r\n  + pytest-cov                            4.1.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pytest-timeout                        2.2.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pytest-xdist                          3.5.0  pyhd8ed1ab_0         conda-forge\/noarch         37kB\r\n  + python                               3.11.7  hab00c5b_1_cpython   conda-forge\/linux-64       31MB\r\n  + python-dateutil                       2.8.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + python-fastjsonschema                2.19.1  pyhd8ed1ab_0         conda-forge\/noarch        225kB\r\n  + python_abi                             3.11  4_cp311              conda-forge\/linux-64     Cached\r\n  + pythran                              0.15.0  py311h92ebd52_1      conda-forge\/linux-64        2MB\r\n  + pytz                           2023.3.post1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + pyyaml                                6.0.1  py311h459d7ec_1      conda-forge\/linux-64     Cached\r\n  + pyzmq                                25.1.2  py311h34ded2d_0      conda-forge\/linux-64      537kB\r\n  + qt-main                              5.15.8  h450f30e_18          conda-forge\/linux-64       61MB\r\n  + readline                                8.2  h8228510_1           conda-forge\/linux-64     Cached\r\n  + referencing                          0.32.1  pyhd8ed1ab_0         conda-forge\/noarch         39kB\r\n  + requests                             2.31.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + rich                                 13.7.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + rich-click                            1.7.3  pyhd8ed1ab_0         conda-forge\/noarch         33kB\r\n  + rpds-py                              0.17.1  py311h46250e7_0      conda-forge\/linux-64        1MB\r\n  + ruff                                 0.1.13  py311h7145743_1      conda-forge\/linux-64        6MB\r\n  + setuptools                           67.1.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + sip                                  6.7.12  py311hb755f60_0      conda-forge\/linux-64     Cached\r\n  + six                                  1.16.0  pyh6c4a22f_0         conda-forge\/noarch       Cached\r\n  + snowballstemmer                       2.2.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + sortedcontainers                      2.4.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + soupsieve                               2.5  pyhd8ed1ab_1         conda-forge\/noarch       Cached\r\n  + sphinx                                7.2.6  pyhd8ed1ab_0         conda-forge\/noarch          1MB\r\n  + sphinx-design                         0.5.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + sphinxcontrib-applehelp               1.0.8  pyhd8ed1ab_0         conda-forge\/noarch         30kB\r\n  + sphinxcontrib-devhelp                 1.0.6  pyhd8ed1ab_0         conda-forge\/noarch         24kB\r\n  + sphinxcontrib-htmlhelp                2.0.5  pyhd8ed1ab_0         conda-forge\/noarch         33kB\r\n  + sphinxcontrib-jsmath                  1.0.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + sphinxcontrib-qthelp                  1.0.7  pyhd8ed1ab_0         conda-forge\/noarch         27kB\r\n  + sphinxcontrib-serializinghtml        1.1.10  pyhd8ed1ab_0         conda-forge\/noarch         29kB\r\n  + sqlalchemy                           2.0.25  py311h459d7ec_0      conda-forge\/linux-64        4MB\r\n  + stack_data                            0.6.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + sysroot_linux-64                       2.12  he073ed8_16          conda-forge\/noarch       Cached\r\n  + tabulate                              0.9.0  pyhd8ed1ab_1         conda-forge\/noarch       Cached\r\n  + threadpoolctl                         3.2.0  pyha21a80b_0         conda-forge\/noarch       Cached\r\n  + tk                                   8.6.13  noxft_h4845f30_101   conda-forge\/linux-64     Cached\r\n  + tokenize-rt                           5.2.0  pyhd8ed1ab_1         conda-forge\/noarch         12kB\r\n  + toml                                 0.10.2  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + tomli                                 2.0.1  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + tornado                               6.3.3  py311h459d7ec_1      conda-forge\/linux-64     Cached\r\n  + traitlets                            5.14.1  pyhd8ed1ab_0         conda-forge\/noarch        110kB\r\n  + types-psutil                        5.9.5.6  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + typing-extensions                     4.9.0  hd8ed1ab_0           conda-forge\/noarch         10kB\r\n  + typing_extensions                     4.9.0  pyha770c72_0         conda-forge\/noarch       Cached\r\n  + tzdata                                2023d  h0c530f3_0           conda-forge\/noarch        120kB\r\n  + urllib3                               2.1.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + wcwidth                              0.2.13  pyhd8ed1ab_0         conda-forge\/noarch         33kB\r\n  + wheel                                0.42.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + xcb-util                              0.4.0  hd590300_1           conda-forge\/linux-64     Cached\r\n  + xcb-util-image                        0.4.0  h8ee46fc_1           conda-forge\/linux-64     Cached\r\n  + xcb-util-keysyms                      0.4.0  h8ee46fc_1           conda-forge\/linux-64     Cached\r\n  + xcb-util-renderutil                   0.3.9  hd590300_1           conda-forge\/linux-64     Cached\r\n  + xcb-util-wm                           0.4.1  h8ee46fc_1           conda-forge\/linux-64     Cached\r\n  + xkeyboard-config                       2.40  hd590300_0           conda-forge\/linux-64     Cached\r\n  + xorg-kbproto                          1.0.7  h7f98852_1002        conda-forge\/linux-64     Cached\r\n  + xorg-libice                           1.1.1  hd590300_0           conda-forge\/linux-64     Cached\r\n  + xorg-libsm                            1.2.4  h7391055_0           conda-forge\/linux-64     Cached\r\n  + xorg-libx11                           1.8.7  h8ee46fc_0           conda-forge\/linux-64     Cached\r\n  + xorg-libxau                          1.0.11  hd590300_0           conda-forge\/linux-64     Cached\r\n  + xorg-libxdmcp                         1.1.3  h7f98852_0           conda-forge\/linux-64     Cached\r\n  + xorg-libxext                          1.3.4  h0b41bf4_2           conda-forge\/linux-64     Cached\r\n  + xorg-libxrender                      0.9.11  hd590300_0           conda-forge\/linux-64     Cached\r\n  + xorg-renderproto                     0.11.1  h7f98852_1002        conda-forge\/linux-64     Cached\r\n  + xorg-xextproto                        7.3.0  h0b41bf4_1003        conda-forge\/linux-64     Cached\r\n  + xorg-xf86vidmodeproto                 2.3.1  h7f98852_1002        conda-forge\/linux-64     Cached\r\n  + xorg-xproto                          7.0.31  h7f98852_1007        conda-forge\/linux-64     Cached\r\n  + xz                                    5.2.6  h166bdaf_0           conda-forge\/linux-64     Cached\r\n  + yaml                                  0.2.5  h7f98852_2           conda-forge\/linux-64     Cached\r\n  + zeromq                                4.3.5  h59595ed_0           conda-forge\/linux-64     Cached\r\n  + zipp                                 3.17.0  pyhd8ed1ab_0         conda-forge\/noarch       Cached\r\n  + zlib                                 1.2.13  hd590300_5           conda-forge\/linux-64     Cached\r\n  + zstd                                  1.5.5  hfc55251_0           conda-forge\/linux-64     Cached\r\n\r\n  Summary:\r\n\r\n  Install: 293 packages\r\n\r\n  Total download: 276MB\r\n\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\n\r\nlibxcrypt                                          100.4kB @   1.1MB\/s  0.1s\r\nmpg123                                             491.1kB @   3.8MB\/s  0.1s\r\nlibiconv                                           705.8kB @   4.8MB\/s  0.1s\r\npixman                                             387.3kB @   2.4MB\/s  0.2s\r\nlibxml2                                            704.9kB @   4.4MB\/s  0.2s\r\nlibsystemd0                                        404.3kB @   2.5MB\/s  0.1s\r\nlibclang                                           133.4kB @ 676.0kB\/s  0.0s\r\nlcms2                                              245.2kB @   1.2MB\/s  0.1s\r\nliblapack                                           14.3kB @  63.9kB\/s  0.1s\r\ntzdata                                             119.6kB @ 530.0kB\/s  0.1s\r\npython-fastjsonschema                              225.2kB @ 798.1kB\/s  0.1s\r\ntraitlets                                          110.3kB @ 377.1kB\/s  0.1s\r\nattrs                                               54.6kB @ 186.2kB\/s  0.1s\r\npip                                                  1.4MB @   4.5MB\/s  0.1s\r\nbeautifulsoup4                                     118.2kB @ 344.9kB\/s  0.1s\r\ntyping-extensions                                   10.2kB @  29.7kB\/s  0.1s\r\npytest                                             244.6kB @ 489.4kB\/s  0.2s\r\nrich-click                                          32.8kB @  65.6kB\/s  0.2s\r\ngreenlet                                           236.0kB @ 450.3kB\/s  0.2s\r\npyzmq                                              536.5kB @   1.0MB\/s  0.2s\r\nlibclang13                                           9.6MB @  13.3MB\/s  0.8s\r\ngstreamer                                            2.0MB @   1.5MB\/s  0.9s\r\nsqlalchemy                                           3.5MB @   2.5MB\/s  0.9s\r\nreferencing                                         39.0kB @  27.7kB\/s  0.4s\r\nhypothesis                                         310.5kB @ 191.8kB\/s  0.2s\r\njinja2                                             111.6kB @  68.9kB\/s  0.2s\r\ncython-lint                                         18.0kB @  11.1kB\/s  0.2s\r\nruff                                                 5.5MB @   3.2MB\/s  1.6s\r\nsphinxcontrib-devhelp                               24.5kB @  11.8kB\/s  0.5s\r\nnbclient                                            64.9kB @  31.2kB\/s  0.5s\r\njupytext                                           102.9kB @  49.5kB\/s  0.5s\r\nnumpy                                                8.2MB @   3.8MB\/s  2.0s\r\nmyst-nb                                             62.7kB @  28.8kB\/s  0.4s\r\nlibgcrypt                                          634.9kB @ 289.5kB\/s  0.6s\r\nnss                                                  2.0MB @ 736.0kB\/s  0.8s\r\nlibcblas                                            14.4kB @   5.3kB\/s  0.4s\r\nfont-ttf-ubuntu                                      1.6MB @ 553.8kB\/s  1.0s\r\nmdurl                                               14.7kB @   4.2kB\/s  0.6s\r\nexceptiongroup                                      20.6kB @   5.9kB\/s  0.6s\r\nlibopenblas                                          5.5MB @   1.6MB\/s  1.5s\r\npytest-xdist                                        36.5kB @  10.0kB\/s  0.2s\r\nipython                                            591.0kB @ 162.1kB\/s  0.2s\r\nglib                                               489.5kB @ 134.1kB\/s  0.2s\r\ncython                                               3.7MB @ 897.7kB\/s  0.6s\r\ngst-plugins-base                                     2.7MB @ 642.6kB\/s  0.6s\r\nsphinxcontrib-htmlhelp                              33.5kB @   7.9kB\/s  0.1s\r\npythran                                              2.4MB @ 550.7kB\/s  0.6s\r\njupyter-cache                                       31.4kB @   7.3kB\/s  0.1s\r\nlibblas                                             14.4kB @   3.3kB\/s  0.1s\r\nlibglib                                              2.7MB @ 609.8kB\/s  0.2s\r\ncxx-compiler                                         6.3kB @   1.4kB\/s  0.1s\r\ntokenize-rt                                         11.8kB @   2.6kB\/s  0.1s\r\nnest-asyncio                                        11.7kB @   2.6kB\/s  0.1s\r\ncomm                                                12.2kB @   1.6kB\/s  3.0s\r\nrpds-py                                              1.0MB @ 135.8kB\/s  3.0s\r\nmatplotlib-base                                      7.9MB @   1.1MB\/s  3.9s\r\npsutil                                             501.4kB @  66.0kB\/s  0.1s\r\ndocutils                                           918.4kB @ 120.9kB\/s  0.1s\r\npython                                              30.8MB @   4.0MB\/s  5.2s\r\nsphinxcontrib-applehelp                             29.5kB @   3.8kB\/s  0.1s\r\nglib-tools                                         110.7kB @  14.3kB\/s  0.0s\r\nsphinx                                               1.3MB @ 165.1kB\/s  0.2s\r\ncompilers                                            7.1kB @ 905.0 B\/s  0.1s\r\nalabaster                                           18.4kB @   2.3kB\/s  0.1s\r\nmeson                                              628.0kB @  79.0kB\/s  0.1s\r\ncolorlog                                            21.9kB @   2.7kB\/s  0.2s\r\nopenblas                                             5.7MB @ 694.2kB\/s  0.5s\r\nfonttools                                            2.8MB @ 345.5kB\/s  0.3s\r\njsonschema-specifications                           16.4kB @   2.0kB\/s  0.1s\r\nsphinxcontrib-serializinghtml                       28.8kB @   3.5kB\/s  0.1s\r\nnumpydoc                                            56.6kB @   6.8kB\/s  0.1s\r\nmyst-parser                                         67.1kB @   8.1kB\/s  0.1s\r\nwcwidth                                             32.7kB @   3.9kB\/s  0.2s\r\nidna                                                50.1kB @   5.9kB\/s  0.2s\r\njsonschema                                          72.3kB @   7.2kB\/s  1.5s\r\nsphinxcontrib-qthelp                                27.0kB @   2.7kB\/s  1.5s\r\nmatplotlib                                           8.4kB @ 651.0 B\/s  2.9s\r\nbabel                                                7.6MB @ 588.2kB\/s  4.7s\r\nlibxkbcommon                                       574.9kB @  44.2kB\/s  0.1s\r\njupyter_core                                        94.7kB @   7.3kB\/s  0.1s\r\nc-compiler                                           6.3kB @ 481.0 B\/s  0.1s\r\ncoverage                                           364.9kB @  27.8kB\/s  0.1s\r\nfortran-compiler                                     6.3kB @ 479.0 B\/s  0.1s\r\nipykernel                                          118.2kB @   9.0kB\/s  0.1s\r\nlibllvm15                                           33.3MB @   2.5MB\/s  9.1s\r\nlibpq                                                2.5MB @ 183.9kB\/s  0.2s\r\nmypy                                                17.7MB @   1.3MB\/s  3.7s\r\nqt-main                                             61.3MB @   4.2MB\/s  7.2s\r\npillow                                              41.6MB @   2.8MB\/s  1.8s\r\n\r\nDownloading and Extracting Packages\r\n\r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n#\r\n# To activate this environment, use\r\n#\r\n#     $ conda activate scipy-dev\r\n#\r\n# To deactivate an active environment, use\r\n#\r\n#     $ conda deactivate\r\n```\r\n<\/details>\r\n\r\n```\r\nmamba activate scipy-dev\r\n```\r\n\r\n<details><summary><code>python dev.py build<\/code><\/summary>\r\n\r\n```\r\n\ud83d\udcbb  ninja -C \/home\/jgross\/Documents\/GitHub\/scipy\/build -j4\r\nninja: Entering directory `\/home\/jgross\/Documents\/GitHub\/scipy\/build'\r\n[0\/1] Regenerating build files.\r\nRegenerating configuration from scratch: Build directory has been generated with Meson version 1.2.3, which is incompatible with the current version 1.3.1.\r\nThe Meson build system\r\nVersion: 1.3.1\r\nSource dir: \/home\/jgross\/Documents\/GitHub\/scipy\r\nBuild dir: \/home\/jgross\/Documents\/GitHub\/scipy\/build\r\nBuild type: native build\r\nProject name: SciPy\r\nProject version: 1.13.0.dev0\r\nC compiler for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc (gcc 12.3.0 \"x86_64-conda-linux-gnu-cc (conda-forge gcc 12.3.0-3) 12.3.0\")\r\nC linker for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc ld.bfd 2.40\r\nC++ compiler for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ (gcc 12.3.0 \"x86_64-conda-linux-gnu-c++ (conda-forge gcc 12.3.0-3) 12.3.0\")\r\nC++ linker for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ ld.bfd 2.40\r\nCython compiler for the host machine: cython (cython 3.0.7)\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram python3 found: YES (\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/python3.11)\r\nFound pkg-config: YES (\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/pkg-config) 0.29.2\r\nRun-time dependency python found: YES 3.11\r\nProgram cython found: YES (\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/cython)\r\nCompiler for C supports arguments -Wno-unused-but-set-variable: YES\r\nCompiler for C supports arguments -Wno-unused-function: YES\r\nCompiler for C supports arguments -Wno-conversion: YES\r\nCompiler for C supports arguments -Wno-misleading-indentation: YES\r\nLibrary m found: YES\r\nFortran compiler for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-gfortran (gcc 12.3.0 \"GNU Fortran (conda-forge gcc 12.3.0-3) 12.3.0\")\r\nFortran linker for the host machine: \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-gfortran ld.bfd 2.40\r\nCompiler for Fortran supports arguments -Wno-conversion: YES\r\nChecking if \"-Wl,--version-script\" : links: YES\r\nProgram pythran found: YES 0.15.0 0.15.0 (\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/pythran)\r\nFound CMake: \/home\/jgross\/.local\/bin\/cmake (3.26.0)\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nRun-time dependency xsimd found: YES 7.6.0\r\nRun-time dependency threads found: YES\r\nLibrary npymath found: YES\r\nLibrary npyrandom found: YES\r\nRun-time dependency pybind11 found: YES 2.11.1\r\nRun-time dependency scipy-openblas found: NO (tried pkgconfig)\r\nRun-time dependency openblas found: YES 0.3.25\r\nDependency openblas found: YES 0.3.25 (cached)\r\nCompiler for C supports arguments -Wno-maybe-uninitialized: YES\r\nCompiler for C supports arguments -Wno-discarded-qualifiers: YES\r\nCompiler for C supports arguments -Wno-empty-body: YES\r\nCompiler for C supports arguments -Wno-implicit-function-declaration: YES\r\nCompiler for C supports arguments -Wno-parentheses: YES\r\nCompiler for C supports arguments -Wno-switch: YES\r\nCompiler for C supports arguments -Wno-unused-label: YES\r\nCompiler for C supports arguments -Wno-unused-result: YES\r\nCompiler for C supports arguments -Wno-unused-variable: YES\r\nCompiler for C supports arguments -Wno-incompatible-pointer-types: YES\r\nCompiler for C++ supports arguments -Wno-cpp: YES\r\nCompiler for C++ supports arguments -Wno-deprecated-declarations: YES\r\nCompiler for C++ supports arguments -Wno-class-memaccess: YES\r\nCompiler for C++ supports arguments -Wno-format-truncation: YES\r\nCompiler for C++ supports arguments -Wno-non-virtual-dtor: YES\r\nCompiler for C++ supports arguments -Wno-sign-compare: YES\r\nCompiler for C++ supports arguments -Wno-switch: YES\r\nCompiler for C++ supports arguments -Wno-terminate: YES\r\nCompiler for C++ supports arguments -Wno-unused-but-set-variable: YES\r\nCompiler for C++ supports arguments -Wno-unused-function: YES\r\nCompiler for C++ supports arguments -Wno-unused-local-typedefs: YES\r\nCompiler for C++ supports arguments -Wno-unused-variable: YES\r\nCompiler for C++ supports arguments -Wno-int-in-bool-context: YES\r\nCompiler for Fortran supports arguments -Wno-argument-mismatch: YES\r\nCompiler for Fortran supports arguments -Wno-conversion: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-intrinsic-shadow: YES\r\nCompiler for Fortran supports arguments -Wno-maybe-uninitialized: YES\r\nCompiler for Fortran supports arguments -Wno-surprising: YES\r\nCompiler for Fortran supports arguments -Wno-uninitialized: YES\r\nCompiler for Fortran supports arguments -Wno-unused-dummy-argument: YES\r\nCompiler for Fortran supports arguments -Wno-unused-label: YES\r\nCompiler for Fortran supports arguments -Wno-unused-variable: YES\r\nCompiler for Fortran supports arguments -Wno-tabs: YES\r\nCompiler for Fortran supports arguments -Wno-argument-mismatch: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-conversion: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-maybe-uninitialized: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-dummy-argument: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-label: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-variable: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-tabs: YES (cached)\r\nChecking if \"Check atomic builtins without -latomic\" : links: YES\r\nConfiguring __config__.py using configuration\r\nChecking for function \"open_memstream\" : NO\r\nConfiguring messagestream_config.h using configuration\r\nCompiler for Fortran supports arguments -w: YES\r\nChecking for size of \"void*\" : 8\r\nCompiler for Fortran supports arguments -w: YES (cached)\r\nBuild targets in project: 248\r\n\r\nSciPy 1.13.0.dev0\r\n\r\n  User defined options\r\n    prefix: \/home\/jgross\/Documents\/GitHub\/scipy\/build-install\r\n\r\nFound ninja-1.11.1 at \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/ninja\r\nCleaning... 0 files.\r\n[26\/1000] Generating scipy\/special\/cython_special with a custom command\r\nscipy\/special\/_generate_pyx.py: all files up-to-date\r\n[53\/1000] Generating scipy\/linalg\/cython_linalg with a custom command\r\nscipy\/linalg\/_generate_pyx.py: all files up-to-date\r\n[193\/997] Compiling C++ object scipy\/stats\/_stats_pythran.cp...86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o\r\nFAILED: scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ -Iscipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/stats -I..\/scipy\/stats -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include -I\/usr\/include -I\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11 -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++17 -O2 -g -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -DENABLE_PYTHON_MODULE -D__PYTHRAN__=3 -DPYTHRAN_BLAS_NONE -Wno-cpp -Wno-deprecated-declarations -Wno-unused-but-set-variable -Wno-unused-function -Wno-unused-variable -Wno-int-in-bool-context -MD -MQ scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -MF scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d -o scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -c scipy\/stats\/_stats_pythran.cpp\r\nIn file included from \/usr\/include\/features.h:392,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/x86_64-conda-linux-gnu\/bits\/os_defines.h:39,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/x86_64-conda-linux-gnu\/bits\/c++config.h:655,\r\n                 from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0\/type_traits:38,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/include\/types\/assignable.hpp:4,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/types\/assignable.hpp:4,\r\n                 from ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\/pythonic\/core.hpp:41,\r\n                 from scipy\/stats\/_stats_pythran.cpp:1:\r\n\/usr\/include\/features-time64.h:21:10: fatal error: bits\/timesize.h: No such file or directory\r\n   21 | #include <bits\/timesize.h>\r\n      |          ^~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n[196\/997] Compiling C++ object scipy\/sparse\/sparsetools\/_sparsetools.cpython-311-x86_64-linux-gnu.so.p\/bsr.cxx.o\r\nninja: build stopped: subcommand failed.\r\nBuild failed!\r\n```\r\n<\/details>","It's weird that this it can't find the header though, because it exists on my system:\r\n```\r\n$ sudo find \/usr\/include \/usr\/local\/include -name 'timesize.h'\r\n\/usr\/include\/x86_64-linux-gnu\/bits\/timesize.h\r\n```","I guess the (low-level) problem is that `-I` is not recursive and `\/usr\/include\/x86_64-linux-gnu\/bits\/timesize.h` won't be found with `-I \/usr\/include` when `\/usr\/include\/features-time64.h` has `#include <bits\/timesize.h>`?","What's the output of `echo | gcc -v -E -`? Among other things, this will show which paths gcc searches for header files.","<details><summary><code>echo | gcc -v -E -<\/code><\/summary>\r\n\r\n```\r\nReading specs from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/specs\r\ncould not find specs file conda.specs\r\nCOLLECT_GCC=gcc\r\nTarget: x86_64-conda-linux-gnu\r\nConfigured with: ..\/configure --prefix=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --with-slibdir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib --libdir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib --mandir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/man --build=x86_64-conda-linux-gnu --host=x86_64-conda-linux-gnu --target=x86_64-conda-linux-gnu --enable-default-pie --enable-languages=c,c++,fortran,objc,obj-c++ --enable-__cxa_atexit --disable-libmudflap --enable-libgomp --disable-libssp --enable-libquadmath --enable-libquadmath-support --enable-libsanitizer --enable-lto --enable-threads=posix --enable-target-optspace --enable-plugin --enable-gold --disable-nls --disable-bootstrap --disable-multilib --enable-long-long --with-sysroot=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/x86_64-conda-linux-gnu\/sysroot --with-build-sysroot=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_build_env\/x86_64-conda-linux-gnu\/sysroot --with-gxx-include-dir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0 --with-pkgversion='conda-forge gcc 12.3.0-3' --with-bugurl=https:\/\/github.com\/conda-forge\/ctng-compilers-feedstock\/issues\/new\/choose\r\nThread model: posix\r\nSupported LTO compression algorithms: zlib\r\ngcc version 12.3.0 (conda-forge gcc 12.3.0-3)\r\nCOLLECT_GCC_OPTIONS='-v' '-E' '-mtune=generic' '-march=x86-64'\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/cc1 -E -quiet -v -iprefix \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/ -isysroot \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot - -mtune=generic -march=x86-64 -dumpbase -\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\"\r\nignoring nonexistent directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/local\/include\"\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include-fixed\"\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/include\"\r\n#include \"...\" search starts here:\r\n#include <...> search starts here:\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include-fixed\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/include\r\nEnd of search list.\r\n# 0 \"<stdin>\"\r\n# 0 \"<built-in>\"\r\n# 0 \"<command-line>\"\r\n# 1 \"<stdin>\"\r\nCOMPILER_PATH=\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/bin\/\r\nLIBRARY_PATH=\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/lib\/\r\nCOLLECT_GCC_OPTIONS='-v' '-E' '-mtune=generic' '-march=x86-64'\r\n```\r\n<\/details>","But maybe you wanted me to mimic the call invoked by ninja\r\n<details><summary><code>$ echo | \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++ -Iscipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p -Iscipy\/stats -I..\/scipy\/stats -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran -I..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include -I\/usr\/include -I\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11 -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++17 -O2 -g -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -DENABLE_PYTHON_MODULE -D__PYTHRAN__=3 -DPYTHRAN_BLAS_NONE -Wno-cpp -Wno-deprecated-declarations -Wno-unused-but-set-variable -Wno-unused-function -Wno-unused-variable -Wno-int-in-bool-context -MD -MQ scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -MF scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d -v -E - 2>&1<\/code><\/summary>\r\n\r\n```\r\nReading specs from \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/specs\r\ncould not find specs file conda.specs\r\nCOLLECT_GCC=\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++\r\nTarget: x86_64-conda-linux-gnu\r\nConfigured with: ..\/configure --prefix=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho --with-slibdir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib --libdir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib --mandir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/man --build=x86_64-conda-linux-gnu --host=x86_64-conda-linux-gnu --target=x86_64-conda-linux-gnu --enable-default-pie --enable-languages=c,c++,fortran,objc,obj-c++ --enable-__cxa_atexit --disable-libmudflap --enable-libgomp --disable-libssp --enable-libquadmath --enable-libquadmath-support --enable-libsanitizer --enable-lto --enable-threads=posix --enable-target-optspace --enable-plugin --enable-gold --disable-nls --disable-bootstrap --disable-multilib --enable-long-long --with-sysroot=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/x86_64-conda-linux-gnu\/sysroot --with-build-sysroot=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_build_env\/x86_64-conda-linux-gnu\/sysroot --with-gxx-include-dir=\/home\/conda\/feedstock_root\/build_artifacts\/gcc_compilers_1699751145211\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/x86_64-conda-linux-gnu\/include\/c++\/12.3.0 --with-pkgversion='conda-forge gcc 12.3.0-3' --with-bugurl=https:\/\/github.com\/conda-forge\/ctng-compilers-feedstock\/issues\/new\/choose\r\nThread model: posix\r\nSupported LTO compression algorithms: zlib\r\ngcc version 12.3.0 (conda-forge gcc 12.3.0-3)\r\nCOLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-I' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p' '-I' 'scipy\/stats' '-I' '..\/scipy\/stats' '-I' '..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran' '-I' '..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include' '-I' '\/usr\/include' '-I' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11' '-fvisibility=hidden' '-D' '_FILE_OFFSET_BITS=64' '-Wall' '-Winvalid-pch' '-std=c++17' '-O2' '-g' '-fvisibility-inlines-hidden' '-fmessage-length=0' '-march=nocona' '-mtune=haswell' '-ftree-vectorize' '-fstack-protector-strong' '-fno-plt' '-O2' '-ffunction-sections' '-pipe' '-isystem' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include' '-D' 'NDEBUG' '-D' '_FORTIFY_SOURCE=2' '-O2' '-isystem' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include' '-fPIC' '-D' 'NPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION' '-D' 'ENABLE_PYTHON_MODULE' '-D' '__PYTHRAN__=3' '-D' 'PYTHRAN_BLAS_NONE' '-Wno-cpp' '-Wno-deprecated-declarations' '-Wno-unused-but-set-variable' '-Wno-unused-function' '-Wno-unused-variable' '-Wno-int-in-bool-context' '-MD' '-MQ' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o' '-MF' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d' '-v' '-E' '-shared-libgcc'\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/cc1 -E -quiet -v -I scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p -I scipy\/stats -I ..\/scipy\/stats -I ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran -I ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include -I \/usr\/include -I \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11 -iprefix \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/ -isysroot \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot -MD -.d -MF scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d -MQ scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o -D _FILE_OFFSET_BITS=64 -D NDEBUG -D _FORTIFY_SOURCE=2 -D NPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -D ENABLE_PYTHON_MODULE -D __PYTHRAN__=3 -D PYTHRAN_BLAS_NONE -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include -isystem \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include - -march=nocona -mtune=haswell -std=c++17 -Wall -Winvalid-pch -Wno-cpp -Wno-deprecated-declarations -Wno-unused-but-set-variable -Wno-unused-function -Wno-unused-variable -Wno-int-in-bool-context -fdiagnostics-color=always -fvisibility=hidden -fvisibility-inlines-hidden -fmessage-length=0 -ftree-vectorize -fstack-protector-strong -fno-plt -ffunction-sections -fPIC -g -fworking-directory -O2 -O2 -O2 -dumpbase -\r\ncc1: warning: command-line option '-std=c++17' is valid for C++\/ObjC++ but not for C\r\ncc1: warning: command-line option '-fvisibility-inlines-hidden' is valid for C++\/ObjC++ but not for C\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\"\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\"\r\nignoring nonexistent directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/local\/include\"\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include-fixed\"\r\nignoring duplicate directory \"\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/..\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/include\"\r\n#include \"...\" search starts here:\r\n#include <...> search starts here:\r\n scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\r\n scipy\/stats\r\n ..\/scipy\/stats\r\n ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran\r\n ..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include\r\n \/usr\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include-fixed\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/include\r\nEnd of search list.\r\n# 0 \"<stdin>\"\r\n# 1 \"\/home\/jgross\/Documents\/repos\/scipy\/build\/\/\"\r\n# 0 \"<built-in>\"\r\n# 0 \"<command-line>\"\r\n# 1 \"\/usr\/include\/stdc-predef.h\" 1\r\n# 0 \"<command-line>\" 2\r\n# 1 \"<stdin>\"\r\nCOMPILER_PATH=\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/libexec\/gcc\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/bin\/\r\nLIBRARY_PATH=\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/lib\/..\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/lib\/:\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/lib\/\r\nCOLLECT_GCC_OPTIONS='-fdiagnostics-color=always' '-I' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p' '-I' 'scipy\/stats' '-I' '..\/scipy\/stats' '-I' '..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/pythran' '-I' '..\/..\/..\/..\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/python3.11\/site-packages\/numpy\/core\/include' '-I' '\/usr\/include' '-I' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11' '-fvisibility=hidden' '-D' '_FILE_OFFSET_BITS=64' '-Wall' '-Winvalid-pch' '-std=c++17' '-O2' '-g' '-fvisibility-inlines-hidden' '-fmessage-length=0' '-march=nocona' '-mtune=haswell' '-ftree-vectorize' '-fstack-protector-strong' '-fno-plt' '-O2' '-ffunction-sections' '-pipe' '-isystem' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include' '-D' 'NDEBUG' '-D' '_FORTIFY_SOURCE=2' '-O2' '-isystem' '\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include' '-fPIC' '-D' 'NPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION' '-D' 'ENABLE_PYTHON_MODULE' '-D' '__PYTHRAN__=3' '-D' 'PYTHRAN_BLAS_NONE' '-Wno-cpp' '-Wno-deprecated-declarations' '-Wno-unused-but-set-variable' '-Wno-unused-function' '-Wno-unused-variable' '-Wno-int-in-bool-context' '-MD' '-MQ' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o' '-MF' 'scipy\/stats\/_stats_pythran.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_..__stats_pythran.cpp.o.d' '-v' '-E' '-shared-libgcc'\r\n```\r\n<\/details>","I see, your gcc will search\r\n\r\n```\r\n \/usr\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\/python3.11\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include-fixed\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/include\r\n \/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/bin\/..\/x86_64-conda-linux-gnu\/sysroot\/usr\/include\r\n```\r\n\r\n`\/usr\/include\/x86_64-linux-gnu` isn't there, that's why the `timesize.h` on your machine isn't found. I don't use conda or mamba, but it seems like `mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/include` should have a functional glibc. Is there no `timesize.h` there?\r\n\r\n\r\n","> it seems like `mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/include` should have a functional glibc. Is there no `timesize.h` there?\r\n\r\nThere is no bits\/, nor timesize.h.\r\n\r\n<details><summary><code>find ~\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/*\/include<\/code><\/summary>\r\n\r\n```\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdatomic.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/quadmath.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/iso646.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/cet.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/gcov.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/bmiintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/lwpintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/clwbintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vpopcntdqintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512dqintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/acc_prof.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vbmi2intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/clzerointrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/mm3dnow.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512pfintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sgxintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/cross-stdarg.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/tsxldtrkintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vbmi2vlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vldqintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/hresetintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/tmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/openacc.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vp2intersectintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xsavesintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xsaveoptintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/lzcntintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/float.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/vpclmulqdqintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512ifmavlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512fp16vlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/fmaintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avxvnniintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vnniintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/unwind.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512bf16intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/movdirintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/ammintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx2intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdalign.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/fxsrintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512ifmaintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/mwaitintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/fma4intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/smmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/amxint8intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/enqcmdintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/omp.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/cldemoteintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xsaveintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512fp16intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/cpuid.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/shaintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/mm_malloc.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/clflushoptintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512erintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512bitalgintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vbmiintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/pmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/mmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/serializeintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/keylockerintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/pkuintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avxintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/uintrintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/ia32intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xsavecintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/wbnoinvdintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vnnivlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/rdseedintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/emmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdbool.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vbmivlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdnoreturn.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/quadmath_weak.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/popcntintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/mwaitxintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stddef.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdarg.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdint-gcc.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx5124vnniwintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xtestintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/prfchwintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/adxintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/f16cintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/cetintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vp2intersectvlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/varargs.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512bf16vlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/rtmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/x86intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/waitpkgintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdint.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/stdfix.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512fintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/amxtileintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/xopintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/bmi2intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/vaesintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vpopcntdqvlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/pconfigintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/tbmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512cdintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\/asan_interface.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\/hwasan_interface.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\/lsan_interface.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\/tsan_interface.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/sanitizer\/common_interface_defs.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/x86gprintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/immintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vlintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/gfniintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/bmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/amxbf16intrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512bwintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx5124fmapsintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/nmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/wmmintrin.h\r\n\/home\/jgross\/.local64\/mambaforge\/envs\/scipy-dev\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/include\/avx512vlbwintrin.h\r\n```\r\n<\/details>","I will try venv and report back.\r\nBtw, with regards to\r\n> you follow all the steps [here](https:\/\/docs.scipy.org\/doc\/scipy\/building\/index.html)\r\n\r\nThere seems to be a typo, `python -m pip sphinx` and `python -m pip mypy` seem to be missing `install` after `pip`.  (There are also two extra backticks in ``` ``pyenv\/pyenv-virtualenv ```)\r\n\r\n<details><summary>screenshots<\/summary>\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/396076\/f4c50feb-72a1-4d21-8372-dc58dc61abfd)\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/396076\/02afce0c-b2dc-487f-a5b7-d84d1a297d40)\r\n\r\n<\/details>","Trying with venv instead:\r\n```\r\n$ git clean -xfd .\r\n$ python -m venv venv\r\n$ source venv\/bin\/activate\r\n```\r\n<details><summary><code>python -m pip install numpy cython pythran pybind11 meson ninja pydevtool rich-click pytest pytest-xdist pytest-timeout pooch threadpoolctl asv gmpy2 mpmath sphinx \"pydata-sphinx-theme==0.9.0\" sphinx-design matplotlib numpydoc jupytext myst-nb mypy typing_extensions types-psutil pycodestyle ruff cython-lint<\/code><\/summary>\r\n\r\n```\r\nCollecting numpy\r\n  Using cached numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\r\nCollecting cython\r\n  Using cached Cython-3.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\nCollecting pythran\r\n  Using cached pythran-0.15.0-py3-none-any.whl (4.3 MB)\r\nCollecting pybind11\r\n  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\r\nCollecting meson\r\n  Using cached meson-1.3.1-py3-none-any.whl (976 kB)\r\nCollecting ninja\r\n  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\r\nCollecting pydevtool\r\n  Using cached pydevtool-0.3.0-py3-none-any.whl (12 kB)\r\nCollecting rich-click\r\n  Using cached rich_click-1.7.3-py3-none-any.whl (32 kB)\r\nCollecting pytest\r\n  Using cached pytest-7.4.4-py3-none-any.whl (325 kB)\r\nCollecting pytest-xdist\r\n  Using cached pytest_xdist-3.5.0-py3-none-any.whl (42 kB)\r\nCollecting pytest-timeout\r\n  Using cached pytest_timeout-2.2.0-py3-none-any.whl (13 kB)\r\nCollecting pooch\r\n  Using cached pooch-1.8.0-py3-none-any.whl (62 kB)\r\nCollecting threadpoolctl\r\n  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\r\nCollecting asv\r\n  Using cached asv-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (371 kB)\r\nCollecting gmpy2\r\n  Using cached gmpy2-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\r\nCollecting mpmath\r\n  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\nCollecting sphinx\r\n  Using cached sphinx-7.2.6-py3-none-any.whl (3.2 MB)\r\nCollecting pydata-sphinx-theme==0.9.0\r\n  Using cached pydata_sphinx_theme-0.9.0-py3-none-any.whl (3.3 MB)\r\nCollecting sphinx-design\r\n  Using cached sphinx_design-0.5.0-py3-none-any.whl (2.2 MB)\r\nCollecting matplotlib\r\n  Using cached matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\r\nCollecting numpydoc\r\n  Using cached numpydoc-1.6.0-py3-none-any.whl (61 kB)\r\nCollecting jupytext\r\n  Using cached jupytext-1.16.1-py3-none-any.whl (152 kB)\r\nCollecting myst-nb\r\n  Using cached myst_nb-1.0.0-py3-none-any.whl (79 kB)\r\nCollecting mypy\r\n  Using cached mypy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\r\nCollecting typing_extensions\r\n  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\r\nCollecting types-psutil\r\n  Using cached types_psutil-5.9.5.20240106-py3-none-any.whl (17 kB)\r\nCollecting pycodestyle\r\n  Using cached pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\r\nCollecting ruff\r\n  Using cached ruff-0.1.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\r\nCollecting cython-lint\r\n  Using cached cython_lint-0.16.0-py2.py3-none-any.whl (12 kB)\r\nCollecting beautifulsoup4\r\n  Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\r\nCollecting packaging\r\n  Using cached packaging-23.2-py3-none-any.whl (53 kB)\r\nCollecting docutils!=0.17.0\r\n  Using cached docutils-0.20.1-py3-none-any.whl (572 kB)\r\nCollecting gast~=0.5.0\r\n  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\r\nCollecting beniget~=0.4.0\r\n  Using cached beniget-0.4.1-py3-none-any.whl (9.4 kB)\r\nCollecting ply>=3.4\r\n  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\r\nRequirement already satisfied: setuptools in .\/venv\/lib\/python3.10\/site-packages (from pythran) (65.5.0)\r\nCollecting doit>=0.36.0\r\n  Using cached doit-0.36.0-py3-none-any.whl (85 kB)\r\nCollecting rich>=10.7.0\r\n  Using cached rich-13.7.0-py3-none-any.whl (240 kB)\r\nCollecting click>=7\r\n  Using cached click-8.1.7-py3-none-any.whl (97 kB)\r\nCollecting pluggy<2.0,>=0.12\r\n  Using cached pluggy-1.3.0-py3-none-any.whl (18 kB)\r\nCollecting tomli>=1.0.0\r\n  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\r\nCollecting iniconfig\r\n  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\nCollecting exceptiongroup>=1.0.0rc8\r\n  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\r\nCollecting execnet>=1.1\r\n  Using cached execnet-2.0.2-py3-none-any.whl (37 kB)\r\nCollecting requests>=2.19.0\r\n  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\r\nCollecting platformdirs>=2.5.0\r\n  Using cached platformdirs-4.1.0-py3-none-any.whl (17 kB)\r\nCollecting pympler\r\n  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\r\nCollecting tabulate\r\n  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\nCollecting pyyaml\r\n  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\r\nCollecting asv-runner>=v0.1.0\r\n  Using cached asv_runner-0.1.0-py3-none-any.whl (47 kB)\r\nCollecting json5\r\n  Using cached json5-0.9.14-py2.py3-none-any.whl (19 kB)\r\nCollecting sphinxcontrib-jsmath\r\n  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\r\nCollecting sphinxcontrib-qthelp\r\n  Using cached sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\r\nCollecting sphinxcontrib-devhelp\r\n  Using cached sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\r\nCollecting sphinxcontrib-htmlhelp>=2.0.0\r\n  Using cached sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\r\nCollecting sphinxcontrib-applehelp\r\n  Using cached sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\r\nCollecting Pygments>=2.14\r\n  Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\r\nCollecting Jinja2>=3.0\r\n  Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\r\nCollecting snowballstemmer>=2.0\r\n  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\r\nCollecting alabaster<0.8,>=0.7\r\n  Using cached alabaster-0.7.16-py3-none-any.whl (13 kB)\r\nCollecting sphinxcontrib-serializinghtml>=1.1.9\r\n  Using cached sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\r\nCollecting imagesize>=1.3\r\n  Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\r\nCollecting babel>=2.9\r\n  Using cached Babel-2.14.0-py3-none-any.whl (11.0 MB)\r\nCollecting python-dateutil>=2.7\r\n  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\nCollecting cycler>=0.10\r\n  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\nCollecting pyparsing>=2.3.1\r\n  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\r\nCollecting contourpy>=1.0.1\r\n  Using cached contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\r\nCollecting pillow>=8\r\n  Using cached pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\r\nCollecting kiwisolver>=1.3.1\r\n  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\r\nCollecting fonttools>=4.22.0\r\n  Using cached fonttools-4.47.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\r\nCollecting markdown-it-py>=1.0\r\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\nCollecting toml\r\n  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\nCollecting nbformat\r\n  Using cached nbformat-5.9.2-py3-none-any.whl (77 kB)\r\nCollecting mdit-py-plugins\r\n  Using cached mdit_py_plugins-0.4.0-py3-none-any.whl (54 kB)\r\nCollecting importlib_metadata\r\n  Using cached importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\r\nCollecting ipykernel\r\n  Using cached ipykernel-6.29.0-py3-none-any.whl (116 kB)\r\nCollecting jupyter-cache>=0.5\r\n  Using cached jupyter_cache-1.0.0-py3-none-any.whl (33 kB)\r\nCollecting nbclient\r\n  Using cached nbclient-0.9.0-py3-none-any.whl (24 kB)\r\nCollecting ipython\r\n  Using cached ipython-8.20.0-py3-none-any.whl (809 kB)\r\nCollecting myst-parser>=1.0.0\r\n  Using cached myst_parser-2.0.0-py3-none-any.whl (77 kB)\r\nCollecting mypy-extensions>=1.0.0\r\n  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\r\nCollecting tokenize-rt>=3.2.0\r\n  Using cached tokenize_rt-5.2.0-py2.py3-none-any.whl (5.8 kB)\r\nCollecting cloudpickle\r\n  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\r\nCollecting zipp>=0.5\r\n  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\r\nCollecting MarkupSafe>=2.0\r\n  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\r\nCollecting attrs\r\n  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\r\nCollecting sqlalchemy<3,>=1.3.12\r\n  Using cached SQLAlchemy-2.0.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\nCollecting mdurl~=0.1\r\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\nCollecting jupyter-core!=5.0.*,>=4.12\r\n  Using cached jupyter_core-5.7.1-py3-none-any.whl (28 kB)\r\nCollecting jupyter-client>=6.1.12\r\n  Using cached jupyter_client-8.6.0-py3-none-any.whl (105 kB)\r\nCollecting traitlets>=5.4\r\n  Using cached traitlets-5.14.1-py3-none-any.whl (85 kB)\r\nCollecting jsonschema>=2.6\r\n  Using cached jsonschema-4.21.0-py3-none-any.whl (85 kB)\r\nCollecting fastjsonschema\r\n  Using cached fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\r\nCollecting six>=1.5\r\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\nCollecting urllib3<3,>=1.21.1\r\n  Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\r\nCollecting certifi>=2017.4.17\r\n  Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\r\nCollecting charset-normalizer<4,>=2\r\n  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\r\nCollecting idna<4,>=2.5\r\n  Using cached idna-3.6-py3-none-any.whl (61 kB)\r\nCollecting soupsieve>1.2\r\n  Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\r\nCollecting nest-asyncio\r\n  Using cached nest_asyncio-1.5.9-py3-none-any.whl (5.3 kB)\r\nCollecting debugpy>=1.6.5\r\n  Using cached debugpy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\nCollecting matplotlib-inline>=0.1\r\n  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\r\nCollecting comm>=0.1.1\r\n  Using cached comm-0.2.1-py3-none-any.whl (7.2 kB)\r\nCollecting pyzmq>=24\r\n  Using cached pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\r\nCollecting tornado>=6.1\r\n  Using cached tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\r\nCollecting psutil\r\n  Using cached psutil-5.9.7-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\r\nCollecting stack-data\r\n  Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\r\nCollecting prompt-toolkit<3.1.0,>=3.0.41\r\n  Using cached prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\r\nCollecting decorator\r\n  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\r\nCollecting jedi>=0.16\r\n  Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\r\nCollecting pexpect>4.3\r\n  Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\r\nCollecting parso<0.9.0,>=0.8.3\r\n  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\r\nCollecting referencing>=0.28.4\r\n  Using cached referencing-0.32.1-py3-none-any.whl (26 kB)\r\nCollecting rpds-py>=0.7.1\r\n  Using cached rpds_py-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\nCollecting jsonschema-specifications>=2023.03.6\r\n  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\r\nCollecting ptyprocess>=0.5\r\n  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\r\nCollecting wcwidth\r\n  Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\r\nCollecting greenlet!=0.4.17\r\n  Using cached greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\r\nCollecting asttokens>=2.1.0\r\n  Using cached asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\r\nCollecting executing>=1.2.0\r\n  Using cached executing-2.0.1-py2.py3-none-any.whl (24 kB)\r\nCollecting pure-eval\r\n  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\r\nInstalling collected packages: wcwidth, snowballstemmer, pure-eval, ptyprocess, ply, ninja, mpmath, json5, gmpy2, fastjsonschema, zipp, urllib3, typing_extensions, types-psutil, traitlets, tornado, tomli, toml, tokenize-rt, threadpoolctl, tabulate, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, soupsieve, six, ruff, rpds-py, pyzmq, pyyaml, pyparsing, pympler, Pygments, pycodestyle, pybind11, psutil, prompt-toolkit, pluggy, platformdirs, pillow, pexpect, parso, packaging, numpy, nest-asyncio, mypy-extensions, meson, mdurl, MarkupSafe, kiwisolver, iniconfig, imagesize, idna, greenlet, gast, fonttools, executing, execnet, exceptiongroup, docutils, decorator, debugpy, cython, cycler, cloudpickle, click, charset-normalizer, certifi, babel, attrs, asv-runner, alabaster, sqlalchemy, requests, referencing, python-dateutil, pytest, mypy, matplotlib-inline, markdown-it-py, jupyter-core, Jinja2, jedi, importlib_metadata, cython-lint, contourpy, comm, beniget, beautifulsoup4, asv, asttokens, stack-data, sphinx, rich, pythran, pytest-xdist, pytest-timeout, pooch, mdit-py-plugins, matplotlib, jupyter-client, jsonschema-specifications, doit, sphinx-design, rich-click, pydevtool, pydata-sphinx-theme, numpydoc, myst-parser, jsonschema, ipython, nbformat, ipykernel, nbclient, jupytext, jupyter-cache, myst-nb\r\nSuccessfully installed Jinja2-3.1.3 MarkupSafe-2.1.3 Pygments-2.17.2 alabaster-0.7.16 asttokens-2.4.1 asv-0.6.1 asv-runner-0.1.0 attrs-23.2.0 babel-2.14.0 beautifulsoup4-4.12.3 beniget-0.4.1 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 comm-0.2.1 contourpy-1.2.0 cycler-0.12.1 cython-3.0.8 cython-lint-0.16.0 debugpy-1.8.0 decorator-5.1.1 docutils-0.20.1 doit-0.36.0 exceptiongroup-1.2.0 execnet-2.0.2 executing-2.0.1 fastjsonschema-2.19.1 fonttools-4.47.2 gast-0.5.4 gmpy2-2.1.5 greenlet-3.0.3 idna-3.6 imagesize-1.4.1 importlib_metadata-7.0.1 iniconfig-2.0.0 ipykernel-6.29.0 ipython-8.20.0 jedi-0.19.1 json5-0.9.14 jsonschema-4.21.0 jsonschema-specifications-2023.12.1 jupyter-cache-1.0.0 jupyter-client-8.6.0 jupyter-core-5.7.1 jupytext-1.16.1 kiwisolver-1.4.5 markdown-it-py-3.0.0 matplotlib-3.8.2 matplotlib-inline-0.1.6 mdit-py-plugins-0.4.0 mdurl-0.1.2 meson-1.3.1 mpmath-1.3.0 mypy-1.8.0 mypy-extensions-1.0.0 myst-nb-1.0.0 myst-parser-2.0.0 nbclient-0.9.0 nbformat-5.9.2 nest-asyncio-1.5.9 ninja-1.11.1.1 numpy-1.26.3 numpydoc-1.6.0 packaging-23.2 parso-0.8.3 pexpect-4.9.0 pillow-10.2.0 platformdirs-4.1.0 pluggy-1.3.0 ply-3.11 pooch-1.8.0 prompt-toolkit-3.0.43 psutil-5.9.7 ptyprocess-0.7.0 pure-eval-0.2.2 pybind11-2.11.1 pycodestyle-2.11.1 pydata-sphinx-theme-0.9.0 pydevtool-0.3.0 pympler-1.0.1 pyparsing-3.1.1 pytest-7.4.4 pytest-timeout-2.2.0 pytest-xdist-3.5.0 python-dateutil-2.8.2 pythran-0.15.0 pyyaml-6.0.1 pyzmq-25.1.2 referencing-0.32.1 requests-2.31.0 rich-13.7.0 rich-click-1.7.3 rpds-py-0.17.1 ruff-0.1.13 six-1.16.0 snowballstemmer-2.2.0 soupsieve-2.5 sphinx-7.2.6 sphinx-design-0.5.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10 sqlalchemy-2.0.25 stack-data-0.6.3 tabulate-0.9.0 threadpoolctl-3.2.0 tokenize-rt-5.2.0 toml-0.10.2 tomli-2.0.1 tornado-6.4 traitlets-5.14.1 types-psutil-5.9.5.20240106 typing_extensions-4.9.0 urllib3-2.1.0 wcwidth-0.2.13 zipp-3.17.0\r\n\r\n[notice] A new release of pip is available: 23.0.1 -> 23.3.2\r\n[notice] To update, run: pip install --upgrade pip\r\n```\r\n<\/details>\r\n\r\n<details><summary><code>python dev.py build<\/code><\/summary>\r\n\r\n```\r\n\ud83d\udcbb  meson setup \/home\/jgross\/Documents\/GitHub\/scipy\/build --prefix \/home\/jgross\/Documents\/GitHub\/scipy\/build-install\r\nThe Meson build system\r\nVersion: 1.3.1\r\nSource dir: \/home\/jgross\/Documents\/GitHub\/scipy\r\nBuild dir: \/home\/jgross\/Documents\/GitHub\/scipy\/build\r\nBuild type: native build\r\nProject name: SciPy\r\nProject version: 1.13.0.dev0\r\nC compiler for the host machine: cc (gcc 11.4.0 \"cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\")\r\nC linker for the host machine: cc ld.bfd 2.38\r\nC++ compiler for the host machine: c++ (gcc 11.4.0 \"c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\")\r\nC++ linker for the host machine: c++ ld.bfd 2.38\r\nCython compiler for the host machine: cython (cython 3.0.8)\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram python3 found: YES (\/home\/jgross\/Documents\/GitHub\/scipy\/venv\/bin\/python)\r\nFound pkg-config: YES (\/usr\/bin\/pkg-config) 0.29.2\r\nRun-time dependency python found: YES 3.10\r\nProgram cython found: YES (\/home\/jgross\/Documents\/GitHub\/scipy\/venv\/bin\/cython)\r\nCompiler for C supports arguments -Wno-unused-but-set-variable: YES\r\nCompiler for C supports arguments -Wno-unused-function: YES\r\nCompiler for C supports arguments -Wno-conversion: YES\r\nCompiler for C supports arguments -Wno-misleading-indentation: YES\r\nLibrary m found: YES\r\nFortran compiler for the host machine: gfortran (gcc 11.4.0 \"GNU Fortran (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\")\r\nFortran linker for the host machine: gfortran ld.bfd 2.38\r\nCompiler for Fortran supports arguments -Wno-conversion: YES\r\nChecking if \"-Wl,--version-script\" : links: YES\r\nProgram pythran found: YES 0.15.0 0.15.0 (\/home\/jgross\/Documents\/GitHub\/scipy\/venv\/bin\/pythran)\r\nRun-time dependency xsimd found: YES 7.6.0\r\nRun-time dependency threads found: YES\r\nLibrary npymath found: YES\r\nLibrary npyrandom found: YES\r\npybind11-config found: YES (\/home\/jgross\/Documents\/GitHub\/scipy\/venv\/bin\/pybind11-config) 2.11.1\r\nRun-time dependency pybind11 found: YES 2.11.1\r\nRun-time dependency scipy-openblas found: NO (tried pkgconfig)\r\nRun-time dependency openblas found: YES 0.3.20\r\nDependency openblas found: YES 0.3.20 (cached)\r\nCompiler for C supports arguments -Wno-maybe-uninitialized: YES\r\nCompiler for C supports arguments -Wno-discarded-qualifiers: YES\r\nCompiler for C supports arguments -Wno-empty-body: YES\r\nCompiler for C supports arguments -Wno-implicit-function-declaration: YES\r\nCompiler for C supports arguments -Wno-parentheses: YES\r\nCompiler for C supports arguments -Wno-switch: YES\r\nCompiler for C supports arguments -Wno-unused-label: YES\r\nCompiler for C supports arguments -Wno-unused-result: YES\r\nCompiler for C supports arguments -Wno-unused-variable: YES\r\nCompiler for C supports arguments -Wno-incompatible-pointer-types: YES\r\nCompiler for C++ supports arguments -Wno-cpp: YES\r\nCompiler for C++ supports arguments -Wno-deprecated-declarations: YES\r\nCompiler for C++ supports arguments -Wno-class-memaccess: YES\r\nCompiler for C++ supports arguments -Wno-format-truncation: YES\r\nCompiler for C++ supports arguments -Wno-non-virtual-dtor: YES\r\nCompiler for C++ supports arguments -Wno-sign-compare: YES\r\nCompiler for C++ supports arguments -Wno-switch: YES\r\nCompiler for C++ supports arguments -Wno-terminate: YES\r\nCompiler for C++ supports arguments -Wno-unused-but-set-variable: YES\r\nCompiler for C++ supports arguments -Wno-unused-function: YES\r\nCompiler for C++ supports arguments -Wno-unused-local-typedefs: YES\r\nCompiler for C++ supports arguments -Wno-unused-variable: YES\r\nCompiler for C++ supports arguments -Wno-int-in-bool-context: YES\r\nCompiler for Fortran supports arguments -Wno-argument-mismatch: YES\r\nCompiler for Fortran supports arguments -Wno-conversion: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-intrinsic-shadow: YES\r\nCompiler for Fortran supports arguments -Wno-maybe-uninitialized: YES\r\nCompiler for Fortran supports arguments -Wno-surprising: YES\r\nCompiler for Fortran supports arguments -Wno-uninitialized: YES\r\nCompiler for Fortran supports arguments -Wno-unused-dummy-argument: YES\r\nCompiler for Fortran supports arguments -Wno-unused-label: YES\r\nCompiler for Fortran supports arguments -Wno-unused-variable: YES\r\nCompiler for Fortran supports arguments -Wno-tabs: YES\r\nCompiler for Fortran supports arguments -Wno-argument-mismatch: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-conversion: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-maybe-uninitialized: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-dummy-argument: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-label: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-unused-variable: YES (cached)\r\nCompiler for Fortran supports arguments -Wno-tabs: YES (cached)\r\nChecking if \"Check atomic builtins without -latomic\" : links: YES\r\nConfiguring __config__.py using configuration\r\nChecking for function \"open_memstream\" : NO\r\nConfiguring messagestream_config.h using configuration\r\nCompiler for Fortran supports arguments -w: YES\r\nChecking for size of \"void*\" : 8\r\nCompiler for Fortran supports arguments -w: YES (cached)\r\nBuild targets in project: 248\r\n\r\nSciPy 1.13.0.dev0\r\n\r\n  User defined options\r\n    prefix: \/home\/jgross\/Documents\/GitHub\/scipy\/build-install\r\n\r\nFound ninja-1.11.1.git.kitware.jobserver-1 at \/home\/jgross\/Documents\/GitHub\/scipy\/venv\/bin\/ninja\r\nMeson build setup OK\r\n\ud83d\udcbb  ninja -C \/home\/jgross\/Documents\/GitHub\/scipy\/build -j4\r\nninja: Entering directory `\/home\/jgross\/Documents\/GitHub\/scipy\/build'\r\n```\r\n<\/details>\r\nThis one seems to be working, it's at `[1083\/1501] Generating 'scipy\/signal\/_upfirdn_apply.cpython-310-x86_64-linux-gnu.so.p\/_upfirdn_apply.c'` currently","Also the CI seems to pass with my updated code.","Ah, sorry, I guess I was premature with the CI before.\r\n\r\nBtw, you might want to update the build instructions, I had to do\r\n```\r\npython -m pip install hypothesis\r\n```\r\nto get the tests to run.","Most of the tests (`python dev.py test -t scipy\/special\/tests\/test_log_softmax.py`) now pass, but the final one is blocked on https:\/\/github.com\/numpy\/numpy\/issues\/25623, because `np.argmax` doesn't support tuple axes even though `np.max` does.","> Ah, sorry, I guess I was premature with the CI before.\r\n> \r\n> Btw, you might want to update the build instructions, I had to do\r\n> \r\n> ```\r\n> python -m pip install hypothesis\r\n> ```\r\n> \r\n> to get the tests to run.\r\n\r\nCheck the devdocs for the most up to date info. https:\/\/scipy.github.io\/devdocs\/building\/index.html#building-from-source\r\n\r\n\r\n","I've pushed a horrible kludge of a commit that special-cases `tuple` arguments to `axis` and uses the old implementation for those arguments.  This makes the test-suite (`python dev.py test -t scipy\/special\/tests\/test_log_softmax.py`) pass, but maybe we want to wait for https:\/\/github.com\/numpy\/numpy\/issues\/25623?","If you approve the CI run again, I expect (fingers crossed) all the tests will pass"],"labels":["enhancement","scipy.special"]},{"title":"ENH: High-dimensional LinearNDInterpolator","body":"### Is your feature request related to a problem? Please describe.\n\nScipy's ``LinearNDInterpolator`` function computes a Delaunay triangulation using the qhull algorithm, then uses it to perform Barycentric interpolation within each cell.\r\n\r\nQhull is a fast and robust algorithm, but the number of elements in the Delaunay triangulation (any mesh really) grows exponentially large with the dimension, making this solution unrealistic for d>4 dimension, even with just moderately sized data sets.\n\n### Describe the solution you'd like.\n\nAn alternative approach, is to only compute the simplex containing each interpolation point.  This is generally faster whenever d>4, as long as the user only needs a polynomially-sized number of interpolation values (thus we don't end up needing to compute the entire mesh).\r\n\r\nOne of our papers proposes and implements such an approach.  It's based on a linear programming formulation of the problem, but with additional geometric checks and features to make sure that it is extremely robust (e.g., backward stable in a sense).\r\n\r\nWe have a Fortran driver published in ACM TOMS under MIT License:  https:\/\/dl.acm.org\/doi\/abs\/10.1145\/3422818\r\n\r\nWe have shown robustness on a wide variety of applications and scaled up to hundred-dimensional interpolation problems.\r\n\r\nIf there is interest, I would love to help get this integrated into scipy.interpolate.LinearNDInterpolator, perhaps we could switch off between this approach and the current approach using Qhull whenever the dimension exceeds some threshold, e.g,. 4d.\n\n### Describe alternatives you've considered.\n\nI believe the LP-based formulation has been known, but un-published for many years before us.  However, it is not so easy to implement this approach in a way that is robust to geometric degeneracy (i.e., the approach flat-out fails for many real-world data sets).\r\n\r\nOur method requires the basic solution to the LP.  IP methods can't give the basic solution, so only simplex methods could be used.\r\n\r\nSome open-source dense simplex-based LP solvers could be slightly faster on randomly generated data sets, but all that I have tested will fail for ill-conditioned and degenerate data sets (e.g., grid aligned data), since this results in degeneracy of the LP formulation.\n\n### Additional context (e.g. screenshots, GIFs)\n\nFor reference, the following figures from the linked paper show scaling of our approach and various alternatives from the literature, including Qhull.\r\n\r\nTime requirements of other approaches from the literature for computing the entire Delaunay triangulation in medium dimensions:\r\n\r\n<img width=\"1542\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/17115193\/68aac536-48dc-4aa2-96fb-ba1f007122fb\">\r\n\r\nTime requirements of our approach in much higher dimensions, for computing a single interpolation point (not the entire mesh):\r\n\r\n<img width=\"1585\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/17115193\/bf009e5f-eb67-4598-bfe0-a06b5962e153\">\r\n","comments":["Hi @thchang , thanks for the proposal! I'm not an `interpolate` resident but this sounds interesting - could you send a proposal on the [mailing list](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/) (with the key points here and linking to this issue) as stated in [the 'Contributing new code' section of our developer documentation](https:\/\/scipy.github.io\/devdocs\/dev\/hacking.html#contributing-new-code)?","This definitely sounds interesting! The speedups are impressive indeed.\r\nISTM what you're doing here is roughly a sort of sparse \/ lazy triangulation. Definitely very non-trivial to implement in a robust way! \r\n\r\nThat said, here's some fine print:\r\n- As Lucas said, it's best to discuss proposed enhancement on the scipy-dev mailing list. One thing you'd need to argue for inclusion into scipy is that the feature is of broad interest. The paper you linked has 4 citations from 2020, so there probably is some other metric \/ indicator \/ reasoning? We are fully aware that citations by themselves are not _the_ way to measure... almost anything; so I'd be interested to hear your arguments on the mailing list. \r\n- Off the cuff, it seems to be quite niche --- and quite a life-saver when Qhull runs out of steam of course. The curse of dimensionality is very real, the main question is whether this is better suited for a general-purpose library like SciPy or a more specialized package.\r\n- Implementation details: there is going to be quite a push-back for including more Fortran code into SciPy. Again, this is not to criticize your choice of the implementation language, at all. It's just that Fortran\/Python glue brings quite a headache for our build and distribution. You'd need to argue about that, too. I suspect rewriting the core algorithm in C\/C++\/Cython is not something you'd fancy doing.\r\n\r\nTL;DR: The proposed enhancement looks very impressive and possibly in scope. What's not clear at the moment is whether to include it to scipy or release as a standalone package (ideally, a drop-in replacement for LinearNDInterpolator?). We can probably advise on how to implement python bindings if you go the separate package route.\r\n","Thank-you all for your feedback,\r\n\r\nI understand the concerns.  I will discuss with my collaborators and reach to to the mailing list if needed.\r\n\r\nWe actually do have some python bindings, but they are not currently in the style of scipy.interpolate, also I don't have experience with wheels, so it requires a fortran compiler be pre-installed which can be called to build on import.\r\n\r\nFrom our perspective, it would be nice to improve accessibility of what we've done, possibly through inclusion in scipy, but also possibly by improving our python bindings to match the scipy style.  I certainly understand the concerns on your end though.\r\n\r\nAs I said, will think on your feedback and send a proposal to the dev mailing list if needed.","Feel free to leave the issue open in the meantime to continue discussions on and for visibility. Having hit the limitations of Qhull in my personal work I find this proposal interesting.","Yeah, build-on-import sounds a bit limiting indeed. Building wheels and\/or conda packages for a range of platforms sounds like a way to go. For building wheels I *think*  you can mostly copy the scipy setup with cibuildwheel et al; for conda, conda-forge builds packages somehow. I'm not very familiar with modern ways so will not detail my outdated knowledge; there definitely are people on the ML who know more and could likely help or advise.\r\n\r\nLast point: if in the end you go the standalone package route, please let's have at least a link\/reference to that package in the LinearNDInterpolator or scipy.spatial documentation. "],"labels":["enhancement","scipy.interpolate"]},{"title":"BUG: Inconsistent behaviour in scipy.sparse.find","body":"### Describe your issue.\r\n\r\nThe `scipy.sparse.find` leads to different results in different scipy versions. \r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nPython 3.8.16 (default, Jan 17 2023, 22:25:28) [MSC v.1916 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 8.7.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from scipy.sparse import csr_matrix, find\r\n\r\nIn [2]: A = csr_matrix([[1.0, 2.0, 0],[2.0, 3, 0], [0, 0, 4]])\r\n\r\nIn [3]: find(A)\r\nOut[3]:\r\n(array([0, 1, 0, 1, 2], dtype=int32),\r\n array([0, 0, 1, 1, 2], dtype=int32),\r\n array([1., 2., 2., 3., 4.]))\r\n\r\n\r\nPython 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 8.15.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: from scipy.sparse import csr_matrix, find\r\n\r\nIn [2]: A = csr_matrix([[1.0, 2.0, 0],[2.0, 3, 0], [0, 0, 4]])\r\n\r\nIn [3]: find(A)\r\nOut[3]: (array([0, 0, 1, 1, 2]), array([0, 1, 0, 1, 2]), array([1., 2., 2., 3., 4.]))\r\n\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nThe results should be the same but they are different.\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nThe first example uses python 3.8.16, scipy 1.6.2, numpy 1.22.4\r\n\r\nThe second uses python 3.10.13, scipy 1.11.3, numpy 1.26.0\r\n```\r\n","comments":[],"labels":["defect","scipy.sparse"]},{"title":"CI, MAINT: special test_intermediate_overlow failure with latest NumPy (bisected)","body":"The failure can be observed here: https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6878964267\/job\/18709821055?pr=19523\r\n\r\nAnd summarized\/preserved below the fold:\r\n\r\n<details>\r\n\r\n```\r\n __________________________ test_intermediate_overlow ___________________________\r\n[gw1] linux -- Python 3.12.0 \/opt\/hostedtoolcache\/Python\/3.12.0\/x64\/bin\/python\r\nscipy\/special\/tests\/test_trig.py:45: in test_intermediate_overlow\r\n    assert_allclose(sinpi(p), std)\r\n        p          = (1.00000000000001+227j)\r\n        sinpi_pts  = [(1.00000000000001+227j), (1e-35+250j), (1e-301+445j)]\r\n        sinpi_std  = [(-8.113438309924894e+295-infj), (1.9507801934611995e+306+infj), (2.205958493464539e+306+infj)]\r\n        std        = (-8.113438309924894e+295-infj)\r\n        sup        = <numpy.testing._private.utils.suppress_warnings object at 0x7f14e8d499a0>\r\n\/opt\/hostedtoolcache\/Python\/3.12.0\/x64\/lib\/python3.12\/contextlib.py:81: in inner\r\n    return func(*args, **kwds)\r\nE   AssertionError: \r\nE   Not equal to tolerance rtol=1e-07, atol=0\r\nE   \r\nE   Mismatched elements: 1 \/ 1 (100%)\r\nE   Max absolute difference among violations: nan\r\nE   Max relative difference among violations: nan\r\nE    ACTUAL: array(-8.113438e+295-infj)\r\nE    DESIRED: array(-8.113438e+295-infj)\r\n        args       = (<function assert_allclose.<locals>.compare at 0x7f14eb463060>, array(-8.11343831e+295-infj), array(-8.11343831e+295-infj))\r\n        func       = <function assert_array_compare at 0x7f14fe931e40>\r\n        kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=0', 'strict': False, ...}\r\n        self       = <contextlib._GeneratorContextManager object at 0x7f14fe93aba0>\r\n```\r\n\r\n<\/details>\r\n\r\nI bisected on NumPy and found the commit below, so maybe @mdhaber might have thoughts on it. It could be that improvements to the NumPy testing machinery actually caught something we didn't catch before, or it could just be some subtle issue with handling non-finite values on the NumPy side, not sure from a quick look, but this gets the dissection started at least.\r\n\r\n```\r\n303015970238ee140da3a0b71e7e9ada0c762d90 is the first bad commit\r\ncommit 303015970238ee140da3a0b71e7e9ada0c762d90\r\nAuthor: Matt Haberland <mhaberla@calpoly.edu>\r\nDate:   Sat Oct 28 23:44:55 2023 -0700\r\n\r\n    MAINT: isclose\/allclose: simplify support for array-like tolerances\r\n\r\n ...\/upcoming_changes\/14343.future_change.rst       |  5 ----\r\n doc\/release\/upcoming_changes\/24878.improvement.rst |  5 ++++\r\n numpy\/_core\/numeric.py                             | 23 ++++++--------\r\n numpy\/_core\/tests\/test_numeric.py                  | 35 ++++++++++++----------\r\n 4 files changed, 34 insertions(+), 34 deletions(-)\r\n delete mode 100644 doc\/release\/upcoming_changes\/14343.future_change.rst\r\n create mode 100644 doc\/release\/upcoming_changes\/24878.improvement.rst\r\n```","comments":["I'll take a look. ","Continued discussion of what exactly should be expected at https:\/\/github.com\/numpy\/numpy\/pull\/24878#issuecomment-1823861087","I merged the temporary skip for this test for now. My reasoning is summarized in the cross-listed PR, but is simply based on a reading through of what Marten and Matt have discussed (it sounds sufficiently convoluted\/philosophical that it may take a little bit to resolve upstream). For now, checking that the real part matches to desired precision while the non-finite imaginary part simply points in the appropriate \"infinity direction\" seems like a compelling solution to me."],"labels":["scipy.special","maintenance","CI"]},{"title":"BUG: deepcopy on stats.rvs breaks seed \/ random generation.","body":"### Describe your issue.\n\nI am using nested dictionary to build flexible modeling. \r\nTo avoid affecting input dict inside a the core function I am using `copy.deepcopy` to create a copy of the input dict and play with it.\r\n\r\nIf this input dict contain a scipy.stats.{}.rvs this breaks the random generation numbers (see example below).\r\n\r\nIn the example bellow, the first drawn histogram is bugged !\n\n### Reproducing Code Example\n\n```python\nfrom scipy import stats\r\nfrom copy import deepcopy\r\nimport matplotlib.pyplot as plt\r\n\r\nsize=1000\r\nerrmodel = {\"func\": stats.lognorm.rvs,\r\n             \"kwargs\":{\"s\":0.6, \"loc\":.001, \"scale\":0.2}\r\n            }\r\n\r\nmodel = deepcopy(errmodel)\r\nfunc = model[\"func\"]\r\nprop = model[\"kwargs\"]\r\n\r\nerr_array = func(size=size, **prop)\r\n### even simplier but not so usual: \r\n# err_array = deepcopy(stats.lognorm.rvs)(size=size, s=0.6, loc=0.001, scale=0.2)\r\n\r\nfig, ax = plt.subplots()\r\nprop = dict(histtype=\"step\", fill=False, lw=2, bins=\"auto\")\r\n\r\n# using np.random.normal in place of stats.norm.rvs leads to the same issue\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array), color=\"tab:blue\", **prop) # Broken !\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array), color=\"tab:orange\", **prop) # works\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array), color=\"tab:green\", **prop) # works\r\n;\n```\n\n\n### Error message\n\n```shell\nIn the code example, the first drawn histogram is bugged.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.0 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/include\r\n    lib directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/include\r\n    lib directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  c++:\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/Users\/runner\/miniforge3\/conda-bld\/scipy-split_1696467658954\/_build_env\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/_build_env\/venv\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: arm64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/rigault\/miniforge3\/envs\/jaxcpu\/bin\/python\r\n  version: '3.10'\n```\n","comments":["resulting plot if it helps\r\n![Capture d\u2019e\u0301cran 2023-11-15 a\u0300 16 19 47](https:\/\/github.com\/scipy\/scipy\/assets\/7812822\/9c73c914-e293-4bb5-bf8d-89f7ac51267e)\r\n","Distribution objects like `scipy.stats.lognorm` grab the global `RandomState` instance and hold onto it to use as a default when the user does not pass a `random_state` argument to `rvs()`. This `RandomState` instance gets cloned when you deepcopy the `rvs` method. Ideally, the `RandomState` instance should not have been grabbed greedily on instantiation of `lognorm`, but rather left as `None` by default until `rvs()` gets called. Since `lognorm.rvs()` builds on top of `norm.rvs()`, essentially, you are reusing the same random bits to generate `err_array` and the first `norm.rvs()` call, so they are correlated, and you get the wrong distribution ultimately. The later calls to `norm.rvs()` are uncorrelated to `err_array`, so they end up with reasonable distributions.\r\n\r\nThe workaround is to explicitly pass your own `RandomState` instance (or these days, preferably a `Generator` from `np.random.default_rng()`) explicitly to the `rvs()` calls.\r\n\r\n```python\r\nrng = np.random.default_rng()\r\nerr_array = func(size=size, random_state=rng, **prop)\r\n...\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array, random_state=rng), color=\"tab:blue\", **prop) # works now !\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array, random_state=rng), color=\"tab:orange\", **prop) # works\r\nax.hist(stats.norm.rvs(loc=0, scale=err_array, random_state=rng), color=\"tab:green\", **prop) # works\r\n```\r\n\r\n@mdhaber, this is a design consideration for the new distribution infrastructure, at least, but I think your planned design would just avoid this trap. It's probably fixable in the old infrastructure with little breakage, but there are workarounds like the one above.","Thanks. Yes, similar issues (gh-16998, gh-8053) are tracked in gh-15928. I'll mention this one there, too.","@rkern thanks for the clarification. \r\n\r\nI indeed know that using directly random generator without explicitly providing a random state could be dangerous, but this is what most of the people still do, so this issue. \r\nI'll change that in my code in the meantime. Thanks !"],"labels":["defect","scipy.stats"]},{"title":"ENH: `scipy.special.log_softmax` could be `2**126` to `2**1022` times more accurate","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nConsider\r\n```python\r\nimport numpy as np\r\nimport scipy.special\r\n\r\neps = np.finfo(np.float32).eps\r\nprint(-scipy.special.log_softmax(np.array([1-np.log(2*eps), 0], dtype=np.float32))) # [1.19209275e-07 1.62492371e+01]\r\nprint(-scipy.special.log_softmax(np.array([1-np.log(eps), 0], dtype=np.float32)))   # [-0.       16.942385]\r\n```\r\n\r\nAs I understand it, scipy implements `log_softmax(x)` as `x - np.max(x) - np.log(np.sum(np.exp(x - np.max(x))))`. However, when the largest value is much larger than the rest of the values (about 16 larger for float32, about 36 larger for float64), `log_softmax` returns `0` at the maximum value, when it could give a much more precise answer.\r\n\r\nThis came up when a transformer I was training with cross-entropy loss on a classification task had loss dominated by `np.finfo(np.float32).eps`.\r\n\r\n### Describe the solution you'd like.\r\n\r\nConsider the following code, demonstrating a more accurate `log_softmax`:\r\n```python\r\nimport numpy as np\r\nimport scipy.special\r\n\r\ndef log_softmax_alt(x):\r\n    maxi = np.argmax(x)\r\n    xoffset = x - x[maxi]\r\n    xoffsetexp = np.exp(xoffset)\r\n    # xoffsetexp[maxi] is currently about 1\r\n    xoffsetexp[maxi] = 0\r\n    xoffsetexp_sum_m1 = np.sum(xoffsetexp)\r\n    return xoffset - np.log1p(xoffsetexp_sum_m1)\r\n\r\n\r\nfor ty in (np.float32, np.float64):\r\n    smallest_log_softmax, smallest_log_softmax_alt, smallest_log_softmax_val, smallest_log_softmax_alt_val = 0, 0, 0, 0\r\n    for i in range(int(1-np.log2(np.finfo(ty).smallest_subnormal))):\r\n        values = np.array([1 + i, 0], dtype=ty)\r\n        log_softmax_values = scipy.special.log_softmax(values)\r\n        log_softmax_values_alt = log_softmax_alt(values)\r\n        if log_softmax_values[0] != 0: smallest_log_softmax, smallest_log_softmax_val = i, log_softmax_values[0]\r\n        if log_softmax_values_alt[0] != 0: smallest_log_softmax_alt, smallest_log_softmax_alt_val = i, log_softmax_values_alt[0]\r\n        if log_softmax_values[0] == 0 and log_softmax_values_alt[0] == 0: break\r\n    print(f\"For {ty}, diff in supported input accuracy is 2**-({smallest_log_softmax_alt} - {smallest_log_softmax}) = 2**-{smallest_log_softmax_alt - smallest_log_softmax}; diff in output accuracy is np.log2({-smallest_log_softmax_val}) - np.log2({-smallest_log_softmax_alt_val}) = {np.log2(-smallest_log_softmax_val) - np.log2(-smallest_log_softmax_alt_val)}\")\r\n```\r\nwhich outputs the numbers in the title of this issue:\r\nFor <class 'numpy.float32'>, diff in supported input accuracy is 2**-(102 - 15) = 2**-87; diff in output accuracy is np.log2(1.1920927533992653e-07) - np.log2(1.401298464324817e-45) = 126.0\r\nFor <class 'numpy.float64'>, diff in supported input accuracy is 2**-(744 - 35) = 2**-709; diff in output accuracy is np.log2(2.2204460492503128e-16) - np.log2(5e-324) = 1022.0\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\nI originally posted this as a [StackOverflow question](https:\/\/stackoverflow.com\/q\/77477327\/377022).\r\nCompanion PyTorch issue: https:\/\/github.com\/pytorch\/pytorch\/issues\/113708\r\nCompanion TensorFlow issue: https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62400","comments":["Thanks @JasonGross, that's pretty clever to take advantage of the fact that $$x_i - \\max_i x_i = 0$$\r\nwhen $$i = \\mathrm{argmax}\\space x_k$$ and then use `log1p`. I'm just curious, do you have a use case for this? I'd imagine in most situations the difference wouldn't actually matter.\r\n\r\nAt the moment your implementation only handles 1d arrays and lacks the axis keyword argument. It also doesn't have handling of non-finite values. Check the current signature and implementation of [log_softmax](https:\/\/github.com\/scipy\/scipy\/blob\/f990b1d2471748c79bc4260baf8923db0a5248af\/scipy\/special\/_logsumexp.py#L228-L298). If you get your implementation to parity, feel free to submit a PR.","In most situations the difference doesn't matter.  It came up for me (in pytorch) when I was overtraining a very small transformer and the loss capped out at lower confidence than it needed to.  I'll submit a PR momentarily"],"labels":["enhancement","scipy.special"]},{"title":"BUG: discontinuity in ellipeinc","body":"### Describe your issue.\n\nFor specific combinations of phi and m, small perturbations to the input on the order of 1e-14 cause the output of ellipeinc to jump by almost 0.1\n\n### Reproducing Code Example\n\n```python\nfrom scipy.special import ellipeinc\r\n\r\nphi = 0.9002019046776508\r\nm = 0.12706025328636256\r\nellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\r\n\r\nphi = 0.5549990348297817\r\nm = 0.12706025328636256\r\nellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\n```\n\n\n### Error message\n\n```shell\nN\/A\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.10.1 1.24.3 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/Users\/spark59\/anaconda3\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/Users\/spark59\/anaconda3\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblis_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/Users\/spark59\/anaconda3\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/Users\/spark59\/anaconda3\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = NEON,NEON_FP16,NEON_VFPV4,ASIMD\r\n    found = ASIMDHP,ASIMDDP\r\n    not found = ASIMDFHM\n```\n","comments":["## Issue Summary:\r\n\r\n#Issue Raised:\r\n\r\nThe issue involves small perturbations in the input to ellipeinc, causing a noticeable change in the output.\r\nReproduction Code:\r\n\r\n# python Copy code\r\nfrom scipy.special import ellipeinc\r\n\r\nphi = 0.9002019046776508\r\nm = 0.12706025328636256\r\nresult1 = ellipeinc(phi, m)\r\nresult2 = ellipeinc(phi + 1e-14, m)\r\n\r\nphi = 0.5549990348297817\r\nm = 0.12706025328636256\r\nresult3 = ellipeinc(phi, m)\r\nresult4 = ellipeinc(phi + 1e-14, m)\r\n# Observed Differences:\r\n\r\nThe differences between result1 and result2 are approximately -9.44e-15.\r\nThe differences between result3 and result4 are approximately -9.88e-15.\r\n# Interpretation:\r\n\r\nThese differences are within the typical range of numerical precision for floating-point arithmetic and are commonly considered negligible.\r\n# Conclusion:\r\n\r\nIf these differences do not lead to practical issues in the usage of ellipeinc and align with the expected behavior, they may be deemed acceptable and might not require immediate intervention.","@sarvadutt your numbers disagree with @syp2001's reported \"almost 0.1\" jump - which SciPy\/NumPy\/Python versions did you use?","Cannot reproduce on main\r\n```\r\nIn [1]: from scipy.special import ellipeinc\r\n   ...:\r\n   ...: phi = 0.9002019046776508\r\n   ...: m = 0.12706025328636256\r\n   ...: ellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\r\nOut[1]: -9.43689570931383e-15\r\n\r\nIn [2]: phi = 0.5549990348297817\r\n   ...: m = 0.12706025328636256\r\n   ...: ellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\r\nOut[2]: -9.880984919163893e-15\r\n```\r\nwith numpy 1.24.3","I can have a go with @syp2001's versions at some point\r\n\r\nEdit: I can reproduce with SciPy 1.10.1 and 1.11.4 (with NumPy 1.24.3) and on main (with NumPy 1.25.2).\r\n\r\n```\r\nIn [1]: from scipy.special import ellipeinc\r\n   ...:\r\n   ...: phi = 0.9002019046776508\r\n   ...: m = 0.12706025328636256\r\n   ...: ellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\r\nOut[1]: 0.0949777168363991\r\n\r\nIn [2]: phi = 0.5549990348297817\r\n   ...: m = 0.12706025328636256\r\n   ...: ellipeinc(phi,m)-ellipeinc(phi+1e-14,m)\r\nOut[2]: 0.09497771683639888\r\n```","@j-bowhay any idea where the problem might be? Here's my setup:\r\n\r\nconfig:\r\n\r\n<details>\r\n\r\n```\r\nIn [2]: scipy.show_config()\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.23\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    lib directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT=0 DYNAMIC_ARCH=0 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=1 VORTEX MAX_THREADS=128\r\n    pc file directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.23\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    args: -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2, -isystem,\r\n      \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib, -L\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib,\r\n      -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -isystem,\r\n      \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include, -D_FORTIFY_SOURCE=2, -isystem,\r\n      \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 15.0.7\r\n  c++:\r\n    args: -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib, -L\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib,\r\n      -ftree-vectorize, -fPIC, -fPIE, -fstack-protector-strong, -O2, -pipe, -stdlib=libc++,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include,\r\n      -D_FORTIFY_SOURCE=2, -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    name: clang\r\n    version: 15.0.7\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.0\r\n  fortran:\r\n    args: -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    commands: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    linker args: -Wl,-pie, -Wl,-headerpad_max_install_names, -Wl,-dead_strip_dylibs,\r\n      -Wl,-rpath,\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib, -L\/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/lib,\r\n      -march=armv8.3-a, -ftree-vectorize, -fPIC, -fno-stack-protector, -O2, -pipe,\r\n      -isystem, \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/mambaforge\/envs\/scipy-dev\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev\/bin\/python3.10\r\n  version: '3.10'\r\n```\r\n\r\n<\/details>\r\n\r\nEnvironment:\r\n\r\n<details>\r\n\r\n```\r\n# packages in environment at \/Users\/lucascolley\/mambaforge\/envs\/scipy-dev:\r\n#\r\n# Name                    Version                   Build  Channel\r\nalabaster                 0.7.13             pyhd8ed1ab_0    conda-forge\r\naom                       3.5.0                h7ea286d_0    conda-forge\r\nappnope                   0.1.3              pyhd8ed1ab_0    conda-forge\r\nasttokens                 2.2.1              pyhd8ed1ab_0    conda-forge\r\nasv                       0.4.2           py310h1b49c16_3    conda-forge\r\nattrs                     23.1.0             pyh71513ae_1    conda-forge\r\nbabel                     2.12.1             pyhd8ed1ab_1    conda-forge\r\nbackcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\nbackports                 1.0                pyhd8ed1ab_3    conda-forge\r\nbackports.functools_lru_cache 1.6.5              pyhd8ed1ab_0    conda-forge\r\nbackports.zoneinfo        0.2.1           py310hbe9552e_7    conda-forge\r\nbeautifulsoup4            4.12.2             pyha770c72_0    conda-forge\r\nbeniget                   0.4.1              pyhd8ed1ab_0    conda-forge\r\nbrotli                    1.0.9                h1a8c8d9_9    conda-forge\r\nbrotli-bin                1.0.9                h1a8c8d9_9    conda-forge\r\nbrotli-python             1.0.9           py310h0f1eb42_9    conda-forge\r\nbzip2                     1.0.8                h3422bc3_4    conda-forge\r\nc-compiler                1.6.0                hd291e01_0    conda-forge\r\nca-certificates           2023.7.22            hf0a4a13_0    conda-forge\r\ncairo                     1.16.0            h1e71087_1016    conda-forge\r\ncctools                   973.0.1             hd1ac623_14    conda-forge\r\ncctools_osx-arm64         973.0.1             h2a25c60_14    conda-forge\r\ncertifi                   2023.7.22          pyhd8ed1ab_0    conda-forge\r\ncfgv                      3.4.0                    pypi_0    pypi\r\ncharset-normalizer        3.2.0              pyhd8ed1ab_0    conda-forge\r\nclang                     15.0.7               hce30654_3    conda-forge\r\nclang-15                  15.0.7          default_h5dc8d65_3    conda-forge\r\nclang_osx-arm64           15.0.7               h77e971b_3    conda-forge\r\nclangxx                   15.0.7          default_h610c423_3    conda-forge\r\nclangxx_osx-arm64         15.0.7               h768a7fd_3    conda-forge\r\nclick                     8.1.7           unix_pyh707e725_0    conda-forge\r\ncloudpickle               2.2.1              pyhd8ed1ab_0    conda-forge\r\ncolorama                  0.4.6              pyhd8ed1ab_0    conda-forge\r\ncomm                      0.1.4              pyhd8ed1ab_0    conda-forge\r\ncompiler-rt               15.0.7               hf8d1dfb_1    conda-forge\r\ncompiler-rt_osx-arm64     15.0.7               hf8d1dfb_1    conda-forge\r\ncompilers                 1.6.0                hce30654_0    conda-forge\r\ncontourpy                 1.1.0           py310h38f39d4_0    conda-forge\r\ncoverage                  7.3.0           py310h2aa6e3c_0    conda-forge\r\ncxx-compiler              1.6.0                h1995070_0    conda-forge\r\ncycler                    0.11.0             pyhd8ed1ab_0    conda-forge\r\ncython                    3.0.0           py310h1253130_0    conda-forge\r\ncython-lint               0.15.0             pyhd8ed1ab_0    conda-forge\r\ndav1d                     1.2.1                hb547adb_0    conda-forge\r\ndebugpy                   1.6.8           py310h1253130_0    conda-forge\r\ndecorator                 5.1.1              pyhd8ed1ab_0    conda-forge\r\ndistlib                   0.3.7                    pypi_0    pypi\r\ndocutils                  0.19            py310hbe9552e_1    conda-forge\r\ndoit                      0.36.0             pyhd8ed1ab_0    conda-forge\r\nexceptiongroup            1.1.3              pyhd8ed1ab_0    conda-forge\r\nexecnet                   2.0.2              pyhd8ed1ab_0    conda-forge\r\nexecuting                 1.2.0              pyhd8ed1ab_0    conda-forge\r\nexpat                     2.5.0                hb7217d7_1    conda-forge\r\nffmpeg                    6.0.0           gpl_h474fd3e_103    conda-forge\r\nfilelock                  3.12.2             pyhd8ed1ab_0    conda-forge\r\nfont-ttf-dejavu-sans-mono 2.37                 hab24e00_0    conda-forge\r\nfont-ttf-inconsolata      3.000                h77eed37_0    conda-forge\r\nfont-ttf-source-code-pro  2.038                h77eed37_0    conda-forge\r\nfont-ttf-ubuntu           0.83                 hab24e00_0    conda-forge\r\nfontconfig                2.14.2               h82840c6_0    conda-forge\r\nfonts-conda-ecosystem     1                             0    conda-forge\r\nfonts-conda-forge         1                             0    conda-forge\r\nfonttools                 4.42.1          py310h2aa6e3c_0    conda-forge\r\nfortran-compiler          1.6.0                h5a50232_0    conda-forge\r\nfreetype                  2.12.1               hd633e50_1    conda-forge\r\nfribidi                   1.0.10               h27ca646_0    conda-forge\r\ngast                      0.5.4              pyhd8ed1ab_0    conda-forge\r\ngettext                   0.21.1               h0186832_0    conda-forge\r\ngfortran                  12.3.0               h1ca8e4b_1    conda-forge\r\ngfortran_impl_osx-arm64   12.3.0               hbbb9e1e_1    conda-forge\r\ngfortran_osx-arm64        12.3.0               h57527a5_1    conda-forge\r\ngmp                       6.2.1                h9f76cd9_0    conda-forge\r\ngmpy2                     2.1.2           py310h2e6cad2_1    conda-forge\r\ngnutls                    3.7.8                h9f1a10d_0    conda-forge\r\ngraphite2                 1.3.13            h9f76cd9_1001    conda-forge\r\ngreenlet                  2.0.2           py310h1253130_1    conda-forge\r\nharfbuzz                  7.3.0                h46e5fef_0    conda-forge\r\nhypothesis                6.82.6             pyha770c72_0    conda-forge\r\nicu                       72.1                 he12128b_0    conda-forge\r\nidentify                  2.5.32                   pypi_0    pypi\r\nidna                      3.4                pyhd8ed1ab_0    conda-forge\r\nimagesize                 1.4.1              pyhd8ed1ab_0    conda-forge\r\nimportlib-metadata        6.8.0              pyha770c72_0    conda-forge\r\nimportlib_metadata        6.8.0                hd8ed1ab_0    conda-forge\r\nimportlib_resources       6.0.1              pyhd8ed1ab_0    conda-forge\r\niniconfig                 2.0.0              pyhd8ed1ab_0    conda-forge\r\nipykernel                 6.25.1             pyh5fb750a_0    conda-forge\r\nipython                   8.14.0             pyhd1c38e8_0    conda-forge\r\nisl                       0.25                 h9a09cb3_0    conda-forge\r\njedi                      0.19.0             pyhd8ed1ab_0    conda-forge\r\njinja2                    3.1.2              pyhd8ed1ab_1    conda-forge\r\njpeg                      9e                   he4db4b2_2    conda-forge\r\njsonschema                4.19.0             pyhd8ed1ab_1    conda-forge\r\njsonschema-specifications 2023.7.1           pyhd8ed1ab_0    conda-forge\r\njupyter-cache             0.6.1              pyhd8ed1ab_0    conda-forge\r\njupyter_client            8.3.0              pyhd8ed1ab_0    conda-forge\r\njupyter_core              5.3.1           py310hbe9552e_0    conda-forge\r\njupytext                  1.15.0             pyhcff175f_0    conda-forge\r\nkiwisolver                1.4.4           py310h2887b22_1    conda-forge\r\nlame                      3.100             h1a8c8d9_1003    conda-forge\r\nlcms2                     2.15                 h481adae_0    conda-forge\r\nld64                      609                 h89fa09d_14    conda-forge\r\nld64_osx-arm64            609                 hc4dc95b_14    conda-forge\r\nlerc                      4.0.0                h9a09cb3_0    conda-forge\r\nlibass                    0.17.1               h4da34ad_0    conda-forge\r\nlibblas                   3.9.0           17_osxarm64_openblas    conda-forge\r\nlibbrotlicommon           1.0.9                h1a8c8d9_9    conda-forge\r\nlibbrotlidec              1.0.9                h1a8c8d9_9    conda-forge\r\nlibbrotlienc              1.0.9                h1a8c8d9_9    conda-forge\r\nlibcblas                  3.9.0           17_osxarm64_openblas    conda-forge\r\nlibclang-cpp15            15.0.7          default_h5dc8d65_3    conda-forge\r\nlibcxx                    16.0.6               h4653b0c_0    conda-forge\r\nlibdeflate                1.17                 h1a8c8d9_0    conda-forge\r\nlibexpat                  2.5.0                hb7217d7_1    conda-forge\r\nlibffi                    3.4.2                h3422bc3_5    conda-forge\r\nlibgfortran               5.0.0           12_3_0_hd922786_1    conda-forge\r\nlibgfortran-devel_osx-arm64 12.3.0               hc62be1c_1    conda-forge\r\nlibgfortran5              12.3.0               ha3a6a3e_1    conda-forge\r\nlibglib                   2.76.4               h24e9cb9_0    conda-forge\r\nlibiconv                  1.17                 he4db4b2_0    conda-forge\r\nlibidn2                   2.3.4                h1a8c8d9_0    conda-forge\r\nlibjpeg-turbo             2.1.4                h1a8c8d9_0    conda-forge\r\nliblapack                 3.9.0           17_osxarm64_openblas    conda-forge\r\nlibllvm15                 15.0.7               h504e6bf_3    conda-forge\r\nlibopenblas               0.3.23          openmp_hc731615_0    conda-forge\r\nlibopus                   1.3.1                h27ca646_1    conda-forge\r\nlibpng                    1.6.39               h76d750c_0    conda-forge\r\nlibsodium                 1.0.18               h27ca646_1    conda-forge\r\nlibsqlite                 3.42.0               hb31c410_0    conda-forge\r\nlibtasn1                  4.19.0               h1a8c8d9_0    conda-forge\r\nlibtiff                   4.5.0                h5dffbdd_2    conda-forge\r\nlibunistring              0.9.10               h3422bc3_0    conda-forge\r\nlibvpx                    1.13.0               h7ea286d_0    conda-forge\r\nlibwebp-base              1.3.1                hb547adb_0    conda-forge\r\nlibxcb                    1.13              h9b22ae9_1004    conda-forge\r\nlibxml2                   2.11.5               he3bdae6_0    conda-forge\r\nlibzlib                   1.2.13               h53f4e23_5    conda-forge\r\nllvm-openmp               16.0.6               h1c12783_0    conda-forge\r\nllvm-tools                15.0.7               h504e6bf_3    conda-forge\r\nmarkdown-it-py            2.2.0              pyhd8ed1ab_0    conda-forge\r\nmarkupsafe                2.1.3           py310h2aa6e3c_0    conda-forge\r\nmatplotlib                3.6.1           py310hb6292c7_1    conda-forge\r\nmatplotlib-base           3.6.1           py310h78c5c2f_1    conda-forge\r\nmatplotlib-inline         0.1.6              pyhd8ed1ab_0    conda-forge\r\nmdit-py-plugins           0.4.0              pyhd8ed1ab_0    conda-forge\r\nmdurl                     0.1.0              pyhd8ed1ab_0    conda-forge\r\nmeson                     1.2.1              pyhd8ed1ab_0    conda-forge\r\nmeson-python              0.13.2             pyh0c530f3_0    conda-forge\r\nmpc                       1.3.1                h91ba8db_0    conda-forge\r\nmpfr                      4.2.0                he09a6ba_0    conda-forge\r\nmpmath                    1.3.0              pyhd8ed1ab_0    conda-forge\r\nmunkres                   1.1.4              pyh9f0ad1d_0    conda-forge\r\nmypy                      1.5.1           py310h2aa6e3c_0    conda-forge\r\nmypy_extensions           1.0.0              pyha770c72_0    conda-forge\r\nmyst-nb                   0.17.2             pyhd8ed1ab_0    conda-forge\r\nmyst-parser               0.18.1             pyhd8ed1ab_0    conda-forge\r\nnbclient                  0.7.4              pyhd8ed1ab_0    conda-forge\r\nnbformat                  5.9.2              pyhd8ed1ab_0    conda-forge\r\nncurses                   6.4                  h7ea286d_0    conda-forge\r\nnest-asyncio              1.5.6              pyhd8ed1ab_0    conda-forge\r\nnettle                    3.8.1                h63371fa_1    conda-forge\r\nnetworkx                  3.1                pyhd8ed1ab_0    conda-forge\r\nninja                     1.11.1               hffc8910_0    conda-forge\r\nnodeenv                   1.8.0                    pypi_0    pypi\r\nnumpy                     1.25.2          py310haa1e00c_0    conda-forge\r\nnumpydoc                  1.5.0              pyhd8ed1ab_0    conda-forge\r\nopenblas                  0.3.23          openmp_hf78f355_0    conda-forge\r\nopenh264                  2.3.1                hb7217d7_2    conda-forge\r\nopenjpeg                  2.5.0                hbc2ba62_2    conda-forge\r\nopenssl                   3.1.4                h0d3ecfb_0    conda-forge\r\np11-kit                   0.24.1               h29577a5_0    conda-forge\r\npackaging                 23.1               pyhd8ed1ab_0    conda-forge\r\nparso                     0.8.3              pyhd8ed1ab_0    conda-forge\r\npcre2                     10.40                hb34f9b4_0    conda-forge\r\npexpect                   4.8.0              pyh1a96a4e_2    conda-forge\r\npickleshare               0.7.5                   py_1003    conda-forge\r\npillow                    9.4.0           py310h5a7539a_1    conda-forge\r\npip                       23.2.1             pyhd8ed1ab_0    conda-forge\r\npixman                    0.40.0               h27ca646_0    conda-forge\r\npkg-config                0.29.2            hab62308_1008    conda-forge\r\npkgutil-resolve-name      1.3.10             pyhd8ed1ab_0    conda-forge\r\nplatformdirs              3.10.0             pyhd8ed1ab_0    conda-forge\r\npluggy                    1.2.0              pyhd8ed1ab_0    conda-forge\r\nply                       3.11                       py_1    conda-forge\r\npooch                     1.7.0              pyha770c72_3    conda-forge\r\npre-commit                3.5.0                    pypi_0    pypi\r\nprompt-toolkit            3.0.39             pyha770c72_0    conda-forge\r\nprompt_toolkit            3.0.39               hd8ed1ab_0    conda-forge\r\npsutil                    5.9.5           py310h8e9501a_0    conda-forge\r\npthread-stubs             0.4               h27ca646_1001    conda-forge\r\nptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\npure_eval                 0.2.2              pyhd8ed1ab_0    conda-forge\r\npybind11                  2.11.1          py310h38f39d4_0    conda-forge\r\npybind11-global           2.11.1          py310h38f39d4_0    conda-forge\r\npycodestyle               2.11.0             pyhd8ed1ab_0    conda-forge\r\npydata-sphinx-theme       0.9.0              pyhd8ed1ab_1    conda-forge\r\npydevtool                 0.3.0              pyhd8ed1ab_0    conda-forge\r\npygments                  2.16.1             pyhd8ed1ab_0    conda-forge\r\npyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge\r\npyproject-metadata        0.7.1              pyhd8ed1ab_0    conda-forge\r\npysocks                   1.7.1              pyha2e5f31_6    conda-forge\r\npytest                    7.4.0              pyhd8ed1ab_0    conda-forge\r\npytest-cov                4.1.0              pyhd8ed1ab_0    conda-forge\r\npytest-timeout            2.1.0              pyhd8ed1ab_0    conda-forge\r\npytest-xdist              3.3.1              pyhd8ed1ab_0    conda-forge\r\npython                    3.10.12         h01493a6_0_cpython    conda-forge\r\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\npython-fastjsonschema     2.18.0             pyhd8ed1ab_0    conda-forge\r\npython_abi                3.10                    3_cp310    conda-forge\r\npythran                   0.13.1          py310h11c608e_0    conda-forge\r\npytorch                   2.0.1                  py3.10_0    pytorch\r\npytz                      2023.3             pyhd8ed1ab_0    conda-forge\r\npyyaml                    6.0.1           py310h2aa6e3c_0    conda-forge\r\npyzmq                     25.1.1          py310h30b7201_0    conda-forge\r\nreadline                  8.2                  h92ec313_1    conda-forge\r\nreferencing               0.30.2             pyhd8ed1ab_0    conda-forge\r\nrequests                  2.31.0             pyhd8ed1ab_0    conda-forge\r\nrich                      13.5.1             pyhd8ed1ab_0    conda-forge\r\nrich-click                1.6.1              pyhd8ed1ab_0    conda-forge\r\nrpds-py                   0.9.2           py310had9acf8_0    conda-forge\r\nruff                      0.1.5           py310hd1b256b_0    conda-forge\r\nsetuptools                67.1.0             pyhd8ed1ab_0    conda-forge\r\nsigtool                   0.1.3                h44b9a77_0    conda-forge\r\nsix                       1.16.0             pyh6c4a22f_0    conda-forge\r\nsnowballstemmer           2.2.0              pyhd8ed1ab_0    conda-forge\r\nsortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge\r\nsoupsieve                 2.3.2.post1        pyhd8ed1ab_0    conda-forge\r\nsphinx                    5.3.0              pyhd8ed1ab_0    conda-forge\r\nsphinx-design             0.5.0              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-applehelp   1.0.7              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-devhelp     1.0.5              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-htmlhelp    2.0.4              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-jsmath      1.0.1              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-qthelp      1.0.6              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-serializinghtml 1.1.9              pyhd8ed1ab_0    conda-forge\r\nsqlalchemy                2.0.20          py310h2aa6e3c_0    conda-forge\r\nstack_data                0.6.2              pyhd8ed1ab_0    conda-forge\r\nsvt-av1                   1.6.0                hb765f3a_0    conda-forge\r\nsympy                     1.12            pypyh9d50eac_103    conda-forge\r\ntabulate                  0.9.0              pyhd8ed1ab_1    conda-forge\r\ntapi                      1100.0.11            he4954df_0    conda-forge\r\nthreadpoolctl             3.2.0              pyha21a80b_0    conda-forge\r\ntk                        8.6.12               he1e0b03_0    conda-forge\r\ntokenize-rt               5.2.0              pyhd8ed1ab_0    conda-forge\r\ntoml                      0.10.2             pyhd8ed1ab_0    conda-forge\r\ntomli                     2.0.1              pyhd8ed1ab_0    conda-forge\r\ntorchaudio                2.0.2                 py310_cpu    pytorch\r\ntorchvision               0.15.2                py310_cpu    pytorch\r\ntornado                   6.3.3           py310h2aa6e3c_0    conda-forge\r\ntraitlets                 5.9.0              pyhd8ed1ab_0    conda-forge\r\ntypes-psutil              5.9.5.6            pyhd8ed1ab_0    conda-forge\r\ntyping-extensions         4.7.1                hd8ed1ab_0    conda-forge\r\ntyping_extensions         4.7.1              pyha770c72_0    conda-forge\r\ntzdata                    2023c                h71feb2d_0    conda-forge\r\nunicodedata2              15.0.0          py310h8e9501a_0    conda-forge\r\nurllib3                   2.0.4              pyhd8ed1ab_0    conda-forge\r\nvirtualenv                20.24.7                  pypi_0    pypi\r\nwcwidth                   0.2.6              pyhd8ed1ab_0    conda-forge\r\nwheel                     0.41.2             pyhd8ed1ab_0    conda-forge\r\nx264                      1!164.3095           h57fd34a_2    conda-forge\r\nx265                      3.5                  hbc6ce65_3    conda-forge\r\nxorg-libxau               1.0.11               hb547adb_0    conda-forge\r\nxorg-libxdmcp             1.1.3                h27ca646_0    conda-forge\r\nxz                        5.2.6                h57fd34a_0    conda-forge\r\nyaml                      0.2.5                h3422bc3_2    conda-forge\r\nzeromq                    4.3.4                hbdafb3b_1    conda-forge\r\nzipp                      3.16.2             pyhd8ed1ab_0    conda-forge\r\nzlib                      1.2.13               h53f4e23_5    conda-forge\r\nzstd                      1.5.2                h4f39d0f_7    conda-forge\r\n```","I am on linux so my compilers are different \r\n```\r\nIn [2]: scipy.show_config()\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=2\r\n    pc file directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    lib directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=2\r\n    pc file directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include,\r\n      -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include,\r\n      -DNDEBUG, -D_FORTIFY_SOURCE=2, -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 11.3.0\r\n  c++:\r\n    args: -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -fvisibility-inlines-hidden, -fmessage-length=0, -march=nocona, -mtune=haswell,\r\n      -ftree-vectorize, -fPIC, -fstack-protector-strong, -fno-plt, -O2, -ffunction-sections,\r\n      -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include, -DNDEBUG, -D_FORTIFY_SOURCE=2,\r\n      -O2, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 11.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.35\r\n  fortran:\r\n    args: -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    commands: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    linker args: -Wl,-O2, -Wl,--sort-common, -Wl,--as-needed, -Wl,-z,relro, -Wl,-z,now,\r\n      -Wl,--disable-new-dtags, -Wl,--gc-sections, -Wl,--allow-shlib-undefined, -Wl,-rpath,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -Wl,-rpath-link,\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib, -L\/home\/jakeb\/miniconda3\/envs\/scipy-dev\/lib,\r\n      -march=nocona, -mtune=haswell, -ftree-vectorize, -fPIC, -fstack-protector-strong,\r\n      -fno-plt, -O2, -ffunction-sections, -pipe, -isystem, \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/include\r\n    name: gcc\r\n    version: 11.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/miniconda3\/envs\/scipy-dev\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.12.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/home\/jakeb\/miniconda3\/envs\/scipy-dev\/bin\/python3.10\r\n  version: '3.10'\r\n```","cc @steppi as you're working on Cephes","If it helps, this is the context in which the bug occurred:\r\n\r\n```python\r\nfrom scipy.special import ellipeinc, ellipk, ellipj\r\n\r\nx = 9\/16\r\nm = 0.12706025328636256\r\nu = ellipk(m) * x\r\nsn, cn, dn, ph = ellipj(u, m)\r\n\r\nellipeinc(ph, m) - ellipeinc(ph+1e-14, m)\r\n```\r\n\r\nIt seems to only happen when x is a fraction where the denominator is a power of 2. The examples that I gave were for x = 9\/16 and x = 11\/32. It also happens at 11\/16, 15\/32, 18\/32, 21\/32, 22\/32, and 23\/32.","> cc @steppi as you're working on Cephes\r\n\r\nThis is on my radar and I hope to take a look sometime the next couple of months. ","> @sarvadutt your numbers disagree with @syp2001's reported \"almost 0.1\" jump - which SciPy\/NumPy\/Python versions did you use?\n\nVersion: 1.11.3"],"labels":["defect","scipy.special"]},{"title":"RFC: Thoughts on making our vendored Cephes a cleaner header only library.","body":"This is a follow up on https:\/\/github.com\/scipy\/scipy\/issues\/19404, the effort to translate compiled kernels for special functions where SciPy provides its own implementation from Cython into header only NVCC compatible C++.\r\n\r\nWhile translating the function [binom](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.binom.html) for the binomial coefficient of two real arguments in https:\/\/github.com\/scipy\/scipy\/pull\/19471, it was observed that the current [Cython implementation](https:\/\/github.com\/scipy\/scipy\/blob\/8a7959d449f46017bef5388f696ce11615176b9f\/scipy\/special\/orthogonal_eval.pxd#L69) makes use of the functions from `Gamma`, `beta`, and `betaln` from our vendored copy of [the Cephes mathematical library](https:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy\/special\/cephes). At the moment Cephes is not header only, but I think getting it to that point should be straightforward if tedious, and I'd be curious to hear others thoughts on the matter.\r\n\r\nPerhaps this should be a separate issue, but regarding \"cleaner\",  the main problem is that there is a header `cephes_names.h` which defines aliases like `beta` -> `cephes_beta`. The idea here is to give a prefix to each symbol for a cephes function, which is a good idea. Unfortunately, up to now, it hasn't been noticed that this aliases leak into global scope for everything in the same translation unit. When I put `#include \"cephes.h\"` inside of the C++ header for `binom`, the aliases were getting expanded inside of Boost, causing compilation errors because the signatures of `expm1` are different in Boost vs Cephes. I don't want to ponder what might have happened if all of the signatures were the same.\r\n\r\nAs a workaround, I wrote a header, https:\/\/github.com\/scipy\/scipy\/pull\/19471\/files#diff-08bc49c27ad8094da8b42c6cffce31b867922eaf105ec5a6d598ba824bb771d8, which undefs all of the aliases, and puts functions from Cephes into a C++ namespace `scipy::special::cephes`. This will work for now, but isn't a long term solution. I think we should fix this problem so that the aliases are only used internally within cephes, and then external code in C would actually call `cephes_beta` etc. External code in C++ could use a namespace instead of the prefix.\r\n\r\nDone improperly, it's possible making Cephes special functions inlined inside of headers could cause a harmful impact on build times. Rather than having one header `\"cephes.h\"` with every function like we have now, I think it would be better to have separate headers for each function (or small group of related functions) and to only include those that are needed in a given file.\r\n\r\nThis seems like it could be a significant effort, so I'd appreciate any wisdom and guidance on how to best carry it out. I'd suspect we'd want to do things in a piecemeal fashion, only moving functions to headers only as we need them.\r\n\r\nHappy to hear your thoughts.\r\n","comments":["Just wanted to +1 this. I think it's annoying work, but important work for things to progress. It should be reasonably straightforward to turn cephes into a header-only C (not C++, but C++ compatible) library. At least then it wouldn't block our other current efforts."],"labels":["scipy.special","RFC"]},{"title":"BUG: Wrong attributes when multiplying sparse array with dense vector","body":"### Describe your issue.\r\n\r\nHi,\r\n\r\nI found an un-expected behavior when elementwise multiplying a sparse array or matrix with a dense vector which has some zero elements. When using the .row & .col attributes on the result of the multiplication we get the nonzero indices of the array prior to the multiplication. However, when using the .nonzero() method on the result of the multiplication we get the correct indices for the nonzero elements. \r\n\r\nThis issue doesn't seem to occur when the dense vector has no zero elements. \r\nSee toy example. \r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np \r\nimport scipy.sparse as ssr\r\n\r\nv = np.array([0,1,0,1])\r\nm = np.array([[0,5,0,0], [5,0,0,3], [0,0,0,0]])\r\nm_sparse = ssr.csr_array(m)\r\n\r\n# Multiply (elementwise) each row of m with v. \r\n# In scipy notation terms, the nonzero indices of resulting multiplication should be: ( [0,1], [1,3] ). \r\n# When using the nonzero method to extract the nonzero positions we get the nonzero indices of m_sparse (i.e. before multiplying by v).\r\n\r\n# With .multiply() (The issue doen't occur if v does not have any zeros):\r\n(m_sparse.multiply(v)).row # output: [0,1,1]\r\n(m_sparse.multiply(v)).col # output: [1,0,3]\r\n(m_sparse.multiply(v)).nonzero() # output: ([0,1], [1,3])\r\n\r\n# A different approach that doesn't cause a problem:\r\n(m_sparse @ ssr.diags(v, offsets=0, format='csr')).tocoo().row # output: [0,1]\r\n(m_sparse @ ssr.diags(v, offsets=0, format='csr')).tocoo().col # output: [1,3]\r\n(m_sparse @ ssr.diags(v, offsets=0, format='csr')).nonzero() # output: ([0,1], [1,3])\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nnot applicable\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nprint(scipy.__version__, numpy.__version__, sys.version_info)\r\n\r\n1.10.1 1.25.0 sys.version_info(major=3, minor=9, releaselevel='final', serial=0)\r\n\r\nscipy.show_config()\r\n\r\n{\r\n\t\"Compiliers\": {\r\n\t\t\"c\": {\r\n\t\t\t\"name\": \"gcc\",\r\n\t\t\t\"linker\": \"ld.bfd\",\r\n\t\t\t\"version\": \"10.2.1\"\r\n\t\t\t\"commands\": \"cc\"\r\n\t\t},\r\n\t\t\"cython\": {\r\n\t\t\t\"name\": \"cython\",\r\n\t\t\t\"linker\": \"cython\",\r\n\t\t\t\"version\": \"0.29.33\",\r\n\t\t\t\"commands\": \"cython\"\r\n\t\t},\r\n\t\t\"c++\": {\r\n\t\t\t\"name\": \"gcc\",\r\n\t\t\t\"linker\": \"ld.bfd\",\r\n\t\t\t\"version\": \"10.2.1\",\r\n\t\t\t\"commands\": \"c++\"\r\n\t\t},\r\n\t\t\"fortran\": {\r\n\t\t\t\"name\": \"gcc\",\r\n\t\t\t\"linker\": \"ld.bfd\",\r\n\t\t\t\"version\": \"10.2.1\",\r\n\t\t\t\"commands\": \"gfortran\"\r\n\t\t},\r\n\t\t\"pythran\": {\r\n\t\t\t\"version\": \"0.12.1\",\r\n\t\t\t\"include directory\": \"\/tmp\/pip-build-env-yjjniym9\/overlay\/lib\/python3.9\/site-packages\/pythran\"\r\n\t\t}\r\n\t},\r\n\t\"Machine Information\": {\r\n\t\t\"host\": {\r\n\t\t\t\"cpu\": \"x86_64\",\r\n\t\t\t\"family\": \"x86_64\",\r\n\t\t\t\"endian\": \"little\",\r\n\t\t\t\"system\": \"linux\"\r\n\t\t},\r\n\t\t\"build\": {\r\n\t\t\t\"cpu\": \"x86_64\",\r\n\t\t\t\"family\": \"x86_64\",\r\n\t\t\t\"endian\": \"little\",\r\n\t\t\t\"system\": \"linux\"\r\n\t\t},\r\n\t\t\"cross-compiled\": false\r\n\t},\r\n\t\"Build Dependencies\": {\r\n\t\t\"blas\": {\r\n\t\t\t\"name\": \"OpenBLAS\",\r\n\t\t\t\"found\": true,\r\n\t\t\t\"version\": \"0.3.18\",\r\n\t\t\t\"detection method\": \"cmake\",\r\n\t\t\t\"include directory\": \"unknown\",\r\n\t\t\t\"lib directory\": \"unknown\",\r\n\t\t\t\"openblas configuration\": \"unknown\",\r\n\t\t\t\"pc file directory\": \"unknown\"\r\n\t\t},\r\n\t\t\"lapack\": {\r\n\t\t\t\"name\": \"OpenBLAS\",\r\n\t\t\t\"found\": true,\r\n\t\t\t\"version\": \"0.3.18\",\r\n\t\t\t\"detection method\": \"cmake\",\r\n\t\t\t\"include directory\": \"unknown\",\r\n\t\t\t\"lib directory\": \"unknown\",\r\n\t\t\t\"openblas configuration\": \"unknown\",\r\n\t\t\t\"pc file directory\": \"unknown\"\r\n\t\t}\r\n\t},\r\n\t\"Python Information\": {\r\n\t\t\"path\": \"\/opt\/python\/cp39-cp39\/bin\/python\",\r\n\t\t\"version\": \"3.9\"\r\n\t}\r\n}\r\n```\r\n","comments":["Hi!,  I am trying to reproduce this issue and I get this error, why do we have inconsistent shapes? does this happen in your environment?, thanks\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/13369072\/e49bbd34-4ba3-4918-9372-a8d9bf875562)\r\n","This looks like a mix-up in the reporting of the error between using `csr_array` in the example, but using `csr_matrix` in the code that actually produced the results.  The Issue shows `csr_array` to create `m_sparse`, but the code that \"doesn't cause a problem\" seems to use `csr_matrix`. (I guess it is possible that it uses `@` instead of `*`... but `*` with `csr_array` creates the `ValueError` reported by @SebastianMunozMora )\r\n\r\nCan you verify this @nirryde ?\r\n\r\n**Further investigation** (assuming using `csr_matrix` to cause the error with `row` and `col`):\r\n`m_sparse.multiply(v)` results in a `coo_matrix` with some of its \"nonzero\" values being `0`.\r\nLook at `m_sparse.multiply(v).data` -> `array([5, 0, 3])`.\r\n\r\nTo remove those zero values, you need the extra step of eliminating the zeros. In your case:\r\n`m_sparse.multiply(v).eliminate_zeros()`\r\n\r\nNote that this means `m_sparse.row` shows the rows for the **stored values**, not for the **nonzero values**.  Perhaps this could be improved in the docs.\r\n\r\nThe choice not to remove zero values when doing element-wise multiplication is to avoid the extra computation. Note that any manipulations of the matrix are still numerically correct when zero values are stored in the matrix. The question is whether it is worth the time to remove them.  ","@dschult My bad, the * in the code that works was supposed to be a @ (I edited the post).\r\nIf I understand correctly the .row and .col properties should return the nonzero indices of the sparse COO array i.e they should coincide with the output of the .nonzero() method.  Otherwise this seems inconsistent with the .row and .col properties representing the 'ij' part in the 'ijv' triplet.  \r\n\r\n\r\n ","The .row and .col properties represent the indices of the sparse COO array, but if the COO array stores some zeros, those values get rows and columns too. The attributes .row and .col only match .nonzero() **only if there aren't any zeros stored explicitly in the coo_array**.\r\n\r\n```python\r\nimport scipy as sp;import numpy as np\r\nA = np.arange(6).reshape(2,3)\r\nAsp = sp.sparse.csr_array(A)\r\nprint(Asp.data)  # -> array([1 2 3 4 5])\r\n# Add a zero value to the array\r\nAsp[1, 0] = 0\r\nprint(Asp.data)  # -> array([1 2 0 4 5])\r\n\r\n# switch to coo format\r\nAcoo=Asp.tocoo(copy=False)\r\nprint(\"row: \", Acoo.row)     # -> array([0 0 1 1 1])\r\nprint(\"col: \", Acoo.col)     # -> array([1 2 0 1 2])\r\nprint(\"data:\", Acoo.data)    # -> array([1 2 0 4 5])\r\nprint(\"nonzero: \", Acoo.nonzero())  # -> (array([0, 0, 1, 1], dtype=int32), array([1, 2, 1, 2], dtype=int32))\r\n\r\n# eliminate zeros   This produces what you were expecting\r\nAcoo.eliminate_zeros()\r\nprint(\"row: \", Acoo.row)     # -> array([0 0 1 1])\r\nprint(\"col: \", Acoo.col)     # -> array([1 2 1 2])\r\nprint(\"data:\", Acoo.data)    # -> array([1 2 4 5])\r\n```\r\n\r\nThis doesn't violate the idea that row and col represent the 'ij' in the 'ijv' triplet precisely when the 'v' in 'ijv' is zero.","@dschult I see... I feel like the documentation does not clarify the fact that an operation (like the multiplication I did) can end up with a sparse matrix with zero elements. In other words, I expect that computations which return a sparse array will never have zero elements. Are there any use cases where we would want to explicitly store zero values?","When you do a multiplication like you did, then any elements that multiply a default value (0) in the other array will result in the value zero.  It can be expensive to update all those zeros if all you want to do with that result is e.g. sum the columns. Rather than copying or reallocating the behind-the-scenes arrays in the sparse matrix, you can just store a value that is zero. It won't affect the sum's result and it will be faster and avoid another allocation of memory to hold intermediate results. \r\n\r\nIf you do want to eliminate any zero entries explicitly stored, the `eliminate_zeros()` method is available and you should use it before any operations that would be expensive or confusing if they report locations where the value is zero. The main difficulty as I see it is when people don't know that the values can be zero -- that you should use `nonzero()` if you want to know that. I think documentation is the place where that can be fixed.\r\n\r\nI suppose you could eliminate zeros after every operation, but as you might expect, that gets cumbersome in the code -- and would also slow down your computation of most complicated expressions. It really is better this way most of the time."],"labels":["defect","scipy.sparse"]},{"title":"BUG: SystemError: initialization of beta_ufunc raised unreported exception","body":"### Describe your issue.\n\nI have a 2D array of points and I want to cluster it. I have used the DBSCAN method. The main function is in C++ and the DBSCAN method is in Python. I have written an interface between both. The code is running fine but it gives Segmentation Fault error when I run it to check memory leaks using Valgrind.\r\n\r\nSo I debugged the Python code and found that just the import call to DBSCAN is throwing the error.\r\n\r\nMy folder structure is as follows:\r\nTest:\r\n\u251c\u2500\u2500 CMakeLists.txt\r\n\u2514\u2500\u2500 src\r\n    \u251c\u2500\u2500 Py_Interface\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pyhelper.hpp\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 Py_Integration.cpp\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Py_Integration.h\r\n    \u251c\u2500\u2500 PythonDep\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Cluster.py\r\n    \u2514\u2500\u2500 main.cpp\r\n\r\nAttached is the zip file of the source code required for reproducing the error.\r\n[Scipy Error.zip](https:\/\/github.com\/scipy\/scipy\/files\/13262450\/Scipy.Error.zip)\r\n\r\nvalgrind installation:\r\n> sudo apt install valgrind\n\n### Reproducing Code Example\n\n```python\nvalgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --verbose --log-file=valgrind-out.txt .\/Exec\n```\n\n\n### Error message\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/suraj\/Projects\/Scipy Error\/build\/..\/src\/PythonDep\/Cluster.py\", line 1, in <module>\r\n    from sklearn.cluster import DBSCAN\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/__init__.py\", line 83, in <module>\r\n    from .base import clone\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 19, in <module>\r\n    from .utils import _IS_32BIT\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/__init__.py\", line 22, in <module>\r\n    from ._param_validation import Interval, validate_params\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/_param_validation.py\", line 15, in <module>\r\n    from .validation import _is_arraylike_not_scalar\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py\", line 25, in <module>\r\n    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/_array_api.py\", line 9, in <module>\r\n    from .fixes import parse_version\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/fixes.py\", line 19, in <module>\r\n    import scipy.stats\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/scipy\/stats\/__init__.py\", line 485, in <module>\r\n    from ._stats_py import *\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/scipy\/stats\/_stats_py.py\", line 46, in <module>\r\n    from . import distributions\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/scipy\/stats\/distributions.py\", line 10, in <module>\r\n    from . import _continuous_distns\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/scipy\/stats\/_continuous_distns.py\", line 32, in <module>\r\n    import scipy.stats._boost as _boost\r\n  File \"\/home\/suraj\/.local\/lib\/python3.8\/site-packages\/scipy\/stats\/_boost\/__init__.py\", line 1, in <module>\r\n    from scipy.stats._boost.beta_ufunc import (\r\nSystemError: initialization of beta_ufunc raised unreported exception\r\nSegmentation fault (core dumped)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.10.0 1.24.4 sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblis_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_opt_info:\r\n    libraries = ['openblas', 'openblas']\r\n    library_dirs = ['\/usr\/local\/lib']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\n```\n","comments":["Could you post a minimal working example? I think it must probably boil down to some parameter edge case for the beta function. Opening zip files is a security risk and strongly discouraged on github.\r\n\r\nAnother possibility is that this is already fixed in scipy 1.11 where the boost functions were overhauled (https:\/\/github.com\/scipy\/scipy\/pull\/17432). Could you test your code with a more recent scipy version to rule this out?","1. I'm trying to upgrade the Scipy version to 1.11.3(the latest stable version) but it is not working. I uninstalled the 1.10.1 and triesd installing again by giving the version then the following error occurred:\r\n```\r\nERROR: Could not find a version that satisfies the requirement scipy==1.11.3 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0, 1.8.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1)\r\nERROR: No matching distribution found for scipy==1.11.3\r\n```\r\n\r\nI'm using Ubuntu 20.04.\r\n\r\n\r\n2. Further, I downloaded the zip of the latest version, extracted it, and placed it in the dist-packages folder, it is not detected by Python.","The minimal code example:\r\n\r\nCMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.5 FATAL_ERROR)\r\n\r\nproject(Exec)\r\n \r\nfind_package(PythonLibs REQUIRED)\r\n\r\ninclude_directories(\r\n\"src\/Py_Interface\")\r\n\r\n\r\ninclude_directories(${PYTHON_INCLUDE_DIRS})\r\n\r\nadd_executable (Exec src\/main.cpp src\/Py_Interface\/Py_Integration.cpp)\r\ntarget_link_libraries (Exec ${PYTHON_LIBRARIES})\r\n```\r\n\r\nmain.cpp\r\n```\r\n#include \"Py_Interface\/Py_Integration.h\"\r\n\r\nint main()\r\n{\r\n    Py_Wrapper();\r\n}\r\n```\r\n\r\nCluster.py\r\n```\r\nfrom sklearn.cluster import DBSCAN\r\n\r\ndef clustering():\r\n    print(\"Clustering\\n\")\r\n```\r\n\r\nPy_Integration.h\r\n```\r\n#ifndef DL_INTEGRATION_H\r\n#define DL_INTEGRATION_H\r\n\r\n#include <Python.h>\r\n#include \"pyhelper.hpp\"\r\n#include <string>\r\n\r\nPyObject *PythonInitialize(std::string script_name, std::string function_name);\r\n\r\nvoid Py_Wrapper();\r\n\r\n#endif \/\/ DL_INTEGRATION_H\r\n\r\n```\r\n\r\nPy_Integration.cpp\r\n```\r\n#include \"Py_Integration.h\"\r\n\r\nusing namespace std;\r\n\r\nvoid Py_Wrapper()\r\n{\r\n    Py_InitializeEx(0); \/\/ Initialize the Python interpreter\r\n\r\n    PyObject *PythonDetectorFunction, *PythonDetectorFunctionArguments;\r\n    PythonDetectorFunction = PythonInitialize(\"Cluster\", \"clustering\"); \r\n\r\n    if (PythonDetectorFunction == NULL)\r\n        PyErr_Print();\r\n    \r\n    PyObject *PythonDetectorFeatureMaps = PyObject_CallObject(PythonDetectorFunction, PythonDetectorFunctionArguments);\r\n\r\n    Py_XDECREF(PythonDetectorFunction);\r\n    Py_XDECREF(PythonDetectorFunctionArguments);\r\n    Py_XDECREF(PythonDetectorFeatureMaps);\r\n}\r\n\r\nPyObject *PythonInitialize(string script_name, string function_name)\r\n{\r\n    PyObject *pName, *pModule, *pFunc;\r\n    PyRun_SimpleString(\"import sys, os\");\r\n    string PythonCodeImportStatement = \"sys.path.append(os.getcwd() + '\/..\/src\/PythonDep\/')\"; \/\/ Relative path for the python code\r\n    PyRun_SimpleString(PythonCodeImportStatement.c_str());\r\n\r\n    \/\/ pNmae is the name of the python script\/module to be called\r\n    pName = PyUnicode_FromString(script_name.c_str());\r\n    if (pName == NULL)\r\n        PyErr_Print();\r\n\r\n    \/\/ Getting the Python module reference inside of the C code\r\n    pModule = PyImport_Import(pName);\r\n    if (pModule == NULL)\r\n        PyErr_Print();\r\n\r\n    \/\/ Python function reference\r\n    pFunc = PyObject_GetAttrString(pModule, function_name.c_str());\r\n    if (pFunc == NULL)\r\n        PyErr_Print();\r\n\r\n    \/\/ Refrence count clean-up\r\n    Py_XDECREF(pName);\r\n    Py_XDECREF(pModule);\r\n\r\n    return pFunc;\r\n}\r\n\r\n```\r\n\r\nI have taken pyhelper.hpp from: https:\/\/github.com\/beno54\/Snake-integer\/blob\/master\/pyhelper.hpp\r\nI have posted the directory structure in the issue post above.","> 1. I'm trying to upgrade the Scipy version to 1.11.3(the latest stable version) but it is not working. I uninstalled the 1.10.1 and triesd installing again by giving the version then the following error occurred:\r\n> \r\n> ```\r\n> ERROR: Could not find a version that satisfies the requirement scipy==1.11.3 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.8.0rc1, 1.8.0rc2, 1.8.0rc3, 1.8.0rc4, 1.8.0, 1.8.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1)\r\n> ERROR: No matching distribution found for scipy==1.11.3\r\n> ```\r\n> \r\n> I'm using Ubuntu 20.04.\r\n> \r\n> 2. Further, I downloaded the zip of the latest version, extracted it, and placed it in the dist-packages folder, it is not detected by Python.\r\n\r\nAre you installing scipy via the Ubuntu package manager? `pip install scipy` or `conda install scipy` instead will definitely get you the latest version.\r\n\r\nThe minimal working example does not contain calls to the boost distributions. We probably need the function you are integrating as that is where the call to boost and the subsequent error occurs.","The minimal working example gives the error, to be specific the `from sklearn.cluster import DBSCAN` statement is the exact cause of the error. ","If importing sklearn causes the issue independent of your code, I would suggest to open an issue in the sklearn GitHub repo. I still find this difficult to untangle.","I raised the issue here since the last line of the error message points to the scipy module.","> If importing sklearn causes the issue independent of your code, I would suggest to open an issue in the sklearn GitHub repo. I still find this difficult to untangle.\r\n\r\nI raised the same on sklearn forum and the response was to raise the issue on this forum. [Reference](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27756#issuecomment-1803573076).","Ok, then it looks like somehow the boost methods are broken in your scipy installation. How did you install scipy exactly? And it would help if you can test your code with scipy 1.11.","I have installed scipy using pip3 install command, it by default installs 1.10.1 version:\r\n```\r\nsuraj@suraj:~$ pip3 install scipy\r\nCollecting scipy\r\n  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\r\nRequirement already satisfied: numpy<1.27.0,>=1.19.5 in .\/.local\/lib\/python3.8\/site-packages (from scipy) (1.24.4)\r\nInstalling collected packages: scipy\r\nSuccessfully installed scipy-1.10.1\r\n\r\n```\r\nand when I try to upgrade it by following command, it does not upgrade:\r\n```\r\nsuraj@suraj:~$ pip3 install --upgrade scipy\r\nRequirement already up-to-date: scipy in .\/.local\/lib\/python3.8\/site-packages (1.10.1)\r\nRequirement already satisfied, skipping upgrade: numpy<1.27.0,>=1.19.5 in .\/.local\/lib\/python3.8\/site-packages (from scipy) (1.24.4)\r\n```","scipy 1.11 requires Python 3.9+, that's why you cannot install a newer version in Python 3.8: https:\/\/scipy.github.io\/devdocs\/release\/1.11.0-notes.html. Should have seen it earlier. Please test your code with a newer Python\/scipy version and let us know the outcome :)."],"labels":["defect","scipy.stats"]},{"title":"ENH: stats.lmoment: add function to calculate sample L-moments","body":"#### Reference issue\r\ngh-19460\r\n\r\n#### What does this implement\/fix?\r\nThe idea of fitting distributions to data with method of L-moments has been tossed around for a while, and gh-19460 brought it back up again. This PR adds a function to compute sample L-moments, which is an essential step of the method of L-moments.\r\n\r\n#### Additional information\r\nNo rush here; I wrote the essential part of the code in https:\/\/github.com\/scipy\/scipy\/issues\/19460#issuecomment-1793547120 so I figured I might as well submit a PR.\r\n\r\nThis needs an email to the mailing list before merge. ([Done](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/N6FZYLSHV6Y3OLJV5KKKNVERAZPLWRPO\/) 3\/11\/2024.)\r\n\r\nI can see arguments for the name being `l_moment` instead of `lmoment`. If there is strong support for changing it, that's OK with me.","comments":["I vote for the name `lmoment`.\r\n\r\nThe argument `order` should not have a default value. Explicitly supplying the order makes the code more readable while adding no burden to the writer. And yes, the `moment` argument of the `moment` function should read `order`; but that would be a breaking change.\r\n\r\nThe `axis` argument should have default value `None`, but I guess we\u2019ve passed the point of no return as the `moment` function has `axis=0` by default. `None` would have been a least-surprise choice, and in C layout working on the 0th axis has the worst performance in terms of memory locality.\r\n\r\nThe `sorted` argument is very useful!\r\n\r\nThe `standardize` argument should not be there, and in particular should not default to `True`, because higher order L-moment over second L-moment is analogous to skew and kurtosis of conventional moment, but skew and kurtosis are not \u201cstandardized\u201d moments and in particular they are not the concern of the `moment` function.","Btw I come across this Python package [lmo](https:\/\/github.com\/jorenham\/Lmo) (BSD license) which deals with L-moments and which appears to be well written. Could be used as a reference, or better, invite the author to incorporate some of the lower-level stuff into SciPy.","> The argument `order` should not have a default value.\r\n\r\nThe is a justifiable perspective. Please keep in mind that the choice is subjective; other qualified people have thought about the same thing and come to a different conclusion. `stats.moment` has a default value, so I included it, but I don't care too strongly either way as long as we realize it is not a matter of \"right\" and \"wrong\".\r\n\r\n> And yes, the moment argument of the moment function should read order; but that would be a breaking change.\r\n\r\nThis can easily be changed in a backward compatible way with [`_rename_parameter`](https:\/\/github.com\/scipy\/scipy\/blob\/bd42f896aa9bb5cad58e8de4d73475c0c6f62d71\/scipy\/_lib\/_util.py#L744C5-L744C22), and I'd be happy to do so if there is some support for it.\r\n\r\n> The `axis` argument should have default value `None`, but I guess we\u2019ve passed the point of no return as the `moment` function has `axis=0` by default. `None` would have been a least-surprise choice, and in C layout working on the 0th axis has the worst performance in terms of memory locality.\r\n\r\nBefore breaking changes are allowed by a SciPy 2.0 release, `scipy.stats` as a whole has passed the point of no return in the choice of `axis=0`. Almost every reducing function has `axis=0` as the default, so this does, too.  Again, I would caution against the use of \"should\"; this is a matter of opinion. I've written about this a few times, but here are the main reasons for each possibility:\r\n- `axis=0`: most convenient for users who pass `pandas` DataFrames\r\n- `axis=None`: same as NumPy\r\n- `axis=-1`: like `axis=0`, assumes slices are independent, but faster for C arrays. Preferred internally for more natural expression of vectorized code.\r\n\r\nI've advocated for `axis=-1` in the past, but that's beyond the scope of this PR.\r\n\r\n> The `sorted` argument is very useful!\r\n\r\nGreat!\r\n\r\n> The `standardize` argument should not be there, and in particular should not default to `True`, because higher order L-moment over second L-moment is analogous to skew and kurtosis of conventional moment, but skew and kurtosis are not \u201cstandardized\u201d moments and in particular they are not the concern of the `moment` function.\r\n\r\nSkewness and kurtosis *are* known as \"[standardized moments](https:\/\/en.wikipedia.org\/wiki\/Standardized_moment)\".\r\nHigher order L-moment ratios *are* also known as \"[standardized L-moments](https:\/\/en.wikipedia.org\/wiki\/L-moment)\".\r\nThis may or may not change your opinion on whether the `standardize` argument should be included, but did you have other rationale? My reason for including it and for setting the default to true was that in my limited research, higher-order L-moments seemed more common to report as L-moment ratios. For instance, R's [`lmom`](https:\/\/cran.r-project.org\/web\/packages\/lmom\/index.html) package - written by Hosking, who introduced the [theory of L-moments](https:\/\/scholar.google.com\/scholar?hl=en&as_sdt=0%2C5&q=l-moments&btnG=&oq=l) - has `ratios` default to True for its `samlu` function. \r\n\r\n> Could be used as a reference, or better, invite the author to incorporate some of the lower-level stuff into SciPy.\r\n\r\nPlease elaborate on the motivation for this comment, since this is already written, and I've used R's `lmom` to generate reference values for testing. Is there functionality that `lmo` offers that this does not, or does this code not meet your standards? \r\n\r\n@fancidev others may not care as much, but I would appreciate it if you would phrase your opinions as such. Thanks!","Thanks for the reply @mdhaber !\r\n\r\nThe `_rename_parameter` thing sounds cool! The reason why I think there should not be a default `order` is because one would say \u201cthe sample mean is xxx\u201d but would not say \u201cthe sample moment is xxx\u201d \u2014 which moment? Therefore the signature of the function should reflect this.\r\n\r\nFor `axis`, pandas defaults to 0 because it is natural for panel data, where the columns often mean very different things (e.g. age, weight, height) so it doesn\u2019t make sense to average them. Consistent with this, pandas stores its data by columns. Since numpy copes with both Fortran and C conventions, `None` looks a reasonable choice. I also find None the least surprising for multidimensional array of data of the same nature.\r\n\r\nThanks for pointing out the term of \u201cstandardized moments\u201d, I haven\u2019t heard of them before. I mentioned Python `lmom` because I find it well written from the looking (though I have not verified its calculations). I find your code good enough for computing the sample L-moment. But in case you want to add more L-moment related functionalities, such as L-moment estimator or more sophisticated statistics, I guess that package could save you much time from reinventing the wheel :-)\r\n\r\nHosking\u2019s R function `samlu` by default returns the 1st and 2nd L-moment together with the 3rd and 4th L-ratio. It may well suit R users\u2019 expectation, but I\u2019m not sure it would be a good choice for scipy as I see scipy more toward providing lower-level building blocks that are easily reusable and composable. Higher level \u201coff-the-shelf\u201d solutions can leave to say statsmodels. (My experience with statsmodels is that when it provides the functionality it\u2019s really nice, but if you want something extra then it\u2019s hard to reuse the existing code and I sometimes have to work out from scratch. Hence my desire for \u201clower-level building blocks\u201d.)"],"labels":["scipy.stats","enhancement"]},{"title":"BUG: zoom ignores spline order and only does nearest-neighbor interpolation when downscaling","body":"### Describe your issue.\n\nscipy.ndimage.zoom seems to ignore the order parameter when downscaling (i.e. when zoom<1) and uses nearest-neighbor interpolation (order=0) instead.\r\n\r\nIs this behavior expected? I would assume linear interpolation (order=1) should be the default when downscaling because nearest-neighbor can produce strange and unnatural results when the downscaling factor is <0.25, i.e. the nearest neighborhood is 4x4 or or larger.\r\n\r\nskimage resize produces the expected output when anti_aliasing is set to True. Perhaps zoom should produce something similar by default? At the very least if order is ignored there should be a runtime warning and an explanation in the docs.\n\n### Reproducing Code Example\n\n```python\nimport scipy\r\nimport matplotlib.pyplot as plt\r\nfrom skimage.transform import resize\r\n\r\nimg1 = scipy.misc.ascent()\r\nimg2 = resize(img1, (128, 128), anti_aliasing=False)\r\nimg3 = resize(img1, (128, 128), anti_aliasing=True)\r\nimg4 = scipy.ndimage.zoom(img1, 0.25,)\r\nimg5 = scipy.ndimage.zoom(img1, 0.25, order=0)\r\nimg6 = scipy.ndimage.zoom(img1, 0.25, order=1)\r\n\r\nplt.subplot(2, 3, 1)\r\nplt.imshow(img1, cmap='gray')\r\nplt.title('Original 512x512')\r\n\r\nplt.subplot(2, 3, 2)\r\nplt.imshow(img2, cmap='gray')\r\nplt.title('skimage resize, no AA, 128x128')\r\n\r\nplt.subplot(2, 3, 3)\r\nplt.imshow(img3, cmap='gray')\r\nplt.title('skimage resize, with AA, 128x128')\r\n\r\nplt.subplot(2, 3, 4)\r\nplt.imshow(img4, cmap='gray')\r\nplt.title('zoom, default order, 128x128')\r\n\r\nplt.subplot(2, 3, 5)\r\nplt.imshow(img5, cmap='gray')\r\nplt.title('zoom, order=0, 128x128')\r\n\r\nplt.subplot(2, 3, 6)\r\nplt.imshow(img6, cmap='gray')\r\nplt.title('zoom, order=1, 128x128')\r\n\r\nplt.show()\n```\n\n\n### Error message\n\n```shell\nNone\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.9.3 1.23.4 sys.version_info(major=3, minor=10, micro=2, releaselevel='final', serial=0)\r\nopenblas64__info:\r\n    library_dirs = ['D:\\\\a\\\\1\\\\s\\\\numpy\\\\build\\\\openblas64__info']\r\n    libraries = ['openblas64__info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\r\nblas_ilp64_opt_info:\r\n    library_dirs = ['D:\\\\a\\\\1\\\\s\\\\numpy\\\\build\\\\openblas64__info']\r\n    libraries = ['openblas64__info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\r\nopenblas64__lapack_info:\r\n    library_dirs = ['D:\\\\a\\\\1\\\\s\\\\numpy\\\\build\\\\openblas64__lapack_info']\r\n    libraries = ['openblas64__lapack_info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\r\nlapack_ilp64_opt_info:\r\n    library_dirs = ['D:\\\\a\\\\1\\\\s\\\\numpy\\\\build\\\\openblas64__lapack_info']\r\n    libraries = ['openblas64__lapack_info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\r\n    not found = AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\n```\n","comments":[],"labels":["defect","scipy.ndimage"]},{"title":"ENH: special: add ufunc for logarithm of the binomial coefficient","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nIn many discrete distributions the logarithm of the binomial coefficient is used in the logpmf calculation. As the binomial coefficient is fundamentally important elsewhere as well, I think it is worth adding a dedicated function for that in `special`. The binomial coefficient also quickly overflows while its logarithm does not.\r\n\r\n### Describe the solution you'd like.\r\n\r\nA function `special.logbinom(x, y)`.\r\n\r\nCC @steppi , as you are working on `binom` in #19471 , do you have a strong opinion here by any chance?","comments":["Hey, If we're implementing this, I have a small implementation here: \r\n```\r\ncimport cython\r\nfrom cython.parallel import prange\r\n\r\ncdef extern from \"math.h\":\r\n    double log(double x)\r\n\r\n'''\r\nUse Stirling's approximation if approximate = True\r\ni.e log(n!) = n*log(n) - n\r\n'''\r\n\r\ncdef double _factorial(double x) nogil:\r\n    cdef double i = 1\r\n    cdef double _fact = 1\r\n    for i in prange(1, x + 1):\r\n        _fact *= i\r\n    return _fact\r\n\r\ncdef double _stirlings_approximation(double x):\r\n    return x * log(x) - x\r\n\r\n@cython.ufunc\r\ncdef double _logbinomial(double x, double y, bint approximate=True):\r\n    cdef double result\r\n    if approximate:\r\n        result = _stirlings_approximation(x) - _stirlings_approximation(y) - _stirlings_approximation(x - y)\r\n    else:\r\n        result = log(_factorial(x)) \/ log((_factorial(y) * _factorial(x - y)))\r\n    return result\r\n```","Using Stirling's isn't a great approach. I think for the general case you'd want to rely on\r\n\r\n$${n \\choose k} = \\frac{1}{(n+1)\\mathrm{B}(n - k + 1, k + 1)}$$\r\n\r\nand use `betaln`.","@dschmitz89, I have no objections to adding this; it's just a matter of bandwidth for someone to implement it properly and someone else to review.\r\n\r\n@aadya940, thanks for you interest, but beyond your use of Stirling's approximation, it looks like you're just computing integer factorials, which won't work for double `x` and `y` in general, leads to a lot of duplicate work in this case for the non-negative integer cases it can handle, and will cause intermediate overflow. You also seem to be using `prange` for a calculation that is inherently sequential. If you're interested in learning more about numerical computation of special functions I can recommend books or other learning materials.","@dschmitz89, if you want to submit a PR; the way to go would be to do `log(binom)` for the integer case from the `binom` source code, `if (k == kx && (std::fabs(n) > 1E-8 || n == 0)) {`, and otherwise do `-cephes::lbeta(1 + n - k, 1 + k) - std::log(n + 1)`. You could just add your implementation to `_binom.h` from https:\/\/github.com\/scipy\/scipy\/pull\/19471, once that PR gets in. But it would have to go through the mailing list first. ","Hey @steppi , I would love to read more about this topic. Do recommend some material I can read. I'll also rewrite my code based on the Points you mentioned! Thanks!","Just a word of caution, we cannot and actually do not use Numerical Recipes because the license is not compatible for distributing code under open source licenses. Even looking at the code is debatable in this context for inspiration hence overall we completely avoid NR. \r\n\r\nAnd I think that link leads to an unofficial copy of the book","@aadya940, I was actually planning to warn you not to read numerical recipes. Beyond the unusual and restrictive licensing, the code quality isn't great either, though the explanations are good. (I've heard the original Fortran edition did feature idiomatic Fortran which might have been considered good for its time.)\r\n\r\nIf you can get a hold of a copy, Nico M. Temme's [Special Functions: An Introduction to the Classical Functions of Mathematical Physics](https:\/\/onlinelibrary.wiley.com\/doi\/book\/10.1002\/9781118032572 ), is excellent, but will require some familiarity with calculus and undergraduate level complex analysis. You might want to take or sit in on an intro complex analysis course at your university if you don't have experience with the subject.\r\n\r\nI've only skimmed through them in the past, but these [freely available lecture notes](http:\/\/www.physics.wm.edu\/~finn\/home\/MathPhysics) by John Michael Finn seem good, and are more gentle since they walk through a lot of the prerequisite material Temme assumes you already know. You won't get the same level of depth here though. Your goal should be to build yourself up to being able to grapple with  Temme.\r\n\r\nIf you haven't already, it would be good to become familiar with the content of the classic article  [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https:\/\/docs.oracle.com\/cd\/E19957-01\/806-3568\/ncg_goldberg.html). The [Boost math documentation](https:\/\/www.boost.org\/doc\/libs\/1_83_0\/libs\/math\/doc\/html\/index.html) is also a great resource.\r\n\r\nIf you want to contribute to SciPy, make sure you check the license before studying any relevant source code. Oddball things like NR or GPL'd things like the GNU Scientific Library or most R source code are out of bounds.\r\n\r\n","Thanks for your suggestions @steppi . Might be a good opportunity to dig into C++ land again for me. But I am rather swamped at the moment so if anyone has time before, by all means write to the mailing list and submit a PR :).","> @aadya940, I was actually planning to warn you not to read numerical recipes. Beyond the unusual and restrictive licensing, the code quality isn't great either, though the explanations are good. (I've heard the original Fortran edition did feature idiomatic Fortran which might have been considered good for its time.)\r\n> \r\n> If you can get a hold of a copy, Nico M. Temme's [Special Functions: An Introduction to the Classical Functions of Mathematical Physics](https:\/\/onlinelibrary.wiley.com\/doi\/book\/10.1002\/9781118032572), is excellent, but will require some familiarity with calculus and undergraduate level complex analysis. You might want to take or sit in on an intro complex analysis course at your university if you don't have experience with the subject.\r\n> \r\n> I've only skimmed through them in the past, but these [freely available lecture notes](http:\/\/www.physics.wm.edu\/~finn\/home\/MathPhysics) by John Michael Finn seem good, and are more gentle since they walk through a lot of the prerequisite material Temme assumes you already know. You won't get the same level of depth here though. Your goal should be to build yourself up to being able to grapple with Temme.\r\n> \r\n> If you haven't already, it would be good to become familiar with the content of the classic article [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https:\/\/docs.oracle.com\/cd\/E19957-01\/806-3568\/ncg_goldberg.html). The [Boost math documentation](https:\/\/www.boost.org\/doc\/libs\/1_83_0\/libs\/math\/doc\/html\/index.html) is also a great resource.\r\n> \r\n> If you want to contribute to SciPy, make sure you check the license before studying any relevant source code. Oddball things like NR or GPL'd things like the GNU Scientific Library or most R source code are out of bounds.\r\n\r\nThank you so much @steppi, I skimmed through the material and It looks really interesting, I'll try to complete it as soon as possible. Also, added Complex Analysis for the next sem. Hoping to contribute something good to SciPy soon, Thanks :-)","Sounds good @aadya940. Complex Analysis is a really beautiful subject. Since the complex derivative is required to be the same along all directions, complex differentiable functions have really nice structure and properties. Looking forward to seeing your first contribution. ","> Sounds good @aadya940. Complex Analysis is a really beautiful subject. Since the complex derivative is required to be the same along all directions, complex differentiable functions have really nice structure and properties. Looking forward to seeing your first contribution.\r\n\r\nThank You @steppi "],"labels":["enhancement","scipy.special"]},{"title":"BUG: wrong output of the stats.beta.fit() function","body":"### Describe your issue.\n\ni got a problem with the scipy fit error in a case where it should not fail as far as i am concerned.\r\n\r\nsee code.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.stats import beta\r\nfrom numpy.random import default_rng\r\nfrom tqdm import tqdm\r\n\r\ndef reproduce_scipy_mode_estimation_error(\r\n        peak1=0.25,\r\n        temp1=0.05,\r\n        peak2=0.07,\r\n        temp2=7.21,\r\n        peak3=0.01,\r\n        temp3=86.57,\r\n        seed=362,\r\n):\r\n    \"\"\"\r\n    This function has been created to reproduce an error of the scipy.fit()\r\n    function.\r\n\r\n    Steps:\r\n        -First, this function draw a sample of size 10000 for 3 different modified\r\n        PERT distributions (with their respective peak and temperature).\r\n\r\n        -Then processes the 3 samples together (ad hoc process, it could be\r\n        something else I guess). This processed data set is the data of\r\n        interest.\r\n\r\n        -Finally, it infers a suitable modified PERT distribution to model\r\n        the processed data.\r\n\r\n    This function gets to estimate:\r\n        low_inferred = 0\r\n        mod_inferred = 1.0252528246011596\r\n        max_inferred = 1\r\n        temp_inferred = -0.002286433872501714\r\n\r\n    while I set min and max to be 0.0 and 1.0 .\r\n    Moreover, lambda must be strictly positive\r\n    \"\"\"\r\n    rand_gen = default_rng(seed)\r\n\r\n    def convert_PERT_param_into_Beta_param(low, peak, high, temperature):\r\n        _loc = low\r\n        _scale = high - low\r\n        _alpha = 1 + temperature * (peak - low) \/ (high - low)\r\n        _beta = 1 + temperature * (high - peak) \/ (high - low)\r\n        return _alpha, _beta, _loc, _scale\r\n\r\n    def convert_Beta_param_into_PERT_param(_alpha, _beta, _loc, _scale):\r\n        _low = _loc\r\n        _peak = _loc + (_alpha - 1) \/ (_alpha + _beta - 2) * _scale\r\n        _high =_loc + _scale\r\n        _scale = _alpha + _beta - 2\r\n        return _low, _peak, _high, _scale\r\n\r\n    # insure the conversion is right\r\n    test_param = [1.5, 3.1, 0., 1.]\r\n    output_test = convert_PERT_param_into_Beta_param(*convert_Beta_param_into_PERT_param(*test_param))\r\n    for elm1, elm2 in zip(test_param, output_test):\r\n        print(elm1, elm2)\r\n        assert round(elm1, 9) == round(elm2, 9)\r\n\r\n\r\n    def draw_a_sample(_alpha, _beta, _loc, _scale, np_random_generator):\r\n        return _loc + _scale * np_random_generator.beta(_alpha, _beta, size=10000)\r\n\r\n    # draw samples and get processed data\r\n    sample_1 = draw_a_sample(*convert_PERT_param_into_Beta_param(0, peak1, 1, temp1), rand_gen)\r\n    sample_2 = draw_a_sample(*convert_PERT_param_into_Beta_param(0, peak2, 1, temp2), rand_gen)\r\n    sample_3 = draw_a_sample(*convert_PERT_param_into_Beta_param(0, peak3, 1, temp3), rand_gen)\r\n\r\n    data = 1 - (1 - sample_1) * (1 - sample_2 * sample_3)\r\n\r\n    # inference among the beta distribution family\r\n    floc = 0\r\n    fscale = 1\r\n    alpha_inferred, beta_inferred, loc_inferred, scale_inferred = beta.fit(data, floc=floc, fscale=fscale, method=\"mm\")\r\n\r\n    # convert into PERT param\r\n    low_inferred, mod_inferred, max_inferred, temp_inferred = convert_Beta_param_into_PERT_param(\r\n        alpha_inferred, beta_inferred, loc_inferred, scale_inferred\r\n    )\r\n\r\n    # print(low_inferred, mod_inferred, max_inferred, temp_inferred)\r\n    if max_inferred <= mod_inferred:\r\n        print(low_inferred, mod_inferred, max_inferred, temp_inferred)\r\n        print(\r\n            peak1,\r\n            temp1,\r\n            peak2,\r\n            temp2,\r\n            peak3,\r\n            temp3,\r\n            seed,\r\n        )\r\n        raise Exception()\r\n\r\n    return low_inferred, mod_inferred, max_inferred, temp_inferred\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    reproduce_scipy_mode_estimation_error()\n```\n\n\n### Error message\n\n```shell\nno error message, only a wrong output\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nscipy==1.11.3\r\nnumpy==1.24.3\r\npython 3.10.11\r\nwindows \r\n\r\nresult of command:\r\n$ python\r\nPython 3.10.11 (tags\/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n1.11.3 1.24.3 sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-qjiwhfk5\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-8hexkyrv\\cp310-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.10'\n```\n","comments":["Please provide a minimal reproducible example (e.g. just the data and the call to the `fit` method). Please provide the output of the `fit` method and explain why it is different from what you expect.","I understand through your answer that i may have been not clear enough. Sorry for that, i spent a lot of time to make a clear and reproductible code to help you out... that's why i was brief in the other sections.\r\n\r\nin case it was not clear to you, the data are provided because the code provided generate them, with the seed, etc... it should reproduce the same behaviour on your side. if  not, let me know and i'll try to help you reproduce it another way.\r\n\r\ni've written the expected output in the docstring of the function:\r\n**expected output**: a peak value (or mode value) between min and max (between low and max, it is the same), and temperature value above 0.\r\n\r\ni've also written the output of the fit method (in terms of PERT parameters) in the docstring:\r\n**output**:\r\nlow_inferred = 0\r\nmod_inferred = 1.0252528246011596\r\nmax_inferred = 1\r\ntemp_inferred = -0.002286433872501714\r\n\r\nIf the modified PERT distribution is not a distribution you are familiar with, it is simply an other possible parametrization of the beta distribution in the special case where alpha>1, beta>1. then instead of min, max, alpha and beta, the parametrization is min, mod, max, temperature with this translation:\r\nmin = min\r\nmod = (alpha-1)\/(alpha+beta-2)\r\nmax = max\r\ntemperature = alpha+beta-2\r\nCheck this out for furthur info: https:\/\/en.wikipedia.org\/wiki\/PERT_distribution\r\nmy code provides the two function to convert the parameters from one convetion to the other one.\r\n\r\nThere is one thing i have not provided yet, it is the screen shot of my debugger:\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/147074835\/36179554-a891-4fa0-bffc-07ec16a4ac8a)\r\n\r\nThis concerns the function _fit_loc_scale_support line 2657 in file scipy\\stats\\_distn_infrastructure.py\r\n\r\nYou can see at line 2726 on the screenshot of my debug session that a,b = 0.,1.. **That is correct.** These values are respectively floc and fscale values i ve passed in argument of the fit() method.\r\n\r\nAt line 2741, the condition is false because actually a_hat > data_a. That is ok to be so here.\r\n\r\nAs a consequence the execution runs further until line 2752 and 2753 where loc_hat and scale_hat are set unconsistently with the arguments floc and fscale i've provided. **That's a problem.** loc_hat happens to be negative, As far as i understand what the method should do, loc_hat should be greater than floc (greater than the \"a\" that appears at line 2726)\r\n\r\nI think the problem of the fit function is directly induced by the wrong value taken by loc_hat at line 2752.\r\n\r\n**possible solution**\r\nloc_hat and scale_hat should not take value unconsistent with floc and fscale passed as argument of .fit() because i assume (i've not checked that out) that loc_scale is used afterwards to estimate with a max-log-likelihood the parameters of the beta. Since loc_hat is negative here, it is possible for the max-log-likelihood step finds parameters that lead to a distribution with negative mode (=most likely value = peak value (synonyms here)). The mode is precisely the wrong \"mod_inferred\" value in this execution.\r\n\r\nI hope that helps you to understand the issue i've posted. Thank you for answering so quickly!","The report seems to be about `scipy.stats.beta.fit`. Since that is the only SciPy function called by your code, all we need to triage this issue are the inputs and outputs of `scipy.stats.beta.fit`. To assess whether there is a bug, we will see whether the moments of the fitted `beta` distribution match the moments of the data, which is all `stats.beta.fit` with `method='mm'` can claim to do. For quicker resolution, it would help if you could trim away the information that is not essential (e.g. information about the PERT distribution, which is your own code; we can't debug that). Thanks!","I went ahead and dug out what was needed:\r\n\r\n```python3\r\n# I created a list `data_global` and added this line to your code\r\n# before the call to `beta.fit`\r\n# data_global.append((data, floc, fscale))\r\n\r\nfrom scipy import stats\r\nimport numpy as np\r\n\r\n# get the data `fit` was called with\r\ndata, floc, fscale = data_global[0]\r\n# isolate the call to `fit` so I can investigate\r\nparams = beta.fit(data, floc=floc, fscale=fscale, method=\"mm\")\r\n# freeze a distribution with the fitted parameter values\r\ndist = stats.beta(*params)\r\n\r\ni = np.arange(1, 5)[:, np.newaxis]\r\nmoments = np.mean(data**i, axis=1)\r\n# raw moments of the data\r\nmoments  # array([0.49939921, 0.33279701, 0.24984718, 0.20017233])\r\n\r\n# moments of the fitted distribution\r\nfor j in i:\r\n  print(dist.moment(j))\r\n# 0.49939883481287894\r\n# 0.3327959694816871\r\n# 0.24954458609611152\r\n# 0.19960995274281085\r\n```\r\nThe moments of the data match the moments of the fitted distribution pretty closely, so it looks like `beta.fit` is working as advertised.","Well, i agree with you to say that this seems to prove that beta.fit() works well.\r\n \r\nI reran my code and the output of the beta.fit() is `alpha, beta, loc, scale = 0.9976558272139538, 1.0000577389135445, 0, 1`. One could indeed say that it is good enough and since alpha<1 it is my mistake to try to convert that into a PERT distribution. I should not because alpha does not satisfy the constraints over the parameters for the conversion to be valid.\r\n\r\nThe issue i am reporting is not just about the moments (that are fine enough), it is about the way the bounds of the support are computed, and along the way inconsistent computations are done. I've done my best to show how and why. So, i rather disagree to accept that this is fully fine but i'll respect your decision.\r\n\r\n**Food for thoughts:**\r\nIn the problem i am trying to solve with scipy at the moment, i need to stick to beta distributions with a mode value strictly between 0 and 1. Those are called PERT distribution but whatever. Scipy is currently unable to stick to this setup even when I specify it manually with floc=0 and fscale=1. It may be fine to keep the code as it is and enable the user to add constraints about the location of the mode value (solution 1). It could be also possible to implement the PERT distributions as stand alone distribution (Those are pretty useful in industrial context) (solution 2). Or maybe the fix I pinpointed in the _fit_loc_scale_support() function could also do the job (solution 3).\r\n\r\n**I think this is worth to be improved in later versions but you are the one that work hard on scipy so I will abide by your opinion. Thank for your work and your time.**","> loc_hat and scale_hat should not take value unconsistent with floc and fscale passed as argument of .fit() because i assume **(i've not checked that out)** that loc_scale is used afterwards\r\n\r\nPlease take a look and let us know what you find.\r\n\r\n>  to estimate with a max-log-likelihood the parameters of the beta. \r\n\r\nNot in the example posted at the top. That code is using `method='mm'` (method of moments), not maximum likelihood estimation.\r\n\r\nIf there is still an issue after looking at the code, please provide an example in which the output of the code is incorrect as a result (or please adjust the title of the issue).","I've taken some time to understand further what was going on. Spoiler: _i think finally scipy is ok!_\r\n\r\n**Theoretically speaking**:\r\nI've taken a closer look to the theoretical problem I was trying to solve. The most likely value was indeed strictly between 0 and 1 (i.e. a beta distribution with parameters a>1 and b>1) but the distribution was very close to a uniform distribution.\r\n\r\n**dependence on method used**:\r\nboth methods \"mm\" and \"mle\" lead to the problem I reported. But I had a code of mine to perform bayesian inference for Beta distribution that coincide exactly with the \"mle\" conclusions (both failed or none did).\r\n\r\n**scipy _fit_loc_scale_support function role**:\r\nI checked further in the code of scipy and finally, even if there was apparently \"inconsistent\" computations inside _fit_loc_scale_support, the way its output is further used make things go right again.\r\n\r\n**conclusion**:\r\nMy conclusion on what happened now incriminates the statistical fluctuations. The histogramme of the distribution at stake sampled with only 10000 values could sometime by chance lead scipy.beta.fit() estimate a<1 or b<1 even if the theoretical distribution did not satisfy these inequalities.\r\nSo, among the 3 solutions I've proposed only the 1st and the 2nd remain relevant. (i.e. enable user to add constraints on the parameters to fit or add PERT distribution to the available distributions in scipy.)\r\n\r\nThank you for your time!","(1) is already possible using [`scipy.stats.fit`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.fit.html). (2) is already resolved in the sense that there has been a long-standing policy not to add alternate parameterizations of existing distributions. The PERT parameterization of beta, in particular, has been discussed a lot on the [mailing list](https:\/\/mail.python.org\/archives\/search?list=scipy-dev%40python.org&q=Pert) already. That said, the policy will probably be reconsidered when https:\/\/github.com\/scipy\/scipy\/issues\/15928 is resolved, and that work is underway (with considerable progress).","Perfect.\r\n\r\ninteresting! I was so focused on beta.fit() thinking it was the same as scipy.stats.fit() that i missed the interesting possibilities offered by scipy.stats.fit(). top. thank you."],"labels":["scipy.stats","enhancement"]},{"title":"Vet SciPy for use of `long` as the default integer","body":"We just merged https:\/\/github.com\/numpy\/numpy\/pull\/24224 changing the default integer in NumPy.  This changes the definition of `np.int_` to intp, which only matters on windows, though.\r\n\r\nThere are three things that might be good to ensure here:\r\n1. In some cases `np.int_` is probably used as long, and needs to be `np.dtype(\"long\").type` (compatible version.\r\n2. I think I saw a cython use `np.int_t`, which is ill defined unfortunately. It needs to be either `npy_long` (because it is hard to create a compatible name).  Or `intp` (with potential casting if required).  Worst case, you need to support both, which would be annoying I admit (fused type).\r\n3. It probably doesn't matter much, but I think many special functions are defined with long loops.  In principle it may make sense to make them int or int64 loops to ensure 64bit integers can match them.\r\n\r\nIt is very likely that this will break something on windows at this moment, the Cython issue should break compilation.","comments":["Some of this is dealt with in gh-19466, now merged, but there are a few broader points above so leaving this open for now."],"labels":["maintenance"]},{"title":"BUG: optimize.least_squares giving poor result compared to optimize.leastsq and optimize.curve_fit","body":"### Describe your issue.\r\n\r\nI am seeing some poor results when using `optimize.least_squares` to fit a 1-D function.  The fitting result is highly dependent on the initial guess.  I understand these issues can be easily caused by a poor initial guess, but I do not believe that is what's going on here.\r\n\r\nIn the attached screenshot, the top left panel shows the poor fitting result if the peak position of the initial model is shifted slightly to the right.  The top right panel uses different parameters where the peak position is shifted slightly to the left and that gives the expected result.  I also ran tests using `optimize.leastsq` and `optimize.curve_fit` and they do not have this issue; one obtains the expected result for both initial guesses.  I also know that `optimize.curve_fit` can give a consistently good fit for different data, as I have about 100 experimental peaks in this dataset with different positions, areas, and widths and I was able to fit them all successfully with `optimize.curve_fit` (but not `optimize.least_squares`).\r\n\r\nOther things I tried, which did not work for me:\r\n\r\n- Switching the optimization algorithm to `lm` or `dogbox`\r\n- Setting `max_nfev` to a very large number (1e6)\r\n- Trying several values for `diff_step` (1e-6 or 1e-8)\r\n\r\nChanging these settings will sometimes give a different optimization result, but still not correct.\r\n\r\nI have attached a screenshot and the codes to reproduce the error as a script and in Jupyter Notebook format.  The data file is also attached.\r\n\r\nThanks for your attention to this issue!  SciPy is extremely valuable to my research and this is the first issue I have had to report in >10 years as a user.\r\n\u00a0\r\n![Screenshot 2023-11-01 094635](https:\/\/github.com\/scipy\/scipy\/assets\/1441560\/56d03a7d-e550-4e77-ac63-e3842ca3dd38)\r\n\r\n[test.ipynb.zip](https:\/\/github.com\/scipy\/scipy\/files\/13229723\/test.ipynb.zip)\r\n\r\n[R.csv.zip](https:\/\/github.com\/scipy\/scipy\/files\/13229811\/R.csv.zip)\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\n#!\/usr\/bin\/env python\r\n\r\nimport numpy as np\r\nimport scipy\r\nimport matplotlib\r\nmatplotlib.use('Agg')\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.optimize import least_squares, leastsq, curve_fit\r\n\r\n# The bug is observed with SciPy version 1.11.3\r\nimport sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n\r\n# Load the data and reshape into 1-D arrays.\r\nX_array = np.loadtxt('R.csv',skiprows=1,delimiter=',')\r\ndeltas = X_array[:,0].flatten()\r\nsignal = X_array[:,1].flatten()\r\n\r\n# Lorentzian model function. x0 = center; A = area; sigma = width\r\ndef lorentzian(x, x0, A, sigma):\r\n    return A * sigma \/ ((x-x0)**2 + sigma**2)\r\n\r\n# Function to calculate the residual.\r\ndef calc_residual(params):\r\n    model = lorentzian(deltas, params[0], params[1], params[2])\r\n    residual = (model - signal)\r\n    return residual\r\n\r\n# Two sets of initial parameters.\r\np_init0=np.array([-62.88, 6e7, 0.02])\r\np_init1=np.array([-62.9, 6e7, 0.02])\r\n\r\n#====================================================#\r\n# The unexpected result using optimize.least_squares #\r\n#====================================================#\r\n\r\nresult0 = least_squares(calc_residual, p_init0)\r\npopt0 = result0.x\r\nmodel_init0 = lorentzian(deltas, *p_init0)\r\nmodel_opt0 = lorentzian(deltas, *popt0)\r\n\r\nresult1 = least_squares(calc_residual, p_init1)\r\npopt1 = result1.x\r\nmodel_init1 = lorentzian(deltas, *p_init1)\r\nmodel_opt1 = lorentzian(deltas, *popt1)\r\n\r\nfig, (ax0, ax1) = plt.subplots(1, 2)\r\nfig.suptitle('Fitting results using optimize.least_squares')\r\nfig.set_size_inches(8, 4)\r\nax0.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax0.plot(deltas, model_opt0, linewidth=0.5, marker='x', label='Fitted Model')\r\nax0.plot(deltas, model_init0, linewidth=1, label='Initial Model')\r\nax1.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax1.plot(deltas, model_opt1, linewidth=0.5, marker='x', label='Fitted Model')\r\nax1.plot(deltas, model_init1, linewidth=1, label='Initial Model')\r\nax0.legend()\r\nfig.savefig('using_least_squares.pdf')\r\n\r\n#====================================================#\r\n#     The expected result using optimize.leastsq     #\r\n#====================================================#\r\n\r\nresult0 = leastsq(calc_residual, p_init0)\r\npopt0 = result0[0]\r\nmodel_init0 = lorentzian(deltas, *p_init0)\r\nmodel_opt0 = lorentzian(deltas, *popt0)\r\n\r\nresult1 = leastsq(calc_residual, p_init1)\r\npopt1 = result1[0]\r\nmodel_init1 = lorentzian(deltas, *p_init1)\r\nmodel_opt1 = lorentzian(deltas, *popt1)\r\n\r\nfig, (ax0, ax1) = plt.subplots(1, 2)\r\nfig.suptitle('Fitting results using optimize.leastsq')\r\nfig.set_size_inches(8, 4)\r\nax0.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax0.plot(deltas, model_opt0, linewidth=0.5, marker='x', label='Fitted Model')\r\nax0.plot(deltas, model_init0, linewidth=1, label='Initial Model')\r\nax1.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax1.plot(deltas, model_opt1, linewidth=0.5, marker='x', label='Fitted Model')\r\nax1.plot(deltas, model_init1, linewidth=1, label='Initial Model')\r\nax0.legend()\r\nfig.savefig('using_leastsq.pdf')\r\n\r\n#====================================================#\r\n#    The expected result using optimize.curve_fit    #\r\n#====================================================#\r\n\r\nresult0 = curve_fit(lorentzian, deltas, signal, p0=p_init0)\r\npopt0 = result0[0]\r\nmodel_init0 = lorentzian(deltas, *p_init0)\r\nmodel_opt0 = lorentzian(deltas, *popt0)\r\n\r\nresult1 = curve_fit(lorentzian, deltas, signal, p0=p_init1)\r\npopt1 = result1[0]\r\nmodel_init1 = lorentzian(deltas, *p_init1)\r\nmodel_opt1 = lorentzian(deltas, *popt1)\r\n\r\nfig, (ax0, ax1) = plt.subplots(1, 2)\r\nfig.suptitle('Fitting results using optimize.curve_fit')\r\nfig.set_size_inches(8, 4)\r\nax0.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax0.plot(deltas, model_opt0, linewidth=0.5, marker='x', label='Fitted Model')\r\nax0.plot(deltas, model_init0, linewidth=1, label='Initial Model')\r\nax1.plot(deltas, signal, linewidth=0.5, marker='o', label='Data')\r\nax1.plot(deltas, model_opt1, linewidth=0.5, marker='x', label='Fitted Model')\r\nax1.plot(deltas, model_init1, linewidth=1, label='Initial Model')\r\nax0.legend()\r\nfig.savefig('using_curve_fit.pdf')\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nNo error message.\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.3 1.22.3 sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/include\r\n    lib directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    pc file directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/include\r\n    lib directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK=0 NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP=0 PRESCOTT MAX_THREADS=128\r\n    pc file directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/lib\/pkgconfig\r\n    version: 0.3.21\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/include\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  c++:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/croot\/scipy_1696543286448\/_build_env\/bin\/x86_64-conda-linux-gnu-gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 11.2.0\r\n  pythran:\r\n    include directory: ..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/home\/leeping\/opt\/miniconda3\/envs\/scipytest111\/bin\/python\r\n  version: '3.9'\r\n```\r\n","comments":["Is anyone able to reproduce this issue?  Several of my students were also seeing it on their machines, so I don't think it's particular to my local machine.","I'm not sure what we could do in this situation. Do you see differences across different scipy versions for the same code? Otherwise it's not clear that there is a bug.\r\n\r\nAs you say there are a whole heap of things that could affect this, like starting parameters, or parameter scaling (the second parameter is a factor of 1e6 larger than the others). It's going to be difficult for devs to spend time looking into this specific problem unless there's clear evidence of a bug.","Thanks for the response.  I saw the same behavior in multiple SciPy versions ranging from v1.7 to v1.11.\r\n\r\nI did some more testing and found the following.  \r\n\r\nWhen using `least_squares(method='lm')` without providing variable scales via `x_scale`, an array of ones is passed as the last argument (`diag`) to `_minpack._lmdif`, which is the entry point of the MINPACK Levenberg-Marquardt algorithm.  (This is because `least_squares.py` computes `diag = 1 \/ x_scale` if `x_scale != 'jac'`).  During the second step of the optimization, the algorithm repeatedly cuts the step size until `xtol` is met, which leads to the poor quality-of-fit I described.  If `xtol` is reduced to 1e-10 then the correct fit is recovered for all algorithms.\r\n\r\nWhen using `leastsq` or `curve_fit`, the default last argument to `_minpack._lmdif` is `diag=None`.  This results in significantly improved behavior of the optimization without a large number of step size cuts.  This is presumably because `_minpack._lmdif` is applying a default scaling to the parameters, (for example using the column norms of the Jacobian).  If I provide `diag=[1, 1, 1]` to calling `leastsq`, the poor behavior of `least_squares` is reproduced.\r\n\r\nTherefore, I think the cause of the poor behavior in `least_squares` for my example was due to the default value of 1.0 for parameter scales.  However, `leastsq` and `curve_fit` applied a default parameter scaling that prevented the problem from appearing.  I think the latter behavior is more desirable and more robust.  Given that `leastsq` is already deprecated and `least_squares` is intended as a replacement, it would be a problem if a user of `leastsq` changes over to `least_squares` and gets very different results.  \r\n\r\nIn conclusion, I agree this isn't a bug per se, but it is arguably a performance regression, and I'd like to propose an enhancement where `least_squares(method='lm')` will pass `diag=None` to `_minpack._lmdif` as the default rather than an array of ones.  This is already the behavior when using `method='lm', x_scale='jac'` as the code explicitly sets `diag=None` in that case, so the enhancement could be implemented by setting the default value of `x_scale='jac'` if `method=='lm'`.  It is possible `x_scale='jac'` is also more robust for the other algorithms but I haven't tested this extensively.  \r\n\r\nIf there is agreement from the developers then we can change this issue from a bug report to an enhancement proposal.","Hi @leeping,\r\n\r\nWe are a team of students from CentraleSup\u00e9lec, we would like to work on this issue for our project for a class. We have reproduced the bug and will implement the enhancement suggested by you to compare the results. Also, analyse for any other fix possible. \r\nCan we work on this? \r\n","@nish0699 we don't assign issues, so you are free to submit a PR with a fix if you want to. Please check [our contributing guide](https:\/\/docs.scipy.org\/doc\/scipy\/dev\/index.html) and feel free to reach out if you have questions. Cheers!","I'm not a SciPy maintainer but I would love to see your class implement this enhancement! :)"],"labels":["defect","scipy.optimize"]},{"title":"ENH: Up-to 200x Faster SIMD-Accelerated Distance Functions","body":"**Introduction**:\r\nSciPy's spatial distance computations are fundamental to various scientific and data science tasks. To accelerate these computations, it is imperative to ensure that the underlying math operations are optimized for modern hardware.\r\n\r\n**Background**:\r\nCurrently, SciPy relies on NumPy for math operations, which further depends on the underlying BLAS implementation. However, these BLAS libraries might not be fully optimized for the latest hardware advancements, potentially limiting the performance.\r\n\r\n**The SimSIMD Solution**:\r\nI've developed a low-level library called [SimSIMD](https:\/\/github.com\/ashvardanian\/simsimd), which provides accelerated implementations of commonly used distance functions. Notably, SimSIMD is already in use by projects like [USearch](https:\/\/github.com\/unum-cloud\/usearch) and [ClickHouse](https:\/\/github.com\/ClickHouse\/ClickHouse), and is an optional backend in [LangChain](https:\/\/github.com\/langchain-ai\/langchain). The library boasts specialized backends for:\r\n\r\n- x86: AVX2 and AVX-512 F\/FP16\/VNNI\r\n- Arm: NEON and SVE\r\n\r\nThese cover most CPUs produced in the past decade, offering potential speed-ups.\r\n\r\n**Evidence**:\r\n- [SciPy distances... up to 200x faster with AVX-512 & SVE](https:\/\/ashvardanian.com\/posts\/simsimd-faster-scipy\/), which includes benchmarks on:\r\n  - [Apple M2](https:\/\/ashvardanian.com\/posts\/simsimd-faster-scipy\/#appendix-1-performance-on-apple-m2-pro)\r\n  - [Intel Sapphire Rapids](https:\/\/ashvardanian.com\/posts\/simsimd-faster-scipy\/#appendix-2-performance-on-4th-gen-intel-xeon-platinum-8480)\r\n  - [AWS Graviton 3](https:\/\/ashvardanian.com\/posts\/simsimd-faster-scipy\/#appendix-3-performance-on-aws-graviton-3)\r\n- [Python, C, Assembly - 2'500x Faster Cosine Similarity](https:\/\/ashvardanian.com\/posts\/python-c-assembly-comparison\/) about using masked loads in AVX-512 in conjunction with BMI2.\r\n- [GCC 12 is 119x slower than AVX-512 SIMD on this task](https:\/\/ashvardanian.com\/posts\/gcc-12-vs-avx512fp16\/) about logarithm approximation and reciprocals for faster divergence computations.\r\n\r\nWhile SimSIMD is not a complete replacement for SciPy's API, it focuses on the most used distance functions. Further functions can be incorporated based on the community's feedback.\r\n\r\n**Proposal**:\r\nGiven the potential benefits, I propose to consider integrating SimSIMD as an optional backend in SciPy, as suggested on the [SciPy Slack](https:\/\/scipy-community.slack.com\/archives\/C03JL2YGCNR\/p1696871432847549). This would offer users an optimized pathway for spatial distance computations on modern hardware platforms.","comments":["Sounds exciting! Which SciPy version did you use in your benchmarks? Many of SciPy's distance metrics were reimplemented in C++ recently (version 1.11.0).","Thanks for this proposal @ashvardanian, this is quite interesting. An optional dependency indeed seems like a good idea to me, since there's a lot of interest in making these distance functions faster (also Cc @Micky774 who worked on accelerating distance functions in scikit-learn).\r\n\r\nMy main question right now is about dtype support - from the README it seems that `float64` is not supported? That's by far the most heavily used dtype, so I'm wondering how you are looking at that.","@rgommers, valid point! I wasn't originally expecting any gains for double-precision functions, but turns out they are quite significant. @dschmitz89, I compare to the most recent version available at the time, which is now 1.11.4. I'm attaching the outcomes of my [last commit](https:\/\/github.com\/ashvardanian\/SimSIMD\/commit\/e5175b442e637e85dd8d89a48da42fd44a918e5d), benchmarked on the Sapphire Rapids CPU.\r\n\r\n## Benchmarking SimSIMD vs. SciPy\r\n\r\n- Vector dimensions: 1536\r\n- Vectors count: 1000\r\n- Hardware capabilities: serial, x86_avx2, x86_avx512, x86_avx2fp16, x86_avx512fp16, x86_avx512vpopcntdq, x86_avx512vnni\r\n- NumPy BLAS dependency: openblas64\r\n- NumPy LAPACK dependency: dep140640983012528\r\n\r\n### Between 2 Vectors, Batch Size: 1\r\n\r\n| Datatype | Method                |                Ops\/s |        SimSIMD Ops\/s | SimSIMD Improvement |\r\n| :------- | :-------------------- | -------------------: | -------------------: | ------------------: |\r\n| `f64`    | `scipy.cosine`        |               63,612 |              572,605 |              9.00 x |\r\n| `f64`    | `scipy.sqeuclidean`   |              238,547 |              915,596 |              3.84 x |\r\n| `f64`    | `numpy.inner`         |              449,499 |              986,522 |              2.19 x |\r\n\r\n### Between 2 Vectors, Batch Size: 1,000\r\n\r\n| Datatype | Method                |                Ops\/s |        SimSIMD Ops\/s | SimSIMD Improvement |\r\n| :------- | :-------------------- | -------------------: | -------------------: | ------------------: |\r\n| `f64`    | `scipy.cosine`        |               68,962 |            1,457,172 |             21.13 x |\r\n| `f64`    | `scipy.sqeuclidean`   |              247,727 |            1,535,547 |              6.20 x |\r\n| `f64`    | `numpy.inner`         |              463,509 |            1,512,004 |              3.26 x |\r\n\r\n","[Porting to SVE-powered Graviton 3 chips](https:\/\/github.com\/ashvardanian\/SimSIMD\/pull\/41\/commits\/8b7f2acf757bbfabbb720791b973bd49e06f0dda) also yields good results.\r\n\r\n## Benchmarking SimSIMD vs. SciPy\r\n\r\n- Vector dimensions: 1536\r\n- Vectors count: 1000\r\n- Hardware capabilities: serial, arm_neon, arm_sve\r\n- NumPy BLAS dependency: openblas64\r\n- NumPy LAPACK dependency: openblas64\r\n\r\n### Between 2 Vectors, Batch Size: 1\r\n\r\n| Datatype | Method                |                Ops\/s |        SimSIMD Ops\/s | SimSIMD Improvement |\r\n| :------- | :-------------------- | -------------------: | -------------------: | ------------------: |\r\n| `f64`    | `scipy.cosine`        |               40,729 |              725,382 |             17.81 x |\r\n| `f64`    | `scipy.sqeuclidean`   |              160,812 |              728,114 |              4.53 x |\r\n| `f64`    | `numpy.inner`         |              473,443 |              767,374 |              1.62 x |\r\n| `f64`    | `scipy.jensenshannon` |               15,684 |               38,528 |              2.46 x |\r\n| `f64`    | `scipy.kl_div`        |               49,983 |               61,811 |              1.24 x |\r\n\r\n### Between 2 Vectors, Batch Size: 1,000\r\n\r\n| Datatype | Method                |                Ops\/s |        SimSIMD Ops\/s | SimSIMD Improvement |\r\n| :------- | :-------------------- | -------------------: | -------------------: | ------------------: |\r\n| `f64`    | `scipy.cosine`        |               41,130 |            1,460,850 |             35.52 x |\r\n| `f64`    | `scipy.sqeuclidean`   |              162,147 |            1,486,255 |              9.17 x |\r\n| `f64`    | `numpy.inner`         |              473,856 |            1,580,136 |              3.33 x |\r\n","@rgommers and @dschmitz89, hi \ud83d\udc4b \r\n\r\nSmall update: with SimSIMD v4 and newer, return values are now also 64-bit floats, similar to SciPy, and more input types are supported in dot-products - including `complex128`, `complex64`, and the [missing](https:\/\/github.com\/numpy\/numpy\/issues\/14753) `complex32`.\r\n\r\nI've also changed the dynamic dispatch strategy. Aside from serial and Arm backends, on x86 I now differentiate Haswell, Skylake, Ice Lake, and Sapphire Rapids. The older CPUs got noticeable speed bump in some workloads, especially the bit-level Hamming and Jaccard distances.\r\n\r\nI am also now covering a broader build matrix including all Python versions supported by PyPi - [105](https:\/\/pypi.org\/project\/simsimd\/#files). It's more than NumPy (35) and SciPy (24) combined, so should be easy to integrate, if more performance is needed \ud83e\udd17 "],"labels":["enhancement","scipy.spatial"]},{"title":"MAINT,ENH: `linprog`, `highs` and callback mechanisms","body":"# SciPy callbacks\r\n\r\nAssociated PR: #19420 (WIP).\r\n\r\nSome comments on additions required for `scipy`. In some sense the callbacks in `highs` are [more functional](https:\/\/github.com\/ERGO-Code\/HiGHS\/blob\/58cc6c216c3c83129273e6331d16e3c7a6366fe1\/docs\/src\/callbacks.md) (they allow, e.g. interrupts) than those of `scipy`. `scipy` only needs to have the `OptimizeResult` available for consumption by the `callback`, that is, it is essentially a `logging` callback.\r\n\r\nIn general, an `OptimizeResult` has:\r\n```\r\n    x : ndarray\r\n        The solution of the optimization.\r\n    success : bool\r\n        Whether or not the optimizer exited successfully.\r\n    status : int\r\n        Termination status of the optimizer. Its value depends on the\r\n        underlying solver. Refer to `message` for details.\r\n    message : str\r\n        Description of the cause of the termination.\r\n    fun, jac, hess: ndarray\r\n        Values of objective function, its Jacobian and its Hessian (if\r\n        available). The Hessians may be approximations, see the documentation\r\n        of the function in question.\r\n    hess_inv : object\r\n        Inverse of the objective function's Hessian; may be an approximation.\r\n        Not available for all solvers. The type of this attribute may be\r\n        either np.ndarray or scipy.sparse.linalg.LinearOperator.\r\n    nfev, njev, nhev : int\r\n        Number of evaluations of the objective functions and of its\r\n        Jacobian and Hessian.\r\n    nit : int\r\n        Number of iterations performed by the optimizer.\r\n    maxcv : float\r\n        The maximum constraint violation.\r\n```\r\n\r\nAs will be seen however, not all of these are set (both in the documentation and implementation).\r\n\r\n## `simplex` callbacks\r\n\r\n- Are called at each solver step\r\n\r\n```\r\n    callback : callable, optional\r\n        If a callback function is provided, it will be called within each\r\n        iteration of the algorithm. The callback must accept a\r\n        `scipy.optimize.OptimizeResult` consisting of the following fields:\r\n\r\n            x : 1-D array\r\n                Current solution vector\r\n            fun : float\r\n                Current value of the objective function\r\n            success : bool\r\n                True only when a phase has completed successfully. This\r\n                will be False for most iterations.\r\n            slack : 1-D array\r\n                The values of the slack variables. Each slack variable\r\n                corresponds to an inequality constraint. If the slack is zero,\r\n                the corresponding constraint is active.\r\n            con : 1-D array\r\n                The (nominally zero) residuals of the equality constraints,\r\n                that is, ``b - A_eq @ x``\r\n            phase : int\r\n                The phase of the optimization being executed. In phase 1 a basic\r\n                feasible solution is sought and the T has an additional row\r\n                representing an alternate objective function.\r\n            status : int\r\n                An integer representing the exit status of the optimization::\r\n\r\n                     0 : Optimization terminated successfully\r\n                     1 : Iteration limit reached\r\n                     2 : Problem appears to be infeasible\r\n                     3 : Problem appears to be unbounded\r\n                     4 : Serious numerical difficulties encountered\r\n\r\n            nit : int\r\n                The number of iterations performed.\r\n            message : str\r\n                A string descriptor of the exit status of the optimization.\r\n```\r\n\r\nIn the implementation, we see:\r\n```python\r\n            solution[:] = 0\r\n            solution[basis[:n]] = T[:n, -1]\r\n            x = solution[:m]\r\n            x, fun, slack, con = _postsolve(\r\n                x, postsolve_args\r\n            )\r\n            res = OptimizeResult({\r\n                'x': x,\r\n                'fun': fun,\r\n                'slack': slack,\r\n                'con': con,\r\n                'status': status,\r\n                'message': message,\r\n                'nit': nit,\r\n                'success': status == 0 and complete,\r\n                'phase': phase,\r\n                'complete': complete,\r\n                })\r\n            callback(res)\r\n```\r\n\r\n## `revised_simplex`\r\n\r\nAs seen in `_linprog_rs.py`\r\n\r\n```\r\n    callback : callable, optional\r\n        If a callback function is provided, it will be called within each\r\n        iteration of the algorithm. The callback function must accept a single\r\n        `scipy.optimize.OptimizeResult` consisting of the following fields:\r\n\r\n            x : 1-D array\r\n                Current solution vector.\r\n            fun : float\r\n                Current value of the objective function ``c @ x``.\r\n            success : bool\r\n                True only when an algorithm has completed successfully,\r\n                so this is always False as the callback function is called\r\n                only while the algorithm is still iterating.\r\n            slack : 1-D array\r\n                The values of the slack variables. Each slack variable\r\n                corresponds to an inequality constraint. If the slack is zero,\r\n                the corresponding constraint is active.\r\n            con : 1-D array\r\n                The (nominally zero) residuals of the equality constraints,\r\n                that is, ``b - A_eq @ x``.\r\n            phase : int\r\n                The phase of the algorithm being executed.\r\n            status : int\r\n                For revised simplex, this is always 0 because if a different\r\n                status is detected, the algorithm terminates.\r\n            nit : int\r\n                The number of iterations performed.\r\n            message : str\r\n                A string descriptor of the exit status of the optimization.\r\n```\r\n\r\nThe implementation:\r\n\r\n```python\r\n        res = OptimizeResult(\r\n            {\r\n                \"x\": x_o,\r\n                \"fun\": fun,\r\n                \"slack\": slack,\r\n                \"con\": con,\r\n                \"nit\": iteration,\r\n                \"phase\": phase,\r\n                \"complete\": False,\r\n                \"status\": status,\r\n                \"message\": \"\",\r\n                \"success\": False,\r\n            }\r\n        )\r\n        callback(res)\r\n```\r\n\r\n## `interior-point`\r\n\r\n```\r\n    callback : callable, optional\r\n        If a callback function is provided, it will be called within each\r\n        iteration of the algorithm. The callback function must accept a single\r\n        `scipy.optimize.OptimizeResult` consisting of the following fields:\r\n\r\n            x : 1-D array\r\n                Current solution vector\r\n            fun : float\r\n                Current value of the objective function\r\n            success : bool\r\n                True only when an algorithm has completed successfully,\r\n                so this is always False as the callback function is called\r\n                only while the algorithm is still iterating.\r\n            slack : 1-D array\r\n                The values of the slack variables. Each slack variable\r\n                corresponds to an inequality constraint. If the slack is zero,\r\n                the corresponding constraint is active.\r\n            con : 1-D array\r\n                The (nominally zero) residuals of the equality constraints,\r\n                that is, ``b - A_eq @ x``\r\n            phase : int\r\n                The phase of the algorithm being executed. This is always\r\n                1 for the interior-point method because it has only one phase.\r\n            status : int\r\n                For revised simplex, this is always 0 because if a different\r\n                status is detected, the algorithm terminates.\r\n            nit : int\r\n                The number of iterations performed.\r\n            message : str\r\n                A string descriptor of the exit status of the optimization.\r\n```\r\n\r\nImplementation:\r\n\r\n```python\r\n        x_o, fun, slack, con = _postsolve(x \/ tau, postsolve_args)\r\n        res = OptimizeResult(\r\n            {\r\n                \"x\": x_o,\r\n                \"fun\": fun,\r\n                \"slack\": slack,\r\n                \"con\": con,\r\n                \"nit\": iteration,\r\n                \"phase\": 1,\r\n                \"complete\": False,\r\n                \"status\": 0,\r\n                \"message\": \"\",\r\n                \"success\": False,\r\n            }\r\n        )\r\n```\r\n\r\n# Mapping to `highspy`\r\n\r\n`highspy` uses the `HighsCallbackDataOut` which has:\r\n\r\n```\r\ntypedef struct {\r\n  int log_type;  \/\/ cast of HighsLogType\r\n  double running_time;\r\n  HighsInt simplex_iteration_count;\r\n  HighsInt ipm_iteration_count;\r\n  double objective_function_value;\r\n  int64_t mip_node_count;\r\n  double mip_primal_bound;\r\n  double mip_dual_bound;\r\n  double mip_gap;\r\n  double* mip_solution;\r\n} HighsCallbackDataOut;\r\n```\r\n\r\nWhich means that (bold entries are not present):\r\n- **x** : Current solution vector (not just for mip)\r\n- fun : Already present\r\n- **success** : Can probably be set to `false` without much trouble (e.g. `_rs` and `_ip`)\r\n- **slack** : The (nominally positive) values of the slack, b_ub - A_ub @ x. \r\n- **con** : The (nominally zero) residuals of the equality constraints, b_eq - A_eq @ x.\r\n- **phase** : `ip` sets this to 1 always\r\n- nit : Already present\r\n- message : Already supported (via the callback interface, not in `HighsCallbackDataOut`)\r\n\r\nHowever, to add things which are only ever going to be used for logging seems like a pointless slowdown though. Additionally, it seems like (https:\/\/github.com\/scipy\/scipy\/issues\/9536) the design of `linprog` callbacks could in general do with some more discussion.\r\n\r\n# Alternatives\r\n\r\nThere is a similar issue open over at HiGHS (https:\/\/github.com\/ERGO-Code\/HiGHS\/issues\/911), but as such, from a solver perspective, the existing implementation seems to be pretty good.\r\n\r\nIf it is acceptable, `highs` can either include a SciPy compatible (guarded by a compile flag) set of callbacks building on what they have, or we could maintain a set in our copy of HiGHs. \r\n\r\n# Possible next steps\r\n\r\nThe choices are:\r\n- Deprecate the callback interface completely\r\n- Expose only `highspy` style callbacks\r\n\r\nAnything in between requires changes to `highs`. I believe in part this issue is linked to \"what is the scop of SciPy's linear solvers\" and so https:\/\/github.com\/scipy\/scipy\/issues\/15915#issuecomment-1774816186, where @fuglede describes his usage of SciPy in the context of also having `highspy` \/ PuLP \/ PyOMO etc. is of use.\r\n\r\nSeeking comments from anyone using `linprog` but also maybe @rgommers , @mckib2 , @mdhaber .\r\n\r\nPersonally, I think it would be best to transparently pass callback functionality through to `highspy`, as that has the lowest maintenance burden to additional feature ratio. Unless it is in wide use, then, I would suggest first deprecating the existing methods https:\/\/github.com\/scipy\/scipy\/issues\/15707 (non-HiGHs) then adding back `highspy` compatible callbacks. Or we could remove existing methods while introducing `highspy` compatible callbacks.","comments":["> Unless it is in wide use, then, I would suggest first deprecating the existing methods #15707 (non-HiGHs) then adding back highspy compatible callbacks.\r\n\r\nAll non-HiGHS methods are already deprecated. They produce a warning stating that they \"will be removed in SciPy 1.11.0\". We missed that deadline only because we didn't think we should remove these methods without a compatible callback interface. If we aren't going to provide a callback interface that is compatible with what they have offered, we will probably need to restart the deprecation cycle or continue to offer one of these methods.\r\n\r\n> However, to add things which are only ever going to be used for logging seems like a pointless slowdown though.\r\n\r\nWhy is there necessarily a slowdown? It is possible to design things such that there is essentially zero overhead if a callback function does not need to be called.\r\n\r\n> Anything in between requires changes to highs\r\n\r\nI thought we are the reason they were adding a callback interface to begin with. Is that not right?\r\n\r\n> they allow, e.g. interrupts\r\n\r\nWe can add that as we have done for `minimize`: if the Python callback raises a `StopIteration` error, we can intercept that and return a nonzero status to HiGHS.\r\nAre there other features of the HiGHS callbacks that you don't think we can support in a backward compatible way?","> > Unless it is in wide use, then, I would suggest first deprecating the existing methods #15707 (non-HiGHs) then adding back highspy compatible callbacks.\r\n> \r\n> All non-HiGHS methods are already deprecated. They produce a warning stating that they \"will be removed in SciPy 1.11.0\". We missed that deadline only because we didn't think we should remove these methods without a compatible callback interface. If we aren't going to provide a callback interface that is compatible with what they have offered, we will probably need to restart the deprecation cycle or continue to offer one of these methods.\r\n> \r\n> > However, to add things which are only ever going to be used for logging seems like a pointless slowdown though.\r\n> \r\n> Why is there necessarily a slowdown? It is possible to design things such that there is essentially zero overhead if a callback function does not need to be called.\r\n\r\n@HaoZeke's comment seems to be in reference to the amount of data being passed around, i.e., copying data to and from structs is easy but does have some overhead cost and we'd need to justify that cost.  `slack` and `con` seem to me to be the big offenders here as they would likely need to be recomputed and stored each time the callback is invoked, or (at best) copied from some internal HiGHS representation.  Passing by mutable reference is likely unacceptable here.\r\n\r\n> \r\n> > Anything in between requires changes to highs\r\n> \r\n> I thought we are the reason they were adding a callback interface to begin with. Is that not right?\r\n\r\nWe are probably on their map, but they seem to have other drivers and I don't see any evidence that we had a lot of input into their design other than expressing what we were currently doing in the legacy SciPy linprog implementations. (I haven't been able to follow very closely though, so this could be wrong).\r\n\r\n> > they allow, e.g. interrupts\r\n> \r\n> We can add that as we have done for `minimize`: if the Python callback raises a `StopIteration` error, we can intercept that and return a nonzero status to HiGHS.\r\n\r\nReading their callback documentation, I'm wondering if it's possible to emulate per-iteration callbacks for simplex and interior point via the Simplex and IPM Interrupt callbacks until the old-style callbacks can be deprecated fully.  The interrupt callbacks seem to be guaranteed to be run every iteration.  I have this in mind, e.g., Simplex:\r\n\r\n- register a Simplex Interrupt Callback\r\n    - put the SciPy user's callback in a wrapper that can calculate things (e.g., `slack`, `con`) and grab more information (e.g., current solution value if there is one) before invoking SciPy callback\r\n- pass the parent `Highs` object (and anything else we need) as `user_callback_data` (this is how we can get any solution info if it's available)\r\n\r\nThere are two assumptions above:\r\n- the in-progress simplex solution is easily available from top level objects such as the parent `Highs` obj\r\n- callbacks are being run in a way which allows thread-safe access to whatever is in `user_callback_data` (probably a good question for HiGHS devs: what are the concurrency guarantees for callbacks?)\r\n\r\n> Are there other features of the HiGHS callbacks that you don't think we can support in a backward compatible way?\r\n\r\nA good rule of thumb seems to be that if you do what Gurobi does, everything else is at least pretty close to that.  So might be worth taking a look at that documentation is and seeing what differences there are.","From 30000 feet and without knowing details, I wonder what is the value of having `scipy.optimize.linprog` in the long run at all if all it does is delegate to `highspy`.","> From 30000 feet and without knowing details, I wonder what is the value of having `scipy.optimize.linprog` in the long run at all if all it does is delegate to `highspy`.\r\n\r\nI had this confusion initially as well @ev-br, and the scope of where SciPy's linprog fits into the ecosystem is discussed by @fuglede in https:\/\/github.com\/scipy\/scipy\/issues\/15915#issuecomment-1774816186 which I think is important enough to be reproduced here for context:\r\n\r\n> Some scattered thoughts:\r\n> \r\n> Already in its current form, highspy also provides some amount of access to the state of the solver (for instance, one thing I found useful was having https:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1264 available in highspy, which enables sharing solver state across process boundaries).\r\n> \r\n> If any of this would be relevant in SciPy, I think it would be if we could provide zero-overhead abstractions to some of the functionality that's already available, but even then, I imagine that that would quickly become very HiGHS-specific and might as well live in highspy itself: IMO, the main value-adds of linprog over highspy at the moment are the small things, not having to manually convert inequality constraints to equality constraints, the fact that CSC arrays are the native citizens of highspy, so you can draw on what's already available in scipy.sparse, and most importantly that it's all zero-overhead: (Very) high-level libraries like PuLP\/PyOMO are super useful for experimentation but very often you run into the issue that generating and passing state to the underlying solver becomes the bottleneck, with relatively short time spent by the solver itself. (I found it quite curious, for instance, that only rather recently did Gurobi add a feature to pass LPs as sparse arrays instead of having to go through their modelling API.)\r\n> \r\n> And even if the more stateful operations were available here, we would still want the stateless one to be available: When you don't need stateful operations, using them adds a good deal of overhead (compare e.g. the first few rows of the \"SciPy (defaults)\"\/\"highspy (defaults)\" columns in https:\/\/github.com\/ERGO-Code\/HiGHS\/discussions\/1198#discussioncomment-5647505); I've had use cases where I'll creatively switch between stateless linprog and stateful highspy based on model size. If one could do something it that domain, I think that would be useful, but even then, probably highspy itself (or HiGHS, really) would be a better home for any improvements.\r\n> \r\n> So that's all if all we care about is performance (which is the main reason for preferring SciPy over e.g. PuLP\/PyOMO at the moment). Another value-add could be if it's possible to design a (preferably zero-overhead) API that caters to a broader set of users: My impression is that things like PuLP\/PyOMO are more accessible than the lower-level APIs in SciPy or highspy; to the point where some users would find it difficult to go to a lower level even when performance requirements ask for it. Now I don't think you want SciPy to compete with PuLP\/PyOMO directly (right?), but if something could be done to introduce a broader class of users to the higher-performance lower-level APIs somehow, then that could be good.\r\n\r\nMy understanding from that is that in the hierarchy there is:\r\n- PuLP \/ PyOMO (high level)\r\n- Scipy (middle)\r\n- Highs + highspy (lowest level)\r\n\r\nWhich seems reasonable. There are other reasons to have SciPy's `linprog` interface, which is intuitive to users and in-use already for a long time. From a practical perspective, `simplex` and older `linprog` methods simply do not compare to the performance and dedicated maintainence of `highs` and the team (@jajhall) has been very responsive and open to working with SciPy (including assisting with relevant bugs). Similarly pragmatic is the fact that users would reasonably expect to have a linear solver suite embedded in SciPy, and HiGHs is the best choice for that now.\r\n\r\nAnother thing (but maybe I am not the best person to talk about this) is that it seems the callbacks were originally a special feature of the `simplex` solver and then later expanded to the others, so it seems there is precedent to first accomodate one implementation and later consider expanding (if ever a need arises).","> > > Unless it is in wide use, then, I would suggest first deprecating the existing methods #15707 (non-HiGHs) then adding back highspy compatible callbacks.\r\n> > \r\n> > \r\n> > All non-HiGHS methods are already deprecated. They produce a warning stating that they \"will be removed in SciPy 1.11.0\". We missed that deadline only because we didn't think we should remove these methods without a compatible callback interface. If we aren't going to provide a callback interface that is compatible with what they have offered, we will probably need to restart the deprecation cycle or continue to offer one of these methods.\r\n> > > However, to add things which are only ever going to be used for logging seems like a pointless slowdown though.\r\n> > \r\n> > \r\n> > Why is there necessarily a slowdown? It is possible to design things such that there is essentially zero overhead if a callback function does not need to be called.\r\n> \r\n> @HaoZeke's comment seems to be in reference to the amount of data being passed around, i.e., copying data to and from structs is easy but does have some overhead cost and we'd need to justify that cost. `slack` and `con` seem to me to be the big offenders here as they would likely need to be recomputed and stored each time the callback is invoked, or (at best) copied from some internal HiGHS representation. Passing by mutable reference is likely unacceptable here.\r\n\r\nYup, also currently there is one data structure which is being used for all the callback kinds, the slowdown here is the fact that say, a callback meant to only interrupt the solver shouldn't need to do the (potentially expensive) operation of fillin a struct which may or may not even be used for logging (depending on if the logging callback is active). I might be wrong, but generally, large data transfer \/ computation purely for logging is almost always only for development \/ small problems, eventually for production cases it becomes difficult to interpret anyway.\r\n\r\n> \r\n> > > Anything in between requires changes to highs\r\n> > \r\n> > \r\n> > I thought we are the reason they were adding a callback interface to begin with. Is that not right?\r\n> \r\n> We are probably on their map, but they seem to have other drivers and I don't see any evidence that we had a lot of input into their design other than expressing what we were currently doing in the legacy SciPy linprog implementations. (I haven't been able to follow very closely though, so this could be wrong).\r\n\r\nThere are other drivers, but I think the HiGHs devs are very open to reasonable changes, as long as they are also likely to be of use in production systems (i.e. don't slow down the solver). This was why for example, per-iteration callbacks were a non-starter (I have since opened a discussion regarding if we might include a compile time guarded path just for SciPy but it would just increase maintainer burdens \/ might cause debugging issues). \r\n\r\n> \r\n> > > they allow, e.g. interrupts\r\n> > \r\n> > \r\n> > We can add that as we have done for `minimize`: if the Python callback raises a `StopIteration` error, we can intercept that and return a nonzero status to HiGHS.\r\n> \r\n> Reading their callback documentation, I'm wondering if it's possible to emulate per-iteration callbacks for simplex and interior point via the Simplex and IPM Interrupt callbacks until the old-style callbacks can be deprecated fully. The interrupt callbacks seem to be guaranteed to be run every iteration. I have this in mind, e.g., Simplex:\r\n> \r\n> * register a Simplex Interrupt Callback\r\n>   \r\n>   * put the SciPy user's callback in a wrapper that can calculate things (e.g., `slack`, `con`) and grab more information (e.g., current solution value if there is one) before invoking SciPy callback\r\n> * pass the parent `Highs` object (and anything else we need) as `user_callback_data` (this is how we can get any solution info if it's available)\r\n> \r\n> There are two assumptions above:\r\n> \r\n> * the in-progress simplex solution is easily available from top level objects such as the parent `Highs` obj\r\n> * callbacks are being run in a way which allows thread-safe access to whatever is in `user_callback_data` (probably a good question for HiGHS devs: what are the concurrency guarantees for callbacks?)\r\n> \r\n\r\nIn general HiGHs consideres thread safety, so if there aren't any guarantees yet they'd be open to fixing that. However, AFAIK think the in-progress simplex solution is not part of the parent `Highs` object, I might be wrong about that though, @jajhall would be best suited to answer that.\r\n\r\n> > Are there other features of the HiGHS callbacks that you don't think we can support in a backward compatible way?\r\n> \r\n> A good rule of thumb seems to be that if you do what Gurobi does, everything else is at least pretty close to that. So might be worth taking a look at that documentation is and seeing what differences there are.\r\n\r\nYeah it seems like most of the linear optimization literature \/ software have different paradigms but generally converge (across C++ \/ Julia \/ Java \/ Matlab \/ other) implementations to Gurobi as a baseline..\r\n","From 10,000m, using our callback mechanism, HiGHS could add a per-iteration callback to simplex with zero overhead for those who don't want. However, even if it were activated, the only information it would make available is execution time and primal or dual objective function value, depending on whether primal or dual simplex is being used. \n\nGenerally it would be a call from the dual simplex algorithm applied to a presolved LP with perturbed costs, so it wouldn't even know the primal objective value without the unacceptable overhead of computing it. And, it would be at a point that is, at best, only dual feasible. So, if the incumbent solution in HiGHS were converted into the corresponding solution for the original LP, it would normally only be a dual solution, and who amongst your users would interpret that? The primal solution that is known in dual simplex - like the dual solution known in primal simplex - is infeasible until optimality.\n\nSurely the current per-factorization callbacks in simplex come frequently enough for any sane user\n\nIn short I'd ditch per-iteration callbacks, and explain the futility of them to any user who objects.\n\nGenerally, we are prepared to add things that are useful to major users, but nothing that would impact meaningfully on performance.\n\nWe are adding callbacks because (1) they are things that optimization solvers offer and (2) because they are required by an upcoming major user of HiGHS.\n\n> IMO, the main value-adds of linprog over highspy at the moment are the small things, not having to manually convert inequality constraints to equality constraints, the fact that CSC arrays are the native citizens of highspy, so you can draw on what's already available in scipy.sparse, and most importantly that it's all zero-overhead:\n\nI do wonder at the value of these advantages when the underlying linprog solver is so much slower than HiGHS. I find it baffling that anything in the SciPy - HiGHS interface could ever be as remotely expensive as solving an LP or MIP. I'm also puzzled by the reference to converting inequality constraints to equalities. HiGHS handles general boxed constraints so, whatever forms of constraints SciPy uses can be communicated without modification. Yes, linprog may be \"zero overhead\" but I'd be amazed if the HiGHS overhead compares remotely with its superior performance.\n","> I find it baffling that anything in the SciPy - HiGHS interface could ever be as remotely expensive as solving an LP or MIP.\r\n\r\nPython modeling interfaces can be _very_ slow (which is why I'm not a huge fan of them, but data scientists sure seem to be).  IIRC, Google's or-tools is a big offender where most of the time can be spent building models using classes and abstractions before anything is converted to actual coefficients and constraint matrices and the solver invoked.  SciPy is different in the Python library ecosystem in this regard by providing light, MATLAB-linprog-like constraint abstractions.  I think the comment about converting inequality constraints to equalities is misleading and should instead point to the (trivial but annoying) changing of constraint formulation from the SciPy\/MATLAB-style `Ax == b_e, Ax <= b_l, Ax >= b_g` to something like `l_i <= x_i <= u_i` that HiGHS expects\r\n\r\n+1 for relaxing the requirement of SciPy linprog simplex callbacks to run every iteration and instead at a factorization points.","\nInteresting comments on speed of modelling languages, but how is this relevant to HiGHS: we just deal in finalised matrices - or are you referring to building models by calls to HiGHS to add individual variables and constraints? JuMP seems to be efficient. Is this because it's written in Julia?\n\n> the (trivial but annoying) changing of constraint formulation from the SciPy\/MATLAB-style Ax == b_e, Ax <= b_l, Ax >= b_g to something like l_i <= x_i <= u_i that HiGHS expects\n\nIf it's the concatenation of three matrices of constraint coefficients into one that's irritating - surely it's not meaningfully expensive - then I can add an alternative method for passing an LP that has these restrictions on the general boxed constraints and then do the concatenation at negligible cost in C++\n\nUse of general boxed constraints is more efficient, particularly when modellers want distinct, finite lower and upper bounds. With the SciPy\/MATLAB style, an extra matrix row is required. ","(At the risk of getting distracted from the core issue at hand...)\r\n\r\n> Interesting comments on speed of modelling languages, but how is this relevant to HiGHS: we just deal in finalised matrices - or are you referring to building models by calls to HiGHS to add individual variables and constraints? JuMP seems to be efficient. Is this because it's written in Julia?\r\n\r\nI don't think there's anything relevant to HiGHS here, just a comment on modeling overhead incurred by some Python libraries that either integrate HiGHS or use other solvers.  JuMP could be more efficient at runtime here due its compilation steps (able to statically transform a modeling representation into finalized matrices). I just pulled up a presentation we gave back in 2021 comparing availability and performance of LP solvers in the Python ecosystem -- it looks like just OR-Tools had the model building problem which added seconds of runtime to a benchmark run against various sizes of dense constraint matrices:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/28734319\/28cb516a-c1a0-421c-8cb5-d7314111486b)\r\n\r\nI can't comment on if it's an issue anymore in OR-Tools and is likely beside the point in this issue -- presented only for your curiosity :)  A counter argument here would be that modeling overhead might be worth it if it's useful to the user and you're not constantly rebuilding the model"],"labels":["enhancement","scipy.optimize","maintenance"]},{"title":"BUG: In test_b_orthonormalize, cannot parameterize Vdtype, Bdtype, or BVdtype using VDTYPES","body":"### Describe your issue.\r\n\r\nIn\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/eff7bfd88a6b92f0e213319628d222ca0116db7c\/scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py#L104-L106\r\n\r\nchanging any of the `REAL_DTYPES` to `VDTYPES` as suggested in https:\/\/github.com\/scipy\/scipy\/issues\/19442#issuecomment-1783399099 causes the tests to fail.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\n@pytest.mark.parametrize(\"Vdtype\", VDTYPES) \r\n@pytest.mark.parametrize(\"Bdtype\", VDTYPES) \r\n@pytest.mark.parametrize(\"BVdtype\", VDTYPES)\r\n```\r\n\r\n\r\n### Error message\r\n\r\nMany failures in `test_lobpcf.py` like:\r\n```shell\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex128-10-50]\r\n```\r\n\r\nFull output:\r\n\r\n<details>\r\n\r\n```shell\r\n=========================== short test summary info ============================\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float32-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-float64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-longdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float32-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-float64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float64-longdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float32-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-float64-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[longdouble-longdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float32-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-float64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[clongdouble-longdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float32-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-float64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex128-longdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex128-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-complex64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float32-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-float64-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[complex64-longdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex128-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex128-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex128-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-complex64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float32-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float32-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float32-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float64-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float64-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-float64-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-longdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-longdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-clongdouble-longdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-clongdouble-1-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-clongdouble-2-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-clongdouble-10-50]\r\nFAILED scipy\/sparse\/linalg\/_eigen\/lobpcg\/tests\/test_lobpcg.py::test_b_orthonormalize[float32-complex128-complex128-1-50]\r\n= 414 failed, 55006 passed, 3014 skipped, 226 xfailed, 10 xpassed, 384 warnings in 134.96s (0:02:14) =\r\n```\r\n\r\n<\/details>\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n<details>\r\n\r\n```shell\r\n1.11.3 1.26.0 sys.version_info(major=3, minor=12, micro=0, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/include\/flexiblas\r\n    lib directory: \/usr\/lib64\r\n    name: flexiblas\r\n    openblas configuration: unknown\r\n    pc file directory: \/usr\/lib64\/pkgconfig\r\n    version: 3.3.1\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/include\/flexiblas\r\n    lib directory: \/usr\/lib64\r\n    name: flexiblas\r\n    openblas configuration: unknown\r\n    pc file directory: \/usr\/lib64\/pkgconfig\r\n    version: 3.3.1\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/usr\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: gcc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 13.2.1\r\n  c++:\r\n    commands: g++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 13.2.1\r\n  cython: \r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.2\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 13.2.1\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/..\/usr\/lib\/python3.12\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:  \r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:   \r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/usr\/bin\/python3\r\n  version: '3.12'\r\n```\r\n\r\n<\/details>","comments":["Issue requested in https:\/\/github.com\/scipy\/scipy\/issues\/19442#issuecomment-1783399099.","Full test output: [test-output.log](https:\/\/github.com\/scipy\/scipy\/files\/13195689\/test-output.log)"],"labels":["defect","scipy.sparse.linalg"]},{"title":"WIP, BUG: handle subnormals jensenshannon","body":"* Fixes #19436\r\n\r\n* this applies a bug fix + regression test for the above issue, on the assumption that we should treat subnormals fairly similar to zero itself, and applies a similar patch to the `cdist`\/`pdist` code paths as well\r\n\r\n* note, however, that I'm not operating from a place of deep expertise here and I can think of a number of things we might want to check\/fix up (note below), though I'd probably argue this makes more sense than returning infinity for these cases\r\n\r\n* some additional things it might make sense to do: check another reference\/implementation, write a test with subnormals at the end of the vectors instead of just at the start, write a test with more than 1 subnormal per vector (and do we actually know exactly what we want in all these cases...)\r\n\r\n[skip cirrus] [skip circle]\r\n","comments":["32-bit CI failure is real, I'll mark as WIP for now since that surprised me.","> It seems like the C code in distance_impl.h doesn't actually get used. \r\n\r\nIt definitely does, the codepaths for the Python-level single call vs. `pdist`\/`cdist` go through different control flows.\r\n\r\nThanks for the suggestions, I'll see if I can improve it based on them.","> It definitely does, the codepaths for the Python-level single call vs. `pdist`\/`cdist` go through different control flows.\r\n\r\nGot it. I just had trouble tracing it due to lack of familiarity with how spatial is structured.","Thinking about this again. If we compute to arbitrary precision\r\n\r\n```python\r\nfrom mpmath import mp\r\n\r\np = mp.mpf(5e-324)\r\nq = mp.zero\r\nm = (p + q) \/ 2.0\r\np * mp.log(p\/m)\r\n# mpf('3.424602094263885e-324')\r\n```\r\n\r\nmy suggestion makes this underflow to zero, and your original solution approximates it with the smallest subnormal. I guess it's debatable which should be preferred.","Wait, we could also sidestep this issue completely by replacing this part of the Python code\r\n\r\n```python\r\nm = (p + q) \/ 2.0\r\nleft = rel_entr(p, m)\r\nright = rel_entr(q, m)\r\n```\r\n\r\nwith\r\n\r\n```python\r\ntwo_m = p + q\r\nleft = xlogy(p, 2*p\/two_m)\r\nright = xlogy(q, 2*q\/two_m)\r\n```\r\n\r\nand doing the equivalent thing in the C code. ","I think that produces `nan` when both vectors are identical (i.e., `[0. 1.]`), instead of `0`. Maybe I'll let this one sit for a day or two and see where things settle.","> I think that produces `nan` when both vectors are identical (i.e., `[0. 1.]`), instead of `0`. Maybe I'll let this one sit for a day or two and see where things settle.\r\n\r\nSorry, that's right, obviously. I think it should be straightforward to make the C code do the right thing (whichever it is we choose) with some conditionals, but annoying to get right in Python without resorting to `_lazywhere`. I guess we could add a new private ufunc to `special` for this."],"labels":["defect","scipy.spatial","C\/C++"]},{"title":"BUG: Jensen-Shannon Distance returns inf ","body":"### Describe your issue.\n\nThe ouput of the Jensen-Shannon Distance by definition is confined to the interval [0, 1]. I found an edge case where it returns inf. \n\n### Reproducing Code Example\n\n```python\nfrom scipy.spatial.distance import jensenshannon\r\n\r\np = [0, 1]\r\nq = [5e-324, 1]\r\n\r\nx = jensenshannon(p, q)\r\nprint(x)\n```\n\n\n### Error message\n\n```shell\nI expect a return value of about 1 but get inf.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nPython 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 8.16.1 -- An enhanced Interactive Python. Type '?' for help.\r\nPyDev console: using IPython 8.16.1\r\nPython 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)] on win32\r\nimport sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n1.11.3 1.26.1 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-dic8cvan\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-_etjukwv\\cp311-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.11'\n```\n","comments":[],"labels":["defect","scipy.spatial"]},{"title":"DEP:optimize: Deprecate \"old style\" bounds in SLSQP","body":"I am slowly getting into the API part of the rewrite of SLSQP solver and I would like to propose a certain deprecation for the `bounds` argument type.\r\n\r\n In the SLSQP method, the bounds on the variables are given as \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/0ae31ea86f44937e6f84fada38609c66b70efc1f\/scipy\/optimize\/_slsqp_py.py#L101-L104\r\n\r\nApart from the factual inconsistency(#6024), this is a quite wasteful way of defining bounds (cf. #19402). @andyfaff in fact mentioned that the bounds are typically used in the dense form, meaning typically all(most) variables are bounded. Also in the code these bounds are converted back and forth to comply with `ScalarFunction` API anyways. \r\n\r\nAs far as I can tell, new bounds meaning a `(n, 2)` shaped array where first column denotes the lower bound and `-np.inf` means unbounded. Similarly the second column for upper bound. It is a bit inconvenient for the user that they have to fill in an array of `-np.inf` to provide upper bounds but that's a surgery that involves all `optimize` module and way beyond the scope of this issue. \r\n\r\nI am not familiar with the new\/old bound style lore too much. However, while I am rewriting this solver anyways, I can accommodate the removal of the old-style bounds and convert them to new style. \r\n\r\nOpinions from the optimize residents would be much appreciated. ","comments":["\"New is always better.\"\r\n\r\nBut I think new bounds typically have separate `lb`\/`ub` arrays, like the `Bounds` object. That is also what `new_bounds_to_old` accepts and `old_bounds_to_new` returns. So you can get probably get by with storing one `inf` and broadcasting.","I think I couldn't follow but I'mnot sure the betterness of the new either. do you mean the Bounds object should be the latest and greatest that we should switch to ? ","> do you mean the Bounds object should be the latest and greatest that we should switch to\r\n\r\nYes, basically.\r\n\r\nThe essential difference is that \r\n- \"new\" bounds are separate `lb` and `ub` array-likes of shape `(n,)` that represent \"no bound\" with +\/- inf, whereas\r\n- \"old\" bounds are array-likes of shape `(n, 2)` that represent \"no bound\" with `None`.\r\n\r\n`Bounds` is a pretty light wrapper around separate `lb` and `ub` arrays, though. So `minimize` will accept a `Bounds` object, but it can easily pass your new (private) SLSQP function either a `Bounds` object or separate `lb`\/`ub` arrays. That distinction is not very important, so do whatever is convenient.\r\n\r\nAs for `fmin_slsqp`, is it possible to just leave that alone and indicate that it will be removed in SciPy 2.0.0? It has been been listed as a [\"Legacy Function\"](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/optimize.html#legacy-functions) for a long time.","Ah OK `Bounds` it is then. \r\n\r\nInternally, `minimize` is calling `fmin_slsqp` so probably we have to separate the private function and point `fmin_slsqp` to it too and handle the old bounds there converted to `Bounds` object somehow. I'll check the `Bounds` object first.","> `fmin_slsqp` to it too and handle the old bounds there converted to `Bounds` object somehow.\r\n\r\nThat works, too. The possibility I had in mind was just to leave `fmin_slsqp` alone and write a new private function used only by `minimize`. The upside is that this would avoid disturbing code that has been marked as \"legacy\"; the downside is that it would require keeping the old SLSQP code around until SciPy 2.0.0.","I think we can live with that. It's not too much work anyways. The bulk of the work is untangling the variables prepared for the fortran call inside `_minimize_slsqp` once I wrap my head around it, the rest is just argument tracking and bookkeeping so I don't expect too much complications there.","> `old_bounds_to_new` returns\r\n\r\nUnfortunately that function returns a `(lb, ub)` tuple instead of a `Bounds` object.\r\n\r\n> \"old\" bounds are (n, 2)-shaped array-likes that represent \"no bound\" with None, whereas\r\n\r\nActually, \"old\" bounds are a sequence of `(min, max)` tuples for each parameter. They can be turned into arrays easily, but the tuples take up a lot of space if they're all individual objects and you have millions of parameters.\r\n\r\n> Internally, `minimize` is calling `fmin_slsqp`\r\n\r\nNo, it calls `_minimize_slsqp`. If @ilayn is re-writing SLSQP, then the entry point can be called `_minimize_slsqp`.\r\n\r\nMy thoughts are:\r\n\r\n1. Gradually alter the `_minimize_*` functions so they *only* accept and work internally with new-style `Bounds` objects.\r\n2. Clean-up `optimize.minimize` so that it doesn't have to do any translation to old-style, but only conversion to new-style if required.\r\n3. In all the traditional `fmin_*` functions, keep accepting old-style but do a conversion to new-style before sending to their corresponding `_minimize_*` function.\r\n4. (as an alternate to 3). Deprecate all old-style use in `fmin_X` and `optimize.minimize`, and in a few releases time (or in Scipy2) not accept them anymore. Or, as Matt says, yank all the `fmin_*` functions. I think this point might be more controversial as they've been around for aaaaagggggggesssss.\r\n5.  ","> No, it calls `_minimize_slsqp`\r\n\r\nAh yes, I got confused by the names. Sorry about that.\r\n\r\n>  If @ilayn is re-writing SLSQP,\r\n\r\nYes it is done except the Python API marriage.","If it's going to be a drop in\/superset of `_minimize_slsqp`, then it's going to use all the same inputs.\r\nDo you have it in your fork?","No unfortunately not yet, but the internals up to `lsq` part is in #19121. I'll try to tidy up and push there.\r\n\r\nStill testing the numerical parts of the slsqp body without the minimize API on top of it (comparing with Fortran with raw calls) and so far so good. Hence I was just about to touch the `minimize_slsqp` part and I got a bit confused with the different old\/new bound styles. ","> Unfortunately that function returns a `(lb, ub)` tuple instead of a Bounds object.\r\n\r\nThat can be changed. It's in a private module. In any case, \"new\" keeps upper and lower in separate arrays, whether they are wrapped in a `Bounds` object or not.\r\n\r\n> Actually, \"old\" bounds are a sequence of (min, max) tuples for each parameter. \r\n\r\nI know, but I wrote \"array-like\". A list of tuples is the standard form of an old-style bound, but one can pass any array-like of shape `(n, 2)` and the code will work (by converting it to a list of tuples). The specific types weren't important for my purposes, but I understand why it is important to you (memory).\r\n\r\n> I think this point might be more controversial as they've been around for aaaaagggggggesssss.\r\n\r\nPerhaps I misremembered how the [legacy status conversation](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/Z33JO27EX6BX7LVBI6CW6MX64VQZIJ66\/#6GLVRHX5XBTZNL62LFN4YPGACY5BDPLA) left off. I thought we decided that we would make a one-time, stand-alone release of all the \"legacy\" code and remove it from SciPy 2.0, but i don't see that there.","Another issue with the `Bounds` is that they can be set to anything so it is becoming a bit difficult to handle the broadcasting and so on to understand the context. \r\n\r\n```python\r\nfrom scipy.optimize import Bounds\r\nx = Bounds()\r\nx.ub = 'mystring' \r\nx\r\nOut[115]: Bounds(array([-inf]), 'mystring')\r\n```\r\n"],"labels":["scipy.optimize","deprecated"]},{"title":"BUG: `scipy.ndimage.value_indices` returns empty dict for `intc`\/`uintc` dtype on Windows","body":"### Describe your issue.\n\n`scipy.ndimage.value_indices` returns an empty dict on Windows, if the input is `intc`\/`uintc` dtype.\r\n\r\nDiscovered in https:\/\/github.com\/cupy\/cupy\/pull\/7947.\n\n### Reproducing Code Example\n\n```python\nimport numpy as xp\r\nimport scipy as xps\r\n\r\n\r\nimage = (xp.random.rand(10,12) * 4).astype(xp.int32)\r\nval_idx = xps.ndimage.value_indices(image)\r\nprint(val_idx.keys())  # dict_keys([0, 1, 2, 3])\r\n\r\nimage = (xp.random.rand(10,12) * 4).astype(xp.intc)\r\nval_idx = xps.ndimage.value_indices(image)\r\nprint(val_idx.keys())  # dict_keys([])   <- UNEXPECTED\n```\n\n\n### Error message\n\n```shell\nn\/a\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.1 sys.version_info(major=3, minor=12, micro=0, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-r6tmzm88\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-q67zaj2m\\cp312-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.12'\n```\n","comments":[],"labels":["defect","scipy.ndimage"]},{"title":"ENH: Use callbacks for highs in linprog","body":"**Do not attempt to review until #19255 is in.** :)\r\n\r\n#### Reference issue\r\nCloses https:\/\/github.com\/scipy\/scipy\/issues\/15707 eventually.\r\n\r\n#### What does this implement\/fix?\r\nBasically passes `callback` information in the same way as the rest of `linprog`.\r\n\r\n#### Additional information\r\n\r\nDraft until its ready. Might need more changes here as well. Depends on #19255.","comments":[],"labels":["enhancement","scipy.optimize"]},{"title":"BUG: integrate.solve_ivp fails for some step sizes if dense_output=True with events but no t_eval","body":"### Describe your issue.\r\n\r\nCalling scipy.integrate.solve_ivp under the following combination of conditions results in failure:\r\n\r\n- Small step size `max_step`.\r\n- `dense_output = True`\r\n- `events` provided.\r\n- `t_eval` not provided. \r\n\r\nThe error appears to be after scipy\/integrate\/_ivp\/ivp.py here: https:\/\/github.com\/scipy\/scipy\/blob\/43bb524ecf1544743493ae69053e38a93c5aee7c\/scipy\/integrate\/_ivp\/ivp.py#L689-L691\r\n\r\nIf an event is found in the current timestep, that when solved gives a time value _exactly equal_ to the start of the interval (i.e. the end of the last interval), appending that time value means a duplicate entry is added to the `ts` array.  When `dense_output = True` and `t_eval` is `None`, this call to `OdeSolution` fails:  https:\/\/github.com\/scipy\/scipy\/blob\/43bb524ecf1544743493ae69053e38a93c5aee7c\/scipy\/integrate\/_ivp\/ivp.py#L727-L731\r\n\r\nThis is because a monotonically increasing \/ decreasing series of time values is required.  It seems to only occur if the maximum step size is controlled to be a small value.  A simple fix might be to simply check for a duplicate value before appending the event time mark, but I haven't thought about this too deeply. \r\n\r\n_Added:_ This error is very similar to https:\/\/github.com\/scipy\/scipy\/issues\/17066#issue-1381164720  The main issue appears to be when the derivative term happens to be an exact multiple of the timestep, so that when the terminating event is reached the solution rounds to exactly the timestep value again - this giving a repeated value.  My workaround is to slightly offset the timestep by a tiny value to prevent this, but this is undesirable.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\n# Example based on the documentation.\r\nimport numpy as np\r\nfrom scipy.integrate import solve_ivp\r\n\r\ndef upward_cannon(t, y):\r\n    return [y[1], -9.80665]\r\n\r\ndef hit_ground(t, y):\r\n    return y[0]\r\n\r\nhit_ground.terminal = True\r\nhit_ground.direction = -1\r\n\r\nsol = solve_ivp(upward_cannon, [0, np.inf],[0, 0.001],\r\n                max_step=0.05 * 0.001 \/ 9.80665,\r\n                events=hit_ground, dense_output=True)\r\nprint(sol.t_events)\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"C:.........\\scratch_4.py\", line 17, in <module>\r\n    sol = solve_ivp(upward_cannon, [0, np.inf],[0, 0.001],\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:.........\\venv\\Lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py\", line 672, in solve_ivp\r\n    sol = OdeSolution(ts, interpolants)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:.........\\venv\\Lib\\site-packages\\scipy\\integrate\\_ivp\\common.py\", line 159, in __init__\r\n    raise ValueError(\"`ts` must be strictly increasing or decreasing.\")\r\nValueError: `ts` must be strictly increasing or decreasing.\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.10.1 1.24.2 sys.version_info(major=3, minor=11, micro=3, releaselevel='final', serial=0)\r\n```\r\n","comments":[],"labels":["defect","scipy.integrate"]},{"title":"BUG: linalg.sqrtm results different between version 1.11.1 and 1.11.2\/1.11.3","body":"### Describe your issue.\r\n\r\nThere is a difference in the result of `scipy.linalg.sqrtm` between version 1.11.1 and the following versions (to date: 1.11.2 and 1.11.3) for specific matrices. In the following code example, the error when calculating the square root of the matrix is infinitesimal in 1.11.1, but gigantic in 1.11.2 and 1.11.3. It seems that `sqrtm` either produced a wrong result in earlier versions, or newer versions fail to calculate the square root correctly.\r\n\r\nI could not create an example with a random matrix, which is why I made a repository with some data that produces the error: https:\/\/github.com\/akug\/mwe_scipy_sqrtm_bug\r\n\r\nThank you for your time and effort maintaining scipy, it's a great library!\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\n# See the repository at https:\/\/github.com\/akug\/mwe_scipy_sqrtm_bug for sample data\r\nimport numpy as np\r\nfrom scipy.linalg import sqrtm\r\n\r\ndata = np.load(\".\/sample_sqrtm_fails.npz\")[\"array\"]\r\nout, error = sqrtm(data, disp=False)\r\nprint(error)  # error in 1.11.2 and 1.11.3 is about 2.3e+24, but in 1.11.1 is 2.1e-28\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n# There is no error message, it just produces different results between versions\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.3 1.26.0 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-358nkn7q\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp311-cp311\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["+1 I'm hitting this error too when using it to calculate the Frechet distance. From https:\/\/github.com\/mseitzer\/pytorch-fid\/issues\/103 people are downgrading scipy to 1.11.1 to bypass this problem but it doesn't look like a long-term solution","Can you check whether the problem persists with SciPy 1.12?","Probably ~gh-17198~? In which case the behaviour change was supposed to be a bug fix.\n\nEDIT: I meant gh-17918","> Can you check whether the problem persists with SciPy 1.12?\r\n\r\nI believe I was using 1.12 when I first encountered this error, so it's likely the problem persists.","Confirmed locally: `$ git revert 4751485b29` (which is the commit from gh-17918) fixes the MWE from  https:\/\/github.com\/akug\/mwe_scipy_sqrtm_bug. The \"error\" goes down to ~1e-28 and the difference `out@out - data` become of the order of fp noise.\r\n","Sigh, if the fortran torture finishes, I'll rewrite this. It is always causing some minor issues. For now we can use a bit more stringent condition for the real squareroot existence check.","WDYT about committing the revert? IOW, what's worse: zero imaginary parts or plain wrong answers in at least some cases? \n\nThat said, if you see a quick simple fix to get us the best both worlds, I'd be happy to work on it to get it in time for 1.13 (soon (tm)).  ","So what is happening here in these lines \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/715913255756af431786ad587392650bdb6f11ce\/scipy\/linalg\/_matfuncs_sqrtm.py#L173-L178\r\n\r\nis that, if the Schur decomposition returns 2x2 blocks on the diagonal, that means the matrix is going to have a complex squareroot and we convert the real form into complex form with `rsf2csf`. The test for looking at subdiagonal is to test whether some cases like \r\n\r\n```\r\n[ x x x x x x ]\r\n[ 0 x x x x x ]\r\n[ 0 0 x x x x ]\r\n[ 0 0 0 y w x ]\r\n[ 0 0 0 z y x ]\r\n[ 0 0 0 0 0 x ]\r\n```\r\n\r\nwhere z is very small noise or nonnegligible. The best world is actually rewrite this in a more stable fashion so that you don't rely on the subdiagonal but that means pretty much rewriting all as in Higham's paper. So I'd say tightly squeezing the tolerances of `allclose` seems like a trade-off. \r\n\r\n","Not to mention, we lost Nick Higham last month, I was hoping to visit him this year during a business trip. Just a tiny remembrance for all of us and for numerical computing world that pretty much all number crunching code uses his contributions with his colleagues. ","A great loss indeed :-(.","Now that I actually looked at the code, indeed I doubt it is going to work, at all.\r\nThat is, without exposing a tolerance to a user. So yeah, rewriting it with a better algorithm I'm big +1, but am unlikely to get to it myself today or tomorrow (tm). So would be great if you do, when you've cycles :-).\r\nThat said, I think I'll budget some time to debug where it goes bananas in _sqrtm_triu in the OP example. If there is a non-crazy fix, it would still be a useful stopgap for not returning plain wrong results, I'd say.  ","OK, https:\/\/github.com\/scipy\/scipy\/pull\/20212 suggests a minimal fix. It actually does look at subdiagonals only: what happens I believe is that while the lower triangular part is all ~1e-16 or less, some elements on the main diagonal are comparable. And then not calling rsf2csf makes sqrtm return garbage. WDYT Ilhan?","Nice I did a similar thing this morning before work but you beat me to it. I'll check when I get home.","Reopening: the https:\/\/github.com\/scipy\/scipy\/pull\/20218 xfailed the test which is broken by gh-20212 on MacOS. Somehow the test passes on our regular MacOS CI , but fails on the wheel builds, https:\/\/github.com\/scipy\/scipy\/actions\/runs\/8201161926\/job\/22429344365\r\n\r\nIf somebody has the right hardware, these steps would helpful: https:\/\/github.com\/scipy\/scipy\/pull\/19816#issuecomment-1985602807","I can't repro also on Win10 and Linux boxes. ","x-ref https:\/\/github.com\/scipy\/scipy\/pull\/20218#issuecomment-1985671107: with enough luck it's old(-er) OpenBLAS on MacOS specific.","https:\/\/github.com\/scipy\/scipy\/pull\/19816#issuecomment-1986693242 thank you @thalassemia for the details! Let's move the sqrtm discussion here.\r\n\r\nSo what happens on main in the now-xfailed test, https:\/\/github.com\/scipy\/scipy\/blob\/v1.12.0\/scipy\/linalg\/tests\/test_matfuncs.py#L413, after gh-20212: \r\n\r\non MacOS:\r\n\r\n```\r\n(Pdb) d0\r\narray([17.92,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,  0.06,\r\n        0.06])\r\n(Pdb) d1\r\narray([ 0.00000000e+00, -1.55245793e-15,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00])\r\n```\r\n\r\nOn linux,\r\n\r\n```\r\n(Pdb) p d1\r\narray([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -4.77048956e-18,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00])\r\n``` \r\n\r\nSo on both systems non-zero subdiagonal elements d1 are fp noise, but the numerical check `d1 > eps * (d1[1:] + d1[:-1])` is too strict and evaluates to False on linux and to True on Mac because `np.finfo(float).eps` is 2.2e-16.  \r\n\r\nThe branching in sqrtm matches that of rsf2scf: https:\/\/github.com\/scipy\/scipy\/blob\/v1.12.0\/scipy\/linalg\/_decomp_schur.py#L286\r\n\r\nNow that I think of this, it is clearly not right: if all elements of on the main diagonal are the same, eigenvalues of 2x2 diagonal blocks are quadratic in the offdiagonal elements.\r\n\r\nSo taking a step back, we have a real Schur quasi-triangular matrix and we need to figure out if we have 2x2 blocks with complex conjugate eigenvalues, or 1x1 blocks. So maybe we should do exactly this: take all 2x2 blocks and check their eigenvalues?\r\n\r\n```\r\n(Pdb) temp = np.lib.stride_tricks.as_strided(T, (T.shape[0]-1, 2, 2), (T.strides[0] + T.strides[1], T.strides[0], T.strides[1]))\r\n(Pdb) np.linalg.eigvals(temp).imag\r\narray([[ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 2.70242693e-17, -2.70242693e-17],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00],\r\n       [ 0.00000000e+00,  0.00000000e+00]])\r\n``` \r\n\r\nNote however we cannot just compate the imaginary parts of the eigenvalues to the machine epsilon. In the failing case from the OP:\r\n\r\n```\r\n(Pdb) abs(np.linalg.eigvals(temp).imag).max()\r\n4.454957759313309e-16\r\n(Pdb) np.finfo(float).eps\r\n2.220446049250313e-16\r\n```\r\n\r\nwhere the offending pair of eigenvalues is\r\n\r\n```\r\n[-5.17835292e-17+4.45495776e-16j, -5.17835292e-17-4.45495776e-16j]\r\n```\r\n\r\nSo we'd need an arbitrary scale, like \"declare it needs conversion if max(imag(eigenvalues)) > 100 epsilon\" which is ugly and brittle. Anybody has better ideas? Short of redoing the whole thing of course :-).","I've reread all the issues again and I think I am guilty of accepting the original PR modifying to `allclose`.\r\n\r\nThe standard treatment of this is getting the Schur decomposition like we already do and pass this `T` array to LAPACK function `?hseqr` to compute the eigenvalues of a Hessenberg reduced array. Then see if there is anything on the negative real axis. The reason why we should not do it ourselves (reading the diagonal element signs) is exactly what you are showing here. That is, we don't have the necessary balancing and scaling mechanism for very large and very small 2x2 array eigenvalue computations. \r\n\r\nHence I guess we have to either wrap that LAPACK function or write a Cython helper to pass `T` to it without wrapping and read the eigenvalues from it then check the real eigenvalue signs.","FWIW, I fail to see how a lapack routine helps here. Eigenvalues we get easily already. A tricky bit (as in, an arbitrary cutoff) is to decide it's real or complex conjugate pair. This we'll needs to do manually, either way. What am I missing? ","> Eigenvalues we get easily already\r\n\r\nT is possibly a quasi triangular array 2x2 blocks `[[a, b], [c, a]]` (notice the nonsymmetricity). Maybe I am missing it but how do we get eigenvalues from that? \r\n\r\nThe main feature of sqrtm implementations is that if there are no eigenvalues on the negative real axis (or very close to it which is the `tol` part) then it can still compute a real `sqrtm` result even though `T` is still quasi-triangular but not upper triangular. \r\n\r\nIf our `sqrtm_triu` does not accept quasi-triangular arrays then there is no hope and we revert the initial commit until we rewrite the whole thing. \r\n\r\n","Yep it doesn't. So nevermind, revert and rewrite it is \ud83d\ude1e ","> T is possibly a quasi triangular array 2x2 blocks [[a, b], [c, a]] (notice the nonsymmetricity). Maybe I am missing it but how do we get eigenvalues from that?\r\n\r\ntemp = np.lib.stride_tricks.as_strided(T, (T.shape[0]-1, 2, 2), (T.strides[0] + T.strides[1], T.strides[0], T.strides[1]))\r\n(Pdb) np.linalg.eigvals(temp)\r\n","You will get duplicates of the real entries like that but I get your point. However if you are going to do that you can just write `eigvals(T)` and be done with it. The idea is to not to recompute the entire eigenvalue set because you already have the Schur decomposition hence you typically perform an `ordeig` or `ordschur` on `T` (in matlab lingo). Let me give an example\r\n\r\n```python\r\nA = np.array([[-3.97840228e-05, 3.37437654e-05, 1.04757857e-04, 1.04593826e-04, 8.63717292e-05],\r\n   [-1.22091575e-05, 1.24712954e-05, -3.22794806e-05, 8.41674713e-05, 2.39096052e-04],\r\n   [ 7.61995878e-06, -5.66445930e-05,  3.61419367e-06, -2.07497760e-04, 2.47792200e-05],\r\n   [-8.97156784e-05, -1.36794833e-05,  1.82891913e-06,  7.55413982e-05, 2.15268581e-05],\r\n   [ 8.41008795e-05, -1.44581008e-04, -1.40197328e-04, -1.00918200e-05, -5.48242449e-05]])\r\n```\r\n\r\nThe sqrtm of this array is immediately complex in our current implementation. However if you look at its eigenvalues of the schur factor\r\n\r\n```python\r\nT, _ = la.schur(A)\r\n```\r\n\r\n```python\r\nT = np.array([[-9.18202589e-05,  2.80510623e-04,  9.46193910e-05, -4.26616607e-05, -5.17312569e-05],\r\n       [-1.31392393e-04, -9.18202589e-05,  1.01555250e-05, 9.36042457e-05, -7.66560438e-05],\r\n       [ 0.00000000e+00,  0.00000000e+00,  1.14803302e-04, -1.08300726e-04, -1.17926207e-04],\r\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, 3.29279175e-05, -1.07751485e-04],\r\n       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, 1.27258036e-04,  3.29279175e-05]])\r\n```\r\n\r\nWe already know the eigenvalues of this array just by looking at it at the diagonal\r\n\r\n```python\r\nilaynvals = [-9.18202589e-05 + stuff, -9.18202589e-05 - stuff,  1.14803302e-04, 3.29279175e-05 + stuff, 3.29279175e-05 - stuff]\r\nactualvals = la.eigvals(T)\r\nactualvals\r\n[-9.18202589e-05+0.00019198j, -9.18202589e-05-0.00019198j, 1.14803302e-04+0.j,  3.29279175e-05+0.0001171j , 3.29279175e-05-0.0001171j]\r\n```\r\n\r\nThis is due to the [property of `?gees`](https:\/\/netlib.org\/lapack\/explore-html\/d5\/d38\/group__gees_gab48df0b5c60d7961190d868087f485bc.html#gab48df0b5c60d7961190d868087f485bc) that T always returns 2x2 blocks [[a, b], [c, a]] such that $bc < 0$ and eigs are $a \\pm \\sqrt{bc}$.\r\n\r\nJust because we know there is nothing on the negative real axis, we know there is a real valued squareroot for this array. In fact if I use the matlab online documentation \r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/1303842\/3ac51b26-b69f-44e9-9e15-13dab2506701)\r\n\r\n\r\nBut I just realized that our `sqrtm_triu` does not exploit this fact and only checks if all eigenvalues are real or not. Hence our current implementation is suboptimal. \r\n\r\nOK let's accept this fact, then coming back to our current issue, we just want to see whether the exact perfect upper triangular array can be assumed if 2x2 off diagonal blocks are small enough. However that turns out to be really difficult as we find out because for very small blocks and for very large blocks deciding that just by looking at the entry magnitudes is not that easy. \r\n\r\nIn fact I would argue this is a band-aid over an existing band-aid. Even if we manage to pull off finding a nice tolerance there will always be some cases that will give false positives and similar issues will be reported. \r\n\r\nMy proposal; let's go back to the original single band-aid situation and get complaints only about linalg returning complex square roots where there exists a real one. Then at least we know our short-coming and we don't have another band-aid on top of it.\r\n\r\nThe rewrite would resolve this issue structurally.\r\n\r\n","Yeah. How about keeping the current band-aid then? Specifically, I'd suggest we\r\n- keep the code as is ATM\r\n- keep this issue open, maybe rename it.\r\n- keep the xfailed test xfailed.\r\n\r\nWhy:\r\n- the check for exact equality was clearly wrong\r\n- the current state fixes the OP test case\r\n- the current state fails in a specially contrived test case.\r\n- the current band-aid is internally consistent: we rely of real-to-complex Schur conversion iff the conversion actually modifies the matrix.\r\n"],"labels":["defect","scipy.linalg"]},{"title":"ENH: Add mixture_distribution and norm_mixture distributions to stats","body":"In this draft PR I am proposing to add two kinds of distributions.\r\n\r\n   * mixture_distribution: this represents a mixture distribution, basically a distribution that has a pdf defined as a linear combination of other distributions (https:\/\/en.wikipedia.org\/wiki\/Mixture_distribution)\r\n   * norm_mixture: a special case of the first one where all the distributions are normal\r\n\r\nThese kinds of distributions are very common, in particular when one needs to describe data that are the superposition of various sources (e.g. signal and background).\r\n\r\nThe implementation is a bit naive: `mixture_distribution` is implemented as a subclass of `rv_continuous` and it is exposed outside similarly to `rv_histogram`. The main problem is that this new distribution does not take as input numerical parameters, but other (frozen) distributions. In this implementation for example it is complicated to implement the `fit` method, which should fit all the parameters of the component distributions and also the weights used in the combination.\r\n\r\nThis PR does not fulfill all the requests by Scipy, but I would like first to discuss the general idea and the implementation.","comments":["I think it would be better to start with capitalized class names for these cases to avoid the confusion in #19411.\r\nHere the user gets a proper class and instance of it, and not some instance plus frozen distribution instance.\r\n\r\n"],"labels":["scipy.stats","enhancement"]},{"title":"ENH: keep axis functionality in sparse.csr_array.getnnz()","body":"### Is your feature request related to a problem? Please describe.\n\nI require this functionality and don't see an efficient way to hack it.\r\nIt is planned to disappear in version 1-13...\n\n### Describe the solution you'd like.\n\nhave it work like before\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["One way to get the results of the axis parameter from `A.getnnz` is:\r\n```python\r\n# for axis=0\r\nnp.array([vec.nnz for vec in A.T])\r\n# for axis=1\r\nnp.array([vec.nnz for vec in A])\r\n```\r\nHere's some code to test this:\r\n```python\r\nimport numpy as np\r\nimport scipy as sp\r\nA = sp.sparse.csr_array([[4, 5, 0], [7, 0, 0]])\r\nA.getnnz(axis=0)  # -> np.array([2, 1, 0])\r\nA.getnnz(axis=1)  # -> np.array([2, 1], dtype=np.int32)\r\n\r\n# for axis=0\r\nnp.array([vec.nnz for vec in A.T])  # -> np.array([2, 1, 0])\r\n# for axis=1\r\nnp.array([vec.nnz for vec in A])  # -> np.array([2, 1])\r\n```","Hi [dschuldt](https:\/\/github.com\/dschult)! I agree that this provides the same functionality, but it will be much less efficient. The following is a comparison of the runtime of the two:\r\n![759ffeb8-75e5-47ed-a854-1c0099d2320a](https:\/\/github.com\/scipy\/scipy\/assets\/79711833\/699e1e26-bc03-4918-b474-3b61ca4c4bfc)\r\n```\r\nA = csr_array(np.ones((1000,1000)))\r\nnpstat = []\r\nscstat = []\r\nfor i in range(100):\r\n    start=time()\r\n    A.getnnz(axis=0)  # -> np.array([2, 1, 0])\r\n    A.getnnz(axis=1)  # -> np.array([2, 1], dtype=np.int32)\r\n    scstat.append(time()-start)\r\n    start=time()\r\n    np.array([vec.nnz for vec in A.T])  # -> np.array([2, 1, 0])\r\n    np.array([vec.nnz for vec in A])  # -> np.array([2, 1])\r\n    npstat.append(time()-start)\r\n\r\nplt.hist(npstat,color='b', alpha=0.5,label='comprrehended nnz')\r\nplt.hist(scstat,color='r', alpha=0.5,label='getnnz(axis=0)')\r\nplt.xlabel('runtime')\r\nplt.ylabel('samples')\r\nplt.title('runtime of 100 samples')\r\nplt.legend()```\r\n","Good point!   \r\nIf you are willing to give up having it work for all sparse array formats, the following works for `csr` format and is just as fast or very slightly faster in my trials than the original function.  A similar (swapped) version would work for `csc`.\r\n\r\n```python\r\nA = sp.sparse.csr_array(np.ones((500,1000)))\r\nassert (A.getnnz(axis=0) == np.bincount(A.indices, minlength=A.shape[1])).all()\r\nassert (A.getnnz(axis=1) == np.diff(A.indptr)).all()\r\n\r\nB = sp.sparse.csr_array([[4, 5, 0], [7, 0, 0]])\r\n# for axis=0\r\nA0 = np.bincount(B.indices, minlength=B.shape[1])  # -> np.array([2, 1, 0])\r\n# for axis=1\r\nA1 = np.diff(B.indptr)       # -> np.array([2, 1])\r\n\r\n# both are 4.56ms on my machine\r\n%timeit A.getnnz(axis=0)\r\n%timeit np.bincount(A.indices, minlength=A.shape[1])\r\n\r\n# getnnz is 15.8\u00b5s,  np.diff is 9.9\u00b5s\r\n%timeit A.getnnz(axis=1)\r\n%timeit np.diff(A.indptr)\r\n```\r\n\r\nOf course, it is also not as readable in my opinion. I'm not saying it is better... just a possible replacement.\r\n\r\nI'm pretty sure that a reasonable and commonly occuring use-case would be enough to make the developers include this feature somehow.  We're looking for how many people use the `axis` keyword and what the use-case is. What are you using `axis=` for?","Thank you! I will use the `np.diff(A.indptr)`! "],"labels":["enhancement","scipy.sparse"]},{"title":"BENCH: fixed disabled (broken) benchmarks in optimize_milp.py","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\ntowards #19389 \r\n\r\n#### What does this implement\/fix?\r\nTo fix the timing out issue, I have uncommented the MILP solver call and added the max_iter and max_time parameters to limit the solver time:\r\n\r\nmax_iter limits the maximum iterations to 1000\r\nmax_time limits the max solve time to 60 seconds\r\nThis will prevent infinite solve times while still attempting to solve the MILP within reasonable time limits.\r\n\r\nThe assertion is kept to validate that the problem is solved successfully within the enforced time and iteration constraints\r\n\r\n","comments":["The failure in the benchmark means that the benchmarks are failing; earlier ASV versions would not report this and unequivocally succeeded. Once the benchmark is fixed (passes the assertion) everything should work.\r\n\r\nFor more of a discussion see https:\/\/github.com\/scipy\/scipy\/pull\/19052","> The failure in the benchmark means that the benchmarks are failing; earlier ASV versions would not report this and unequivocally succeeded. Once the benchmark is fixed (passes the assertion) everything should work.\r\n> \r\n> For more of a discussion see #19052\r\n\r\nOkk got it, will look forward to updated versions of ASV","> > The failure in the benchmark means that the benchmarks are failing; earlier ASV versions would not report this and unequivocally succeeded. Once the benchmark is fixed (passes the assertion) everything should work.\r\n> > For more of a discussion see #19052\r\n> \r\n> Okk got it, will look forward to updated versions of ASV\r\n\r\nIt is this pr, not ASV that is broken. Please run this locally and it should be clear what the issue is ","to make it fast, do we need to use a smaller problem i.e update \"milp_benchmarks.npz\"?"],"labels":["scipy.optimize","needs-work","Benchmarks"]},{"title":"BENCH: fix disabled (broken) benchmarks","body":"In #19052 some broken benchmarks were disabled eg. https:\/\/github.com\/scipy\/scipy\/blob\/d7207038d8668ee5f7faf4bc4b6e3566290138f7\/benchmarks\/benchmarks\/optimize_milp.py#L51-L54\r\n\r\nIt would be nice to fix these. Time allowing, I hope to update this issue with a tracker containing all the disabled benchmarks. However, for now, this serves as a reminder, and there is nothing to stop anyone from working on this immediately.\r\n\r\ncc @h-vetinari ","comments":[],"labels":["good first issue","maintenance","Benchmarks"]},{"title":"BUG: grey_erosion discrepancy in different calling styles","body":"### Describe your issue.\n\nOutput mismatch when calling `grey_erosion(im, structure=kernel)` vs `grey_erosion(im, (filt_sz, filt_sz))` when kernel is defined as flat, square array of ones. The expectations is that the values would match.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.ndimage import grey_erosion\r\nimport numpy as np\r\n\r\nim = np.random.rand(50, 50)\r\n\r\nfilt_sz = 7\r\nkernel = np.ones((filt_sz, filt_sz))\r\n\r\nout_with_kernel = grey_erosion(im, structure=kernel)\r\nout_with_sz = grey_erosion(im, (filt_sz, filt_sz))\r\n\r\nassert (abs(out_with_kernel - out_with_sz) < 1e-5).all(), 'Mismatch detected'\n```\n\n\n### Error message\n\n```shell\nMismatch detected.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.9.1 1.24.3 sys.version_info(major=3, minor=8, micro=18, releaselevel='final', serial=0)\r\nlapack_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/include']\r\nlapack_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/include']\r\nblas_mkl_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/include']\r\nblas_opt_info:\r\n    libraries = ['mkl_rt', 'pthread']\r\n    library_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['\/Users\/rakshit\/opt\/anaconda3\/include']\n```\n","comments":[],"labels":["defect","scipy.ndimage"]},{"title":"BUG: Crash in `MessageStream` due to incorrect use of `__dealloc__`","body":"### Describe your issue.\r\n\r\nThere's a segfault when using `ConvexHull` (In particular, the segfault happens in `MessageStream` in some cases).\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport signal\r\nfrom scipy.spatial import ConvexHull\r\nimport numpy as np\r\n\r\ndef alarm_handler(*args, **kwargs):\r\n    raise Exception('ALARM')\r\n\r\nsignal.signal(signal.SIGALRM, alarm_handler)\r\nsignal.alarm(1)\r\nrng = np.random.default_rng()\r\npoints = rng.random((30, 2))\r\nhull = ConvexHull(points)\r\n```\r\n\r\nAnd a minor change to `MessageStream` to simulate slowness in the constructor:\r\n```\r\ndiff --git a\/scipy\/_lib\/messagestream.pyx b\/scipy\/_lib\/messagestream.pyx\r\nindex 318b6165f..a152e576a 100644\r\n--- a\/scipy\/_lib\/messagestream.pyx\r\n+++ b\/scipy\/_lib\/messagestream.pyx\r\n@@ -4,6 +4,7 @@ from cpython cimport PyBytes_FromStringAndSize\r\n\r\n import os\r\n import tempfile\r\n+import time\r\n\r\n cdef extern from \"messagestream.h\":\r\n     stdio.FILE *messagestream_open_memstream(char **, size_t *)\r\n@@ -17,6 +18,8 @@ cdef class MessageStream:\r\n     \"\"\"\r\n\r\n     def __cinit__(self):\r\n+        time.sleep(int(os.getenv('MSG_SLEEP', default=2)))\r\n+\r\n```\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nSegfault\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\nI believe this was introduced by https:\/\/github.com\/scipy\/scipy\/commit\/c4df6497a28e090796f3f012072673e9ebcb87ff (Which seems to be in v1.9.0+), in particular, when `self.close()` is called now by `__dealloc__` instead of `__del__`.\r\n\r\nWhat *appears* to be happening is that if `__cinit__` is interrupted (for example, with a signal, but it doesn't necessarily need to be a signal-caused exception -- it can be any exception thrown while the constructor is running), Cython will attempt to dealloc the object. But because the object is not fully allocated(? I'm not sure if this part is correct), calling its member method (`self.close()`) leads to a crash. The Cython documentation recommends that `__dealloc__` shouldn't be calling any methods in general:\r\n\r\n> You need to be careful what you do in a __dealloc__() method. By the time your __dealloc__() method is called, the object may already have been partially destroyed and may not be in a valid state as far as Python is concerned, so you should avoid invoking any Python operations which might touch the object. In particular, don\u2019t call any other methods of the object or do anything which might cause the object to be resurrected. It\u2019s best if you stick to just deallocating C data.\r\n\r\n\r\nInstead of `__dealloc__`, I believe we should switch it back to using `__del__` and calling `close()` from a python context (`def close()`) instead, which is safe to do. Something like this:\r\n\r\n\r\n```\r\ndiff --git a\/scipy\/_lib\/messagestream.pxd b\/scipy\/_lib\/messagestream.pxd\r\nindex c15eaee95..aa78243dd 100644\r\n--- a\/scipy\/_lib\/messagestream.pxd\r\n+++ b\/scipy\/_lib\/messagestream.pxd\r\n@@ -8,4 +8,3 @@ cdef class MessageStream:\r\n     cdef bint _removed\r\n     cdef size_t _memstream_size\r\n     cdef char *_memstream_ptr\r\n-    cpdef close(self)\r\ndiff --git a\/scipy\/_lib\/messagestream.pyx b\/scipy\/_lib\/messagestream.pyx\r\nindex 318b6165f..7c5867652 100644\r\n--- a\/scipy\/_lib\/messagestream.pyx\r\n+++ b\/scipy\/_lib\/messagestream.pyx\r\n@@ -4,6 +4,7 @@ from cpython cimport PyBytes_FromStringAndSize\r\n\r\n import os\r\n import tempfile\r\n+import time\r\n\r\n cdef extern from \"messagestream.h\":\r\n     stdio.FILE *messagestream_open_memstream(char **, size_t *)\r\n@@ -17,6 +18,8 @@ cdef class MessageStream:\r\n     \"\"\"\r\n\r\n     def __cinit__(self):\r\n+        time.sleep(int(os.getenv('MSG_SLEEP', default=2))) # NOTE: This is test code and should be removed\r\n+\r\n         # Try first in-memory files, if available\r\n         self._memstream_ptr = NULL\r\n         self.handle = messagestream_open_memstream(&self._memstream_ptr,\r\n@@ -42,7 +45,7 @@ cdef class MessageStream:\r\n                 os.remove(self._filename)\r\n             raise OSError(f\"Failed to open file {self._filename}\")\r\n\r\n-    def __dealloc__(self):\r\n+    def __del__(self):\r\n         self.close()\r\n\r\n     def get(self):\r\n@@ -78,7 +81,7 @@ cdef class MessageStream:\r\n     def clear(self):\r\n         stdio.rewind(self.handle)\r\n\r\n-    cpdef close(self):\r\n+    def close(self):\r\n         if self.handle != NULL:\r\n             stdio.fclose(self.handle)\r\n             self.handle = NULL\r\n```\r\n","comments":["As another observation, I think (anything) throwing in the constructor is kind of bad, and this scenario is no exception. It's possible that the change I proposed above won't crash, but `close()` may not get called since the object is only partially constructed(?) and we might leak the temporary file, if one is used. It's bad, but probably not a big deal. Alternatively we can move the whole close definition into `__del__()` but even then I'm not certain it would run properly since it accesses the object's members (which might not exist).\r\n\r\nWith the proposed change, I'm getting this (No segfaults, but `close()` isn't called so we can leak the temp file):\r\n\r\n```\r\npython3 test.py\r\nAttributeError: 'NoneType' object has no attribute 'close'\r\nException ignored in: 'scipy.spatial._qhull._Qhull.__dealloc__'\r\nTraceback (most recent call last):\r\n  File \"\/var\/www\/test.py\", line 14, in <module>\r\n    hull = ConvexHull(points)\r\nAttributeError: 'NoneType' object has no attribute 'close'\r\n```\r\n\r\nEdit: I just saw some nuances in the original PR that introduced this, so perhaps the proposed fix isn't good \ud83d\ude13 . I'll leave it up to the experts.","Thanks for the detailed report @Toad2186. That was indeed a bit nuanced, as the discussion in gh-14328 shows. Cc @tirthasheshpatel and @mckib2. ","I opened a ticket with Cython who gave some additional insights:\r\nhttps:\/\/groups.google.com\/u\/1\/g\/cython-users\/c\/x5c1AB5oPOg\r\n\r\ntl;dr: Seems like there's too much happening in the native-alloc and dealloc functions. They gave some pretty useful clarifications and also suggested possibly a refactor which might be a big lift -- I believe it's also sufficient to delineate the very few things that needs to happen in `__cinit__` and `__dealloc__`, and put most of the body into the python version: `__init__` and `__del__`."],"labels":["defect","scipy.spatial"]},{"title":"BUG: a hang on 32-bit Linux with `scipy-openblas32` in CI","body":"Found when working on gh-19380. Reproducer:\r\n```\r\ndocker pull quay.io\/pypa\/manylinux2014_i686\r\ndocker run -v $(pwd):\/scipy --platform=linux\/i386 quay.io\/pypa\/manylinux2014_i686 \/bin\/bash -c \"cd \/scipy && \\\r\n        uname -a && \\\r\n        python3.9 -m venv test && \\\r\n        source test\/bin\/activate && \\\r\n        python -m pip install doit click rich_click pydevtool meson ninja && \\\r\n        python -m pip install numpy==1.22.4 cython==0.29.35 pybind11 pytest pytest-timeout pytest-xdist pytest-env 'Pillow<10.0.0' mpmath pythran pooch meson hypothesis scipy-openblas32 && \\\r\n        python dev.py build --use-scipy-openblas && \\\r\n        python dev.py test -v -s cluster\"\r\n```\r\n\r\nThe 32-bit Linux CI job we have passes on `main` currently - that job is using OpenBLAS `0.3.21.dev`.\r\n\r\nThe one BLAS function that is called inside `kmeans2` is `sgemm`\/`dgemm`: https:\/\/github.com\/scipy\/scipy\/blob\/8a130d34ed741995b2d98196e5ff24969e8d5ca1\/scipy\/cluster\/_vq.pyx#L46-L53\r\n\r\nCould be a bug in that Cython code of course. EDIT: other `cluster` and `linalg` tests are also hanging - it's not a code issue.","comments":["Ah, I think there is nothing wrong with either our Cython code or OpenBLAS, but we're getting bitten here by `dlopen`'ing the OpenBLAS shared library with `ctypes` and `RTLD_GLOBAL`. xref https:\/\/github.com\/MacPython\/openblas-libs\/pull\/87#issuecomment-1277615608 and https:\/\/github.com\/MacPython\/openblas-libs\/pull\/120.\r\n\r\nThat job builds `numpy` 1.22.4 from source, and in `main` there's some manual manipulation to put `openblas.so` in `\/usr\/local\/lib\/` for the SciPy build - which has a side effect of having the NumPy build picking up that exact same wheel. While in the Docker reproducer above, NumPy fails to find a BLAS library and hence uses `lapack_lite`. ","Indeed, tests pass if I build both numpy and scipy against the same `scipy-openblas32`: \r\n```\r\ndocker run -v $(pwd):\/scipy --platform=linux\/i386 quay.io\/pypa\/manylinux2014_i686 \/bin\/bash -c \"cd \/scipy && \\\r\n        python3.9 -m venv venv && \\\r\n        source venv\/bin\/activate && \\\r\n        python -m pip install -U pip && \\\r\n        python -m pip install doit click rich_click pydevtool meson-python ninja scipy-openblas32 cython==3.0.2 && \\\r\n        python -c 'import scipy_openblas32 as o; print(o.get_pkg_config())' > scipy-openblas.pc && \\\r\n        export PKG_CONFIG_PATH=\/scipy && \\\r\n        python -m pip install git+https:\/\/github.com\/rgommers\/numpy.git@debug-linux32-issue --no-build-isolation && \\\r\n        echo 'import scipy_openblas32' > venv\/lib\/python3.9\/site-packages\/numpy\/_distributor_init_local.py && \\\r\n        python -m pip install pybind11 pytest pytest-timeout pytest-xdist pytest-env pythran pooch hypothesis && \\\r\n        python dev.py build --use-scipy-openblas && \\\r\n        python dev.py test -v -s cluster\"\r\n```","Regarding the BLAS (and LAPACK) libraries used by NumPy and SciPy installed into the same env:\r\n\r\n1. A single BLAS library that both NumPy and SciPy are built against and depend upon is the best situation (conda-forge, Spack, Nix, Homebrew and every Linux distro are in this situation),\r\n2. Both NumPy and SciPy vendoring a name-mangled BLAS library (via `auditwheel` & co) works,\r\n3. One of NumPy\/SciPy containing a vendored name-mangled BLAS and the other one using `scipy-openblas32`\/`64` works too,\r\n4. Having both using the same `scipy-openblas32`\/`64` works too (EDIT: in CI at least, it isn't releasable because it may run into (5) when any other library uses BLAS too),\r\n5. Having both use _different and non-mangled_ BLAS's does not work, it leads to symbol clashes.\r\n\r\nNext steps:\r\n- Getting rid of `tools\/openblas_support.py` would be great. This can be done, I think it only needs the ability in `scipy_openblas32` to write out a pkg-config file that doesn't do the `libopenblas_python` thing but contains the normal link flags. Then it's a 1:1 replacement, and we can simply use `pip` instead of the hardcoded logic for downloading the right binary in `openblas_support.py` without changing anything else to how we build wheels.\r\n- After that, I think we need symbol prefixes (maybe `scipywheel_`?) in the scipy-openblas wheels, to avoid the problem with (5) above. I was originally not in favor of symbol prefixes to not make things harder - but I think I was wrong, they're necessary.\r\n  - It's not entirely clear to me if that will be enough - we also have to worry about `libgcc` & co.\r\n\r\nThe wheels are still a big win already though; upgrading OpenBLAS used to be a pain, and now it comes for free, and we can switch between versions with a familiar `pip install ...` interface.","> Getting rid of `tools\/openblas_support.py` would be great. This can be done, I think it only needs the ability in `scipy_openblas32` to write out a pkg-config file that doesn't do the `libopenblas_python` thing but contains the normal link flags. Then it's a 1:1 replacement, and we can simply use `pip` instead of the hardcoded logic for downloading the right binary in `openblas_support.py` without changing anything else to how we build wheels.\r\n\r\n@mattip do you think this is right? There is one difference between `libopenblas.so` in the tarballs and `python_libopenblas.so` in the wheel, namely that it went through `auditwheel` already and hence the public BLAS\/LAPACK symbols have protected visibility. Which means they should remain usable.\r\n\r\nThat said, I think we need to figure out this issue first - in particular, decide if we need a symbol prefix after all. ","The current pkg-config file is created [here](https:\/\/github.com\/MacPython\/openblas-libs\/blob\/80fcba37ab19b00d8087c76234f23c53a4fcfff9\/local\/scipy_openblas64\/__init__.py#L58) and can emit whatever flags we choose. Right now it is set up to emit the `64_` suffix and set the numpy define for 64-bit interfaces, and not do that for 32-bit interfaces. \r\n\r\nI think it should be relatively straightforward to package the scipy-openblas sharedobjects\/dlls into the scipy wheel. I am not sure why we would need a new prefix since cibuildwheel uses [auditwheel\/delocate\/devolve](https:\/\/cibuildwheel.readthedocs.io\/en\/stable\/options\/#repair-wheel-command) to mangle the file names and package them into the wheel the same as now. So if there is currently no conflict, there should be no conflict new conflict.","Ahh, I think I see. If both scipy and numpy are installed through wheels, then there is no conflict. The conflict is specifically when one uses the scipy-openblas wheel in development mode, which uses `RT_GLBL` in `_distributor_init` to pre-load the OpenBLAS symbols. Technically we could add prefix symbols to the scipy-openblas builds, it might cause some confusion when debugging.","> The conflict is specifically when one uses the scipy-openblas wheel in development mode,\r\n\r\nYes indeed - and it may also happen if the environment contains a third package which also uses a BLAS library. E.g. a conda env with pytorch installed."],"labels":["defect","scipy.cluster"]},{"title":"Cross-Compile scipy for riscv target","body":"i am cross-compiling scipy for riscv target. i am building on ubuntu and installed riscv64-linux-gnu-gfortran, riscv64-linux-gnu-gcc\r\n, riscv64-linux-gnu-g++. i have configured openblas and lapack libs. and when i build i am getting errors. please help me get through this. i am attaching necessary files for reference.\r\n[meson_log.txt](https:\/\/github.com\/scipy\/scipy\/files\/12889855\/meson_log.txt)\r\n[meson_output_error.txt](https:\/\/github.com\/scipy\/scipy\/files\/12889856\/meson_output_error.txt)\r\n[scipy_cross_config.ini.txt](https:\/\/github.com\/scipy\/scipy\/files\/12889857\/scipy_cross_config.ini.txt)\r\n\r\n\r\n\r\nscipy$ \/home\/vboxuser\/.local\/bin\/meson setup build --cross-file=cross_config.ini\r\nDEPRECATION: c_args in the [properties] section of the machine file is deprecated, use the [built-in options] section.\r\nDEPRECATION: c_link_args in the [properties] section of the machine file is deprecated, use the [built-in options] section.\r\nDEPRECATION: fortran_args in the [properties] section of the machine file is deprecated, use the [built-in options] section.\r\nDEPRECATION: fortran_link_args in the [properties] section of the machine file is deprecated, use the [built-in options] section.\r\nThe Meson build system\r\nVersion: 1.2.2\r\nSource dir: \/home\/vboxuser\/k510_build\/k510_buildroot\/package\/scipy\r\nBuild dir: \/home\/vboxuser\/k510_build\/k510_buildroot\/package\/scipy\/build\r\nBuild type: cross build\r\nProject name: SciPy\r\nProject version: 1.12.0.dev0\r\nC compiler for the host machine: riscv64-linux-gnu-gcc (gcc 9.4.0 \"riscv64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04) 9.4.0\")\r\nC linker for the host machine: riscv64-linux-gnu-gcc ld.bfd 2.34\r\nC++ compiler for the host machine: riscv64-linux-gnu-g++ (gcc 9.4.0 \"riscv64-linux-gnu-g++ (Ubuntu 9.4.0-1ubuntu1~20.04) 9.4.0\")\r\nC++ linker for the host machine: riscv64-linux-gnu-g++ ld.bfd 2.34\r\nCython compiler for the host machine: cython (cython 3.0.3)\r\nC compiler for the build machine: cc (gcc 9.4.0 \"cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\r\nC linker for the build machine: cc ld.bfd 2.34\r\nC++ compiler for the build machine: c++ (gcc 9.4.0 \"c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\r\nC++ linker for the build machine: c++ ld.bfd 2.34\r\nCython compiler for the build machine: cython (cython 3.0.3)\r\nBuild machine cpu family: x86_64\r\nBuild machine cpu: x86_64\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nTarget machine cpu family: x86_64\r\nTarget machine cpu: x86_64\r\nProgram python3 found: YES (\/usr\/bin\/python3)\r\nFound Pkg-config: NO\r\nRun-time dependency python found: YES 3.8\r\nProgram cython found: YES (\/home\/vboxuser\/.local\/bin\/cython)\r\nCompiler for C supports arguments -Wno-unused-but-set-variable: YES \r\nCompiler for C supports arguments -Wno-unused-function: YES \r\nCompiler for C supports arguments -Wno-conversion: YES \r\nCompiler for C supports arguments -Wno-misleading-indentation: YES \r\nLibrary m found: NO\r\nFortran compiler for the host machine: riscv64-linux-gnu-gfortran (gcc 9.4.0 \"GNU Fortran (Ubuntu 9.4.0-1ubuntu1~20.04) 9.4.0\")\r\nFortran linker for the host machine: riscv64-linux-gnu-gfortran ld.bfd 2.34\r\nCompiler for Fortran supports arguments -Wno-conversion: YES \r\nChecking if \"-Wl,--version-script\" : links: NO \r\nProgram pythran found: YES (\/home\/vboxuser\/.local\/bin\/pythran)\r\nFound CMake: NO\r\nRun-time dependency xsimd found: NO (tried pkgconfig and cmake)\r\nRun-time dependency threads found: YES\r\nLibrary npymath found: YES\r\nLibrary npyrandom found: YES\r\npybind11-config found: YES (\/home\/vboxuser\/.local\/bin\/pybind11-config) 2.11.1\r\nRun-time dependency pybind11 found: YES 2.11.1\r\nRun-time dependency openblas found: NO (tried pkgconfig and cmake)\r\nRun-time dependency openblas found: NO \r\n\r\nscipy\/meson.build:159:9: ERROR: Dependency lookup for OpenBLAS with method 'pkgconfig' failed: Pkg-config binary for machine 1 not found. Giving up.\r\n\r\nA full log can be found at \/home\/vboxuser\/k510_build\/k510_buildroot\/package\/scipy\/build\/meson-logs\/meson-log.txt\r\n","comments":["Hi @srianvesh, thanks for the report. When you cross compiled OpenBLAS, it produced a `openblas.pc` file. The problem here is that the build now cannot find that installed file. You should run `make install` (with the correct flags) for OpenBLAS, so it gets installed in a directory tree matching what you'll have on your RISC-V machine. That will then contain the `openblas.so` shared library and `openblas.pc`. \r\n\r\n\r\nYour native `pkg-config` binary should then be pointed at the directory that that `openblas.pc` file is in, via `export PKG_CONFIG_PATH=\/path\/to\/directory\/pcfile\/is\/in` (it's possible your cross toolchain does this automatically already, not sure). Once you've done that, running\r\n```\r\n$ pkg-config --cflags openblas\r\n```\r\nshould return a compile flag starting with `-I`, rather than a \"not found\" error message.","Actually i am trying to install scipy from my buildroot which has external toolchain. so when i select scipy package, the installation fails as lapack, blas require fortran support in the external toolchain. The external toolchain i have doesn't have fortran as supproted langauage. so i am trying to cross-compile the scipy for RISCV64 target. ","After modifying the cross-config.inc with target, the meson build has generated output without error.(Not sure this is the desired output)However, \r\nwhen running the command $ninja -C build, target python errors are coming.\r\n**meson build**\r\ncommand: $: ~\/.local\/bin\/meson setup build  --cross-file=scipy_cross_config.ini\r\n[scipy_cross_config.ini.txt](https:\/\/github.com\/scipy\/scipy\/files\/12916951\/scipy_cross_config.ini.txt)\r\n[meson_output.txt](https:\/\/github.com\/scipy\/scipy\/files\/12916959\/meson_output.txt)\r\n\r\n**ninja build**\r\ncommand: $: ninja -C build\r\noutput\/-error: \r\nninja: Entering directory `build'\r\n[1\/1607] Compiling C object scipy\/lib_fortranobject.a.p\/.._.._.._.local_lib_python3.10_site-packages_numpy_f2py_src_fortranobject.c.o\r\nFAILED: scipy\/lib_fortranobject.a.p\/.._.._.._.local_lib_python3.10_site-packages_numpy_f2py_src_fortranobject.c.o \r\nriscv64-linux-gnu-gcc -Iscipy\/lib_fortranobject.a.p -Iscipy -I..\/scipy -I..\/..\/..\/.local\/lib\/python3.10\/site-packages\/numpy\/core\/include -I..\/..\/..\/.local\/lib\/python3.10\/site-packages\/numpy\/f2py\/src -I\/usr\/include\/python3.10 -I\/usr\/include\/x86_64-linux-gnu\/python3.10 -I\/opt\/include -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c99 -O2 -g -Wno-unused-but-set-variable -Wno-unused-function -Wno-conversion -Wno-misleading-indentation -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_9_API_VERSION -MD -MQ scipy\/lib_fortranobject.a.p\/.._.._.._.local_lib_python3.10_site-packages_numpy_f2py_src_fortranobject.c.o -MF scipy\/lib_fortranobject.a.p\/.._.._.._.local_lib_python3.10_site-packages_numpy_f2py_src_fortranobject.c.o.d -o scipy\/lib_fortranobject.a.p\/.._.._.._.local_lib_python3.10_site-packages_numpy_f2py_src_fortranobject.c.o -c ..\/..\/..\/.local\/lib\/python3.10\/site-packages\/numpy\/f2py\/src\/fortranobject.c\r\nIn file included from \/usr\/include\/python3.10\/Python.h:8,\r\n                 from ..\/..\/..\/.local\/lib\/python3.10\/site-packages\/numpy\/f2py\/src\/fortranobject.h:7,\r\n                 from ..\/..\/..\/.local\/lib\/python3.10\/site-packages\/numpy\/f2py\/src\/fortranobject.c:2:\r\n\/usr\/include\/python3.10\/pyconfig.h:88:14: fatal error: riscv64-linux-gnu\/python3.10\/pyconfig.h: No such file or directory\r\n   88 | #    include <riscv64-linux-gnu\/python3.10\/pyconfig.h>\r\n      |              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n[2\/1607] Generating scipy\/linalg.pxd with a custom command\r\n[3\/1607] Generating scipy\/__init__.py with a custom command\r\n[4\/1607] Generating scipy\/_lib\/__init__.py with a custom command\r\n[5\/1607] Generating scipy\/special.pxd with a custom command\r\n[6\/1607] Generating scipy\/optimize.pxd with a custom command\r\n[7\/1607] Generating scipy\/_lib\/_ccallback_c.pxd with a custom command\r\n[8\/1607] Generating scipy\/generate-version with a custom command\r\nninja: build stopped: subcommand failed.\r\n\r\n[ninja_output.txt](https:\/\/github.com\/scipy\/scipy\/files\/12916986\/ninja_output.txt)\r\n\r\nPlease guide how to proceed.\r\nMeson is taking host python, however error is displayed because of target python packages un-availability.\r\n\r\n\r\n\r\n"],"labels":["Build issues"]},{"title":"Cirrus CI jobs don't actually merge `main`","body":"See [this log](https:\/\/cirrus-ci.com\/task\/5415734172450816?logs=clone#L23) for an example. It shows:\r\n```\r\nAutomatic merge went well; stopped before committing as requested\r\n```\r\nand the job fails further down with an issue that's fixed already in `main`. The problem stems from the `--no-commit` here: https:\/\/github.com\/scipy\/scipy\/blob\/f607e5df463bd164765769342d4659111d52a86f\/ci\/cirrus_general_ci.yml#L23-L26\r\n\r\n@andyfaff glancing at `git blame`, I'm not clear on what happened there - maybe it got broken ~8 months ago in the Boost PR that touched those lines after your last change to deal with Alpine. Would you be able to have a look at that?","comments":["The cirrus clone has the following logic:\r\n\r\n- git clone the main https:\/\/github.com\/scipy\/scipy.git repo.\r\n- fetch the PR changeset (git fetch origin pull\/$CIRRUS_PR\/head:pull\/$CIRRUS_PR)\r\n- checkout the main branch of the repo, this should be fully up to date\r\n- merge in the PR changeset (with a `-no-commit`) on to the tip of the main branch. It shouldn't be necessary to do a commit here.\r\n- update the submodules\r\n- run the tests.\r\n\r\nI tried this process on PR 19364 on my own computer and can see the change belonging to that PR in the codebase, although the commits do not show up in the log because there is no commit.\r\n\r\nHowever, I've tried reproducing the run contained in the log file and am experiencing issues with merging the changeset, there is a merge conflict with the following:\r\n\r\n```\r\ngit clone https:\/\/github.com\/scipy\/scipy.git \/src\r\ncd src\r\ngit fetch origin pull\/19371\/head:pull\/19371\r\ngit checkout main\r\ngit -c user.email=\"you@example.com\" merge --no-commit pull\/19371\r\n```\r\n\r\nSo I don't know how the log says that the automatic merge worked.  I also tried:\r\n\r\n```\r\ngit fetch origin\r\ngit checkout main\r\ngit rebase origin\/main\r\ngit checkout pull\/19371\r\ngit rebase origin\/main\r\n```\r\nThe rebase step also produces a merge conflict. I don't know how Github CI manages the clone and merge.\r\n\r\n","@rgommers, can you try merging the changeset in 19371 on your local computer?","Yes, it has a conflict, as the GitHub UI also indicates in gh-19731:\r\n```\r\n$ gh pr checkout 19371  # creates `ansari_anp` branch and switching to it\r\n$ git checkout main\r\n$ git merge ansari_anp \r\nAuto-merging scipy\/stats\/tests\/test_axis_nan_policy.py\r\nCONFLICT (content): Merge conflict in scipy\/stats\/tests\/test_axis_nan_policy.py\r\nAutomatic merge failed; fix conflicts and then commit the result.\r\n```\r\n\r\n`--no-commit` makes no difference here, either way there's a conflict marker in the file:\r\n```diff\r\n<<<<<<< HEAD\r\n    (stats.entropy, tuple(), dict(), 1, 1, False, lambda x: (x,)),\r\n    (stats.entropy, tuple(), dict(), 2, 1, True, lambda x: (x,))\r\n=======\r\n    (stats.ansari, tuple(), {}, 2, 2, False, None),\r\n>>>>>>> ansari_anp\r\n]\r\n```\r\nIf that happens, the CI job should simply fail to run I think, rather than proceed and fail halfway through. How about using `git status --porcelain` here to check that (see https:\/\/stackoverflow.com\/questions\/5139290\/how-to-check-if-theres-nothing-to-be-committed-in-the-current-branch)?\r\n","I'm surprised that the failed merge didn't stop the run"],"labels":["CI"]},{"title":"BUG: Expectation computation with vector mean fails","body":"### Describe your issue.\n\nMany methods of `scipy.stats.norm` allow you to pass vectors for `scale` and `loc`. For example,\r\n\r\n```python\r\nnorm(loc=[1, 2, 3]).pdf(0.5)\r\n# >>> array([0.35206533, 0.1295176 , 0.0175283 ])\r\n```\r\n\r\nI'd like to do the same for computing expectation, ie.\r\n\r\n```python\r\nnorm(loc=[1, 2, 3]).expect()\r\n# Expected output: array([1, 2, 3])\r\n```\r\n\r\nThe code instead errors (see below).\n\n### Reproducing Code Example\n\n```python\nfrom scipy.stats import norm\r\nimport numpy as np\r\n\r\nnorm(loc=[1, 2, 3]).expect()\r\n# TypeError: can only concatenate list (not \"float\") to list\r\n\r\nnorm(loc=np.array([1, 2, 3])).expect()\r\n# ValueError: operands could not be broadcast together with shapes (3,) (2,)\n```\n\n\n### Error message\n\n```shell\nFor the first example:\r\n\r\nFile ~\/tt_newsvendor\/.venv\/lib\/python3.11\/site-packages\/scipy\/stats\/_distn_infrastructure.py:2896, in rv_continuous.expect(self, func, args, loc, scale, lb, ub, conditional, **kwds)\r\n   2894         return func(x) * self.pdf(x, *args, **lockwds)\r\n   2895 if lb is None:\r\n-> 2896     lb = loc + _a * scale\r\n   2897 if ub is None:\r\n   2898     ub = loc + _b * scale\r\n\r\nTypeError: can only concatenate list (not \"float\") to list\r\n\r\n\r\nFor the second example:\r\nFile ~\/tt_newsvendor\/.venv\/lib\/python3.11\/site-packages\/scipy\/stats\/_distn_infrastructure.py:534, in rv_frozen.expect(self, func, lb, ub, conditional, **kwds)\r\n    532     return self.dist.expect(func, a, loc, lb, ub, conditional, **kwds)\r\n    533 else:\r\n--> 534     return self.dist.expect(func, a, loc, scale, lb, ub,\r\n    535                             conditional, **kwds)\r\n\r\nFile ~\/tt_newsvendor\/.venv\/lib\/python3.11\/site-packages\/scipy\/stats\/_distn_infrastructure.py:2908, in rv_continuous.expect(self, func, args, loc, scale, lb, ub, conditional, **kwds)\r\n   2906 alpha = 0.05  # split body from tails at probability mass `alpha`\r\n   2907 inner_bounds = np.array([alpha, 1-alpha])\r\n-> 2908 cdf_inner_bounds = cdf_bounds[0] + invfac * inner_bounds\r\n   2909 c, d = loc + self._ppf(cdf_inner_bounds, *args) * scale\r\n   2911 # Do not silence warnings from integration.\r\n\r\nValueError: operands could not be broadcast together with shapes (3,) (2,)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.0 sys.version_info(major=3, minor=11, micro=2, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/opt\/arm64-builds\/include\r\n    lib directory: \/opt\/arm64-builds\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/opt\/arm64-builds\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.11.0\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 14.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.1.0\r\n  pythran:\r\n    include directory: ..\/..\/pip-build-env-iq776ekw\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cibw-run-dp6he_rc\/cp311-macosx_arm64\/build\/venv\/bin\/python\r\n  version: '3.11'\n```\n","comments":[],"labels":["defect","scipy.stats"]},{"title":"BUG:  fsolve + pchip + max, can't find root ","body":"### Describe your issue.\r\n\r\nI originally posted this on[ stack ](https:\/\/stackoverflow.com\/questions\/77259842\/fsolve-pchip-max-not-finding-root) thinking the error might be mine but now I think it's some sort of bug. \r\n\r\nI am trying to find the roots of multiple equations over a grid (want the equations to be 0 evaluated at the points in the grid) and am stacking all the function values as a vector. I was able to do this in Matlab,\r\n\r\nAfter playing around with every aspect of the code, I discovered the max functions are giving it trouble. The max terms are squared to make them a smooth function, but when they are squared, fsolve just returns the initial guess. Ironically, when the squared term is removed, fsolve does return something besides the initial guess, but it's not the root. When the max terms (and mu terms) are completely removed, fsolve successfully finds a root. \r\n\r\nThere is also a pchip being used, which may add some complications, but the same thing is happening when I use a spline \r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.interpolate import pchip, Akima1DInterpolator\r\nfrom scipy.interpolate import InterpolatedUnivariateSpline as spline\r\n\r\ndef ap(x, alpha, beta, delta, r, w, kgrid, zgrid, piz):\r\n    m = len(kgrid)\r\n    pp1 = pchip(kgrid, x[:m])\r\n    pp2 = pchip(kgrid,x[m:2*m])\r\n    #pp1 = spline(kgrid, x[:m],k=3)\r\n    #pp2 = spline(kgrid,x[m:2*m],k=3)\r\n    res = np.zeros(len(x))\r\n    for i in range(m):\r\n        kp1 = x[i]\r\n        c = (1 + r - delta) * kgrid[i] + w * zgrid[0] - kp1\r\n        kpp1 = pp1(kp1)\r\n        cp1 = (1 + r - delta) * kp1 + w * zgrid[0] - kpp1\r\n        kpp2 = pp2(kp1)\r\n        cp2 = (1 + r - delta) * kp1 + w * zgrid[1] - kpp2\r\n        mu1 = x[i + 2*m]\r\n        res[i] = c**(-1) - max(mu1, 0)**2 - beta * (1 + r - delta) * (piz[0, 0] * cp1**(-1) + piz[0, 1] * cp2**(-1))\r\n        kp2 = x[i + m]\r\n        c = (1 + r - delta) * kgrid[i] + w * zgrid[1] - kp2\r\n        kpp1 = pp1(kp2)\r\n        cp1 = (1 + r - delta) * kp2 + w * zgrid[0] - kpp1\r\n        kpp2 = pp2(kp2)\r\n        cp2 = (1 + r - delta) * kp2 + w * zgrid[1] - kpp2\r\n        mu2 = x[i + 3*m]\r\n        res[i + m] = c**(-1) - max(mu2, 0)**2 - beta * (1 + r - delta) * (piz[1, 0] * cp1**(-1) + piz[1, 1] * cp2**(-1))\r\n        res[i + 2*m] = max(-mu1, 0)**2 - kp1\r\n        res[i + 3*m] = max(-mu2, 0)**2 - kp2\r\n    return res\r\n````\r\n\r\nIs the function and the evaluation is\r\n\r\n\r\n```\r\nalpha = 0.36\r\nbeta = 0.99\r\ndelta = 0.025\r\nzgrid = np.array([1.25, 0.75])\r\npiz = np.array([[0.9, 0.1], [0.1, 0.9]])\r\nm = 51\r\nkgrid = np.concatenate((np.linspace(0, 2, 20), np.linspace(2.5, 25, 20), np.linspace(30, 200, 11)))\r\ntheta = 1.8  \r\nr=.034\r\nw = (1-alpha)*(r\/alpha)**(alpha\/(alpha-1))\r\nx0 = np.zeros(4*m)\r\nx0[:m] = kgrid\r\nx0[m:2*m] = kgrid\r\nx0[2*m:3*m] = -np.sqrt(kgrid)\r\nx0[3*m:] = -np.sqrt(kgrid)\r\n\r\ndef to_solve(x):\r\n    return ap(x, alpha, beta, delta, r, w, kgrid, zgrid, piz)\r\n\r\nx, f = fsolve(to_solve, x0, full_output=True,xtol=1e-12)[:2]\r\n```\r\n\r\nChanging the tolerance didn't help. I also tried simplifying and writing like \r\n\r\n\r\n```\r\ndata = alpha, beta, delta, r, w, kgrid, zgrid, piz\r\n\r\nx = fsolve(ap,x0,args=data)\r\n```\r\n\r\nBut it changed nothing. I looked a long time for something on this, searched github for files that used this combination of functions, but to no avail.\r\n\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.10.1 1.24.3 sys.version_info(major=3, minor=11, micro=4, releaselevel='final', serial=0)\r\nlapack_armpl_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n    libraries = ['mkl_rt']\r\n    library_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\include']\r\nlapack_opt_info:\r\n    libraries = ['mkl_rt']\r\n    library_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\include']\r\nblas_armpl_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n    libraries = ['mkl_rt']\r\n    library_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\include']\r\nblas_opt_info:\r\n    libraries = ['mkl_rt']\r\n    library_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\lib']\r\n    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\r\n    include_dirs = ['C:\/ProgramData\/anaconda3\\\\Library\\\\include']\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\r\n    not found = AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\r\n```\r\n","comments":["Someone on stack found that if number of iterations was high enough, scipy.optimize.minimize() will return a non initial guess. But if you just run the default it returns the initial guess after about a minute (Matlab finds the root in about 3 seconds on the same machine)","@mdhaber is it possible for this to get tagged? \r\n\r\nPerhaps I didn't meet some threshold for posting a minimal version of the code. But I don't know how to simplify it. Namely, this system (of equations) requires two pchip interpolation components, so if I wanted to remove one of the max terms, (comment out mu2, res[i+m], and res[i+3m]) I would have more unknowns than equations. Moreover, I know that the code I presented has a root, and even if I found a way to simplify, it may not","@nealockwood hmm I'm not seeing evidence of a bug here. It's not surprising that a gradient-based rootfinder is having trouble with a function that involves `max`, because `max` is not smooth. You could try a soft maximum function and\/or try `scipy.optimize.root` with another `method`.\r\n\r\n> I am trying to find the roots of multiple equations over a grid (want the equations to be 0 evaluated at the points in the grid) and am stacking all the function values as a vector. \r\n\r\nHow many inputs and outputs does each equation have? That is, for each equation, how many values are varied to find the root, and how many outputs need to become zero?\r\n\r\nIf each of your equations is scalar-in, scalar-out, you have other options.","@mdhaber thank you so much for quickly responding! \r\n\r\nTo briefly highlight why I think it's a bug: fsolve in matlab is able to find the root pretty fast, the max terms are squared (making them smooth functions), removing the squared term actually seems to improve the performance (returning something other than the initial guess), I have unsuccessfully tried other max alternatives, , and [one is able](https:\/\/github.com\/scipy\/scipy\/issues\/19370#issuecomment-1765494483) to get a non-trivial solution by treating it as a minimization problem (minimizing normed value) when given a lot of iterations. \r\n\r\nEach function has a scalar output, but the goal is to solve the system over a fixed grid. So essentially, there are 4 equations I want to be 0 over a grid of m points. To do this, I stack the residual terms in a vector. So for example, with m=51, there are 204 residual terms, or you could think of it as each grid point corresponding to 4 of the 204 residual terms. ","The documentation of Matlab's `fsolve` says it uses a trust-region dogleg algorithm \"similar in nature\" to the that of MINPACK. This suggests that it is not identical to the one in MINPACK, which `scipy.optimize.fsolve` wraps. It is not guaranteed for all algorithms to solve all problems from an arbitrary guess, so failure to solve this problem does not necessarily indicate a bug. Often, it is just a shortcoming of the algorithm.\r\n\r\nAssuming that `max` is from `math` (not NumPy), I see that `max(x, 0)**2` is continuous and has continuous first derivative. However, the function may not be *sufficiently* smooth. The second derivative is discontinuous, and the first derivative is 0 for half the real line, which I would expect to cause problems.\r\n\r\nYou might try different algorithms (exposed with `scipy.optimize.root`) or a minimization algorithm - maybe a global minimization algorithm, even. I can't guarantee that any will solve the problem, though, even if there is a solution.\r\n\r\nThere is an effort to replace the Fortran code of MINPACK (https:\/\/github.com\/scipy\/scipy\/issues\/18566), which we can hope will uncover and fix bugs in the implementation. In the meantime, I don't think there is much we can do to illuminate bugs with this example. Would you agree @ilayn?","Thanks for the detail!","Our past and current role was\/is to only facilitate\/vendor these external Fortran77 libraries and hope that these are internally consistent and tested over time by actual experts. \r\n\r\nAs we started looking into these old libraries a bit more critically, it is becoming more apparent that there are a lot of edge cases that such historical code do not handle and do not report back. However we are also bounded by our own expertise in all different subjects hence it would be typical that we would not catch up the current state of art in integration or optimization and so on unfortunately.\r\n\r\nBut our hope is that as we convert these tools to a more readable format, more experts can look into the code and spot obvious points of improvement, low-hanging-fruit fixes etc. or get more encouraged to teach us how to do things better. Also we would be able to see what happens when we push in functions like `max()` and see if we can incorporate some tiny bumps here and there. Currently, we are still far from that point, but getting there. All help is welcome.\r\n","I will see if I can recruit some help! "],"labels":["defect","scipy.optimize"]},{"title":"ENH: scipy.ndimage.rotate is a lot slower than OpenCV equivalent","body":"### Describe your issue.\r\n\r\nWhen doing a rotation of a 2D image, the implementation in `scipy.ndimage.rotate()` is an order of magnitude slower than OpenCV's equivalent.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom threadpoolctl import threadpool_limits\r\nthreadpool_limits(0)\r\nimport numpy as np\r\nimport cv2\r\ncv2.setNumThreads(0)\r\nfrom scipy.ndimage import rotate as scipy_rotate\r\n\r\nimage = np.random.random_integers(0, 255, (20 ,1000, 1000)).astype(np.uint8)\r\n\r\ndef opencv():\r\n    M = cv2.getRotationMatrix2D((500, 500), 10, 1.0)\r\n    for i in range(20):\r\n        cv2.warpAffine(image[i], M, (1000, 1000))\r\n\r\ndef scipy():\r\n    for i in range(20):\r\n        scipy_rotate(image[i], 10, order=1, reshape=False)\r\n\r\n%timeit scipy()\r\n%timeit opencv()\r\n%timeit scipy()\r\n%timeit opencv()\r\n```\r\n\r\nThe result:\r\n\r\n```shell\r\n504 ms \u00b1 1.34 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n61 ms \u00b1 289 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n504 ms \u00b1 808 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n61 ms \u00b1 467 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\nOpenCV is opencv-python==4.8.1.78\r\n\r\n1.11.2 1.25.2 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:  \r\n    detection method: pkgconfig\r\n    found: true   \r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig \r\n    version: 0.3.21.dev\r\n  lapack: \r\n    detection method: pkgconfig                                                                                                                                                                   found: true    \r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig \r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:         \r\n  c:                                      \r\n    commands: cc \r\n    linker: ld.bfd\r\n1.11.2 1.25.2 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:  \r\n    detection method: pkgconfig\r\n    found: true   \r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig \r\n    version: 0.3.21.dev\r\n  lapack: \r\n    detection method: pkgconfig                                                                                                                                                                   found: true    \r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig \r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:         \r\n  c:                                      \r\n    commands: cc \r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-tshsyzfz\/overlay\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp311-cp311\/bin\/python\r\n  version: '3.11'\r\n```\r\n","comments":["I have seen the same performance issue compared to OpenCV equivalent with `skimage.transform.warp_polar()`, in a code path that ends up calling the same underlying SciPy function as this does, `_nd_image.geometric_transform()`. So it's quite possible that speeding up `geometric_transform()` will be beneficial for more than just `rotate()`.","I switched the label\/title to \"enhancement\" just because for performance issues we don't usually consider those bugs for i.e., backporting to old minor releases like we would for bugs\/incorrect results. So, if we did get a speedup, we'd usually put that in the next new minor release.","Thanks for the explanation, will note for next time.","Note I updated the example and results to disable multi-threading, which OpenCV uses by default.","In general, OpenCV will probably often have better performance than SciPy. After all, it is a dedicated image processing library while SciPy has a much wider scope. Also, compared to other modules in scipy, `scipy.ndimage` does not get that many updates.\r\n\r\nPR to improve the performance is welcome though!"],"labels":["enhancement","scipy.ndimage"]},{"title":"DOC: Guidance on available quadrature methods","body":"### Issue with current documentation:\n\nThe current documentation lists all available quadrature methods (given function object) in SciPy. However, it does not offer any guidance on which one to choose. I assume that most people just use `quad` because of this.\n\n### Idea or request for content:\n\nPlease add a table in the documentation that compares available quadratures so that we can easily check:\r\n- If they support vectorized functions.\r\n- If they support vector-valued functions.\r\n- If they support multiple integration.\r\n- Advantages and disadvantages of each (computational cost, when to use one over another, etc.).\r\nThis would be very useful for users.\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Pull requests most welcome! ","There are plans to remove several of them, at which point the choice will be very easy and this issue will be addressed. \r\n\r\nRight now, I can only recommend `quad` (and the higher dimensional variants) for general use.\r\n\r\n`quad_vec` is not as robust, but it supports vector values functions.\r\n\r\n`qmc_quad` can be useful if the problem is too high-dimensional for quad to finish in reasonable time. I would not make it an essential part of another library at this time because there is a more robust alternative on the horizon.\r\n\r\nWhen I have time, I will propose removing `quadrature` and `romberg`. More information is here: https:\/\/github.com\/scipy\/scipy\/issues\/18574#issuecomment-1703320598..","I wasn't aware of that thread! I support 100% the cleaning effort. I hope that the final interface supports multiple integration, vectorized input and vector-valued functions at the same time.\r\n\r\nI am currently the maintainer of a Python package for functional data analysis (essentially statistics and machine learning, but with functions instead of vectors), and efficient integration routines are essential to define inner products and norms in this context. Currently we are using a wrapper around `quad_vec` to achieve multiple integration (see https:\/\/github.com\/scipy\/scipy\/issues\/12209#issuecomment-1398138098), but the lack of vectorized input support of `quad_vec` was a performance issue in some cases. Thus, I was looking for alternatives. I am glad to have asked before changing anything!","If you can build from source, I'd suggest checking out the branch from https:\/\/github.com\/scipy\/scipy\/pull\/19173 and trying `_tanhsinh`, which might do what you're looking for. It is already merged - but with the shortcomings addressed by gh-19173 - so you can also try is with the latest nightly wheels. LMK what else you'd like to see when it becomes public.","I cannot use it right now, as my project is a library, and it should be able to work with several SciPy versions. That said, it looks promising. I do not think that support for multiple integration is done yet (is that planned?). Also the documentation regarding the dimensions of the arrays involved is a bit confusing right now.","> I do not think that support for multiple integration is done yet (is that planned?)\r\n\r\n`nquad` just recursively calls `quad`, so we could do that.\r\n\r\n> Also the documentation regarding the dimensions of the arrays involved is a bit confusing right now.\r\n\r\nWhich part? This?\r\n> ``func`` must be an elementwise function: each element ``func(x)[i]`` must equal ``func(x[i])`` for all indices ``i``.\r\n\r\nor the parts stating that input arrays need to be broadcastable?","> `nquad` just recursively calls `quad`, so we could do that.\r\n\r\nThat is a possibility for a first implementation. Having that as part of the unified interface also allows for more optimized implementations, if any.\r\n\r\n> Which part? This?\r\n> \r\n> > `func` must be an elementwise function: each element `func(x)[i]` must equal `func(x[i])` for all indices `i`.\r\n\r\nThis is clear right now, although it will need to be further clarified in the case of multiple integration. \r\n\r\n> or the parts stating that input arrays need to be broadcastable?\r\n\r\nThis part is what is more confusing for me, yeah.","> This part is what is more confusing for me, yeah.\r\n\r\nIs the confusing part the word \"broadcastable\"? Does this help?\r\nhttps:\/\/numpy.org\/doc\/stable\/user\/basics.broadcasting.html","No, I know what broadcasting is, I use that every day. But it is difficult to see the relationships between the dimensions of the different arrays involved (`input`, `output`, `a`, `b`, etc) as the dimensions are not explicit in the text. I would have preferred if you assigned names to the dimensions and added them to the documentation, e. g.:\r\n- `f` accepts an array of dimension `n_points` $\\times$ `dim_domain` and returns an array of dimension `n_points` $\\times$ `dim_codomain`.\r\n- `a` and `b` are 1D arrays of length `dim_domain` or floats (when the limits are the same for all variables).","There are no such requirements. It behaves almost as though one had applied `np.vectorize` to `quad`, which only works with scalars. The main difference is that the output is a result object where each field is an array rather than the output being an array of result objects. It is for integration of a univariate, scalar-valued functions. IIUC `quad_vec` does this in part, but it doesn't accept arrays for `a` and `b` because it thinks of the callable as a vector-valued function of a scalar. (Is this actually different than independent scalar-valued functions of a scalar? I can see the difference if the callable were also a function of a vector, but it is not.)","> LMK what else you'd like to see when it becomes public.\r\n\r\nI tried quickly what happened just by copy-pasting code. Am I wrong in thinking that it does not allow vector-valued functions out of the box? I think that behavior should be changed, so it can be used as a replacement of `quad_vec`.","It depends.\r\n\r\nCurrently, the callable needs to written in a way that I call \"strictly elementwise\": \"each element `func(x)[i]` must equal `func(x[i])` for all indices `i`.\" Actually, the requirement is a bit less restrictive than that - in addition to the variable `x`, you can also pass parameters. But when `x` and all the parameters are broadcasted to the same shape, the function needs to work elementwise. \r\n\r\nA vector-valued integrand like:\r\n\r\n```python3\r\ndef f(x):\r\n    return [sin(x), cos(x)]\r\n```\r\n\r\ndoes not satisfy the requirement. However, any vector-valued integrand can be expressed in a way that does satisfy the requirement. For example, the integrand could be written like:\r\n\r\n```python3\r\ndef f(x, c1, c2):\r\n    return c1*sin(x) + c2*cos(x)\r\n```\r\n\r\nand in addition to `x`, one would pass `c1 = np.asarray([1, 0])`, `c2 = np.asarray([0, 1])` as `args=(c1, c2)`. (There are more efficient ways, but this is conceptually simple.)\r\n\r\n---\r\n\r\nI realize this is cumbersome and inefficient for your use case, but our primary use case was univariate statistical distributions, so it was enough for our needs.\r\n\r\nActually, the original version of the framework this function uses (which was written for a scalar root-finding algorithm but has also been extended to scalar minimization, scalar differentiation, and scalar root bracketing) supported something like what you want. I changed that for efficiency (https:\/\/github.com\/scipy\/scipy\/pull\/18728#issuecomment-1612483600): if we can assume that the funciton works strictly elementwise, elements for which the integral estimate has already converged can be removed from further iterative refinement. At the time, I had offline discussions with the reviewer about how to offer both possibilities (efficiency vs callable flexibility), but we couldn't agree on an interface that would allow the best of both worlds, so we went with the simpler thing - make the function more efficient for the immediate use case.\r\n\r\nDoes this make sense? If so, would you be willing to suggest an interface that would allow the user to get the best of both worlds?\r\n\r\nFor instance, `_tanhsinh` could have a parameter `elementwise` which might be `False` by default, so it would natively support vector-valued, non-elementwise callables like you want. For a potential speed increase, users could specify `elementwise=True` when their function satisfies the elementwise requirement.\r\n\r\nAlternatively, `_tanhsinh` could try to automatically detect whether the function works strictly elementwise, but there would be some overhead associated with that, and it might not be bulletproof.\r\n\r\nThanks for your thoughts!\r\n","I still do not get why that optimization is not applicable to vector valued functions (but I did not understand the finer details).\r\n\r\nIn my case I can make that `func(x[i]) == func(x)[i]`, but there are extra dimensions corresponding to the vector output (`func(x).shape == (len(x), dim_codomain)`), and this quadrature is not able to deal with that.\r\n\r\n(I would also like if I could choose the dimension(s) that have to match with those of `x` through an `axis` parameter, but this is not strictly necessary as I can always move all extra dimensions to the end if necessary and back again).","I see that the description of the requirement is not complete. I will think about how to improve it.\r\n\r\n> this quadrature is not able to deal with that.\r\n\r\nIt is currently able to solve any problem `quad_vec` can solve, but it might require a reformulation of the callable to express the problem.\r\n\r\n> I still do not get why that optimization is not applicable to vector valued functions\r\n\r\nOne can get only part of the benefit if the function is vector valued because `_tanhsinh` has no way of stopping the callable from evaluating the components of the integrand vector for which the integral has already converged.","The close was a misclick on my cell phone. \r\n\r\nI'll think about this some more."],"labels":["scipy.integrate","Documentation"]},{"title":"ENH: Generalized SVD (wrap LAPACK ggsvd3)","body":"### Is your feature request related to a problem? Please describe.\n\n_No response_\n\n### Describe the solution you'd like.\n\nWrapping ggsvd3 (for generalized SVD) was previously blocking on bumping the minimum supported LAPACK version to something >= 3.6; see e.g. https:\/\/github.com\/scipy\/scipy\/issues\/5266#issuecomment-809783316 and earlier comments.\r\n\r\nNow that support for older LAPACK has been dropped, how difficult would it be to wrap ggsvd3?\n\n### Describe alternatives you've considered.\n\nIt looks like someone put together a standard wrapper, but it hasn't been touched since 2018 so I'm not sure how viable a solution it is today: https:\/\/github.com\/bnaecker\/pygsvd\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Not difficult but someone needs to spare some time to wrap it. I'm trying to get some other work finished but probably I can come back to this in about 10 days or so in case someone else does not take a stab at it.","Thanks, and no rush at all - just figured it would be nice to have eventually!\r\n\r\nFor what it's worth, I tried running `_cython_signature_generator.py` this morning but it no longer works with netlib's LAPACK 3.7.1 in its current form; may need to roll some of the updates from #18247 back into the script to get it running again. I believe it'll also need to be run using numpy >= 1.26.0, which fixed a few bugs in `f2py` that were preventing it from working on LAPACK; see https:\/\/github.com\/numpy\/numpy\/issues\/24008"],"labels":["enhancement","scipy.linalg"]},{"title":"BUG: scipy.linalg.cossin fails for specific values of (p, q) and separate=False","body":"### Describe your issue.\n\nI found a case where the scipy.linalg.cossin function returns an incorrect diagonal block matrix (D) when separate=False. Using terminology like https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.linalg.cossin.html, if q > m - p then `assert np.allclose(U@D@Vt, X)` fails.\r\n\r\nI developed a workaround in https:\/\/github.com\/sfcaracciolo\/cossin_wrapper. I use separate=True to build each matrix from theta.\n\n### Reproducing Code Example\n\n```python\nimport scipy  as sp \r\nimport numpy as np \r\n\r\n# generate a random dimension m\r\nrng = np.random.default_rng()\r\nm = rng.integers(50, high=100)\r\np = np.random.randint(10, 40) # always p < m\r\nq = np.random.randint(m-p+1, m-1) # always m-p < q < m \r\n# next line works\r\n# q = np.random.randint(1, m-p-1) # always q < m-p\r\nX = sp.stats.unitary_group.rvs(m) # random unitary matrix\r\nU, D, Vt = sp.linalg.cossin(X, p=p, q=q, separate=False)\r\nassert np.allclose(U@D@Vt, X)\n```\n\n\n### Error message\n\n```shell\nTraceback (most recent call last):\r\n  File \"E:\\Repositorios\\cossin_wrapper\\tests\\issue.py\", line 13, in <module>\r\n    assert np.allclose(U@D@Vt, X)\r\nAssertionError\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nPython: 3.10.0\r\nSciPy: 1.11.3\r\nNumPy: 1.26.0\n```\n","comments":["That indeed smells like a bug. Since U and V are just block diags of the individual parts, probably the bug is living in the D generation part. Since you already did the exercise, do you have any insights on where the issue might be on the SciPy code? I will circle back to this at some point anyways but if you already know that would be great. ","> That indeed smells like a bug. Since U and V are just block diags of the individual parts, probably the bug is living in the D generation part. Since you already did the exercise, do you have any insights on where the issue might be on the SciPy code? I will circle back to this at some point anyways but if you already know that would be great.\r\n\r\nYes, the issue is in D and begins in line 199. The issue is to suppose that the inner blocks of D11, D12, D21 and D22 share some dimension between them. For instance,  in line 199, the position of Id12 depends on n11.\r\n\r\nThe correct shape of each inner block is in these figures:\r\n\r\n![imagen](https:\/\/github.com\/scipy\/scipy\/assets\/41028670\/a6472a3f-1c47-4783-aa6d-c3221c7913de)\r\n\r\n![imagen](https:\/\/github.com\/scipy\/scipy\/assets\/41028670\/dfe0635b-d87d-4705-8782-2fd03a6b04a9)\r\n\r\nwhere $m_1 = m-p$, $m_2 = m-q$, $f_m(a,b) = min(a,b)-w$, $f_M(a,b) = max(a-b,0)$ and $w = min(p, q, m_1, m_2)$ ($r$ in scipy code)"],"labels":["defect","scipy.linalg"]},{"title":"Warning generated by SLSQP is useless","body":"https:\/\/github.com\/scipy\/scipy\/blob\/166e1f2b1ea0a1a2c3d7b030bd829549f8a5844a\/scipy\/optimize\/_optimize.py#L422-L429\r\n\r\nThe above code generates a warning for some calls to SLSQP (see https:\/\/github.com\/scipy\/scipy\/issues\/11403#issuecomment-717676023). \r\n\r\nUsers cannot do anything with this warning, as the reason for the warning is due to the implementation of SLSQP. The warning is also not informative as the result is correct always, so the message is not warning about any potential problem.\r\n\r\nPerhaps scipy developers want to keep the warning during development in case it may catch some unexpected effects of a patch, but it should not be given to users.\r\n","comments":["There is some development going on to write this in pure Python and after holidays, I am hoping to get the first version out this week. Then we can address this in a better fashion. I'll add this to the list in #19130 \r\n\r\n","In the meantime, would it be possible for you @MLopez-Ibanez to give us a relatively simple example that generates such warnings? I would love to have more test use-cases from the field out there.","My use case is quite complicated. The last line here generates the problem: https:\/\/github.com\/MLopez-Ibanez\/AsteroidRoutingProblem\/blob\/43100624442f78d0cd79c22746961116df381f76\/transfer_example.py#L58\r\n\r\nI could track the x and f values and produce a reproducible testcase, but it will be quite sensitive to any rounding.","If you can do that and intercept the optimize\/slsqp call arguments, it would be great. Otherwise, tough luck, we'll find another way.","Hopefully it works on your end!\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.optimize import minimize\r\nimport warnings\r\nwarnings.filterwarnings(\"error\")\r\n\r\ncache = np.array([0.00000000000000000000,30.00000000000000000000,507.86729214661846754097,1.4901161193847656e-08,30.00000000000000000000,507.86729214239687735244,0.00000000000000000000,30.00000001490116119385,507.86729189393923888929,0.28330612182617187500,46.95701599121093750000,324.44191198093199091090,0.28330613672733306885,46.95701599121093750000,324.44191197851279184761,0.28330612182617187500,46.95701600611209869385,324.44191187825890665408,0.55680523448597329939,58.56468419817139192673,260.37720189168533124757,0.55680524938713449323,58.56468419817139192673,260.37720188989499092713,0.55680523448597329939,58.56468421307255312058,260.37720182590476269979,1.08946754477820362261,79.26573118908873993860,192.99746069668549353082,1.08946755967936481646,79.26573118908873993860,192.99746069555160943310,1.08946754477820362261,79.26573120398990113245,192.99746066100135521992,1.74503441009820647878,103.81590689311030928366,148.23042625604853128607,1.74503442499936767263,103.81590689311030928366,148.23042625533850014108,1.74503441009820647878,103.81590690801147047750,148.23042623541573448165,2.65250355097983181452,137.47534024082756332064,113.24962538257469191194,2.65250356588099300836,137.47534024082756332064,113.24962538217167207222,2.65250355097983181452,137.47534025572872451448,113.24962537101509951754,3.80948033196084923091,180.35984226007067832143,88.36897870449629976974,3.80948034686201042476,180.35984226007067832143,88.36897870425835321839,3.80948033196084923091,180.35984227497183951527,88.36897869813196848554,5.22941691281192966301,232.90235688110337264334,72.47660600356016402657,5.22941692771309085686,232.90235688110337264334,72.47660600322741686341,5.22941691281192966301,232.90235689600453383719,72.47660600082301129987,6.33240227878139627649,272.71124095716561441805,73.25426203096870381160,5.72805819587127817982,250.89928924783617958383,70.15863935413993601742,5.72805821077243937367,250.89928924783617958383,70.15863935367745796157,5.72805819587127817982,250.89928926273734077768,70.15863935328448519613,5.99373916588939259498,259.22184459490478047883,70.10079295313937564060,5.99373918079055378882,259.22184459490478047883,70.10079295267055954355,5.99373916588939259498,259.22184460980594167268,70.10079295397365228837,5.89624500976088938842,255.18606006323980750494,70.01075562349195990919,5.89624502466205058226,255.18606006323980750494,70.01075562301200250204,5.89624500976088938842,255.18606007814096869879,70.01075562338999702661,5.94040735993559998462,255.67503881860915271318,70.00761726349843172557,5.94040737483676117847,255.67503881860915271318,70.00761726301756482371,5.94040735993559998462,255.67503883351031390703,70.00761726349644220591,6.00218129886272944162,255.89775042671931259974,70.00593946425257740884,6.00218131376389063547,255.89775042671931259974,70.00593946377077259058,6.00218129886272944162,255.89775044162047379359,70.00593946429719949265,6.24606670216814485741,256.32798384517593603960,70.00064138519582002118,6.24606671706930605126,256.32798384517593603960,70.00064138471078933890,6.24606670216814485741,256.32798386007709723344,70.00064138533291213662,6.94548377347351841848,256.99378353590168444498,69.98708019972383453933,6.94548378837467961233,256.99378353590168444498,69.98708019922899836729,6.94548377347351841848,256.99378355080284563883,69.98708020001060958748,9.07291384009567636326,258.16975342359256728741,69.94809399948911732281,9.07291385499683755711,258.16975342359256728741,69.94809399896317358980,9.07291384009567636326,258.16975343849372848126,69.94809400006624855450,15.55473525843356519260,260.38032530943468145779,69.82862582968766673730,15.55473527333472638645,260.38032530943468145779,69.82862582904796511230,15.55473525843356519260,260.38032532433584265164,69.82862583093577768523,43.62254884642859309452,267.20665391263355559204,69.18636932546489504148,43.62254886132975428836,267.20665391263355559204,69.18636932359459024156,43.62254884642859309452,267.20665392753471678589,69.18636933158018109680,730.00000000000000000000,421.27121280917708645575,110.41289070806716665629,166.25817793158677204701,294.73349656786768946404,69.18662610724906869564,104.93651933215724625370,280.96921240170308919915,69.41773490564590076701,71.16737262670879715643,273.38937614288698796372,73.78169973107425505532,46.37703122445661563233,267.82492613565892725092,69.08804206576243700511,46.37703123935777682618,267.82492613565892725092,69.08804206359123156744,46.37703122445661563233,267.82492615056008844476,69.08804207289385601598,730.00000000000000000000,420.45035441363364725476,110.43630014471359856998,180.08040263924380042226,297.67549473334918275214,69.30470597092876516854,110.55251168671989603354,282.15272177430091460337,69.38488437924510776611,75.05158911479929884081,274.22679804252879876003,71.27501491405504907561,51.21630107309208312927,268.90533985336008981903,68.86679617878891690452,51.21630108799324432312,268.90533985336008981903,68.86679617583784818180,51.21630107309208312927,268.90533986826125101288,68.86679618836626559641,729.99999998942143975000,420.09189534198264937004,110.44662414576600895089,211.60711710966910459319,304.62943730486421145542,69.67572451398459065786,124.66290206103926152537,285.26421622803235322863,69.27777799710953843260,84.54212436129563457143,276.32805262594109763086,70.06412480902199035881,61.28713176168179188608,271.14843173247822960548,67.73994238639342313490,61.28713177658295307992,271.14843173247822960548,67.73994237790483907702,61.28713176168179188608,271.14843174737939079932,67.73994240802919364342,730.00000000000000000000,419.68428191649388736550,110.45843905736700207854,326.97102465542633353834,330.16267354770695874322,73.49133540042394940883,183.43065270642483710617,298.27919680328102458589,69.36553277221148050558,119.23824905680402252983,284.02065071522542893945,69.33245764324907156606,87.36329713453994827432,276.94052214551351198679,69.93778921200738807329,71.00955549598923255417,273.30799608934347588729,74.47135828743422791831,62.56593045032163047381,271.43248107480917497014,67.36223981927419401927,62.56593046522279166766,271.43248107480917497014,67.36223980855456261452,62.56593045032163047381,271.43248108971033616399,67.36223984384021434835,730.00000000000000000000,419.89060309540536763961,110.45244870054489183531,344.64623895421323140908,334.17592611310772099387,74.45114736065437455181,194.22459733046622432084,300.71746864811240129711,69.45860924999568908333,125.55161531803832986043,285.44245886325393257721,69.28049473562283822048,91.55542710396214545199,277.88064790936346071248,69.77330020966428492102,74.29476840607004817230,274.04133989073557131633,71.59277255673801221292,65.46520747065790146735,272.07737057220475662689,65.52566052455750877925,65.46520748555906266120,272.07737057220475662689,65.52566050180581669338,65.46520747065790146735,272.07737058710591782074,65.52566055167062586406,0.00000000000000000000,0.01000000000000000021,1532039.62036360707134008408]).reshape(-1, 3)\r\n\r\ndef fun(x):\r\n    index = np.equal(cache[:,0:2], x).all(axis=1)\r\n    print(index)\r\n    f = cache[index,2]\r\n    if (len(f) != 1):\r\n        print(f'Could not find {x} in the cache')\r\n        assert False\r\n    return f[0]\r\n\r\nres = minimize(fun, x0 = [0,30], bounds = ((0,730), (0.01,730)), method = 'SLSQP',\r\n                       options = dict(maxiter=1000))\r\n```\r\n","> Users cannot do anything with this warning, as the reason for the warning is due to the implementation of SLSQP. The warning is also not informative as the result is correct always, so the message is not warning about any potential problem.\r\n\r\nThe ultimate reason for the clipping is because some user functions absolutely don't want to access outside bounds. THe warning was inserted because clipping is not part of the optimisation algorithm, and it wasn't clear if clipping would adversely affect the minimisation. At the moment it seems to be only SLSQP that uses that specific clipping function.\r\n\r\nIf the SLSQP iteration could be improved to avoid the clipping that would be even better.","> Hopefully it works on your end!\r\n\r\nYes I can also generate the warning, thank you for the example @MLopez-Ibanez. We are not a big fun of zip attachments due to security reasons. So I've edited your example with the raw data instead for others. "],"labels":["defect","scipy.optimize"]},{"title":"BUG: SVD-LOBPCG benchmarks sparse_linalg_svds.py","body":"#### Reference issue\r\nCloses gh-18945\r\n\r\n#### What does this implement\/fix?\r\nOriginally attempted in https:\/\/github.com\/scipy\/scipy\/pull\/19292 to add a separate unit test checking the accuracy of the iterative solvers. However, one cannot open a file with test matrices from benchmarking in a unit test, since the benchmark directory outside of the scipy main tree. So this PR adds a rudimentary accuracy check directly to benchmarking, just like in  https:\/\/github.com\/scipy\/scipy\/pull\/18954","comments":["@tylerjereddy from the PR description:\r\n\"Originally attempted in https:\/\/github.com\/scipy\/scipy\/pull\/19292 to add a separate unit test checking the accuracy of the iterative solvers. However, one cannot open a file with test matrices from benchmarking in a unit test, since the benchmark directory outside of the scipy main tree.\"","I'm not sure why you'd try that in the first place, I just mean that you could effectively use code duplication (or abstraction, but in the other direction SciPy lib-> benchmarks) to test what you benchmark without any cross-talk between the suites. I'm not saying that's what you should do, but it did cross my mind if it could be done concisely.","@lobpcg We agreed about adding asserts and stuff here on the premise that it was about keeping the benchmark in working order. If you are changing your mind and using it as a replacement for a proper unit test, we don't agree. If you need this tested, make a unit test. If you need data files in the unit test suite, put them there.","@rkern - this benchmarking is NOT in working order, although the trouble is now hidden alter turning off globally all warnings in benchmarking. The benchmark needs to be fixed and the asserts will be used to make sure that the fix is valid, as described in https:\/\/github.com\/scipy\/scipy\/issues\/18945 \r\n\r\n@tylerjereddy At the same time, I see additional value in running these examples with various parameters in a unit test, but that I now feel would be better for a separate issue\/PR, also requiring access to somehow read the file with matrices, which currently is located in benchmarks\/benchmarks\/svds_benchmark_files\/svds_benchmark_files.npz and thus is not readable from scipy\/sparse\/linalg\/_eigen\/tests\/test_svds.py\r\n\r\nI have actually tried it in https:\/\/github.com\/lobpcg\/scipy\/blob\/632f3c45e972382dd5ab877ef9ec555020085ebe\/scipy\/sparse\/linalg\/_eigen\/tests\/test_svds.py\r\n\r\n```\r\n    @pytest.mark.filterwarnings(\"ignore:Exited at iteration\")\r\n    @pytest.mark.filterwarnings(\"ignore:Exited postprocessing\")\r\n    @pytest.mark.parametrize(\"problem\", (\"abb313\", \"illc1033\", \"illc1850\",\r\n                                         \"qh1484\", \"rbs480a\", \"tols4000\",\r\n                                         \"well1033\", \"well1850\", \"west0479\", \"west2021\"))\r\n    def test_MatrixMarket(self, problem):\r\n        if self.solver == 'propack':\r\n            pytest.skip(\"PROPACK failing (Aug. 2023)\")\r\n        dir_path = os.path.dirname(os.path.realpath(__file__))\r\n        datafile = os.path.join(dir_path, \"svds_benchmark_files\",\r\n                                \"svds_benchmark_files.npz\")\r\n        matrices = np.load(datafile, allow_pickle=True)\r\n        A = matrices[problem][()]\r\n        _, s, _ = svd(A.toarray(), full_matrices=False)\r\n        rng = np.random.default_rng(0)\r\n        _, ss, _ = svds(A, k=25, solver=self.solver, random_state=rng)\r\n        assert_allclose(s, ss, atol=1e-2)\r\n```\r\n\r\nbut then give up after realizing that the file to open is not accessible.","Move the file into the `tests\/` directory. This PR is not a replacement for #19292.","@j-bowhay CircleCI config appears to still be broken?","@j-bowhay great, thanks for your help!","Done, ready for the final review.","This PR is completed 3 months ago now. Could someone review it and consider merging?","> Could someone review it and consider merging?\r\n\r\nI did review, and not everything is done yet."],"labels":["scipy.sparse.linalg","Benchmarks"]},{"title":"TST: lobpcg: add unit tests for accuracy matching benchmark tests for performance","body":"#### Reference issue\r\nCloses gh-19355\r\n\r\n#### What does this implement\/fix?\r\nSee gh-19355\r\n","comments":["@tylerjereddy @rkern - ready for review","Linux Meson tests \/ Meson build (3.12-dev, false) (pull_request) Failing is unrelated to this PR.","Waiting for reviews...","This PR is completed 2 months ago now. Could someone review it and consider merging?"],"labels":["scipy.sparse.linalg","maintenance"]},{"title":"ENH: add unit tests for accuracy matching benchmark tests for performance","body":"### Is your feature request related to a problem? Please describe.\n\nhttps:\/\/github.com\/scipy\/scipy\/pull\/18954 has introduced a few new benchmark tests for iterative eigensolvers where their convergence is not necessarily guaranteed. Corresponding unit tests should be thus added to the check convergence. Similarly for pre-existing benchmark tests treated in https:\/\/github.com\/scipy\/scipy\/pull\/19292. \n\n### Describe the solution you'd like.\n\nNew unit tests matching the existing benchmark tests but checking for convergence.\n\n### Describe alternatives you've considered.\n\nhttps:\/\/github.com\/scipy\/scipy\/pull\/18954 includes accuracy asserts to be on a safe side, but performance benchmarks is not a good place to also assess the accuracy.   \n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":[],"labels":["enhancement","scipy.sparse.linalg"]},{"title":"BUG: stats.nct.pdf inconsistent behavior when compared to MATLAB and R","body":"### Describe your issue.\r\n\r\nHello,\r\n\r\nWhen computing stats.nctpdf with large t and nc values, the function returns 'inf'. \r\nThis behavior is inconsistent with both MATLAB and R. This \"bug\" appeared with version scipy 1.10 (behavior was consistent before).\r\nI'm not sure whether it's truly a bug or whether MATLAB and R are both wrong in not returning inf.\r\n\r\nThe following R code (equivalent to the Python code) returns 0.0043403 instead of 'inf' (R version 4.3.1):\r\n\r\n```R\r\nt <- 8.45316107575123\r\nN <- 25\r\ndf <- N-1\r\nsafe_int <- .9999\r\nsafe_range <- t \/ sqrt(N) + c(-1, 1) * qt(1 - (1 - safe_int) \/ 2, df) \/ sqrt(N)\r\n\r\ndt(t, df, ncp=safe_range[[2]]*sqrt(N))\r\n```\r\nSame value is returned in MATLAB (R2023b):\r\n\r\n```MATLAB\r\nt = 8.45316107575123;\r\nN = 25;\r\ndf = N-1;\r\nsafe_int = .9999;\r\nsafe_range = t \/ sqrt(N) + [-1, 1] * tinv(1 - (1 - safe_int) \/ 2, df) \/ sqrt(N);\r\n\r\nnctpdf(t, df, safe_range(2)*sqrt(N))\r\n```\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy import stats\r\n\r\nt = 8.45316107575123\r\nN = 25\r\ndf = N-1\r\nsafe_int = .9999\r\nsafe_range = t \/ np.sqrt(N) + np.array([-1, 1]) * stats.t.ppf(1 - (1 - safe_int) \/ 2, df=df) \/ np.sqrt(N)\r\n\r\n# SciPy 1.9.3 returns 0.0043403; SciPy 1.10 and above return Inf\r\nstats.nct.pdf(t, df, nc=safe_range[1]*np.sqrt(N))\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\nN\/A\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n1.11.3 1.26.0 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: C:\/Users\/constama\/miniforge3\/Library\/include\r\n    lib directory: C:\/Users\/constama\/miniforge3\/Library\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: C:\\bld\\scipy-split_1696467770591\\_h_env\\Library\\lib\\pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: C:\/Users\/constama\/miniforge3\/Library\/include\r\n    lib directory: C:\/Users\/constama\/miniforge3\/Library\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: C:\\bld\\scipy-split_1696467770591\\_h_env\\Library\\lib\\pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: C:\/Users\/constama\/miniforge3\/Library\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: clang-cl\r\n    linker: lld-link\r\n    name: clang-cl\r\n    version: 17.0.0\r\n  c++:\r\n    commands: clang-cl\r\n    linker: lld-link\r\n    name: clang-cl\r\n    version: 17.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: flang-new\r\n    linker: lld-link\r\n    name: flang\r\n    version: 17.0.0\r\n  pythran:\r\n    include directory: ..\\..\\_h_env\\lib\\site-packages\\pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\bld\\scipy-split_1696467770591\\_h_env\\python.exe\r\n  version: '3.10'\r\n```\r\n","comments":["> I'm not sure whether it's truly a bug or whether MATLAB and R are both wrong in not returning inf.\r\n\r\nThanks for reporting the issue. It is a bug, and it is still present in the development version of SciPy.  Here's a simplified example, with integer parameter values so it is easy to compare to the result computed by Wolfram Alpha:\r\n\r\n```\r\nIn [28]: import scipy\r\n\r\nIn [29]: scipy.__version__\r\nOut[29]: '1.12.0.dev0+1813.0a3dce2'\r\n\r\nIn [30]: from scipy.stats import nct\r\n\r\nIn [31]: nct.pdf(8, 24, 13)\r\nOut[31]: inf\r\n```\r\n\r\nWith Wolfram Alpha, [`PDF[NoncentralStudentTDistribution[24, 13], 8]`](https:\/\/www.wolframalpha.com\/input?i=PDF%5BNoncentralStudentTDistribution%5B24%2C+13%5D%2C+8%5D) gives `0.0017644351379120543496495442757378120219461588078650315935117814288005136461742269261929088489060060675999625476400913578446784114357283664002733506219393887469961463578572949425885674236825307738127452` (after clicking on \"More digits\" a couple times).","too much boost ?\r\n\r\n```\r\n>>> from scipy import stats\r\n>>> stats.nct.pdf(8, 24, 13)\r\n0.0017644351379121064\r\n>>> stats.nct.pdf(8, 24, 13) - .0017644351379120543496\r\n5.204170427930421e-17\r\n\r\n>>> import scipy\r\n>>> scipy.__version__\r\n'1.7.3'\r\n```","Boost isn't used by `nct.pdf` currently.\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/stats\/_continuous_distns.py#L7631-L7709","The development version of boost\/math gives the correct result in this case.  [`noncentralt_pdf.cpp`](https:\/\/github.com\/WarrenWeckesser\/experiments\/blob\/master\/c%2B%2B\/boost\/noncentralt\/noncentralt_pdf.cpp) takes `x`, `df` and `nc` from the command line and evaluates the PDF using boost's `non_central_t` distribution:\r\n\r\n```\r\n$ .\/noncentralt_pdf 8 24 13\r\nx  = 8\r\ndf = 24\r\nnc = 13\r\np  =  1.7644351379120544e-03\r\n```\r\n\r\nAs noted in the comment in the SciPy code, the boost implementation has issues in the left tail.","@WarrenWeckesser are you willing to open an issue with Boost about the left tail of the pdf? It would be nice if we could switch to Boost's implementation after all rather than stitching ours and theirs together. Here's a demonstration in SciPy (using Boost for the PDF):\r\n```python3\r\nimport numpy as np\r\nfrom scipy import stats\r\nimport matplotlib.pyplot as plt\r\ndist = stats.nct(8, 8.5)\r\nx = np.linspace(-3, 5)\r\nplt.semilogy(x, dist.pdf(x))\r\nplt.show()\r\n```\r\n<img width=\"485\" alt=\"image\" src=\"https:\/\/github.com\/scipy\/scipy\/assets\/6570539\/be5bbec7-2d51-4acf-920b-dd2810d8ff7d\">\r\n\r\nOr maybe in the meantime just going with Boost is the lesser of two evils.","one possible candidate for change compared to my version is\r\n#17302 boost\r\n\r\n(because pdf code itself is unchanged0","Thanks @josef-pkt, that is in fact the source of the problem reported here.  When the `b` parameter of `hyp1f1` is 1.5 and the third parameter (`x`) is greater than 50, `hyp1f1` returns `inf`:\r\n\r\n```\r\nIn [17]: import scipy\r\n\r\nIn [18]: scipy.__version__\r\nOut[18]: '1.12.0.dev0+1814.166e1f2'\r\n\r\nIn [19]: from scipy.special import hyp1f1\r\n\r\nIn [20]: hyp1f1(13.0, [1.499999, 1.5, 1.500001], 61.0).tolist()\r\nOut[20]: [1.3550915047757008e+39, inf, 1.3550800368458125e+39]\r\n```\r\nThe problem is that the underlying boost function `hypergeometric_1F1` throws an exception when it shouldn't.  I reported the boost issue here: https:\/\/github.com\/boostorg\/math\/issues\/1034.\r\n\r\nI'll see if we can work around the boost bug in our wrapper of `hypergeometric_1F1`. This needs to be fixed regardless of whether or not we replace the current implementation of the PDF with the boost noncentral t PDF function.\r\n\r\nThe problem with the PDF calculation in the left tail is a separate issue.  @mdhaber, if you already have the evidence, go ahead and file a boost issue.  I probably won't get to that in the immediate future.\r\n\r\n","I ask because I don't ever work with C++. We'll see if they'll take the report in Python form.","There is a fix for the left tail issue in the works at the Boost end now, however, the accuracy is still poor and I see no easy fix at present.  Would I be correct in thinking that a handful of digits correct is \"good enough\" for most stats usages?","Probably (although we are regularly surprised at what is requested). In any case, I think that fixing that discontinuity would be enough of an improvement to switch to the Boost implementation so we can fix the garbage output reported in this issue.\r\n\r\n_Update: when we do fix this, check the cases in gh-19450._","Can I ad that if one wants to use scipy to develop methods used in validation then accuracy is important? Was a user of Minitab in previous life in quality and developing sampling plans was part of the job. Accuracy similar to Minitab would be preferable.","@jzmaddock is this now already part of the SciPy release? I am running into the same problems. Running SciPy 1.12.0.","I can't comment on SciPy, but this would have been in the last Boost release (1.84), so it probably depends what Boost release is installed when SciPy is built?","SciPy was recently updated to use 1.83 on main, so would need an update before the next release. It sounds like that isn't trivial and introduces some errors which would need resolving.","Anything on our side that we need to look at?","From looking at the source code, I managed to fix this myself by converting the calculation into log space before converting back. The problem with the scipy implementation (at least for me) was that the numbers got exceptionally large before being reduced down.\r\n\r\nI've written my own implementation for a project that I'm using Pytorch but the idea is the same\r\n\r\n```python\r\nPi = torch.acos(torch.zeros(1)) * 2\r\n\r\ndef nct_pdf(x, df, nc, norm=True):\r\n    \"\"\"Non-central t distribution probability density function.\r\n\r\n    There are problems with the scipy implementation so this function fixes\r\n    those by performing most of the calculations in log space.\r\n    Most of the function is also implemented in pytorch.\r\n    Maths taken from: https:\/\/en.wikipedia.org\/wiki\/Noncentral_t-distribution#Probability_density_function\r\n\r\n    Args:\r\n        x (torch tensor) : Values to evaluate the pdf at.\r\n        df (int) : Degrees of freedom.\r\n        nc (float) : Non-centrallity parameter.\r\n\r\n    Returns:\r\n        pdf (torch.tensor) : Probability density at each x.\r\n    \"\"\"\r\n    n = torch.tensor(df)\r\n    nc = torch.tensor(nc)\r\n\r\n    x2 = x ** 2\r\n    mu2 = nc ** 2\r\n    mu2x2 = mu2 * x2\r\n    _2 = torch.tensor(2)\r\n\r\n    # Student T(x; mu = 0)\r\n    lgamma2 = torch.lgamma(n \/ 2)\r\n    lgamma12 = torch.lgamma((n + 1) \/ 2)\r\n    stmu_z = (\r\n        lgamma12 - lgamma2 - ((n + 1)\/2) * torch.log(1 + x2\/n)\r\n        - 0.5 * torch.log(n * Pi) - mu2\/2\r\n    )\r\n\r\n    # A_v(x; mu) and B_v(x; mu) terms\r\n    zterm = mu2x2 \/ (2 * (x2 + n))\r\n    Av = loghyp1f1((df + 1)\/2, .5, zterm)\r\n    Bv = (\r\n        torch.log(torch.sqrt(_2) * nc * x \/ torch.sqrt(x2 + n))\r\n        + torch.lgamma(n\/2 + 1) - lgamma12\r\n        + loghyp1f1(df\/2 + 1, 1.5, zterm)\r\n    )\r\n\r\n    pdf = torch.exp(stmu_z + Av) + torch.exp(stmu_z + Bv)\r\n    pdf = torch.nan_to_num(pdf)\r\n    if norm:\r\n        pdf \/= torch.sum(pdf)\r\n        pdf = torch.nan_to_num(pdf)\r\n    return torch.clamp(pdf, 0, 1).to(x.dtype)\r\n```\r\n\r\nwhere the function `loghyp1f1` is the logarithm of the [confluent hypergeometric function](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.hyp1f1.html) that I've written in Fortran and that I'm calling from Python.\r\n\r\nHappy to submit a pull request if this would be useful.","I think that I get 2 extra errors with boost 1.84. Not sure which end they are due to.\r\n\r\n```\r\nscipy\/stats\/tests\/test_continuous_basic.py:170: in test_cont_basic\r\n    check_sf_isf(distfn, arg, distname)\r\n        arg        = ()\r\n        distfn     = <scipy.stats._continuous_distns.wald_gen object at 0x119987a00>\r\n        distname   = 'wald'\r\n        m          = 1.0\r\n        n_fit_samples = 200\r\n        rng        = RandomState(MT19937) at 0x11E5B2F40\r\n        rvs        = array([2.00676157, 0.29004979, 0.62849545, 1.6186909 , 0.3169379 ,\r\n       0.24333655, 1.62317229, 0.62463867, 3.540782...25, 0.33951168, 2.29954848, 0.79173576, 0.32611827,\r\n       2.27908213, 0.92946228, 1.14471458, 0.27071012, 1.77247785])\r\n        sn         = 500\r\n        v          = 1.0\r\nscipy\/stats\/tests\/test_continuous_basic.py:597: in check_sf_isf\r\n    npt.assert_almost_equal(distfn.sf(distfn.isf([0.1, 0.5, 0.9], *arg), *arg),\r\n        arg        = ()\r\n        distfn     = <scipy.stats._continuous_distns.wald_gen object at 0x119987a00>\r\n        msg        = 'wald'\r\nscipy\/stats\/_distn_infrastructure.py:2164: in sf\r\n    place(output, cond, self._sf(*goodargs))\r\n        _a         = 0.0\r\n        _b         = inf\r\n        args       = ()\r\n        cond       = array([1, 1, 1])\r\n        cond0      = 1\r\n        cond1      = array([ True,  True,  True])\r\n        cond2      = array([0, 0, 0])\r\n        dtyp       = dtype('float64')\r\n        goodargs   = [array([4.47482874e+248, 6.75841306e-001, 2.37624709e-001])]\r\n        kwds       = {}\r\n        loc        = array(0)\r\n        output     = array([0., 0., 0.])\r\n        scale      = array(1)\r\n        self       = <scipy.stats._continuous_distns.wald_gen object at 0x119987a00>\r\n        x          = array([4.47482874e+248, 6.75841306e-001, 2.37624709e-001])\r\nscipy\/stats\/_continuous_distns.py:10743: in _sf\r\n    return invgauss._sf(x, 1.0)\r\n        self       = <scipy.stats._continuous_distns.wald_gen object at 0x119987a00>\r\n        x          = array([4.47482874e+248, 6.75841306e-001, 2.37624709e-001])\r\nscipy\/stats\/_continuous_distns.py:4784: in _sf\r\n    return np.exp(self._logsf(x, mu))\r\n        mu         = 1.0\r\n        self       = <scipy.stats._continuous_distns.invgauss_gen object at 0x118fdf7c0>\r\n        x          = array([4.47482874e+248, 6.75841306e-001, 2.37624709e-001])\r\nscipy\/stats\/_continuous_distns.py:4781: in _logsf\r\n    return a + np.log1p(-np.exp(b - a))\r\nE   RuntimeWarning: divide by zero encountered in log1p\r\n        a          = array([-2.23741437e+248, -4.25683513e-001, -6.07213810e-002])\r\n        b          = array([-2.23741437e+248, -1.87520797e+000, -3.19210227e+000])\r\n        fac        = array([4.72728505e-125, 1.21640343e+000, 2.05141819e+000])\r\n        mu         = 1.0\r\n        self       = <scipy.stats._continuous_distns.invgauss_gen object at 0x118fdf7c0>\r\n        x          = array([4.47482874e+248, 6.75841306e-001, 2.37624709e-001])\r\n---\r\nscipy\/stats\/tests\/test_fast_gen_inversion.py:118: in test_rvs_and_ppf\r\n    assert_allclose(rng1.ppf(q), rng2.ppf(q), atol=1e-10)\r\n        args       = ()\r\n        distname   = 'wald'\r\n        q          = [0.001, 0.1, 0.5, 0.9, 0.999]\r\n        rng1       = <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x12000d7e0>\r\n        rng2       = <scipy.stats._sampling.FastGeneratorInversion object at 0x12000dd50>\r\n        rvs1       = array([0.49415957, 0.08335714, 0.32718863, 0.68736644, 1.00888463,\r\n       0.21064594, 0.27862896, 3.03444895, 0.356945...43, 1.01103814, 0.22827809, 0.4331922 , 0.31824336,\r\n       2.24261899, 0.24624813, 0.63768449, 0.66763203, 1.55694817])\r\n        rvs2       = array([1.13945461, 2.10754385, 0.44018833, 1.04186099, 0.15792175,\r\n       1.18501888, 0.60517616, 0.12677009, 0.557995...38, 0.16108085, 0.2168502 , 3.36257858, 0.33278517,\r\n       2.67267979, 2.19542446, 2.64032752, 0.28419598, 0.75260723])\r\n        urng       = Generator(PCG64) at 0x12085C9E0\r\n..\/..\/..\/..\/..\/..\/mambaforge\/envs\/scipy-dev\/lib\/python3.10\/contextlib.py:79: in inner\r\n    return func(*args, **kwds)\r\nE   AssertionError: \r\nE   Not equal to tolerance rtol=1e-07, atol=1e-10\r\nE   \r\nE   Mismatched elements: 1 \/ 5 (20%)\r\nE   Max absolute difference: 4.47482874e+248\r\nE   Max relative difference: 2.08808116e+248\r\nE    x: array([7.921848e-002, 2.376247e-001, 6.758413e-001, 4.474829e+248,\r\nE          8.354865e+000])\r\nE    y: array([0.079218, 0.237625, 0.675841, 2.143034, 8.354865])\r\n        args       = (<function assert_allclose.<locals>.compare at 0x120206950>, array([7.92184778e-002, 2.37624709e-001, 6.75841306e-001, 4.47482874e+248,\r\n       8.35486493e+000]), array([0.07921848, 0.23762471, 0.67584131, 2.14303391, 8.35486495]))\r\n        func       = <function assert_array_compare at 0x116558e50>\r\n        kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=1e-10', 'verbose': True}\r\n        self       = <contextlib._GeneratorContextManager object at 0x11652f130>\r\n```","> From looking at the source code, I managed to fix this myself by converting the calculation into log space before converting back. The problem with the scipy implementation (at least for me) was that the numbers got exceptionally large before being reduced down.\r\n\r\nIf SciPy is just calling the Boost implementation then we shouldn't spuriously overflow - if we do it's a bug and we'll fix that.\r\n\r\n> I think that I get 2 extra errors with boost 1.84. Not sure which end they are due to.\r\n\r\nI fear someone will have to explain that output to this non-Python person ;)","> > From looking at the source code, I managed to fix this myself by converting the calculation into log space before converting back. The problem with the scipy implementation (at least for me) was that the numbers got exceptionally large before being reduced down.\r\n> \r\n> If SciPy is just calling the Boost implementation then we shouldn't spuriously overflow - if we do it's a bug and we'll fix that.\r\n\r\nIs scipy just calling boost? I've found some nct pdf code [here](https:\/\/github.com\/scipy\/scipy\/blob\/7977e9e023e7890c9f73931d5da3aa32d4c3cdff\/scipy\/stats\/_continuous_distns.py#L7735) - if this is being used it might benefit from performing the bulk of the calculations in log space.\r\n\r\n","> > From looking at the source code, I managed to fix this myself by converting the calculation into log space before converting back. The problem with the scipy implementation (at least for me) was that the numbers got exceptionally large before being reduced down.\r\n> \r\n> If SciPy is just calling the Boost implementation then we shouldn't spuriously overflow - if we do it's a bug and we'll fix that.\r\n> \r\n> > I think that I get 2 extra errors with boost 1.84. Not sure which end they are due to.\r\n> \r\n> I fear someone will have to explain that output to this non-Python person ;)\r\n\r\nThe new errors with BooostMath 1.84 are unrelated to this issue. They originate from Boost's implementation of the inverse survival function of the inverse gaussian distribution, see [here](https:\/\/github.com\/scipy\/scipy\/blob\/f0b65169c49261a0f9c65ce506fb9de7c22234ab\/scipy\/stats\/_continuous_distns.py#L4802). Wald is a special case of the inverse gaussian distribution for $\\mu=1$. The reported errors look like the inverse survival function blows up for $q=0.1$ or $q=0.9$ on some Mac hardware.\r\n\r\nSide note: the RunTimeError in `logsf` does not look good either, will see if something can be done there.",">The new errors with BooostMath 1.84 are unrelated to this issue. They originate from Boost's implementation of the inverse survival function of the inverse gaussian distribution, see [here](https:\/\/github.com\/scipy\/scipy\/blob\/f0b65169c49261a0f9c65ce506fb9de7c22234ab\/scipy\/stats\/_continuous_distns.py#L4802). Wald is a special case of the inverse gaussian distribution for . The reported errors look like the inverse survival function blows up for or on some Mac hardware.\r\n\r\nHmmm, I don't have a mac here, but I have tried to reproduce and completely failed on Windows\/MSVC, @mborland  are you able to try on your Mac?\r\n\r\nHere's the test program I used, it tests every representable q value from 0.09999999 to 0.10000001, and takes a good couple of hours to run even in release mode.  There is a bit of \"wobble\" in the quantiles returned (up to 9ULP on my machine), but that's to be expected given that (a) the root finder stops once it's \"close enough\" and (b) there may be multiple abscissa values that are equally correct.  Which is to say, the quantile doesn't quite always monotonically decrease for increasing q.\r\n\r\n```\r\n#define BOOST_MATH_PROMOTE_DOUBLE_POLICY false\r\n\r\n#include <iostream>\r\n#include <boost\/math\/distributions.hpp>\r\n\r\nvoid print_error(double val, int distance)\r\n{\r\n   std::cout << \"New high error of \" << distance << \" ULP at \" << std::setprecision(std::numeric_limits<double>::max_digits10) << val << std::endl;\r\n}\r\n\r\nint main()\r\n{\r\n   boost::math::inverse_gaussian g;\r\n\r\n   double q = 0.09999999;\r\n\r\n   double p1_last = 5;\r\n   double p2_last = 5;\r\n\r\n   int distance_1 = 0;\r\n   int distance_2 = 0;\r\n\r\n   std::cout << std::setprecision(std::numeric_limits<double>::max_digits10) <<\r\n      quantile(complement(g, 0.09)) << \" \" << quantile(complement(g, 0.1)) << \" \" <<\r\n      quantile(complement(g, 0.11)) << std::endl;\r\n\r\n   while (q < 0.10000001)\r\n   {\r\n      q = boost::math::float_next(q);\r\n\r\n      double p1 = quantile(g, 1 - q);\r\n      double p2 = quantile(complement(g, q));\r\n\r\n      if (!boost::math::isnormal(p1))\r\n         abort();\r\n      if (!boost::math::isnormal(p2))\r\n         abort();\r\n\r\n      int d = boost::math::float_distance(p1_last, p1);\r\n      if (d > distance_1)\r\n      {\r\n         print_error(q, d);\r\n         distance_1 = d;\r\n      }\r\n      d = boost::math::float_distance(p2_last, p2);\r\n      if (d > distance_2)\r\n      {\r\n         print_error(q, d);\r\n         distance_2 = d;\r\n      }\r\n      p1_last = p1;\r\n      p2_last = p2;\r\n   }\r\n\r\n   return 0;\r\n}\r\n```\r\n","Update: that was tested against develop, there was an old root finding bug which resurfaced in 1.84 causing excessive iterations to be used, I need to double check that...","This is our bad: it's a resurfacing of https:\/\/github.com\/boostorg\/math\/issues\/184 as a result of accepting a PR we shouldn't have :(  The issue is now better tested and fixed again in 1.85 which will be out shortly (I hope).  Apologies all round.","> This is our bad: it's a resurfacing of https:\/\/github.com\/boostorg\/math\/issues\/184 as a result of accepting a PR we shouldn't have :(  The issue is now better tested and fixed again in 1.85 which will be out shortly (I hope).  Apologies all round.\n\nThanks for taking a look so quickly!","thanks a lot @jzmaddock ! I think branching for the next SciPy version is happening very soon, so 1.85 may not make it in this time around - it should make it for the next minor release though, which by the sounds of it should help address this issue. Hope that answers your question @boersmamarcel !"],"labels":["defect","scipy.stats"]},{"title":"ENH: Add Sheather-Jones bandwidth for KDE within histograms","body":"### Is your feature request related to a problem? Please describe.\n\nCurrently, I'm not aware whether it's possible to add custom bandwidths for the KDE for histograms.\n\n### Describe the solution you'd like.\n\nI'd like there to be optional bandwidths, namely those listed [https:\/\/www.kaggle.com\/code\/yuqizheng\/intro-to-kernel-density-estimation-kde](here). In particular, Sheather-Jones is said to have the best performance empirically. \n\n### Describe alternatives you've considered.\n\nI believe the only alternative is not Sheather-Jones itself, but from another paper called the Improved Sheather-Jones which I don't know much about.\n\n### Additional context (e.g. screenshots, GIFs)\n\nI learned these from Givens, G. H. G., & Hoeting, J. A. H. (2013). Computational Statistics (2nd ed.). Hoboken, NJ, United States: Wiley. The derivation for Sheather Jones is difficult, but I have the solution in the link provided, along with R-code for calculating it. There are libraries in R which do the same.","comments":["Would it be good for me to do a pull request?","Did you independently write all the R code you referred to, and if so, are you willing to distribute a Python version under SciPy's license? (The license of most R code is some form of the GPL, which is not compatible with that of SciPy.)\r\n\r\nI see in https:\/\/github.com\/statsmodels\/statsmodels\/issues\/7205#issuecomment-1712722883 the suggestion that it be added to stats models. As a more advanced method, do you think it should be in statsmodels instead? Also, is the need not met by the Python libraries that already offer it? (e.g. KDEpy)?\r\n\r\nIf you are still interested in submitting a PR, I'd suggest reaching out to the mailing list to find a reviewer before getting started. \r\nhttps:\/\/projects.scipy.org\/scipylib\/mailing-lists.html\r\nOtherwise, there is a chance that a PR would not be reviewed\/merged in reasonable time.","Yes I did write the code myself in R, and I'd be willing to distribute a Python version with the SciPy license. To be honest, I think that based on the textbook I read since it should be the go-to KDE method I think all libraries should have it.\r\n\r\nQuick question, does it being included in SciPy prevent it from being allowed in stats models (under licensing requirements)?","No, but it probably wouldn't need to be in statsmodels if it were in SciPy because SciPy is a dependency of statsmodels.","FWIW, here is another quite comprehensive post that claims SJ should generally be the default: https:\/\/aakinshin.net\/posts\/kde-bw\/"],"labels":["scipy.stats","enhancement"]},{"title":"selecting libblas on aarch64 doesn't work","body":"### Describe your issue.\n\nI select libblas according to the [documentation](http:\/\/scipy.github.io\/devdocs\/building\/blas_lapack.html).  It works fine on amd64, but not on aarch64.  Why would that be?\r\n\r\n\n\n### Reproducing Code Example\n\n```python\npip wheel -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack scipy\n```\n\n\n### Error message\n\n```shell\nCollecting scipy==1.10.1 (from -r base_adjusted.txt (line 117))\r\n  Using cached scipy-1.10.1.tar.gz (42.4 MB)\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Installing backend dependencies ... done\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [41 lines of output]\r\n      The Meson build system\r\n      Version: 1.2.2\r\n      Source dir: \/tmp\/pip-wheel-kci0verh\/scipy_2fa55fd7189e4f70917f11e7e81f343a\r\n      Build dir: \/tmp\/pip-wheel-kci0verh\/scipy_2fa55fd7189e4f70917f11e7e81f343a\/.mesonpy-383i5t4u\/build\r\n      Build type: native build\r\n      Project name: SciPy\r\n      Project version: 1.10.1\r\n      C compiler for the host machine: cc (gcc 9.4.0 \"cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\r\n      C linker for the host machine: cc ld.bfd 2.34\r\n      C++ compiler for the host machine: c++ (gcc 9.4.0 \"c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\r\n      C++ linker for the host machine: c++ ld.bfd 2.34\r\n      Cython compiler for the host machine: cython (cython 0.29.36)\r\n      Host machine cpu family: aarch64\r\n      Host machine cpu: aarch64\r\n      Compiler for C supports arguments -Wno-unused-but-set-variable: YES\r\n      Compiler for C supports arguments -Wno-unused-function: YES\r\n      Compiler for C supports arguments -Wno-conversion: YES\r\n      Compiler for C supports arguments -Wno-misleading-indentation: YES\r\n      Compiler for C supports arguments -Wno-incompatible-pointer-types: YES\r\n      Library m found: YES\r\n      Fortran compiler for the host machine: gfortran (gcc 9.4.0 \"GNU Fortran (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\")\r\n      Fortran linker for the host machine: gfortran ld.bfd 2.34\r\n      Compiler for Fortran supports arguments -Wno-conversion: YES\r\n      Checking if \"-Wl,--version-script\" : links: YES\r\n      Program cython found: YES (\/tmp\/pip-build-env-l6ul9zkr\/overlay\/bin\/cython)\r\n      Program python found: YES (\/usr\/bin\/python3.8-pyston2.3)\r\n      Found pkg-config: \/usr\/bin\/pkg-config (0.29.1)\r\n      Run-time dependency python found: YES 3.8\r\n      Program pythran found: YES (\/tmp\/pip-build-env-l6ul9zkr\/overlay\/bin\/pythran)\r\n      Run-time dependency threads found: YES\r\n      Library npymath found: YES\r\n      Library npyrandom found: YES\r\n      Did not find CMake 'cmake'\r\n      Found CMake: NO\r\n      Run-time dependency openblas found: NO (tried pkgconfig and cmake)\r\n      Run-time dependency openblas found: NO (tried pkgconfig)\r\n\r\n      ..\/..\/scipy\/meson.build:134:7: ERROR: Dependency \"OpenBLAS\" not found, tried pkgconfig\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n(using pip head)\n```\n","comments":["Are you sure that you used `-C` and not `--config-settings`? If the latter, then it's probably due to `pip` being too old on the aarch64 machine - you need at least 23.1. Before that, `pip` silently swallowed duplicate keys, hence the first one would go missing. \r\n\r\nYou could also check by swapping `-Dblas=blas` and `-Dlapack=lapack` around - `blas` is detected first, so if you put it last it should show up in the build log as being detected.","> Are you sure that you used -C and not --config-settings? If the latter, then it's probably due to pip being too old on the aarch64 machine - you need at least 23.1\r\n\r\nI tried both `-C` and `--config-settings`.\r\n\r\nI mentioned that I'm using pip development head:\r\n```\r\n$ pip --version\r\npip 23.3.dev0\r\n```\r\n\r\nI also mentioned that the same build works fine on amd64 (using same docker and script, tooling versions are the same).\r\n\r\n> You could also check by swapping -Dblas=blas and -Dlapack=lapack around - blas is detected first, so if you put it last it should show up in the build log as being detected.\r\n\r\nI tried setting `-Dblas=blas` only, so there is no question of shadowing the config-settings.  But the `Dependency \"OpenBLAS\" not found` error persists.\r\n\r\nSo I think there is a different problem, specific to aarch64.  @rgommers ","I have no idea how passing around settings, which is all implemented in pure Python code, could be different on x86-64 vs aarch64. To eliminate something else, you could check if `python -m build -Csetup-args=-Dblas=blas -Csetup-args=-Dlapack=lapack` does work on aarch64. If it does, it's possibly an issue in `pip`. ","> I have no idea how passing around settings, which is all implemented in pure Python code, could be different on x86-64 vs aarch64.\r\n\r\nIndeed, one bad assumption I was making was that my amd64 build still worked.  It has the same problem.   It was working before, so it seems like a regression, or something non-hermetic in the build.\r\n\r\n`-Dblas` being correctly passed to `meson setup` appears to depend on the set of requirements I pass to `pip wheel`.  If I specify only \"scipy\", `-Dblas` is passed.  If I specify my project's full requirements, it's not.\r\n\r\nDiff the verbose output of `pip wheel` between the two cases, and the part about building scipy is identical.  (Note:  I intentionally don't have fortran lib installed, so that the build fails and shows the meson command line so I can confirm `-Dblas`.  See end of diff.)\r\n\r\ngood: `meson setup ... -Ddebug=false -Doptimization=2 -Dblas=blas -Dlapack=lapack`\r\nbad: `meson setup ... -Ddebug=false -Doptimization=2`\r\n\r\n_[Diff redacted.  The problem depends on using `-r` itself, see next update.]_","Actually, it has nothing to do with my project's requirements.\r\n\r\nIf I specify the scipy target as a requirements file to `pip wheel`, `-Dblas` is not passed to meson:\r\n\r\n```sh\r\necho 'scipy==1.10.1' > requirements.txt\r\npip wheel --config-settings \"setup-args=-Dblas=blas\" --config-settings \"setup-args=-Dlapack=lapack\" -r requirements.txt\r\n```\r\n--> `meson setup ... -Ddebug=false -Doptimization=2`\r\n\r\nSpecifying the target directly works:\r\n```sh\r\npip wheel --config-settings \"setup-args=-Dblas=blas\" --config-settings \"setup-args=-Dlapack=lapack\" scipy==1.10.1\r\n```\r\n--> `meson setup ... -Ddebug=false -Doptimization=2 -Dblas=blas -Dlapack=lapack`\r\n\r\npip head, 23.2.1, and 22.3.1 all behave this way (of course old pip only passes the last setup-args).  So I'm still confused about how this was ever working for me.","Thanks for figuring it out - that makes more sense. `pip wheel` has various UX quirks, it looks like this is one of them. Using `python -m build` is anyway the recommended way of building wheels for any Python package, it's more robust. So I would use that instead.","The use case is building wheels for all transitive dependencies of our application.  `pip wheel` seems best suited for that, since we just pass the requirements file.\r\n\r\nI'll try filing a `pip` bug, but I'm still wondering if there is a regression somewhere (maybe the `wheel` package?), as I'm fairly sure this was working before.","Is https:\/\/github.com\/pypa\/pip\/issues\/12273 related?\r\n\r\nAnd it mentions `setuptools`, so I wonder if the regression is there.","filed https:\/\/github.com\/pypa\/pip\/issues\/12310","That issue may be related, but it's hard to tell. A separate issue seems warranted indeed - I'm following along there, let's see what the response is there.\r\n\r\n`setuptools` should not be related, aside from perhaps the explanation of some weirdness in `pip` due to legacy stuff. SciPy does not use `setuptools` since version 1.9.0"],"labels":["Build issues"]},{"title":"BUG: `rv_discrete.expect` fails when duplicate positions","body":"### Describe your issue.\n\nWhen `scipy.stats.rv_discrete` is given `values` with duplicate positions, the output of `expect` is not correct.\r\n\r\nBelow is an example where all the mass is in `x=1`, given as two diracs `x=[1, 1]` with  weights `[0.1, 0.9]`. When evaluating `P(X >= 0)` using `scipy.stats.rv_discrete.expect`, it gives 0.2 instead of 1.\r\n\r\nChanging the positions to `x = [1, 1+1e-15]` gives the correct answer 1.\n\n### Reproducing Code Example\n\n```python\nimport scipy\r\nx = [1, 1]\r\nweights = [0.1, 0.9]\r\ndist = scipy.stats.rv_discrete(a=0, b=2, values=(x, weights))\r\nprint(dist.expect(lambda x: (x >= 0)))  # Prints 0.2 instead of 1\r\nprint(dist.expect(lambda x: 1, lb=0, ub=2))  # Same\n```\n\n\n### Error message\n\n```shell\nNo error message\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.0 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"clang\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"15.0.7\",\r\n      \"commands\": \"x86_64-apple-darwin13.4.0-clang\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"3.0.2\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"clang\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"15.0.7\",\r\n      \"commands\": \"x86_64-apple-darwin13.4.0-clang++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld64\",\r\n      \"version\": \"12.3.0\",\r\n      \"commands\": \"\/Users\/runner\/miniforge3\/conda-bld\/scipy-split_1695898265789\/_build_env\/bin\/x86_64-apple-darwin13.4.0-gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.14.0\",\r\n      \"include directory\": \"..\/..\/..\/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeho\/lib\/python3.11\/site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"darwin\"\r\n    },\r\n    \"build\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"darwin\"\r\n    },\r\n    \"cross-compiled\": false\r\n  },\r\n  \"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"blas\",\r\n      \"found\": true,\r\n      \"version\": \"3.9.0\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/include\",\r\n      \"lib directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/lib\",\r\n      \"openblas configuration\": \"unknown\",\r\n      \"pc file directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"lapack\",\r\n      \"found\": true,\r\n      \"version\": \"3.9.0\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/include\",\r\n      \"lib directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/lib\",\r\n      \"openblas configuration\": \"unknown\",\r\n      \"pc file directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/lib\/pkgconfig\"\r\n    },\r\n    \"pybind11\": {\r\n      \"name\": \"pybind11\",\r\n      \"version\": \"2.11.1\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/include\"\r\n    }\r\n  },\r\n  \"Python Information\": {\r\n    \"path\": \"\/Users\/alexandreperez\/dev\/lib\/mambaforge\/envs\/test\/bin\/python\",\r\n    \"version\": \"3.11\"\r\n  }\r\n}\n```\n","comments":["I guess there is an assumption in the data that `values` define a proper discrete probability distribution, ie. list of unique x values with associated probabilities.\r\nIt might also assume strict monotonicity, but most likely not for the case where computation is based on full numeration of all support points.\r\n\r\nBut the returned value is still weird.\r\n\r\n\r\nI don't see in the code where cdf is computed, but I guess it requires strictly monotonic x (unique integers in range)\r\n","Isn't it weird that `expect` uses the unicity assumption? Since it computes $\\\\sum_i f(x_i) p_i$, it should give the same result whether $(x_i)_{1 \\\\leq i \\\\leq n}$ has duplicates or $(x_i)\\_{1 \\\\leq i \\\\leq n}$ are unique with aggregated weights. Or am I missing something?"],"labels":["defect","scipy.stats"]},{"title":"BUG: `test_lfilter_bad_object` crashes interpreter built with `--with-assertions`","body":"### Describe your issue.\n\nWhen the test suite reaches the following test, it crashes the interpreter if it's been built `--with-assertions`:\r\n\r\n```pytb\r\nscipy\/signal\/tests\/test_signaltools.py::test_lfilter_bad_object\r\n```\n\n### Reproducing Code Example\n\n```python\npip install scipy pytest hypothesis\r\npython -c 'import scipy; scipy.test(verbose=2, extra_argv=[\"-s\"], tests=[\"scipy.signal.tests.test_signaltools\"])'\r\n# '-s' added to get the error message, it's not necessary to reproduce\n```\n\n\n### Error message\n\n```shell\nlib\/python3.11\/site-packages\/scipy\/signal\/tests\/test_signaltools.py::test_lfilter_bad_object Fatal Python error: _Py_CheckSlotResult: Slot * of type float succeeded with an exception set\r\nPython runtime state: initialized\r\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\r\n\r\nCurrent thread 0x00007fe48851e740 (most recent call first):\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/numpy\/core\/numeric.py\", line 834 in convolve\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/scipy\/signal\/_signaltools.py\", line 2137 in <lambda>\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/numpy\/lib\/shape_base.py\", line 379 in apply_along_axis\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/scipy\/signal\/_signaltools.py\", line 2137 in lfilter\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/python_api.py\", line 952 in raises\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/scipy\/signal\/tests\/test_signaltools.py\", line 1906 in test_lfilter_bad_object\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/python.py\", line 194 in pytest_pyfunc_call\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/python.py\", line 1792 in runtest\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 169 in pytest_runtest_call\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 262 in <lambda>\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 341 in from_call\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 261 in call_runtest_hook\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 222 in call_and_report\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 133 in runtestprotocol\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/runner.py\", line 114 in pytest_runtest_protocol\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/main.py\", line 350 in pytest_runtestloop\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/main.py\", line 325 in _main\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/main.py\", line 271 in wrap_session\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/main.py\", line 318 in pytest_cmdline_main\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/_pytest\/config\/__init__.py\", line 169 in main\r\n  File \"\/tmp\/venv\/lib\/python3.11\/site-packages\/scipy\/_lib\/_testutils.py\", line 94 in __call__\r\n  File \"<string>\", line 1 in <module>\r\n\r\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.linalg._flinalg, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._direct, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.signal._spectral, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy.stats._statlib, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, scipy._lib._fpumode (total: 122)\r\nAborted (core dumped)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.3 1.26.0 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\n\/tmp\/venv\/lib\/python3.11\/site-packages\/scipy\/__config__.py:146: UserWarning: Install `pyyaml` for better output\r\n  warnings.warn(\"Install `pyyaml` for better output\", stacklevel=1)\r\n{\r\n  \"Compilers\": {\r\n    \"c\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"cc\"\r\n    },\r\n    \"cython\": {\r\n      \"name\": \"cython\",\r\n      \"linker\": \"cython\",\r\n      \"version\": \"0.29.36\",\r\n      \"commands\": \"cython\"\r\n    },\r\n    \"c++\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"c++\"\r\n    },\r\n    \"fortran\": {\r\n      \"name\": \"gcc\",\r\n      \"linker\": \"ld.bfd\",\r\n      \"version\": \"10.2.1\",\r\n      \"commands\": \"gfortran\"\r\n    },\r\n    \"pythran\": {\r\n      \"version\": \"0.14.0\",\r\n      \"include directory\": \"..\/..\/tmp\/pip-build-env-358nkn7q\/overlay\/lib\/python3.11\/site-packages\/pythran\"\r\n    }\r\n  },\r\n  \"Machine Information\": {\r\n    \"host\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"linux\"\r\n    },\r\n    \"build\": {\r\n      \"cpu\": \"x86_64\",\r\n      \"family\": \"x86_64\",\r\n      \"endian\": \"little\",\r\n      \"system\": \"linux\"\r\n    },\r\n    \"cross-compiled\": false\r\n  },\r\n  \"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/usr\/local\/include\",\r\n      \"lib directory\": \"\/usr\/local\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\",\r\n      \"pc file directory\": \"\/usr\/local\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"openblas\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.21.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/usr\/local\/include\",\r\n      \"lib directory\": \"\/usr\/local\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\",\r\n      \"pc file directory\": \"\/usr\/local\/lib\/pkgconfig\"\r\n    },\r\n    \"pybind11\": {\r\n      \"name\": \"pybind11\",\r\n      \"version\": \"2.11.0\",\r\n      \"detection method\": \"config-tool\",\r\n      \"include directory\": \"unknown\"\r\n    }\r\n  },\r\n  \"Python Information\": {\r\n    \"path\": \"\/opt\/python\/cp311-cp311\/bin\/python\",\r\n    \"version\": \"3.11\"\r\n  }\r\n}\n```\n","comments":["I'm rebuilding numpy and scipy with debug information right now, and I'll attach the backtrace when I'm done.","<details>\r\n<summary>Backtrace<\/summary>\r\n\r\n```\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0  __pthread_kill_implementation (threadid=<optimized out>, signo=signo@entry=6, no_tid=no_tid@entry=0) at pthread_kill.c:44\r\n44\tpthread_kill.c: No such file or directory.\r\n(gdb) bt\r\n#0  __pthread_kill_implementation (threadid=<optimized out>, signo=signo@entry=6, no_tid=no_tid@entry=0) at pthread_kill.c:44\r\n#1  0x00007f2ecf8a2e2f in __pthread_kill_internal (signo=6, threadid=<optimized out>) at pthread_kill.c:78\r\n#2  0x00007f2ecf852cc2 in __GI_raise (sig=sig@entry=6) at ..\/sysdeps\/posix\/raise.c:26\r\n#3  0x00007f2ecf83b4ed in __GI_abort () at abort.c:79\r\n#4  0x00007f2ecfd41664 in fatal_error_exit (status=-1) at Python\/pylifecycle.c:2626\r\n#5  0x00007f2ecfd41b4d in fatal_error (fd=2, header=0, prefix=0x0, msg=0x0, status=-1) at Python\/pylifecycle.c:2807\r\n#6  0x00007f2ecfd41d77 in _Py_FatalErrorFormat (func=0x7f2ecfe2bcc0 <__func__.18> \"_Py_CheckSlotResult\", \r\n    format=0x7f2ecfe2b9a0 \"Slot %s of type %s succeeded with an exception set\") at Python\/pylifecycle.c:2857\r\n#7  0x00007f2ecfb8373b in _Py_CheckSlotResult (obj=<float at remote 0x7f2ec7a02270>, slot_name=0x7f2ecfe22174 \"*\", success=1)\r\n    at Objects\/call.c:96\r\n#8  0x00007f2ecfb5cf7b in binary_op1 (v=<float at remote 0x7f2ec7a02270>, w=<float at remote 0x7f2ebf745670>, op_slot=16, \r\n    op_name=0x7f2ecfe22174 \"*\") at Objects\/abstract.c:894\r\n#9  0x00007f2ecfb5d953 in PyNumber_Multiply (v=<float at remote 0x7f2ec7a02270>, w=<float at remote 0x7f2ebf745670>)\r\n    at Objects\/abstract.c:1111\r\n#10 0x00007f2ece250801 in OBJECT_dot (ip1=0x5631b71e3ab0 \"p\\\"\\240\\307.\\177\", is1=8, ip2=0x5631b734f920 \"pVt\\277.\\177\", is2=-8, \r\n    op=0x5631b71e3180 \"\", n=1, __NPY_UNUSED_TAGGEDignore=0x7f2ec510e010)\r\n    at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/arraytypes.c.src:3890\r\n#11 0x00007f2ece3dddc2 in _pyarray_correlate (ap1=0x7f2ec510e850, ap2=0x7f2ec510e6d0, typenum=17, mode=2, inverted=0x7ffea9ef14d0)\r\n    at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/multiarraymodule.c:1312\r\n#12 0x00007f2ece3de3ff in PyArray_Correlate (op1=<numpy.ndarray at remote 0x7f2ec510e850>, \r\n    op2=<numpy.ndarray at remote 0x7f2ec510e6d0>, mode=2) at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/multiarraymodule.c:1494\r\n#13 0x00007f2ece3e2323 in array_correlate (__NPY_UNUSED_TAGGEDdummy=<module at remote 0x7f2eceac43b0>, args=0x7f2ed01b3a20, \r\n    len_args=3, kwnames=0x0) at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/multiarraymodule.c:3132\r\n#14 0x00007f2ecfcd92c3 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b39b0, throwflag=0)\r\n    at Python\/ceval.c:5096\r\n#15 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b39b0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#16 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecddd4d60, locals=0x0, \r\n    args=0x7f2ed01b39a0, argcount=2, kwnames=0x0) at Python\/ceval.c:6439\r\n#17 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecddd4d60>, stack=0x7f2ed01b39a0, \r\n    nargsf=9223372036854775810, kwnames=0x0) at Objects\/call.c:393\r\n#18 0x00007f2ecfb8335a in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecddd4d60>, args=0x7f2ed01b39a0, nargsf=9223372036854775810, kwnames=0x0)\r\n    at .\/Include\/internal\/pycore_call.h:92\r\n--Type <RET> for more, q to quit, c to continue without paging--c\r\n#19 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<function at remote 0x7f2ecddd4d60>, args=0x7f2ed01b39a0, \r\n    nargsf=9223372036854775810, kwnames=0x0) at Objects\/call.c:299\r\n#20 0x00007f2ece36696b in dispatcher_vectorcall (self=0x7f2ecddd2eb0, args=0x7f2ed01b39a0, len_args=-9223372036854775806, kwnames=0x0)\r\n    at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/arrayfunction_override.c:588\r\n#21 0x00007f2ecfb8335a in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<numpy._ArrayFunctionDispatcher at remote 0x7f2ecddd2eb0>, args=0x7f2ed01b39a0, nargsf=9223372036854775810, kwnames=0x0)\r\n    at .\/Include\/internal\/pycore_call.h:92\r\n#22 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<numpy._ArrayFunctionDispatcher at remote 0x7f2ecddd2eb0>, \r\n    args=0x7f2ed01b39a0, nargsf=9223372036854775810, kwnames=0x0) at Objects\/call.c:299\r\n#23 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3938, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#24 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3938, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#25 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ebf748540, locals=0x0, \r\n    args=0x7f2ec53c0928, argcount=1, kwnames=0x0) at Python\/ceval.c:6439\r\n#26 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ebf748540>, stack=0x7f2ec53c0928, nargsf=1, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#27 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ebf748540>, \r\n    tuple=(<numpy.ndarray at remote 0x7f2ec510e850>,), kwargs={}) at Objects\/call.c:245\r\n#28 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ebf748540>, \r\n    args=(<numpy.ndarray at remote 0x7f2ec510e850>,), kwargs={}) at Objects\/call.c:328\r\n#29 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ebf748540>, \r\n    args=(<numpy.ndarray at remote 0x7f2ec510e850>,), kwargs={}) at Objects\/call.c:355\r\n#30 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ebf748540>, \r\n    callargs=(<numpy.ndarray at remote 0x7f2ec510e850>,), kwdict={}, use_tracing=0) at Python\/ceval.c:7357\r\n#31 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3830, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#32 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3830, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#33 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecdba9800, locals=0x0, \r\n    args=0x7f2ed01b3800, argcount=3, kwnames=0x0) at Python\/ceval.c:6439\r\n#34 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecdba9800>, stack=0x7f2ed01b3800, \r\n    nargsf=9223372036854775811, kwnames=0x0) at Objects\/call.c:393\r\n#35 0x00007f2ecfb8335a in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecdba9800>, args=0x7f2ed01b3800, nargsf=9223372036854775811, kwnames=0x0)\r\n    at .\/Include\/internal\/pycore_call.h:92\r\n#36 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<function at remote 0x7f2ecdba9800>, args=0x7f2ed01b3800, \r\n    nargsf=9223372036854775811, kwnames=0x0) at Objects\/call.c:299\r\n#37 0x00007f2ece36696b in dispatcher_vectorcall (self=0x7f2ecdbaccb0, args=0x7f2ed01b3800, len_args=-9223372036854775805, kwnames=0x0)\r\n    at ..\/numpy-1.26.0\/numpy\/core\/src\/multiarray\/arrayfunction_override.c:588\r\n#38 0x00007f2ecfb8335a in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<numpy._ArrayFunctionDispatcher at remote 0x7f2ecdbaccb0>, args=0x7f2ed01b3800, nargsf=9223372036854775811, kwnames=0x0)\r\n    at .\/Include\/internal\/pycore_call.h:92\r\n#39 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<numpy._ArrayFunctionDispatcher at remote 0x7f2ecdbaccb0>, \r\n    args=0x7f2ed01b3800, nargsf=9223372036854775811, kwnames=0x0) at Objects\/call.c:299\r\n#40 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3738, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#41 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3738, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#42 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ec8b62340, locals=0x0, \r\n    args=0x7f2ebf750198, argcount=3, kwnames=0x0) at Python\/ceval.c:6439\r\n#43 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ec8b62340>, stack=0x7f2ebf750198, nargsf=3, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#44 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ec8b62340>, \r\n    tuple=([<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>, None, <float at remote 0x7f2ec7a02270>]), kwargs={}) at Objects\/call.c:245\r\n#45 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ec8b62340>, \r\n    args=([<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>, None, <float at remote 0x7f2ec7a02270>]), kwargs={}) at Objects\/call.c:328\r\n#46 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ec8b62340>, \r\n    args=([<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>, None, <float at remote 0x7f2ec7a02270>]), kwargs={}) at Objects\/call.c:355\r\n#47 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ec8b62340>, \r\n    callargs=([<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>], [<float at remote 0x7f2ec7a91690>, None, <float at remote 0x7f2ec7a02270>]), kwdict={}, use_tracing=0) at Python\/ceval.c:7357\r\n#48 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3658, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#49 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b35d0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#50 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ec6ecdbc0, locals=0x0, \r\n    args=0x7f2ed008c690 <_PyRuntime+58928>, argcount=0, kwnames=0x0) at Python\/ceval.c:6439\r\n#51 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ec6ecdbc0>, stack=0x7f2ed008c690 <_PyRuntime+58928>, \r\n    nargsf=0, kwnames=0x0) at Objects\/call.c:393\r\n#52 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ec6ecdbc0>, tuple=(), kwargs={})\r\n    at Objects\/call.c:245\r\n#53 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ec6ecdbc0>, \r\n    args=(), kwargs={}) at Objects\/call.c:328\r\n#54 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ec6ecdbc0>, args=(), kwargs={}) at Objects\/call.c:355\r\n#55 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ec6ecdbc0>, \r\n    callargs=(), kwdict={}, use_tracing=0) at Python\/ceval.c:7357\r\n#56 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3518, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#57 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3518, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#58 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc2b2c00, locals=0x0, \r\n    args=0x7f2ec536a668, argcount=1, kwnames=0x0) at Python\/ceval.c:6439\r\n#59 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc2b2c00>, stack=0x7f2ec536a668, nargsf=1, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#60 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc2b2c00>, \r\n    tuple=(<Function at remote 0x7f2ec6e63c40>,), kwargs=0x0) at Objects\/call.c:245\r\n#61 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc2b2c00>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>,), kwargs=0x0) at Objects\/call.c:328\r\n#62 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc2b2c00>, args=(<Function at remote 0x7f2ec6e63c40>,), \r\n    kwargs=0x0) at Objects\/call.c:355\r\n#63 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc2b2c00>, \r\n    callargs=(<Function at remote 0x7f2ec6e63c40>,), kwdict=0x0, use_tracing=0) at Python\/ceval.c:7357\r\n#64 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b33f0, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#65 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b32c0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#66 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc5ad120, locals=0x0, \r\n    args=0x7f2ebf7455b8, argcount=1, kwnames=('pyfuncitem',)) at Python\/ceval.c:6439\r\n#67 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc5ad120>, stack=0x7f2ebf7455b8, \r\n    nargsf=9223372036854775809, kwnames=('pyfuncitem',)) at Objects\/call.c:393\r\n#68 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, args=0x7ffea9f023d0, nargsf=1, \r\n    kwargs={'pyfuncitem': <Function at remote 0x7f2ec6e63c40>}) at Objects\/call.c:152\r\n#69 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, obj=<HookCaller at remote 0x7f2ecc00b560>, args=(), \r\n    kwargs={'pyfuncitem': <Function at remote 0x7f2ec6e63c40>}) at Objects\/call.c:482\r\n#70 0x00007f2ecfc24b1b in slot_tp_call (self=<HookCaller at remote 0x7f2ecc00b560>, args=(), \r\n    kwds={'pyfuncitem': <Function at remote 0x7f2ec6e63c40>}) at Objects\/typeobject.c:7623\r\n#71 0x00007f2ecfb83cc0 in _PyObject_MakeTpCall (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b560>, args=0x7f2ed01b32b8, nargs=0, keywords=('pyfuncitem',)) at Objects\/call.c:214\r\n#72 0x00007f2ecfb8333e in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b560>, args=0x7f2ed01b32b8, nargsf=9223372036854775808, kwnames=('pyfuncitem',))\r\n    at .\/Include\/internal\/pycore_call.h:90\r\n#73 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<HookCaller at remote 0x7f2ecc00b560>, args=0x7f2ed01b32b8, \r\n    nargsf=9223372036854775808, kwnames=('pyfuncitem',)) at Objects\/call.c:299\r\n#74 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b3258, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#75 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b31e0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#76 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc28b7e0, locals=0x0, \r\n    args=0x7f2ec5369ac8, argcount=1, kwnames=0x0) at Python\/ceval.c:6439\r\n#77 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc28b7e0>, stack=0x7f2ec5369ac8, nargsf=1, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#78 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc28b7e0>, \r\n    tuple=(<Function at remote 0x7f2ec6e63c40>,), kwargs=0x0) at Objects\/call.c:245\r\n#79 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc28b7e0>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>,), kwargs=0x0) at Objects\/call.c:328\r\n#80 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc28b7e0>, args=(<Function at remote 0x7f2ec6e63c40>,), \r\n    kwargs=0x0) at Objects\/call.c:355\r\n#81 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc28b7e0>, \r\n    callargs=(<Function at remote 0x7f2ec6e63c40>,), kwdict=0x0, use_tracing=0) at Python\/ceval.c:7357\r\n#82 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b30b8, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#83 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2f88, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#84 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc5ad120, locals=0x0, \r\n    args=0x7f2ebf747378, argcount=1, kwnames=('item',)) at Python\/ceval.c:6439\r\n#85 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc5ad120>, stack=0x7f2ebf747378, \r\n    nargsf=9223372036854775809, kwnames=('item',)) at Objects\/call.c:393\r\n#86 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, args=0x7ffea9f070d0, nargsf=1, kwargs={'item': <Function at remote 0x7f2ec6e63c40>})\r\n    at Objects\/call.c:152\r\n#87 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, obj=<HookCaller at remote 0x7f2ecc00b740>, args=(), \r\n    kwargs={'item': <Function at remote 0x7f2ec6e63c40>}) at Objects\/call.c:482\r\n#88 0x00007f2ecfc24b1b in slot_tp_call (self=<HookCaller at remote 0x7f2ecc00b740>, args=(), \r\n    kwds={'item': <Function at remote 0x7f2ec6e63c40>}) at Objects\/typeobject.c:7623\r\n#89 0x00007f2ecfb842c3 in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<HookCaller at remote 0x7f2ecc00b740>, \r\n    args=(), kwargs={'item': <Function at remote 0x7f2ec6e63c40>}) at Objects\/call.c:343\r\n#90 0x00007f2ecfb84323 in PyObject_Call (callable=<HookCaller at remote 0x7f2ecc00b740>, args=(), \r\n    kwargs={'item': <Function at remote 0x7f2ec6e63c40>}) at Objects\/call.c:355\r\n#91 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<HookCaller at remote 0x7f2ecc00b740>, \r\n    callargs=(), kwdict={'item': <Function at remote 0x7f2ec6e63c40>}, use_tracing=0) at Python\/ceval.c:7357\r\n#92 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2f00, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#93 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2d80, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#94 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc28bba0, locals=0x0, \r\n    args=0x7f2ec78a2458, argcount=2, kwnames=0x0) at Python\/ceval.c:6439\r\n#95 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc28bba0>, stack=0x7f2ec78a2458, nargsf=2, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#96 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc28bba0>, \r\n    tuple=(<Function at remote 0x7f2ec6e63c40>, 'call'), kwargs={}) at Objects\/call.c:245\r\n#97 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc28bba0>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>, 'call'), kwargs={}) at Objects\/call.c:328\r\n#98 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc28bba0>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>, 'call'), kwargs={}) at Objects\/call.c:355\r\n#99 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc28bba0>, \r\n    callargs=(<Function at remote 0x7f2ec6e63c40>, 'call'), kwdict={}, use_tracing=0) at Python\/ceval.c:7357\r\n#100 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2cd8, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#101 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2ba0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#102 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc28b560, locals=0x0, \r\n    args=0x7f2ec5491c18, argcount=2, kwnames=0x0) at Python\/ceval.c:6439\r\n#103 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc28b560>, stack=0x7f2ec5491c18, nargsf=2, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#104 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc28b560>, \r\n    tuple=(<Function at remote 0x7f2ec6e63c40>, <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < Session\\n\\n         ...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:245\r\n#105 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc28b560>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>, <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < Session\\n\\n         ...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:328\r\n#106 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc28b560>, \r\n    args=(<Function at remote 0x7f2ec6e63c40>, <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < Session\\n\\n         ...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:355\r\n#107 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc28b560>, \r\n    callargs=(<Function at remote 0x7f2ec6e63c40>, <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < Session\\n\\n         ...(truncated), kwdict=0x0, use_tracing=0)\r\n    at Python\/ceval.c:7357\r\n#108 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2a78, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#109 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2948, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#110 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc5ad120, locals=0x0, \r\n    args=0x7f2ec51401d8, argcount=1, kwnames=('item', 'nextitem')) at Python\/ceval.c:6439\r\n#111 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc5ad120>, stack=0x7f2ec51401d8, \r\n    nargsf=9223372036854775809, kwnames=('item', 'nextitem')) at Objects\/call.c:393\r\n#112 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, args=0x7ffea9f0e420, nargsf=1, \r\n    kwargs={'item': <Function at remote 0x7f2ec6e63c40>, 'nextitem': <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < ...(truncated)) at Objects\/call.c:152\r\n#113 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, obj=<HookCaller at remote 0x7f2ecc00b8d0>, args=(), \r\n    kwargs={'item': <Function at remote 0x7f2ec6e63c40>, 'nextitem': <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < ...(truncated)) at Objects\/call.c:482\r\n#114 0x00007f2ecfc24b1b in slot_tp_call (self=<HookCaller at remote 0x7f2ecc00b8d0>, args=(), \r\n    kwds={'item': <Function at remote 0x7f2ec6e63c40>, 'nextitem': <Function(keywords=<NodeKeywords at remote 0x7f2ec6eed6c0>, own_markers=[], extra_keyword_matches=set(), stash=<Stash at remote 0x7f2ec7d8bc70>, _report_sections=[], user_properties=[], originalname='test_lfilter_notimplemented_input', _obj=<function at remote 0x7f2ec6ee42c0>, _fixtureinfo=<FuncFixtureInfo at remote 0x7f2ec6eed400>, fixturenames=['check_fpu_mode', 'request'], funcargs={}, _request=<FixtureRequest(_pyfuncitem=<...>, fixturename=None, _scope=<Scope(_value_='function', _name_='Function', __objclass__=<EnumType(_generate_next_value_=<function at remote 0x7f2eceab7ba0>, __module__='_pytest.scope', __annotations__={'Function': '_ScopeName', 'Class': '_ScopeName', 'Module': '_ScopeName', 'Package': '_ScopeName', 'Session': '_ScopeName'}, __doc__='\\n    Represents one of the possible fixture scopes in pytest.\\n\\n    Scopes are ordered from lower to higher, that is:\\n\\n              ->>> higher ->>>\\n\\n    Function < Class < Module < Package < ...(truncated)) at Objects\/typeobject.c:7623\r\n#115 0x00007f2ecfb83cc0 in _PyObject_MakeTpCall (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b8d0>, args=0x7f2ed01b2930, nargs=0, keywords=('item', 'nextitem')) at Objects\/call.c:214\r\n#116 0x00007f2ecfb8333e in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b8d0>, args=0x7f2ed01b2930, nargsf=9223372036854775808, kwnames=('item', 'nextitem'))\r\n    at .\/Include\/internal\/pycore_call.h:90\r\n#117 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<HookCaller at remote 0x7f2ecc00b8d0>, args=0x7f2ed01b2930, \r\n    nargsf=9223372036854775808, kwnames=('item', 'nextitem')) at Objects\/call.c:299\r\n#118 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b28b0, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#119 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b28b0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#120 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc2a4720, locals=0x0, \r\n    args=0x7f2ecb8bcd48, argcount=1, kwnames=0x0) at Python\/ceval.c:6439\r\n#121 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc2a4720>, stack=0x7f2ecb8bcd48, nargsf=1, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#122 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc2a4720>, \r\n    tuple=(<Session at remote 0x7f2ecb9352b0>,), kwargs=0x0) at Objects\/call.c:245\r\n#123 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc2a4720>, \r\n    args=(<Session at remote 0x7f2ecb9352b0>,), kwargs=0x0) at Objects\/call.c:328\r\n#124 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc2a4720>, args=(<Session at remote 0x7f2ecb9352b0>,), \r\n    kwargs=0x0) at Objects\/call.c:355\r\n#125 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc2a4720>, \r\n    callargs=(<Session at remote 0x7f2ecb9352b0>,), kwdict=0x0, use_tracing=0) at Python\/ceval.c:7357\r\n#126 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2788, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#127 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2658, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#128 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc5ad120, locals=0x0, \r\n    args=0x7f2ecc5fff78, argcount=1, kwnames=('session',)) at Python\/ceval.c:6439\r\n#129 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc5ad120>, stack=0x7f2ecc5fff78, \r\n    nargsf=9223372036854775809, kwnames=('session',)) at Objects\/call.c:393\r\n#130 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, args=0x7ffea9f13120, nargsf=1, \r\n    kwargs={'session': <Session at remote 0x7f2ecb9352b0>}) at Objects\/call.c:152\r\n#131 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, obj=<HookCaller at remote 0x7f2ecc00b9c0>, args=(), \r\n    kwargs={'session': <Session at remote 0x7f2ecb9352b0>}) at Objects\/call.c:482\r\n#132 0x00007f2ecfc24b1b in slot_tp_call (self=<HookCaller at remote 0x7f2ecc00b9c0>, args=(), \r\n    kwds={'session': <Session at remote 0x7f2ecb9352b0>}) at Objects\/typeobject.c:7623\r\n#133 0x00007f2ecfb83cc0 in _PyObject_MakeTpCall (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b9c0>, args=0x7f2ed01b2650, nargs=0, keywords=('session',)) at Objects\/call.c:214\r\n#134 0x00007f2ecfb8333e in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00b9c0>, args=0x7f2ed01b2650, nargsf=9223372036854775808, kwnames=('session',))\r\n    at .\/Include\/internal\/pycore_call.h:90\r\n#135 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<HookCaller at remote 0x7f2ecc00b9c0>, args=0x7f2ed01b2650, \r\n    nargsf=9223372036854775808, kwnames=('session',)) at Objects\/call.c:299\r\n#136 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b25e8, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#137 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b24b0, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#138 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc2a45e0, locals=0x0, \r\n    args=0x7f2ecbb3ede8, argcount=1, kwnames=0x0) at Python\/ceval.c:6439\r\n#139 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc2a45e0>, stack=0x7f2ecbb3ede8, nargsf=1, \r\n    kwnames=0x0) at Objects\/call.c:393\r\n#140 0x00007f2ecfb83e52 in _PyVectorcall_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    func=0x7f2ecfb8440c <_PyFunction_Vectorcall>, callable=<function at remote 0x7f2ecc2a45e0>, \r\n    tuple=(<Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=False, asyncio...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:245\r\n#141 0x00007f2ecfb8422a in _PyObject_Call (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, callable=<function at remote 0x7f2ecc2a45e0>, \r\n    args=(<Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=False, asyncio...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:328\r\n#142 0x00007f2ecfb84323 in PyObject_Call (callable=<function at remote 0x7f2ecc2a45e0>, \r\n    args=(<Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=False, asyncio...(truncated), kwargs=0x0)\r\n    at Objects\/call.c:355\r\n#143 0x00007f2ecfce356d in do_call_core (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=<function at remote 0x7f2ecc2a45e0>, \r\n    callargs=(<Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=False, asyncio...(truncated), kwdict=0x0, use_tracing=0)\r\n    at Python\/ceval.c:7357\r\n#144 0x00007f2ecfcdb496 in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2388, throwflag=0)\r\n    at Python\/ceval.c:5381\r\n#145 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2258, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#146 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc5ad120, locals=0x0, \r\n    args=0x7f2ecbf030f8, argcount=1, kwnames=('config',)) at Python\/ceval.c:6439\r\n#147 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc5ad120>, stack=0x7f2ecbf030f8, \r\n    nargsf=9223372036854775809, kwnames=('config',)) at Objects\/call.c:393\r\n#148 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, args=0x7ffea9f17e20, nargsf=1, \r\n    kwargs={'config': <Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=Fals...(truncated)) at Objects\/call.c:152\r\n#149 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc5ad120>, obj=<HookCaller at remote 0x7f2ecc00aca0>, args=(), \r\n    kwargs={'config': <Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=Fals...(truncated)) at Objects\/call.c:482\r\n#150 0x00007f2ecfc24b1b in slot_tp_call (self=<HookCaller at remote 0x7f2ecc00aca0>, args=(), \r\n    kwds={'config': <Config(option=<Namespace(keyword='', markexpr='not slow', maxfail=0, continue_on_collection_errors=False, confcutdir=None, noconftest=False, keepduplicates=False, collect_in_virtualenv=False, importmode='prepend', basetemp=None, durations=None, durations_min=<float at remote 0x7f2ecc5fd4b0>, version=0, plugins=[], traceconfig=False, showfixtures=False, show_fixtures_per_test=False, verbose=1, no_header=False, no_summary=False, reportchars='fE', disable_warnings=False, showlocals=True, tbstyle='short', showcapture='all', fulltrace=False, color='auto', code_highlight='yes', capture='no', runxfail=False, pastebin=None, assertmode='rewrite', xmlpath=None, junitprefix=None, doctestmodules=False, doctestreport='udiff', doctestglob=[], doctest_ignore_import_errors=False, doctest_continue_on_failure=False, last_failed_no_failures='all', stepwise=False, stepwise_skip=False, logger_disable=[], forked=False, maxworkerrestart=None, dist='no', tx=[], distload=False, rsyncdir=[], rsyncignore=[], looponfail=Fals...(truncated)) at Objects\/typeobject.c:7623\r\n#151 0x00007f2ecfb83cc0 in _PyObject_MakeTpCall (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00aca0>, args=0x7f2ed01b2230, nargs=0, keywords=('config',)) at Objects\/call.c:214\r\n#152 0x00007f2ecfb8333e in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<HookCaller at remote 0x7f2ecc00aca0>, args=0x7f2ed01b2230, nargsf=9223372036854775808, kwnames=('config',))\r\n    at .\/Include\/internal\/pycore_call.h:90\r\n#153 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<HookCaller at remote 0x7f2ecc00aca0>, args=0x7f2ed01b2230, \r\n    nargsf=9223372036854775808, kwnames=('config',)) at Objects\/call.c:299\r\n#154 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2180, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#155 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2090, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#156 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecc81d260, locals=0x0, \r\n    args=0x7f2ecc875778, argcount=1, kwnames=('verbose', 'extra_argv', 'tests')) at Python\/ceval.c:6439\r\n#157 0x00007f2ecfb84528 in _PyFunction_Vectorcall (func=<function at remote 0x7f2ecc81d260>, stack=0x7f2ecc875778, \r\n    nargsf=9223372036854775809, kwnames=('verbose', 'extra_argv', 'tests')) at Objects\/call.c:393\r\n#158 0x00007f2ecfb839c6 in _PyObject_FastCallDictTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc81d260>, args=0x7ffea9f1a500, nargsf=1, \r\n    kwargs={'verbose': 2, 'extra_argv': ['-s'], 'tests': ['scipy.signal.tests.test_signaltools']}) at Objects\/call.c:152\r\n#159 0x00007f2ecfb84890 in _PyObject_Call_Prepend (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<function at remote 0x7f2ecc81d260>, obj=<PytestTester(module_name='scipy') at remote 0x7f2ecdb6de90>, args=(), \r\n    kwargs={'verbose': 2, 'extra_argv': ['-s'], 'tests': ['scipy.signal.tests.test_signaltools']}) at Objects\/call.c:482\r\n#160 0x00007f2ecfc24b1b in slot_tp_call (self=<PytestTester(module_name='scipy') at remote 0x7f2ecdb6de90>, args=(), \r\n    kwds={'verbose': 2, 'extra_argv': ['-s'], 'tests': ['scipy.signal.tests.test_signaltools']}) at Objects\/typeobject.c:7623\r\n#161 0x00007f2ecfb83cc0 in _PyObject_MakeTpCall (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<PytestTester(module_name='scipy') at remote 0x7f2ecdb6de90>, args=0x7f2ed01b2078, nargs=0, \r\n    keywords=('verbose', 'extra_argv', 'tests')) at Objects\/call.c:214\r\n#162 0x00007f2ecfb8333e in _PyObject_VectorcallTstate (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, \r\n    callable=<PytestTester(module_name='scipy') at remote 0x7f2ecdb6de90>, args=0x7f2ed01b2078, nargsf=9223372036854775808, \r\n    kwnames=('verbose', 'extra_argv', 'tests')) at .\/Include\/internal\/pycore_call.h:90\r\n#163 0x00007f2ecfb840d9 in PyObject_Vectorcall (callable=<PytestTester(module_name='scipy') at remote 0x7f2ecdb6de90>, \r\n    args=0x7f2ed01b2078, nargsf=9223372036854775808, kwnames=('verbose', 'extra_argv', 'tests')) at Objects\/call.c:299\r\n#164 0x00007f2ecfcd72cc in _PyEval_EvalFrameDefault (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2020, throwflag=0)\r\n    at Python\/ceval.c:4774\r\n#165 0x00007f2ecfcbea68 in _PyEval_EvalFrame (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, frame=0x7f2ed01b2020, throwflag=0)\r\n    at .\/Include\/internal\/pycore_ceval.h:73\r\n#166 0x00007f2ecfce028a in _PyEval_Vector (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, func=0x7f2ecebd2020, \r\n    locals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, args=0x0, \r\n    argcount=0, kwnames=0x0) at Python\/ceval.c:6439\r\n#167 0x00007f2ecfcc2446 in PyEval_EvalCode (co=<code at remote 0x7f2eceb80f30>, \r\n    globals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    locals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>})\r\n    at Python\/ceval.c:1154\r\n#168 0x00007f2ecfd4b1c9 in run_eval_code_obj (tstate=0x7f2ed00a6a18 <_PyRuntime+166328>, co=0x7f2eceb80f30, \r\n    globals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    locals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>})\r\n    at Python\/pythonrun.c:1712\r\n#169 0x00007f2ecfd4b2b5 in run_mod (mod=0x5631b5d8c2a8, filename='<string>', \r\n    globals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    locals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    flags=0x7ffea9f1ccf0, arena=0x7f2eceb1b7d0) at Python\/pythonrun.c:1733\r\n#170 0x00007f2ecfd4ae9b in PyRun_StringFlags (\r\n    str=0x7f2eceb5c5f0 \"import scipy; scipy.test(verbose=2, extra_argv=[\\\"-s\\\"], tests=[\\\"scipy.signal.tests.test_signaltools\\\"])\\n\", \r\n    start=257, \r\n    globals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    locals={'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': <type at remote 0x5631b5d3beb0>, '__spec__': None, '__annotations__': {}, '__builtins__': <module at remote 0x7f2eceb88a90>, 'scipy': <module at remote 0x7f2ecea282c0>}, \r\n    flags=0x7ffea9f1ccf0) at Python\/pythonrun.c:1603\r\n#171 0x00007f2ecfd487b5 in PyRun_SimpleStringFlags (\r\n    command=0x7f2eceb5c5f0 \"import scipy; scipy.test(verbose=2, extra_argv=[\\\"-s\\\"], tests=[\\\"scipy.signal.tests.test_signaltools\\\"])\\n\", flags=0x7ffea9f1ccf0) at Python\/pythonrun.c:487\r\n#172 0x00007f2ecfd76efd in pymain_run_command (\r\n    command=0x5631b5d1ce00 L\"import scipy; scipy.test(verbose=2, extra_argv=[\\\"-s\\\"], tests=[\\\"scipy.signal.tests.test_signaltools\\\"])\\n\") at Modules\/main.c:255\r\n#173 0x00007f2ecfd77ad0 in pymain_run_python (exitcode=0x7ffea9f1cd94) at Modules\/main.c:592\r\n#174 0x00007f2ecfd77c77 in Py_RunMain () at Modules\/main.c:680\r\n#175 0x00007f2ecfd77d31 in pymain_main (args=0x7ffea9f1ce10) at Modules\/main.c:710\r\n#176 0x00007f2ecfd77df1 in Py_BytesMain (argc=3, argv=0x7ffea9f1cf78) at Modules\/main.c:734\r\n#177 0x00005631b535f185 in main (argc=3, argv=0x7ffea9f1cf78) at .\/Programs\/python.c:15\r\n```\r\n<\/details>","Thanks for the report @mgorny. This test is a bit funky, and already skipped for interpreter debug builds: https:\/\/github.com\/scipy\/scipy\/blob\/9c71e154b164e0f96d90a09015770b69060ba7ee\/scipy\/signal\/tests\/test_signaltools.py#L1898-L1905 \r\n\r\nThe relevant [`lfilter` C code](https:\/\/github.com\/scipy\/scipy\/blob\/9c71e154b164e0f96d90a09015770b69060ba7ee\/scipy\/signal\/_lfilter.c.in) hasn't been touched in 14 years, and has comments like:\r\n```C\r\n\/* There is the start of an OBJECT_filt, but it may need work *\/\r\n\r\n\/* My reference counting might not be right *\/\r\n```\r\n\r\nThe code doesn't seem to raise `TypeError` nor have any kind of input validation, so it seems to rely on Python\/NumPy to handle object arrays with non-numeric Python objects in them. That seems inherently fragile.\r\n\r\nRelated issues:\r\n- other missing input validation for `lfilter`: gh-11359.\r\n- `pytest-leaks` reporting a leak for `test_nonnumeric_dtypes[lfilter]`: gh-11142\r\n\r\nThe `dtype=object` code path seems a little weird and incomplete. Both `lfilter` and `medfilt` have it. We probably can address this issue quickly with better input validation, and in addition should consider deprecation object arrays completely.","hi folks, any updates? I keep getting the error : ValueError: could not convert b, a, and x to a common type\r\n\r\n"],"labels":["defect","scipy.signal"]},{"title":"TST: `cluster`: add missing dtype checks","body":"Of the tests in `cluster` which use the xp-agnostic assertions (`xp_assert_close` et al. added in gh-19251), two currently have the dtype check turned off. It is not currently clear how to resolve this, but it would be nice to have full coverage of dtype checks.\r\n\r\n- [ ] In [`test_cut_tree`](https:\/\/github.com\/scipy\/scipy\/blob\/45e875d819c24a3adaa187a1d7050c164d3e937d\/scipy\/cluster\/tests\/test_hierarchy.py#L1268-L1296), `cutree.dtype` varies between `int32` and `int64` over platforms (caught by CI).\r\n- [ ] In [`test_py_vq`](https:\/\/github.com\/scipy\/scipy\/blob\/45e875d819c24a3adaa187a1d7050c164d3e937d\/scipy\/cluster\/tests\/test_vq.py#L130-L139), `label1.dtype` varies between `int32` and `int64` over platforms (caught by CI).\r\n\r\nIt would be good to document which dtypes we expect to be returned for which inputs, although that would be a huge effort if applied to the whole of SciPy. Likewise, it would be nice to have full test coverage of every accepted input dtype, but that seems infeasible at scale.\r\n\r\ncc @mdhaber","comments":[],"labels":["scipy.cluster","maintenance"]},{"title":"least_square.py line 825: make_strictly_feasible rstep=0 (1.11.2)","body":"rstep parameter was set to 0 in make_strictly_feasible call and it's causing nan error in this line: optimize._lsq.trf.py  line 234   Delta = norm(x0 * scale_inv \/ v**0.5)\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/bf776169c753fff655200dc15ae26db95a083b02\/scipy\/optimize\/_lsq\/least_squares.py#L825","comments":["Do you have an example to reproduce this?","> Do you have an example to reproduce this?\r\n\r\nHello @j-bowhay. If you can install volmdlr: https:\/\/github.com\/Dessia-tech\/volmdlr. It's a python open-source package for 3D engineering applications. Then, in the tests folders we have several examples. You can use tests\/edges\/test_arc3d.py -> test_minimum_distance: https:\/\/github.com\/Dessia-tech\/volmdlr\/blob\/master\/tests\/edges\/test_arc3d.py#L154","A small, reproducing code snippet separate from external libs is always nice to have, as is an indication of which version of SciPy you used. That also helps for regression test development.\r\n\r\nAlso, `make_strictly_feasible` issues were recently discussed in https:\/\/github.com\/scipy\/scipy\/issues\/19103, and we should perhaps ping @hwalinga and @nmayorov from gh-19111 fix that was merged\/slated for SciPy `1.11.3`.","> A small, reproducing code snippet separate from external libs is always nice to have, as is an indication of which version of SciPy you used. That also helps for regression test development.\r\n> \r\n> Also, `make_strictly_feasible` issues were recently discussed in #19103, and we should perhaps ping @hwalinga and @nmayorov from [gh-19111](https:\/\/github.com\/scipy\/scipy\/pull\/19111) fix that was merged\/slated for SciPy `1.11.3`.\r\n\r\n@tylerjereddy I understand. It was after version 1.11.2. We use least_squares to perform some complicated calculations between geometrical entities, that's why it's hard to easily reproduce the behavior. I will see if I can reproduce the same behavior with a simpler code snippet. But it seems that the problem is when the lower bounds are 0.0, 0.0, np.nextafter returns really small subatomic floating values (5e-324), that makes calculations tend to infinity and then we get some zerodivision errors in code execution. When I changed my lower bounds and initial condition from zero to a really small value like 1e-10, I didn't get errors:\r\n\r\n![image](https:\/\/github.com\/scipy\/scipy\/assets\/89979482\/64722ac1-4018-4552-9a89-c9d9a9bfce6b)\r\n"],"labels":["defect","scipy.optimize"]},{"title":"ENH: Add an option to compute weighted (in physics called \"radical\") Voronoi diagram in Scipy.Spatial.Voronoi.","body":"### Is your feature request related to a problem? Please describe.\n\nWhile analyzing the structure of a multi-component system, especially in amorphous materials, it's necessary to take atom type into account, that is, the line segment of each neighbor pair is divided by the Voronoi polyhedron face according to the ratio of the \"weights\" of the two atoms. In practice, the weight usually depends on the atom radius.\n\n### Describe the solution you'd like.\n\nAdd an option to Scipy.Spatial.Voronoi, named \"weights\" or something else, takes one numpy array as a variable, the length of which is the same as the first variable-points, and finally gives the result of a weighted Voronoi diagram. \r\nThe default value of the new option \"weights\" is an array with all elements=1, the length depends on points. The default value will give the previous Voronoi diagram such that each polyhedron face still bisects the neighbor pair segment vertically.\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["I assume that's the same thing as https:\/\/en.wikipedia.org\/wiki\/Power_diagram ?\r\n\r\nFrom a quick search, it looks like there's an MIT-licensed Python implementation in 2D that leverages SciPy `ConvexHull` here: https:\/\/gist.github.com\/marmakoide\/45d5389252683ae09c2df49d0548a627\r\n\r\nI see a Julia implementation, but looks like GPL license so I won't look at the source.\r\n\r\nAre you proposing support in arbitrary (`N`) dimensions?\r\n\r\nIt looks like one of the seminal papers (https:\/\/doi.org\/10.1137\/0214006) has been cited 306 times, which perhaps isn't too bad from a general usefulness standpoint.","Not exactly the same as Power diagram. The Power diagram is based on the definition of power distance. However, in 2D case, from the perspective of Euclidean distance, consider the (Euclidean) distance of the neighbor points to the Voronoi edge, the ratio actually depends on the (Euclidean) distance between the center (of the circle).\r\n\r\nI prefer the ratio being a prescribed constant, which is independent of the (Euclidean) distance between the center. (Actually I'm not sure whether this idea is reasonable in physical background, and it seems that nobody has discussed this issue academically.)\r\n_If this idea is too odd or unusual to be implemented, power diagram still meets the needs for research._\r\n\r\nNevertheless, it will be useful to support arbitrary dimensions, at least 3D, for real space systems. I propose to add such a feature."],"labels":["enhancement","scipy.spatial"]},{"title":"ENH: Add option to silence Gimbal Warning in `Rotation.as_euler()`","body":"### Is your feature request related to a problem? Please describe.\n\nBecause of Gimbal Lock, `Rotation`'s `as_euler(..)` can emit a warning when such a case is encountered.\r\n\r\nWhile typically one can stop warning with a `catch_warnings()` and `warnings.filter` this messes up other warnings because of python\/cpython#73858.\r\n\r\nIt would be great to have a option to not emit the warning at all when computing euler angles.\n\n### Describe the solution you'd like.\n\nAdd an `gimbal_lock_ok=False` default keyword argument to `as_euler()` and do not emit the warning when set to `True`. Any other naming would be ok.\n\n### Describe alternatives you've considered.\n\nLeave things as is until `catch_warnings` fixed upstream.\r\n\r\nAdd an attribute to `Rotation` to disable warnings.\n\n### Additional context (e.g. screenshots, GIFs)\n\nThis affects napari, napari\/napari#6241, we would like to use SciPy's rotation, but the warning issues are a bit annoying in Desktop App. \r\n\r\nI also already have a patch, and extracted #19302 that only refactor test before sending the patch.","comments":[],"labels":["enhancement","scipy.spatial"]},{"title":"ENH: Implemenation of other solvers from ODEPACK in ```scipy.integrate```","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nCurrently ```scipy.integrate``` implements different solvers through its ```ode``` generic interface class. Using ```ode```, one can use solvers like vode\/zvode, lsoda, dopr5 etc. That is, only one solver from ODEPACK (that is lsoda) is implemented in ```scipy```. However, ODEPACK contains many more like lsode, lsodes, lsodar, lsodpk, lsodkr (for explicit systems) and lsodi, lsoibt, lsodis (detailed documentation of each of these can be found on the [netlib website](https:\/\/www.netlib.org\/odepack\/opkd-sum)). \r\n\r\nFor my work, I want to use lsodes specifically because it uses some extra optimization for sparse Jacobians. lsoda does support banded jacobians but there have been issues which hint at some bad implementation (might be ```f2py``` thing, but I am not quite sure (#10793 & #10864). I am quite surprised that these possible bugs have been open issues with zero to none discussion for 4+ years now.\r\n\r\n### Describe the solution you'd like.\r\n\r\nThis issue is basically to gauge community thoughts on adding more solvers from ODEPACK, and also perhaps fixing the issues (if any) that lsoda might have. I would also like to know why lsoda was chosen specifically, instead of the more general lsodes.\r\n\r\n\r\n### Describe alternatives you've considered.\r\n\r\nI looked into an old library that Hans Petter Langtangen wrote called [odespy](https:\/\/github.com\/hplgit\/odespy) which aimed to do exactly this. It adapted a lot of solvers from ODEPACK and constructed user-friendly Python interfaces to use them, and lsodes was one of them. Unfortunately, Hans passed away in late 2016 after battling with cancer (RIP), and virtually no development happened of that project after it so it remains in Python 2.7. No one has taken the ownership of that repository and the few forks of it have been very lukewarm attempts at trying to keep it alive.\r\n\r\nIn all honesty, I would like to change that. I am open to be a maintainer of a new fork of ```odespy``` which can become the new main repo for the project. But that's a whole other thing, I will have to contact Hans' colleagues and\/or students and then I will see if I can get any permissions for it and so on.\r\n\r\nBut first, I would like to have discussion with the SciPy community about this. Because, I think ```odespy``` was (is) a great library. It should not die. So if it possible to integrate parts of it into ```scipy.integrate```, I believe it will be in benefit of the whole community as a whole. Also this issue given a chance to look back at those open issues that are currently in the implementation of lsoda as well (I would really like if readers of this issue also go through them, a special thanks to @MatthewFlamm for finding them).\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["Hi @kmaitreys, thanks for opening this discussion. As for why LSODA was wrapped first\/only, I don't know - that's from the earliest times of SciPy. Maybe @rkern remembers? It may just be a case of all-round good performance and familiarily (see, e.g., [Benchmarking of numerical integration methods for ODE models of biological systems - Scientific Reports (2021)](https:\/\/www.nature.com\/articles\/s41598-021-82196-2)).\r\n\r\nWhy bugs went unsolved: basically because all Fortran 77 is very hard to maintain, there are few maintainers who know Fortran well and even fewer who enjoy working on Fortran 77 (that number is likely to be 0 actually). We'd really like to get rid of all F77 code - see gh-18566. Most of this code is of really poor quality, and that goes for ODEPACK too I think (and the README of `odespy` says _\"There have been some unidentified problems with running this solver (segmentation fault).\"_ for a reason).\r\n\r\n> Because, I think `odespy` was (is) a great library. It should not die. So if it possible to integrate parts of it into `scipy.integrate`, I believe it will be in benefit of the whole community as a whole.\r\n\r\nI think new solvers that add capabilities not yet present in `scipy.integrate` are very welcome. However, it would be best to translate the solver into Python\/Cython\/C\/C++ first.","Thanks for the reply @rgommers and telling me about why the bugs still exist. Difficulties in handling of F77 legacy code is completely understandable. \r\n\r\n> I think new solvers that add capabilities not yet present in scipy.integrate are very welcome. However, it would be best to translate the solver into Python\/Cython\/C\/C++ first.\r\n\r\nI agree, but I am not sure if all the solvers from ODEPACK have some rewrites of them in other than F77 (what about F90\/95 btw? they are fairly modern and simple languages to handle?). I know that a C++ LSODA implementation [exists](https:\/\/github.com\/dilawar\/libsoda-cxx), but what about others? I tried finding more but didn't land up with anything of substance. If nothing, implementing this C++ implementation could be a good start, especially since ```mrgsolve``` have done it, and they think it was [good](https:\/\/github.com\/metrumresearchgroup\/mrgsolve\/issues\/504). ","https:\/\/www.stochasticlifestyle.com\/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran\/\r\n\r\nI think this a fairly nice comparison of the current landscape. To me at least, [Sundials](https:\/\/computation.llnl.gov\/projects\/sundials)\u2018 CVODE seems like a more appealing prospect than wrapping LSODE","Thanks for the reply @j-bowhay. I researched into SUNDIALS's suite of solvers and how it can be thought as a successor to ODEPACK. Now they are two options, I write my own wrapper to SUNDIALS's ```CVODE``` or I use a wrapper which has already been written. One library I found was ```scikits.odes```, ~but unfortunately it only supports till Python 3.9 (my project currently is Python 3.10+~).\r\n\r\n[Edit: ```scikits.odes``` does support Python 3.10+]\r\n\r\nDo you have some more suggestions?","There is https:\/\/github.com\/bjodah\/pycvodes although I do not have any experience with it. For my personal use (and this pains me to say as a SciPy maintainer) I usually use [DifferentialEquations.jl](https:\/\/docs.sciml.ai\/DiffEqDocs\/stable\/). I would love to see more modern solvers in SciPy to close the gap with the Julia ecosystem however this isn't something I have the capacity to implement.","Yeah I have looked into Julia's library. The problem is, my RHS function and Jacobian function are in Python, a bit involved, and JIT-compiled through Numba. So I am not sure if I will be able to call Julia from Python for my solver. But if you have any suggestions, we can discuss it (although at some other platform, because that will not be relevant to this scipy specific issue)","In future, if anyone stumbles on this thread, ```pycvodes``` will not work with Numba due to some issues which I believe originate from Cython (or whatever Python-C\/C++ interop mechanism which uses ```memory_view``` instead of arrays for internal time step calculations).","The other wish list item for me would be for the scipy solvers to take a low-level callable. Any wrapped solver written in compiled code will lose most of its speed if it has to keep calling back to python land for the rhs function.","@j-bowhay Would that [LSODA implementation](https:\/\/github.com\/Nicholaswogan\/numbalsoda) be a good proof-of-concept of what you mentioned?","@rgommers Considering your last insight on the state of Fortran dependencies in `scipy`, I would think that you (and the greater `scipy` community) will not be thrilled to add one more method from ODEPACK to `scipy.integrate`? I actually want to add `lsodes` as an additional method for systems with arbitrary sparse structure and recently started the implementation on my fork. Systems like large chemical network solving chemical kinetics are stiff and pre-existing codes have overwhelmingly relied on `DLSODES` and I would love that functionality as an option as well.\r\n\r\nI would love to raise a PR after I am done with implementing it, but I have a feeling it will not be well received. In that scenario, do you have anything that would help if I wanted to publish my implementation on PyPI perhaps, in the form of a very scipy like wrapper of `DLSODES`? Maybe some documentation on how to interface with Fortran\/C code using `f2py` and so on. I think for an implementation in `scipy` it would not be much difficult but I am unsure about how will I hook up any external dependencies in a standalone package. ","Hi @kmaitreys, I suspect you are right - we're trying hard to get rid of Fortran 77, so we'd much prefer to not add more F77 code at this point. The functionality sounds useful, but ODEPACK code as it stands isn't maintainable for us.\r\n \r\n> I think for an implementation in `scipy` it would not be much difficult but I am unsure about how will I hook up any external dependencies in a standalone package.\r\n\r\nI had a closer look, and it's a bit of a pain because of the dependency on LAPACK. That makes building wheels for distribution on PyPI hard - it's issue number one for SciPy wheels. There are several potential paths to take:\r\n\r\n1. Keep as F77 code, and distribute it on conda-forge only, plus sdist on PyPI, (probably lowest-effort right now, dealing with LAPACK in conda-forge is way easier than for PyPI).\r\n2. Keep as F77 code, and distribute on PyPI in the same way as SciPy does (hard packaging-wise)\r\n3. Translate code to C++\/C\/Cython and contribute it to SciPy after all (not sure of effort, but squashes the distribution problem completely).\r\n\r\nI really can't recommend (2) unfortunately. We're vendoring OpenBLAS and gfortran runtime libraries, which takes a lot of effort - and while you could copy the approach we take for NumPy\/SciPy, it's fragile and the machinery for it (https:\/\/github.com\/MacPython\/openblas-libs + CI scripts in this repo) is undergoing surgery right now.","Thank you so much for replying, Ralf. I really appreciate the directions you have given. I will look into them. Might be difficult at the moment for me as it does sound a bit involved but some day, I think it would be a nice standalone utility for people looking for such solvers.\n\nCheers!"],"labels":["enhancement","scipy.integrate"]},{"title":"ENH: Unnecessary copy in Rotation.as_euler ?","body":"### Is your feature request related to a problem? Please describe.\n\nI have the impression that `Rotation.as_euler(...)` makes unnecessary copies of arrays. \r\n\r\nThe core of the logic is there:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/65df3c0cc190d113842963b1bf6c37be38539717\/scipy\/spatial\/transform\/_rotation.pyx#L1680-L1685\r\n\r\n\r\nBut `_compute_euler_from_quat` does not seem to modify `quat`,  and `self.as_quat()` appear to always copy in the context (`canonical=False` by default):\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/65df3c0cc190d113842963b1bf6c37be38539717\/scipy\/spatial\/transform\/_rotation.pyx#L1442-L1451\r\n\r\n\r\nPlus if my understanding is correct\r\n\r\n```\r\n    if quat.ndim == 1: \r\n         quat = quat[None, :]\r\n```\r\n\r\nIs doing the exact opposite as the `if self._single\/else` of `as_quat()` we called just before.\n\n### Describe the solution you'd like.\n\nI _think_ in this case it should be safe to directly pass `self._quat`, though that could be a problem for subclasses that have their own `as_quat()` implementation so backward incompatible. I guess that could affects things like Jax that reimplement or wrap scipy rotations ?\r\n\r\nThough it should be possible to have either `as_quat(copy=False)` keyword maybe ?\n\n### Describe alternatives you've considered.\n\nLeave it as is. It is likely nobody manipulate large enough rotation matrix that the performance has an impact ? And maybe Cython also realize no copy are needed and does not do so ?\n\n### Additional context (e.g. screenshots, GIFs)\n\nJust came across it, and though it might be worth opening if someone want to play with perf.\r\n\r\nFeel free to close if you think it's too much a niche optimisation.","comments":[],"labels":["enhancement","scipy.spatial"]},{"title":"BUG: firwin(pass_zero=False) does not block DC","body":"When using [`scipy.signal.firwin()`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.firwin.html) to build a high-pass filter with `pass_zero=False`, the filter has non-zero gain at DC. Smaller values of `numtaps` will result in higher gain at DC.\r\n\r\nThis directly contradicts the docs, which plainly state:\r\n\r\n> If [`pass_zero` is] False, the DC gain is 0\r\n\r\nMathematically, \"zero gain at DC\" is equivalent to \"filter coefficients should sum up to zero\". Indeed if the filter coefficients don't sum up to zero, then the convolution with a constant function (i.e. DC) is not zero.\r\n\r\nThe problem is most obvious with the smallest possible value of `numtaps`, i.e. 3:\r\n\r\n```python\r\nnp.sum(scipy.signal.firwin(numtaps=3, cutoff=0.1, pass_zero=False))\r\n#\u00a0-> 0.9656274950638282\r\n```\r\n\r\nBut it's still there with higher values:\r\n\r\n```python\r\nnp.sum(scipy.signal.firwin(numtaps=1001, cutoff=0.1, pass_zero=False))\r\n# -> 0.0003207874153351442\r\n```\r\n\r\nThis bug only seems to affect `pass_zero=False`. If `pass_zero=True`, then the filter sums to exactly 1, as it should.\r\n\r\nSince `pass_zero=True` works, one workaround is to use that to design a lowpass filter and then manually turn it into a highpass filter:\r\n\r\n```python\r\nkernel = -scipy.signal.firwin(numtaps=1001, cutoff=0.1)\r\nkernel[int(kernel.size\/2)] += 1\r\nnp.sum(kernel)\r\n#\u00a0-> 8.326672684688674e-17 (correct within numerical precision)\r\n```\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n<details>\r\n\r\n```shell\r\n1.11.2 1.23.5 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-xfuqzuqe\/overlay\/lib\/python3.10\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp310-cp310\/bin\/python\r\n  version: '3.10'\r\n```\r\n\r\n<\/details>","comments":[],"labels":["defect","scipy.signal"]},{"title":"Reuse conjugate gradient in newton-cg","body":"Scipy has a conjugate gradient solver for linear equations, `scipy.sparse.linalg.cg`. This could be used in the optimization method \"Newton-CG\" of `scipy.optimize.minimize` which currently reimplements CG.","comments":["Related\/same issue: https:\/\/github.com\/scipy\/scipy\/issues\/8792\r\n\r\nI am not a linalg guy but if someone points me which lines could just be replaced by a call to `scipy.sparse.linalg.cg` I would give it a shot.","We took a look at it. \r\nThe lines are at https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/optimize\/_optimize.py#L2051 and we compared the cg implementation with the implementation of `scipy.sparse.linalg.cg` at  https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/sparse\/linalg\/_isolve\/iterative.py#L308 , \r\nbut i dont think we can just easily replace it by `scipy.sparse.linalg.cg`, since there are some optimizations for the matrix-vector product, with the hessian evaluations.","My background is more numerical linear algebra, but my understanding is that although the [CG iterations in Newton-CG](https:\/\/github.com\/scipy\/scipy\/blob\/ae3ca70a72ef04547fbc28501009246cef5ee6c8\/scipy\/optimize\/_optimize.py#L2199C1-L2205) are similar to [those in the CG implementation](https:\/\/github.com\/scipy\/scipy\/blob\/ae3ca70a72ef04547fbc28501009246cef5ee6c8\/scipy\/sparse\/linalg\/_isolve\/iterative.py#L416), Newton-CG needs to handle negative curvature differently (e.g., the presence of negative eigenvalues). With standard CG, the algorithm will fail if the matrix is not symmetric positive definite. However, with Newton-CG even if the Hessian is indefinite (i.e., has negative curvature), we can still extract a useful descent direction from the current iterate of CG; note the break when negative curvature is detected [here](https:\/\/github.com\/scipy\/scipy\/blob\/ae3ca70a72ef04547fbc28501009246cef5ee6c8\/scipy\/optimize\/_optimize.py#L2194C26-L2194C26). \r\n\r\nWith a bit of work, we could combine the CG component of Newton-CG with the pure CG implementation, but we would need add logic to return the partial progress in the case of negative curvature. My instinct is that combining ~5 lines of the two CG implementations doesn't merit the additional complexity, but that is up to the maintainers to decide.\r\n\r\nLooking over the code though, we may want to reconsider some of the termination conditions. For example, the default allows 20*n iterations of CG, where n is the dimension of the Hessian---this may lead to too much work finding a descent direction when taking an approximate step would be preferable. @ccmagruder Do you have any insights?\r\n\r\n"],"labels":["enhancement","scipy.optimize"]},{"title":"BUG: Imprecise Gauss-Legendre weights","body":"### Describe your issue.\r\n\r\nThe Gauss-Legendre weights generated by scipy are inaccurate. A comparison to SymPy's analytic roots shows that while the accuracy is good for small numbers of nodes, the discrepancies become noticeable for large numbers of nodes.\r\n\r\nOur C++ implementation of Gauss-Legendre quadrature in https:\/\/github.com\/wavefunction91\/IntegratorXX\/blob\/master\/include\/integratorxx\/quadratures\/primitive\/gausslegendre.hpp maintains an accuracy of `10*eps` in the nodes and weights at least up to 920 nodes. In contrast, the discrepancy in scipy grows with the number of nodes, and reaches an error for the weights of  `1.9785e-14` with 500 nodes, which is 89 times machine epsilon, and `1.3362e-13` with 1000 nodes or 601 times epsilon.\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nfrom sympy.integrals.quadrature import gauss_legendre\r\nfrom sympy.core import S\r\nfrom sympy import sqrt\r\nimport numpy\r\nimport os\r\nimport time\r\nimport scipy\r\n\r\n# Generate tests with 20 digit precision\r\nndigits = 20\r\n\r\nfor order in [10, 30, 50, 100, 300, 500, 1000]:\r\n    x, w = gauss_legendre(order, ndigits)\r\n    xi, wi = scipy.special.roots_legendre(order)\r\n    dx = xi-x\r\n    dw = wi-w\r\n\r\n    print(f'{order=} {max(abs(dx))=} {max(abs(dw))=}')\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\norder=10 max(abs(dx))=8.5408873170492866089e-17 max(abs(dw))=9.9880261667743136522e-16\r\norder=30 max(abs(dx))=1.4661039174140508279e-16 max(abs(dw))=4.0621141449071046232e-15\r\norder=50 max(abs(dx))=1.5321894279588313403e-16 max(abs(dw))=4.9165183798412732599e-15\r\norder=100 max(abs(dx))=1.6209754214900271019e-16 max(abs(dw))=7.0182654149157263719e-15\r\norder=300 max(abs(dx))=1.6611925458256612820e-16 max(abs(dw))=8.4826851919822337760e-15\r\norder=500 max(abs(dx))=1.6198403973407063394e-16 max(abs(dw))=1.9785190888976244499e-14\r\norder=1000 max(abs(dx))=1.6572623129504013284e-16 max(abs(dw))=1.3361781919090518247e-13\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n```shell\r\n>>> import sys, scipy, numpy; print(scipy.__version__, numpy.__version__, sys.version_info); scipy.show_config()\r\n1.10.1 1.24.4 sys.version_info(major=3, minor=11, micro=5, releaselevel='final', serial=0)\r\nlapack_armpl_info:\r\n  NOT AVAILABLE\r\nlapack_mkl_info:\r\n  NOT AVAILABLE\r\nopenblas_lapack_info:\r\n    libraries = ['flexiblas', 'flexiblas']\r\n    library_dirs = ['\/usr\/lib64']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nlapack_opt_info:\r\n    libraries = ['flexiblas', 'flexiblas']\r\n    library_dirs = ['\/usr\/lib64']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_armpl_info:\r\n  NOT AVAILABLE\r\nblas_mkl_info:\r\n  NOT AVAILABLE\r\nblis_info:\r\n  NOT AVAILABLE\r\nopenblas_info:\r\n    libraries = ['flexiblas', 'flexiblas']\r\n    library_dirs = ['\/usr\/lib64']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nblas_opt_info:\r\n    libraries = ['flexiblas', 'flexiblas']\r\n    library_dirs = ['\/usr\/lib64']\r\n    language = c\r\n    define_macros = [('HAVE_CBLAS', None)]\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\r\n    not found = AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\r\n```\r\n","comments":["Pull requests welcome! "],"labels":["enhancement","scipy.special"]},{"title":"BLD: start building with openblas wheel","body":"OpenBLAS wheels are now [available on PyPI](https:\/\/pypi.org\/project\/scipy-openblas64\/) thanks to @mattip.\r\n\r\nWe should figure out how to use it within the scipy build system.","comments":["Once numpy\/numpy#24749 gets merged, I can try to duplicate that work here."],"labels":["enhancement","Build issues"]},{"title":"ENH: small curvature check in _minimize_newtoncg","body":"#### Reference issue\r\nNone, but same as https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/26721\/commits\/fdc0fa36b6924b7f721a0cc94892b666f01da634 of PR https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/26721. See also the findings in the PR comments.\r\n\r\n#### What does this implement\/fix?\r\nIn https:\/\/github.com\/scipy\/scipy\/commit\/39f51e4cb10ac7c9c1cd8b3baad87af7b18598f2, a check for small curvature was introduced in newton-cg optimizer, in the inner conjugate gradient loop `curve <= 3 * eps`.\r\nThe problem with this is that it is not scale invariant, i.e. multiplying the objective function by some constant changes the triggering of this condition.\r\n\r\nThis PR propses to compare the curvature according to https:\/\/arxiv.org\/abs\/1803.02924, Algo 1 Capped Conjugate Gradient, instead to a quantity of the problem, i.e. to the norm of the step `p`.\r\n\r\n#### Additional information\r\n\r\n","comments":["This shows a number of test failures.","> This shows a number of test failures.\r\n\r\nI can't reproduce locally. All tests pass on my laptop."],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: linalg: support array API for standard extension functions","body":"_For context, this work was started as part of my internship at [Quansight Labs](https:\/\/github.com\/Quansight-Labs), which ran until the end of September 2023._\r\n#### Reference issue\r\nTowards gh-19068 and gh-18867.\r\nPlease see gh-19068 for context.\r\n\r\n#### What does this implement\/fix?\r\nSupport is added for the functions in the [array API standard `linalg` extension](https:\/\/data-apis.org\/array-api\/latest\/extensions\/linear_algebra_functions.html). This allows users to input arrays from any compatible array library.\r\n\r\nTests are modified to allow testing with `numpy.array_api`, `cupy` and `torch`. Some new tests are added for exceptions for unsupported parameters.\r\n\r\n#### Additional information\r\nI was not able to convert every relevant test since lots of NumPy-specifc things are used. We may want to find ways to convert some of these, or write new tests to serve the same purpose.\r\n\r\n`TestSVD` is a little strange due to the way the `lapack_driver` parameter is tested. I have tried to apply a minimal refactor here, but a more substantial refactor may result in something more readable. It is a bit of a misnomer that all of our array API compatible tests are under `TestSVD_GESDD`, just because `gesdd` is the default value.\r\n\r\nLots of tests are failing for PyTorch CUDA, but hopefully we just need to wait for https:\/\/github.com\/pytorch\/pytorch\/issues\/106773 to come through.","comments":[">  I think it is quite useful to test non-numpy arrays; without testing it's almost certainly going to be broken. I think of these as testing optional dependencies - just like we have tests with mpmath, scikit-umfpack and a whole bunch of other optional runtime dependencies.\r\n\r\nTesting is always nice indeed but the question is what to do when it is broken. I think none of us want to go chasing around PyTorch or CuPy repos for fixing things that is not really meant for us to do just to get our tests out to the greenland. ","> Testing is always nice indeed but the question is what to do when it is broken. I think none of us want to go chasing around PyTorch or CuPy repos for fixing things that is not really meant for us to do just to get our tests out to the greenland.\r\n\r\nI think the same of something breaks in NumPy, Cython, pytest, Sphinx or wherever else: we file an issue and skip the test or put a temporary upper bound. We're using pretty core\/standard functions here, so I am not too worried about seeing too many regressions once things work. That would be really surprising. And in terms of debugging or even contributing upstream, I'd much rather work with CuPy or PyTorch than with things like pytest\/sphinx\/mpmath.\r\n\r\nAlso, the CuPy and PyTorch teams (and Dask and JAX) have invested large amounts of effort in NumPy and SciPy compatibility, so I'm pretty sure they'd appreciate and are willing to address bug reports.","> The changes here from integer to floating point arrays for testing make sense. Functions like solve are inherently floating point-only. We also need to test that integers (and lists, and other array-like's) are still converted correctly and we don't break backwards compat. However, that can be a single small test. That many tests use integers is a matter of previous authors taking a shortcut because it didn't matter much, rather than all those tests using integers by design.\r\n\r\nI've had a go at adding tests into each class with the explicit purpose of checking that integer dtypes are coerced correctly. Once these are improved to the standard required, and thanks to the dtype checks in the new assertions, we should be in a better situation with testing dtypes than we were before this PR. There are only a few places where the dtype checks are turned off now due to `torch` outputting a lower precision. One of these is in one of the dtype test functions, which we should figure out how to remove.\r\n\r\nI think they are missing checking complex dtypes, and we may want to change the examples that they test with (I've mostly copied from one of the nearby simple tests).\r\n\r\n> Same here, the tol values are way too stringent. and also below\r\n\r\nI have tried to make them all sensible values now :+1: ","> I think the same of something breaks in NumPy, Cython, pytest, Sphinx or wherever else: we file an issue and skip the test or put a temporary upper bound.\r\n\r\nThey are dependencies pytorch et al are not though. But anyways let's see how it goes and we can react if it becomes an annoyance. ","FYI Ilhan (and other maintainers), I will be at university during October and November, so unfortunately I won't have time to work on this PR until December. I would certainly like to return to it then though :) feel free to ping about anything but I can't promise a quick response.","You have done excellent work already and thank you for your offer. Maybe we can cover some distance in the meantime but probably it would be here until you come back. ","Hi @rgommers @ilayn , I think that this is ready for another look now. I will have plenty of time between December 3rd and January 9th to contribute, so it would be great to get `linalg` converted in that time. Of course, things are very busy with `np2` and a major SciPy release, so I understand if other things have to take priority :)"],"labels":["enhancement","scipy.linalg","array types"]},{"title":"ENH\/TST\/MAINT: fft: follow-ups for array API support","body":"After gh-19005, here are some follow-ups:\r\n- [x] Look at all of the non-standard functions (like `fft2`) and rewrite them in terms of standard functions where possible to enable GPU support. https:\/\/github.com\/scipy\/scipy\/pull\/19263\r\n- [x] Remove some unnecessary test skips in `fftlog` and add comments to explain skips in `basic`. https:\/\/github.com\/scipy\/scipy\/pull\/19262\r\n- [x] Clean up dtype conversions in tests. https:\/\/github.com\/scipy\/scipy\/pull\/19282\r\n- [x] Update `doc\/source\/dev\/api-dev\/array_api.rst` to reflect support in `fft`. https:\/\/github.com\/scipy\/scipy\/pull\/19281\r\n- [x] Revert to separate `execute` functions for 1-D vs n-D in basic. https:\/\/github.com\/scipy\/scipy\/pull\/19261\r\n- [x] Remove unnecessary `TestNamespaces` now that it's incorporated in `xp_assert_close`. https:\/\/github.com\/scipy\/scipy\/pull\/19264\r\n- [x] `arg_err_msg` should be given a better name (something like `xp_unsupported_param_msg` but more concise?) and moved to `_lib._array_api.py` for use in other submodules. https:\/\/github.com\/scipy\/scipy\/pull\/19265\r\n- [ ] Write an array-agnostic `pad` or get it added to the standard (see https:\/\/github.com\/scipy\/scipy\/pull\/19005#discussion_r1319201782).\r\n- [ ] The `device` keyword of `fftfreq` and `rfftfreq` is not tested since it only works with PyTorch currently. When NumPy supports it, we should make appropriate changes to `fftfreq` and `rfftfreq` and start testing with it (numpy\/numpy#25076). This should remove the need for the `fft` change in gh-19001\r\n\r\nEdit: I have now completed most of the tasks; anyone is welcome to take the rest on!","comments":[],"labels":["scipy.fft","array types"]},{"title":"ENH: Use `highspy` in `linprog`","body":"Supersedes https:\/\/github.com\/scipy\/scipy\/pull\/18642\/.\r\n\r\n#### Reference issue\r\nCloses https:\/\/github.com\/scipy\/scipy\/issues\/15915. Closes #15888. Closes #19734.\r\n\r\n#### What does this implement\/fix?\r\n- Replaces existing `linprog` Cython bindings with upstream `highspy`\r\n\r\nTests pass (locally anyway). However, there are still plenty of improvements which can be made (once `highspy` works a bit more robustly https:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1405)\r\n#### Additional information\r\nNeeds a PR over at `highs` (https:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1460) and over at the SciPy fork (https:\/\/github.com\/scipy\/HiGHS\/pull\/63).\r\n\r\n\r\nDraft status until:\r\n- ~[ ] Changes merged upstream in a new PyPI release~ Unrelated\r\n- [x] Be 100 % backwards compatible (or document changes)\r\n- [x] Fixup the enum translation (should be quick, just too tired rn)\r\n- [x] Fixup the commit history, early stages are derived from #18642 and should have co-commit credit for @mckib2\r\n- [x] Figure out \/ fix the xfail on Linux 32-bit\r\n- [x] Add the git submodule back\r\n- [x] Point to the `scipy` fork of `highs`","comments":["Also @mckib2 if you'd like to take a look that'd be great (or maybe after its out of draft status).","This is almost done, except the build process on the SciPy CI is stricter than over at HiGHs (w.r.t warnings) so some more work is required.","You can silence classes of warnings by adding defining them (if they're not already present) here: https:\/\/github.com\/scipy\/scipy\/blob\/21dcad26e80a6f5dbda4ef7827cad6c2ecb20291\/scipy\/meson.build#L287-L300\r\n\r\nand then adding them to `cpp_args` for the relevant build target.","It'd still be nice to get them fixed upstream, but that shouldn't be blocking for finalizing this PR.","> It'd still be nice to get them fixed upstream, but that shouldn't be blocking for finalizing this PR.\r\n\r\nThanks that helped, but I'm not sure of the best approach for the rest, it seems like using a shared library ends up with [(expected) lookup errors](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6372621860\/job\/17295562238?pr=19255), but the [static library fails the wheel build](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6372457292\/job\/17295259101?pr=19255). In both cases there's a [too many public symbols error](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6372621860\/job\/17295562392?pr=19255).\r\n\r\n```bash\r\n..\/build\/subprojects\/highs\/highspy\/_highs_options.cpython-310-x86_64-linux-gnu.so: too many public symbols!\r\n000000000000d980 T PyInit__highs_options\r\n0000000000031570 T _Z10first_wordRNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEi\r\n0000000000031240 T _Z10strToLowerPc\r\n000000000002df90 T _Z11checkOptionRK15HighsLogOptionsRK15OptionRecordInt\r\n000000000002e050 T _Z11checkOptionRK15HighsLogOptionsRK18OptionRecordDouble\r\n000000000002cca0 T _Z11highsLogDevRK15HighsLogOptions12HighsLogTypePKcz\r\n000000000002e130 T _Z12checkOptionsRK15HighsLogOptionsRKSt6vectorIP12OptionRecordSaIS4_EE\r\n000000000002c8f0 T _Z12highsLogUserRK15HighsLogOptions12HighsLogTypePKcz\r\n0000000000030290 T _Z12reportOptionP8_IO_FILERK15OptionRecordIntb13HighsFileType\r\n000000000002fe00 T _Z12reportOptionP8_IO_FILERK16OptionRecordBoolb13HighsFileType\r\n00000000000[30](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6372457292\/job\/17295259302?pr=19255#step:11:31)660 T _Z12reportOptionP8_IO_FILERK18OptionRecordDoubleb13HighsFileType\r\n0000000000030a90 T _Z12reportOptionP8_IO_FILERK18OptionRecordStringb13HighsFileType\r\n0000000000030e90 T _Z13reportOptionsP8_IO_FILERKSt6vectorIP12OptionRecordSaIS3_EEb13HighsFileType\r\n000000000002dc40 T _Z14boolFromStringNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERb\r\n00000000000[31](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6372457292\/job\/17295259302?pr=19255#step:11:32)4b0 T _Z14first_word_endRNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEi\r\n000000000002dd40 T\r\n...\r\n```\r\n\r\n\r\nWhat is the best approach to dealing with libraries which (ideally) should be installed by the user? OR, I'm missing something about the way the static library should be linked, since other sub-libraries are linked statically (`quadpack`).","The current version seems quite close to ready, modulo the use of a wrap file (I assume that's for convenience during development, and it'll change to a git submodule at the end) and this error:\r\n```\r\nmeson-python: error: Could not map installation path to an equivalent wheel directory: '{libdir_static}\/libhighs.a'\r\n```\r\n\r\nThe static library linking looks fine here, the only problem is that the `libhighs` definition in the subproject has `install: true` and hence by default it's in the list of to-be-installed files, but that's not what we want. This fixes the wheel build:\r\n```\r\npython -m build -Cinstall-args=--skip-subprojects\r\n```\r\n\r\nTo make that the default behavior, add this to `pyproject.toml`:\r\n```toml\r\n[tool.meson-python.args]\r\ninstall = ['--skip-subprojects']\r\n```","Note that that feature does require Meson 1.2.0, so you should bump the minimum version in the top-level `meson.build` file as well.","Thanks @rgommers, I'll get to updating this in a few days but:\r\n\r\n> ... modulo the use of a wrap file (I assume that's for convenience during development, and it'll change to a git submodule at the end) \r\n\r\nI was thinking we ought to keep it as a submodule? It should change to point to the Scipy\/Highs instead of my own but other than that it would make more sense to keep the meson-first subproject approach. The only issue I can think of is that the `highs` repo ends up [under subprojects](https:\/\/mesonbuild.com\/Subprojects.html#why-must-all-subprojects-be-inside-a-single-directory) which may be logically less than pleasant.\r\n\r\nThe way the build is structured now, the subproject defines all the necessary variables (including the files needed), which means we would very rarely need to update the `meson.build` in SciPy. e.g. With the submodule system, if a new file is added, we need to update the `meson.build` in SciPy. Under the current scheme, we'd continue as-is because it would be part of the variables exported from the subproject (we'd still need to update the wrap-file hash though).\r\n\r\nPerhaps @eli-schwartz has more comments on submodules v\/s subprojects?","Using it as a subproject is very much preferable. You get component isolation: it's not SciPy's job to be concerned about how highs gets built, you just want to automatically use whatever the highs subproject does to define its static-library artifact(s), and then you consume them.\r\n\r\nThis also unlocks the ability to switch from:\r\n```meson\r\nhighs_proj = subproject('highs', ......\r\n```\r\nto\r\n```meson\r\nhighs_proj = dependency('highs')\r\n```\r\n\r\nalong with specifying in *SciPy*, `default_options: ['wrap_mode=forcefallback']`\r\n\r\nThis would allow downstream builders (especially linux distros) the option to build highs as an external package and then build SciPy against that. But people doing `pip install scipy` or using `python dev.py build` would by default use the subproject.","That all sounds right - I didn't mean a git submodule _instead_ of a subproject. Rather, I meant that I'm missing a git submodule under `subprojects\/`. Or am I missing something obvious and that gets added automatically somehow? I haven't worked with wrap files before, but it seems to me like the sdist should contain a copy of HiGHS and the build should work offline after checking it out.\r\n\r\n> This would allow downstream builders (especially linux distros) the option to build highs as an external package and then build SciPy against that. But people doing `pip install scipy` or using `python dev.py build` would by default use the subproject.\r\n\r\nYes, that is what I had in mind as well.","`meson dist --include-subprojects` will create sdists that have the subproject included from the wrap file.\n\nWhether to use subprojects via a wrap file or a git submodule (or both at the same time!) is largely unimportant as far as meson is concerned. The main thing that strikes me about the topic is that git submodules tend to be larger to download than a tarball, and, if you create \"light\" sdists that only download the subprojects with an online build when the person building the wheel hasn't disabled the use of subprojects, the tarball version works without git installed, while the submodule requires installing git.\n\nI think both options are defensible choices, but adding a meson subproject as a git submodule will probably be more pleasant for you if you expect most people to interact with the source code via git using long-running clones, distribute sdists with subprojects included, and don't support GitHub autogenerated tag tarballs at all...\n\n... which are all qualities that SciPy has.","Thanks, that explains it. Then I had the right thing in mind:\r\n\r\n> modulo the use of a wrap file (I assume that's for convenience during development, and it'll change to a git submodule at the end)\r\n\r\nThe wrap file is indeed nicer now, when you're developing this PR while making fixes to HiGHS as well. And once it's done, you can swap the wrap file for a git submodule under `subprojects\/`.","Failures seem unrelated (`cython_special`) but I can reproduce them (with a newly created environment, not my older one) so I assume it has something to do with a dependency update (can't any obvious contenders though).","This is ready pending the companion PR to SciPy `highs` (https:\/\/github.com\/scipy\/HiGHS\/pull\/63). Upstream `highs` already includes these changes :)","Once https:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1467 is in and https:\/\/github.com\/scipy\/HiGHS\/pull\/63 is updated, this will also close #15888.","@mtsokol is the numpy private namespace related error something being worked on elsewhere? ","> @mtsokol is the numpy private namespace related error something being worked on elsewhere?\r\n\r\nDefinitely caused by `numpy.core` rename. I will open a PR with a fix. ","@HaoZeke It looks that the error is coming from `array-api-compat` library vendored in SciPy. The fix for it was already merged four days ago here: https:\/\/github.com\/data-apis\/array-api-compat\/pull\/63. \r\n\r\nSo I think `array-api-compat`s submodule in SciPy just needs to be checked-out to the latest commit (by entering the submodule and checking-out to `main` branch pulling latest changes). ","> @HaoZeke It looks that the error is coming from `array-api-compat` library vendored in SciPy. The fix for it was already merged four days ago here: [data-apis\/array-api-compat#63](https:\/\/github.com\/data-apis\/array-api-compat\/pull\/63).\r\n> \r\n> So I think `array-api-compat`s submodule in SciPy just needs to be checked-out to the latest commit (by entering the submodule and checking-out to `main` branch pulling latest changes).\r\n\r\nAwesome, that worked for the previous error, but now there's one last [DeprecationWarning](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/6606360165\/job\/17942482362?pr=19255)..\r\n\r\n```bash\r\nDeprecationWarning: numpy.core.multiarray is deprecated and has been renamed to numpy._core.multiarray. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray._ARRAY_API.\r\n```","Ok, this one will require a small PR to be fixed. \r\nIn a nutshell - we already seen such errors for astropy as `_ARRAY_API` is imported from deprecated `numpy.core._multiarray_umath` from C level in objects built with NumPy 1.x. To solve it, deprecation warnings were removed for `_ARRAY_API` imports: https:\/\/github.com\/numpy\/numpy\/pull\/24943.\r\n\r\nHere I see that the warning comes from the file `distance_pybind.cpp` in SciPy from pybind11 initialization `#include <pybind11\/numpy.h>` (Here's a line in pybind header: https:\/\/github.com\/pybind\/pybind11\/blob\/8a099e44b3d5f85b20f05828d919d2332a8de841\/include\/pybind11\/numpy.h#L266 - it's already fixed in the main branch in pybind11 but it still needs to be released - therefore removing the warning on the NumPy side will be proper solution). \r\n\r\nI need to remove the same warning for `numpy.core.multiarray` - I will do it tomorrow!","Here are two PRs to address warnings\/errors present in dev CI stage:\r\nnumpy: https:\/\/github.com\/numpy\/numpy\/pull\/24985\r\nscipy: https:\/\/github.com\/scipy\/scipy\/pull\/19426\r\n","@rgommers, with @mtsokol's timely PRs this should pass CI and I think it would be a good time to merge this in, there are still callbacks (#19420) but they'd add significant complexity to the PR and should be reviewed separately. The [OOM bug](https:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1467) will require a fairly trivial update of the `.wrap` and submodule later.","I see a relevant failure in `Linux Meson tests \/ Prerelease deps, coverage, and 64-bit BLAS (3.9)`:\r\n```\r\n=========================== short test summary info ============================\r\nFAILED scipy\/optimize\/tests\/test_milp.py::test_milp_timeout_16545[options0-Time limit reached. (HiGHS Status 13:] - assert None is not None\r\n= 1 failed, 57430 passed, 2497 skipped, 257 xfailed, 16 xpassed in 1535.82s (0:25:35) =\r\n```\r\n\r\nThe refguide check gets killed - not 100% sure if this might be a flake, but other contemporary PRs pass that check.","> I see a relevant failure in `Linux Meson tests \/ Prerelease deps, coverage, and 64-bit BLAS (3.9)`:\r\n> \r\n> ```\r\n> =========================== short test summary info ============================\r\n> FAILED scipy\/optimize\/tests\/test_milp.py::test_milp_timeout_16545[options0-Time limit reached. (HiGHS Status 13:] - assert None is not None\r\n> = 1 failed, 57430 passed, 2497 skipped, 257 xfailed, 16 xpassed in 1535.82s (0:25:35) =\r\n> ```\r\n> \r\n> The refguide check gets killed - not 100% sure if this might be a flake, but other contemporary PRs pass that check.\r\n\r\nI believe this is a known flaky test (e.g., gh-18339).  Sometimes it finds a solution and quits before exhaustion, and sometimes it doesn't.  The \"right thing\" to do here IMO is to disable the test until MIP callbacks are implemented, remove the timeout, and check that the solution is preserved after hitting the first callback announcing a solution has been found.\r\n\r\nIf you want to keep the test, messing with the timeout may be necessary as upstream fixes\/updates will inevitably change the window of opportunity for seeing a solution and quitting before done.","OK, thanks for trying to see if git manages to detect the file rename. This was mainly me asking because merging 45 commits with a non-negligible amount of churn would normally be squash-merged.\r\n\r\nThat said, if you're up to doing an interactive rebase that condenses the history to 2-5 clean commits with self-contained increments (e.g. one could be centered around the file rename), that would actually be the best solution IMO.","@rgommers sorry for the wait, this now has a smaller set of more reasonable commits, and should be good to go.\r\n\r\nEDIT: Test failure doesn't seem to be related.. ","> Test failure doesn't seem to be related..\r\n\r\nyep, seeing that on other PRs","Thanks for the clean-up here @HaoZeke!\r\n\r\nI'll admit that I find this hard to review (there's still a fair amount of back and forth in the commits, there are unrelated formatting changes mixed in, and few indications what got \"just\" translated rather than written from scratch, etc.), but given that this seems to pass the existing tests, that's a very good sign already (assuming we have some halfway decent test coverage \ud83d\ude05).","@h-vetinari I could perhaps do another \"from scratch\" clean-room implementation in a few weeks, which would fix (at-least) the formatting, but perhaps it might be more expedient to let it out into the wild as is? I'd be happy to make other changes to get this up to speed.","> but perhaps it might be more expedient to let it out into the wild as is\r\n\r\n+1 for this. I haven't forgotten about this PR, it was just a busy period + holiday season. Planning to get this reviewed this week.","The linter is unhappy - the `No explicit stacklevel keyword argument found` at least look like they need to be fixed up.","One other key thing here that I am still reviewing\/testing more carefully is that with the use as a subproject, we get extra constraints from the upstream `meson.build`:\r\n- minimum Meson version is set to 1.2.0\r\n- optional dependency on `zlib`; looks like that is for file reading code only and we should disable it unconditionally when calling `subproject('highs')`\r\n- usage of `git` for a version string in `highspy` (looks robust to failure, but still)\r\n- a `run_command('python3', '-c', ...` that looks less robust\r\n- logic for `threads` and `libatomic` dependencies that now needs to be kept in sync in two places (our own versions, and the copied ones in highs)\r\n    - has`threads_dep = dependency('threads', required: true` while we have `required: false`. I'm not sure if it can fail on niche platforms (mingw is problematic IIRC).\r\n- An unnecessary dependency on `libm` (our top-level `meson.build` explains why it's only needed for C)\r\n- an explicit `declare_dependency(link_args: '-lstdc++')` that doesn't look right\r\n\r\nSo this isn't as close as I thought yet. I'd really like for our fork of highs not to diverge from upstream, so let's see how easy it is to patch things up there first.","> > but perhaps it might be more expedient to let it out into the wild as is\r\n> \r\n> +1 for this. I haven't forgotten about this PR, it was just a busy period + holiday season. Planning to get this reviewed this week.\r\n\r\nThanks for the comprehensive review and comments @rgommers, I'll address these both upstream and here this weekend.","@rgommers I've addressed the issues raised on a PR to HiGHS here (sorry for the delay):\r\nhttps:\/\/github.com\/ERGO-Code\/HiGHS\/pull\/1603\r\n\r\nBeyond that I've made a few cosmetic changes (rolling back `black`) and I think I've responded to all the comments (below):\r\n\r\n- [X] `minimum Meson version is set to 1.2.0`\r\n\t- Set to match SciPy's `1.1.0`\r\n- [x] `optional dependency on `zlib`; looks like that is for file reading code only and we should disable it unconditionally when calling `subproject('highs')``\r\n- [X] `usage of `git` for a version string in `highspy` (looks robust to failure, but still)`\r\n\t- Not used if called as subproject\r\n- [X] `a `run_command('python3', '-c', ...` that looks less robust`\r\n\t- Not used if called as a subproject\r\n- [ ] `logic for `threads` and `libatomic` dependencies that now needs to be kept in sync in two places (our own versions, and the copied ones in highs)`\r\n    - [ ] `has`threads_dep = dependency('threads', required: true` while we have `required: false`. I'm not sure if it can fail on niche platforms (mingw is problematic IIRC).`\r\n- I'm not sure what can be done for this one, `threads` are a dependency of `highs`, and the `libatomic` fallback is required, not just for SciPy, so I've left it in\r\n- [X] `An unnecessary dependency on `libm` (our top-level `meson.build` explains why it's only needed for C)`\r\n\t- Only used if the `C` interface is built\r\n- [X] `an explicit `declare_dependency(link_args: '-lstdc++')` that doesn't look right`\r\n\t- Removed\r\n\r\n- [X] The linter is unhappy - the `No explicit stacklevel keyword argument found` at least look like they need to be fixed up.\r\n\t- The `linter` errors  seems spurious, all the `warn` calls have `stacklevel` set explicitly, and the line flagged isn't that long\r\n\t- Can't reproduce locally with `ruff lint scipy\/optimize | grep highs`","[Failing tests](https:\/\/github.com\/scipy\/scipy\/actions\/runs\/7769105154\/job\/21187811506?pr=19255) seem unrelated:\r\n```\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_method[uint8] - OverflowError: Python integer 504 out of bounds for uint8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_method[int8] - OverflowError: Python integer 184 out of bounds for int8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_valid[uint8] - OverflowError: Python integer 504 out of bounds for uint8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_valid[int8] - OverflowError: Python integer 184 out of bounds for int8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_same[uint8] - OverflowError: Python integer 504 out of bounds for uint8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_same[int8] - OverflowError: Python integer 184 out of bounds for int8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_all[uint8] - OverflowError: Python integer 504 out of bounds for uint8\r\nFAILED scipy\/signal\/tests\/test_signaltools.py::TestCorrelateReal::test_rank3_all[int8] - OverflowError: Python integer 184 out of bounds for int8\r\n= 8 failed, 45923 passed, 2378 skipped, 147 xfailed, 16 xpassed in 342.04s (0:05:42) =\r\nError: Process completed with exit code 1.\r\n```","@HaoZeke I'd like to branch for SciPy `1.13.0` around Sunday (17th) -- will\/should this be ready by then or should we bump the milestone to `1.14.0`?","> @HaoZeke I'd like to branch for SciPy `1.13.0` around Sunday (17th) -- will\/should this be ready by then or should we bump the milestone to `1.14.0`?\r\n\r\nI believe this is good to go, pending reviews which I will solicit a bit more aggressively for the deadline (maybe the mailing list or slack).. (perhaps @rgommers or @mckib2 or maybe @lucascolley)\r\n\r\n","I'd much prefer to merge this right after branching `1.13.x`. This doesn't seem critical for NumPy 2.0 support, and it's too large a change to merge at the last minute."],"labels":["enhancement","scipy.optimize"]},{"title":"ENH: Proposal to extend the scipy.spatial.transform module to cover proper rigid transformations, with the addition of Transformation and Translation objects","body":"**Proposal Overview**\r\nThis is an issue to discuss a proposal to extend the  [`scipy.spatial.transform`](https:\/\/scipy.github.io\/devdocs\/reference\/spatial.transform.html) module. Currently, only 3-D rotations are supported under the module. What I would like to add is a `Translation` object which would apply 3-D translational offsets to points. This would be fairly basic on its own, but would become much more powerful by allowing composition with `Rotations`. Such a composition would be captured in a new `Transformation` object, where each `Transformation` represents a `Rotation` followed by a `Translation`. This would fully implement what are formally known as [proper rigid transformations](https:\/\/en.wikipedia.org\/wiki\/Rigid_transformation), or the 3-D [special Euclidean group SE(3)](https:\/\/en.wikipedia.org\/wiki\/Euclidean_group).\r\n\r\n**Applications**\r\nThe end goal I am shooting for here is allowing for the representation of arbitrary _coordinate frames_ in 3 dimensions. Working with and converting between coordinate frames is fundamental functionality that is critical to the fields of robotics, aerospace, mechanical engineering, computer graphics, and anything that has to do with observing, modeling, analyzing, or controlling the physical world in 3-D space. I think that this is a natural extension to the `scipy.spatial.transform` module that would be broadly useful to the scientific community over many domains.\r\n\r\nThis page gives a good introduction to the problem and some canonical representations: http:\/\/motion.cs.illinois.edu\/RoboticSystems\/CoordinateTransformations.html \r\n![ros_tf](https:\/\/github.com\/scipy\/scipy\/assets\/14363975\/8fe99015-c7bc-44fe-a4ca-ab7121362475)\r\n\r\n\r\n**Proposed Object Structure**\r\nProposed object structure in the `scipy.spatial.transform` module:\r\n```\r\n- _BaseTransformation: Abstract base class to define types of transformations\r\n      - Rotation: Current Rotation class, now subclassed\r\n      - Translation: New Translation subclass\r\n      - Transformation: A proper rigid transformation, defined as a rotation followed by a translation.\r\n                        Formed from the composition of one or more _BaseTransformation objects.\r\n```\r\n\r\nAll `_BaseTransformation` objects would be composable with each other using the `__mul__`operator, can compose with itself with the `__pow__` operator, has an identity operation that can be generated with an `identity()` constructor, is invertible with an `inv()` method, and can act on 3-vectors with an `apply()` method. Each object would be able to hold multiple transformations, and we would follow the same broadcasting rules as currently followed by the `Rotations` class.\r\n\r\n**Proposed API**\r\n```python\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation as R, Translation as t, Transformation as T\r\n\r\n### Translation ###\r\n## Basic operations\r\nt1 = t.from_vector([x1, y1, z1])  # Form a Translation that represents an offset of [x1, y1, z1] from the origin\r\nt1.as_vector()  # Returns np.array([x1, y1, z1])\r\nt1.inv()  # Returns a Translation that represents an offset of [-x1, -y1, -z1] from the origin\r\nt1.apply([a, b, c])  # Returns np.array([a + x1, b + y1, c + z1])\r\n\r\n## Other constructors\r\nt0 = t.identity()  # Same as t.from_vector([0, 0, 0])\r\n\r\n## Composition with __mul__ and __pow__\r\nt2 = t.from_vector([x2, y2, z2])\r\nt1 * t2  # Same as t.from_vector([x1 + x2, y1 + y2, z1 + z2])\r\nt1 ** n  # Same as t.from_vector([n * x1, n * y1, n * z1])\r\n\r\n### Transformation ###\r\n## Basic operations\r\nr1 = R.from_rotvec([u1, v1, w1])\r\nT1 = T.from_transformations([r1, t1])  # A Transformation defined by rotation, followed by translation. TODO: ordering?\r\nT1.as_basetransformations()  # Returns (r1, t1)\r\nT1 = t1 * r1  # Same as T.from_transformations([r1, t1])\r\nT1.inv()  # Returns a transformation equivalent to r1.inv() * t1.inv()\r\nT1.apply([a, b, c])  # Same as t1.apply(r1.apply([a, b, c]))\r\n\r\n## Attributes\r\nT1.rotation  # Returns r1\r\nT1.translation  # Returns t1\r\nT1 == T1.translation * T1.rotation  # Always holds true (though TODO we need to think about how to make comparisons)\r\n\r\n## Composition rules\r\n# Every composition of an arbitrary number of rotations and translations is equivalent to a single rotation followed\r\n# by a single translation. A Transformation object will store only that simplified pair, and not the chain of\r\n# component translations that generated it\r\n# When composing multiple transformations, the following relationships hold. This is enough to simplify any chain\r\n# of transformations\r\nT1.apply([a, b, c]) = (t1 * r1).apply([a, b, c]) = t1.apply(r1.apply([a, b, c]))  # Same as above\r\nr1 * t1 = t.from_vector(r1.apply(t1.as_vector())) * r1  # So in most cases, r1 * t1 != t1 * r1\r\nr2 * (t1 * r1) = (r2 * t1) * (r2 * r1)\r\nt2 * (t1 * r1) = (t2 * t1) * r1\r\nX * r2 * r1 * Y = X * (r2 * r1) * Y  # Follows from above, not a base axiom\r\nX * t2 * t1 * Y = X * (t2 * t1) * Y  # Follows from above, not a base axiom\r\n\r\n## Other constructors\r\nT.identity()  # Same as t.identity() * R.identity()\r\nT1_copy = T.from_transformations(T1)  # Can take in Translation objects, and single inputs are ok if not array_like\r\nT2 = t1 * t2 * r1 * T1 * r2  # Can chain together Translations, Rotations, and Transformations.\r\nT2.apply([x, y, z]) # Same as t1.apply(t2.apply(r1.apply(T1.apply(r2.apply([x, y, z])))))\r\n\r\n## Composition with __mul__ and __pow__\r\nT1 * T1  # Same as t1 * r1 * t1 * r1\r\nT1 ** 2  # Same as T1 * T1\r\nT1 ** -2  # Same as T1.inv() * T1.inv()\r\nT1 ** n  # Same as t1 ** (n % 1) * r1 ** (n % 1) * (t1 * r1) * ... * (t1 * r1)  # TODO: does non-integer n make sense? \r\n\r\n## Change of reference frames\r\nT1_wrt_T2 = T1 * T2.inv()  # The T1 coordinate frame expressed in reference frame T2\r\n\r\n## Other methods\r\nT1.rotate_about([x2, y2, z2], r2)  # Same as (r2 * t2) * t2.inv() * T1\r\n                                   # The first argument could be points or a Translation object\r\nT1.rotate_about([x2, y2, z2], R.from_rotvec(theta*np.array([u2, v2, w2])))  # Rotate angle theta about a line uvw\r\nT1.rotate_about(T1.translation, r2)  # A pure rotation of T1 about its local coordinate origin\r\n\r\n### New top-level methods in scipy.spatial.transform ###\r\nTranslationLerp(times, translations)  # Analagous to scipy.spatial.transform.Slerp(times, rotations), TODO: better name?\r\nTranslationSpline(times, translations)  # Analagous to scipy.spatial.transform.RotationSpline(times, rotations)\r\nTransformationLerp(times, transformations)  # Composition of TranslationLerp and Slerp, TODO: better name?\r\nTransformationSpline(times, transformations)  # Composition of TranslationSpline and RotationSpline\r\n```\r\n\r\n**Out Of Scope Enhancements**\r\nFor this proposal, the following would be excellent future additions to the  `scipy.spatial.transform` module, but are out of scope of this effort. However, the modularity of the class structure introduced here lends itself to easy extension of the latter 3 items. Scale, shear, and reflection transformations would be their own `_BaseTransformation` subclasses, and would also be able to be composed with all other `_BaseTransformations` to form a `Transformation` object.\r\n\r\n- 2-D transformations (we have something like this in `scipy.ndimage.affine_transform` but that's for pixel grids specifically)\r\n- N-D transformations for N > 3\r\n- Scale transformations\r\n- Shear transformations\r\n- Reflections\r\n\r\nRelated to https:\/\/github.com\/scipy\/scipy\/issues\/17753, which proposes general affine transformations (I think a good future direction, where this issue would be the first step).\r\n\r\n**Implementations in Other Libraries**\r\nI found the following python libraries which implement this sort of functionality. I am having trouble finding the original discussion around adding the `Rotation` class to scipy, but I would expect the arguments to incorporate it to apply to the proposed extensions here as well. I think there is quite a bit of value in having this in scipy as a reference implementation, for not relying on additional package imports for this functionality, and for putting this functionality under the umbrella of a larger developer community.\r\n- [pytransform3d](https:\/\/dfki-ric.github.io\/pytransform3d\/index.html) (highly recommended for people looking for more functionality)\r\n- [rigid-body-motion](https:\/\/rigid-body-motion.readthedocs.io\/en\/latest\/)\r\n- [transforms3d](http:\/\/matthew-brett.github.io\/transforms3d\/)\r\n\r\n**Feedback Wanted**\r\nI am interested in implementing this, [having done some work](https:\/\/github.com\/scipy\/scipy\/pulls?q=is%3Apr+author%3Ascottshambaugh+rotation) on the `Rotation` class, but have not started writing code yet. I'm hoping to solicit feedback on the proposal before that - most importantly whether this should be done at all at, and then scope, naming, API changes would all be very welcome feedback. I'm also interested in thoughts on how best to break up this work - one large PR, or multiple smaller ones? A summary of this proposal [has been sent to the mailing list](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/IXY2PJJRN2GMXVIPHE5XEHSV3MPEMTG5\/#IXY2PJJRN2GMXVIPHE5XEHSV3MPEMTG5). (And pinging @nmayorov specifically)","comments":["@scottshambaugh \r\n\r\nThank you for putting up this proposal. In general it looks good and interesting. \r\n\r\nMy initial thoughts as I read through the text:\r\n\r\n1. May `RigidTransform` be a better name than `Transformation` to allow for future inclusion of other more general transformations?\r\n2. The proposed list of methods of `_BaseTransformation` looks reasonable to me.\r\n3. The API of `Translation` is minimal, but I don't know what else should it provide, looks simple and reasonable.\r\n4. The `Transformation` is more interesting:\r\n    1. Is constructor `from_transformations` actually necessary? Can it be said that `rt = t * r` is a more readable way to create a rigid transform?\r\n    2. And having attributes `rotation` and `translation`, do we need `as_basetransformations`?\r\n    3. I'm quite sure that creating from and representing as 4x4 homogeneous transform matrices should be implemented. The name might be `as_matrix\/from_matrix`.\r\n    4. I don't think the exact equality is appropriate, possibly `is_approx_equal` can be implemented.\r\n5. Talking about composition algorithm for `Transformation`, do you see any difficulties or subtle points in the implementation? One way I see is to use homogeneous matrix representation.\r\n6. What about handling objects of different types in composition, like `Translation` followed by `Transformation`, followed by `Rotation`. How do we implement it technically?\r\n7. Can you explain `rotate_about` method?\r\n8. Perhaps it's not a good idea to create specific classes for `Translation` alone.  There will be a lot of duplication and extra work to do (considering docstring, examples, etc.) My tentative suggestion is to have something like `RigidTransformSpline` and `RigidTransformLinearInterpolator` (or `RigidMotion...`). These classes my handle pure `Rotation` and `Translation` specifically (if only `Translation` given -- the interpolation result is also `Translation`). Not sure if it's a good design from some OOP perspective, but it looks OK from the first glance.\r\n10. Are there any subtleties in interpolation of translation and rotation simultaneously? Can we just do it completely independently? It looks like we can, which means that the motion of the frame origin and the axes attitude are interpolated independently.\r\n11. Anyway I think interpolation classes should be designed after `Translation` and `Transformation` is implemented.\r\n","This is an uninformed question from the peanut gallery, I only read the mailing list announcement :)\r\n\r\nI'm always surprised when I see `.apply()` in Python. Why not use `__call__()` instead and do `t(vec)` rather than `t.apply(vec)`? Would this make sense with the rest of the API?","@nmayorov thank you for taking a look and I'm pleased to hear that you think this is generally a good idea to include.\r\n\r\n> 1) May `RigidTransform` be a better name than `Transformation` to allow for future inclusion of other more general transformations?\r\n\r\nI am ok with this but we have to be careful about naming. I think that mathematically, we are really defining a _Proper Rigid Transformation_ here, where a _Rigid Transform_ would also include reflections. While this proposal is only to get to SE(3), I think we should not preclude future extensions. Which future transformations and their combinations would we want to protect for? Below is my (non-expert, could have errors) understanding of how the different combinations shake out into groups, which as I understand it would be their own classes if we ever went down this route. My original thought was that we would be able to lump additional features in the future into the `Transformation` class until it covered the entire Affine group, and break out classes in the future with reduced scope as desired. But I'm not opposed to limiting scope of classes with restrictive naming up front.\r\n\r\n- Rotation: Special Orthogonal Group SO(3) (quaternions double cover this with Special Unitary Group SU(2))     \r\n- Rotation + Translation: Proper Rigid Transformations \/ Special Euclidean Group SE(3)     \r\n- Rotation + Translation + Reflection: Rigid Transformations \/ Euclidean Group E(3)     \r\n- Rotation + Reflection: Orthogonal Group O(3)     \r\n- Rotation + Reflection + Scale: Conformal Orthogonal Group CO(3)     \r\n- Rotation + Scale: Conformal Special Orthogonal Group CSO(3)     \r\n- Rotation + Shear + Scale: Special Linear Group GL(3)     \r\n- Rotation + Reflection + Shear + Scale: General Linear Group GL(3)     \r\n- Translation + Reflection: Affine Reflection Group Aff(O(3))\r\n- Translation + Reflection + Scale: Conformal Affine Reflection Group Aff(CO(3))\r\n- Reflection + Scale: Conformal Reflection Group?\r\n- Rotation + Reflection + Shear + Scale + Translation: Transformations \/ Affine Group Aff(3)     \r\n- Others?\r\n\r\n> 4i) Is constructor `from_transformations` actually necessary? Can it be said that rt = t * r is a more readable way to create a rigid transform?\r\n\r\nMight not be necessary, yeah.\r\n\r\n> 4ii) And having attributes `rotation` and `translation`, do we need `as_basetransformations`?\r\n> 4iii) I'm quite sure that creating from and representing as 4x4 homogeneous transform matrices should be implemented. The name might be `as_matrix`\/`from_matrix`.\r\n\r\nI agree with having the matrix methods. There are a lot of different ways that you can represent rigid motion in 3D space. I think that [the ones implemented by pytransforms3d]( https:\/\/dfki-ric.github.io\/pytransform3d\/transformations.html) are a pretty much complete list. I don\u2019t think we need all of those to start with. But due to the wide number of representations I now think it probably makes sense to not have `T.translation` and `T.rotation` as public attributes. Instead, we should grab that representation with `as_translationrotation()` or similar. \r\n\r\n> 4iv) I don't think the exact equality is appropriate, possibly `is_approx_equal` can be implemented.    \r\n\r\nI agree, though we will need a tolerance for both translational and rotational error. Perhaps implement `approx_equal` for `Translation` and have the one for `Transformation` call that one and the one for `Rotations`.\r\n\r\n> 5) Talking about composition algorithm for `Transformation`, do you see any difficulties or subtle points in the implementation? One way I see is to use homogeneous matrix representation.\r\n\r\nWe could represent it under the hood any number of ways. My hunch is that we will retain the most precision when multiplying by keeping it as a position and quaternion rather than as a 4x4 matrix, but that could use some experimentation.\r\n\r\n> 6) What about handling objects of different types in composition, like `Translation` followed by `Transformation`, followed by `Rotation`. How do we implement it technically?\r\n\r\nI think that in the `__mul__` method we cast all `Rotation` and `Translation` objects to a `Transformation` object (using the identity for the undefined portion), and then just define multiplication of `Transformation` objects.\r\n\r\n> 7) Can you explain `rotate_about` method?\r\n\r\nTo use a physical analogy, the point xyz and vector uvw define a line in 3D space. The coordinate frame defined by a transformation is linked by a rigid bar to that line, and the combined system is rotated about the line by an angle theta. I see a lot of use for this.\r\n\r\n> 8) Perhaps it's not a good idea to create specific classes for `Translation` alone. There will be a lot of duplication and extra work to do (considering docstring, examples, etc.) My tentative suggestion is to have something like `RigidTransformSpline` and `RigidTransformLinearInterpolator` (or `RigidMotion`...). These classes my handle pure `Rotation` and `Translation` specifically (if only `Translation` given -- the interpolation result is also `Translation`). Not sure if it's a good design from some OOP perspective, but it looks OK from the first glance.\r\n\r\nWill have to think about this.\r\n\r\n> 9) Are there any subtleties in interpolation of translation and rotation simultaneously? Can we just do it completely independently? It looks like we can, which means that the motion of the frame origin and the axes attitude are interpolated independently.\r\n> 10) Anyway I think interpolation classes should be designed after Translation and Transformation is implemented.\r\n\r\nThere are a couple ways to do it, I think doing it independently makes most sense as a starting point. There is also a method called ScLERP that couples them, see [the figures here]( https:\/\/www.researchgate.net\/figure\/Comparison-of-ScLERp-based-path-generation-for-door-handle-twisting-task-with-a_fig2_349282592). I agree it makes sense to defer the interpolation methods until after the base implementation is done.\r\n\r\n\r\n@adeak I do like your syntax better. The `apply()` method is to match the syntax in the current implementation of `Rotation`, so I think if we want to add the `__call__` syntax then that should be broken out into a separate conversation.\r\n\r\n\r\nI also want to tag @AlexanderFabisch as the author of pytransform3d, he might have thoughts on the proposal \/ implementation details, and whether this makes sense to include as part of scipy in general.","> I also want to tag @AlexanderFabisch as the author of pytransform3d, he might have thoughts on the proposal \/ implementation details, and whether this makes sense to include as part of scipy in general.\r\n\r\nSure, feel free to use code from pytransform3d. It has been tested for at least some time and might be a good starting point. In particular, if you want to provide a Rotation-class-like interface for rigid transformations, you might find some of the batch conversion functions useful.\r\n\r\nInitially I thought it makes more sense to separate it from scipy as it is a bit out of scope, but now I'm not opposed to moving some of the more basic functionality and have more maintainers for this code.\r\n\r\nHowever, to be honest, this feels very unnatural for me:\r\n\r\n```python\r\n## Composition with __mul__ and __pow__\r\nt2 = t.from_vector([x2, y2, z2])\r\nt1 * t2  # Same as t.from_vector([x1 + x2, y1 + y2, z1 + z2])\r\nt1 ** n  # Same as t.from_vector([n * x1, n * y1, n * z1])\r\n```\r\n\r\nThe right operator to compose translations is `+` and if you want to apply it multiple times you should use `*`. We can avoid this discussion by not defining a translation class and handling translation as a special case of a rigid transformation. We could add a method `T.from_translation(...)`.","> The right operator to compose translations is + and if you want to apply it multiple times you should use *. We can avoid this discussion by not defining a translation class and handling translation as a special case of a rigid transformation. We could add a method T.from_translation(...)\r\n\r\nI think this makes sense, and would simplify the implementation if we drop the Translation class. We could overload the addition, subtraction, and multiplication operators of Rotations and Transformations to accept numpy array inputs (and probably lists that are then cast to numpy arrays), do a shape check, and then perform the composition. This'll let us leverage normal addition and multiplication operations on numpy arrays for translations. And these compositions are all associative so we shouldn't run into any issues with order of operations.\r\n\r\n```python\r\n## Before:\r\nT2 = r2 * t3 * t2.inv() * r1 * T1 * t1\r\n\r\n## After:\r\nT2 = r2 * array([x3, y3, z3]) - array([x2, y2, z2]) + r1 * T1 * [x1, y1, z1]\r\n\r\n# What about the multiplication operator in this case?\r\nr * 2 * array([x, y, z]) * 2 * r\r\n# easiest is to raise an error and force the user to group as below, though not the most ergonomic\r\nr * (2 * array([x, y, z]) * 2) * r\r\n\r\n# We could also do the below, but the + operator would not be commutative here which I feel is unintuitive\r\nT2 = r2 + array([x3, y3, z3]) - array([x2, y2, z2]) + r1 * T1 + [x1, y1, z1]\r\nr + 2 * array([x, y, z]) * 2 + r\r\n```","> My original thought was that we would be able to lump additional features in the future into the Transformation class until it covered the entire Affine group, and break out classes in the future with reduced scope as desired. But I'm not opposed to limiting scope of classes with restrictive naming up front.\r\n\r\nSo basically your idea is that `Transformation` may compose any number of \"elementary\" transformations? \r\nIt looks like a reasonable general approach and honestly I wasn't thinking in this direction.\r\nI guess it will change drastically how we want to approach everything.\r\n\r\n> I agree with having the matrix methods. There are a lot of different ways that you can represent rigid motion in 3D space. I think that the ones implemented by pytransforms3d are a pretty much complete list. I don\u2019t think we need all of those to start with. But due to the wide number of representations I now think it probably makes sense to not have T.translation and T.rotation as public attributes. Instead, we should grab that representation with as_translationrotation() or similar.\r\n\r\nIf we take an arbitrary transformation it might be not possible to represent it as only rotation and translation. \r\nPerhaps on the contrary it's a good idea to provide components as attributes, and then as new transformations are added they will become accessible through new attributes.\r\n\r\nAnd pytransform3d and its tutorial pages look really awesome I must say.\r\n\r\n> I agree, though we will need a tolerance for both translational and rotational error. Perhaps implement approx_equal for Translation and have the one for Transformation call that one and the one for Rotations.\r\n\r\nAgain it must be considered as a more general concept then, like checking equality for all elementary parts.\r\n\r\n> I think that in the __mul__ method we cast all Rotation and Translation objects to a Transformation object (using the identity for the undefined portion), and then just define multiplication of Transformation objects.\r\n\r\nI guess if two counterparts have the same type, then a particular multiplication (composition) is done and the same type is returned, without reverting to `Transformation`.\r\n\r\n>To use a physical analogy, the point xyz and vector uvw define a line in 3D space. The coordinate frame defined by a transformation is linked by a rigid bar to that line, and the combined system is rotated about the line by an angle theta. I see a lot of use for this.\r\n\r\nI think I get this geometric meaning.\r\n\r\n> There are a couple ways to do it, I think doing it independently makes most sense as a starting point. There is also a method called ScLERP that couples them, see the figures here. I agree it makes sense to defer the interpolation methods until after the base implementation is done.\r\n\r\nFor a general `Transformation` I'd say it's not clear how to approach it. \r\nI think it might be solved case-by-case, but without any general classes or functions on `scipy` side.\r\nLike if you want SE3 interpolation -- combine `CubicSpline` with `RotationSpline`, etc.\r\n\r\n> @adeak I do like your syntax better. The apply() method is to match the syntax in the current implementation of Rotation, so I think if we want to add the __call__ syntax then that should be broken out into a separate conversation.\r\n\r\nI don't think it's terribly important whether to use `apply` or `__call__`.\r\nAnother option could have been `@` for composition and `*` to apply a transformation to vectors.\r\nBut at this point we better just to stick to what was adopted originally.\r\n\r\n> I think this makes sense, and would simplify the implementation if we drop the Translation class. We could overload the addition, subtraction, and multiplication operators of Rotations and Transformations to accept numpy array inputs (and probably lists that are then cast to numpy arrays), do a shape check, and then perform the composition. This'll let us leverage normal addition and multiplication operations on numpy arrays for translations. And these compositions are all associative so we shouldn't run into any issues with order of operations.\r\n\r\nHonestly, I'm not very fond of the idea to introduce additional operators to compose specific transformations.\r\nA single operation is consistent from programming and group theory viewpoints (which looks like a correct approach to model this field as as whole).\r\nSurely we can write `t = Translation.from_vector(a + b)`, but when we have two transformation objects `t1` and `t2` we definitely can only compose it with `t1 * t2`, `t1**2 * t2.inv()`, etc. At least this should be a starting point.\r\n\r\n----- \r\n\r\nIt appears as if there are two ways:\r\n\r\n1. Create an elementary `Translation` and treat `Transformation` as a general composition (to be extended with new elementary transformations).\r\n2. Focus on `Translation` + `Rotation` as something like `Se3Transform`.\r\n\r\nI think the first approach is more logical and has a potential to create a general system, which you are aiming for eventually.\r\n","Thanks for the feedback! I think this is enough to go prototype a draft implementation. To be clear, I don't have any plans personally to extend this further than just translations and rotations, but I see it as a natural extension of the module that we might want to pursue someday. So that might change the tradeoff of how much generality to build in - if we don't see the need to protect for that then I think the `+` operator for translations makes most sense.","Sure, let's figure out details during the discussion in the PR.","Should I dare ask about performing translations\/transformations in periodic containers? Is that explicitly out of scope? I know that downstream consumers like MDAnalysis often do a fair bit of work on top of the base ecosystem because of PBC handling."],"labels":["enhancement","scipy.spatial"]},{"title":"MAINT: special: array API dispatch follow-up","body":"gh-19023 added support for alternative backends to some special functions. We discussed in the PR that a few items would need to be resolved in follow-up work:\r\n\r\n- The wrapped functions are no longer ufuncs. It is not documented that these functions *were* ufuncs, but nonetheless, users may be relying on this behavior. What should we do about that?\r\n- There is no documentation of the Array API compatibility. Something needs to be added.\r\n\r\nMarking with 1.12 milestone because we should consider whether it is ok for gh-19023 should be included in a release before these items have been addressed. (It might be fine, but it should be an explicit decision.)\r\n\r\nOnce these issues are addressed, we could consider enabling Array API support without the environment variable. (We still might want to require it, but that should be an explicit decision, too.)\r\n\r\nOf course, we also want to open an Array API extension proposal and add support for more functions, but those ideas don't need to be tracked here.","comments":["@rgommers how do you want the ufunc issue to be addressed? Start with an email to the mailing list?","fyi, I think he may be on vacation for a little bit","I think an email to the mailing list to ask if anyone is relying on some functions being ufuncs is a good start. It's hard to make a plan without knowing that. ","> There is no documentation of the Array API compatibility. Something needs to be added.\r\n\r\nwhereabouts would this live? Are you thinking of https:\/\/scipy.github.io\/devdocs\/tutorial\/special.html? If so, the same is needed for `fft` and `cluster` at some point. Or just an update to https:\/\/scipy.github.io\/devdocs\/dev\/api-dev\/array_api.html? In any case I think what we have so far is sufficient for 1.12 considering everything is marked as experimental.\r\n\r\n---\r\n\r\n> The wrapped functions are no longer ufuncs. It is not documented that these functions were ufuncs, but nonetheless, users may be relying on this behavior. What should we do about that?\r\n\r\n[Mailing list thread for reference](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/A7QTA56RIWEGVWLPDDDCE72SVF3GLA5U\/#VFBT6D5RACMKGJQGMTILD4KRUUJFB67H)\r\n\r\nThe conclusion from the mailing list seems to be (from Ralf):\r\n\r\n> We reserve the right to move functions to being ufuncs, and we should also reserve the right to move them away from it\r\n\r\n> I believe functions in special being ufuncs yes or no is mostly leaking of implementation details - using the ufunc machinery was nice and performant, but quite a few of the methods don't really make sense as Albert points out\r\n\r\n> I'd be inclined to keep only the few methods that are actually useful, and not the whole API surface of ufuncs. The not so useful methods and attributes can be deprecated first in a wrapper. Finally, a wrapper would also help fix the issues with using `inspect.signature`, `inspect.isroutine` & co, and attributes like `__module__`\r\n\r\nThere wasn't a clear consensus though so maybe a bit more discussion is needed. If we do go ahead with this, https:\/\/scipy.github.io\/devdocs\/reference\/special.html#special-functions-scipy-special needs to be updated to clarify that it is no longer true that they are \"technically\" ufuncs (with the environment variable set).\r\n\r\n@mdhaber @rgommers should I open a PR to remove the ufuncs line from the docs for 1.12? Then we can open an issue for the deprecation process (unless more discussion is needed first).","> whereabouts would this live?\r\n\r\nThe [tutorial](https:\/\/scipy.github.io\/devdocs\/tutorial\/special.html), [array API documentation](https:\/\/scipy.github.io\/devdocs\/dev\/api-dev\/array_api.html), and the [`special` API docs](https:\/\/scipy.github.io\/devdocs\/reference\/special.html) are all good places to mention it. Most importantly (I think), if we deprecate the unnecessary ufunc-ness and make these regular functions, then we can add to the function-specific documentation. The [decorator approach](https:\/\/github.com\/scipy\/scipy\/pull\/19023#issuecomment-1712152176) was working fine except for the ufunc hiccup.\r\n\r\n> should I open a PR to remove the ufuncs line from the docs for 1.12? Then we can open an issue for the deprecation process (unless more discussion is needed first).\r\n\r\nI don't think it's essential for 1.12. After all, the release notes are explicit about array API support being experimental and encouraging testing. If users fine out the hard way that ufuncness is broken, they're more likely to report it. As for deprecation discussion, putting together a way of collecting feedback about which ufunc features should be retained would move that along.","> There is no documentation of the Array API compatibility. Something needs to be added.\r\n\r\nThere is a decent amount of documentation at http:\/\/scipy.github.io\/devdocs\/dev\/api-dev\/array_api.html. I'd say that that is the right place for now. I would not add it to the tutorials for individual submodules. Once we think it's time, we need one central place in the user-facing docs that explains everything. There should not be differences in behavior between different submodules, so starting to explain in per-submodule docs seems like it will be messy and lead to duplication.\r\n\r\n> I don't think it's essential for 1.12. After all, the release notes are explicit about array API support being experimental and encouraging testing. If users fine out the hard way that ufuncness is broken, they're more likely to report it. As for deprecation discussion, putting together a way of collecting feedback about which ufunc features should be retained would move that along.\r\n\r\nAgreed with this.\r\n","Just removing the milestone here - there is relevant discussion about the ufunc changes, but nothing that needs to worry us before we start considering moving out of an experimental env variable"],"labels":["scipy.special","maintenance","array types"]},{"title":"ENH: support a fixed bandwidth in multivariate gaussian_kde","body":"### Is your feature request related to a problem? Please describe.\n\nAt the moment, multivariate `gaussian_kde` selects a bandwidth automatically for each dimension separately. While this makes sense for a lot of use cases, it cannot be applied to coordinates representing points on a plane. This is different than what sklearn is doing in neighbors.KernelDensity, where a single bandwidth is used across all dimensions.\n\n### Describe the solution you'd like.\n\nI would like to have an option to specify not only the factor of the bandwidth but fix the bandwidth across all dimensions to a set scalar (i.e. meters in case of geographic points with x, y coordinates used as variables). As far as I can tell, there is no way achieving that right now.\n\n### Describe alternatives you've considered.\n\nI can use sklearn directly, which is often fine. However, the issue arised in seaborn (https:\/\/github.com\/mwaskom\/seaborn\/issues\/3472) which wraps `gaussian_kde` in its `kdeplot` and where adding sklearn as a dependency is a no-go.\n\n### Additional context (e.g. screenshots, GIFs)\n\nSee the original issue in seaborn for visual representation of an issue and a comparison with a sklearn implementation -> https:\/\/github.com\/mwaskom\/seaborn\/issues\/3472","comments":["Does specifying a constant for `bw_method` not do what you're looking for?","No, that seems to only scale the bandwidth per axis based on its extent, it does not fix the bandwidth.","Ok. This is a fair request, and it this looks pretty easy to do. Internally, `gaussian_kde`'s `_compute_covariance` method currently calculates the raw (not scaled by bandwidth factor) covariance matrix from the data. It sounds like you just want it to use the identity matrix. I'd just add `covariance` as a keyword parameter, if that sounds good. Next step would be to see what the mailing list thinks.","> It sounds like you just want it to use the identity matrix. I'd just add `covariance` as a keyword parameter, if that sounds good.\r\n\r\nCan't tell, that is too deep in the internals for me. `covariance` would take which options?","The `covariance` parameter would accept a covariance matrix (a symmetric positive definite array, which defines the shape of the normal distribution) as the argument. The identity matrix (`np.eye(d)`, where `d=2` in your 2d case) would work for your use case. `seaborn` would then update their UI to expose something related to the covariance as an option, but I'm not sure exactly what they'd want to do.","Ah, cool! Yes, that would work. "],"labels":["scipy.stats","enhancement"]},{"title":"MAINT, TST: NEP 50 cleanup summary","body":"A few test failures left to consider for NEP 50 casting stuff via i.e., `NPY_PROMOTION_STATE=weak python dev.py test -j 32` (with NumPy `1.25.2`) are pasted below the fold, excluding those dealt with in gh-19238.\r\n\r\n<details>\r\n\r\n```\r\n===================================================================================== short test summary info =====================================================================================\r\nFAILED scipy\/io\/tests\/test_idl.py::TestIdict::test_idict - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_byte - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_int16 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_int32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_float32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_float64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_complex32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_bytes - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_complex64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_uint16 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_uint32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_int64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestScalars::test_uint64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_byte - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_int16 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_int32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_float32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_float64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_complex32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_bytes - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_complex64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_uint16 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_uint32 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_int64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_uint64 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestCompressed::test_compressed - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_1d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_2d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_4d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_5d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_6d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_7d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestArrayDimensions::test_8d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_scalars - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_scalars_replicated - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_scalars_replicated_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_arrays - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_arrays_replicated - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_arrays_replicated_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_inheritance - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestStructures::test_arrays_corrupt_idl80 - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointers::test_pointers - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_1d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_2d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_4d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_5d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_6d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_7d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerArray::test_8d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_scalars - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_pointers_replicated - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_pointers_replicated_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_arrays - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_arrays_replicated - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestPointerStructures::test_arrays_replicated_3d - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::TestTags::test_description - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::test_null_pointer - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_idl.py::test_invalid_pointer - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/io\/tests\/test_paths.py::TestPaths::test_readsav - OverflowError: Python integer 4294967296 out of bounds for uint32\r\nFAILED scipy\/special\/tests\/test_basic.py::TestCombinatorics::test_comb - OverflowError: Python int too large to convert to C long\r\n======================================================= 62 failed, 44021 passed, 2450 skipped, 149 xfailed, 12 xpassed in 114.41s (0:01:54) =======================================================\r\n```\r\n\r\n<\/details>","comments":["This small patch seems to clean up about another 60 failures:\r\n\r\n```diff\r\ndiff --git a\/scipy\/io\/_idl.py b\/scipy\/io\/_idl.py\r\nindex 68714248f..a14cfb081 100644\r\n--- a\/scipy\/io\/_idl.py\r\n+++ b\/scipy\/io\/_idl.py\r\n@@ -318,7 +318,7 @@ def _read_record(f):\r\n     record = {'rectype': _read_long(f)}\r\n \r\n     nextrec = _read_uint32(f)\r\n-    nextrec += _read_uint32(f) * 2**32\r\n+    nextrec += _read_uint32(f).astype(np.int64) * 2**32\r\n \r\n     _skip_bytes(f, 4)\r\n \r\n@@ -789,7 +789,7 @@ def readsav(file_name, idict=None, python_dict=False,\r\n \r\n             # Read position of next record and return as int\r\n             nextrec = _read_uint32(f)\r\n-            nextrec += _read_uint32(f) * 2**32\r\n+            nextrec += _read_uint32(f).astype(np.int64) * 2**32\r\n \r\n             # Read the unknown 4 bytes\r\n             unknown = f.read(4)\r\n```","To be fair, the original code also looked pretty weird.","That seems like a valid fix to me, looks like an improvement with or without NEP 50. Want to submit that as a PR?","There's only 1 failure left to consider here, assuming the cross-linked PR gets merged in some form eventually:\r\n\r\n```\r\n____________________________________________________________________________________________________________________ TestCombinatorics.test_comb _____________________________________________________________________________________________________________________\r\n[gw4] linux -- Python 3.11.2 \/home\/treddy\/python_venvs\/py_311_scipy_dev\/bin\/python\r\nscipy\/special\/tests\/test_basic.py:1419: in test_comb\r\n    assert_equal(special.comb(ii, ii-1, exact=True), ii)\r\nE   OverflowError: Python int too large to convert to C long\r\n        ii         = 9223372036854775808\r\n        self       = <scipy.special.tests.test_basic.TestCombinatorics object at 0x7f85fbc1f850>\r\n====================================================================================================================== short test summary info =======================================================================================================================\r\nFAILED scipy\/special\/tests\/test_basic.py::TestCombinatorics::test_comb - OverflowError: Python int too large to convert to C long\r\n========================================================================================= 1 failed, 44086 passed, 2450 skipped, 149 xfailed, 12 xpassed in 109.92s (0:01:49) =========================================================================================\r\n```","For that last one, I spent some time this afternoon and decided to ask for advice upstream, because I don't particularly like the idea that a Python integer can error out in an `isnan()` check given that no integer can be an IEEE NaN: https:\/\/github.com\/numpy\/numpy\/issues\/24712\r\n\r\nIt may be that performance is chosen over purity or something like that? Anyway, if that's the case it seems we can cast it to a NumPy uint64 if we want to avoid the error.","Sounds like Sebastian may fix that upstream for `2.0.0`."],"labels":["maintenance"]},{"title":"ENH: Advice\/examples\/tutorial in docs on perf\/problem sizes\/time of `scipy.sparse.eigs` \/ `scipy.sparse.eigsh` (for sparse, symmetric, positive semidefinite matrices)","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nI'm trying to compute eigenvectors of the matting laplacian as in https:\/\/www.vision.huji.ac.il\/SpectralMatting\/\r\n\r\nIn my example, the laplacian is a sparse symmetric, positive semidefinite matrix of shape = (128400, 128400), with 2119789 nonzero float32 entries (approx every row has 16 non-zero entries)\r\n\r\nCurrently I could not wait for completion of `scipy.sparse.linalg.eigsh` on my laptop - very slow. I understand that ARPACK is used under the hood, but having some examples of problems of various sizes\/structures would be nice (maybe with run-times on Google Colab). (`scipy.sparse.linalg.spsolve` works with this design matrix quite well btw and is very fast; and also people used matlab for these kinds of problems in practice, but I haven't checked if `eigs` works fine there for this particular laplacian)\r\n\r\nMaybe related: https:\/\/stackoverflow.com\/questions\/59416098\/finding-smallest-eigenvectors-of-large-sparse-matrix-over-100x-slower-in-scipy\r\nShould I change COO to CSR or CSC for maximum speed? Should I simply increase the `tol`? Thank you :)\r\n\r\nI think dealing with top eigenvectors of sparse laplacians is very common, so some tutorial page and a collection of example matrices (found some ideas at https:\/\/gist.github.com\/denis-bz\/502a22ee092f0758ee11c9b731828602) would be very useful.\r\n\r\n[L_values.zip](https:\/\/github.com\/scipy\/scipy\/files\/12597465\/L_values.zip)\r\n[L_indices.zip](https:\/\/github.com\/scipy\/scipy\/files\/12597504\/L_indices.zip)\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy\r\n\r\nindices = np.load('L_indices.zip')['indices']\r\nvalues = np.load('L_values.zip')['values']\r\nshape = (128400, 128400)\r\n\r\nL = scipy.sparse.coo_array((values, indices), shape = shape)\r\n#L = scipy.sparse.diags(np.ones(128400, dtype = 'float32'))\r\n\r\nprint('L:', L.shape, L.dtype, 'values:', values.shape, values.dtype, 'indices:', indices.shape, indices.dtype)\r\n# L: (128400, 128400) float32 values: (2119789,) float32 indices: (2, 2119789) int32\r\n\r\nprint(scipy.sparse.linalg.eigsh(L, k = 2)[1].shape)\r\n```\r\n\r\n### Describe the solution you'd like.\r\n\r\n_No response_\r\n\r\n### Describe alternatives you've considered.\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["One more example. Here in octave `eigs` completes in 30 seconds, and I couldn't wait for completion with scipy. Am I doing anything wrong?\r\n\r\n[L_octave.mat.zip](https:\/\/github.com\/scipy\/scipy\/files\/12609553\/L_octave.mat.zip)\r\n\r\n```matlab\r\n% bug.m\r\n% run as octave -q bug.m\r\n\r\nload L_octave.mat;\r\neigs_num=50;\r\n\r\nL = sparse(i, j, v, m, n);\r\n\r\ntic;\r\n[u00,d00]=eigs(L,eigs_num+1,'sm');\r\ntoc\r\n% Elapsed time is 31.7543 seconds\r\n% on a laptop\r\n```\r\n```python\r\nimport time\r\nimport scipy\r\n\r\neigs_num=50;\r\n\r\ni, j, v, m, n = map(scipy.io.loadmat('L_octave.mat').get, ['i', 'j', 'v', 'm', 'n'])\r\n\r\ni = i.squeeze(1).astype('int32') - 1\r\nj = j.squeeze(1).astype('int32') - 1\r\nv = v.squeeze(1)\r\nm = int(m)\r\nn = int(n)\r\n\r\nprint(i.shape, j.shape, v.shape, int(m), int(n))\r\n# (3188406,) (3188406,) (3188406,) 128400 128400\r\n\r\nL = scipy.sparse.coo_matrix((v, (i, j)), shape = (m, n), dtype = 'float64')\r\n\r\ntic = time.time()\r\n\r\nu00, d00 = scipy.sparse.linalg.eigs(L, eigs_num + 1, which='SM');\r\n\r\nprint(time.time() - tic)\r\n# never completes\r\n```\r\n\r\n","Here are the lowest-magnitude eigvals of this laplacian matrix. As expected for a laplacian matrix, the lowest-magnitude eigval is ~0.0. But then the eigenvalues are also very small and close to repeated. So I'm thinking this matrix is ill-conditioned. But I dumped this matrix from the original MatLab code of https:\/\/www.vision.huji.ac.il\/SpectralMatting\/, so these kinds of laplacian matrices are quite common.\r\n\r\nMaybe octave applies some special case preproc\/postproc for these kinds of matrices?\r\n\r\nLooking at the output eigvecs of octave, it seems that it worked correctly. If it uses arpack as well, then probably the same special-casing can be done for scipy.sparse.linalg.eigs.\r\n\r\n```\r\n  -1.6303e-16\r\n   7.9107e-06\r\n   8.9512e-06\r\n   1.0534e-05\r\n   1.5940e-05\r\n   2.2632e-05\r\n   2.6382e-05\r\n   3.6819e-05\r\n   3.9009e-05\r\n   4.1792e-05\r\n   4.5809e-05\r\n   4.6043e-05\r\n   5.8754e-05\r\n   5.7562e-05\r\n   5.7791e-05\r\n   6.1737e-05\r\n   6.6136e-05\r\n   7.0927e-05\r\n   7.2839e-05\r\n   8.8532e-05\r\n   8.9893e-05\r\n   9.6097e-05\r\n   9.7594e-05\r\n   1.0023e-04\r\n   1.0827e-04\r\n   1.2584e-04\r\n   1.2854e-04\r\n   1.2944e-04\r\n   1.3525e-04\r\n   1.4659e-04\r\n   1.5044e-04\r\n   1.6361e-04\r\n   1.7249e-04\r\n   1.8328e-04\r\n   1.9357e-04\r\n   2.0120e-04\r\n   2.0597e-04\r\n   2.2589e-04\r\n   2.3646e-04\r\n   2.6108e-04\r\n   2.7415e-04\r\n   2.7831e-04\r\n   3.2248e-04\r\n   3.4078e-04\r\n   3.4820e-04\r\n   3.8075e-04\r\n   3.8184e-04\r\n   4.1542e-04\r\n   4.2452e-04\r\n   4.5051e-04\r\n   4.7676e-04\r\n```","Here are octave codes for `eigs`:\r\n(Links removed)","Octave is GPL licensed. GPL licenses are not compatible with our license, so we cannot refer to that code. To avoid license compatibility issues, I have removed the links.","Okay, I waited for completion. SciPy+ARPACK took ~250 seconds (while using eigsh), and octave+ARPACK took 11 seconds (using eigs). Quite a serious difference, given that both packages supposedly use ARPACK (according to the docs)"],"labels":["enhancement","scipy.sparse.linalg"]},{"title":"BUG: Left and right bases for signal.find_peaks must\/should be strictly monotonically increasing.","body":"### Describe your issue.\r\n\r\nProbably related to issue #16006 but with different symptoms.\r\nI am not sure it is a bug or a feature.\r\n\r\nWhen performing peak identifications with [`signal.find_peaks`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.find_peaks.html) peaks are well found but their base boundaries are somehow mislabeled\/misindexed preventing to perform meaningful integration after detection.\r\n\r\nIn the MCVE below, peaks are found easily but some `left_bases` and\/or `right_bases` are repeated while we may expect not (and being strictly monotonically increasing).\r\n\r\nThe returned solution is not totally false and probably may be explained by the method used to find peaks:\r\n```\r\nL = [ 503,  503,  503, 1980, 2773, 3215, 3615, 4004]\r\nR = [1046, 1476, 3215, 3215, 3215, 4926, 4926, 4926]\r\n```\r\nAnyway, it makes some peak being enclosed with neighbor peaks.\r\n\r\n![Enclosed peaks](https:\/\/i.imgur.com\/4T9r4Tj.png)\r\n\r\nThis solution is more exact, clean and useful:\r\n```\r\nL = [ 503, 1046, 1476, 1980, 2773, 3215, 3615, 4004]\r\nR = [1046, 1476, 1980, 2773, 3215, 3615, 4004, 4926]\r\n```\r\nAs it directly allows to integrate each peak separately and seems to be more compliant with the current [documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.signal.peak_prominences.html#scipy.signal.peak_prominences):\r\n\r\n> The peaks\u2019 bases as indices in x to the left and right of each peak. The higher base of each pair is a peak\u2019s lowest contour line.\r\n\r\n![Split peaks](https:\/\/i.imgur.com\/ZBsoMo5.png)\r\n\r\nA naive solution to fix this is provided in the MCVE below, see `clean_base_indices` function.\r\n\r\nThank you for the great work and this awesome package!\r\n\r\n### Reproducing Code Example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom scipy import signal\r\nimport matplotlib.pyplot as plt\r\n\r\ndata = pd.read_csv(\"https:\/\/gist.githubusercontent.com\/jlandercy\/461c36c967c3257d7eaf4f27ecfb9a60\/raw\/f0dd3e63a06291261c629d657ebb55cb4f2990b6\/chromato.csv\", sep=\";\")\r\nx = data[\"x0\"].values\r\ny = data[\"yb\"].values\r\n\r\npeaks = signal.find_peaks(y, width=20.)\r\n#peaks = signal.find_peaks(y, prominence=1.)\r\n\r\n# (array([ 726, 1297, 1691, 2189, 2945, 3390, 3758, 4279], dtype=int64),\r\n# {'prominences': array([13.46167671, 13.99969829, 15.01796069, 12.48824929, 11.72739421,\r\n#         12.56004554, 10.03979127,  8.69610265]),\r\n#  'left_bases': array([ 503,  503,  503, 1980, 2773, 3215, 3615, 4004], dtype=int64),\r\n#  'right_bases': array([1046, 1476, 3215, 3215, 3215, 4926, 4926, 4926], dtype=int64),\r\n#  'widths': array([147.61008396, 135.79017141, 147.63044826, 172.1335768 ,\r\n#          60.89290957,  72.01385079,  94.56465009, 135.28674746]),\r\n#  'width_heights': array([6.5105502 , 6.8888271 , 7.23222624, 6.10395427, 5.59850917,\r\n#         5.99604523, 4.78110177, 4.14248593]),\r\n#  'left_ips': array([ 655.85768704, 1229.62465879, 1624.06622795, 2107.71131882,\r\n#         2916.32946911, 3352.8763468 , 3712.40414302, 4211.35891641]),\r\n#  'right_ips': array([ 803.467771  , 1365.4148302 , 1771.6966762 , 2279.84489562,\r\n#         2977.22237867, 3424.89019759, 3806.9687931 , 4346.64566388])})\r\n\r\nfig, axe = plt.subplots()\r\naxe.plot(x, y)\r\naxe.scatter(x[peaks[0]], y[peaks[0]], color=\"orange\")\r\naxe.scatter(x[peaks[1][\"left_bases\"]], y[peaks[1][\"left_bases\"]], color=\"green\", marker=\"x\")\r\naxe.scatter(x[peaks[1][\"right_bases\"]], y[peaks[1][\"right_bases\"]], color=\"red\", marker=\"+\")\r\nfor left, right in zip(peaks[1][\"left_bases\"], peaks[1][\"right_bases\"]):\r\n    axe.fill_between(x[left:right], y[left:right], color=\"blue\", alpha=0.20)\r\naxe.set_title(\"Chromatogram\")\r\naxe.set_xlabel(\"Time\")\r\naxe.set_ylabel(\"Signal\")\r\naxe.grid()\r\n\r\n# Naive solution (mean cases):\r\ndef clean_base_indices(lefts, rights):\r\n    \"\"\"Clean peak base limits\"\"\"\r\n    _lefts = np.copy(lefts)\r\n    _rights = np.copy(rights)\r\n    for i in range(len(lefts)-1):\r\n        if lefts[i] == lefts[i+1]:\r\n            _lefts[i+1] = rights[i]\r\n        if rights[i] == rights[i+1]:\r\n            _rights[i] = lefts[i+1]\r\n    return _lefts, _rights\r\n\r\n# Expected base indices:\r\nlefts, rights = clean_base_indices(peaks[1][\"left_bases\"], peaks[1][\"right_bases\"])\r\n#(array([ 503, 1046, 1476, 1980, 2773, 3215, 3615, 4004], dtype=int64),\r\n# array([1046, 1476, 1980, 2773, 3215, 3615, 4004, 4926], dtype=int64))\r\n```\r\n\r\n\r\n### Error message\r\n\r\n```shell\r\n# Instead of getting those indices which group some peaks together: \r\n#  'left_bases': array([ 503,  503,  503, 1980, 2773, 3215, 3615, 4004], dtype=int64),\r\n#  'right_bases': array([1046, 1476, 3215, 3215, 3215, 4926, 4926, 4926], dtype=int64),\r\n\r\n# It should be like this which isolate each peak:\r\n#  'left_bases': array([ 503,  1046,  1476, 1980, 2773, 3215, 3615, 4004], dtype=int64),\r\n#  'right_bases': array([1046, 1476, 1980, 2773, 3215, 3615, 4926, 4926], dtype=int64),\r\n```\r\n\r\n\r\n### SciPy\/NumPy\/Python version and system information\r\n\r\n<details>\r\n\r\n```shell\r\nTested on multiple versions:\r\n\r\n1.11.1 1.24.4 sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/c\/opt\/64\/include\r\n    lib directory: \/c\/opt\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\r\n    pc file directory: c:\/opt\/64\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.35\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-gwuhujd5\\overlay\\Lib\\site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-t4t9eut3\\cp310-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.10'\r\n\r\n1.10.1 1.20.3 sys.version_info(major=3, minor=9, micro=7, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: c:\/opt\/openblas\/if_32\/64\/include\r\n    lib directory: c:\/opt\/openblas\/if_32\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= PRESCOTT MAX_THREADS=4\r\n    pc file directory: c:\/opt\/openblas\/if_32\/64\/lib\/pkgconfig\r\n    version: 0.3.18\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: c:\/opt\/openblas\/if_32\/64\/include\r\n    lib directory: c:\/opt\/openblas\/if_32\/64\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= PRESCOTT MAX_THREADS=4\r\n    pc file directory: c:\/opt\/openblas\/if_32\/64\/lib\/pkgconfig\r\n    version: 0.3.18\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.33\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.3.0\r\n  pythran:\r\n    include directory: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-u63ta2f1\\overlay\\Lib\\site-packages\\pythran\r\n    version: 0.12.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: windows\r\nPython Information:\r\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-a1px0t3e\\cp39-win_amd64\\build\\venv\\Scripts\\python.exe\r\n  version: '3.9'\r\n```\r\n\r\n<\/details>","comments":[],"labels":["defect","scipy.signal"]},{"title":"BUG: io: scipy.io.loadmat error message recommend use of function from deprecated namespace","body":"Originally reported by @ldevillez in #19211\r\n\r\n> When using `scipy.io.loadmat` we can have the following message:\r\n> \r\n> ```\r\n> MatReadWarning: Duplicate variable name XXXX in stream - replacing previous with new\r\n> Consider mio5.varmats_from_mat to split file into single variable files\r\n> ```\r\n> \r\n> If we use `scipy.io.matlab.mio5.varmats_from_mat` we then get:\r\n> \r\n> ```\r\n> DeprecationWarning: Please use `varmats_from_mat` from the `scipy.io.matlab` namespace, the `scipy.io.matlab.mio5` namespace is deprecated.\r\n> ```\r\n> \r\n> If we use `scipy.io.matlab.varmats_from_mat`we have:\r\n> \r\n> ```\r\n> AttributeError: module 'scipy.io.matlab' has no attribute 'varmats_from_mat'\r\n> ```\r\n\r\n\r\nEither this function needs exposing in the `scipy.io.matlab` namespace or as suggested by @h-vetinari:\r\n\r\n> My preference would be to fix `scipy.io.loadmat` (or add an option), so that\r\n> \r\n> ```\r\n> MatReadWarning: Duplicate variable name XXXX in stream - replacing previous with new\r\n> Consider mio5.varmats_from_mat to split file into single variable files\r\n> ```\r\n> \r\n> is not necessary in the first place.","comments":["These some usage in the wild although not much\r\n\r\nhttps:\/\/github.com\/AGPlested\/ASCAM\/blob\/9b7052df2da3bcebb550ef898af22bba310523ad\/src\/core\/readdata.py#L7","@matthew-brett maybe you have an opinion on whether `varmats_from_mat` should be exposed into the `scipy.io.matlab` namespace?"],"labels":["defect","scipy.io"]},{"title":"ENH: LinearOperator matvec should be inferable from matmat and only one of them required","body":"### Is your feature request related to a problem? Please describe.\n\nIn  `LinearOperator`, the parameter `matvec` is mandatory, i.e. cannot be deduced from `matmat`. Logically, only one of them `matvec` or `matmat` should be mandated. Currently, only inferring of `matmat` from `matvec` is implemented but not the other way around. \n\n### Describe the solution you'd like.\n\nAllow `matmat` to be mandatory instead of `matvec`. \n\n### Describe alternatives you've considered.\n\nCurrently, if the user want to have a custom `matmat`, both `matvec` and `matmat` must be provided, since the parameter `matvec` is mandatory. Not only it is illogical, but also it may lead to non-trivial errors if the user-provided `matvec` and `matmat` are inconsistent with each other.\n\n### Additional context (e.g. screenshots, GIFs)\n\nSee https:\/\/github.com\/scipy\/scipy\/issues\/19207#issuecomment-1711785535","comments":["For clarification, `LinearOperator` itself already has the internal logic to infer `matvec` from `matmat`. But `_CustomLinearOperator`, which is the implementation used when one constructs `LinearOperator` instances directly from `matvec\/matmat` functions instead of subclassing, explicitly requires `matvec`. It should also allow either `matmat` or `matvec` to be supplied. I conjecture that this was omitted just to simplify the implementation; I don't think it is required.","I was getting an error in the past providing only 'matmat'. I can try it again on the main. "],"labels":["enhancement"]},{"title":"TST: Test failures with openblas >=0.3.24 in conda-forge on aarch","body":"We recently switched to OpenBLAS 0.3.24 as the default in conda-forge, and now that we have a new [PR](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/248) for the scipy recipe and a corresponding CI run, and aarch is blowing up pretty badly.\r\n\r\nThe failure mode is very reminiscent of https:\/\/github.com\/numpy\/numpy\/issues\/24660 actually, further cementing the impression that something's wrong with OpenBLAS 0.3.24 on aarch. CC @martin-frbg\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED linalg\/tests\/test_basic.py::TestLstsq::test_random_overdet - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_tall_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_tall_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_delete_last_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_unit_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_unit_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_neg_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_neg_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_itemize_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_itemize_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_native_byte_order_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRdelete_f::test_non_native_byte_order_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_sqr_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_sqr_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_p_col_tall - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_p_col_sqr - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_tall_p_col_fat - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_fat_p_row_fat - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_fat_p_row_sqr - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_fat_p_row_tall - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_economic_p_col_eco - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_economic_p_col_sqr - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_economic_p_col_fat - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_Mx1_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_Mx1_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_unit_strides_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_unit_strides_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_unit_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_unit_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_neg_strides_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_neg_strides_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_neg_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_neg_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_itemsize_strides_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_itemsize_strides_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_itemsize_strides_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_itemsize_strides_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_native_byte_order_1_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_native_byte_order_p_row - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_native_byte_order_1_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRinsert_f::test_non_native_byte_order_p_col - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_tall_rank_1 - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_tall_rank_p - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_unit_strides_rank_1 - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_unit_strides_rank_p - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_neg_strides_rank_1 - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_neg_strides_rank_p - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_itemsize_strides_rank_1 - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_itemsize_strides_rank_p - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_native_byte_order_rank_1 - AssertionError:\r\nFAILED linalg\/tests\/test_decomp_update.py::TestQRupdate_f::test_non_native_byte_order_rank_p - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::test_pftri - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::test_sfrk_hfrk - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::TestBlockedQR::test_geqrt_gemqrt - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::TestBlockedQR::test_tpqrt_tpmqrt - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::test_pstrf - AssertionError:\r\nFAILED linalg\/tests\/test_lapack.py::test_pstf2 - AssertionError:\r\n= 60 failed, 41935 passed, 2805 skipped, 134 xfailed, 12 xpassed in 5302.31s (1:28:22) =\r\n```\r\n\r\nThe failures are not at all inconsequential, with many of them looking something like:\r\n```\r\nMismatched elements: 144 \/ 144 (100%)\r\nMax absolute difference: 1.56318134\r\nMax relative difference: 1.56318134\r\n x: array([[-0.054046,  0.205639, -0.123517,  0.260355,  0.474265, -0.249005,\r\n         0.177618, -0.301207,  0.618385, -0.037394,  0.023149,  0.252298],\r\n       [ 0.205639,  0.756895, -0.286572,  0.154587,  0.321836,  0.283857,...\r\n y: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],...\r\n```\r\ni.e. complete garbage.\r\n\r\nAs an almost irrelevant aside, there is one extra failure on osx, but that's \"just\" a deterioration of the tolerance:\r\n```\r\n=========================== short test summary info ============================\r\nFAILED odr\/tests\/test_odr.py::TestODR::test_implicit - AssertionError: \r\nArrays are not almost equal to 6 decimals\r\n\r\nMismatched elements: 1 \/ 25 (4%)\r\nMax absolute difference: 1.52291797e-06\r\nMax relative difference: 8.81522156e-07\r\n x: array([[ 2.108927e+00, -1.943767e+00,  7.026353e-02, -4.717525e-02,\r\n         5.251554e-02],\r\n       [-1.943767e+00,  2.048149e+00, -6.160049e-02,  4.626880e-02,...\r\n y: array([[ 2.108927e+00, -1.943769e+00,  7.026355e-02, -4.717527e-02,\r\n         5.251558e-02],\r\n       [-1.943769e+00,  2.048151e+00, -6.160052e-02,  4.626883e-02,...\r\n= 1 failed, 54544 passed, 2850 skipped, 244 xfailed, 11 xpassed in 1036.02s (0:17:16) =\r\n```","comments":["Weren't there supposed to be weekly test builds of the OpenBLAS development branch together with scipy ? The last change that had a remote chance to specifically affect ARMV8 occured more than a month before the release. Any change in compilers etc. on your side ?","There's been an [effort](https:\/\/github.com\/scipy\/scipy\/pull\/18942) towards testing ~nightly~ weekly [builds](https:\/\/anaconda.org\/scientific-python-nightly-wheels\/openblas-libs\/files) from https:\/\/anaconda.org\/scientific-python-nightly-wheels, but from what I can see that's only [in use](https:\/\/github.com\/search?q=repo%3Ascipy%2Fscipy%20%2F--nightly%2F&type=code) in two jobs, none of which test on linux-aarch64:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/9f7549abcc61f5dc72ca000cef7bc43066c74527\/.github\/workflows\/linux_meson.yml#L281\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/9f7549abcc61f5dc72ca000cef7bc43066c74527\/.github\/workflows\/linux_musl.yml#L68\r\n\r\nCC @rgommers @honno @steppi @tupui @andyfaff","Hmm, I think we should indeed expand the usage of OpenBLAS weekly builds. We chose the strategy that we choose for all testing of pre-releases of other dependencies: do it in one or two jobs that are labeled as \"pre-release tests\". The rationale for that is that nightlies are typically much more unstable than released versions, and we want the signal of the failure but not all of our CI jobs turning red. Instead, we tend to go report or fix the problem upstream and then wait till the next nightly with the fix is out, and in the meantime just ignore the failure on the pre-release job.\r\n\r\nHowever, OpenBLAS is quite different from other dependencies - failures are much more likely to be architecture-specific. This bug report highlights the need to expand our usage of the weekly builds. It seems like that would have failed on aarch64 (let's confirm that first by upgrading the relevant CI job) but not on other platforms.\r\n\r\nI'll have a closer look later this morning.\r\n\r\n> Any change in compilers etc. on your side ?\r\n\r\nThis could also be a root cause of course. @h-vetinari is testing in conda-forge, where the build time dependencies for the `openblas` package are likely to have changed between 0.3.23 and 0.3.24. While on the weekly builds they didn't - so if the weekly builds pass, it's due to a conda-forge compiler\/binutils\/etc. change.\r\n\r\n> As an almost irrelevant aside, there is one extra failure on osx, but that's \"just\" a deterioration of the tolerance\r\n\r\nThat is fixed now, see gh-19200.","> @h-vetinari is testing in conda-forge, where the build time dependencies for the `openblas` package are likely to have changed between 0.3.23 and 0.3.24.\r\n\r\nJust checked https:\/\/github.com\/conda-forge\/openblas-feedstock\/pull\/153; we bumped from GCC 11 to GCC 12, but I'd be surprised if this was the reason...","One of the good aspects of M1\/M2 architecture macOS is that it becomes easy to test linux_aarch64 via docker.\r\n\r\nThe linux_aarch64 CI run uses system provided OpenBLAS. The weekly linux_aarch64 wheel build uses https:\/\/anaconda.org\/multibuild-wheels-staging\/openblas-libs\/v0.3.20-571-g3dec11c6\/download\/openblas-v0.3.20-571-g3dec11c6-manylinux2014_aarch64.tar.gz.\r\n\r\nIf we're picking up test errors I would hope that we could slowly expand upstream test suites so they become less of an issue over time.\r\n","I saw no relevant errors with my installation of numpy 1.24 and a clang build of 0.3.24 , unfortunately I cannot seem to get either numpy 1.26.0b1 or any scipy installed on this Android\/Termux system ","@steppi volunteered for debugging this one.","Tested by building 0.3.24 for TARGET=ARMV8 on Apple M1 (gcc 12.2, Asahi Linux, host gcc103 in the gcc compile farm) and replacing the libopenblas in a pip-installed numpy-1.26.0rc1 or scipy1.11.2 (the former using INTERFACE64, the latter not) - no errors in linalg (none overall in scipy, 9 errors in some permissions-related test in numpy). Something must be borked in the conda build, maybe using a gfortran version different from the gcc one ?","> Something must be borked in the conda build, maybe using a gfortran version different from the gcc one ?\r\n\r\nThe build was using GCC 12 for both.","@h-vetinari, I was unable to reproduce in a QEMU instance created with the following command\r\n\r\n```bash\r\nqemu-system-aarch64 -M virt -m 2048 -cpu cortex-a72 -smp 2 -drive file=ubuntu-jammy-aarch64-install.qcow2,if=virtio -bios \/usr\/share\/qemu-efi-aarch64\/QEMU_EFI.fd -device virtio-net-device,netdev=net0 -netdev tap,id=net0,ifname=tap0,script=no,downscript=no -nographic\r\n```\r\n\r\nneither when building OpenBLAS from source, or when using a conda build with OpenBLAS set at 0.3.24. I did find issues when using incompatible gfortran and gcc versions, but what I observed were segfaults, not garbage results.\r\n\r\nBased on the changes made in [OpenBLAS for 0.3.24](https:\/\/github.com\/OpenMathLib\/OpenBLAS\/releases\/tag\/v0.3.24), it doesn't seem likely that the issue is within OpenBLAS itself. I suspect @martin-frbg is correct that there is something wrong with the conda build. Please let me know if you have any pointers that could help me reproduce this\r\n\r\n\r\n","> or when using a conda build with OpenBLAS set at 0.3.24\r\n\r\nWe marked those builds as broken (considering the impact on numpy & scipy CI), so to install them for would require:\r\n```\r\nconda install -c conda-forge\/label\/broken blas=*=openblas\r\n```\r\nHaving a scipy install (ideally from conda-forge), together with that blas build (blas-2.118-openblas, to be exact) should reproduce the errors on aarch.","> > or when using a conda build with OpenBLAS set at 0.3.24\r\n> \r\n> We marked those builds as broken (considering the impact on numpy & scipy CI), so to install them for would require:\r\n> \r\n> ```\r\n> conda install -c conda-forge\/label\/broken blas=*=openblas\r\n> ```\r\n> \r\n> Having a scipy install (ideally from conda-forge), together with that blas build (blas-2.118-openblas, to be exact) should reproduce the errors on aarch.\r\n\r\nThank you. Much appreciated, I'll try this now.","@h-vetinari \r\n\r\nWas still unable to reproduce after installing blas-2.118-openblas through the steps you gave above and trying with a scipy install from conda-forge. I wonder if it could be due to a difference in QEMU settings. Can you help point out where QEMU is set up for the failing test runs?","Thanks a lot for digging into this! It's good news that things don't seem to break for you.\r\n\r\n> Can you help point out where QEMU is set up for the failing test runs?\r\n\r\nMost of the setup is done [here](https:\/\/github.com\/conda-forge\/docker-images\/blob\/main\/download-qemu-static.sh#L8). While I don't doubt that this could be a QEMU bug, what surprises me is that it was so clearly triggered by a change in 0.3.24. The same QEMU (7.2.0) runs fine with openblas 0.3.23, but not with 0.3.24.\r\n\r\nAbove it looked like you're testing through QEMU as well? Which version are you using? And could you try with 7.2.0 perhaps?\r\n\r\n","Thanks. \r\n\r\n> Above it looked like you're testing through QEMU as well? Which version are you using? And could you try with 7.2.0 perhaps?\r\n\r\nI was using 6.2.0, which is what was available through apt on Ubuntu 22.0.4. I\u2019ll try 7.2.0 next. \r\n","This reproduces with [scipy 1.11.4](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/258)...","And it also keeps failing with the newest 0.3.25. So given that previous investigations have showed this to work on native hardware, that leaves QEMU as the prime suspect. We're trying to upgrade from 7.2.0 to 8.1.2, but the weird thing is that @steppi's passing run in emulation used an _older_ QEMU (6.2.0). It's possible that there's an undiscovered regression hiding there somewhere...","These failures [persist](https:\/\/github.com\/conda-forge\/scipy-feedstock\/pull\/267) on aarch for SciPy 1.12, OpenBLAS 0.3.26 & QEMU 8.1.3"],"labels":["scipy.linalg","CI"]},{"title":"ENH: sparse.linalg: `LinearOperator` dtype determination in `_interface.py`","body":"#### Reference issue\r\nCloses gh-19207\r\n\r\n#### What does this implement\/fix?\r\nWhen determining the `dtype`:\r\nMake `dtype` the smallest possible.\r\n\r\n#### BOGO\r\nAdds an ability for ARPACK to work with integer type matrices, previously resulting in a Not Implemented type error.","comments":["Reducing the dtype of the test vector is sensible; otherwise, they essentially all come out as `float64`. But the other two are unnecessary given a correctly-implemented `LinearOperator`.","Using `matmat` substituting `matvec` for dtype determination fails above; see https:\/\/github.com\/scipy\/scipy\/issues\/19207#issuecomment-1712842372\r\nSo revert the change.","I have also tried to include `rmatvec` or `rmatmat` in dtype determination in\r\n```\r\n            #if self.rmatvec is not None:\r\n            #    v = np.zeros((self.shape[0],), dtype=np.int8)\r\n            #    rdtype = np.asarray(self.rmatvec(v)).dtype\r\n            #    self.dtype = np.promote_types(self.dtype, rdtype)\r\n```\r\nbut they are not None if not present, as I have anticipated and hoped for, instead giving the `Not Implemented` error... So I give up.","Ready for review please ","@rkern I guess that I should also update in scipy\/sparse\/linalg\/_eigen\/arpack\/arpack.py the following\r\n```\r\ndef _aslinearoperator_with_dtype(m):\r\n    m = aslinearoperator(m)\r\n    if not hasattr(m, 'dtype'):\r\n        x = np.zeros(m.shape[1])\r\n        m.dtype = (m * x).dtype\r\n    return m\r\n```\r\n?","I don't think so. `LinearOperator` objects will always have a `.dtype` attribute set (unless if one writes a particularly broken subclass).","> I don't think so. `LinearOperator` objects will always have a `.dtype` attribute set (unless if one writes a particularly broken subclass).\r\n\r\nMaybe remove it then as never being executed? It is inconsistent with my changes following the original framework.","Oh, that's the code that already exists? Yes, it should be removed and replaced with just `aslinearoperator()` wherever it is called. It's a very old function that predates the dtype inference now incorporated directly into `LinearOperator`. It's harmless, as it will never take that branch, but also useless.","> Oh, that's the code that already exists? Yes, it should be removed and replaced with just `aslinearoperator()` wherever it is called. It's a very old function that predates the dtype inference now incorporated directly into `LinearOperator`. It's harmless, as it will never take that branch, but also useless.\r\n\r\nI have first thought it would be better for a separate PR but waiting for the final review have just done it. ","Could someone please review it? It's completed.   ","This PR is 4 months old now. Could someone review it and consider merging?"],"labels":["enhancement","scipy.sparse.linalg"]},{"title":"BUG: LinearOperator dtype determination broken","body":"### Describe your issue.\r\n\r\nCalling the `LinearOperator` without specifying the dtype activates the dtype determination in ```scipy\/sparse\/linalg\/_interface.py\", line 182, in _init_dtype\r\n        self.dtype = np.asarray(self.matvec(v)).dtype```\r\nthat looks strange. \r\n\r\nIn `\/scipy\/sparse\/linalg\/_interface.py` the code in question is\r\n```\r\n    def _init_dtype(self):\r\n        \"\"\"Called from subclasses at the end of the __init__ routine.\r\n        \"\"\"\r\n        if self.dtype is None:\r\n            v = np.zeros(self.shape[-1])\r\n            self.dtype = np.asarray(self.matvec(v)).dtype\r\n```\r\n\r\n `v.dtype` is `np.float64` so `self.dtype` will be at least  `np.float64`, like in the docstring example:\r\n```\r\n    >>> import numpy as np\r\n    >>> from scipy.sparse.linalg import LinearOperator\r\n    >>> def mv(v):\r\n    ...     return np.array([2*v[0], 3*v[1]])\r\n    ...\r\n    >>> A = LinearOperator((2,2), matvec=mv)\r\n    >>> A\r\n    <2x2 _CustomLinearOperator with dtype=float64>\r\n```\r\neven though it only multiplies by `int`, so should not   `self.dtype` be `int` in this example?\r\nNaively, is not `self.dtype` suppose to match the `dtype` of the underlying matrix implicitly used for the product? Maybe use `v = np.zeros(self.shape[-1], type=np.int8)` i.e. with the \"smallest\" `dtype`?\r\n\r\nNeed to invoke `matmat` in `dtype` determination together with `matvec` and pickup the largest if both   `matmat` and `matvec` are provided; see https:\/\/github.com\/scipy\/scipy\/issues\/19213\r\n\r\n","comments":["The provided `matvec` must correctly handle the case when the input is `(n,)`-shaped. This is a documented requirement (see the [`Notes`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.linalg.LinearOperator.html)). The `A_matmat` function here is an invalid value for the `matvec` argument.\r\n\r\nDo you have a reproducer with a valid `matvec` implementation?","> The provided `matvec` must correctly handle the case when the input is `(n,)`-shaped. This is a documented requirement (see the [`Notes`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.linalg.LinearOperator.html)). The `A_matmat` function here is an invalid value for the `matvec` argument.\n> \n> Do you have a reproducer with a valid `matvec` implementation?\n\nI have forgotten that, thanks. But why is this a requirement? It prevents a valid matmat to be used also as matvec, which is illogical.\n\nAnother strange requirement is that matvec is mandatory, i.e. cannot be deduced from matmat. Logically, only one of them matvec or matmat should be mandated.","Yes, `_CustomLinearOperator` imposes extra requirements that don't exist for `LinearOperator`, which indeed can infer `matvec` from `matmat` and vice versa. Probably just to simplify the logic. A contribution would be welcome in that vein, but this doesn't have anything to do with `dtype` determination.","In in `scipy\/sparse\/linalg\/_interface.py`\r\nthe code\r\n```\r\nv = np.zeros(self.shape[-1])\r\n self.dtype = np.asarray(self.matvec(v)).dtype\r\n```\r\nis replaced with\r\n```\r\nv = np.zeros((self.shape[-1], 1))\r\nself.dtype = np.asarray(self.matmat(v)).dtype\r\n```\r\nthat could solve the trouble in https:\/\/github.com\/scipy\/scipy\/issues\/19212\r\n\r\nBut my testing it in https:\/\/github.com\/scipy\/scipy\/pull\/19209\/commits\/89f5fe5e5d6f6b1ef3b93f6b087f85caa15df38e gives multiple errors from tests in `scipy\/optimize\/` that probably are not written compliant to generate valid `matmat` from `matvec`? The errors are like:\r\n```\r\nE   ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)\r\n```\r\nand `E   ValueError: cannot reshape array of size 121 into shape (11,1)`\r\nI do not know how to deal with this sudden can of worms.","It is indeed possible that those `LinearOperator` `matvec`s were written incorrectly and assumed they are only passed `(n,)`-shaped arrays (which they were, previously) and not also `(n, 1)` shaped arrays, which they should. Those are certainly worth fixing, in their own right, but not to enable you to change to using `.matmat()` in the dtype-determination. That code should be left alone."],"labels":["defect"]},{"title":"BUG: Overflow and segfaults with fancy indexing on CSR\/CSC when number of returned elements exceeds 2**32.","body":"### Describe your issue.\n\nWhen the number of elements exceeds `2 ** 32`, scipy's CSR\/CSC will overflow during indptr computation and therefore either crash with OOB access or throw exceptions saying \"negative dimension is not allowed\" etc.\n\n### Reproducing Code Example\n\n```python\nimport numpy as np\r\nimport scipy.sparse as ssp\r\n\r\nx = ssp.coo_matrix(\r\n    (np.ones(2 ** 17), (np.zeros(2 ** 17, dtype='int64'), np.arange(2 ** 17, dtype='int64')))\r\n).tocsr()\r\nx[np.zeros(2 ** 17, dtype='int64'), :]\n```\n\n\n### Error message\n\n```shell\ndouble free or corruption (out)\r\nAborted (core dumped)\r\n\r\n(Other errors like segfaults may also occur)\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.2 1.24.4 sys.version_info(major=3, minor=9, micro=17, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= HASWELL MAX_THREADS=2\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  c++:\r\n    commands: c++\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld.bfd\r\n    name: gcc\r\n    version: 10.2.1\r\n  pythran:\r\n    include directory: ..\/..\/tmp\/pip-build-env-ucip512m\/overlay\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: linux\r\nPython Information:\r\n  path: \/opt\/python\/cp39-cp39\/bin\/python\r\n  version: '3.9'\n```\n","comments":["The overflow happens here:\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/709f0d3bd416e4407a580ebd870d9ddf44692b74\/scipy\/sparse\/_compressed.py#L705\r\n\r\nIf `indptr` is of dtype int32, computing cumsum will overflow when the number of elements to return is actually greater than `2**32`, so the size to allocate the returned `indices` and `data` will be incorrect.","I still need to confirm, but I suspect that this issue is fixed when using coo_array instead of coo_matrix, because we no longer downcast indices so aggressively.","I can confirm that this behaviour also existed in 1.10.\r\n\r\nWith a `coo_array`, it didn't segfault, but I also don't have enough memory to see it finish allocating \ud83d\ude05.\r\n\r\n`coo_array` still segfaults with if we pass in 32 bit row and column indices.\r\n\r\nI figure the correct thing to do here is probably error, and tell the user they need to upcast for `_array`. I'm not sure if there is a great way to check for this without incurring some performance penalty.\r\n\r\n----\r\n\r\nNotably:\r\n\r\n```python\r\nrow_nnz = np.broadcast_to(np.array(2 ** 17, dtype=np.int32), 2 ** 17)\r\n\r\n# This overflows:\r\nres_indptr = np.cumsum(row_nnz, dtype=np.int32)\r\nres_indptr\r\n# array([ 131072,  262144,  393216, ..., -262144, -131072,       0],\r\n#       dtype=int32)\r\n\r\n# This segfaults:\r\nres_indptr = np.zeros(M+1, dtype=np.int32)\r\nnp.cumsum(row_nnz, out=res_indptr)\r\n```\r\n"],"labels":["defect","scipy.sparse"]},{"title":"ENH: Adding option to plot either finite or infinite or both entities in voronoi_plot_2d function ","body":"### Is your feature request related to a problem? Please describe.\n\nPlotting only ridge lines or vertices is not possible in voronoi_plot_2d function in the file \r\n\r\n[scipy](https:\/\/github.com\/scipy\/scipy\/tree\/main)\/[scipy](https:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy)\/[spatial](https:\/\/github.com\/scipy\/scipy\/tree\/main\/scipy\/spatial)\r\n\/_plotutils.py \r\n\r\nand is not so convienient for anyone who wants to plot only single entity (either finite entity or infinite entity )\n\n### Describe the solution you'd like.\n\nJust need to be collect 2 more arguments and apply condition inside function .\n\n### Describe alternatives you've considered.\n\n_No response_\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":[],"labels":["enhancement","scipy.spatial"]},{"title":"DOC: How to use LowLevelCallable with package code (or pybind11?)","body":"### Issue with current documentation:\r\n\r\nDoes the new `LowLevelCallable` interface support pybind11 exported functions\/capsules?\r\n\r\n\r\nI apologize if this is just unsupported feature, and I'm not sure if this counts as a feature request, doc issue, or question, but it would nice to know how\/if they can be used interchangeably. I'm sure there are plenty of use cases I can imagine where one might want to do something like: \r\n\r\n```cpp\r\n#include <cmath>\r\n#include <pybind11\/pybind11.h>\r\nnamespace py = pybind11;\r\n\r\ndouble square(double x) {\r\n    return x * x;\r\n}\r\n\r\ndouble exp(double x) {\r\n    return std::exp(x);\r\n}\r\n\r\nPYBIND11_MODULE(_funcs, m) {\r\n    m.def(\"square\", &square);\r\n    m.def(\"exp\", &exp);\r\n}\r\n```\r\n\r\nIn a package, and then in SciPy: \r\n\r\n```python\r\nfrom scipy import LowLevelCallable\r\nfrom my_package import _funcs\r\nsquare_f = LowLevelCallable(_funcs.square, user_data=None, signature=\"double (double)\")\r\n...\r\nimport scipy.integrate as integrate\r\nintegrate(square_f, ...)\r\n```\r\n\r\nAt first I thought I could do this, since pybind11 uses PyCapsule's. \r\n\r\n```python\r\n_funcs\r\n# <module 'my_package._funcs' from '<user_path>\/build\/cp39\/my_package\/_funcs.cpython-39-darwin.so'>\r\n\r\n_funcs.square\r\n# <function my_package._funcs.PyCapsule.square>\r\n\r\nstr(_funcs.square)\r\n# <built-in method square of PyCapsule object at 0x7fe93f6641b0>\r\n```\r\n\r\nAfter glancing at [the source code](https:\/\/github.com\/scipy\/scipy\/blob\/v1.11.2\/scipy\/_lib\/_ccallback.py#L26-L179), I came up with: \r\n\r\n```python\r\nfrom scipy._lib._ccallback import _ccallback_c\r\ncapsule_obj = _ccallback_c.get_raw_capsule( _funcs.square.__self__, \"double (double)\", 0)\r\nsquare_callable = LowLevelCallable(capsule_obj)\r\n```\r\n\r\nSciPy let's me construct the LowLevelCallable actually, but I couldn't use it (crashes the kernel). Unfortunately my knowledge of inner Python FFI-mechanics is at its limit at this point. Is this possible to do with SciPy?\r\n\r\nIf so, could someone mention pybind11 in the documentation somewhere? pybind11 is [almost as popular](https:\/\/pypistats.org\/packages\/pybind11) as [cffi](https:\/\/pypistats.org\/packages\/cffi), so I would think it deserves a mention if it's a possibility.   \r\n\r\n\r\n\r\n### Idea or request for content:\r\n\r\n_No response_\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":[],"labels":["Documentation","query"]},{"title":"ENH: Derivatives for `CloughTocher2DInterpolator`","body":"### Is your feature request related to a problem? Please describe.\n\nThe [CloughTocher2DInterpolator](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.interpolate.CloughTocher2DInterpolator.html#scipy.interpolate.CloughTocher2DInterpolator) does not provide a method for computing its partial derivatives at points, despite telling the user it is C1 smooth. There is demand for this feature both from me and others ([Stack Overflow post](https:\/\/stackoverflow.com\/questions\/61883348\/how-to-get-the-first-and-second-derivatives-after-interpolating-with-scipy-inter) and others I know by word of mouth, including @lucaeros).\n\n### Describe the solution you'd like.\n\nImplement derivative computation in one of two ways that appear to be common among other interpolators and splines:\r\n\r\n1. Add optional `dx` and `dy` arguments to the `__call__` method to specify the order of the partial derivative in the x and y directions.\r\n2. Add a method called `partial_derivative` that takes the same `dx` and `dy` arguments to specify the order of the derivative.\n\n### Describe alternatives you've considered.\n\nWithout an exact\/analytic derivative computation routine, I am left to one of two options:\r\n\r\n1. Compute derivatives using finite differences. This provides derivatives that are consistent with the spline, but not accurate to machine precision.\r\n2. Create a new interpolator of the CloughTocher2DInterpolator's `grad` attribute. This provides exact derivatives at the training data points but the derivatives are not necessarily consistent with the spline elsewhere.\r\n\r\nBased on looking at the source code, it seems like there is some support for complex numbers. This would enable use of [complex-step derivative approximation](https:\/\/mdolab.engin.umich.edu\/wiki\/guide-complex-step-derivative-approximation) to compute derivatives which provides accuracy to machine precision. However, I wasn't able to get the hidden `_evaluate_complex` method to work properly (and I probably shouldn't be using \"private\" methods anyway).\n\n### Additional context (e.g. screenshots, GIFs)\n\n_No response_","comments":["Using finite differences does seem to be suboptimal indeed :-). FWIW, I don't think there's an easy way to use complex step differentiation --- you need to be able to evaluate at arguments (`xi`) with nonzero imaginary part, but the `_evaluate_complex` routine (which indeed should not be used directly) deals with complex-valued `values`. And does complex-step differentiation actually generalize to d>1?\r\n\r\nHowever. Using explicit formulas for the cubics on each simplex, https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/interpolate\/interpnd.pyx#L623, it should indeed be possible to construct the values of the derivatives. \r\n\r\nAPI-wise, `__call__(xi, dx=0, dy=0)` should evaluate the derivatives in-place, and `partial_derivative(dx, dy)` method should return an object representing the corresponding derivative. (cf `BivariateSpline.partial_derivative` in 2D; `PPoly.derivative` in 1D etc)  It will be a piecewise quadratic though, which scipy.interpolate does not have. One could probably shoehorn these quadratics into cubics with zero leading coefficients but that starts to smell fishy. So I'd rather stop at an in-place evaluation.\r\n\r\nOverall this all sounds very reasonable, and we'd be happy to review a pull request with an implementation."],"labels":["enhancement","scipy.interpolate"]},{"title":"ENH: Would you consider making ccallback part of the public scipy interface?","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nHi,\r\n\r\nLately, I've been tinkering with a numerical code of mine written in Cython.\r\nFollowing closely the inner workings of scipy.integrate.quad, I've managed to have my Cython code accept either a Python callable or a LowLevelCallable wrapping a cdef Cython function.\r\n\r\nFor this, I had to copy both https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/_lib\/ccallback.pxd and https:\/\/github.com\/scipy\/scipy\/blob\/main\/scipy\/_lib\/src\/ccallback.h to make them available to Cython.\r\n\r\nUnless I'm mistaken, this is the only way to `cimport ccallback_t, ccallback_prepare, ccallback_release, CCALLBACK_DEFAULTS, ccallback_signature_t` in my Cython code.\r\n\r\n### Describe the solution you'd like.\r\n\r\nWould you consider making these files part of the public interface? Just like I can write the following in my Cython code \r\n\r\n```python\r\ncimport scipy.linalg.cython_blas\r\ncimport scipy.linalg.cython_lapack\r\n```\r\n\r\nI would love to be able to write something like\r\n\r\n```python\r\nfrom scipy.ccallback cimport ( ccallback_t, ccallback_prepare, ccallback_release, \r\n    CCALLBACK_DEFAULTS, ccallback_signature_t)\r\n```\r\n\r\n\r\n\r\n### Describe alternatives you've considered.\r\n\r\nCopying the files in my own source code\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["Thanks @gabrielfougeron, I would suggest emailing the mailing list with this proposal to gauge interest and because that is generally required for new additions to the public api","Thanks for the suggestion, I'll do that :-)\r\n\r\nFor x-ref: https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/Q2EMWF5ECXPBKSHCXVQLFNPRD4RBMZMW\/"],"labels":["enhancement","Cython"]},{"title":"ENH: add Landau distribution to SciPy.stats.","body":"<!-- \r\nThanks for contributing a pull request! Please ensure that\r\nyour PR satisfies the checklist before submitting:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#checklist-before-submitting-a-pr\r\n\r\nAlso, please name and describe your PR as you would write a\r\ncommit message:\r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/development_workflow.html#writing-the-commit-message\r\n\r\nDepending on your changes, you can skip CI operations and save time and energy: \r\nhttp:\/\/scipy.github.io\/devdocs\/dev\/contributor\/continuous_integration.html#skipping\r\n\r\nNote that we are a team of volunteers; we appreciate your\r\npatience during the review process.\r\n\r\nAgain, thanks for contributing!\r\n-->\r\n\r\n#### Reference issue\r\nNo reference issue.\r\n\r\n#### What does this implement\/fix?\r\nAdd [Landau distribution](https:\/\/en.wikipedia.org\/wiki\/Landau_distribution), a special case of [`scipy.stats.levy_stable`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.levy_stable.html) when `alpha=beta=1`.\r\n\r\n#### Additional information\r\nThe specific implementation of `landau` I am providing is much faster than `levy_stable(alpha=1, beta=1)` (approx 1500 times faster for the `pdf`), making it suitable for applications which are not possible with the current `levy_stable`. It also fixes a discontinuity I am finding in the `levy_stable(alpha=1, beta=1, loc=0, scale=1).pdf` at x approx 180, and `levy_stable(alpha=1, beta=1, loc=0, scale=1).cdf` at x approx 300. \r\n\r\nI think it would be useful to have it implemented separately, and adding a reference to it, along side `levy_l`, `levy`, `cauchy` and `norm`, in the documentation of `levy_stable`. In my field, radiation detectors, the Landau distribution is encountered a lot and many colleagues (including me) complain that it is not easily available in Python.\r\n\r\nFinally, I think I went through all the steps of the [SciPy contributor guide](https:\/\/scipy.github.io\/devdocs\/dev\/contributor\/contributor_toc.html). All tests are passing locally. The only issue is that I  was not able to build the documentation locally due to `UndefinedError(\"'generate_nav_html' is undefined\")` and could not find a solution to this. I hope you can provide me some assistance, in case you find this contribution of interest.","comments":["This should be approved as a feature before review. Please propose this as a feature on the [scipy-dev](https:\/\/projects.scipy.org\/scipylib\/mailing-lists.html) mailing list.","Here is [the mailing list thread](https:\/\/mail.python.org\/archives\/list\/scipy-dev@python.org\/thread\/3LYHFK2GJXCEHTK3UY3DEZKPJZNB2KH7\/#ZHDRHRQTZZJDLK5JGZ2BCCXXLHPXPMCQ).\r\n\r\nThere was roughly a +0.5 from @WarrenWeckesser since\r\n\r\n> The specialized code could probably be made much faster and\/or more accurate than the implementation of `levy_stable`.\r\n\r\nbut,\r\n\r\n> It would be good to get more feedback from users (probably mostly physicists) on whether there is demand for it.","Thanks for the contribution @SengerM . As a trained physicist myself,  I agree that this distribution is important enough to be added to SciPy.\r\n\r\nBefore the actual initial review, be aware that this PR could become a lengthy process. Do not get discouraged by long response time. Many maintainers of `scipy.stats` are working on a rewrite of the distribution infrastructure funded by grant money, so there is currently less reviewer time available than usual.\r\n\r\nSome comments from 10k feet:\r\n\r\n- please merge the main branch into your branch and check that all tests pass locally. Especially the linting job found a lot of issues which are annoying but easy to fix.\r\n- the code for PDF and CDF calculation should go in a dedicated new file, the `continuous_distns.py` file is already way too big.\r\n- Adding a `rvs` method to generate samples is not necessary at this point. The distribution infrastructure uses numerical inversion of the CDF already, which looks similar to what you implemented.\r\n- Since `levy_stable` is problematic and you seem to be familiar with Root, could you use Root to generate reference values to compare against for CDF and PDF. This is what we also did for example with another physics inspired distribution: #17505\r\n- Could you provide a simple use case from physics in the examples section (really as simple as possible)? SciPy's distributions often uses different parametrizations than the specific domains which confuses many users. Or before that, maybe a short use case description in the github discussion so that review becomes easier. ","Another physicist here. I've been dreaming of having a landau distribution available in scipy for years now. Hope this gets traction!"],"labels":["scipy.stats","enhancement","needs-work"]},{"title":"ENH: Multivariate log beta function","body":"#### Reference issue\r\nCloses #17977\r\n\r\n#### What does this implement\/fix?\r\nImplements the log of the multivariate beta function in `special`.\r","comments":["Thanks @dschmitz89. I think this function could be useful to have but I\u2019m not a big fan of adding pure python functions to special that are straightforward to implement. If it\u2019s going in special I think it should be written in a compiled language for speed and use the lanczos approximation as discussed in the boost documentation [here](https:\/\/www.boost.org\/doc\/libs\/1_83_0\/libs\/math\/doc\/html\/math_toolkit\/lanczos.html) in order to achieve better precision than the exp log sum would achieve. I used the lanczos approximation for a four gamma ratio here https:\/\/github.com\/scipy\/scipy\/blob\/8bb5e31784573f56a8de8362c15e61d66225f15b\/scipy\/special\/_hyp2f1.pxd#L351, in a way which  could be generalized to an arbitrary number of arguments. \r\n\r\n I don\u2019t think the existing special infrastructure can support creating ufuncs with an arbitrary number of args, so it would require using the numpy c api directly. I\u2019m going to be working on the special infrastructure, so might try to find a way to more easily support ufuncs with an arbitrary of args. \r\n\r\n\r\n\r\n","FYI: Instead of an arbitrary number of arguments, the function could be implemented as a [generalized ufunc](https:\/\/numpy.org\/devdocs\/reference\/c-api\/generalized-ufuncs.html) (gufunc) and take a 1D array as input. I implemented [multivariate_logbeta](https:\/\/github.com\/WarrenWeckesser\/ufunclab#multivariate_logbeta), the log of the multivariate beta function, in [ufunclab](https:\/\/github.com\/WarrenWeckesser\/ufunclab).  It is implemented as a gufunc with shape signature `(n)->()`, meaning its core calculation takes a 1D input of length `n` and returns a scalar.\r\n\r\nE.g.\r\n\r\n```\r\nIn [24]: from ufunclab import multivariate_logbeta\r\n\r\nIn [25]: multivariate_logbeta([0.05, 1.5, 6, 2.5])\r\nOut[25]: -4.994274761180311\r\n\r\nIn [26]: x = np.array([[0.05, 1.5, 6, 2.5], [1, 1, 1, 3], [13, 15, 39, 17]])\r\n\r\nIn [27]: x\r\nOut[27]: \r\narray([[ 0.05,  1.5 ,  6.  ,  2.5 ],\r\n       [ 1.  ,  1.  ,  1.  ,  3.  ],\r\n       [13.  , 15.  , 39.  , 17.  ]])\r\n\r\nIn [28]: multivariate_logbeta(x)  # Compute the value for each row of x\r\nOut[28]: array([  -4.99427476,   -4.09434456, -108.0746389 ])\r\n```\r\n\r\nThe core calculation is in the C++ file [multivariate_logbeta_gufunc.h](https:\/\/github.com\/WarrenWeckesser\/ufunclab\/blob\/main\/src\/multivariate_logbeta\/multivariate_logbeta_gufunc.h).  A configuration file in the same directory is used by a code generation script to generate all the boilerplate code that implements the extension module.  The implementation uses [`std::lgamma`](https:\/\/en.cppreference.com\/w\/cpp\/numeric\/math\/lgamma);  I haven't looked into where this relatively naive implementation might lose precision.\r\n\r\n","> FYI: Instead of an arbitrary number of arguments, the function could be implemented as a [generalized ufunc](https:\/\/numpy.org\/devdocs\/reference\/c-api\/generalized-ufuncs.html) (gufunc) and take a 1D array as input. I implemented [multivariate_logbeta](https:\/\/github.com\/WarrenWeckesser\/ufunclab#multivariate_logbeta), the log of the multivariate beta function, in [ufunclab](https:\/\/github.com\/WarrenWeckesser\/ufunclab). It is implemented as a gufunc with shape signature `(n)->()`, meaning its core calculation takes a 1D input of length `n` and returns a scalar.\r\n> \r\n> E.g.\r\n> \r\n> ```\r\n> In [24]: from ufunclab import multivariate_logbeta\r\n> \r\n> In [25]: multivariate_logbeta([0.05, 1.5, 6, 2.5])\r\n> Out[25]: -4.994274761180311\r\n> \r\n> In [26]: x = np.array([[0.05, 1.5, 6, 2.5], [1, 1, 1, 3], [13, 15, 39, 17]])\r\n> \r\n> In [27]: x\r\n> Out[27]: \r\n> array([[ 0.05,  1.5 ,  6.  ,  2.5 ],\r\n>        [ 1.  ,  1.  ,  1.  ,  3.  ],\r\n>        [13.  , 15.  , 39.  , 17.  ]])\r\n> \r\n> In [28]: multivariate_logbeta(x)  # Compute the value for each row of x\r\n> Out[28]: array([  -4.99427476,   -4.09434456, -108.0746389 ])\r\n> ```\r\n> \r\n> The core calculation is in the C++ file [multivariate_logbeta_gufunc.h](https:\/\/github.com\/WarrenWeckesser\/ufunclab\/blob\/main\/src\/multivariate_logbeta\/multivariate_logbeta_gufunc.h). A configuration file in the same directory is used by a code generation script to generate all the boilerplate code that implements the extension module. The implementation uses [`std::lgamma`](https:\/\/en.cppreference.com\/w\/cpp\/numeric\/math\/lgamma); I haven't looked into where this relatively naive implementation might lose precision.\r\n\r\nNice. Yeah, I think that\u2019s the way to go. I think that\u2019s what I\u2019d been trying to say, since @dschmitz89\u2019s implementation also takes a single array, but I didn\u2019t know the term gufunc. I\u2019m +1 for adding your implementation as a log multivariate beta but don\u2019t think we should directly add a multivariate beta by just exponentiating it. I\u2019m also +1 for adding the gufunc boilerplate generation to special to make it easier to add gufuncs in the future. ","Thanks for your comments. Adding a version implemented in C as gufunc without infrastructure for such purposes is far beyond my current C skills. I had assumed that as `multigammaln` was implemented in Python, we could add `multivariate_beta` also in Python. Could we leave the native implementation for a follow up? Otherwise I am tempted to close this and focus on other things.","> Thanks for your comments. Adding a version implemented in C as gufunc without infrastructure for such purposes is far beyond my current C skills. I had assumed that as `multigammaln` was implemented in Python, we could add `multivariate_beta` also in Python. Could we leave the native implementation for a follow up? Otherwise I am tempted to close this and focus on other things.\r\n\r\nA Python implementation might be fine for the log of the multivariate beta,  but just exponentiating the log wouldn\u2019t make for a good multivariate beta implementation. ","> > Thanks for your comments. Adding a version implemented in C as gufunc without infrastructure for such purposes is far beyond my current C skills. I had assumed that as `multigammaln` was implemented in Python, we could add `multivariate_beta` also in Python. Could we leave the native implementation for a follow up? Otherwise I am tempted to close this and focus on other things.\r\n> \r\n> A Python implementation might be fine for the log of the multivariate beta, but just exponentiating the log wouldn\u2019t make for a good multivariate beta implementation.\r\n\r\nOk, I will change this PR to the log of the multivariate beta then, that is easy to do. For the multivariate beta we wait for a Lanczos approximation based implementation.","@steppi: care to take another look? This implements the log of the multivariate beta now.","> @steppi: care to take another look? This implements the log of the multivariate beta now.\r\n\r\nI've ended being busier than expected this week. I have some other things that need to take priority before we branch on December 6th but I'll try to find time to help get this into 1.12 if possible."],"labels":["enhancement","scipy.special"]},{"title":"ENH: Allow further customisation of lapack input parameters for linalg.svd lapack_driver='gesvd'","body":"### Is your feature request related to a problem? Please describe.\r\n\r\nAs a follow-up to a ticket I logged as bug that wasn't a bug ([18980](https:\/\/github.com\/scipy\/scipy\/issues\/18980)) I've done further digging and after building numpy and scipy against Intel MKL in CONDA I can get the same results as my C++ if I utilise Cython to create my own wrapper for the lapack DGESVD function.\r\n\r\nHowever, because of the way the 'full_matrices' method defines the values for both JOBU and JOBVT parameters in the lapack call [here](https:\/\/github.com\/scipy\/scipy\/blob\/d89af411c10d67f32df19df0b18e420f9d3e75fa\/scipy\/linalg\/flapack_gen.pyf.src#L423C110-L423C110) there isn't a way to call 'Path 1t' in the DGESVD function, which is designed for scenarios where the number of columns is significantly larger than the number of rows, natively using the SciPy `linalg.svd` function.  \r\n\r\nIn order to call this particular path the inputs need to be definable as JOBU='S' and  JOBVT='N', which isn't able to be currently be done directly in SciPy, since 'full_matrices=True' appears to be used to set the values of JOBU and JOBVT together, so there's no way to set them independantly.\r\n\r\n### Describe the solution you'd like.\r\n\r\nA kwarg that enables the use of this path natively in `linalg.svd` when using `lapack_driver='gesvd'`.\r\n\r\n### Describe alternatives you've considered.\r\n\r\nI've been able to achieve what I required using Cython code, as per below - but it's considerably more messing around.\r\n\r\n```\r\n# cython: wraparound = False\r\n# cython: boundscheck = False\r\n\r\nfrom scipy.linalg.cython_lapack cimport dgesvd\r\nfrom scipy.linalg.cython_blas cimport dgemm\r\n\r\ncpdef lapack_dgesvd(\r\n    char* JOBU,\r\n    char* JOBVT,\r\n    int M,\r\n    int N,\r\n    double[::1] A,\r\n    int LDA,\r\n    double[::1] S,\r\n    double[::1] U,\r\n    int LDU,\r\n    double[::1] VT,\r\n    int LDVT,\r\n    double[::1] WORK,\r\n    int LWORK,\r\n):\r\n    cdef int INFO\r\n    dgesvd(JOBU, JOBVT, &M, &N, &A[0], &LDA, &S[0], &U[0], &LDU, &VT[0], &LDVT, &WORK[0], &LWORK, &INFO)\r\n\r\n    return INFO\r\n```\r\n\r\n### Additional context (e.g. screenshots, GIFs)\r\n\r\n_No response_","comments":["This is typically not needed in the `svd` API but you should have been able to use the `scipy.linalg.lapack.dgesvd`. Unfortunately that one is also short-circuited at that wrapper. I still don't know what it would matter since the results can be truncated to have the smaller matrix. If you have performance reasons, then Cythonizing is the way to go anyways."],"labels":["enhancement","scipy.linalg"]},{"title":"BUG: `.format` is no longer a class attribute for spmatrix, sparray","body":"### Describe your issue.\n\nIn scipy 1.10.* the `format` attribute of the sparse classes was a string, in 1.11.* it's a property, so is only a string on an instance.\r\n\r\nIt is sometimes useful to have the mapping between format string and class available without having to instantiate it.\r\n\r\n`format` was changed into a property in @stefanv's:\r\n\r\n* https:\/\/github.com\/scipy\/scipy\/pull\/18507\r\n\r\nAs a solution, I would suggest that `.format` reverts to being a class attribute.\r\n\r\nThat said, I don't know the original reasoning behind this change. I'm also opening this bug report because I tried something and it didn't work, not because already written code broke. I don't know if this change broke anything in the wild.\n\n### Reproducing Code Example\n\n```python\nfrom scipy import sparse\r\n\r\nassert sparse.csr_matrix.format == \"csr\", sparse.csr_matrix.format\n```\n\n\n### Error message\n\n```shell\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nCell In[5], line 3\r\n      1 from scipy import sparse\r\n----> 3 assert sparse.csr_matrix.format == \"csr\", sparse.csr_matrix.format\r\n\r\nAssertionError: <property object at 0x1076c74a0>\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\n1.11.1 1.24.4 sys.version_info(major=3, minor=9, micro=16, releaselevel='final', serial=0)\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/usr\/local\/include\r\n    lib directory: \/usr\/local\/lib\r\n    name: openblas\r\n    openblas configuration: USE_64BITINT= DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\r\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\r\n    pc file directory: \/usr\/local\/lib\/pkgconfig\r\n    version: 0.3.21.dev\r\n  pybind11:\r\n    detection method: config-tool\r\n    include directory: unknown\r\n    name: pybind11\r\n    version: 2.10.4\r\nCompilers:\r\n  c:\r\n    commands: cc\r\n    linker: ld64\r\n    name: clang\r\n    version: 13.0.0\r\n  c++:\r\n    commands: c++\r\n    linker: ld64\r\n    name: clang\r\n    version: 13.0.0\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.35\r\n  fortran:\r\n    commands: gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 11.3.0\r\n  pythran:\r\n    include directory: ..\/..\/..\/..\/..\/..\/private\/var\/folders\/24\/8k48jl6d249_n_qfxwsl6xvm0000gn\/T\/pip-build-env-_xo5eh53\/overlay\/lib\/python3.9\/site-packages\/pythran\r\n    version: 0.13.1\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\n  cross-compiled: false\r\n  host:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\nPython Information:\r\n  path: \/private\/var\/folders\/24\/8k48jl6d249_n_qfxwsl6xvm0000gn\/T\/cibw-run-i6v4tzte\/cp39-macosx_x86_64\/build\/venv\/bin\/python\r\n  version: '3.9'\n```\n","comments":[],"labels":["defect","scipy.sparse"]},{"title":"META: `slsqp` optimizer","body":"Hi all,\r\n\r\nafter looking at the long list of `scipy.optimize` issues I thought it was a good idea to generate a separate tracking issue for the `slsqp` optimizer. Why? There is almost a dozen related issues and `slsqp` is currently being rewritten in Python in https:\/\/github.com\/scipy\/scipy\/pull\/19121. So we could see if the new implementation potentially closes some of them.\r\n\r\nCC @ilayn \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/3816 list for the `args` argument needs to be coerced into tuple\r\n#6689 : proposes to use nlopt's implememtation. Can definitely be closed once #19121 is merged.\r\n#7519\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/15179\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/14915\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/14394 asks to return Lagrange multipliers\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/10416 `slsqp` does not work for more than 32bit decision variables\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/8065 `slsqp` converges to maximum, not minimum\r\n#19362 The warning emitted is not helpful\r\n\r\n**Undocumented parameter `acc`**\r\n#3824\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/7518","comments":[],"labels":["scipy.optimize"]},{"title":"ENH: optimization of all eigenvector analytic computation in LaplacianNd class","body":"### Describe your issue.\n\nMerged in https:\/\/github.com\/scipy\/scipy\/pull\/19057\/, computing the eigenvectors involves using lists to quickly get a short readable code. Eigenvectors are computed analytically one-by-one to follow the ordered eigenvalues, including the case of all which is not efficient. Thus, some optimization could be done, e.g., writing a separate fully vectorized code to compute all eigenvectors and reorder them after that. Low priority but useful.\n\n### Reproducing Code Example\n\n```python\nfrom scipy.sparse.linalg import LaplacianNd\r\ngrid_shape = (2, 3)\r\nlap = LaplacianNd(grid_shape, boundary_conditions='dirichlet')\r\nlap.eigenvectors()\n```\n\n\n### Error message\n\n```shell\nNo error, but takes longer than necessary.\n```\n\n\n### SciPy\/NumPy\/Python version and system information\n\n```shell\nThe current main, with 1.12.0 milestone\n```\n","comments":[],"labels":["defect","scipy.sparse.linalg"]},{"title":"WIP:ENH:optimize:Rewrite SLSQP solver","body":"#### Reference issue\r\nPart of #18566 \r\n\r\n#### What does this implement\/fix?\r\nIn the current SQP solver code, there are multiple serious bugs that makes this code unsuitable for production code which I found out completely by accident. A few of them are\r\n\r\n1. Probably the obvious one is that the function `linmin` is fully broken because of uninitialized variables due to this initial jump \r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/maintenance\/1.11.x\/scipy\/optimize\/slsqp\/slsqp_optmz.f#L1660\r\nreturning junk depending on the compiler. But since exact line search is typically not used, it stayed under the radar (or that would be my guess). It turns out NLOPT also turned this branch off. Hence that's dead code but pythonized it anyways. When GOTO's are unrolled it just reads quite bad however there is no point losing time on that part.\r\n2. The linear algebra is completely hand-woven. No blas functions are used and not much care is given to arithmetic ops. Related to that it is not difficult to choke the code with large arrays. \r\n3. The helper functions are mostly trivial given scipy.linalg and numpy already has everything needed at a high level. Depending on the slowdown amount I'll Cythonize\/Pythranize the parts if necessary. This is all regular matrix ops hence performance is likely to stay the same as with previous rewrites. I'll rewrite LDL update in a bit more contemporary fashion. \r\n4. Through random data examples, the accuracy of `lsei` and hence `lsq` is already increased probably due to <strike>blas\/lapack<\/strike> better `nnls` implementation. Thus probably things will get slightly better in terms of solvability.\r\n\r\nfunction conversion progress\r\n\r\n-  [ ] slsqp (trivial wrapper)\r\n-  [ ] slsqpb\r\n-  [x] lsq (another not trivial but a bogus wrapper that converts `xl, xu` bounds to `Gx >= h` format)\r\n-  [x] lsei\r\n-  [x] lsi\r\n-  [x] ldp\r\n- [x]  linmin\r\n\r\n\r\nI'll add docstrings and other details as things shape up.I just wanted to put it here in case someone is interested and have comments in the early stage. \r\n\r\n\r\n#### Additional information\r\nI'd appreciate if there are large problems that you know of bigger than a couple of thousand variables to test things out properly.\r\n\r\n","comments":["This PR is great! Seeing how much of those black box algorithms can actually be done with standard linear algebra functions is a revelation. I am thinking that the inequality constrained least squares codes could also have a place in `optimize` in future as they are quite common.","Indeed things are quite straightforward but as with other fortran swaps, the really difficult part is to follow the memory pointeers such that we don't break too much code. However there are a bit too many booby traps and I have not grasped the whole idea behind the code yet. So things are going still slow. \r\n\r\nBut I'm testing the `ldp`, `lsi` and `lsei` separately for my own sanity hence they are kinda sorta robust now. It's up to `optimize` folk to decide if they are worth it. I honestly can't decide. ","Oh and if you have such problems lying around, I'd really appreciate if you also test them, with and without consistent constraints.","> Oh and if you have such problems lying around, I'd really appreciate if you also test them, with and without consistent constraints.\r\n\r\nNot anymore unfortunately but I will look for some examples on the web. `cvxpy` is a good reference for the best solution in my experience.","I finished SLSQP conversion but there are so many tiny helper functions piled up on top of the core function, I can't figure out how to get things out of the way without breaking so that the call reaches down to the translated part with `Bound` objects. I'll try to push the part that I have as soon as I have the chance so folks can have a look at it. "],"labels":["enhancement","scipy.optimize","Fortran"]}]
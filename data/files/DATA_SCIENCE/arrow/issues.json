[{"title":"cmake log doesn't adequately report what options imply what other options: ARROW_FLIGHT appears to imply ARROW_COMPUTE, but cmake doesn't say this","body":"### Describe the enhancement requested\r\n\r\nI am building Arrow with ```-DARROW_COMPUTE:BOOL=false``` and ```-DARROW_FLIGHT:BOOL=true```.\r\ncmake prints:\r\n```\r\n--   ARROW_COMPUTE=false [default=OFF]\r\n--       Build all Arrow Compute kernels\r\n```\r\nand\r\n```\r\n--   ARROW_FLIGHT=true [default=OFF]\r\n--       Build the Arrow Flight RPC System (requires GRPC, Protocol Buffers)\r\n```\r\n\r\nHowever, after build and install are done all headers for the ARROW_COMPUTE option are installed in ```include\/arrow\/compute\/*```. libdata\/pkgconfig\/arrow-compute.pc, also specific to ARROW_COMPUTE, aren't installed.\r\n\r\nProblems:\r\n1) cmake should either not allow the combination ```-DARROW_COMPUTE:BOOL=false -DARROW_FLIGHT:BOOL=true```, or should warn that ARROW_COMPUTE is changed to \"ON\".\r\n2) It appears that the build with the above flags turns ARROW_COMPUTE to ON, but libdata\/pkgconfig\/arrow-compute.pc doesn't get installed.\r\n\r\nVersion: 15.0.2\r\nFreeBSD 14.0\r\n\r\n### Component(s)\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40634: [C#] ArrowStreamReader should not be null","body":"### What changes are included in this PR?\r\n\r\nSmall refactoring in the IPC reader implementation classes of how the schema is read in order to support getting the schema asynchronously through ArrowStreamReader and avoiding the case where ArrowStreamReader.Schema returns null because no record batches have yet been read.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nA new method ArrowStreamReader.GetSchema has been added to allow the schema to be gotten asynchronously.\r\n\r\nCloses #40634 \n* GitHub Issue: #40634","comments":[],"labels":["Component: C#","awaiting review"]},{"title":"Multiple test failures on PowerPC (big endian), on 15.0.2","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nWhen running the test suite on Gentoo PowerPC (POWER9), I'm getting the following test failures:\r\n\r\n```\r\nThe following tests FAILED:\r\n         18 - arrow-compute-internals-test (Failed)\r\n         25 - arrow-utility-test (Failed)\r\n         40 - parquet-internals-test (Failed)\r\n         41 - parquet-reader-test (Failed)\r\n         43 - parquet-arrow-test (Failed)\r\n         44 - parquet-arrow-internals-test (Failed)\r\n         45 - parquet-encryption-test (Failed)\r\n         46 - parquet-encryption-key-management-test (Failed)\r\nErrors while running CTest\r\n```\r\n\r\nBuild log: [build.txt](https:\/\/github.com\/apache\/arrow\/files\/14731364\/build.txt)\r\nTest log: [test.txt](https:\/\/github.com\/apache\/arrow\/files\/14731370\/test.txt)\r\n\r\nSome of them clearly look like endianness issues, others are unclear to me.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"GH-40762: [C++] Fix issue with define ARROW_CXX_COMPILER_FLAGS:","body":"Fix issue with define ARROW_CXX_COMPILER_FLAGS:\nhttps:\/\/github.com\/apache\/arrow\/issues\/40762\n\n* GitHub Issue: #40762","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n",":warning: GitHub issue #40762 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: C++","awaiting review"]},{"title":"[C++] Issue with define ARROW_CXX_COMPILER_FLAGS","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe define `ARROW_CXX_COMPILER_FLAGS` has issues with some forms of flags. For instance our build system automatically generates suppressions like `-D__DATE__=\\\"redacted\\\"`. The result is a compiler error in `cpp\/src\/arrow\/util\/config.h.cmake`.\r\n\r\nWhile cmake could provide correctly escaped flag strings, it is much easier to use raw strings to solve the issue, e.g.:\r\n\r\n```\r\n#define ARROW_CXX_COMPILER_FLAGS R\"(@CMAKE_CXX_FLAGS@)\"\r\n```\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"GH-40282: Use C++ type traits","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nUse C++ type traits to avoid redefining in Python\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\nChanged all functions that check types in `type_traits.h` so that they can be used in Python.\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nYes\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40282","comments":[":warning: GitHub issue #40282 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #40282 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #40282 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: Python","awaiting review"]},{"title":"Provide more ways to publish Binary Artifacts","body":"### Describe the enhancement requested\n\nCurrently, `apache.jfrog.io` is the only release channel for various Binary Artifacts of Apache Arrow. Therefore, if `apache.jfrog.io` stops serving, there will be no other channel to obtain Binary Artifacts. However, `apache.jfrog.io` does not provide stable service. The following is a list of multiple service outages in recent years.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/12686\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/34675\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/40744\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/40759\r\n\r\nConsidering that this problem has occurred many times, it has seriously affected various downstream software that relies on Apache Arrow. Whether to consider adding at least one other release channel for the Binary Artifacts of Apache Arrow, such as GitHub Releases or `downloads.apache.org` to avoid `apache.jfrog.io` becoming the only release channel for Binary Artifacts. And provide alternatives for obtaining Binary Artifacts in the installation instructions, even if it may require more troublesome methods such as manual installation of DPKG.\r\n\r\nThe implementation of this measure can effectively reduce risks in the downstream software supply chain and avoid the risk that a single component cannot be installed, resulting in the entire software being unable to be installed, or even the software being unable to continue to work normally.\n\n### Component(s)\n\nRelease","comments":["+1 \nI talked about this with @jbonofre yesterday and he mentioned that while repository.apache.org currently only hosts java binaries the underlying nexus software can host a number of package repos (rpm, deb, python,...).\n\nGitHub releases could certainly also be a fallback for some things but they don't over repo functionality we need for at the minimum the Linux packages.\n\ncc @raulcd @kou ","I think we have different options: nexus, dist, gh. \n\nLet me investigate a bit what could be the easiest one to integrate in our build. "],"labels":["Type: enhancement","Component: Release"]},{"title":"Unable install Apache Arrow using Debian instructions","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI'm building a docker image and it started failing from today with below error when installing deb package\r\n\r\nError:\r\n`[6\/8] RUN  apt install -y -V .\/apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb:                                                  \r\n#9 0.379 \r\n#9 0.379 WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n#9 0.379 \r\n#9 0.430 Reading package lists...\r\n#9 1.761 E: Invalid archive signature\r\n#9 1.761 E: Internal error, could not locate member control.tar{.zst,.lz4,.gz,.xz,.bz2,.lzma,}\r\n#9 1.761 E: Could not read meta data from \/apache-arrow-apt-source-latest-jammy.deb\r\n#9 1.761 E: The package lists or status file could not be parsed or opened.\r\n------\r\nexecutor failed running [\/bin\/sh -c apt install -y -V .\/apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb]: exit code: 100`\r\n\r\n\r\nDockerfile:\r\n`FROM ubuntu\r\nRUN apt update\r\nRUN  apt install -y -V ca-certificates lsb-release wget\r\nRUN wget https:\/\/apache.jfrog.io\/artifactory\/arrow\/$(lsb_release --id --short | tr 'A-Z' 'a-z')\/apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb\r\nRUN  apt install -y -V .\/apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb\r\nRUN  apt update\r\nRUN  apt install -y -V libarrow-dev `\r\n\n\n### Component(s)\n\nC++","comments":["We are having the same problem - it is an issue for all binary artifacts.\r\nLooks like the Apache JFrog account has been disabled [https:\/\/apache.jfrog.io\/artifactory\/arrow\/](https:\/\/apache.jfrog.io\/artifactory\/arrow\/)."],"labels":["Type: bug","Component: C++"]},{"title":"Why is pyarrow.dataset direct from S3 so much slower than using dataset locally and upload\/download separately?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI'm using `pyarrow.dataset.dataset` and `pyarrow.dataset.write_dataset` to convert a newline-delimited (jsonl) file to parquet, and seeing very different end-to-end processing times for the following three approaches:\r\n\r\n1. Let the `dataset` API handle all the filesystem details (223s)\r\n2. Pass `dataset` an `s3fs.S3Filesystem` object (70s)\r\n3. Use `smart_open` to handle download\/upload from\/to S3 and use `dataset` on local filesystem (30s)\r\n\r\nMore detail with code snippets documented in [this StackOverflow question](https:\/\/stackoverflow.com\/questions\/78207687\/pyarrow-dataset-s3-performance-different-with-pyarrow-filesystem-s3fs-indirect).\r\n\r\nFrom previous use of `pyarrow.parquet.ParquetFile` I know that options like `buffer_size` and `pre_buffer` can impact performance and I thought there might be similar options with the `dataset` API but I couldn't find anything in the documentation, would greatly appreciate some insight into this.\n\n### Component(s)\n\nPython","comments":[],"labels":["Component: Python","Type: usage"]},{"title":"[Java] Enable Spotless Plugin","body":"### Describe the enhancement requested\n\nEnabling [Spotless](https:\/\/github.com\/diffplug\/spotless) plugin for Arrow Java is an important step towards code quality. \r\nThere was an existing effort: https:\/\/github.com\/apache\/arrow\/pull\/39713, but the PR is too large to be evaluated and a better approach would be to enable it module by module and it requires some evaluation. \n\n### Component(s)\n\nJava","comments":[],"labels":["Type: enhancement","Component: Java"]},{"title":"[C++] Boost download still fails","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThis is very much ongoing for 15.0.0, 15.0.1, 15.0.2, 16.0.0.dev... from which I derive that either the system that handles hashes is not working or that neither of the download targets is working. I get the same as [benz0li](https:\/\/github.com\/benz0li) for sourceforge and strange behavior with ifrog where t reliably fails from a frankfurt datacenter but seems to work in zurich. I tried boost version 1.81.0 (from source), 1.84.0 (seems to be pretty late) and 1.75.0 (referenced in this bug). So my conclusion is that this is something systemic. Unfortunately the boost version for arrow is a stripped down version (which is awesome to save network bandwidth and download time), so we cannot simply point to a local version--even if that was simple. Any other hacks to try in the meantime?\r\n\r\nSee [_Originally](https:\/\/github.com\/apache\/arrow\/issues\/34675) reference from @benz0li in https:\/\/github.com\/apache\/arrow\/issues\/34675#issuecomment-1479053800_\r\n            \r\n\r\n### Component(s)\r\n\r\nC++\r\n\r\n### Details\r\n\r\nBuilding from `15.0.2` I get the following log:\r\n\r\n```\r\n-- ARROW_BOOST_BUILD_VERSION: 1.81.0\r\n-- ARROW_BOOST_BUILD_SHA256_CHECKSUM: 9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574\r\n```\r\n[...]\r\n```\r\n[ 14%] Built target zstd_ep\r\nCMake Error at boost_ep-stamp\/boost_ep-download-RELEASE.cmake:37 (message):\r\n  Command failed: 1\r\n\r\n   '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/external\/cmake-3.23.2-linux-x86_64\/bin\/cmake' '-Dmake=' '-Dconfig=' '-P' '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-RELEASE-impl.cmake'\r\n\r\n  See also\r\n\r\n    \/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-*.log\r\n\r\n\r\n-- stdout output is:\r\n...skipping to end...\r\nomplete]\r\n-- [download 49% complete]\r\n```\r\n[...]\r\n```\r\n-- [download 100% complete]\r\n-- verifying file...\r\n       file='\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz'\r\n-- SHA256 hash of\r\n    \/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz\r\n  does not match expected value\r\n    expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n      actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n-- Hash mismatch, removing...\r\n-- Using src='https:\/\/apache.jfrog.io\/artifactory\/arrow\/thirdparty\/7.0.0\/boost_1_81_0.tar.gz'\r\n-- [download 100% complete]\r\n-- [download 9% complete]\r\n-- [download 22% complete]\r\n-- [download 34% complete]\r\n-- [download 46% complete]\r\n-- [download 58% complete]\r\n-- [download 70% complete]\r\n-- [download 82% complete]\r\n-- [download 94% complete]\r\n-- [download 100% complete]\r\n-- verifying file...\r\n       file='\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz'\r\n-- SHA256 hash of\r\n    \/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz\r\n  does not match expected value\r\n    expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n      actual: 'f799db17e37a963a08674fe3a565b4acb07681de084d26eb14c305d654caef66'\r\n-- Hash mismatch, removing...\r\n-- Using src='https:\/\/boostorg.jfrog.io\/artifactory\/main\/release\/1.81.0\/source\/boost_1_81_0.tar.gz'\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n```\r\n[...]\r\n```\r\n-- [download 100% complete]\r\n-- verifying file...\r\n       file='\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz'\r\n-- SHA256 hash of\r\n    \/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz\r\n  does not match expected value\r\n    expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n      actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n-- Hash mismatch, removing...\r\n-- Using src='https:\/\/sourceforge.net\/projects\/boost\/files\/boost\/1.81.0\/boost_1_81_0.tar.gz'\r\n-- [download 100% complete]\r\n-- [download 0% complete]\r\n-- [download 1% complete]\r\n```\r\n[...]\r\n```\r\n-- [download 100% complete]\r\n-- verifying file...\r\n       file='\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz'\r\n-- SHA256 hash of\r\n    \/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_1_81_0.tar.gz\r\n  does not match expected value\r\n    expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n      actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n-- Hash mismatch, removing...\r\n\r\n-- stderr output is:\r\nCMake Error at boost_ep-stamp\/download-boost_ep.cmake:170 (message):\r\n  Each download failed!\r\n\r\n    \r\n    \r\n\r\n\r\nCMake Error at boost_ep-stamp\/boost_ep-download-RELEASE-impl.cmake:9 (message):\r\n  Command failed (1):\r\n\r\n   '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/external\/cmake-3.23.2-linux-x86_64\/bin\/cmake' '-P' '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\r\n\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir\/boost_ep-prefix\/src\/boost_ep-stamp\/download-boost_ep.cmake'\r\n\r\n\r\n\r\nCMake Error at boost_ep-stamp\/boost_ep-download-RELEASE.cmake:47 (message):\r\n  Stopping after outputting logs.\r\n\r\n\r\ngmake[2]: *** [CMakeFiles\/boost_ep.dir\/build.make:99: boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download] Error 1\r\ngmake[2]: Leaving directory '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir'\r\ngmake[1]: *** [CMakeFiles\/Makefile2:862: CMakeFiles\/boost_ep.dir\/all] Error 2\r\ngmake[1]: Leaving directory '\/home\/marcus\/.cache\/bazel\/_bazel_marcus\/a6d0f072dd138e2ef57898fd8d473593\/sandbox\/linux-sandbox\/5\/execroot\/__main__\/bazel-out\/k8-opt\/bin\/external\/arrow\/arrow.build_tmpdir'\r\ngmake: *** [Makefile:146: all] Error 2\r\n```\r\n\r\nNote that for one download the hash is different from the others, but none match:\r\n\r\n* First entry (I don't see a source for that in the logs)\r\n   * expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n   * actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n* src='https:\/\/apache.jfrog.io\/artifactory\/arrow\/thirdparty\/7.0.0\/boost_1_81_0.tar.gz'\r\n   * expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n   * actual: 'f799db17e37a963a08674fe3a565b4acb07681de084d26eb14c305d654caef66'\r\n* src='https:\/\/boostorg.jfrog.io\/artifactory\/main\/release\/1.81.0\/source\/boost_1_81_0.tar.gz'\r\n   * expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n   * actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n* src='https:\/\/sourceforge.net\/projects\/boost\/files\/boost\/1.81.0\/boost_1_81_0.tar.gz'\r\n   * expected: '9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574'\r\n   * actual: '205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6'\r\n","comments":["Confirmed for both 15.0.0 and 15.0.2, with this error message:\r\n\r\n```\r\n-- stderr output is:\r\nCMake Error at boost_ep-stamp\/download-boost_ep.cmake:170 (message):\r\n  Each download failed!\r\n\r\n    error: downloading 'https:\/\/apache.jfrog.io\/artifactory\/arrow\/thirdparty\/7.0.0\/boost_1_81_0.tar.gz' failed\r\n          status_code: 22\r\n          status_string: \"HTTP response code said error\"\r\n          log:\r\n          --- LOG BEGIN ---\r\n          timeout on name lookup is not supported\r\n    Trying 18.232.172.199:443...\r\n\r\n  Connected to apache.jfrog.io (18.232.172.199) port 443 (#0)\r\n```","After patching in the reported hash I can build arrow again.\r\n\r\n```\r\ndiff --git a\/cpp\/thirdparty\/versions.txt b\/cpp\/thirdparty\/versions.txt\r\nindex 18bb6c9b6..aebdf28e5 100644\r\n--- a\/cpp\/thirdparty\/versions.txt\r\n+++ b\/cpp\/thirdparty\/versions.txt\r\n@@ -57,7 +57,7 @@ ARROW_AWSSDK_BUILD_SHA256_CHECKSUM=2d552fb1a84bef4a9b65e34aa7031851ed2aef5319e02\r\n ARROW_AZURE_SDK_BUILD_VERSION=azure-core_1.10.3\r\n ARROW_AZURE_SDK_BUILD_SHA256_CHECKSUM=dd624c2f86adf474d2d0a23066be6e27af9cbd7e3f8d9d8fd7bf981e884b7b48\r\n ARROW_BOOST_BUILD_VERSION=1.81.0\r\n-ARROW_BOOST_BUILD_SHA256_CHECKSUM=9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574\r\n+ARROW_BOOST_BUILD_SHA256_CHECKSUM=205666dea9f6a7cfed87c7a6dfbeb52a2c1b9de55712c9c1a87735d7181452b6\r\n ARROW_BROTLI_BUILD_VERSION=v1.0.9\r\n ARROW_BROTLI_BUILD_SHA256_CHECKSUM=f9e8d81d0405ba66d181529af42a3354f838c939095ff99930da6aa9cdf6fe46\r\n ARROW_BZIP2_BUILD_VERSION=1.0.8\r\n```\r\n\r\nI think there are multiple issues here:\r\n* The hash is currently wrong. Though why would it ever change?\r\n* The hash cannot be controlled from the build environment but must rather be patched in.\r\n* The build system takes two separate boost versions (normal and trimmed) into account, but there is only one hash, so only one version can ever be working.\r\n* Source 'https:\/\/apache.jfrog.io\/artifactory\/arrow\/thirdparty\/7.0.0\/boost_1_81_0.tar.gz' is currently broken (hence the different hash)."],"labels":["Type: bug","Component: C++"]},{"title":"[JS] - Decimal conversions are broken","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nConverting a decimal to a number is broken for negatives and those with decimal places.\r\n\r\nI'd like to add support for this.\n\n### Component(s)\n\nJavaScript","comments":[],"labels":["Type: bug","Component: JavaScript"]},{"title":"Cannot read data if endpoint is s3 on a \"secure\" Minio server","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI would like to read a CSV file from my TLS-secured object storage (minio).\r\n\r\nHere is my code and configuration:\r\n\r\n```\r\nfrom pyarrow import fs, csv, parquet\r\nminio = fs.S3FileSystem(\r\n    endpoint_override=\"https:\/\/localhost:port\",\r\n    access_key=\"user1234\",\r\n    secret_key=\"password1234\",\r\n)\r\n\r\ndataCSV = minio.open_input_file(\"bucket\/filename.csv\")\r\n```\r\nI get the following error message:\r\n\r\n```\r\nOSError: When reading information for key 'filename.csv' in bucket 'bucket': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 60, SSL peer certificate or SSH remote key was not OK\r\n```\r\nIf I use the same credentials to read the data with duckDB, it works perfectly.\r\n\r\nWhat am I doing wrong?\n\n### Component(s)\n\nPython","comments":["I tried all parameter I found on the internet to disable certificate validation or to povide a valid certificate to S3FileSystem directly. No success.\r\n\r\nPlease, does have anyone any idea?"],"labels":["Component: Python","Type: usage"]},{"title":"GH-40751: [C++] Fix protobuf package name setting for builds with substrait","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nThe problem #40751 seems to be introduced by #40399. Though I'm not entirely sure about the purpose of that, it seems to be missing an `OR ARROW_SUBSTRAIT` in the `if` branch in https:\/\/github.com\/apache\/arrow\/commit\/5baca0f16e924c42741729f041b31a02883548b9#diff-5cdc95f4e1b618f2f3ef10d370ce05a1ac05d9d401aecff3ccbb3d76bd366b6aR1815\r\n\r\nBecause other than `ARROW_ORC`, `ARROW_WITH_OPENTELEMETRY` and `ARROW_FLIGHT`, `ARROW_SUBSTRAIT` also implies `ARROW_WITH_PROTOBUF`:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/5baca0f16e924c42741729f041b31a02883548b9\/cpp\/cmake_modules\/ThirdpartyToolchain.cmake#L421-L423\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nAdd the possible missing condition of `ARROW_SUBSTRAIT` for the questioning `if` branch.\r\n\r\n### Are these changes tested?\r\n\r\nManually tested.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nNone.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40751","comments":[":warning: GitHub issue #40751 **has been automatically assigned in GitHub** to PR creator.","cc @tobim @kou ","@kou shall we run the CIs for the related job? ","Thank you both for reviewing. Appreciate it. \n\nCould anyone help with the CI failure? Seems unrelated to the change. ","It seems to be related to that issue.\r\n\r\n- https:\/\/github.com\/apache\/arrow\/issues\/40756.\r\n\r\n#### R \/ rhub\/debian-gcc-devel:latest (pull_request) \r\n\r\n- https:\/\/github.com\/apache\/arrow\/actions\/runs\/8397572909\/job\/23001093393?pr=40753\r\n\r\n<details><summary>error message<\/summary>\r\n\r\n```\r\nCMake Error at boost_ep-stamp\/boost_ep-download-RELEASE.cmake:37 (message):\r\n  Command failed: 1\r\n\r\n   '\/usr\/bin\/cmake' '-Dmake=' '-Dconfig=' '-P' '\/tmp\/Rtmp90Fqh8\/file738ab7a567\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-RELEASE-impl.cmake'\r\n\r\n  See also\r\n\r\n    \/tmp\/Rtmp90Fqh8\/file738ab7a567\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-*.log\r\n\r\n\r\n-- stdout output is:\r\n...skipping to end...\r\n\/boost_1_81_0.tar.gz'\r\n\r\n```\r\n\r\n<\/details>\r\n\r\n#### C++ \/ AMD64 Windows 2019 C++17 AVX2 (pull_request)\r\n\r\n- https:\/\/github.com\/apache\/arrow\/actions\/runs\/8397572916\/job\/23001094324?pr=40753\r\n\r\n<details><summary>error message<\/summary>\r\n\r\n```\r\nFAILED: boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download D:\/a\/arrow\/arrow\/build\/cpp\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download \r\nC:\\Windows\\system32\\cmd.exe \/C \"cd \/D D:\\a\\arrow\\arrow\\build\\cpp\\boost_ep-prefix\\src && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -P D:\/a\/arrow\/arrow\/build\/cpp\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-DEBUG.cmake && \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E touch D:\/a\/arrow\/arrow\/build\/cpp\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download\"\r\nCMake Error at boost_ep-stamp\/boost_ep-download-DEBUG.cmake:37 (message):\r\n  Command failed: 1\r\n\r\n   'C:\/Program Files\/CMake\/bin\/cmake.exe' '-Dmake=' '-Dconfig=' '-P' 'D:\/a\/arrow\/arrow\/build\/cpp\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-DEBUG-impl.cmake'\r\n\r\n  See also\r\n\r\n    D:\/a\/arrow\/arrow\/build\/cpp\/boost_ep-prefix\/src\/boost_ep-stamp\/boost_ep-download-*.log\r\n\r\n\r\n-- stdout output is:\r\n...skipping to end...\r\nprojects\/boost\/files\/boost\/1.81.0\/boost_1_81_0.tar.gz'\r\n```\r\n\r\n<\/details>","In my opinion, the rest of the errors do not seem to be caused by the changes in that code.\r\n\r\n#### R \/ Check minimum supported Arrow C++ Version (13.0.0) (pull_request)\r\n\r\n- https:\/\/github.com\/apache\/arrow\/actions\/runs\/8397572909\/job\/23001093923?pr=40753\r\n\r\n<details><summary>error message<\/summary>\r\n\r\n```\r\n--2024-03-22 23:28:28--  https:\/\/apache.jfrog.io\/artifactory\/arrow\/ubuntu\/apache-arrow-apt-source-latest-jammy.deb\r\nResolving apache.jfrog.io (apache.jfrog.io)... 18.214.194.113, 18.232.172.199, 3.95.117.170\r\nConnecting to apache.jfrog.io (apache.jfrog.io)|18.214.194.113|:443... connected.\r\nHTTP request sent, awaiting response... 302 Moved Temporarily\r\nLocation: https:\/\/landing.jfrog.com\/reactivate-server\/apache [following]\r\n--2024-03-22 23:28:28--  https:\/\/landing.jfrog.com\/reactivate-server\/apache\r\nResolving landing.jfrog.com (landing.jfrog.com)... 18.214.194.113, 18.232.172.199, 3.95.117.170\r\nConnecting to landing.jfrog.com (landing.jfrog.com)|18.214.194.113|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 11533 (11K) [text\/html]\r\nSaving to: \u2018apache-arrow-apt-source-latest-jammy.deb\u2019\r\n\r\n     0K .......... .                                          100% 4.20M=0.003s\r\n\r\n2024-03-22 23:28:28 (4.20 MB\/s) - \u2018apache-arrow-apt-source-latest-jammy.deb\u2019 saved [11533\/11533]\r\n\r\n\r\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n\r\nReading package lists...\r\nE: Invalid archive signature\r\nE: Internal error, could not locate member control.tar{.zst,.lz4,.gz,.xz,.bz2,.lzma,}\r\nE: Could not read meta data from \/home\/runner\/work\/arrow\/arrow\/apache-arrow-apt-source-latest-jammy.deb\r\nE: The package lists or status file could not be parsed or opened.\r\nError: Process completed with exit code 100.\r\n\r\n```\r\n\r\n<\/details>\r\n","> In my opinion, the rest of the errors do not seem to be caused by the changes in that code.\r\n> \r\n> #### R \/ Check minimum supported Arrow C++ Version (13.0.0) (pull_request)\r\n> * [apache\/arrow\/actions\/runs\/8397572909\/job\/23001093923?pr=40753](https:\/\/github.com\/apache\/arrow\/actions\/runs\/8397572909\/job\/23001093923?pr=40753)\r\n> \r\n> error message\r\n> ```\r\n> --2024-03-22 23:28:28--  https:\/\/apache.jfrog.io\/artifactory\/arrow\/ubuntu\/apache-arrow-apt-source-latest-jammy.deb\r\n> Resolving apache.jfrog.io (apache.jfrog.io)... 18.214.194.113, 18.232.172.199, 3.95.117.170\r\n> Connecting to apache.jfrog.io (apache.jfrog.io)|18.214.194.113|:443... connected.\r\n> HTTP request sent, awaiting response... 302 Moved Temporarily\r\n> Location: https:\/\/landing.jfrog.com\/reactivate-server\/apache [following]\r\n> --2024-03-22 23:28:28--  https:\/\/landing.jfrog.com\/reactivate-server\/apache\r\n> Resolving landing.jfrog.com (landing.jfrog.com)... 18.214.194.113, 18.232.172.199, 3.95.117.170\r\n> Connecting to landing.jfrog.com (landing.jfrog.com)|18.214.194.113|:443... connected.\r\n> HTTP request sent, awaiting response... 200 OK\r\n> Length: 11533 (11K) [text\/html]\r\n> Saving to: \u2018apache-arrow-apt-source-latest-jammy.deb\u2019\r\n> \r\n>      0K .......... .                                          100% 4.20M=0.003s\r\n> \r\n> 2024-03-22 23:28:28 (4.20 MB\/s) - \u2018apache-arrow-apt-source-latest-jammy.deb\u2019 saved [11533\/11533]\r\n> \r\n> \r\n> WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n> \r\n> Reading package lists...\r\n> E: Invalid archive signature\r\n> E: Internal error, could not locate member control.tar{.zst,.lz4,.gz,.xz,.bz2,.lzma,}\r\n> E: Could not read meta data from \/home\/runner\/work\/arrow\/arrow\/apache-arrow-apt-source-latest-jammy.deb\r\n> E: The package lists or status file could not be parsed or opened.\r\n> Error: Process completed with exit code 100.\r\n> ```\r\n\r\nSeems to be the same as #40759 and #40744."],"labels":["Component: C++","awaiting merge"]},{"title":"[C++] CMake error for ninja-debug preset","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI'm building arrow cpp using CMake preset `ninja-debug`, with system provided protobuf:\r\n`cmake ..\/arrow\/cpp --preset=ninja-debug`\r\n\r\nAnd got error message:\r\n```\r\nCMake Error at cmake_modules\/ThirdpartyToolchain.cmake:1823 (message):\r\n  ARROW_WITH_PROTOBUF must be propagated in the build tooling installation.\r\n  Please extend the mappings of ARROW_PROTOBUF_ARROW_CMAKE_PACKAGE_NAME and\r\n  ARROW_PROTOBUF_ARROW_PC_PACKAGE_NAME for newly introduced dependencies on\r\n  protobuf.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:543 (include)\r\n```\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"[Python] Getting \"List child array invalid\" when calling MapArray.from_arrays with offsetted offsets","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI'm trying to call `pa.MapArray.from_arrays`, in a simple example where I create a MapArray of empty maps:\r\n```\r\n    offsets = pa.array([0, 0, 0, 0, 0, 0], pa.int32())\r\n\r\n    map_array = pa.MapArray.from_arrays(\r\n        offsets,\r\n        pa.array([], pa.string()),\r\n        pa.array([], pa.string()),\r\n    )\r\n    assert len(map_array) == 5\r\n    assert map_array.to_pylist() == [[]] * 5\r\n```\r\n\r\nIt works fine but if I try the same thing with an offsetted view of `offsets` (aka a slice), it fails:\r\n\r\n```\r\n    with pytest.raises(\r\n        pa.ArrowInvalid,\r\n        match=r\"List child array invalid: Invalid: Struct child array #0 has length smaller than expected for struct array \\(0 < 1\\)\",\r\n    ):\r\n        pa.MapArray.from_arrays(\r\n            offsets[1:],\r\n            pa.array([], pa.string()),\r\n            pa.array([], pa.string()),\r\n        )\r\n\r\n```\r\n\r\nFor now as a work around I copy the data:\r\n```\r\n    offset_shift = offsets[1:]\r\n    offset_shift_copy = pa.array(\r\n       offset.to_pylist(),\r\n        offset_shift.type,\r\n    )\r\n    map_array = pa.MapArray.from_arrays(\r\n        offset_shift_copy,\r\n        pa.array([], pa.string()),\r\n        pa.array([], pa.string()),\r\n    )\r\n    assert len(map_array) == 4\r\n    assert map_array.to_pylist() == [[]] * 4\r\n```\r\n\r\nMy intuition is that the `offset` isn't interpreted correctly in the underlying code, but I can't really tell where. \r\n\r\nFor context, this is happening when I try to do my own cast of a chunked MapArray. I have to iterate through the chunks and change slightly the underlying value array, and this happens.\r\n\r\nTested with pyarrow==15.0.2 \/ Python 3.11.8\r\n\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[Python][Packaging] Strip unnecessary symbols from libarrow.so to reduce wheel package size","body":"### Describe the enhancement requested\r\n\r\nThere has been some effort in order to reduce pyarrow size and there are some issues opened in order to split pyarrow wheels in order to have `pyarrow-core`, `pyarrow-all`, etcetera.\r\n\r\nThere seems to be the possibility to strip unnecessary symbols via `strip --discard-all`. The `libarrow.so` file seems to be reduced from 61MB to 45 MB.\r\n\r\nAnother example:\r\n```\r\n-rwxrwxr-x 1 user group  49M Feb 22 18:55 libarrow.so.1600.0.0\r\n\r\n$ strip --strip-unneeded libarrow.so.1600.0.0\r\n\r\n-rwxrwxr-x 1 user group  33M Mar 12 15:27 libarrow.so.1600.0.0\r\n```\r\nThis issue is to investigate the possibility of using strip and the possible disadvantages.\r\n\r\n_edited to fix typo_\r\n\r\n### Component(s)\r\n\r\nPackaging, Python","comments":[],"labels":["Type: enhancement","Component: Python","Component: Packaging"]},{"title":"[JS] Fix instanceof or move away from instanceof within arrow-js","body":"### Describe the enhancement requested\r\n\r\nThere are a number of places that arrow's js package uses `instanceof`.  For example:\r\n\r\n```\r\n\/\/ from table.ts in the Table constructor\r\nif (args[0] instanceof Schema) {\r\n    schema = args.shift() as Schema<T>;\r\n}\r\n```\r\n\r\nUnfortunately, this is quite brittle.  This will fail whenever a library ends up with multiple instances of the arrow library.  For example:\r\n\r\n * LanceDb currently uses arrow version 14.  If the calling user is using any other version of arrow then node will happily install two versions of arrow and the checks will fail. (also, these failures tend to be very opaque)\r\n * Even if the user is using the exact same version of arrow there are cases where two instances of arrow are created.  For example, our users using vite have run into https:\/\/github.com\/vitejs\/vite\/issues\/3910\r\n\r\nI would argue that some of the core types (at least `Schema` but maybe also `Table` \/ `Vector` \/ `RecordBatch` too) should be considered stable and users should be allowed to have different versions \/ instances.  We tried working around this in lancedb by declaring arrow a peer dependency and that solves the first bullet above but it's also somewhat unconventional (for javascript) and inconvenient for our users (they are forced to use whatever version of arrow we are using and when we change our arrow version it is a breaking change).\r\n\r\nThis is not critical, we've worked around this by re-building the schema if the input from our user looks like a schema, but I wanted to see if other JS users\/maintainers felt similarly and, if so, I might try and find some time to explore a fix.\r\n\r\nSome potential fixes:\r\n\r\n * Add methods like `isArrowSchema` and use those methods instead of `instanceof`\r\n * Customize the definition of `instanceof` by implementing https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Symbol\/hasInstance on types like `Schema`.\r\n * Stick with the current but upstream methods like https:\/\/github.com\/lancedb\/lancedb\/blob\/dfc518b8fbc5920cf7dafd072335257be1e5d9c5\/nodejs\/lancedb\/sanitize.ts#L491 so that this kind of \"schema rebuild\" is maintained here in arrow.\r\n\r\n### Component(s)\r\n\r\nJavaScript","comments":[],"labels":["Type: enhancement","Component: JavaScript"]},{"title":"Parquet export doesn't handle interval types","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nC++ Parquet export doesn't handle ::arrow::month_interval and ::arrow::day_time_interval . Attempting to export interval values return a status of `Unhandled type for Arrow to Parquet schema conversion: month_interval`. The status is generated in https:\/\/github.com\/apache\/arrow\/blob\/2babda0ba22740c092166b5c5d5d7aa9b4797953\/cpp\/src\/parquet\/arrow\/schema.cc#L441-L445 , but the comment doesn't call out the interval types.\r\n\r\nIdeally, these types get converted to Parquet INTERVAL type: https:\/\/github.com\/apache\/parquet-format\/blob\/master\/LogicalTypes.md#interval .\r\n\n\n### Component(s)\n\nParquet","comments":[],"labels":["Type: bug","Component: Parquet"]},{"title":"[Java][FlightRPC] Increase the default backpressure threshold in Java Arrow Flight servers","body":"### Describe the enhancement requested\n\ngrpc-java recently added a toggle for changing the buffering threshold before backpressure is triggered (https:\/\/github.com\/grpc\/grpc-java\/issues\/5433).\r\n\r\nThe default is 32KB. For Arrow Flight loads, this should be set higher: 1-10MB. There FlightServer.Builder should also let users specify the threshold.\r\n\r\nDocumentation should recommend users not use complicated backpressure strategies after making this change.\n\n### Component(s)\n\nFlightRPC, Java","comments":["@jduo planning on a PR for this? "],"labels":["Type: enhancement","Component: Java","Component: FlightRPC"]},{"title":"[Packaging] `apache.jfrog.io\/artifactory\/arrow\/ubuntu`  likely down?","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nSeeing a bunch of these in our CI\r\n\r\n```\r\nErr:6 https:\/\/apache.jfrog.io\/artifactory\/arrow\/ubuntu focal InRelease\r\n  Clearsigned file isn't valid, got 'NOSPLIT' (does the network require authentication?)\r\nHit:8 http:\/\/ppa.launchpad.net\/ubuntu-toolchain-r\/test\/ubuntu focal InRelease\r\nReading package lists...\r\nE: Failed to fetch https:\/\/apache.jfrog.io\/artifactory\/arrow\/ubuntu\/dists\/focal\/InRelease  Clearsigned file isn't valid, got 'NOSPLIT' (does the network require authentication?)\r\nE: The repository 'https:\/\/apache.jfrog.io\/artifactory\/arrow\/ubuntu focal InRelease' is not signed.\r\n```\r\n\r\nLooks like issue as the one here (https:\/\/github.com\/apache\/arrow\/issues\/34680#issuecomment-1479045108), same time last year (~March 22 2023)\n\n### Component(s)\n\nOther","comments":["Hey, yeah it's the same issue sadly. I have already contacted ASF INFRA and they are in contact with jfrog to get this up again asap.","Thank you! Appreciate it"],"labels":["Type: bug","Component: Other"]},{"title":"pyarrow\/pandas interop inconsistent when constructing empty dataframes with non-empty range indices","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```\r\npa.__version__ => 15.0.2\r\npandas.__version__ => 3.0.0.dev0+584.g41383cf140\r\n```\r\n\r\nConsider:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nfrom pandas.testing import assert_frame_equal\r\nfrom io import BytesIO\r\n\r\ndata = pd.DataFrame(index=pd.RangeIndex(0, 10), data=None)\r\n\r\ntab = pa.table(data)\r\n\r\nbuf = BytesIO()\r\n\r\npa.parquet.write_table(tab, buf)\r\n\r\ngot = pa.parquet.read_table(buf)\r\n\r\nassert tab.shape == got.shape # False\r\n\r\n# and hence\r\nexpect = data\r\n\r\ngot_tab = tab.to_pandas()\r\n\r\nassert_frame_equal(expect, got_tab) # True\r\n\r\ngot_pq = got.to_pandas()\r\nassert_frame_equal(expect, got_pq) # False\r\n\r\n# and hence\r\nbuf = BytesIO()\r\ndata.to_parquet(buf, index=None)\r\ngot = pd.read_parquet(buf)\r\n\r\nassert_frame_equal(data, got) # False\r\n```\r\n\r\nThe round-trip through `pa.table` was fixed in #26599.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[R] fix max_rows_per_group must be a positive number","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```r\r\narrow::write_dataset(dataset = x, path = \"...\", partitioning = somekeys,\r\n                         max_rows_per_file = 0L)\r\n# Error: Invalid: max_rows_per_group must be a positive number\r\n```\r\n\r\nThe document says \r\n\r\n```\r\nmax_rows_per_file: maximum number of rows per file. If greater than 0\r\n          then this will limit how many rows are placed in any single\r\n          file. Default is 0L.\r\n\r\nmin_rows_per_group: write the row groups to the disk when this number\r\n          of rows have accumulated. Default is 0L.\r\n```\r\n\r\nTwo things:\r\n\r\n- the error references an argument that was not provided\r\n- either update the documentation or accept `0` as an acceptable argument (that just reasserts the behavior of \"no limit\")\r\n\n\n### Component(s)\n\nR","comments":["Thanks for reporting this @r2evans, can confirm I can reproduce this.  The error is being propagated from the C++ layer, and I think it might be related to #39995, but need to double-check."],"labels":["Type: bug","Component: R"]},{"title":"[C++] Add support for precision timestamp literals","body":"### Describe the enhancement requested\n\nSubstrait added new precision timestamp types to replace the old (micros only) timestamp types.  #40695 adds support for converting the types but we are unable to convert the literals due to https:\/\/github.com\/substrait-io\/substrait\/issues\/611\r\n\r\nOnce that issue is resolved we can upgrade and add support for using the precision timestamp literals.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[C++] Add Substrait support for arrow-specific types (paramaeterized)","body":"### Describe the enhancement requested\n\nThis request extends #40695 to also add support for parameterized arrow-specific types such as decimal256, fixed_size_list, and large_list.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40623: [Python][Docs] Add workaround for autosummary","body":"### Rationale for this change\r\n\r\nSphinx < 7.3.0 has a problem that auto generated files may use wrong suffix.\r\n\r\nSee https:\/\/github.com\/sphinx-doc\/sphinx\/issues\/12147 for details.\r\n\r\n### What changes are included in this PR?\r\n\r\nUse `.rst` as the first `source_suffix` item as a workaround of this problem.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes.\n* GitHub Issue: #40623","comments":["@github-actions crossbow submit preview-docs",":warning: GitHub issue #40623 **has been automatically assigned in GitHub** to PR creator.","Revision: ff40db90d15e8019662efe1443b65caa663f5cee\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-6d9e322d6f](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-6d9e322d6f)\n\n|Task|Status|\n|----|------|\n|preview-docs|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-6d9e322d6f-github-preview-docs)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8389220323\/job\/22975003850)|","Generated.\r\n\r\nhttp:\/\/crossbow.voltrondata.com\/pr_docs\/40739\/python\/generated\/pyarrow.Schema.html\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/27350\/ac64c673-74d8-4003-8a44-eceb44acaf9e)\r\n"],"labels":["Component: Documentation","awaiting committer review"]},{"title":"Memory leak still showed on parquet.write_table and Table.from_pandas","body":"Hi,\r\n\r\nI tried to save Pandas dataframe to parquet files, and encountered a memory leak issue. Even i have installed nightly build pyarrow 16.0.0.dev356 from the server, as the comment mentioned this issue is fixed from https:\/\/github.com\/apache\/arrow\/issues\/37989\r\n\r\nAny idea? \r\n\r\nhere is the memory usage by using memory profiler. \r\n\r\nLine #    Mem usage    Increment  Occurrences   Line Contents\r\n=============================================================\r\n    33    425.8 MiB    425.8 MiB           1           @profile\r\n    34                                                 def to_parquet(self, df: pd.DataFrame, filename: str):\r\n    35    537.6 MiB    111.9 MiB           1               table = Table.from_pandas(df)\r\n    36    559.1 MiB     21.4 MiB           1               parquet.write_table(table, filename, compression=\"snappy\")\r\n    37    559.1 MiB      0.0 MiB           1               del table\r\n    38                                                     #df.to_parquet(filename, compression=\"snappy\")\r\n\r\nMy method\r\n```\r\nimport pandas as pd\r\n\r\nfrom memory_profiler import profile\r\nfrom pyarrow import parquet\r\nfrom pyarrow import Table\r\n\r\n@profile\r\ndef to_parquet(self, df: pd.DataFrame, filename: str):\r\n    table = Table.from_pandas(df)\r\n    parquet.write_table(table, filename, compression=\"snappy\")\r\n    del table\r\n    #df.to_parquet(filename, compression=\"snappy\")\r\n```\r\n\r\nMy related installed packages in the docker:\r\nnumpy                     1.22.4\r\npandas                    2.1.4\r\npyarrow                   16.0.0.dev356\r\npyarrow-hotfix            0.6  --> from dask\r\ndask                      2024.2.1\r\n\r\nOS: Ubuntu 22.04\r\n\r\n\r\n### Component(s)\r\n\r\nParquet, Python","comments":["Hi @guozhans .\r\nHave you tried to release the memory pool? [pyarrow.MemoryPool](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.MemoryPool.html#pyarrow.MemoryPool.release_unused)\r\n\r\nI encountered a similar issue:\r\nEvery time I used **pyarrow.parquet.ParquetDataset** to load parquet from S3, the memory usage continued to increase and cannot be released, so I used **release_unused** after the I\/O operations:\r\n```python\r\nimport pyarrow as pa\r\n\r\npool = pa.default_memory_pool()\r\n# ...\r\npool.release_unused()\r\n```\r\nHowever, the occupied memory cannot be released immediately until I executed next time. On the other hand, it\u2019s not sure how much memory can be released.\r\n"],"labels":["Type: bug","Component: Parquet","Component: Python"]},{"title":"[Go] Need help on reading parquet from S3","body":"Hi team,\r\n\r\nI would like to read a parquet file from S3 with high performance. Is there any hit or an example for me to start with? I have some ideas , but not sure which one is recommended or any better solutions?\r\n\r\nOne approach is to write a customized reader (internally it's leveraging S3 API to fetch a range of bytes) and passed it to function  `file.NewParquetReader()`. \r\n\r\nAnother approach is to send S3 API to fetch the last 8 bytes of parquet file to get the footer, metadata first, and then send S3 APIs to read each row group to get data using `file.NewPageReader()`.\r\n\r\n### Component(s)\r\n\r\nGo","comments":["Personally, I would use https:\/\/github.com\/wolfeidau\/s3iofs to open the file which will internally leverage the s3 API to fetch the byte ranges and just pass it to `file.NewParquetReader` like you suggested. \r\n\r\nI would only go down to creating your own page readers if you find the above isn't performant enough. It's unlikely that going down to that level would provide much in the way of performance gains. ","Thanks @zeroshade for the quick reply.\r\n\r\nJust tried the s3iofs file reader ,  140MB file takes 12 mins to read VS if reading from local , it's ~ 14s or less.  It's expected to see slowness when reading from S3, but 12 mins is too long for our application. I have to check if there is other way to improve the performance. ","12 minutes seems *really* bad, much worse than I'd expect. I've definitely seen better performance from S3 than that in the past, so I wonder where that time is being spent"],"labels":["Component: Go","Type: usage"]},{"title":"GH-40674: [GLib] Don't assume gint64 and int64_t use the same type","body":"### Rationale for this change\r\n\r\nGLib doesn't guarantee that `gint64` and `int64_t` use the same type:\r\n\r\nhttps:\/\/docs.gtk.org\/glib\/types.html#gint64\r\n\r\n> Note that on platforms with more than one 64-bit standard integer\r\n> type, gint64 and int64_t are not necessarily implemented by the same\r\n> 64-bit integer type. For example, on a platform where both long and\r\n> long long are 64-bit, it might be the case that one of those types is\r\n> used for gint64 and the other is used for int64_t.\r\n\r\n### What changes are included in this PR?\r\n\r\nAdd explicit casts.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\n* GitHub Issue: #40674","comments":["@github-actions crossbow submit homebrew-cpp",":warning: GitHub issue #40674 **has been automatically assigned in GitHub** to PR creator.","Revision: f5658711846ae17cb20b6cdf6ea5f7ec05d48f17\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-b669a5bcbf](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-b669a5bcbf)\n\n|Task|Status|\n|----|------|\n|homebrew-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b669a5bcbf-github-homebrew-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8386187592\/job\/22966186572)|","@github-actions crossbow submit homebrew-cpp","Revision: 2209e7e0058342240f64d358756b6cffaa6c8d26\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-312a01d850](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-312a01d850)\n\n|Task|Status|\n|----|------|\n|homebrew-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-312a01d850-github-homebrew-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8389160322\/job\/22974813668)|","@github-actions crossbow submit homebrew-cpp","Revision: 416ece05659b92cb5ac91c8ea609ee408f703f89\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-919a68a959](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-919a68a959)\n\n|Task|Status|\n|----|------|\n|homebrew-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-919a68a959-github-homebrew-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8390373893\/job\/22978467705)|"],"labels":["Component: C++ - Gandiva","Component: GLib","awaiting committer review"]},{"title":"[Packaging][CentOS] Drop support for 7","body":"### Describe the enhancement requested\n\nBecause CentOS 7 will reach EOL on 2024-06-30: https:\/\/www.centos.org\/download\/\r\n\r\n> End-of-life\r\n> 2024-06-30 (end of [RHEL 7 Maintenance Support 2 Phase](https:\/\/access.redhat.com\/support\/policy\/updates\/errata#Life_Cycle_Dates))\r\n\r\nWe can drop support for CentOS 7 after we release 16.0.0 because 17.0.0 will be released after 2024-06-30.\n\n### Component(s)\n\nPackaging","comments":["A word of caution: conda-forge still uses CentOS as the base image to built binaries. We're in the progress to move to a newer base image but as we need an alternative to CentOS now, it takes a bit more time. Progress here: https:\/\/github.com\/conda-forge\/conda-forge.github.io\/issues\/1941\r\n\r\nThe relevant part is probably already tested by the crossbow conda jobs, though.","Thanks for the info!\r\n\r\nI'll remove only https:\/\/github.com\/apache\/arrow\/tree\/main\/dev\/tasks\/linux-packages\/apache-arrow\/yum\/centos-7 and related files for this issue. So conda will not be affected by this.\r\n"],"labels":["Type: enhancement","Component: Packaging"]},{"title":"[Packaging][Debian] Drop support for bullseye","body":"### Describe the enhancement requested\n\nDebian GNU\/Linux bullseye will reach EOL on 2024-07: https:\/\/wiki.debian.org\/DebianReleases\r\n\r\nWe can drop support for it after we release 16.0.0 because 17.0.0 will be released after 2024-07.\n\n### Component(s)\n\nPackaging","comments":[],"labels":["Type: enhancement","Component: Packaging"]},{"title":"[Go] Require Go 1.21 or later","body":"### Describe the enhancement requested\n\nBecause Go 1.20 or earlier reached EOL.\n\n### Component(s)\n\nGo","comments":[],"labels":["Type: enhancement","Component: Go"]},{"title":"GH-40731: [C++][Parquet] Minor enhancement code of encryption","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n* Applying more `std::move` in encryption\r\n* Trying to use `NULLPTR` rather than `NULL`.\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nApplying more `std::move` in encryption\r\n\r\n### Are these changes tested?\r\n\r\nNo need\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40731","comments":[":warning: GitHub issue #40731 **has been automatically assigned in GitHub** to PR creator.","Also cc @pitrou "],"labels":["Component: Parquet","Component: C++","awaiting committer review"]},{"title":"[C++][Parquet] Minor enhancement for encryption","body":"### Describe the enhancement requested\n\nApplying some minor code enhancement\n\n### Component(s)\n\nC++, Parquet","comments":[],"labels":["Type: enhancement","Component: Parquet","Component: C++"]},{"title":"GH-39131: [JS] Add at() for array like types","body":"Simpler version of https:\/\/github.com\/apache\/arrow\/pull\/40712 that preserves `get`. \n* GitHub Issue: #39131","comments":[],"labels":["Component: JavaScript","awaiting committer review"]},{"title":"GH-40755: [JS] fix decimal conversions","body":"### Rationale for this change\r\n\r\nFixes  https:\/\/github.com\/apache\/arrow\/issues\/40755\r\n\r\nFurther work is required to complete:\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/37920\r\n\r\nDecimals are broken - need a correct way to convert decimals to numbers in js\r\n\r\nAlso included an option to include a denominator (BigInt(1\/scale)) as scale is part of the metadata\r\n\r\n### What changes are included in this PR?\r\n\r\nSubmitting a correct way to convert decimals to numbers\r\n\r\n### Are these changes tested?\r\nYes, includes a test\r\n\r\n### Are there any user-facing changes?\r\nNo\r\n\r\n **This PR contains a \"Critical Fix\".**\r\n* GitHub Issue: #37920\n* GitHub Issue: #40755","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","Thanks for the pull request.\r\n\r\nThis does not yet fix encoding so we cannot close https:\/\/github.com\/apache\/arrow\/issues\/37920.\r\n\r\n```ts\r\narrow.vectorFromArray([1, 12, 34], new arrow.Decimal(3, 18))\r\n```\r\n\r\nAlso, it seems like this still ignores the scale. If I try the example in https:\/\/github.com\/apache\/arrow\/issues\/37920#issue-1916512787, I still get 1000 instead of 1 (the scale is ignored).\r\n\r\n```ts\r\nconst hex = 'FFFFFFFF780000001000000000000A000C000600050008000A000000000104000C000000080008000000040008000000040000000100000014000000100014000800060007000C00000010001000000000000107100000001C0000000400000000000000010000006400000008000C0004000800080000001200000003000000FFFFFFFF8800000014000000000000000C0016000600050008000C000C0000000003040018000000300000000000000000000A0018000C00040008000A0000003C00000010000000030000000000000000000000020000000000000000000000000000000000000000000000000000003000000000000000000000000100000003000000000000000000000000000000E8030000000000000000000000000000E02E0000000000000000000000000000D0840000000000000000000000000000FFFFFFFF00000000';\r\nconst bytes = Uint8Array.from(hex.match(\/.{1,2}\/g).map(s => parseInt(s, 16)));\r\nconst vec = arrow.tableFromIPC(bytes).getChild('d')!;\r\n\r\nconsole.log(vec.get(0))         \/\/ [1000, 0, 0, 0] <- byte array looks correct\r\nconsole.log(`${vec.get(0)}`)    \/\/ \"1000\" <- incorrect, should be 1; ignores scale\r\nconsole.log(Number(vec.get(0))) \/\/ 1000 <- incorrect, should be 1\r\n```","`vec.get(0).valueOf(1000n)` return the correct value but it would be nice to pass the scale though. It's in the type and we have access to that in the getter but will need to pass it to BigNum. ","Are the tests off? I\u2019ll take a look soon.\r\n\r\nI made it so you pass the denominator through instead of scale. The reason\r\nfor this is that with 10^9 numbers to convert you don\u2019t need to do a scale\r\nto denominator conversion for each number. Passing the denominator as a\r\nbigint instead will make the function much faster.\r\n\r\n\r\nOn Thu, Mar 21, 2024 at 19:27 Dominik Moritz ***@***.***>\r\nwrote:\r\n\r\n> vec.get(0).valueOf(1000n) return the correct value but it would be nice\r\n> to pass the scale though. It's in the type but I don't know what is the\r\n> best way to get that. This otherwise looks correct.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#issuecomment-2014197628>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAFZTGAX2QDJV7ECPJO3IW3YZOJHNAVCNFSM6AAAAABFCPNKICVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJUGE4TONRSHA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n","Thanks for looking at the tests. \r\n\r\nI see. How do you imagine someone using `get` then? We should probably do the correct conversion there. Maybe we can cache the scale to denominator conversion somewhere if you're worried about performance. ","I think the best way forward is to get this pull request in but make a new issue that this pull request is attached to. We probably need to refactor the whole decimal handling in ways that are orthogonal to this fix. How does that sound @dioptre? ","I was thinking the same. Perhaps we go for the scale first if you want to\r\nkeep it in that format for the future and we can optimize later.\r\n\r\nIf you are happy with that I can change denominator to scale and we can\r\nlook at cache later.\r\n\r\nOn Fri, Mar 22, 2024 at 06:32 Dominik Moritz ***@***.***>\r\nwrote:\r\n\r\n> I think the best way forward is to get this pull request in but make a new\r\n> issue that this pull request is attached to. We probably need to refactor\r\n> the whole decimal handling in ways that are orthogonal to this fix. How\r\n> does that sound @dioptre <https:\/\/github.com\/dioptre>?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#issuecomment-2015106723>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAFZTGCLRZM7FC46J25DUFLYZQXFVAVCNFSM6AAAAABFCPNKICVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJVGEYDMNZSGM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n","Let's change to scale and make a new issue. Then we can merge. ","Updated the PR, tests look ok?","Ah your [suggestion broke it](https:\/\/github.com\/apache\/arrow\/pull\/40729\/commits\/cff7709d63f03f497dbdc3e97a6fcbd16a1d2f62) so I reverted. I also made it a function per your request.\r\n\r\nIt was my understanding that the scale is included in the metadata, and there's no real metadata in the arrow bytes. This confused me at first as I was looking for it. So seems we need the valueOf(scale)?\r\n\r\n> console.log(vec.get(0))         \/\/ [1000, 0, 0, 0] <- byte array looks correct\r\n> console.log(`${vec.get(0)}`)    \/\/ \"1000\" <- incorrect, should be 1; ignores scale\r\n> console.log(Number(vec.get(0))) \/\/ 1000 <- incorrect, should be 1\r\n> ```\r\n\r\n","> Ah your [suggestion broke it](https:\/\/github.com\/apache\/arrow\/pull\/40729\/commits\/cff7709d63f03f497dbdc3e97a6fcbd16a1d2f62) so I reverted.\r\n\r\nWhat did it break?","Also, please make a new issue that this pull request refers to since it doesn't address the referenced issue fully yet. ","It wouldn't build, we had the same issue on our older code (pre es2020) -\r\nthink ** and bigint support is no bueno for older builds\r\n\r\nOn Fri, Mar 22, 2024 at 1:45\u202fPM Dominik Moritz ***@***.***>\r\nwrote:\r\n\r\n> Ah your suggestion broke it\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729\/commits\/cff7709d63f03f497dbdc3e97a6fcbd16a1d2f62>\r\n> so I reverted.\r\n>\r\n> What did it break?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#issuecomment-2015880468>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAFZTGH5XXQHASPVBEOS3JTYZSJ6TAVCNFSM6AAAAABFCPNKICVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJVHA4DANBWHA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n","Updated the fix issue above, kept a reference ^^^","I fixed the title of this pull request. Now we just need the integration tests to pass. ",":warning: GitHub issue #40755 **has been automatically assigned in GitHub** to PR creator.","Think should be good to run again","I took the literal definition bignum to number?\r\n\r\n\r\nOn Fri, Mar 22, 2024 at 21:14 Dominik Moritz ***@***.***>\r\nwrote:\r\n\r\n> ***@***.**** commented on this pull request.\r\n> ------------------------------\r\n>\r\n> In js\/src\/util\/bn.ts\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#discussion_r1536556864>:\r\n>\r\n> >      }\r\n> -    return number;\r\n> +    return Number(number);\r\n>\r\n> Isn't this incorrect when we have larger numbers?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#pullrequestreview-1956298728>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAFZTGGRD74D7ATOE5L72U3YZT6QVAVCNFSM6AAAAABFCPNKICVHI2DSMVQWIX3LMV43YUDVNRWFEZLROVSXG5CSMV3GSZLXHMYTSNJWGI4TQNZSHA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n","Fair. We're need to refactor how all of this is used for decimals a bit anyway so lets leave it. ","Sounds good. Let me know if there\u2019s anything else I can help with.\r\n\r\nOn Sat, Mar 23, 2024 at 05:53 Dominik Moritz ***@***.***>\r\nwrote:\r\n\r\n> Fair. We're need to refactor how all of this is used for decimals a bit\r\n> anyway so lets leave it.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/pull\/40729#issuecomment-2016486277>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAFZTGCM4BLWM6XSGVLWT6LYZV3NJAVCNFSM6AAAAABFCPNKICVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJWGQ4DMMRXG4>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"],"labels":["Component: JavaScript","awaiting changes"]},{"title":"GH-40716: [Java][Integration] Fix test_package_java in verification scripts","body":"### Rationale for this change\r\n\r\nJPMS changed the location of JNI libs in the dist dir.\r\n\r\n### What changes are included in this PR?\r\n\r\n* Update the dist path in the verification script\r\n\r\n### Are these changes tested?\r\n\r\nCI\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\n* GitHub Issue: #40716","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","@github-actions crossbow submit verify-rc-source-integration*","Revision: 316f7a1fcef8a117ccea413498171b3a9a6e8f11\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-dfed1bd723](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-dfed1bd723)\n\n|Task|Status|\n|----|------|\n|verify-rc-source-integration-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369511\/job\/22949444943)|\n|verify-rc-source-integration-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369589\/job\/22949445441)|\n|verify-rc-source-integration-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369502\/job\/22949444947)|\n|verify-rc-source-integration-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369612\/job\/22949445460)|\n|verify-rc-source-integration-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369483\/job\/22949444970)|\n|verify-rc-source-integration-macos-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-macos-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369647\/job\/22949445804)|\n|verify-rc-source-integration-macos-conda-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-dfed1bd723-github-verify-rc-source-integration-macos-conda-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8380369527\/job\/22949445358)|","Is this for #40716? Could you prepend `GH-40716: ` to the title?","@github-actions crossbow submit verify-rc-source-integration*",":warning: GitHub issue #40716 **has been automatically assigned in GitHub** to PR creator.","Revision: 3447a7dba0ad0e44b569717e86ee8e75f8969c35\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-bb264f7e57](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-bb264f7e57)\n\n|Task|Status|\n|----|------|\n|verify-rc-source-integration-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110939\/job\/22955182002)|\n|verify-rc-source-integration-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110969\/job\/22955182081)|\n|verify-rc-source-integration-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382111064\/job\/22955182367)|\n|verify-rc-source-integration-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110955\/job\/22955182034)|\n|verify-rc-source-integration-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110982\/job\/22955182221)|\n|verify-rc-source-integration-macos-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-macos-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110880\/job\/22955181876)|\n|verify-rc-source-integration-macos-conda-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-bb264f7e57-github-verify-rc-source-integration-macos-conda-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8382110912\/job\/22955181901)|","@github-actions crossbow submit verify-rc-source-integration*","Revision: a633677ae01e79b44584dce2a37f3ae3053c5031\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-3166d44c69](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-3166d44c69)\n\n|Task|Status|\n|----|------|\n|verify-rc-source-integration-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149382\/job\/22987478861)|\n|verify-rc-source-integration-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149958\/job\/22987480715)|\n|verify-rc-source-integration-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149587\/job\/22987479481)|\n|verify-rc-source-integration-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149714\/job\/22987479667)|\n|verify-rc-source-integration-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149643\/job\/22987479785)|\n|verify-rc-source-integration-macos-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-macos-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393149561\/job\/22987479450)|\n|verify-rc-source-integration-macos-conda-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3166d44c69-github-verify-rc-source-integration-macos-conda-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8393150380\/job\/22987481989)|","@raulcd This fixes the Java issue in the above scripts. In the second crossbow run I triggered, Java passed for all but almalinux, which has a known issue fixed on main. After rebasing to verify almalinux, a new non-Java issue has been introduced which can be seen in the third crossbow run."],"labels":["awaiting review"]},{"title":"[R] write_dataset returns nothing","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe docs say\r\n\r\n```\r\nwrite_dataset              package:arrow               R Documentation\r\n\r\nWrite a dataset\r\n\r\nDescription:\r\n\r\n     This function allows you to write a dataset. By writing to more\r\n     efficient binary storage formats, and by specifying relevant\r\n     partitioning, you can make it much faster to read and query.\r\n\r\nUsage:\r\n\r\n     write_dataset(\r\n       dataset,\r\n       path,\r\n       format = c(\"parquet\", \"feather\", \"arrow\", \"ipc\", \"csv\", \"tsv\", \"txt\", \"text\"),\r\n       partitioning = dplyr::group_vars(dataset),\r\n       basename_template = paste0(\"part-{i}.\", as.character(format)),\r\n       hive_style = TRUE,\r\n       existing_data_behavior = c(\"overwrite\", \"error\", \"delete_matching\"),\r\n       max_partitions = 1024L,\r\n       max_open_files = 900L,\r\n       max_rows_per_file = 0L,\r\n       min_rows_per_group = 0L,\r\n       max_rows_per_group = bitwShiftL(1, 20),\r\n       ...\r\n     )\r\n\r\nValue:\r\n\r\n     The input \u2018dataset\u2019, invisibly\r\n```\r\n\r\nbut when I write datasets, I get nothing in return:\r\n\r\n```r\r\nres <- arrow::write_dataset(mtcars, \"mt\")\r\nres\r\n# NULL\r\n```\r\n\r\nOn Ubuntu-23.10, R-4-3.2, arrow_15.0.1.\n\n### Component(s)\n\nR","comments":["Part of me says I don't _need_ the data, I'd rather have all of the filenames created ... but that's more of a feature-request than a bug-report.\r\n\r\n```r\r\nres <- arrow::write_dataset(mtcars, \"mt\", partitioning = c(\"cyl\", \"gear\"))\r\nres\r\n# NULL\r\nlist.files(\"mt\", recursive = TRUE, full.names = TRUE)\r\n# [1] \"mt\/cyl=4\/gear=3\/part-0.parquet\" \"mt\/cyl=4\/gear=4\/part-0.parquet\" \"mt\/cyl=4\/gear=5\/part-0.parquet\" \"mt\/cyl=6\/gear=3\/part-0.parquet\"\r\n# [5] \"mt\/cyl=6\/gear=4\/part-0.parquet\" \"mt\/cyl=6\/gear=5\/part-0.parquet\" \"mt\/cyl=8\/gear=3\/part-0.parquet\" \"mt\/cyl=8\/gear=5\/part-0.parquet\"\r\n```\r\n\r\nThat is, I don't want to need to do `list.files(..)`, since `arrow` knows internally what files were created. A recursive search can get a little annoying if there are many existing files.","Thanks for reporting this @r2evans.  \r\n\r\nI can confirm that I can reproduce this - nothing is returned.\r\n\r\nThinking through the various options of what would make sense to return, I had a few thoughts.\r\n\r\nLooking at other examples, such as `DBI::dbWriteTable()` (as sometimes we think of working with datasets as analogous to working with a database connection), that function just returns `TRUE` invisibly to indicate success.\r\n\r\nHowever, looking at the [tidyverse design guide](https:\/\/design.tidyverse.org\/out-invisible.html), a principle referenced there is:\r\n\r\n> If a function is called primarily for its side-effects, it should invisibly return a useful output. If there\u2019s no obvious output, return the first argument. This makes it possible to use the function with in a pipeline.\r\n\r\nI'm not sure how common it is to use `write_dataset()` mid-pipeline, though it's feasible, so that could make sense too.\r\n\r\nI can see why you might want to have the list of files created, but I'm a bit hesitant to diverge from the behaviour of existing similar APIs, and this might be better thought through as a separate feature request.\r\n\r\nCurious to see what @amoeba thinks here?","I agree that returning something useful is a good thing. Often, yes, mid-pipeline functions pass the data through (often invisibly), but I agree that I don't see this being a mid-pipeline kind of operation.\r\n\r\nMy thought is to return the filenames created. While we are calling this function primarily for its side-effect, what we don't know is exactly what filenames will be created; the only time we \"know\" is when we're writing completely new data (partitioning subdirs do not exist or are known to be empty). If that is ever important (I have an internal use-case where it is), then we have to _infer_ what files were created, something that can be ambiguous in situations where there are pre-existing files.\r\n\r\nDo you think returning a `character` vector with the final filenames is both meaningful and feasible?\r\n\r\nAnother argument for this: _if_ this returns files and it is being used mid-pipe where data would be more useful, there is an easy workaround:\r\n\r\n```r\r\n# dplyr pipe\r\ndat %>%\r\n  mutate(...) %>%\r\n  { write_dataset(., ...); .; } %>%\r\n  summarize(...)\r\n# native pipe\r\ndat |>\r\n  transform(..) |>\r\n  (\\(.x) { write_dataset(.x, ...); .x; })() |>\r\n  aggregate(..., data = _)\r\n```\r\n\r\nBut if the function instead returns data, there is no unambiguous way to immediately know the filenames created."],"labels":["Type: bug","Component: R"]},{"title":"GH-40224: [C++] fix: improve the backpressure handling in the dataset writer","body":"### Rationale for this change\r\n\r\nThe dataset writer would fire the resume callback as soon as the underlying dataset writer's queues freed up, even if there were pending tasks.  Backpressure is not applied immediately and so a few tasks will always trickle in.  If backpressure is pausing and then resuming frequently this can lead to a buildup of pending tasks and uncontrolled memory growth.\r\n\r\n### What changes are included in this PR?\r\n\r\nThe resume callback is not called until all pending write tasks have completed.\r\n\r\n### Are these changes tested?\r\n\r\nThere is quite an extensive set of tests for the dataset writer already and they continue to pass.  I ran them on repeat, with and without stress, and did not see any issues.\r\n\r\nHowever, the underlying problem (dataset writer can have uncontrolled memory growth) is still not tested as it is quite difficult to test.  I was able to run the setup described in the issue to reproduce the issue.  With this fix the repartitioning task completes for me.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\n* GitHub Issue: #40224","comments":[":warning: GitHub issue #40224 **has been automatically assigned in GitHub** to PR creator."],"labels":["Critical Fix","awaiting committer review"]},{"title":"GH-40720: [Python] Simplify and improve perf of creation of the column names in Table.to_pandas","body":"### Rationale for this change\r\n\r\nThe `pandas_compat.py` has over the years grown quite complex and a lot of pandas compatibility code, which probably can be simplified nowadays because of not supporting old pandas and Python versions anymore.\r\n\r\nOne part of the code where this is the case is in the reconstruction of the `.columns` Index object of the resulting DataFrame. Right now that always goes through a MultiIndex (even for simple column names), which has quite some overhead of the simple case. And it also has some old Python\/pandas compat code that could be removed.\r\n\r\n### What changes are included in this PR?\r\n\r\nThe simplification to not go through a MultiIndex for the simple cases gives a nice speed-up as well:\r\n\r\n```python\r\nIn [1]: table = pa.table({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3], 'c': [3, 4, 5]})\r\n\r\nIn [2]: %timeit table.to_pandas()\r\n251 \u00b5s \u00b1 1.26 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)   # <-- main\r\n68.1 \u00b5s \u00b1 894 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)  # <-- PR\r\n```\r\n\r\n### Are these changes tested?\r\n\r\nWe should have extensive existing tests for this\r\n\r\n### Are there any user-facing changes?\r\n\r\nThat should not be the case\n* GitHub Issue: #40720","comments":["@github-actions crossbow submit -g python","Revision: 795839686d6192cece092934491f6d2bb59b0d99\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-4f3eabeb1a](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-4f3eabeb1a)\n\n|Task|Status|\n|----|------|\n|example-python-minimal-build-fedora-conda|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-example-python-minimal-build-fedora-conda)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379584946\/job\/22946862806)|\n|example-python-minimal-build-ubuntu-venv|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-example-python-minimal-build-ubuntu-venv)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585210\/job\/22946863698)|\n|test-conda-python-3.10|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585113\/job\/22946863539)|\n|test-conda-python-3.10-cython2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-cython2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585502\/job\/22946865858)|\n|test-conda-python-3.10-hdfs-2.9.2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-hdfs-2.9.2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585401\/job\/22946865087)|\n|test-conda-python-3.10-hdfs-3.2.1|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-hdfs-3.2.1)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585037\/job\/22946863286)|\n|test-conda-python-3.10-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379584935\/job\/22946862851)|\n|test-conda-python-3.10-pandas-nightly|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-pandas-nightly)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585125\/job\/22946863498)|\n|test-conda-python-3.10-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585338\/job\/22946864526)|\n|test-conda-python-3.10-substrait|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.10-substrait)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585208\/job\/22946863734)|\n|test-conda-python-3.11|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379584987\/job\/22946862937)|\n|test-conda-python-3.11-dask-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11-dask-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585383\/job\/22946864520)|\n|test-conda-python-3.11-dask-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11-dask-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585276\/job\/22946864320)|\n|test-conda-python-3.11-hypothesis|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11-hypothesis)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585416\/job\/22946865101)|\n|test-conda-python-3.11-pandas-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11-pandas-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585569\/job\/22946866262)|\n|test-conda-python-3.11-spark-master|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.11-spark-master)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585362\/job\/22946864628)|\n|test-conda-python-3.12|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.12)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585620\/job\/22946866234)|\n|test-conda-python-3.8|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.8)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585622\/job\/22946866266)|\n|test-conda-python-3.8-pandas-1.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.8-pandas-1.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585089\/job\/22946863363)|\n|test-conda-python-3.8-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.8-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585390\/job\/22946864765)|\n|test-conda-python-3.9|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.9)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585312\/job\/22946864465)|\n|test-conda-python-3.9-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-conda-python-3.9-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379584929\/job\/22946862825)|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585216\/job\/22946864064)|\n|test-debian-12-python-3-amd64|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4f3eabeb1a-azure-test-debian-12-python-3-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22946866241)|\n|test-debian-12-python-3-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-debian-12-python-3-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585239\/job\/22946864180)|\n|test-fedora-39-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4f3eabeb1a-azure-test-fedora-39-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22946867413)|\n|test-ubuntu-20.04-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4f3eabeb1a-azure-test-ubuntu-20.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22946865127)|\n|test-ubuntu-22.04-python-3|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4f3eabeb1a-github-test-ubuntu-22.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8379585711\/job\/22946866897)|"],"labels":["Component: Python","awaiting committer review"]},{"title":"[Python] pandas compat: clean-up code in columns Index reconstruction in Table.to_pandas","body":"The `pandas_compat.py` has over the years grown quite complex and a lot of pandas compatibility code, which probably can be simplified nowadays because of not supporting old pandas and Python versions anymore.\r\n\r\nOne aspect that I noticed while working on `to_pandas` was the complexity in the reconstruction of the `.columns` Index object of the resulting DataFrame. Right now that always goes through a MultiIndex (even for simple column names), which has quite some overhead of the simple case.","comments":[],"labels":["Component: Python"]},{"title":"Can't use `array.RecordBuilder,NewRecord` when one of the fields has `arrow.NullType`","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nIf we use `new(arrow.NullType)` as data type for one of the fields in the record, the `array.RecordBuilder,NewRecord` fails while validatiing the schema.\r\n\r\nWorkaround: use `arrow.Null` variable.\r\n\r\nProper fix:\r\n```go\r\nvar(\r\n   Null = new(NullType)\r\n)\r\n```\r\nin the code.\r\n\r\ncc: @zeroshade \r\n\r\n### Component(s)\r\n\r\nGo","comments":["@zeroshade could you update the following lines?\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/1ab25b8424cde6afaadad27d0e62d802dc8ba6f0\/go\/arrow\/datatype_null.go#L31-L32\r\n\r\nit could be something like\r\n```go\r\nvar (\r\n    Null DataType = new(NullType)\r\n)\r\n```\r\ninstead of 2"],"labels":["Type: bug","Component: Go"]},{"title":"GH-38325: [Python] Implement PyCapsule interface for Device data in PyArrow","body":"### Rationale for this change\r\n\r\nPyArrow implementation for the specification additions being proposed in https:\/\/github.com\/apache\/arrow\/pull\/40708\r\n\r\n### What changes are included in this PR?\r\n\r\nNew `__arrow_c_device_array__` method to `pyarrow.Array` and `pyarrow.RecordBatch`, and support in the `pyarrow.array(..)`, `pyarrow.record_batch(..)` and `pyarrow.table(..)` functions to consume objects that have those methods.\r\n\r\n### Are these changes tested?\r\n\r\nYes (for CPU only for now, https:\/\/github.com\/apache\/arrow\/pull\/40385 is a prerequisite to test this for CUDA)\r\n\r\n\n* GitHub Issue: #38325","comments":[],"labels":["Component: Python","awaiting changes"]},{"title":"[CI][Java][Integration] Integration verification jobs are failing due to libarrow_cdata_jni.so not found","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nVerification integration jobs have been failing for a long time:\r\n[verify-rc-source-integration-linux-almalinux-8-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326820551\/job\/22783238495)\r\n[verify-rc-source-integration-linux-conda-latest-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326821549\/job\/22783241848)\r\n[verify-rc-source-integration-linux-ubuntu-20.04-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326821294\/job\/22783240707)\r\n[verify-rc-source-integration-linux-ubuntu-22.04-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326822265\/job\/22783288184)\r\n[verify-rc-source-integration-macos-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326821150\/job\/22783239900)\r\n[verify-rc-source-integration-macos-conda-amd64](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8326820569\/job\/22783238615)\r\n\r\nIt seems that they have been failing since the 19th of January. The commits between the last success and the first failure are: https:\/\/github.com\/apache\/arrow\/compare\/55afcf0450aa2b611e78335bdbfd77e55ae3bc9f...05b8f366e17ee6f21df4746bb6a65be399dfb68d\r\nI suspect https:\/\/github.com\/apache\/arrow\/issues\/39001 is the culprit.\r\n\r\nBased on the logs:\r\n```\r\n [ERROR]   StreamTest.getNextError:192 ? IllegalState error loading native libraries: java.io.FileNotFoundException: arrow_cdata_jni\/x86_64\/libarrow_cdata_jni.so\r\n[ERROR]   StreamTest.getSchemaError:213 ? IllegalState error loading native libraries: java.io.FileNotFoundException: arrow_cdata_jni\/x86_64\/libarrow_cdata_jni.so\r\n[ERROR]   StreamTest.roundtripDictionary:169->roundtrip:228 ? IllegalState error loading native libraries: java.io.FileNotFoundException: arrow_cdata_jni\/x86_64\/libarrow_cdata_jni.so\r\n[ERROR]   StreamTest.roundtripStrings:131->roundtrip:257->roundtrip:228 ? IllegalState error loading native libraries: java.io.FileNotFoundException: arrow_cdata_jni\/x86_64\/libarrow_cdata_jni.so\r\n[ERROR]   StreamTest.testRoundtripInts:94->roundtrip:257->roundtrip:228 ? IllegalState error loading native libraries: java.io.FileNotFoundException: arrow_cdata_jni\/x86_64\/libarrow_cdata_jni.so\r\n```\n\n### Component(s)\n\nContinuous Integration, Integration, Java","comments":["cc @jduo @danepitkin ","Yes it looks related to changing the location of JNI libraries. JNI libraries now need to be in <library_name>\/<arch>\/<lib name>.<extension>","The verification scripts are mostly passing on main and fully passing in an open PR https:\/\/github.com\/apache\/arrow\/pull\/40514. Are the verification script tests here failing in a different environment?","Those are on main and those are verify integration not verify java. I think the verify java are good.","Thank you for the clarification!"],"labels":["Type: bug","Component: Java","Component: Continuous Integration","Component: Integration","Priority: Blocker"]},{"title":"[Java] Bump org.apache.maven dependencies from 3.8.7 to 3.9.6","body":"### Describe the enhancement requested\n\nAfter #40514 merges (closing #40515), let's try again to bump Maven to 3.9.6.\r\n\r\nWe should maybe open a new PR that supersedes #39451 because rebasing that might be messy.\n\n### Component(s)\n\nJava","comments":["take","Sorry @ianmcook I didn't see you have self assigned. Please ignore my take, I checked from github mobile \ud83d\ude4f","@vibhatha you're welcome to take it :)"],"labels":["Type: enhancement","Component: Java"]},{"title":"[Docs][Format] Update ADBC spec version to 1.1.0 in Arrow spec docs","body":"### Describe the enhancement requested\n\nThe ADBC pages in the Arrow specifications docs need to be updated to refer to ADBC spec version 1.1.0:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/format\/ADBC.rst\r\nhttps:\/\/github.com\/apache\/arrow\/tree\/main\/docs\/source\/format\/ADBC\r\n\r\nThe content on these pages could also be pared down somewhat and replaced with links to ADBC docs pages such as https:\/\/arrow.apache.org\/adbc\/current\/format\/specification.html\n\n### Component(s)\n\nDocumentation","comments":[],"labels":["Type: enhancement","Component: Documentation"]},{"title":"Error when saving attached data.frame containing nested data.frame (character versus all NA logical column)","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nWhen saving the attached rds file to a parquet file, I get the error as in the repress `Error: Invalid: Problem with column 3 (au_orcid): Invalid: Expecting a character vector`.\r\n\r\nThe problem seems to be that one column only contains NA, and these are interpreted as logical by R, while arrow expects a character vector.\r\n\r\n```\r\narrow          15.0.1     2024-03-19 [1] https:\/\/apache.r-universe.dev (R 4.3.3)\r\n\r\nsetting  value\r\n version  R version 4.3.3 (2024-02-29)\r\n os       macOS Sonoma 14.4\r\n system   aarch64, darwin20\r\n ui       X11\r\n language (EN)\r\n collate  en_US.UTF-8\r\n ctype    en_US.UTF-8\r\n tz       Europe\/Zurich\r\n date     2024-03-21\r\n pandoc   3.1.12.3 @ \/opt\/homebrew\/bin\/pandoc\r\n```\r\n\r\n\r\n``` r\r\nlibrary(arrow)\r\n#>\r\n#> Attaching package: \u2018arrow\u2019\r\n#> The following object is masked from \u2018package:utils\u2019:\r\n#>\r\n#>     timestamp\r\ndata <- readRDS(\u201cproblem.rds\u201d)\r\narrow::write_dataset(\r\n    data,\r\n    path = \u201c~\/problem\u201d,\r\n    partitioning = c(\u201cpublication_year\u201d, \u201cpage\u201d),\r\n    format = \u201cparquet\u201d\r\n)\r\n#> Error: Invalid: Problem with column 3 (au_orcid): Invalid: Expecting a character vector\r\n```\r\n<sup>Created on 2024-03-21 with [reprex v2.1.0](https:\/\/reprex.tidyverse.org\/)<\/sup>\r\n[problem.rds.zip](https:\/\/github.com\/apache\/arrow\/files\/14696782\/problem.rds.zip)\r\n\r\n\n\n### Component(s)\n\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"GH-38325: [Python] Expand the Arrow PyCapsule Interface with C Device Data support","body":"### Rationale for this change\r\n\r\nWe defined a protocol exposing the C Data Interface (schema, array and stream) in Python through PyCapsule objects and dunder methods `__arrow_c_schema\/array\/stream__` (https:\/\/github.com\/apache\/arrow\/issues\/35531 \/ https:\/\/github.com\/apache\/arrow\/pull\/37797).\r\n\r\nWe also expanded the C Data Interface with device capabilities: https:\/\/arrow.apache.org\/docs\/dev\/format\/CDeviceDataInterface.html (https:\/\/github.com\/apache\/arrow\/pull\/34972).\r\n\r\nThis expands the Python exposure of the interface with support for the newer Device structs.\r\n\r\n### What changes are included in this PR?\r\n\r\nUpdate the specification to defined two additional dunders:\r\n\r\n\r\n* `__arrow_c_device_array__` returns a pair of PyCapsules containing a C ArrowSchema and ArrowDeviceArray, where the latter uses \"arrow_device_array\" for the capsule name\r\n* `__arrow_c_device_stream__` returns a PyCapsule containing a C ArrowDeviceArrayStream, where the capsule must have a name of \"arrow_device_array_stream\" \r\n\r\n### Are these changes tested?\r\n\r\nSpec-only change\r\n\n* GitHub Issue: #38325","comments":[":warning: GitHub issue #38325 **has been automatically assigned in GitHub** to PR creator.","I don't think this is controversial, but perhaps this can be publicized on the ML to entice more feedback?\r\n\r\nIt could perhaps even be publicized towards the DLPack and Numba developers \/ communities."],"labels":["Component: Documentation","awaiting changes"]},{"title":"MINOR: [C++][Parquet][Docs] Increase chunk_size in docs","body":"### Rationale for this change\r\n\r\nIs there a reason we're using a low value here. All other examples use `128*1024` or `64*1024`.\r\n\r\nI was stumped by this as I used it without really reading about the parameter and spent a day figuring out why my parquet writes were so slow.\r\n\r\n### What changes are included in this PR?\r\n\r\nIncrease `chunk_size` to `64*1024`\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes.","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","@wjones127 Do you remember why you choose this chunk size in the example?\r\n(Is it just enough for the example?)"],"labels":["Component: C++","awaiting review"]},{"title":"GH-40700: [Go][CI] Update go.sum dependencies","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40700","comments":[":warning: GitHub issue #40700 **has been automatically assigned in GitHub** to PR creator.","@github-actions crossbow submit -g go","Revision: 62c22ebef161369d11f870a21c7ab6e131f4a884\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-86a70cc935](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-86a70cc935)\n\n|Task|Status|\n|----|------|\n|test-debian-12-go-1.19|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-86a70cc935-azure-test-debian-12-go-1.19)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22926380149)|\n|test-debian-12-go-1.21|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-86a70cc935-azure-test-debian-12-go-1.21)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22926379438)|","ok, I tried. I did run `go mod tidy` locally but it still fails. @zeroshade any idea what might be the problem with the nightly job failure for `test-debian-12-go-1.21`?","I'm not at my computer at the moment, but try outright deleting the go.sum file and then running go mod tidy to regenerate it.","@github-actions crossbow submit -g go","Revision: 24266776b8ffd842b636409af865a28371797188\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-0550e44b8a](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-0550e44b8a)\n\n|Task|Status|\n|----|------|\n|test-debian-12-go-1.19|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-0550e44b8a-azure-test-debian-12-go-1.19)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22940503393)|\n|test-debian-12-go-1.21|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-0550e44b8a-azure-test-debian-12-go-1.21)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22940504099)|","@github-actions crossbow submit -g go","Revision: def15dd4b9f9f18e79e3d32173a01f4472a07581\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-6db690d21d](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-6db690d21d)\n\n|Task|Status|\n|----|------|\n|test-debian-12-go-1.19|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-6db690d21d-azure-test-debian-12-go-1.19)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22940823610)|\n|test-debian-12-go-1.21|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-6db690d21d-azure-test-debian-12-go-1.21)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22940822904)|","> try outright deleting the go.sum file and then running go mod tidy to regenerate it.\r\n\r\nIt doesn't seem to work. It is not urgent, take a look whenever you have some time! @zeroshade "],"labels":["Component: Go","awaiting committer review"]},{"title":"[Go][CI] test-debian-12-go-1.21 fails with `go: updates to go.mod needed`","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n[test-debian-12-go-1.21](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22911693077) has been failing since 18th of March. With the following failure:\r\n```\r\n+ source_dir=\/arrow\/go\r\n+ export 'GOFLAGS= -gcflags=all=-d=checkptr'\r\n+ GOFLAGS=' -gcflags=all=-d=checkptr'\r\n+ pushd \/arrow\/go\/arrow\r\n+ [[ -n '' ]]\r\n+ go install -v .\/...\r\ngo: updates to go.mod needed; to update it:\r\n\tgo mod tidy\r\n1\r\n```\r\n\r\nThe commits introduced since the last success and the first failure are: https:\/\/github.com\/apache\/arrow\/compare\/b448b33808f2dd42866195fa4bb44198e2fc26b9...0956f3f7c9f8d4f976275cd670744b52ee30cbf3\r\n\r\nThere were several dependency version bumps from dependabot.\n\n### Component(s)\n\nContinuous Integration, Go","comments":["Ah, we don't have any Go 21 job in `.github\/workflows\/go.yml`.\r\n\r\nIt seems that Go supports only the 2 newer major releases:\r\n\r\nhttps:\/\/go.dev\/doc\/devel\/release\r\n\r\n> Release Policy\r\n>\r\n> Each major Go release is supported until there are two newer major releases. For example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was supported until the Go 1.8 release. We fix critical problems, including [critical security problems](https:\/\/go.dev\/security), in supported releases as needed by issuing minor revisions (for example, Go 1.6.1, Go 1.6.2, and so on).\r\n\r\nIt seems that Go 1.21 and 1.22 are only supported versions now.\r\n\r\n@zeroshade Can we use Go 1.21 and 1.22 instead of 1.19 and 1.20 in `.github\/workflows\/go.yml`.","@kou so we started trying to bump ADBC to use Go 1.21 and ran into a whole mess of packaging issues with the docker images as pointed out [here](https:\/\/github.com\/apache\/arrow-adbc\/pull\/1646#discussion_r1531206030)\r\n\r\nI'd love for us to use Go 1.21 and 1.22 instead of 1.19 and 1.20, but I think I'll need your help figuring out the packaging issues before we can do so. Ultimately the problem is getting the go1.21 and go1.22 packages installed in the images in such a way that allows the builds to still work, since the OS package manager channels don't seem to have the newer versions of go on them yet.","Sure! I can help the ADBC issue.\r\n\r\nI've opened an issue for Go 1.21 or later: #40733"],"labels":["Type: bug","Component: Go","Component: Continuous Integration"]},{"title":"GH-40698: [C++] Create registry for Devices to map DeviceType to MemoryManager in C Device Data import","body":"### Rationale for this change\r\n\r\nFollow-up on https:\/\/github.com\/apache\/arrow\/pull\/39980#discussion_r1483235845\r\n\r\nRight now, the user of `ImportDeviceArray` or `ImportDeviceRecordBatch` needs to provide a `DeviceMemoryMapper` mapping the device type and id to a MemoryManager. We provide a default implementation of that mapper that just knows about the default CPU memory manager (and there is another implementation in `arrow::cuda`, but you need to explicitly pass that to the import function)\r\n\r\nTo make this easier, this PR adds a registry such that default device mappers can be added separately.\r\n\r\n\r\n### What changes are included in this PR?\r\n\r\nThis PR adds two new public functions to register device types (`RegisterDeviceMemoryManager`) and retrieve the mapper from the registry (`GetDeviceMemoryManager`).\r\n\r\nFurther, it provides a `RegisterCUDADevice` to optionally register the CUDA devices (by default only CPU device is registered).\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #40698","comments":[":warning: GitHub issue #40698 **has been automatically assigned in GitHub** to PR creator.","@pitrou I would appreciate a preliminary review to check if this is going in the right direction\r\n\r\n(of course still need to add tests, docs, clean-up naming, etc, and testing it now with CUDA)\r\n\r\nFor now I didn't go for a dual public \/ `Impl` class structure like we do for other registries, because it seems that the class itself doesn't need to be public in this case. Just the register \/ get function should be sufficient for users?\r\n\r\nAnd I added it to device.h\/cc right now, but actually if this will only be used for the C Device interface, could also move it to bridge.h\/cc","@github-actions crossbow submit test-cuda-python","Revision: e16e24d5cdac0ffefbdeb01b5b045be3f2b5b118\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-3c9b11581b](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-3c9b11581b)\n\n|Task|Status|\n|----|------|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-3c9b11581b-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8375455699\/job\/22932828088)|","> @pitrou I would appreciate a preliminary review to check if this is going in the right direction\r\n\r\nYes, this is looking ok, though the implementation can be simplified a bit.\r\n\r\n> For now I didn't go for a dual public \/ `Impl` class structure like we do for other registries, because it seems that the class itself doesn't need to be public in this case. Just the register \/ get function should be sufficient for users?\r\n\r\nAgreed. No need to expose the registry class itself for now.\r\n\r\n> And I added it to device.h\/cc right now, but actually if this will only be used for the C Device interface, could also move it to bridge.h\/cc\r\n\r\nSince the API is minimal and doesn't require any addition includes, we can keep it in `device.h` IMHO.",">  though the implementation can be simplified a bit\r\n\r\nYou mean to not even use the internal class to store the mapping, but just have the register\/get functions and store the unordered_map in a global variable?\r\n","No, the class is ok, but the `call_once` is not required if instead using a static local variable.","@github-actions crossbow submit test-cuda-python","Revision: f33872d7ebb16eed3c8c7a258e32ab71afa6dc32\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-2f2ec1b2f6](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-2f2ec1b2f6)\n\n|Task|Status|\n|----|------|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-2f2ec1b2f6-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8387772009\/job\/22970619066)|"],"labels":["Component: C++","awaiting committer review"]},{"title":"[C++] Create registry for Devices to map DeviceType to MemoryManager in C Device Data import","body":"### Describe the enhancement requested\n\nFollow-up on https:\/\/github.com\/apache\/arrow\/pull\/39980#discussion_r1483235845\r\n\r\nRight now, the user of `ImportDeviceArray` or `ImportDeviceRecordBatch` needs to provide a `DeviceMemoryMapper` mapping the device type and id to a MemoryManager. We provide a default implementation of that mapper that just knows about the default CPU memory manager (and there is another implementation in `arrow::cuda`, but you need to explicitly pass that to the import function)\r\n\r\nTo make this easier, the suggestion was to create a registry such that default device mappers can be added separately.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-36026: [C++][ORC] Check TZDB availability for ORC","body":"### Rationale for this change\r\n\r\nWhen \/usr\/share\/zoneinfo is unavailable and TZDIR env is unset, creating C++ ORC reader will crash on Windows. We need to eagerly check this and prevent followup crash.\r\n\r\n### What changes are included in this PR?\r\n\r\nEagerly check TZDB availability before creating ORC reader\/writer.\r\n\r\n### Are these changes tested?\r\n\r\nYes, added a test case to make sure the check work as expected.\r\n\r\n### Are there any user-facing changes?\r\n\r\nUsers on Windows (or other cases when TZDB is not availble) will clearly see this error message instead of crash.\n* GitHub Issue: #36026","comments":["I have filed https:\/\/issues.apache.org\/jira\/browse\/ORC-1661 to follow up this issue. For now I'd propose to eagerly check TZDB availability. Please let me know what do you think. @jorisvandenbossche @pitrou @kou ","I think it would be more useful to find why the exception isn't properly caught.","The answer might be that `ORC_END_CATCH_NOT_OK` does not catch _all_ exceptions.","> I think it would be more useful to find why the exception isn't properly caught.\r\n\r\nAgreed. To my surprise that Windows behaves differently when opening a non-existent file: https:\/\/github.com\/apache\/orc\/pull\/1856. I will debug it on my Windows PC after work.","If my reading of the code in that PR is correct, it is testing that it raises a `TimezoneError` on the Orc side, but `ORC_END_CATCH_NOT_OK` only catches those three:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/a36cc62f45e0ebe594f97212728531148afa6e82\/cpp\/src\/arrow\/adapters\/orc\/adapter.cc#L73-L83","TimezoneError is added in ORC 2.0.0, but we are still on 1.9.2. @jorisvandenbossche ","I was looking at ORC 1.9.3 as well, which already has TimezoneError? https:\/\/github.com\/apache\/orc\/blob\/a765c316adfb80b47fb8c38831f8c0e0d7294cdb\/c%2B%2B\/src\/Timezone.cc#L684-L685","@jorisvandenbossche Sorry for the confusion. I thought you were talking about this change: https:\/\/github.com\/apache\/orc\/pull\/1587 which is ORC 2.0.0 only.\r\n\r\nI tried to use ORC 1.9.2 (w\/o the aforementioned path existence check) to open a reader on Windows w\/o TZDIR set. It actually throws a `ParseError` exception here: https:\/\/github.com\/apache\/orc\/blob\/v1.9.2\/c%2B%2B\/src\/OrcFile.cc#L62. This also aligns with the bug description here: https:\/\/github.com\/apache\/orc\/issues\/1577#issuecomment-1674168417","But that ParseError is still caught and re-raised as a TimezoneError in the snippet I linked? (https:\/\/github.com\/apache\/orc\/blob\/a765c316adfb80b47fb8c38831f8c0e0d7294cdb\/c%2B%2B\/src\/Timezone.cc#L684-L685)\r\n\r\n(and sorry for causing the confusion! because I _was_ initially also talking about your PR for 2.0, but then afterwards also went checking the code of 1.9)","> But that ParseError is still caught and re-raised as a TimezoneError in the snippet I linked? (https:\/\/github.com\/apache\/orc\/blob\/a765c316adfb80b47fb8c38831f8c0e0d7294cdb\/c%2B%2B\/src\/Timezone.cc#L684-L685)\r\n\r\nYes, this is what I don't understand either. I am trying to reproduce this on the arrow side.\r\n","Instead of spending time reproducing it, perhaps ensure that all C++ exceptions are caught? This would be more robust anyway, since Orc C++ uses exceptions.","> Yes, this is what I don't understand either.\r\n\r\nWell, it would perfectly explain why it is crashing. And the bug report you linked to (https:\/\/github.com\/apache\/orc\/issues\/1577#issuecomment-1674168417) actually points to the line (704) that raises the TimezoneError. So that seems to all fit correctly together?","@jorisvandenbossche You're right. This issue does not relate to Windows and I can also reproduce this on my Mac with a wrong TZDIR env set. The ORC C++ library throws `TimezoneError` when it catches `ParseError` as you point out at line 704. However, TimezoneError is not in a publicly installed header: https:\/\/github.com\/apache\/orc\/blob\/main\/c%2B%2B\/src\/Timezone.hh#L113. As @pitrou says, perhaps we should just catch everything to avoid crash."],"labels":["Component: C++","awaiting changes"]},{"title":"GH-40695 [C++] Expand Substrait type support","body":"### Rationale for this change\r\n\r\nSee #40695 \r\n\r\n### What changes are included in this PR?\r\n\r\nThis PR does a few things:\r\n\r\n * Substrait is upgraded to the latest version\r\n * Support is added for the parameterized timestamp type (but not literals due to https:\/\/github.com\/substrait-io\/substrait\/issues\/611).\r\n * Support is added for the following arrow-specific types:\r\n   * fp16\r\n   * date_millis\r\n   * time_seconds\r\n   * time_millis\r\n   * time_nanos\r\n   * large_string\r\n   * large_binary\r\n\r\nWhen adding support for the new timestamp types I also relaxed the restrictions on the time zone column.  Substrait puts time zone information in the function and not the type.  In other words, to print the \"America\/New York\" value of a column of instants one would do something like `to_char(my_timestamp, \"America\/New York\")` instead of `to_char(cast(my_timestamp, timestamp(\"nanos\", \"America\/New York\")`.\r\n\r\nHowever, the current implementation makes it impossible to produce or consume a plan with `to_char(my_timestamp, \"America\/New York\")` because it would reject the type because it has a non-UTC time zone.  With this latest change, we treat any non-empty timezone as a timezone_tz type.\r\n\r\nIn addition, I have enabled conversions from \"encoded types\" to their unencoded representation.  E.g. a type of `DICTIONARY<INT32>` will convert to `INT32`.  At a logical expression \/ plan perspective these encodings are irrelevant.  If anything, they may belong in a more physical plan representation.  Should a need for them arise we can dig into it more later.  However, I believe it is better to err on the side of generating \"something\" rather than failing in these cases.  I don't consider this last change critical and can back it out if need be.\r\n\r\n### Are these changes tested?\r\n\r\nYes, I added new unit tests\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes, via the Substrait conversion.  These changes should be backwards compatible in that they only add functionality in places that previously reported \"Not Supported\".\r\n* GitHub Issue: #40695","comments":[":warning: GitHub issue #40695 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: C++","Component: Python","awaiting change review"]},{"title":"[C++] Add Substrait support for arrow-specific types (non-paramaeterized)","body":"### Describe the enhancement requested\n\nThe Arrow<->Substrait conversion currently only works with the types that are supported by both Arrow and Substrait.  I would like to use Substrait expression conversion for filter pushdown (polars can convert to a pyarrow.compute expression, and datafusion can consume a substrait expression, and I would like to bridge the two).\r\n\r\nThis is currently blocked by the fact that polars uses large_string by default and pyarrow.compute expressions fail to serialize if they contain a large_string type.\r\n\r\nSince I know that both the source and destination are arrow I should be able to use the arrow-specific types (substrait will consider them user defined types).\r\n\r\nTo simplify things, this request only asks for support for non-parameterized types.  Arrow-specific parameterized types (e.g. decimal256, large_string, etc.) can come in a future request.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40693: [Go] Fix Decimal type precision loss on GetOneForMarshal","body":"### Rationale for this change\r\n\r\nLoss of precision when using `GetOneForMarshal` on `Decimal128` and `Decimal256`\r\n\r\n### What changes are included in this PR?\r\n\r\nFixes for precision loss with `DecimalType.GetOneForMarshal`\r\n\r\n* GitHub Issue: #40693","comments":[":warning: GitHub issue #40693 **has been automatically assigned in GitHub** to PR creator.","This seems to work:\r\n```go\r\n\ttyp := a.DataType().(*arrow.Decimal256Type)\r\n\tf := (&big.Float{}).SetInt(a.Value(i).BigInt())\r\n\tscale := big.NewInt(10)\r\n\tscale.Exp(scale, big.NewInt(int64(typ.Scale)), nil)\r\n\tf.Quo(f, new(big.Float).SetInt(scale))\r\n\treturn f.Text('f', int(typ.Scale))\r\n\t\/\/ return f.Text('g', int(typ.Precision))\r\n```\r\n\r\nBut the `f` format is not what the tests expect... with `g` it results in further floaty errors, as in `0.99000000000000000000000000827180612553027674871409` or `1234567890.1234567889999999999999999938549877259069`","@hermanschaaf Came through and fixed these \ud83e\udd47 ","@zeroshade PTAL when you have a chance? Thanks!"],"labels":["Component: Go","awaiting review"]},{"title":"[Go] Decimal types `GetOneForMarshal` loses precision","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nDecimal types seem to lose precision with GetOneForMarshal.\r\n\r\n```\r\n\/\/ Decimal128(38,20)\r\n        \t            \texpected: \"0.99\"\r\n        \t            \tactual  : \"0.99000000000000000000189735380184963276\"\r\n\r\n        \t            \texpected: \"1234567890.123456789\"\r\n        \t            \tactual  : \"1234567890.1234567890000000000059023411\"\r\n\r\n\/\/ Decimal256(50,25)\r\n        \t            \texpected: \"0.99\"\r\n        \t            \tactual  : \"0.9899999999999999103090032649279560980669703806889\"\r\n\r\n        \t            \texpected: \"1234567890.123456789\"\r\n        \t            \tactual  : \"1234567890.1234566771518943399663035833026736798823\"\r\n```\r\n\r\nWill follow up with a PR including these tests.\n\n### Component(s)\n\nGo","comments":[],"labels":["Type: bug","Component: Go"]},{"title":"MINOR: [R] Update maintainer in package description","body":"### Rationale for this change\r\n\r\nUpdate R package maintainer to Jon\r\n\r\n### What changes are included in this PR?\r\n\r\nUpdate maintainer field, swapping over Nic and Jon!\r\n\r\n### Are these changes tested?\r\n\r\nNope\r\n\r\n### Are there any user-facing changes?\r\n\r\nNah!","comments":["CC @jonkeane ! :)"],"labels":["Component: R","awaiting committer review"]},{"title":"[Docs] Include nanoarrow in the Implementation Status docs page","body":"### Describe the enhancement requested\r\n\r\nFor some of the sections on the [Implementation Status](https:\/\/arrow.apache.org\/docs\/status.html) page, we should include a column for [nanoarrow](https:\/\/arrow.apache.org\/nanoarrow).\r\n\r\n@paleolimbot @jorisvandenbossche \r\n\r\n### Component(s)\r\n\r\nDocumentation","comments":["One challenge with this is that nanoarrow now has language bindings and isn't just a C library. I'm not sure how we would render this in the Implementation Status. Maybe the column for nanoarrow could just represent the nanoarrow C library, and there could be footnotes referring to places where the nanoarrow bindings lag behind the nanoarrow C library.","Thanks for opening this! I can take this on...I agree that representing the C library is the best approach. The R and Python bindings have the extra feature of being able to convert to\/from existing R\/Python types; however, can build or consume any type (supported by the C library) from\/to buffers."],"labels":["Type: enhancement","Component: Documentation"]},{"title":"GH-40631: [C++] Add lost conjunctions back in FoldConstants and GuaranteeConjunctionMembers","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nWe have lost some conjunctions check in FoldConstants and GuaranteeConjunctionMembers, this will cause our simplification operations on some expressions to fail.\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nAdd the `and` and `or` conjunctions check into `FoldConstants` and `GuaranteeConjunctionMembers` , so we could simplify some expressions normally.\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nYes\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nNo\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40631","comments":[":warning: GitHub issue #40631 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: C++","awaiting review"]},{"title":"GH-40510: [C++] Return scalar for pure ScalarFunctions called with no arguments","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nWe should return scalar for pure ScalarFunctions called with no arguments.\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nThis is a temporary solution, and there are several issues that need to be discussed while I try to solve it:\r\n1. For the impure function, it is possible not to specify the input length. Should we also return scalar?\r\n2. Implementation:\r\n   1. The current implementation plan is to construct scalar in function exec stage without parameters and without specifying length and stuff it into ExecResult. The upper layer returns the scalar value based on `have_all_scalars_` when EmitResult sends the results. \r\n   2. Personally, I think there is another An implementation plan is to add scalar to the variant value in ExecResult, which is more friendly to the exec logic of the function? However, this implementation method requires more changes to ScalarExecutor.\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nYes\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nNo\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40510","comments":[":warning: GitHub issue #40510 **has been automatically assigned in GitHub** to PR creator.","I'm not very familiar with python and R APIs, but it seems that our changes will affect the behavior of many UDFs. "],"labels":["Component: C++","awaiting changes"]},{"title":"[C++] Add MemoryPool API to resize without copying the data","body":"### Describe the enhancement requested\r\n\r\nA common memory (re)allocation pattern is to persist a dynamically-sized scratch buffer (for example when decompressing or decoding data). Currently, the two options to achieve that are suboptimal:\r\n1. either resize the buffer when needed, but if the buffer's base pointer changes, the existing data will be copied even though it is ephemeral\r\n2. or allocate a new buffer when needing to increase the scratch size, but it is wasteful if the buffer could otherwise have been resized in place\r\n\r\nBoth jemalloc and mimalloc expose APIs that would allow us to implement a more optimal behavior of resizing without copying existing data: [mi_expand](https:\/\/microsoft.github.io\/mimalloc\/group__malloc.html#gaaee66a1d483c3e28f585525fb96707e4) for mimalloc, [xallocx](https:\/\/jemalloc.net\/jemalloc.3.html) for jemalloc.\r\n\r\nOne possible API would be:\r\n```c++\r\n  \/\/ MemoryPool methods\r\n  Status ReallocateNoCopy(int64_t old_size, int64_t new_size, int64_t alignment, uint8_t** ptr);\r\n  Status ReallocateNoCopy(int64_t old_size, int64_t new_size, uint8_t** ptr);\r\n\r\n  \/\/ ResizableBuffer methods\r\n  Status ResizeNoCopy(const int64_t new_size, bool shrink_to_fit);\r\n  Status ResizeNoCopy(const int64_t new_size);\r\n```\r\n\r\n\r\n### Component(s)\r\n\r\nC++","comments":["@mapleFU @felipecrv ","Will glance at malloc(3)'s api.\r\n\r\n>  allocate a new buffer when needing to increase the scratch size, but it is wasteful if the buffer could otherwise have been resized in place\r\n\r\nWhats the difference between this and not call shrink_to_fit when goes to smaller?","These functions are very likely to fail.\r\n\r\n> The xallocx() function returns the real size of the resulting resized allocation pointed to by ptr, which is a value less than size if the allocation could not be adequately grown in place.\r\n\r\nI don't understand why using these to implement `MemoryPool::Reallocate` is not enough.\r\n"],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40684: [Java][Docs] JNI module debugging with IntelliJ","body":"### Rationale for this change\r\n\r\nAdding documentation for debugging JNI-based Java modules. \r\n\r\n\r\n### What changes are included in this PR?\r\n\r\nDocumentation update for developer docs for Java development.\r\n\r\n### Are these changes tested?\r\n\r\nLocally built the docs and it shows the expected content. \r\n\r\n### Are there any user-facing changes?\r\n\r\nN\/A\n* GitHub Issue: #40684","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n",":warning: GitHub issue #40684 **has been automatically assigned in GitHub** to PR creator.","@github-actions crossbow submit ubuntu-docs","```\nUnable to match any tasks for `ubuntu-docs`\nThe Archery job run can be found at: https:\/\/github.com\/apache\/arrow\/actions\/runs\/8355549320\n```","@github-actions crossbow submit preview-docs","Revision: 37bfdb2480a79a986a9aff61c32e02fa05f0c07e\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-797ca9fc83](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-797ca9fc83)\n\n|Task|Status|\n|----|------|\n|preview-docs|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-797ca9fc83-github-preview-docs)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8358427852\/job\/22879707891)|"],"labels":["Component: Documentation","awaiting changes"]},{"title":"[Java][Docs] JNI module debugging with IntelliJ","body":"### Describe the enhancement requested\n\nAt the moment the Java development documentation doesn't include a section on debugging JNI-based modules like `arrow-dataset`. \n\n### Component(s)\n\nJava","comments":[],"labels":["Type: enhancement","Component: Java"]},{"title":"GH-38768: [Python] Empty slicing an array backwards beyond the start is now empty","body":"### What changes are included in this PR?\r\n\r\n`_normalize_slice` now relies on `slice.indices` (https:\/\/docs.python.org\/3\/reference\/datamodel.html#slice.indices).\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nFixing wrong data returned in an edge case.\r\n* GitHub Issue: #40642\n* GitHub Issue: #38768","comments":[],"labels":["Component: Python","awaiting changes"]},{"title":"[Python] Can a Struct field with \"non-nullable\" sub attributes be also nullable in pyarrow.json.read_json ?","body":"### Describe the enhancement requested\r\n\r\n```\r\nimport pyarrow as pa\r\n\r\nstruct_type = pa.struct([\r\n    ('dimensionCm', pa.int64(), False)  # Non-nullable field within the struct, the code works when set to True\r\n])\r\n\r\nschema = pa.schema([\r\n    ('id', pa.string(), False),\r\n    ('heightCm', struct_type, True)  # Struct field marked as nullable\r\n])\r\n\r\ntable = pj.read_json(\"test.json\", parse_options=pj.ParseOptions(explicit_schema=schema))\r\n```\r\n```\r\n# Json payload example\r\n{\"id\": \"test1\", \"heightCm\": {\"dimensionCm\": 10}}\r\n{\"id\": \"test2\", \"heightCm\": {\"dimensionCm\": 20}}\r\n{\"id\": \"test3\"}\r\n```\r\n\r\nA Struct field with nested \"not null\" field can't parse json files and return \"ArrowInvalid: JSON parse error: a required field was null\" error.\r\nBut if the nested attributes is nullable, then no error is returned.\r\n\r\nCan it be changed to allow Struct type to be used to parse Json file even though it has non-nullable inner attribute inside?\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[Java] JDK 22 Support","body":"### Describe the enhancement requested\n\nVerify JDK 22 and enable it in CI jobs.\n\n### Component(s)\n\nJava","comments":["We need to wait for the Eclipse Temurin 22 release and then for the maven\/temurin docker release before we can add to CI.","I can take a look at this.","Here is the link to the maven docker registry: https:\/\/hub.docker.com\/_\/maven\r\n\r\nIt will probably take a week or two before the container is available."],"labels":["Type: enhancement","Component: Java"]},{"title":"[R] s3_bucket crashes RStudio in 15.0.1","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nAny attempt at instantiating an S3Filesystem object in 15.0.1 crashes R in RStudio. You get the dialog box that the R session has crashed and your only option is to restart the session. There are no error messages in the console.\r\n\r\nRolling back to 14.0.0.2 resolves the issue.\r\n\r\nI am experiencing this on an M1 Mac running Sonoma 14.2.1.\n\n### Component(s)\n\nR","comments":["Have you checked that arrow was compiled with S3 support with `arrow_info()`? The most recent version on CRAN has issues with the arrow binary, discussing in #40667.","Yes, sorry, that was the first thing I checked. They report the following:\r\n\r\n```\r\n> arrow_info()\r\nArrow package version: 15.0.1\r\n\r\nCapabilities:\r\n               \r\nacero      TRUE\r\ndataset    TRUE\r\nsubstrait FALSE\r\nparquet    TRUE\r\njson       TRUE\r\ns3         TRUE\r\ngcs       FALSE\r\nutf8proc   TRUE\r\nre2        TRUE\r\nsnappy     TRUE\r\ngzip       TRUE\r\nbrotli     TRUE\r\nzstd       TRUE\r\nlz4        TRUE\r\nlz4_frame  TRUE\r\nlzo       FALSE\r\nbz2        TRUE\r\njemalloc   TRUE\r\nmimalloc  FALSE\r\n\r\nMemory:\r\n                  \r\nAllocator jemalloc\r\nCurrent    0 bytes\r\nMax        0 bytes\r\n\r\nRuntime:\r\n                        \r\nSIMD Level          none\r\nDetected SIMD Level none\r\n\r\nBuild:\r\n                                    \r\nC++ Library Version           15.0.0\r\nC++ Compiler              AppleClang\r\nC++ Compiler Version 15.0.0.15000100\r\n\r\n> arrow_with_s3()\r\n[1] TRUE\r\n```\r\n\r\nI was a little surprised that the binary wasn't built with gcs support, but it's reporting s3 support. When I run `s3_bucket` it appears to run through the auth process (for out setup, I'm redirected to a web page, verify an alphanumeric code, click authorize) and then the R session immediately crashes.","Thanks for the report @joranE. How did you install arrow here? I'd like to try to reproduce the error so we can start debugging your issue. If you just ran `install.packages`, I'm guessing something about your setup triggered a source build. If you're not sure, running the following and pasting the output here would be helpful:\r\n\r\n```r\r\nSys.setenv(\"ARROW_R_DEV\"=TRUE)\r\ninstall.packages(\"arrow\")\r\n```","You're right, sorry again, it was the end of a long workday I'm usually much better about bug reports.\r\n\r\nMy initial install was in the context of an renv project and you're right it was pulling from the Posit Package Manager (`https:\/\/packagemanager.posit.co\/cran\/latest`) which doesn't have macOS binaries so it built 15.0.1 from source on my machine. That's the version from which I was originally reporting, with the apparent presence of S3 support but R session  crashes upon use of `s3_bucket`. Reading & writing local parquet files worked fine with that version built from source.\r\n\r\nSubsequently, when I've tried installing the binary from CRAN (`https:\/\/cran.rstudio.com`) with & without the environment variable set as you requested, I see what folks have reported from the other issue, i.e. `arrow_info()` reports essentially zero capabilities as below:\r\n\r\n```\r\n> arrow_info()\r\nArrow package version: 15.0.1\r\n\r\nCapabilities:\r\n               \r\nacero      TRUE\r\ndataset   FALSE\r\nsubstrait FALSE\r\nparquet   FALSE\r\njson      FALSE\r\ns3        FALSE\r\ngcs       FALSE\r\nutf8proc  FALSE\r\nre2       FALSE\r\nsnappy    FALSE\r\ngzip      FALSE\r\nbrotli    FALSE\r\nzstd      FALSE\r\nlz4       FALSE\r\nlz4_frame FALSE\r\nlzo       FALSE\r\nbz2       FALSE\r\njemalloc  FALSE\r\nmimalloc  FALSE\r\n\r\nMemory:\r\n                 \r\nAllocator  system\r\nCurrent   0 bytes\r\nMax       0 bytes\r\n\r\nRuntime:\r\n                        \r\nSIMD Level          none\r\nDetected SIMD Level none\r\n\r\nBuild:\r\n                                    \r\nC++ Library Version           15.0.1\r\nC++ Compiler              AppleClang\r\nC++ Compiler Version 14.0.0.14000029\r\n```"],"labels":["Type: bug","Component: R"]},{"title":"[Python] test_cuda_numba_interop fails locally","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI get some failures locally when running `test_cuda_numba_interop`. This is probably because I have some rather old CUDA hardware, but I wonder why some of those tests (and not all of them) are requiring 8.4 while my device has compute capability 8.0. This used to work some (long?) time ago.\r\n\r\nExample traceback:\r\n```\r\n_________________________________________________________________ test_numba_memalloc[uint8-pyarrow.cuda] _________________________________________________________________\r\nTraceback (most recent call last):\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/driver.py\", line 2824, in add_ptx\r\n    driver.cuLinkAddData(self.handle, enums.CU_JIT_INPUT_PTX,\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/driver.py\", line 327, in safe_cuda_api_call\r\n    self._check_ctypes_error(fname, retcode)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/driver.py\", line 395, in _check_ctypes_error\r\n    raise CudaAPIError(retcode, msg)\r\nnumba.cuda.cudadrv.driver.CudaAPIError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  [...]\r\n  File \"\/home\/antoine\/arrow\/dev\/python\/pyarrow\/tests\/test_cuda_numba_interop.py\", line 169, in test_numba_memalloc\r\n    darr[:5] = 99\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/devices.py\", line 232, in _require_cuda_context\r\n    return fn(*args, **kws)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/devicearray.py\", line 667, in __setitem__\r\n    return self._do_setitem(key, value)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/devicearray.py\", line 726, in _do_setitem\r\n    _assign_kernel(lhs.ndim).forall(n_elements, stream=stream)(lhs, rhs)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/dispatcher.py\", line 486, in __call__\r\n    specialized = self.dispatcher.specialize(*args)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/dispatcher.py\", line 715, in specialize\r\n    specialization.compile(argtypes)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/dispatcher.py\", line 926, in compile\r\n    kernel.bind()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/dispatcher.py\", line 197, in bind\r\n    self._codelibrary.get_cufunc()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/codegen.py\", line 195, in get_cufunc\r\n    cubin = self.get_cubin(cc=device.compute_capability)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/codegen.py\", line 170, in get_cubin\r\n    linker.add_ptx(ptx.encode())\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/numba\/cuda\/cudadrv\/driver.py\", line 2827, in add_ptx\r\n    raise LinkerError(\"%s\\n%s\" % (e, self.error_log))\r\nnumba.cuda.cudadrv.driver.LinkerError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\r\nptxas application ptx input, line 9; fatal   : Unsupported .version 8.4; current version is '8.0'\r\n---------------------------------------------------------------------------- Captured log call ----------------------------------------------------------------------------\r\nERROR    numba.cuda.cudadrv.driver:driver.py:392 Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\r\n```\r\n\r\nI have a Pascal GPU (GeForce GT 1030).\r\n\r\n```\r\n$ nvidia-smi \r\nTue Mar 19 17:19:19 2024       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ...  Off  | 00000000:04:00.0  On |                  N\/A |\r\n| 35%   36C    P0    N\/A \/  19W |    579MiB \/  2048MiB |      5%      Default |\r\n|                               |                      |                  N\/A |\r\n+-------------------------------+----------------------+----------------------+\r\n```\n\n### Component(s)\n\nGPU, Python","comments":["@kkraus14 Any ideas here?","The PTX ISA version is 8.4 but your driver only supports 8.0. I'm not sure why Numba would.be generating a newer PTX version.\n\n@gmarkall any chance you could help here?"],"labels":["Type: bug","Component: Python","Component: GPU"]},{"title":"[C++][Python] Sporadic asof join test failure","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI sporadically get this failure when running the PyArrow tests locally:\r\n```\r\n__________________________________________________________________________ test_table_join_asof ___________________________________________________________________________\r\nTraceback (most recent call last):\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/runner.py\", line 340, in from_call\r\n    result: Optional[TResult] = func()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/runner.py\", line 240, in <lambda>\r\n    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_hooks.py\", line 501, in __call__\r\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_manager.py\", line 119, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 181, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_result.py\", line 99, in get_result\r\n    raise exc.with_traceback(exc.__traceback__)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 166, in _multicall\r\n    teardown.throw(outcome._exception)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/threadexception.py\", line 87, in pytest_runtest_call\r\n    yield from thread_exception_runtest_hook()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/threadexception.py\", line 63, in thread_exception_runtest_hook\r\n    yield\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 166, in _multicall\r\n    teardown.throw(outcome._exception)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/unraisableexception.py\", line 90, in pytest_runtest_call\r\n    yield from unraisable_exception_runtest_hook()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/unraisableexception.py\", line 65, in unraisable_exception_runtest_hook\r\n    yield\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 166, in _multicall\r\n    teardown.throw(outcome._exception)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/logging.py\", line 849, in pytest_runtest_call\r\n    yield from self._runtest_for(item, \"call\")\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/logging.py\", line 832, in _runtest_for\r\n    yield\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 166, in _multicall\r\n    teardown.throw(outcome._exception)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/capture.py\", line 883, in pytest_runtest_call\r\n    return (yield)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 166, in _multicall\r\n    teardown.throw(outcome._exception)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/skipping.py\", line 256, in pytest_runtest_call\r\n    return (yield)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 102, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/runner.py\", line 182, in pytest_runtest_call\r\n    raise e\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/runner.py\", line 172, in pytest_runtest_call\r\n    item.runtest()\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/python.py\", line 1772, in runtest\r\n    self.ihook.pytest_pyfunc_call(pyfuncitem=self)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_hooks.py\", line 501, in __call__\r\n    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_manager.py\", line 119, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 138, in _multicall\r\n    raise exception.with_traceback(exception.__traceback__)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/pluggy\/_callers.py\", line 102, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/home\/antoine\/mambaforge\/envs\/pyarrow\/lib\/python3.10\/site-packages\/_pytest\/python.py\", line 195, in pytest_pyfunc_call\r\n    result = testfunction(**testargs)\r\n  File \"\/home\/antoine\/arrow\/dev\/python\/pyarrow\/tests\/test_table.py\", line 2800, in test_table_join_asof\r\n    assert r.combine_chunks() == pa.table({\r\nAssertionError: assert pyarrow.Table\\ncolA: int64\\ncol2: string\\ncolC: double\\n----\\ncolA: [[1,1,5,6,7]]\\ncol2: [[\"a\",\"b\",\"a\",\"b\",\"f\"]]\\ncolC: [[null,null,null,null,null]] == pyarrow.Table\\ncolA: int64\\ncol2: string\\ncolC: double\\n----\\ncolA: [[1,1,5,6,7]]\\ncol2: [[\"a\",\"b\",\"a\",\"b\",\"f\"]]\\ncolC: [[1,null,null,null,null]]\r\n  \r\n  Full diff:\r\n    pyarrow.Table\r\n    colA: int64\r\n    col2: string\r\n    colC: double\r\n    ----\r\n    colA: [[1,1,5,6,7]]\r\n    col2: [[\"a\",\"b\",\"a\",\"b\",\"f\"]]\r\n  - colC: [[1,null,null,null,null]]\r\n  ?         ^\r\n  + colC: [[null,null,null,null,null]]\r\n  ?         ^^^^\r\n========================================================================= short test summary info =========================================================================\r\nFAILED pyarrow\/tests\/test_table.py::test_table_join_asof - assert pyarrow.Table\\ncolA: int64\\ncol2: string\\ncolC: double\\n----\\ncolA: [[1,1,5,6,7]]\\ncol2: [[\"a\",\"b\",\"a\",\"b\",\"f\"]]\\ncolC: [[null,null,null,null,null]] == pyarrow...\r\n```\n\n### Component(s)\n\nC++, Python","comments":["cc @JerAguilon ","This also happened in one nightly build in the last run: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8351957186\/job\/22861244165"],"labels":["Type: bug","Component: C++","Component: Python"]},{"title":"[GLib] apache-arrow-glib 15.0.2 failed to build","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n\ud83d\udc4b trying to build the latest release, but run into some build issue. The error log is as below:\r\n\r\n\r\n<details>\r\n<summary>error build log<\/summary>\r\n\r\n```\r\n[16\/101] clang++ -Iarrow-glib\/libarrow-glib.1500.dylib.p -I. -I..\/c_glib -I\/opt\/homebrew\/Cellar\/brotli\/1.1.0\/include -I\/opt\/homebrew\/Cellar\/lz4\/1.9.4\/include -I\/opt\/homebrew\/Cellar\/zstd\/1.5.5\/include -I\/opt\/homebrew\/Cellar\/utf8proc\/2.9.0\/include -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.2\/include -I\/opt\/homebrew\/Cellar\/pcre2\/10.43\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include\/glib-2.0 -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/lib\/glib-2.0\/include -I\/opt\/homebrew\/opt\/gettext\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX14.sdk\/usr\/include\/ffi -fdiagnostics-color=always -Wall -Winvalid-pch -std=c++17 -O3 -Wmissing-declarations -Wno-incompatible-function-pointer-types -DUTF8PROC_EXPORTS -MD -MQ arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o -MF arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o.d -o arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o -c ..\/c_glib\/arrow-glib\/composite-array.cpp\r\nFAILED: arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o\r\nclang++ -Iarrow-glib\/libarrow-glib.1500.dylib.p -I. -I..\/c_glib -I\/opt\/homebrew\/Cellar\/brotli\/1.1.0\/include -I\/opt\/homebrew\/Cellar\/lz4\/1.9.4\/include -I\/opt\/homebrew\/Cellar\/zstd\/1.5.5\/include -I\/opt\/homebrew\/Cellar\/utf8proc\/2.9.0\/include -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.2\/include -I\/opt\/homebrew\/Cellar\/pcre2\/10.43\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include\/glib-2.0 -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/lib\/glib-2.0\/include -I\/opt\/homebrew\/opt\/gettext\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX14.sdk\/usr\/include\/ffi -fdiagnostics-color=always -Wall -Winvalid-pch -std=c++17 -O3 -Wmissing-declarations -Wno-incompatible-function-pointer-types -DUTF8PROC_EXPORTS -MD -MQ arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o -MF arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o.d -o arrow-glib\/libarrow-glib.1500.dylib.p\/composite-array.cpp.o -c ..\/c_glib\/arrow-glib\/composite-array.cpp\r\n..\/c_glib\/arrow-glib\/composite-array.cpp:613:10: error: cannot initialize return object of type 'const gint64 *' (aka 'const long *') with an rvalue of type 'const typename LargeListArray::offset_type *' (aka 'const long long *')\r\n  return garrow_base_list_array_get_value_offsets<arrow::LargeListArray>(\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\n```\r\n\r\n<\/details>\r\n\r\nfull build log, https:\/\/github.com\/Homebrew\/homebrew-core\/actions\/runs\/8344623050\/job\/22837685244\r\nrelates to Homebrew\/homebrew-core#166595\r\n\n\n### Component(s)\n\nGLib","comments":["The error seems to be:\r\n```\r\n   FAILED: arrow-glib\/libarrow-glib.1500.dylib.p\/array-builder.cpp.o \r\n  clang++ -Iarrow-glib\/libarrow-glib.1500.dylib.p -I. -I..\/c_glib -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.2\/include -I\/opt\/homebrew\/Cellar\/pcre2\/10.43\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/include\/glib-2.0 -I\/opt\/homebrew\/Cellar\/glib\/2.80.0\/lib\/glib-2.0\/include -I\/opt\/homebrew\/opt\/gettext\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX13.sdk\/usr\/include\/ffi -fdiagnostics-color=always -D_GLIBCXX_ASSERTIONS=1 -D_LIBCPP_ENABLE_ASSERTIONS=1 -Wall -Winvalid-pch -std=c++17 -O3 -Wmissing-declarations -MD -MQ arrow-glib\/libarrow-glib.1500.dylib.p\/array-builder.cpp.o -MF arrow-glib\/libarrow-glib.1500.dylib.p\/array-builder.cpp.o.d -o arrow-glib\/libarrow-glib.1500.dylib.p\/array-builder.cpp.o -c ..\/c_glib\/arrow-glib\/array-builder.cpp\r\n  ..\/c_glib\/arrow-glib\/array-builder.cpp:5171:41: error: cannot initialize a parameter of type 'const int64_t *' (aka 'const long long *') with an lvalue of type 'const gint64 *' (aka 'const long *')\r\n      return arrow_builder->AppendIndices(values, values_length, valid_bytes);\r\n                                          ^~~~~~\r\n  \/opt\/homebrew\/Cellar\/apache-arrow\/15.0.2\/include\/arrow\/array\/builder_dict.h:691:39: note: passing argument to parameter 'values' here\r\n    Status AppendIndices(const int64_t* values, int64_t length,\r\n                                        ^\r\n  ..\/c_glib\/arrow-glib\/array-builder.cpp:5405:41: error: cannot initialize a parameter of type 'const int64_t *' (aka 'const long long *') with an lvalue of type 'const gint64 *' (aka 'const long *')\r\n      return arrow_builder->AppendIndices(values, values_length, valid_bytes);\r\n                                          ^~~~~~\r\n  \/opt\/homebrew\/Cellar\/apache-arrow\/15.0.2\/include\/arrow\/array\/builder_dict.h:691:39: note: passing argument to parameter 'values' here\r\n    Status AppendIndices(const int64_t* values, int64_t length,\r\n                                        ^\r\n  2 errors generated.\r\n```\r\ncc @kou ","Hmm. It seems that 64-bit integer type provided by GLib (`gint64`) and C++ (`int64_t`) became incompatible...\r\nI'll add explicit casts...","> Hmm. It seems that 64-bit integer type provided by GLib (`gint64`) and C++ (`int64_t`) became incompatible...\r\n\r\nThis seems to be due to a misconfiguration on our end. Should be fixed by https:\/\/github.com\/Homebrew\/homebrew-core\/pull\/166473.\r\n\r\nNevertheless,\r\n\r\n> I'll add explicit casts...\r\n\r\nis probably still a good idea. See https:\/\/github.com\/Homebrew\/homebrew-core\/pull\/166595#issuecomment-2011017711","Thanks for the info!\r\nI didn't know the `gint64` note:\r\n\r\nhttps:\/\/docs.gtk.org\/glib\/types.html#gint64\r\n\r\n> Note that on platforms with more than one 64-bit standard integer type, gint64 and int64_t are not necessarily implemented by the same 64-bit integer type. For example, on a platform where both long and long long are 64-bit, it might be the case that one of those types is used for gint64 and the other is used for int64_t."],"labels":["Type: bug","Component: GLib"]},{"title":"[R] arrow R package 15.0.1 missing dataset, parquet, etc. on Mac M2","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nAfter installing Arrow in R, the Arrow binary looks like it was compiled without many features.\r\n\r\nArrow 14.0.2 was working before updating. \r\n\r\nI had previously installed arrow and R via homebrew, but had switched to installing R via `brew install --cask r` and arrow via `install.packages(\"arrow\")` from within R. Homebrew arrow was still installed until yesterday, but I think R was using the binary installed with the arrow package. Giving this background in case this is specific to something about my setup.\r\n\r\nSystem: Apple M2 Pro, Sonoma 14.4\r\n\r\nR info:\r\n\r\n```\r\nR version 4.3.3 (2024-02-29) -- \"Angel Food Cake\"\r\nPlatform: aarch64-apple-darwin20 (64-bit)\r\n```\r\n\r\n`arrow::arrow_info()` output:\r\n\r\n```\r\nArrow package version: 15.0.1\r\n\r\nCapabilities:\r\n\r\nacero      TRUE\r\ndataset   FALSE\r\nsubstrait FALSE\r\nparquet   FALSE\r\njson      FALSE\r\ns3        FALSE\r\ngcs       FALSE\r\nutf8proc  FALSE\r\nre2       FALSE\r\nsnappy    FALSE\r\ngzip      FALSE\r\nbrotli    FALSE\r\nzstd      FALSE\r\nlz4       FALSE\r\nlz4_frame FALSE\r\nlzo       FALSE\r\nbz2       FALSE\r\njemalloc  FALSE\r\nmimalloc  FALSE\r\n\r\nMemory:\r\n\r\nAllocator  system\r\nCurrent   0 bytes\r\nMax       0 bytes\r\n\r\nRuntime:\r\n\r\nSIMD Level          none\r\nDetected SIMD Level none\r\n\r\nBuild:\r\n\r\nC++ Library Version           15.0.1\r\nC++ Compiler              AppleClang\r\nC++ Compiler Version 14.0.0.14000029\r\n```\r\n\r\n\n\n### Component(s)\n\nR","comments":["I have the same issue here. I tried many things including removing and reinstalling R from scratch again including install `arrow` package using `install_arrow()` with many different options, and all failed.","I was able to install arrow 15.0.1 on a linux docker container (rocker\/rstudio). The compiled arrow had parquet and dataset enabled. Is there an easy way to tell R arrow to use a different binary? I'd like to test the homebrew version installed by `brew install apache-arrow`.","I am able to get a working install of arrow 15.0.2.100000328 by using:`arrow::install_arrow(nightly = TRUE)`","Same issue here. Tried both the Mac (Intel chip) binary builds from CRAN and R-universe.","> I am able to get a working install of arrow 15.0.2.100000328 by using:`arrow::install_arrow(nightly = TRUE)`\r\n\r\nI couldn't, and here's the error message\r\n```\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  dlopen(\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so, 0x0006): symbol not found in flat namespace '__ZN5arrow12ArrayBuilder13AppendScalarsERKSt6vectorISt10shared_ptrINS_6ScalarEESaIS4_EE'\r\nError: loading failed\r\nExecution halted\r\nERROR: loading failed\r\n* removing \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/arrow\u2019\r\n* restoring previous \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/arrow\u2019\r\n```","@letitbk, given the `00LOCK-arrow` path, I wonder if a library was in use? I ran `install_arrow()` from R running in a shell, no other instances running. It's been a while since I've dealt with R `00LOCK` stuff, so I could be wrong.","@blongworth thanks for your suggestion, but I encountered the another message, and here's the entire error message. \r\n```\r\n> arrow::install_arrow(nightly = TRUE)\r\ntrying URL 'https:\/\/nightlies.apache.org\/arrow\/r\/src\/contrib\/arrow_15.0.2.100000328.tar.gz'\r\nContent type 'application\/x-gzip' length 4238620 bytes (4.0 MB)\r\n==================================================\r\ndownloaded 4.0 MB\r\n\r\n* installing *source* package \u2018arrow\u2019 ...\r\n** using staged installation\r\n*** Disabling Arrow build with GCS on gcc-13.\r\n*** Set ARROW_GCS=ON to explicitly enable.\r\n*** pkg-config found.\r\n*** Latest available nightly for 15.0.2.100000328: 15.0.2.100000328\r\n*** Found libcurl and OpenSSL >= 3.0.0\r\n*** Successfully retrieved libarrow (darwin-arm64-openssl-3.0)\r\nPKG_CFLAGS=-DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS\r\nPKG_LIBS=-L\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/lib -L\/opt\/homebrew\/opt\/openssl@3\/lib -larrow_dataset -lparquet -larrow_acero -larrow -larrow_bundled_dependencies -framework Security -lcurl -lssl -lcrypto \r\n** libs\r\nusing C++ compiler: \u2018Homebrew clang version 17.0.6\u2019\r\nusing C++17\r\nusing SDK: \u2018MacOSX14.4.sdk\u2019\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c RTasks.cpp -o RTasks.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c altrep.cpp -o altrep.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c array.cpp -o array.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c array_to_vector.cpp -o array_to_vector.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c arraydata.cpp -o arraydata.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c arrowExports.cpp -o arrowExports.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c bridge.cpp -o bridge.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c buffer.cpp -o buffer.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c chunkedarray.cpp -o chunkedarray.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compression.cpp -o compression.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compute-exec.cpp -o compute-exec.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compute.cpp -o compute.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c config.cpp -o config.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c csv.cpp -o csv.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c dataset.cpp -o dataset.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c datatype.cpp -o datatype.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c expression.cpp -o expression.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c extension-impl.cpp -o extension-impl.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c feather.cpp -o feather.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c field.cpp -o field.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c filesystem.cpp -o filesystem.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c io.cpp -o io.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c json.cpp -o json.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c memorypool.cpp -o memorypool.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c message.cpp -o message.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c parquet.cpp -o parquet.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c r_to_arrow.cpp -o r_to_arrow.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatch.cpp -o recordbatch.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatchreader.cpp -o recordbatchreader.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatchwriter.cpp -o recordbatchwriter.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c safe-call-into-r-impl.cpp -o safe-call-into-r-impl.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c scalar.cpp -o scalar.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c schema.cpp -o schema.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c symbols.cpp -o symbols.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c table.cpp -o table.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c threadpool.cpp -o threadpool.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -DPARQUET_STATIC -DARROW_STATIC -I\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/include -I\/opt\/homebrew\/opt\/openssl@3\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -DARROW_R_WITH_GCS -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/opt\/llvm\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c type_infer.cpp -o type_infer.o\r\n\/opt\/homebrew\/bin\/g++-13 -fopenmp -std=gnu++17 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -L\/Library\/Frameworks\/R.framework\/Resources\/lib -L\/opt\/homebrew\/opt\/gettext\/lib -L\/opt\/homebrew\/opt\/llvm\/lib -Wl,-rpath,\/opt\/homebrew\/opt\/llvm\/lib -o arrow.so RTasks.o altrep.o array.o array_to_vector.o arraydata.o arrowExports.o bridge.o buffer.o chunkedarray.o compression.o compute-exec.o compute.o config.o csv.o dataset.o datatype.o expression.o extension-impl.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o r_to_arrow.o recordbatch.o recordbatchreader.o recordbatchwriter.o safe-call-into-r-impl.o scalar.o schema.o symbols.o table.o threadpool.o type_infer.o -L\/private\/var\/folders\/7c\/9klc1txn7ml64wkb2zcq24_w0000gr\/T\/RtmpFAtig8\/R.INSTALL60b457156403\/arrow\/libarrow\/arrow-15.0.2.100000328\/lib -L\/opt\/homebrew\/opt\/openssl@3\/lib -larrow_dataset -lparquet -larrow_acero -larrow -larrow_bundled_dependencies -framework Security -lcurl -lssl -lcrypto -F\/Library\/Frameworks\/R.framework\/.. -framework R -Wl,-framework -Wl,CoreFoundation\r\nld: warning: search path '\/opt\/homebrew\/opt\/gettext\/lib' not found\r\nld: warning: reexported library with install name '\/opt\/homebrew\/opt\/llvm\/lib\/libunwind.1.dylib' found at '\/opt\/homebrew\/Cellar\/llvm\/17.0.6_1\/lib\/libunwind.1.0.dylib' couldn't be matched with any parent library and will be linked directly\r\ninstalling to \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\r\n** R\r\n** inst\r\n** byte-compile and prepare package for lazy loading\r\n** help\r\n*** installing help indices\r\n** building package indices\r\n** testing if installed package can be loaded from temporary location\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  dlopen(\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so, 0x0006): symbol not found in flat namespace '__ZN5arrow12ArrayBuilder13AppendScalarsERKSt6vectorISt10shared_ptrINS_6ScalarEESaIS4_EE'\r\nError: loading failed\r\nExecution halted\r\nERROR: loading failed\r\n* removing \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/arrow\u2019\r\n* restoring previous \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/arrow\u2019\r\n```\r\n\r\nPerhaps the issue is related to the version mismatch?","Hi all, sorry for the trouble. The most recent submission (15.0.1) ran into some trouble after acceptance when being built on macOS. This lead to the reduced feature-set. We're actively working on preparing a new submission so the macOS version of the package has the usual feature-set but it may take some time.\r\n\r\nIn the mean time, 15.0.1 is now on R-universe so you should be able to install a binary using:\r\n\r\n```r\r\ninstall.packages('arrow', repos = c('https:\/\/apache.r-universe.dev'))\r\n```\r\n\r\n@letitbk I'm not sure what's going on with your issue directly above but can you try installing from R-universe and report back?","The new version from r-universe installs fine and is compiled with all needed features. Thank you!!\r\n\r\n@letitbk, agree with @amoeba about trying the r-universe version, but it looks like your compile can't find some needed libraries it's expecting to find in homebrew's install path. Getting a working precompiled arrow binary is much easier. \r\n\r\nI had issues with R installed via `brew install r` and arrow a while back. Make sure your R is installed from CRAN or via `brew install --cask r` to avoid some odd binary incompatibility issues.","thank you so much @blongworth the new version from r-universe installs fine for me as well!!","Thank @amoeba and the Arrow folks. A couple of hours turn around for new builds is pretty great! CRAN, of course, is another story."],"labels":["Type: bug","Component: R"]},{"title":"[Python] [ADBC_DRIVER] [FlightSQL] flight: no authorization header on the response (Unknown; AuthenticateBasicToken) when creating new connection.","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nBelow is my python code to connect to my flight sql jdbc server.\r\n\r\n```\r\nwith flight_sql.connect(uri=\"grpc+tls:\/\/192.168.140.77:32222\",\r\n                          db_kwargs={\r\n                                    \"username\": \"username\",\r\n                                    \"password\": \"Password\",\r\n                                    \"adbc.flight.sql.client_option.tls_skip_verify\": \"true\"\r\n                                    }\r\n                          ) as conn:\r\n```\r\nMy python client code throws an exception and it does get the data.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/multiprocessing\/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/multiprocessing\/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"\/Users\/a.bashir\/Desktop\/python_codes\/newproj\/venv\/arrow_py_client.py\", line 19, in execute_arrow_query\r\n    with flight_sql.connect(uri=\"grpc+tls:\/\/192.168.140.77:32222\",\r\n  File \"\/Users\/a.bashir\/Desktop\/python_codes\/newproj\/venv\/lib\/python3.10\/site-packages\/adbc_driver_flightsql\/dbapi.py\", line 120, in connect\r\n    conn = adbc_driver_manager.AdbcConnection(db, **(conn_kwargs or {}))\r\n  File \"adbc_driver_manager\/_lib.pyx\", line 644, in adbc_driver_manager._lib.AdbcConnection.__init__\r\n  File \"adbc_driver_manager\/_lib.pyx\", line 385, in adbc_driver_manager._lib.check_error\r\nadbc_driver_manager._lib.OperationalError: ADBC_STATUS_UNKNOWN (1): [FlightSQL] [FlightSQL] flight: no authorization header on the response (Unknown; AuthenticateBasicToken)\r\n```\r\n\r\nThis issue is also part of below issue: \r\nhttps:\/\/github.com\/apache\/arrow\/issues\/40664\n\n### Component(s)\n\nJava, Python","comments":[],"labels":["Component: Java","Component: Python","Type: usage"]},{"title":"[JAVA] [ARROW-JDBC] Ssl Handler Exception - Swallowing a harmless 'connection reset by peer \/ broken pipe'","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI have implemented flight sql server in java with tls protocol. Below is my server build code. The server internally connects with trino to fetch iceberg data and send it to the client. \r\n\r\n```\r\nLocation location = Location.forGrpcTls(\"localhost\", 32222);\r\n        final ArrowFlightServer example = new ArrowFlightServer(location);\r\n        Location listenLocation = Location.forGrpcTls(\"0.0.0.0\", 32222);\r\n        File certificate = new File(SSL_CERT_PATH + \"\/cert.crt\");\r\n        File priveteKey = new File(SSL_CERT_PATH + \"\/cert.key\");\r\n        try (final BufferAllocator allocator = example.rootAllocator; final FlightServer server\r\n                = FlightServer.builder(allocator, listenLocation, example)\r\n                                                .useTls(certificate, priveteKey)\r\n                        .headerAuthenticator(example.callHeaderInfo)\r\n                        .build()) {\r\n            server.start();\r\n            server.awaitTermination();\r\n        }\r\n```\r\nI am using _adbc_driver_flightsql 0.9.0_ to connect to server from python client. Using below code, I am able to fetch data. I am using custom call headers in this scenario. \r\n```\r\nwith flight_sql.connect(uri=\"grpc+tls:\/\/192.168.140.77:32222\",\r\n                          db_kwargs={\r\n                                    \"adbc.flight.sql.rpc.call_header.username\": \"username\",\r\n                                    \"adbc.flight.sql.rpc.call_header.password\": \"Password\",\r\n                                    \"adbc.flight.sql.client_option.tls_skip_verify\": \"true\"\r\n                                    }\r\n                          ) as conn:\r\n\r\n    with conn.cursor() as cur:\r\n        cur.execute(\r\n             \"SELECT * FROM postgresql.public.tesla limit 10\"\r\n                    )\r\n        \r\n        x = cur.fetch_arrow_table()\r\n        num_rows = x.num_rows\r\n        print(x)\r\n```\r\n\r\nBut I get one server side exception when I change log level to DEBUG. Even though I get data as expected but in server logs I get below exception. When log level is set to INFO, I don't get this exception. Additionally, If I disable TLS and use non-secure method, I don't get this server side exception with log level set to DEBUG.\r\n\r\n`2024-03-19 15:42:32 DEBUG io.netty.handler.ssl.SslHandler - [id: 0xa6229d8a, L:\/192.168.140.77:32222 - R:\/192.168.140.77:65174] Swallowing a harmless 'connection reset by peer \/ broken pipe' error that occurred while writing close_notify in response to the peer's close_notify`\r\n\r\nSecondly if I change my python code to use standard jdbc username\/password parameters like below,\r\n```\r\nwith flight_sql.connect(uri=\"grpc+tls:\/\/192.168.140.77:32222\",\r\n                          db_kwargs={\r\n                                    \"username\": \"username\",\r\n                                    \"password\": \"Password\",\r\n                                    \"adbc.flight.sql.client_option.tls_skip_verify\": \"true\"\r\n                                    }\r\n                          ) as conn:\r\n```\r\n My python client code throws an exception and it does get the data at all.\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/multiprocessing\/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/multiprocessing\/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"\/Users\/a.bashir\/Desktop\/python_codes\/newproj\/venv\/arrow_py_client.py\", line 19, in execute_arrow_query\r\n    with flight_sql.connect(uri=\"grpc+tls:\/\/192.168.140.77:32222\",\r\n  File \"\/Users\/a.bashir\/Desktop\/python_codes\/newproj\/venv\/lib\/python3.10\/site-packages\/adbc_driver_flightsql\/dbapi.py\", line 120, in connect\r\n    conn = adbc_driver_manager.AdbcConnection(db, **(conn_kwargs or {}))\r\n  File \"adbc_driver_manager\/_lib.pyx\", line 644, in adbc_driver_manager._lib.AdbcConnection.__init__\r\n  File \"adbc_driver_manager\/_lib.pyx\", line 385, in adbc_driver_manager._lib.check_error\r\nadbc_driver_manager._lib.OperationalError: ADBC_STATUS_UNKNOWN (1): [FlightSQL] [FlightSQL] flight: no authorization header on the response (Unknown; AuthenticateBasicToken)\r\n```\r\nWonder what I am doing wrong.\n\n### Component(s)\n\nJava, Python","comments":[],"labels":["Component: Java","Component: Python","Type: usage"]},{"title":"[Release][Packaging] Updating homebrew formula for Arrow 15.0.2 fails","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI have been trying to update the homebrew-core Formula for Arrow 15.0.2 both with linxubrew and macOS brew using:\r\n```\r\ndev\/release\/post-13-homebrew.sh 15.0.2 raulcd\r\n```\r\nboth OS fail with the following error:\r\n```\r\nYour branch is behind 'origin\/master' by 3 commits, and can be fast-forwarded.\r\n  (use \"git pull\" to update your local branch)\r\nerror: branch 'apache-arrow-15.0.2' not found\r\nSwitched to a new branch 'apache-arrow-15.0.2'\r\nUpdating apache-arrow formulae\r\nWarning: bump-formula-pr is a developer command, so Homebrew's\r\ndeveloper mode has been automatically turned on.\r\nTo turn developer mode off, run:\r\n  brew developer off\r\n\r\nError: Parameter 'version': Expected type T.nilable(String), got type Version with value #<Version:0x0000000107fe472...0.2\", @detected_from_url=true>\r\nCaller: \/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/call_validation.rb:133\r\nDefinition: \/opt\/homebrew\/Library\/Homebrew\/utils\/github.rb:523\r\nPlease report this issue:\r\n  https:\/\/docs.brew.sh\/Troubleshooting\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/configuration.rb:296:in `call_validation_error_handler_default'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/configuration.rb:303:in `call_validation_error_handler'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/call_validation.rb:300:in `report_error'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/call_validation.rb:136:in `block in validate_call_skip_block_type'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/signature.rb:234:in `block in each_args_value_type'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/signature.rb:228:in `each'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/signature.rb:228:in `each_args_value_type'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/call_validation.rb:133:in `validate_call_skip_block_type'\r\n\/opt\/homebrew\/Library\/Homebrew\/vendor\/bundle\/ruby\/3.1.0\/gems\/sorbet-runtime-0.5.11294\/lib\/types\/private\/methods\/call_validation.rb:109:in `block in create_validator_slow_skip_block_type'\r\n\/opt\/homebrew\/Library\/Homebrew\/utils\/github.rb:628:in `check_for_duplicate_pull_requests'\r\n\/opt\/homebrew\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb:474:in `check_closed_pull_requests'\r\n\/opt\/homebrew\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb:459:in `check_new_version'\r\n\/opt\/homebrew\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb:164:in `bump_formula_pr'\r\n\/opt\/homebrew\/Library\/Homebrew\/brew.rb:86:in `<main>'\r\n```\n\n### Component(s)\n\nPackaging, Release","comments":["This was happen by https:\/\/github.com\/apache\/arrow\/blob\/ed47ad22c8537b32abf27580e75fcf514be11f7e\/dev\/release\/post-13-homebrew.sh#L50-L57\r\nright?\r\n\r\nIt seems that this is a bug of https:\/\/github.com\/Homebrew\/brew .\r\nCould you report this to the upstream?\r\n\r\nI think that this is needed:\r\n\r\n```diff\r\ndiff --git a\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb b\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb\r\nindex 4ed5dbb345..e3c853eb1b 100644\r\n--- a\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb\r\n+++ b\/Library\/Homebrew\/dev-cmd\/bump-formula-pr.rb\r\n@@ -464,6 +464,7 @@ module Homebrew\r\n       specs[:tag] = tag if tag.present?\r\n       version = Version.detect(url, **specs)\r\n       return if version.null?\r\n+      version = version.to_s\r\n     end\r\n \r\n     check_throttle(formula, version)\r\n```","\r\n> It seems that this is a bug of https:\/\/github.com\/Homebrew\/brew . Could you report this to the upstream?\r\n\r\nThanks @kou ! Reported to brew.\r\n"],"labels":["Type: bug","Component: Packaging","Component: Release"]},{"title":"[JAVA] [arrow-jdbc] Unmapped data type 2014 (TIMESTAMP_WITH_TIMEZONE)","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI have an iceberg table with column type \"TIMESTAMP_WITH_TIMEZONE\"\r\n\r\nWhen I run my flight_sql jdbc client to query this table, I get below error in my flight sql client code.\r\n\r\n```\r\nException in thread \"Thread-0\" java.lang.RuntimeException: java.sql.SQLException: Error while executing SQL \"SELECT * FROM datawarehouse.school_data_raw.schools limit 100\": Unknown error: java.lang.UnsupportedOperationException: Unmapped JDBC type: 2014\r\n\tat ai.saal.arrowjdbcclient.ArrowJdbcClient.runQueryOnArrow(ArrowJdbcClient.java:123)\r\n\tat ai.saal.arrowjdbcclient.ArrowJdbcClient.run(ArrowJdbcClient.java:41)\r\n\tat java.base\/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.sql.SQLException: Error while executing SQL \"SELECT * FROM datawarehouse.school_data_raw.schools limit 100\": Unknown error: java.lang.UnsupportedOperationException: Unmapped JDBC type: 2014\r\n\tat cfjd.org.apache.calcite.avatica.Helper.createException(Helper.java:56)\r\n\tat cfjd.org.apache.calcite.avatica.Helper.createException(Helper.java:41)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:163)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaStatement.executeQuery(AvaticaStatement.java:227)\r\n\tat ai.saal.arrowjdbcclient.ArrowJdbcClient.runQueryOnArrow(ArrowJdbcClient.java:97)\r\n\t... 2 more\r\nCaused by: cfjd.org.apache.arrow.flight.FlightRuntimeException: INTERNAL: Unknown error: java.lang.UnsupportedOperationException: Unmapped JDBC type: 2014\r\n\tat cfjd.org.apache.arrow.flight.CallStatus.toRuntimeException(CallStatus.java:131)\r\n\tat cfjd.org.apache.arrow.flight.grpc.StatusUtils.fromGrpcRuntimeException(StatusUtils.java:164)\r\n\tat cfjd.org.apache.arrow.flight.grpc.StatusUtils$1.next(StatusUtils.java:250)\r\n\tat cfjd.org.apache.arrow.flight.sql.FlightSqlClient$PreparedStatement.<init>(FlightSqlClient.java:941)\r\n\tat cfjd.org.apache.arrow.flight.sql.FlightSqlClient.prepare(FlightSqlClient.java:728)\r\n\tat cfjd.org.apache.arrow.flight.sql.FlightSqlClient.prepare(FlightSqlClient.java:708)\r\n\tat org.apache.arrow.driver.jdbc.client.ArrowFlightSqlClientHandler.prepare(ArrowFlightSqlClientHandler.java:170)\r\n\tat org.apache.arrow.driver.jdbc.ArrowFlightMetaImpl.prepareAndExecute(ArrowFlightMetaImpl.java:161)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaConnection.prepareAndExecuteInternal(AvaticaConnection.java:675)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaStatement.executeInternal(AvaticaStatement.java:156)\r\n\t... 4 more\r\n```\r\n\r\nQuery I am running is **SELECT * FROM datawarehouse.school_data_raw.schools limit 100** \r\n\r\nTable structure is :\r\n\r\n_CREATE TABLE datawarehouse.school_data_raw.schools ( school_id varchar, school_name varchar, ingested_at timestamp(6) with time zone ) WITH ( format = 'PARQUET', format_version = 1, location = 's3a:\/\/data-warehouse\/raw\/school_data\/schools' )_\r\n\n\n### Component(s)\n\nJava","comments":["Would it be possible to get a reproducible script? "],"labels":["Component: Java","Type: usage"]},{"title":"[Python] Add compression option to pyarrow.csv.WriteOptions","body":"### Describe the enhancement requested\r\n\r\nCan we support gzip compression with csv writing? This would also apply to pyarrow.dataset.write_dataset.\r\n\r\nThe default extension when writing text files should also reflect the compression format..  I.e. abc.csv.gz and abc.csv.zip\r\n\r\n### Component(s)\r\n\r\nPython","comments":["It's not built in into the csv writer itself (for the reading side it is), but you can let it compress on the fly using `CompressedOutputStream`. See the usage example at https:\/\/arrow.apache.org\/docs\/python\/csv.html#usage:\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.csv as csv\r\n\r\nwith pa.CompressedOutputStream(\"tips.csv.gz\", \"gzip\") as out:\r\n    csv.write_csv(table, out)\r\n```\r\n\r\nThat's only the direct CSV writer, and doesn't work through `pyarrow.dataset`. For compressed CSV in writing datasets we have an open feature request: https:\/\/github.com\/apache\/arrow\/issues\/34085"],"labels":["Component: Python","Type: usage"]},{"title":"[C#] Need pointers on creating Columns with values (values = ArrowArray) of type Timestamp and Decimal","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nHello,\r\n\r\nI am looking to understand how to create Columns with values of following types \r\n- Timestamp \r\n- Decimal\r\n\r\nWhile reading this code in a DateFrame, I am getting an exception with message = `decimal256` for Decimal and `timestamp` for Timestamp.\r\n\r\nSuper confusing. Appreciate any help with this\r\n\r\nCode\r\n```\r\n# Create DECIMAL Column\r\n########################\r\npublic CreateDecimalColumn() {\r\n\r\nDecimal256Type decimal256Type = new Decimal256Type(10, 3)\r\nvar builder = new Decimal256Array.Builder(decimal256Type);\r\nCreateColumn<decimal>(\r\n    values: values,\r\n    nullable: nullable,\r\n    appendRange: (vals) => { builder.AppendRange(vals); },\r\n    appendValue: (val) => { builder.Append(val); },\r\n    appendNull: () => { builder.AppendNull(); });\r\nreturn builder.Build(memoryAllocator);*\/\r\n}\r\n\r\n\r\n# Create TIMESTAMP Column\r\n##########################\r\npublic CreateTimestampColumn() {\r\n\r\nvar builder = new TimestampArray.Builder();\r\nCreateColumn<DateTimeOffset>(\r\n    values: values,\r\n    nullable: nullable,\r\n    appendRange: (vals) => { builder.AppendRange(vals); },\r\n    appendValue: (val) => { builder.Append(val); },\r\n    appendNull: () => { builder.AppendNull(); });\r\nreturn builder.Build(memoryAllocator);*\/\r\n}\r\n\r\n\r\n# Helper Method \/ Common code\r\n#############################\r\npublic void CreateColumn(object values, bool nullable, Action<IList<T> appendRange, Action<T> appendValue, Action appendValue)\r\nif (!nullable)\r\n{\r\n    appendRange((IList<T>)values);\r\n    return;\r\n}\r\n\r\nforeach (var value in (IList<T?>)values)\r\n{\r\n    if (value.HasValue)\r\n    {\r\n        appendValue(value.Value);\r\n    }\r\n    else\r\n    {\r\n        appendNull();\r\n    }\r\n}\r\n```\n\n### Component(s)\n\nC#","comments":[],"labels":["Component: C#","Type: usage"]},{"title":"GH-40630: [Go][Parquet] Enable writing of Parquet footer without closing file","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nSee #40630\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\n1. Added `FlushWithFooter` method to *file.Writer\r\n2. To support `FlushWithFooter`, refactored `Close` in a way that changes the order of operations in two ways:\r\n   a. closure of open row group writers is now done after using `defer` to ensure closure of the sink, instead of before\r\n   b. wiping out of encryption keys is now done by the same deferred function, ensuring that it happens even upon error\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n`file_writer_test.go` has been extended to cover `FlushWithFooter` in a manner equivalent to the existing coverage.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nOnly the addition of a new public method as described above.  No breaking changes to any existing public interfaces, unless the two minor order-of-operation changes described above are somehow a problem.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n\r\nI'm not sure it's a critical fix, but one of the minor changes described above may reduce the likelihood that an attack could inject an error (e.g., an I\/O error) to prevent an encryption key from being wiped from memory.\r\n\n* GitHub Issue: #40630","comments":[":warning: GitHub issue #40630 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: Go","awaiting review"]},{"title":"[C++] Lengthy destruction of ScannerRecordBatchReader","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThe destruction of the ScannerRecordBatchReader object returned by arrow::dataset::Scanner::ToRecordBatchReader() on a Parquet dataset of 1 GB with ~ 10 million rows and 77 row groups (https:\/\/overturemaps-us-west-2.s3.amazonaws.com\/release\/2024-03-12-alpha.0\/theme%3Dbuildings\/type%3Dbuilding\/part-00000-4dfc75cd-2680-4d52-b5e0-f4cc9f36b267-c000.zstd.parquet) is extremely long, when reading for example just only a few rows, due to SerialIterator::~SerialIterator() iterating until the end of the dataset. It would be desirable that the destruction of the batch reader doesn't trigger such lengthy operations.\r\n\r\nthread_pool.h has the following comment, but trying to implement that is beyond my understanding of the libarrow\/libparquet deep internals:\r\n```\r\n  \/\/\/ Note: The iterator's destructor will run until the given generator is fully\r\n  \/\/\/ exhausted. If you wish to abandon iteration before completion then the correct\r\n  \/\/\/ approach is to use a stop token to cause the generator to exhaust early.\r\n```\r\n\r\nInvoking explicitly the Close() method on the record batch reader doesn't improve performance either.\r\n\r\nThis is the result of the analysis of https:\/\/github.com\/OSGeo\/gdal\/issues\/9497\r\n\r\nVersion: libarrow\/libparquet from apache-arrow-15.0.0\r\n\r\nRelated stack trace:\r\n```\r\n#0  arrow::io::RandomAccessFile::ReadAsync (this=0x555555fb42e0, ctx=..., position=31630301, nbytes=29124528) at \/home\/even\/arrow\/cpp\/src\/arrow\/io\/interfaces.cc:169\r\n#1  0x00007fffdf8eb916 in arrow::io::internal::ReadRangeCache::LazyImpl::MaybeRead (this=0x7fffd41ffe00, entry=0x7fffd420b3e0) at \/home\/even\/arrow\/cpp\/src\/arrow\/io\/caching.cc:270\r\n#2  0x00007fffdf8eb74b in arrow::io::internal::ReadRangeCache::Impl::WaitFor (this=0x7fffd41ffe00, ranges=std::vector of length 33, capacity 33 = {...}) at \/home\/even\/arrow\/cpp\/src\/arrow\/io\/caching.cc:249\r\n#3  0x00007fffdf8ebd8f in arrow::io::internal::ReadRangeCache::LazyImpl::WaitFor (this=0x7fffd41ffe00, ranges=std::vector of length 0, capacity 0) at \/home\/even\/arrow\/cpp\/src\/arrow\/io\/caching.cc:304\r\n#4  0x00007fffdf8ea26c in arrow::io::internal::ReadRangeCache::WaitFor (this=0x7fffd4200900, ranges=std::vector of length 0, capacity 0) at \/home\/even\/arrow\/cpp\/src\/arrow\/io\/caching.cc:331\r\n#5  0x00007fffec678c0f in parquet::SerializedFile::WhenBuffered (this=0x555556014860, row_groups=std::vector of length 1, capacity 1 = {...}, column_indices=std::vector of length 33, capacity 33 = {...}) at \/home\/even\/arrow\/cpp\/src\/parquet\/file_reader.cc:418\r\n#6  0x00007fffec6750f3 in parquet::ParquetFileReader::WhenBuffered (this=0x7fffd41f52d0, row_groups=std::vector of length 1, capacity 1 = {...}, column_indices=std::vector of length 33, capacity 33 = {...}) at \/home\/even\/arrow\/cpp\/src\/parquet\/file_reader.cc:905\r\n#7  0x00007fffec41d812 in parquet::arrow::RowGroupGenerator::FetchNext (this=0x7fffd4001630) at \/home\/even\/arrow\/cpp\/src\/parquet\/arrow\/reader.cc:1123\r\n#8  0x00007fffec41d342 in parquet::arrow::RowGroupGenerator::FillReadahead (this=0x7fffd4001630) at \/home\/even\/arrow\/cpp\/src\/parquet\/arrow\/reader.cc:1102\r\n#9  0x00007fffec41d1ca in parquet::arrow::RowGroupGenerator::operator() (this=0x7fffd4001630) at \/home\/even\/arrow\/cpp\/src\/parquet\/arrow\/reader.cc:1090\r\n#10 0x00007fffec42da9c in std::_Function_handler<arrow::Future<std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()> > (), parquet::arrow::RowGroupGenerator>::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#11 0x00007fffecfce934 in std::function<arrow::Future<std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()> > ()>::operator()() const (this=0x7fffd42026a0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#12 0x00007fffecfcca75 in arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::State::PullSource (this=0x7fffd42026a0) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:1158\r\n#13 0x00007fffecfdb2e1 in arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::InnerCallback::operator() (this=0x555557324588, maybe_next_ref=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:1335\r\n#14 0x00007fffecfd9dfb in arrow::Future<std::shared_ptr<arrow::RecordBatch> >::WrapResultOnComplete::Callback<arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::InnerCallback>::operator()(arrow::FutureImpl const&) && (this=0x555557324588, impl=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:442\r\n#15 0x00007fffecfd94f1 in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::FnImpl<arrow::Future<std::shared_ptr<arrow::RecordBatch> >::WrapResultOnComplete::Callback<arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::InnerCallback> >::invoke(arrow::FutureImpl const&) (this=0x555557324580, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:152\r\n#16 0x00007fffdf9ba13c in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator()(arrow::FutureImpl const&) && (this=0x7fffffffb7b0, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:140\r\n#17 0x00007fffdf9b97ea in arrow::ConcreteFutureImpl::RunOrScheduleCallback (self=std::shared_ptr<arrow::FutureImpl> (use count 2, weak count 1) = {...}, callback_record=..., in_add_callback=true) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:110\r\n#18 0x00007fffdf9b9224 in arrow::ConcreteFutureImpl::AddCallback(arrow::internal::FnOnce<void (arrow::FutureImpl const&)>, arrow::CallbackOptions) (this=0x5555573d4400, callback=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:64\r\n#19 0x00007fffdf9b7112 in arrow::FutureImpl::AddCallback(arrow::internal::FnOnce<void (arrow::FutureImpl const&)>, arrow::CallbackOptions) (this=0x5555573d4400, callback=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:229\r\n#20 0x00007fffecfcc857 in arrow::Future<std::shared_ptr<arrow::RecordBatch> >::AddCallback<arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::InnerCallback, arrow::Future<std::shared_ptr<arrow::RecordBatch> >::WrapResultOnComplete::Callback<arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::InnerCallback> > (this=0x7fffffffb900, on_complete=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:493\r\n#21 0x00007fffecfca507 in arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> >::operator() (this=0x7fffd4200610) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:1086\r\n#22 0x00007fffec436eff in std::_Function_handler<arrow::Future<std::shared_ptr<arrow::RecordBatch> > (), arrow::MergedGenerator<std::shared_ptr<arrow::RecordBatch> > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#23 0x00007fffecfcaeb2 in std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()>::operator()() const (this=0x7fffd4202a60) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#24 0x00007fffed1034e6 in arrow::dataset::SlicingGenerator::operator() (this=0x7fffd42005f0) at \/home\/even\/arrow\/cpp\/src\/arrow\/dataset\/file_parquet.cc:564\r\n#25 0x00007fffed10b8fd in std::_Function_handler<arrow::Future<std::shared_ptr<arrow::RecordBatch> > (), arrow::dataset::SlicingGenerator>::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#26 0x00007fffecfcaeb2 in std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()>::operator()() const (this=0x555555f9b5f0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#27 0x00007fffecfc8c07 in arrow::FutureFirstGenerator<std::shared_ptr<arrow::RecordBatch> >::operator() (this=0x555555bc34c0) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:665\r\n#28 0x00007fffecfc6c55 in std::_Function_handler<arrow::Future<std::shared_ptr<arrow::RecordBatch> > (), arrow::FutureFirstGenerator<std::shared_ptr<arrow::RecordBatch> > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#29 0x00007fffecfcaeb2 in std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()>::operator()() const (this=0x555555a154d0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#30 0x00007fffed051d3a in arrow::DefaultIfEmptyGenerator<std::shared_ptr<arrow::RecordBatch> >::operator() (this=0x7fffb4061370) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:2036\r\n#31 0x00007fffed047b49 in std::_Function_handler<arrow::Future<std::shared_ptr<arrow::RecordBatch> > (), arrow::DefaultIfEmptyGenerator<std::shared_ptr<arrow::RecordBatch> > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#32 0x00007fffecfcaeb2 in std::function<arrow::Future<std::shared_ptr<arrow::RecordBatch> > ()>::operator()() const (this=0x7fffb40513d0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#33 0x00007fffed052222 in arrow::EnumeratingGenerator<std::shared_ptr<arrow::RecordBatch> >::operator() (this=0x7fffb40616f0) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:1524\r\n#34 0x00007fffed047dba in std::_Function_handler<arrow::Future<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > > (), arrow::EnumeratingGenerator<std::shared_ptr<arrow::RecordBatch> > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#35 0x00007fffed05cf20 in std::function<arrow::Future<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > > ()>::operator()() const (this=0x555555a15580) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#36 0x00007fffed052777 in arrow::FutureFirstGenerator<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > >::operator() (this=0x555555ea1250) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:665\r\n#37 0x00007fffed048210 in std::_Function_handler<arrow::Future<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > > (), arrow::FutureFirstGenerator<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#38 0x00007fffed05cf20 in std::function<arrow::Future<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> > > ()>::operator()() const (this=0x555555bc2ee0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#39 0x00007fffed052cb2 in arrow::MappingGenerator<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> >, arrow::dataset::EnumeratedRecordBatch>::operator() (this=0x55555775b360) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:163\r\n#40 0x00007fffed0484d5 in std::_Function_handler<arrow::Future<arrow::dataset::EnumeratedRecordBatch> (), arrow::MappingGenerator<arrow::Enumerated<std::shared_ptr<arrow::RecordBatch> >, arrow::dataset::EnumeratedRecordBatch> >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#41 0x00007fffed033f92 in std::function<arrow::Future<arrow::dataset::EnumeratedRecordBatch> ()>::operator()() const (this=0x5555578691a0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#42 0x00007fffed059b82 in arrow::MergedGenerator<arrow::dataset::EnumeratedRecordBatch>::operator() (this=0x555555fb3a60) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:1086\r\n#43 0x00007fffed04f29a in std::_Function_handler<arrow::Future<arrow::dataset::EnumeratedRecordBatch> (), arrow::MergedGenerator<arrow::dataset::EnumeratedRecordBatch> >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#44 0x00007fffed033f92 in std::function<arrow::Future<arrow::dataset::EnumeratedRecordBatch> ()>::operator()() const (this=0x55555598f5a0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#45 0x00007fffed05ac48 in arrow::MappingGenerator<arrow::dataset::EnumeratedRecordBatch, std::optional<arrow::compute::ExecBatch> >::operator() (this=0x555555c166e0) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/async_generator.h:163\r\n#46 0x00007fffed04fc1d in std::_Function_handler<arrow::Future<std::optional<arrow::compute::ExecBatch> > (), arrow::MappingGenerator<arrow::dataset::EnumeratedRecordBatch, std::optional<arrow::compute::ExecBatch> > >::_M_invoke(std::_Any_data const&) (__functor=...) at \/usr\/include\/c++\/9\/bits\/std_function.h:286\r\n#47 0x00007fffed05f73e in std::function<arrow::Future<std::optional<arrow::compute::ExecBatch> > ()>::operator()() const (this=0x5555561360c0) at \/usr\/include\/c++\/9\/bits\/std_function.h:688\r\n#48 0x00007fffe6545d41 in arrow::acero::(anonymous namespace)::SourceNode::<lambda()>::operator()(void) const (__closure=0x555555ea0e28) at \/home\/even\/arrow\/cpp\/src\/arrow\/acero\/source_node.cc:201\r\n#49 0x00007fffe6549b77 in arrow::Callback::operator() (this=0x555555ea0e28, maybe_control=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:837\r\n#50 0x00007fffe6551051 in arrow::Future<std::optional<int> >::WrapResultOnComplete::Callback<arrow::Loop(Iterate) [with Iterate = arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>; Control = std::optional<int>; BreakValueType = int]::Callback>::operator()(const arrow::FutureImpl &) (this=0x555555ea0e28, impl=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:442\r\n#51 0x00007fffe6550f39 in arrow::internal::FnOnce<void(const arrow::FutureImpl&)>::FnImpl<arrow::Future<std::optional<int> >::WrapResultOnComplete::Callback<arrow::Loop(Iterate) [with Iterate = arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>; Control = std::optional<int>; BreakValueType = int]::Callback> >::invoke(const arrow::FutureImpl &) (this=0x555555ea0e20, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:152\r\n#52 0x00007fffdf9ba13c in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator()(arrow::FutureImpl const&) && (this=0x5555578d3e70, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:140\r\n#53 0x00007fffdf9b97ea in arrow::ConcreteFutureImpl::RunOrScheduleCallback (self=std::shared_ptr<arrow::FutureImpl> (use count 2, weak count 1) = {...}, callback_record=..., in_add_callback=false) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:110\r\n#54 0x00007fffdf9b9b4e in arrow::ConcreteFutureImpl::DoMarkFinishedOrFailed (this=0x5555578d4490, state=arrow::FutureState::SUCCESS) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:148\r\n#55 0x00007fffdf9b8fc5 in arrow::ConcreteFutureImpl::DoMarkFinished (this=0x5555578d4490) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:39\r\n#56 0x00007fffdf9b7072 in arrow::FutureImpl::MarkFinished (this=0x5555578d4490) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:224\r\n#57 0x00007fffe65593ae in arrow::Future<std::optional<int> >::DoMarkFinished (this=0x5555565a5b48, res=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:658\r\n#58 0x00007fffe65591c9 in arrow::Future<std::optional<int> >::MarkFinished (this=0x5555565a5b48, res=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:403\r\n#59 0x00007fffe65595c6 in arrow::detail::MarkNextFinished<arrow::Future<std::optional<int> >, arrow::Future<std::optional<int> >, false, false>::operator()(arrow::Result<std::optional<int> > const&) && (this=0x5555565a5b48, res=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:111\r\n#60 0x00007fffe6559573 in arrow::Future<std::optional<int> >::WrapResultOnComplete::Callback<arrow::detail::MarkNextFinished<arrow::Future<std::optional<int> >, arrow::Future<std::optional<int> >, false, false> >::operator()(arrow::FutureImpl const&) && (this=0x5555565a5b48, impl=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:442\r\n#61 0x00007fffe655952f in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::FnImpl<arrow::Future<std::optional<int> >::WrapResultOnComplete::Callback<arrow::detail::MarkNextFinished<arrow::Future<std::optional<int> >, arrow::Future<std::optional<int> >, false, false> > >::invoke(arrow::FutureImpl const&) (this=0x5555565a5b40, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:152\r\n#62 0x00007fffdf9ba13c in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator()(arrow::FutureImpl const&) && (this=0x7fffffffc7b0, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:140\r\n#63 0x00007fffdf9b97ea in arrow::ConcreteFutureImpl::RunOrScheduleCallback (self=std::shared_ptr<arrow::FutureImpl> (use count 2, weak count 1) = {...}, callback_record=..., in_add_callback=true) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:110\r\n#64 0x00007fffdf9b9224 in arrow::ConcreteFutureImpl::AddCallback(arrow::internal::FnOnce<void (arrow::FutureImpl const&)>, arrow::CallbackOptions) (this=0x555557405e20, callback=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:64\r\n#65 0x00007fffdf9b7112 in arrow::FutureImpl::AddCallback(arrow::internal::FnOnce<void (arrow::FutureImpl const&)>, arrow::CallbackOptions) (this=0x555557405e20, callback=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:229\r\n#66 0x00007fffe655910b in arrow::Future<std::optional<int> >::AddCallback<arrow::detail::MarkNextFinished<arrow::Future<std::optional<int> >, arrow::Future<std::optional<int> >, false, false>, arrow::Future<std::optional<int> >::WrapResultOnComplete::Callback<arrow::detail::MarkNextFinished<arrow::Future<std::optional<int> >, arrow::Future<std::optional<int> >, false, false> > > (this=0x7fffffffc8d0, on_complete=..., opts=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:493\r\n#67 0x00007fffe6551877 in arrow::detail::ContinueFuture::operator()<arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)>, const std::optional<arrow::compute::ExecBatch>&>(arrow::Future<std::optional<int> >, arrow::acero::(anonymous namespace)::SourceNode::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)> &&) const (this=0x7fffffffc9bf, next=..., f=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:181\r\n#68 0x00007fffe655149a in arrow::detail::ContinueFuture::IgnoringArgsIf<arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)>, arrow::Future<std::optional<int> >, const std::optional<arrow::compute::ExecBatch>&>(std::false_type, arrow::Future<std::optional<int> > &&, arrow::acero::(anonymous namespace)::SourceNode::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)> &&) const (this=0x7fffffffc9bf, next=..., f=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:193\r\n#69 0x00007fffe655120c in arrow::Future<std::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)>, arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const arrow::Status&)> >::operator()(const arrow::Result<std::optional<arrow::compute::ExecBatch> > &) (this=0x555555ea0518, result=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:545\r\n#70 0x00007fffe6551091 in arrow::Future<std::optional<arrow::compute::ExecBatch> >::WrapResultOnComplete::Callback<arrow::Future<std::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)>, arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const arrow::Status&)> > >::operator()(const arrow::FutureImpl &) (this=0x555555ea0518, impl=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.h:442\r\n#71 0x00007fffe6550f81 in arrow::internal::FnOnce<void(const arrow::FutureImpl&)>::FnImpl<arrow::Future<std::optional<arrow::compute::ExecBatch> >::WrapResultOnComplete::Callback<arrow::Future<std::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const std::optional<arrow::compute::ExecBatch>&)>, arrow::acero::(anonymous namespace)::SourceNode::StartProducing()::<lambda()>::<lambda(const arrow::Status&)> > > >::invoke(const arrow::FutureImpl &) (this=0x555555ea0510, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:152\r\n#72 0x00007fffdf9ba13c in arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator()(arrow::FutureImpl const&) && (this=0x7fffb41af678, a#0=...) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:140\r\n#73 0x00007fffdf9b956e in arrow::ConcreteFutureImpl::RunOrScheduleCallback(std::shared_ptr<arrow::FutureImpl> const&, arrow::FutureImpl::CallbackRecord&&, bool)::{lambda()#1}::operator()() (__closure=0x7fffb41af668) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/future.cc:106\r\n#74 0x00007fffdf9bf88a in arrow::internal::FnOnce<void ()>::FnImpl<arrow::ConcreteFutureImpl::RunOrScheduleCallback(std::shared_ptr<arrow::FutureImpl> const&, arrow::FutureImpl::CallbackRecord&&, bool)::{lambda()#1}>::invoke() (this=0x7fffb41af660) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:152\r\n#75 0x00007fffdfa19f7d in arrow::internal::FnOnce<void ()>::operator()() && (this=0x7fffffffcb70) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/functional.h:140\r\n#76 0x00007fffdfa146ad in arrow::internal::SerialExecutor::RunLoop (this=0x55555611d5b0) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/thread_pool.cc:252\r\n#77 0x00007fffed03d806 in arrow::internal::SerialExecutor::IterateGenerator<arrow::dataset::TaggedRecordBatch>(arrow::internal::FnOnce<arrow::Result<std::function<arrow::Future<arrow::dataset::TaggedRecordBatch> ()> > (arrow::internal::Executor*)>)::SerialIterator::Next() (this=0x555555a15110) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/thread_pool.h:363\r\n#78 0x00007fffed03d6e2 in arrow::internal::SerialExecutor::IterateGenerator<arrow::dataset::TaggedRecordBatch>(arrow::internal::FnOnce<arrow::Result<std::function<arrow::Future<arrow::dataset::TaggedRecordBatch> ()> > (arrow::internal::Executor*)>)::SerialIterator::~SerialIterator() (this=0x555555a15110, __in_chrg=<optimized out>) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/thread_pool.h:338\r\n#79 0x00007fffed055120 in arrow::Iterator<arrow::dataset::TaggedRecordBatch>::Delete<arrow::internal::SerialExecutor::IterateGenerator<arrow::dataset::TaggedRecordBatch>(arrow::internal::FnOnce<arrow::Result<std::function<arrow::Future<arrow::dataset::TaggedRecordBatch> ()> > (arrow::internal::Executor*)>)::SerialIterator>(void*) (ptr=0x555555a15110) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/iterator.h:193\r\n#80 0x00007fffecf3284c in std::unique_ptr<void, void (*)(void*)>::~unique_ptr (this=0x555555a152b8, __in_chrg=<optimized out>) at \/usr\/include\/c++\/9\/bits\/unique_ptr.h:292\r\n#81 0x00007fffed02e578 in arrow::Iterator<arrow::dataset::TaggedRecordBatch>::~Iterator (this=0x555555a152b8, __in_chrg=<optimized out>) at \/home\/even\/arrow\/cpp\/src\/arrow\/util\/iterator.h:87\r\n#82 0x00007fffed0288f4 in arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader::~ScannerRecordBatchReader (this=0x555555a152a0, __in_chrg=<optimized out>) at \/home\/even\/arrow\/cpp\/src\/arrow\/dataset\/scanner.cc:96\r\n#83 0x00007fffed02aded in __gnu_cxx::new_allocator<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader>::destroy<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader> (this=0x555555a152a0, __p=0x555555a152a0) at \/usr\/include\/c++\/9\/ext\/new_allocator.h:152\r\n#84 0x00007fffed02a189 in std::allocator_traits<std::allocator<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader> >::destroy<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader> (__a=..., __p=0x555555a152a0) at \/usr\/include\/c++\/9\/bits\/alloc_traits.h:496\r\n#85 0x00007fffed0296c9 in std::_Sp_counted_ptr_inplace<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader, std::allocator<arrow::dataset::(anonymous namespace)::ScannerRecordBatchReader>, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x555555a15290) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:557\r\n#86 0x00007fffeda2d84c in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x555555a15290) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:148\r\n#87 std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x555555a15290) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:148\r\n#88 std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=<optimized out>, __in_chrg=<optimized out>) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:730\r\n#89 std::__shared_ptr<arrow::RecordBatchReader, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=<optimized out>, __in_chrg=<optimized out>) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:1169\r\n#90 std::__shared_ptr<arrow::RecordBatchReader, (__gnu_cxx::_Lock_policy)2>::reset (this=0x555555993288) at \/usr\/include\/c++\/9\/bits\/shared_ptr_base.h:1287\r\n#91 OGRParquetDatasetLayer::ResetReading (this=0x555555992ec0) at \/home\/even\/gdal\/gdal\/ogr\/ogrsf_frmts\/parquet\/ogrparquetdatasetlayer.cpp:104\r\n```\r\n\r\n### Component(s)\r\n\r\nC++","comments":["@bkietz "],"labels":["Type: bug","Component: C++"]},{"title":"[C++] malloc: Incorrect checksum for freed object: probably modified after being freed in arrow-bit-util-benchmark","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nWhile running benchmarks locally for separate reasons, I got this:\r\n\r\n```\r\narrow-bit-util-benchmark(3970,0x1f22abac0) malloc: Incorrect checksum for freed object 0x118010800: probably modified after being freed.\r\nCorrupt value: 0xc\r\narrow-bit-util-benchmark(3970,0x1f22abac0) malloc: *** set a breakpoint in malloc_error_break to debug\r\n```\r\n\r\nThis was on an macOS M1, with a recent checkout (ea5b620aac61c4f67d116efb0655688cb2175a80), running `archery benchmark run --suite-filter=arrow-bit-util-benchmark --preserve`. The backtrace is:\r\n\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\r\n  * frame #0: 0x000000018a3d8384 libsystem_malloc.dylib`malloc_error_break\r\n    frame #1: 0x000000018a3bba14 libsystem_malloc.dylib`malloc_vreport + 748\r\n    frame #2: 0x000000018a3dfea8 libsystem_malloc.dylib`malloc_zone_error + 104\r\n    frame #3: 0x000000018a3c74c4 libsystem_malloc.dylib`free_list_checksum_botch + 40\r\n    frame #4: 0x000000018a3b4cbc libsystem_malloc.dylib`small_free_list_remove_ptr_no_clear + 960\r\n    frame #5: 0x000000018a3b22c4 libsystem_malloc.dylib`free_small + 692\r\n    frame #6: 0x0000000102eb6d00 libarrow.1600.dylib`arrow::BaseMemoryPoolImpl<arrow::(anonymous namespace)::SystemAllocator>::Free(unsigned char*, long long, long long) + 44\r\n    frame #7: 0x0000000102eb72b4 libarrow.1600.dylib`arrow::PoolBuffer::~PoolBuffer() + 72\r\n    frame #8: 0x0000000102eb6f74 libarrow.1600.dylib`arrow::PoolBuffer::~PoolBuffer() + 12\r\n    frame #9: 0x0000000100007438 arrow-bit-util-benchmark`arrow::bit_util::CopyBitmapWithOffsetBoth(benchmark::State&) + 432\r\n    frame #10: 0x000000010053a96c libbenchmark.1.dylib`benchmark::internal::BenchmarkInstance::Run(long long, int, benchmark::internal::ThreadTimer*, benchmark::internal::ThreadManager*, benchmark::internal::PerfCountersMeasurement*) const + 160\r\n    frame #11: 0x000000010054d50c libbenchmark.1.dylib`benchmark::internal::(anonymous namespace)::RunInThread(benchmark::internal::BenchmarkInstance const*, long long, int, benchmark::internal::ThreadManager*, benchmark::internal::PerfCountersMeasurement*) + 96\r\n    frame #12: 0x000000010054d0c4 libbenchmark.1.dylib`benchmark::internal::BenchmarkRunner::DoNIterations() + 692\r\n    frame #13: 0x000000010054ddf4 libbenchmark.1.dylib`benchmark::internal::BenchmarkRunner::DoOneRepetition() + 180\r\n    frame #14: 0x0000000100533a04 libbenchmark.1.dylib`benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter*, benchmark::BenchmarkReporter*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 2876\r\n    frame #15: 0x0000000100532e68 libbenchmark.1.dylib`benchmark::RunSpecifiedBenchmarks() + 64\r\n    frame #16: 0x0000000100473e74 libbenchmark_main.1.dylib`main + 136\r\n    frame #17: 0x000000018a2160e0 dyld`start + 2360\r\n```\r\n\r\nIt looks like the failing test is `CopyBitmapWithOffsetBoth`:\r\n\r\n```\r\nCopyBitmapWithOffset\/8192                     636 ns          635 ns      1102970 bytes_per_second=12.0217Gi\/s\r\n-- LOG(2): Running CopyBitmapWithOffsetBoth\/8192 for 1\r\narrow-bit-util-benchmark(54792,0x1f22abac0) malloc: Incorrect checksum for freed object 0x13b00c200: probably modified after being freed.\r\nCorrupt value: 0xc\r\narrow-bit-util-benchmark(54792,0x1f22abac0) malloc: *** set a breakpoint in malloc_error_break to debug\r\nfish: Job 1, '\/var\/folders\/vm\/bbcxtl3s63l7gn9\u2026' terminated by signal SIGABRT (Abort)\r\n```\r\n\r\nWhen I run the test in isolation I don't get the error though. Does this look like something spurious or is it worth investigating?\n\n### Component(s)\n\nC++","comments":["It would be worth taking a look indeed. Perhaps we're two eagerly copying words and going past the destination buffer.","Thanks @pitrou. \r\n\r\nI'll make some time to look into this further but anyone reading this issue is welcome to look onto it too.","I found something likely to be the cause using ASAN. I think I can take a deeper look if you haven't put much effort on it, @amoeba ?\n\nThanks. ","If you could take a look that would be great @zanmato1984. I haven't gotten to this yet."],"labels":["Type: bug","Component: C++"]},{"title":"[Python] Conventions around PyCapsule Interface and choosing Array\/Stream export","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\n\ud83d\udc4b I've been excited about the PyCapsule interface, and have been implementing it in my [geoarrow-rust project](https:\/\/github.com\/geoarrow\/geoarrow-rs). Every function call accepts any Arrow PyCapsule interface object, no matter its producer. It's really amazing!\r\n\r\nFundamentally, my question is whether the existence of methods on an object should allow for an inference of its storage type. That is, should it be possible to observe whether a producer object is chunked or not based on whether it exports `__arrow_c_array__` or `__arrow_c_stream__`? I had been expecting yes, as pyarrow implements only the former on `Array` and `RecordBatch` and only the latter on `ChunkedArray` and `Table` (to my knowledge). But this question came up [here](https:\/\/github.com\/shapely\/shapely\/pull\/1953#discussion_r1529247534), where nanoarrow implements _both_ `__arrow_c_array__` and `__arrow_c_stream__` \r\n\r\n\r\nI'd argue that it's simpler to only define a single type of export method on a class and allow the consumer to convert to a different representation if they need. This communicates more information about how the existing data is already stored in memory. But in general I think it's really useful if the community is able to agree on a convention here, which will inform whether consumers can expect this invariant to hold or not.\r\n\r\n### Component(s)\r\n\r\nPython","comments":["> nanoarrow implements both `__arrow_c_array__` and `__arrow_c_stream__`\r\n\r\nFor reference, the PR implementing the `nanoarrow.Array` is https:\/\/github.com\/apache\/arrow-nanoarrow\/pull\/396 . It is basically a ChunkedArray and is currently the only planned user-facing Arrayish thing, although it's all very new (feel free to comment on that PR!). Basically, I found that maintaining both a chunked and a non-chunked pathway in geoarrow-pyarrow resulted in a lot of Python loops over chunks and I wanted to avoid forcing nanoarrow users to maintain two pathways. Many pyarrow methods might give you back a `Array` or a `ChunkedArray`; however, many `ChunkedArray`s only have one chunk. The whole thing is imperfect and a bit of a compromise.\r\n\r\n> Fundamentally, my question is whether the existence of methods on an object should allow for an inference of its storage type\r\n\r\nMy take on this is that as long as the object has an unambiguous interpretation as a contiguous array (or *might* have one, since it might take a loop over something that is not already Arrow data to figure this out), I think it's fine for `__arrow_c_array__` to exist.  As long as an object has an unambiguous interpretation as zero or more arrays (or *might* have one), I think `__arrow_c_stream__` can exist. I don't see those as mutually exclusive...for me this is like `pyarrow.array()` returning either a `ChunkedArray` or an `Array`: it just doesn't know until it sees the input what type it needs to unambiguously represent it.\r\n\r\nFor something like an `Array` or `RecordBatch` (or something like a `numpy` array) that is definitely Arrow and is definitely contiguous, I am not sure what the benefit would be for `__arrow_c_stream__` to exist and it is probably just confusing if it does.\r\n\r\nThere are other assumptions that can't be captured by the mere existence of either of those, like exactly how expensive it will be to call any one of those methods. In https:\/\/github.com\/shapely\/shapely\/pull\/1953  both are fairly expensive because the data are not Arrow yet. For a database driver, it might expensive to consume the stream because the data haven't arrived over the network yet.\r\n\r\nThe Python buffer protocol has a `flags` field to handle consumer requests along these lines (like a request for contiguous, rather than strided, memory) that could be used to disambiguate some of these cases if it turns out that disambiguating them is important. It is also careful to note that the existence of the buffer protocol implementation does not imply that attempting to get the buffer will succeed.\r\n\r\nFor consuming in nanoarrow, the current approach is to use `__arrow_c_stream__` whenever possible since this has the fewest constraints (arrays need not be in memory yet, need not be contiguous, might not be fully consumed). Then it falls back on `__arrow_c_array__`. The entrypoint is `nanoarrow.c_array_stream()`, which will happily accept either (generates a length-one stream if needed).","Being able to infer the input structure also significantly helps static typing. For example, I have type hints that I'm writing for geoarrow-rust that [include](https:\/\/github.com\/geoarrow\/geoarrow-rs\/blob\/cfe91bbe15c20b8fd814f7e93294f9c134b9d87a\/python\/core\/python\/geoarrow\/rust\/core\/_rust.pyi#L1199-L1205):\r\n\r\n```py\r\n@overload\r\ndef centroid(input: ArrowArrayExportable) -> PointArray: ...\r\n@overload\r\ndef centroid(input: ArrowStreamExportable) -> ChunkedPointArray: ...\r\ndef centroid(\r\n    input: ArrowArrayExportable | ArrowStreamExportable,\r\n) -> PointArray | ChunkedPointArray: ...\r\n```\r\n\r\nI'm not sure which overload a type checker would pick if the input object had _both_ dunder methods. I suppose it would always return the union. But being able to use structural types in this way is quite useful for static type checking and IDE autocompletion, which are really sore spots right now with pyarrow.","> for me this is like `pyarrow.array()` returning either a `ChunkedArray` or an `Array`: it just doesn't know until it sees the input what type it needs to unambiguously represent it.\r\n\r\nDoes `pyarrow.array` ever return a `ChunkedArray`? I tried to pass in a list of lists and it inferred a `ListArray`. I thought `pyarrow.array` only ever returned an `Array` and `pyarrow.chunked_array` only ever returned a `ChunkedArray`?\r\n\r\n> For something like an `Array` or `RecordBatch` (or something like a `numpy` array) that is definitely Arrow and is definitely contiguous, I am not sure what the benefit would be for `__arrow_c_stream__` to exist and it is probably just confusing if it does.\r\n\r\nSo your argument is that `Array` should never have `__arrow_c_stream__`, but that `ChunkedArray` should have both `__arrow_c_array__` and `__arrow_c_stream__`?","> Does pyarrow.array ever return a ChunkedArray?\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\ntype(pa.array([\"a\" * 2 ** 20 for _ in range(2**10)]))\r\n#> pyarrow.lib.StringArray\r\ntype(pa.array([\"a\" * 2 ** 20 for _ in range(2**11)]))\r\n#> pyarrow.lib.ChunkedArray\r\n```\r\n\r\nThis is also true of the [`__arrow_array__` protocol](https:\/\/arrow.apache.org\/docs\/python\/extending_types.html#controlling-conversion-to-pyarrow-array-with-the-arrow-array-protocol).\r\n\r\n> I'm not sure which overload a type checker would pick if the input object had both dunder methods.\r\n\r\nWould the typing hints be significantly different for a `ChunkedPointArray` vs a `PointArray`?\r\n\r\n> So your argument is that Array should never have __arrow_c_stream__, but that ChunkedArray should have both __arrow_c_array__ and __arrow_c_stream__?\r\n\r\nMaybe *could* is more like it. pyarrow has an `Array` class for a specifically contiguous array...nanoarrow doesn't at the moment (at least in a user-facing nicely typed sort of way)."],"labels":["Component: Python","Type: usage"]},{"title":"GH-40646: [C++] Re-order loads and stores in MemoryPoolStats update","body":"### Rationale for this change\r\n\r\nIssue loads as soon as possible so the latency of waiting for memory is masked by doing other operations.\r\n\r\n### What changes are included in this PR?\r\n\r\n - Make all the read-modify-write operations use `memory_order_acq_rel`\r\n - Make all the loads and stores use `memory_order_acquire`\/`release` respectively\r\n - Statically specialize the implementation of `UpdateAllocatedBytes` so `bytes_allocated_` can be updated without waiting for the load of the old value\r\n\r\n### Are these changes tested?\r\n\r\nBy existing tests.\r\n\r\n* GitHub Issue: #40646","comments":[":warning: GitHub issue #40646 **has been automatically assigned in GitHub** to PR creator.","@ursabot please benchmark","Benchmark runs are scheduled for commit 3333c48284d8e706f4afc31fe41d1f0d2824eba1. Watch https:\/\/buildkite.com\/apache-arrow and https:\/\/conbench.ursa.dev for updates. A comment will be posted here when the runs are complete.","Thanks for your patience. Conbench analyzed the 7 benchmarking runs that have been run so far on PR commit 3333c48284d8e706f4afc31fe41d1f0d2824eba1.\n\nThere were 10 benchmark results indicating a performance regression:\n\n- Pull Request Run on `ursa-thinkcentre-m75q` at [2024-03-18 23:18:21Z](https:\/\/conbench.ursa.dev\/compare\/runs\/cd88130563174952b37acc47cbe02b23...d76b9630ef4243df9156c650fc0c564d\/)\n  - [`ArrayScalarKernel` (C++) with params=<Subtract, DoubleType>\/size:524288\/inverse_null_proportion:0, source=cpp-micro, suite=arrow-compute-scalar-arithmetic-benchmark](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065f4db8e98773858000cfcfd652f8ef...065f8cce94f07c64800041f60bab1032)\n  - [`CopyEmptyVector` (C++) with params=<SMALL_VECTOR(int)>, source=cpp-micro, suite=arrow-small-vector-benchmark](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065f4db731727ce38000c02413bad497...065f8ccccf0d740e80001c6cf8cbee9c)\n- and 8 more (see the report linked below)\n\nThe [full Conbench report](https:\/\/github.com\/apache\/arrow\/runs\/22813157645) has more details.","Does it actually change anything for x86?","If you're really interested in reducing the contention costs for MemoryPool statistics, then I would suggest taking a look at https:\/\/travisdowns.github.io\/blog\/2020\/07\/06\/concurrency-costs.html ","> Does it actually change anything for x86?\r\n\r\nNothing, unless the compiler decides to re-order the loads and the stores. Which it didn't in this case.\r\n\r\nI suspect my change regarding `max_memory_` didn't lead to a cheaper sequence of instructions \u2014 I assumed `max_memory_.store(allocated, seq_cst)` (written as `max_memory_ = allocated;` in the code) was `lock`-prefixed and more expensive (combined with the `seq_cst` load) than the `compare_exchange` I added.\r\n\r\n\r\n> If you're really interested in reducing the contention costs for MemoryPool statistics, then I would suggest taking a look at https:\/\/travisdowns.github.io\/blog\/2020\/07\/06\/concurrency-costs.html\r\n\r\nNot just the contention (the least of the issues really), but the time it takes to perform all the memory operations.","> Does it actually change anything for x86?\r\n\r\nWouldn't acq-rel cheaper than original total-ordering?","> > Does it actually change anything for x86?\r\n> \r\n> Wouldn't acq-rel cheaper than original total-ordering?\r\n\r\nOnly on architectures with a weaker memory model (e.g. ARM). x86 guarantees that all the stores are ordered (not immediately visible, but they are ordered). [1] explains this much better than I ever could.\r\n\r\n`acq-rel` can enable the *compiler* to re-order operations if it decides that can unlock optimizations. Not the case here, so I'm manually experimenting with different orderings to mask the latency of fetch\/load operations on atomic variables.\r\n\r\n[1] https:\/\/research.swtch.com\/hwmm#x86\r\n\r\n","I pushed some re-ordering of loads and stores that I believe can work better on CPUs with higher latency on the memory system [1]. Note that my code updates `max_memory_` correctly (I removed the comment about \"no need to be rigorous\"). The `lock cmpxchg` that I introduced never appears in profiles, so I'm keeping it. I also suspect it would be beneficial on contented workloads because we can give up on updating `max_memory_` if another threads increases it before the current thread.\r\n\r\nWhen benchmarking on an Intel CPU, compared to the baseline, this version doesn't cost less, but it should improve the situation on CPUs that are spending more time [1] in the load issued by `bytes_allocated_.fetch_add(diff)` (`lock xadd` instruction on x86). My hypothesis is that by not immediately using the result of the `xadd`, the CPU can wait for the memory system in the background while performing other operations. It also clusters all the `lock`-prefixed instructions together which is recommended practice.\r\n\r\nAnnotated `perf report` of `arrow::memory_pool::internal::JemallocAllocator::AllocateAligned` after these changes:\r\n\r\n```asm\r\n#   Percent\u2502       push %rbp\r\n           \u2502       push %r15\r\n           \u2502       push %r14\r\n           \u2502       push %r13\r\n           \u2502       push %r12\r\n           \u2502       push %rbx\r\n           \u2502       push %rax\r\n           \u2502       mov  %r8,%r13\r\n           \u2502       mov  %rcx,%rbx\r\n           \u2502       mov  %rdx,%r12\r\n           \u2502       mov  %rsi,%r15\r\n           \u2502       mov  %rdi,%r14\r\n           \u2502       incb 0x13f6583(%rip)        # 61bc853 <__TMC_END__+0x13935b>\r\n1     5.31 \u2502       xor  %edi,%edi\r\n           \u2502       mov  %rdx,%rsi\r\n           \u2502     \u2192 call __sanitizer_cov_trace_const_cmp8@plt\r\n           \u2502       test %r12,%r12\r\n           \u2502     \u2193 js   6c\r\n1     5.56 \u2502       incb 0x13f6570(%rip)        # 61bc855 <__TMC_END__+0x13935d>\r\n           \u2502       mov  %rsp,%rdi\r\n           \u2502       mov  %r12,%rsi\r\n           \u2502       mov  %rbx,%rdx\r\n           \u2502       mov  %r13,%rcx\r\n           \u2502     \u2192 call arrow::memory_pool::internal::JemallocAllocator::AllocateAligned@plt\r\n           \u2502       test %r14,%r14\r\n           \u2502     \u2193 je   136\r\n           \u2502       incb 0x13f6552(%rip)        # 61bc857 <__TMC_END__+0x13935f>\r\n           \u2502       mov  (%rsp),%rax\r\n           \u2502       mov  %rax,(%r14)\r\n           \u2502       test %rax,%rax\r\n           \u2502     \u2193 je   8b\r\n           \u2502       incb 0x13f6541(%rip)        # 61bc858 <__TMC_END__+0x139360>\r\n           \u2502     \u2193 jmp  11e\r\n           \u2502 6c:   incb 0x13f6532(%rip)        # 61bc854 <__TMC_END__+0x13935c>\r\n           \u2502       lea  typeinfo name for arrow::json::TableReader+0x22a,%rdx\r\n           \u2502       mov  %r14,%rdi\r\n           \u2502       mov  $0x4,%esi\r\n           \u2502     \u2192 call arrow::Status::FromArgs<char const (&) [21]>@plt\r\n           \u2502     \u2193 jmp  11e\r\n           \u2502 8b:   incb 0x13f6518(%rip)        # 61bc859 <__TMC_END__+0x139361>\r\n1     5.54 \u2502       mov  0x40(%r15),%rbx\r\n           \u2502       mov  %r12,%rsi\r\n4    22.34 \u2502       lock xadd %rsi,0x48(%r15)\r\n4    22.47 \u2502       lock add  %r12,0x50(%r15)\r\n6    33.09 \u2502       lock incq 0x58(%r15)\r\n           \u2502       mov  %rsi,%r13\r\n           \u2502       add  %r12,%r13\r\n           \u2502     \u2193 jo   14a\r\n           \u2502       incb 0x13f64f1(%rip)        # 61bc85b <__TMC_END__+0x139363>\r\n           \u2502       mov  %rbx,%rdi\r\n           \u2502       mov  %r13,%rsi\r\n           \u2502     \u2192 call __sanitizer_cov_trace_cmp8@plt\r\n           \u2502       cmp  %r13,%rbx\r\n1     5.69 \u2502     \u2193 jge  103\r\n           \u2502       incb 0x13f64dd(%rip)        # 61bc85d <__TMC_END__+0x139365>\r\n           \u2502 d0:   incb 0x13f64d8(%rip)        # 61bc85e <__TMC_END__+0x139366>\r\n           \u2502       mov  %rbx,%rax\r\n           \u2502       lock cmpxchg %r13,0x40(%r15)\r\n           \u2502       sete %bpl\r\n           \u2502       mov  %rax,%rbx\r\n           \u2502       mov  %rax,%rdi\r\n           \u2502       mov  %r13,%rsi\r\n           \u2502     \u2192 call __sanitizer_cov_trace_cmp8@plt\r\n           \u2502       test %bpl,%bpl\r\n           \u2502     \u2193 jne  10b\r\n           \u2502       cmp  %r13,%rbx\r\n           \u2502     \u2193 jge  10b\r\n           \u2502       incb 0x13f64af(%rip)        # 61bc860 <__TMC_END__+0x139368>\r\n           \u2502     \u2191 jmp  d0\r\n           \u2502103:   incb 0x13f64a3(%rip)        # 61bc85c <__TMC_END__+0x139364>\r\n           \u2502     \u2193 jmp  111\r\n           \u250210b:   incb 0x13f649e(%rip)        # 61bc85f <__TMC_END__+0x139367>\r\n           \u2502111:   incb 0x13f649a(%rip)        # 61bc861 <__TMC_END__+0x139369>\r\n           \u2502       movq $0x0,(%r14)\r\n           \u250211e:   incb 0x13f648e(%rip)        # 61bc862 <__TMC_END__+0x13936a>\r\n           \u2502       mov  %r14,%rax\r\n           \u2502       add  $0x8,%rsp\r\n           \u2502       pop  %rbx\r\n           \u2502       pop  %r12\r\n           \u2502       pop  %r13\r\n           \u2502       pop  %r14\r\n           \u2502       pop  %r15\r\n           \u2502       pop  %rbp\r\n           \u2502     \u2190 ret\r\n```\r\n\r\nBenchmarks (based on the old code) on a Zen 3 CPU show that the CPU can get stuck waiting for the value produced by `lock xadd` instead of progressing:\r\n\r\n```asm\r\n    0.18 |    lock xadd %rax,(%rdi)\r\n   80.73 |    add    %rsi,%rax\r\n```\r\n\r\nDoing useful work that doesn't depend on `%rax` (the result of `lock xadd`) should mask the latency of the memory load across the memory system.\r\n\r\nFrom [1]: \r\n\r\n> In Zen 3, a single 32MB L3 cache pool is shared among all 8 cores in a chiplet, vs. Zen 2's two 16MB pools each shared among 4 cores in a core complex, of which there were two per chiplet. This new arrangement improves the cache hit rate as well as performance in situations that require cache data to be exchanged among cores, but increases cache latency from 39 cycles in Zen 2 to 46 clock cycles and halves per-core cache bandwidth, although both problems are partially mitigated by higher clock speeds. \r\n\r\n[1] https:\/\/en.wikipedia.org\/wiki\/Zen_3#Features","Which benchmark should I run? I'd like to testing on my M1 Pro","Still not understanding that, in x86, though the instr is same ( this might help: https:\/\/darkcoding.net\/software\/rust-atomics-on-x86\/ ) , relaxed might possible to get compiler-reordering. Besides, see cases above:\r\n\r\n1. https:\/\/github.com\/apache\/kudu\/blob\/647726ad6b2aab0c6a6d34e16e027debd8a827eb\/src\/kudu\/util\/high_water_mark.h#L29\r\n2. https:\/\/github.com\/facebook\/rocksdb\/blob\/6ddfa5f06140c8d0726b561e16dc6894138bcfa0\/monitoring\/histogram.cc#L76\r\n\r\nI think just relaxed is enough here.","You might want to add multi-threaded allocation benchmarks (or perhaps just MemoryPoolStats nano-benchmarks).","Is that as simple as adding `>ThreadRange(1, 32);` to the relevant benchmark's args? When I do that to the `AllocateDeallocate` benchmark (memory_pool_benchark.cc), I see about a 10x improvement at 32 threads when I comment out the entire body of `UpdateAllocatedBytes` and about half that if I just comment out what was added in Arrow 13 (https:\/\/github.com\/apache\/arrow\/commit\/ddfa8eed9b188fcc7b38767d1858c2588c588f05#diff-2111aac8ee579e238fb15d4380f2ea1e2f3e2830da939ec3f07c61dc68038d1f).\r\n\r\nI put the full results in a Gist: https:\/\/gist.github.com\/amoeba\/b95102829280dbe2b1f64e6c23a5f594. A [branch on my fork](https:\/\/github.com\/amoeba\/arrow\/commits\/exp\/memory-pool-bench-threads\/) shows the changes I made to the benchmark and code.","![reorderloadstores](https:\/\/github.com\/apache\/arrow\/assets\/207795\/58ad547b-3374-412b-920a-8381344f7ce6)\r\n","> > Does it actually change anything for x86?\r\n> \r\n> Wouldn't acq-rel cheaper than original total-ordering?\r\n\r\nIf you mean that if acq-rel is cheaper than the original seq-cst on x86, the answer is no: RMW using acq-rel is promoted to seq-cst on x86.\r\n\r\nIf you mean that if acq-rel is cheaper than x86's \"total-store-ordering\" memory model, the answer is either no: x86's \"total-store-ordering\" guarantees acquire-release semantic (actually a little subtly stronger than acquire-release), even for plain loads and stores. Not mentioning the fact the RMW using acq-rel gets promoted to seq-cst.\r\n\r\nAs @felipecrv mentioned, only on weaker memory models such as AArch64 can acq-rel be cheaper than seq-cst - compilers can do reorderings for acq-rel that are not allowed for seq-cst. There are online discussions such as [1] implying that even solely hardware can do this too.\r\n\r\n[1] https:\/\/stackoverflow.com\/questions\/65568185\/for-purposes-of-ordering-is-atomic-read-modify-write-one-operation-or-two","You're right, the generated instr would be same in x86, and weaker memory-order might able to get compiler reorder?","> You're right, the generated instr would be same in x86, and weaker memory-order might able to get compiler reorder?\r\n\r\nYes. If the weaker memory order has the same asm as the stronger one, hardware treats them the same and only compiler can make some difference of reordering.","Great. I'm also interested in this. I can testing on ARM CPU tomorrow","I like this PR, along with the thorough benchmarking and analysis. But I have a question similar to @mapleFU 's.\r\n\r\nIIUC, the memory pool stats update is not fully synchronized with the actual allocator action (alloc\/dealloc) so it should be expected to be \"accurate most of the time\" rather than \"accurate all the time\". For example, it could be the case that two concurrent threads have the following interleaving:\r\n1. Thread 1 allocates 1MB (actual peak memory 1MB)\r\n2. Thread 2 allocates 2MB (actual peak memory 3MB)\r\n3. Thread 1 updates stats (stats allocated memory 1MB, peak memory 1MB)\r\n4. Thread 1 frees 1MB\r\n5. Thread 1 updates stats (stats allocated memory 0MB, peak memory 1MB)\r\n6. Thread 2 updates stats (stats allocated memory 2MB, peak memory 2MB)\r\n...\r\n\r\nWe'll see stats reports peak memory 2MB whereas the actual peak memory is 3MB, and it doesn't matter which memory order is used within the stats itself.\r\n\r\nGiven the above nature of memory pool stats, should relaxed memory order be good enough (assuming it has better performance)? Though it could be more \"inaccurate\" than acq-rel.\r\n\r\nThanks.","AFAIU, \"relaxed\" means the statistics could become entirely wrong, e.g. they could diverge more and more from the actual values. \"acq-rel\" ensures they converge to the actual values if no allocation happens for a long enough period of time.","> AFAIU, \"relaxed\" means the statistics could become entirely wrong, e.g. they could diverge more and more from the actual values. \"acq-rel\" ensures they converge to the actual values if no allocation happens for a long enough period of time.\r\n\r\nI think `compare_exchange` would gurantee the \"update\" maximum would be right (rather than diverge more and more from the actual values). Other operations just mean a little ( not more and more) from the actual values?","The maximum would be the least sensitive to divergence actually, become it's replacing one correct value with another correct value. At worse, it would not report the true maximum, but it would report a reasonable value nonetheless.\r\n\r\nNon-atomic in-place addition, though, could easily diverge if some additions are \"lost\".\r\n","And again, if we want the fastest possible while correct, we can invest some time porting the \"level 0\" solution from https:\/\/travisdowns.github.io\/blog\/2020\/07\/06\/concurrency-costs.html"],"labels":["Component: C++","awaiting change review"]},{"title":"[C++] Investigate using std::memory_order in MemoryPoolStats to improve performance","body":"### Describe the enhancement requested\r\n\r\nMemoryPoolStats keeps track of stats (bytes_allocated, max_memory, total_allocated_bytes, num_allocs) which are useful in a variety of contexts. The current implementation doesn't take advantage of functionality in std::memory (See [Release-Acquire Ordering](https:\/\/en.cppreference.com\/w\/cpp\/atomic\/memory_order#Release-Acquire_ordering)) which may improve performance for some types of applications on certain kinds of hardware.\r\n\r\nFor example, max_memory is monotonically increasing so it can use a relaxed load which is essentially free on x86. \r\n\r\nI don't have an example which shows the current performance impact and automated benchmarking like in https:\/\/github.com\/apache\/arrow\/commit\/ddfa8eed9b188fcc7b38767d1858c2588c588f05 doesn't exercise the right workload to show it so any ideas would be welcome.\r\n\r\n### Component(s)\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40644: [Python] Allow passing a mapping of column names to `rename_columns`","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nSee #40644 \r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\nTests have been added.\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40644","comments":[":warning: GitHub issue #40644 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: Python","awaiting committer review"]},{"title":"[Python] Allow `rename_columns` to take a mapping","body":"### Describe the enhancement requested\n\nPandas allows a mapping to be passed like `df.rename(columns={'foo': 'bar'})`. This is often very useful when you only want to rename a subset of the columns. Currently, `rename_columns` only accepts a list of all column names.\r\n\r\nI propose that `rename_columns` be extended to accept a `Mapping[str, str]`. All columns with a name matching a key of the mapping will be renamed to the relevant value. If any key does not correspond to at least one column then a `KeyError` should be raised.\r\n\r\nExample:\r\n```python\r\n>>> import pyarrow as pa\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame({'n_legs': [2, 4, 5, 100],\r\n...                    'animals': [\"Flamingo\", \"Horse\", \"Brittle stars\", \"Centipede\"]})\r\n>>> batch = pa.RecordBatch.from_pandas(df)\r\n>>> new_names = {\"n_legs\": \"n\", \"animals\": \"name\"}\r\n>>> batch.rename_columns(new_names)\r\npyarrow.RecordBatch\r\n n: int64\r\n name: string\r\n ----\r\n n: [2,4,5,100]\r\n name: [\"Flamingo\",\"Horse\",\"Brittle stars\",\"Centipede\"]\r\n```\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] BUG: Empty slicing an array backwards beyond the start should be empty","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI think post https:\/\/github.com\/apache\/arrow\/pull\/39240, slicing with an empty slice under the same conditions returns the first element when the result should be empty\r\n\r\n```python\r\nIn [14]: import pyarrow as pa\r\n    ...: print(pa.__version__)\r\n    ...: array = pa.chunked_array([list(\"abcde\")])\r\n    ...: array[slice(-6, -6, -1)]\r\n16.0.0.dev318\r\nOut[14]: \r\n<pyarrow.lib.ChunkedArray object at 0x11a1ce4d0>\r\n[\r\n  [\r\n    \"a\"\r\n  ]\r\n]\r\n\r\nIn [15]: list(\"abcde\")[slice(-6, -6, -1)]\r\nOut[15]: []\r\n```\r\n\r\ncc @LucasG0 @jorisvandenbossche \r\n\r\n### Component(s)\r\n\r\nPython","comments":["Thanks for the catch! Yet another annoying corner case ..\r\n\r\nI was wondering if there isn't some existing functionality to \"normalize\" slice objects, and https:\/\/stackoverflow.com\/questions\/6246084\/sanitize-python-slice pointed to https:\/\/docs.python.org\/3\/reference\/datamodel.html#slice.indices\r\n\r\nWe could test if that helps for us. Although for the case above it still gives negative start\/stop values .. :\r\n\r\n```\r\nIn [3]: slice(-6, -6, -1).indices(5)\r\nOut[3]: (-1, -1, -1)\r\n```","(and so this case actually worked before, so going to mark as critical for 16.0)","Using `slices.indices` seems to be a good call, thanks @jorisvandenbossche. I do not think still having negative values is an issue here, I created https:\/\/github.com\/apache\/arrow\/pull\/40682."],"labels":["Type: bug","Component: Python","Priority: Critical"]},{"title":"[R] to_arrow() loses group_by()","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nWhen passing a duckdb dataset to arrow with `to_arrow()` grouping information is lost.\r\n\r\n```r\r\ncars |> arrow::to_duckdb() |> dplyr::group_by(speed) |> arrow::to_arrow()\r\n# output is ungrouped\r\n#RecordBatchReader (query)\r\n#speed: double\r\n#dist: double\r\n\r\ncars |> arrow::to_duckdb() |> arrow::to_arrow() |> dplyr::group_by(speed) \r\n# output is grouped\r\n#RecordBatchReader (query)\r\n#speed: double\r\n#dist: double\r\n#\r\n#* Grouped by speed\r\n#See $.data for the source Arrow object\r\n```\r\n### Component(s)\r\n\r\nR","comments":["Thanks @xtimbeau, can confirm this is reproducible on Ubuntu on Arrow 15.0.1, looks like it's something we might need to pull out of the DuckDB object and apply to the Arrow object, though I haven't checked to see if this is a regression or something we never implemented.  \r\n\r\nWould you be interested in submitting a PR at all?","you mean correct the bug in the code?\r\nIf it involves some R I can accept the challenge\u2026 but C I prefer to pass!\r\n\r\nLe ven. 22 mars 2024 \u00e0 14:56, Nic Crane ***@***.***> a \u00e9crit :\r\n\r\n> Thanks @xtimbeau <https:\/\/github.com\/xtimbeau>, can confirm this is\r\n> reproducible on Ubuntu on Arrow 15.0.1, looks like it's something we might\r\n> need to pull out of the DuckDB object and apply to the Arrow object, though\r\n> I haven't checked to see if this is a regression or something we never\r\n> implemented.\r\n>\r\n> Would you be interested in submitting a PR at all?\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/apache\/arrow\/issues\/40640#issuecomment-2015164232>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ANA2KENUG5MZX6JO73CA743YZQ2BVAVCNFSM6AAAAABE35U5YWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMJVGE3DIMRTGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n"],"labels":["Type: bug","Component: R","Priority: Critical"]},{"title":"[Parquet] Make default fallback encoding choice smarter","body":"### Describe the enhancement requested\n\nCurrently, the choice of default encoding for a non-dictionary data page is trivial.\r\nIt happens in two places:\r\n1. in the `FallbackToPlainEncoding` function for columns for which dictionary encoding is attempted:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/5718a2862b4254d8bf938912d8958837ac7313a5\/cpp\/src\/parquet\/column_writer.cc#L1567-L1580\r\n2. in the `ColumnWriter::Make` factory function for columns for which dictionary encoding is not attempted:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/5718a2862b4254d8bf938912d8958837ac7313a5\/cpp\/src\/parquet\/column_writer.cc#L2375-L2382\r\n\r\nI'll note that parquet-mr does not limit dictionary encoding fallback to PLAIN, even for \"v1\" Parquet files:\r\nhttps:\/\/github.com\/apache\/parquet-mr\/blob\/95b004c3df473e3ab0963dc5136934ce5235d5df\/parquet-column\/src\/main\/java\/org\/apache\/parquet\/column\/values\/factory\/DefaultV1ValuesWriterFactory.java#L124-L139\r\n\r\nWe should probably consolidate the logic from the two functions above and make it more sophisticated, allowing the best encoding for the selected Parquet version.\r\n\r\nAlso related: https:\/\/github.com\/apache\/arrow\/issues\/38441\n\n### Component(s)\n\nC++, Parquet","comments":[],"labels":["Type: enhancement","Component: Parquet","Component: C++"]},{"title":"[C#] ArrowStreamReader.Schema returns null before the first record batch is read","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nIf the schema hasn't already been read, this should force it to be read.\n\n### Component(s)\n\nC#","comments":[],"labels":["Type: bug","Component: C#"]},{"title":"[Python] ORC Reader aborts when timezone file is missing","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThis is an upstream report of https:\/\/github.com\/pandas-dev\/pandas\/issues\/56292\r\n\r\nI noticed when running the pandas test suite I was getting this error:\r\n\r\n```\r\npandas\/tests\/io\/test_orc.py::test_orc_reader_basic terminate called after throwing an instance of 'orc::TimezoneError'\r\n  what():  Can't open \/usr\/share\/zoneinfo\/US\/Pacific\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007eff1a912780 (most recent call first):\r\n```\r\n\r\nThe workaround is to create that timezone file:\r\n\r\n```\r\n$ sudo mkdir -p \/usr\/share\/zoneinfo\/US\r\n$ sudo ln -s \/usr\/share\/zoneinfo\/America\/Los_Angeles \/usr\/share\/zoneinfo\/US\/Pacific\r\n```\r\n\r\nAlthough I think the error should be handled more gracefully than via abort\n\n### Component(s)\n\nPython","comments":["@wgtmac will improve this.\r\nSee also:\r\n* https:\/\/github.com\/apache\/arrow\/issues\/36026#issuecomment-2006087340 and related discussions\r\n* https:\/\/github.com\/apache\/orc\/pull\/1587","This seems to be related to the installed version of tz database on the test machine. I checked my laptop and the path `\/usr\/share\/zoneinfo\/US\/Pacific` exists. Could you verify the version by checking `\/usr\/share\/doc\/tzdata\/version` file? @WillAyd ","That file does not exist for me. This is running popOS 22.04","Could you try installing the `tzdata-legacy` package?","I don't see that package for 22.04 - I think first appeared in 23.04?","Oh, sorry. Could you install `tzdata`?","It is already installed - `tzdata is already the newest version (2024a-0ubuntu0.22.04).`","Hmm. `tzdata` must install `\/usr\/share\/zoneinfo\/US\/Pacific`: https:\/\/packages.ubuntu.com\/jammy\/all\/tzdata\/filelist ","Ah OK - interesting indeed. That must have been deleted off of my system somehow, but I do see that in a recovery OS.\r\n\r\nHappy to close this issue if we want to chalk it up to an unsupported system configuration"],"labels":["Type: bug","Component: Python"]},{"title":"[R] I can't update from arrow 13 to 14, and 15.","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nHere is the details. @amoeba \r\n```\r\n> install.packages(\"arrow\")\r\nInstalling package into \u2018\/usr\/local\/lib\/R\/4.3\/site-library\u2019\r\n(as \u2018lib\u2019 is unspecified)\r\ntrying URL 'https:\/\/cran.rstudio.com\/src\/contrib\/arrow_15.0.1.tar.gz'\r\nContent type 'application\/x-gzip' length 4373377 bytes (4.2 MB)\r\n==================================================\r\ndownloaded 4.2 MB\r\n\r\n* installing *source* package \u2018arrow\u2019 ...\r\n** package \u2018arrow\u2019 successfully unpacked and MD5 sums checked\r\n** using staged installation\r\n*** pkg-config found.\r\n*** Trying Arrow C++ found by pkg-config: \/usr\/local\/Cellar\/apache-arrow\/15.0.1\r\n**** C++ library version 15.0.1 is supported by R version 15.0.1\r\n------------------------- NOTE ---------------------------\r\nThere was an issue preparing the Arrow C++ libraries.\r\nSee https:\/\/arrow.apache.org\/docs\/r\/articles\/install.html\r\n----------------------------------------------------------\r\n\r\nTest compile error: In file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:20:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cstdint:149:5: error: <cstdint> tried including <stdint.h> but didn't find libc++'s <stdint.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cstdint> tried including <stdint.h> but didn't find libc++'s <stdint.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:22:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/memory:883:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/align.h:13:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cstddef:46:5: error: <cstddef> tried including <stddef.h> but didn't find libc++'s <stddef.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cstddef> tried including <stddef.h> but didn't find libc++'s <stddef.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:22:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/memory:884:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/allocate_at_least.h:13:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/allocator_traits.h:14:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/construct_at.h:23:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/new:96:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cstdlib:90:5: error: <cstdlib> tried including <stdlib.h> but didn't find libc++'s <stdlib.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cstdlib> tried including <stdlib.h> but didn't find libc++'s <stdlib.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:22:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/memory:898:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/shared_ptr.h:31:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/unique_ptr.h:17:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__functional\/hash.h:28:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cstring:66:5: error: <cstring> tried including <string.h> but didn't find libc++'s <string.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cstring> tried including <string.h> but didn't find libc++'s <string.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:22:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/memory:898:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/shared_ptr.h:42:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/atomic:523:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__chrono\/duration.h:19:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/ratio:83:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/climits:46:5: error: <climits> tried including <limits.h> but didn't find libc++'s <limits.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <climits> tried including <limits.h> but didn't find libc++'s <limits.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:22:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/memory:898:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__memory\/shared_ptr.h:42:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/atomic:2668:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cmath:320:5: error: <cmath> tried including <math.h> but didn't find libc++'s <math.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cmath> tried including <math.h> but didn't find libc++'s <math.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:23:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/string:561:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__string\/char_traits.h:24:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cstdio:104:5: error: <cstdio> tried including <stdio.h> but didn't find libc++'s <stdio.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cstdio> tried including <stdio.h> but didn't find libc++'s <stdio.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:23:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/string:561:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__string\/char_traits.h:29:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cwchar:108:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cwctype:54:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cctype:43:5: error: <cctype> tried including <ctype.h> but didn't find libc++'s <ctype.h> header.           This usually means that your header search paths are not configured properly.            The header search paths should contain the C++ Standard Library headers before           any C Standard Library.\r\n#   error <cctype> tried including <ctype.h> but didn't find libc++'s <ctype.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:23:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/string:561:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__string\/char_traits.h:29:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cwchar:108:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cwctype:59:5: error: <cwctype> tried including <wctype.h> but didn't find libc++'s <wctype.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cwctype> tried including <wctype.h> but didn't find libc++'s <wctype.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:23:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/string:561:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__string\/char_traits.h:29:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cwchar:113:5: error: <cwchar> tried including <wchar.h> but didn't find libc++'s <wchar.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cwchar> tried including <wchar.h> but didn't find libc++'s <wchar.h> header. \\\r\n    ^\r\nIn file included from <stdin>:1:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/api.h:22:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array.h:41:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/array_base.h:26:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/array\/data.h:27:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/buffer.h:29:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/device.h:26:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/result.h:27:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/status.h:25:\r\nIn file included from \/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include\/arrow\/util\/string_builder.h:21:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/ostream:171:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/ios:221:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__locale:18:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/mutex:192:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__mutex_base:20:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/system_error:149:\r\nIn file included from \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/__errc:104:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX.sdk\/usr\/include\/c++\/v1\/cerrno:31:5: error: <cerrno> tried including <errno.h> but didn't find libc++'s <errno.h> header.           This usually means that your header search paths are not configured properly.           The header search paths should contain the C++ Standard Library headers before           any C Standard Library, and you are probably using compiler flags that make that           not be the case.\r\n#   error <cerrno> tried including <errno.h> but didn't find libc++'s <errno.h> header. \\\r\n    ^\r\n11 errors generated.\r\nFailing compile command: clang++ -E -I\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX.sdk\/usr\/include -I\/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -g -O2 -std=gnu++17 -xc++ -\r\nPKG_CFLAGS=-I\/usr\/local\/Cellar\/apache-arrow\/15.0.1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3\r\nPKG_LIBS=-larrow_dataset -lparquet -larrow_acero -larrow \r\nERROR: configuration failed for package \u2018arrow\u2019\r\n* removing \u2018\/usr\/local\/lib\/R\/4.3\/site-library\/arrow\u2019\r\n* restoring previous \u2018\/usr\/local\/lib\/R\/4.3\/site-library\/arrow\u2019\r\nWarning in install.packages :\r\n  installation of package \u2018arrow\u2019 had non-zero exit status\r\n```\r\n### Component(s)\r\n\r\nR","comments":["This looks like arrow installed via brew is found and then there is a mixup of the headers.\r\nPlease try installing again and set the following envvars before you do: `FORCE_BUNDLED_BUILD=true` and `LIBARROW_BINARY=true`.","> This looks like arrow installed via brew is found and then there is a mixup of the headers. Please try installing again and set the following envvars before you do: `FORCE_BUNDLED_BUILD=true` and `LIBARROW_BINARY=true`.\r\n\r\nI tried this but no luck. I have apache-arrow 15.0.0.1 installed at the system level but upgrading it in RStudio is still giving to non-zero exit status.","> I have apache-arrow 15.0.0.1 installed at the system level\r\n\r\nYes via brew which seems to be the problem, you can also try removing that (if you don't need it for anything else obviously but even than you should be able to reinstall it after installing arrow)\r\n\r\nYou can also use https:\/\/apache.r-universe.dev\/arrow to get a binary with all features.","I wonder if something is misconfigured with your XCode\/CommandLineTools installation. I can install arrow from source linking against Homebrew's arrow 15.0.1 just fine. That said, installing from R-universe is a great option.\r\n\r\nIf you want to try again @YinOsu, can you start a fresh R session, set this environment variable, reinstall from source, and paste the results here in a `<details><\/details>` block?\r\n\r\n```r\r\n> Sys.setenv(\"ARROW_R_DEV\"=TRUE)\r\n> install.packages(\"arrow\", type=\"source\")\r\n```\r\n\r\nMine is below for comparison:\r\n\r\n<details>\r\n<summary>Output for <code>install.packages(\"arrow\", type=\"source\")<\/code><\/summary>\r\n\r\n```sh\r\n> install.packages(\"arrow\", type=\"source\")\r\ntrying URL 'https:\/\/cran.rstudio.com\/src\/contrib\/arrow_15.0.1.tar.gz'\r\nContent type 'application\/x-gzip' length 4373377 bytes (4.2 MB)\r\n==================================================\r\ndownloaded 4.2 MB\r\n\r\n* installing *source* package \u2018arrow\u2019 ...\r\n** package \u2018arrow\u2019 successfully unpacked and MD5 sums checked\r\n** using staged installation\r\n*** pkg-config found.\r\n*** Trying Arrow C++ found by pkg-config: \/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\r\n**** C++ library version 15.0.1 is supported by R version 15.0.1\r\nPKG_CFLAGS=-I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3\r\nPKG_LIBS=-L\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/lib -larrow_dataset -lparquet -larrow_acero -larrow\r\n** libs\r\nusing C++ compiler: \u2018Apple clang version 15.0.0 (clang-1500.3.9.4)\u2019\r\nusing C++17\r\nusing SDK: \u2018\u2019\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c RTasks.cpp -o RTasks.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c altrep.cpp -o altrep.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c array.cpp -o array.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c array_to_vector.cpp -o array_to_vector.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c arraydata.cpp -o arraydata.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c arrowExports.cpp -o arrowExports.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c bridge.cpp -o bridge.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c buffer.cpp -o buffer.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c chunkedarray.cpp -o chunkedarray.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compression.cpp -o compression.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compute-exec.cpp -o compute-exec.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c compute.cpp -o compute.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c config.cpp -o config.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c csv.cpp -o csv.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c dataset.cpp -o dataset.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c datatype.cpp -o datatype.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c expression.cpp -o expression.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c extension-impl.cpp -o extension-impl.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c feather.cpp -o feather.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c field.cpp -o field.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c filesystem.cpp -o filesystem.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c io.cpp -o io.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c json.cpp -o json.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c memorypool.cpp -o memorypool.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c message.cpp -o message.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c parquet.cpp -o parquet.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c r_to_arrow.cpp -o r_to_arrow.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatch.cpp -o recordbatch.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatchreader.cpp -o recordbatchreader.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c recordbatchwriter.cpp -o recordbatchwriter.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c safe-call-into-r-impl.cpp -o safe-call-into-r-impl.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c scalar.cpp -o scalar.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c schema.cpp -o schema.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c symbols.cpp -o symbols.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c table.cpp -o table.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c threadpool.cpp -o threadpool.o\r\nclang++ -arch arm64 -std=gnu++17 -I\"\/Library\/Frameworks\/R.framework\/Resources\/include\" -DNDEBUG -I\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/include   -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -DARROW_R_WITH_S3 -I'\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/cpp11\/include' -I\/opt\/R\/arm64\/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c type_infer.cpp -o type_infer.o\r\nclang++ -arch arm64 -std=gnu++17 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -L\/Library\/Frameworks\/R.framework\/Resources\/lib -L\/opt\/R\/arm64\/lib -o arrow.so RTasks.o altrep.o array.o array_to_vector.o arraydata.o arrowExports.o bridge.o buffer.o chunkedarray.o compression.o compute-exec.o compute.o config.o csv.o dataset.o datatype.o expression.o extension-impl.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o r_to_arrow.o recordbatch.o recordbatchreader.o recordbatchwriter.o safe-call-into-r-impl.o scalar.o schema.o symbols.o table.o threadpool.o type_infer.o -L\/opt\/homebrew\/Cellar\/apache-arrow\/15.0.1_1\/lib -larrow_dataset -lparquet -larrow_acero -larrow -F\/Library\/Frameworks\/R.framework\/.. -framework R -Wl,-framework -Wl,CoreFoundation\r\ninstalling to \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\r\n** R\r\n** inst\r\n** byte-compile and prepare package for lazy loading\r\n** help\r\n*** installing help indices\r\n** building package indices\r\n** testing if installed package can be loaded from temporary location\r\n** checking absolute paths in shared objects and dynamic libraries\r\n** testing if installed package can be loaded from final location\r\n** testing if installed package keeps a record of temporary installation path\r\n* DONE (arrow)\r\n\r\nThe downloaded source packages are in\r\n\t\u2018\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpoX8XNC\/downloaded_packages\u2019\r\n```"],"labels":["Type: bug","Component: R"]},{"title":"[C++] Lost several conjunctions in SimplifyWithGuarantee","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe codes below seems to have forgotten to handle the conjunctions of AND and OR.\r\n\r\nForget to handle `AND` ?\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/5718a2862b4254d8bf938912d8958837ac7313a5\/cpp\/src\/arrow\/compute\/expression.cc#L914\r\n\r\nForget to handle `AND` an `OR` ?\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/5718a2862b4254d8bf938912d8958837ac7313a5\/cpp\/src\/arrow\/compute\/expression.cc#L877-L903\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"[Go][Parquet] Enable writing of Parquet footer without closing file","body":"### Describe the enhancement requested\n\nThe Parquet file format allows a file to continue to accumulate row groups after a footer has been written, as long as a new and cumulative footer is written afterward.  This is useful if one is writing a stream of data directly to Parquet and need to make sure that that data is fully durable and readable within some time bound.  For this purpose I propose a new method `FlushWithFooter`  on `file.Writer` that like its sibling `Close` would close any open row group and prepare and write out the file footer.  Unlike `Close` it would leave the writer's metadata structures intact, allowing subsequent row groups to be written without starting over, thus ensuring that the metadata written into subsequent footers via `FlushWithFooter` or `Close` is inclusive of all row groups written since the beginning of the file.\r\n\r\nThe alternative, and what is supported today, is to close the open file once the time bound has been reached and start a new one.  This works for durability, but is inefficient for readers since they must now open and process the footers of a potentially much larger number of files.  The typical workflow is to have a second process \"compact\" these smaller files to produce larger files that not only consolidate footers but apply other optimizations (such as z-ordering) that holistically reorganize the consolidate data to match observed or expected query patterns.  While effective for readers of older data, such compactions take time and significant resources to execute, putting a practical lower bound on the freshness of their outputs.\r\n\r\nThis feature, if adopted, would allow writers to produce data into a modest and predictable number of files within a strict time bound for durability such that readers enjoy that same time bound and modest number of files to efficiently query fresh data without intervening compaction.  Compaction would still be recommended, both to apply holistic optimizations and to collapse the extra footers inserted into the original files, but it would be less urgent since compaction would no longer be a constraint on freshness or the manageability of file cardinality.\n\n### Component(s)\n\nGo, Parquet","comments":[],"labels":["Type: enhancement","Component: Parquet","Component: Go"]},{"title":"GH-36399: [Python] Add missing join methods to `RecordBatch`","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nWe should try to keep the methods of Table and RecordBatch as close to each other as possible.\r\n\r\n### What changes are included in this PR?\r\n\r\nThis moves the `join` and `join_asof` methods from `Table` to `_Tablular` which makes them available for `RecordBatch`. Additionally, `_perform_join` is modified to use a `RecordBatchReaderSourceNode` rather than a `TableSourceNode` when either of the operands is a single `RecordBatch`.\r\n\r\n### Are these changes tested?\r\n\r\nYes, all the tests which applied to `Table.join` and `Table.join_asof` have been parameterized to also test `RecordBatch.join` and `RecordBatch.join_asof`.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo, I have kept all argument names the same for backwards compatibility. Ideally we might rename the `right_table` argument to just `right` to avoid confusion - but this would be a breaking change.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #36399","comments":[":warning: GitHub issue #30915 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #30915 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #30915 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #30915 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: Python","awaiting review"]},{"title":"[R] In R\/MacOS writing a parquet to a non existant path crashes","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nhere is a simple reprex:\r\n\r\n```r\r\nif(fs::file_exists(\"\/tmp\/test\")) fs::dir_delete(\"\/tmp\/test\")\r\ndata <- tibble::tibble(\r\n  x = 1:10,\r\n  g = floor(0:9\/5))\r\narrow::write_parquet(data, \"\/tmp\/test\/part-0.parquet\") # crashes R\r\nfs::dir_create(\"\/tmp\/test\")\r\narrow::write_parquet(data, \"\/tmp\/test\/part-0.parquet\") # if path exists, everything is ok\r\n```\r\nRunning arrow 15.0.1 on R 4.3.3 on MacOS 14.4\r\n\r\n### Component(s)\r\n\r\nR","comments":["Thanks for reporting this @xtimbeau.  I can't reproduce it on Ubuntu, so might be an issue that only shows up on macOS.  Would you mind seeing if you can run the code again with the debugger attached so we can see if there are weird things going on in the C++?  There are instructions on how to do that here: https:\/\/arrow.apache.org\/docs\/r\/articles\/developers\/debugging.html#running-r-code-with-the-c-debugger-attached"],"labels":["Type: bug","Component: R"]},{"title":"[Python][CI] Failing test_dateutil_tzinfo_to_string due to new release of python-dateutil ","body":"### Rationale for this change\r\n\r\n`test_dateutil_tzinfo_to_string` started failing with:\r\n\r\n```Python\r\n         tz = dateutil.tz.gettz('Europe\/Paris')\r\n>       assert pa.lib.tzinfo_to_string(tz) == 'Europe\/Paris'\r\nE       AssertionError: assert 'Europe\/Monaco' == 'Europe\/Paris'\r\nE         - Europe\/Paris\r\nE         + Europe\/Monaco\r\n```\r\n\r\nand it is most probably due to new release of `python-dateutil` package.\r\n\r\n### What changes are included in this PR?\r\n\r\nCurrently, we are using this PR to debug the test on Windows platform.\r\n\r\n### Are these changes tested?\r\n\r\nA tests is failing and it needs to be fixed.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","We have assumed correctly there is an issue in `python-dateutil`:\r\n\r\n```\r\n_______________________ test_dateutil_tzinfo_to_string ________________________\r\n    def test_dateutil_tzinfo_to_string():\r\n        pytest.importorskip(\"dateutil\")\r\n        import dateutil.tz\r\n    \r\n        tz = dateutil.tz.UTC\r\n        assert pa.lib.tzinfo_to_string(tz) == 'UTC'\r\n        tz = dateutil.tz.gettz('Europe\/Paris')\r\n        print(tz)\r\n>       assert pa.lib.tzinfo_to_string(tz) == 'Europe\/Paris'\r\nE       AssertionError: assert 'Europe\/Monaco' == 'Europe\/Paris'\r\nE         - Europe\/Paris\r\nE         + Europe\/Monaco\r\npyarrow\\tests\\test_types.py:356: AssertionError\r\n---------------------------- Captured stdout call -----------------------------\r\ntzfile('Europe\/Monaco')\r\n============================== warnings summary ===============================\r\npyarrow\/tests\/test_pandas.py::TestConvertMetadata::test_empty_list_metadata\r\n```\r\n\r\n Will report upstream.","Upstream issue: https:\/\/github.com\/dateutil\/dateutil\/issues\/1353"],"labels":["Component: Python","awaiting review"]},{"title":"[R] Collect crashes on R when partioning col is in parquet files and in subfolder names","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nBuilding datasets from folders, subfolders and parquet files can make R arrow crashes:\r\n\r\nHere is the reprex (run the 3 tests separatly):\r\n\r\n```r\r\n# test one hive field in parquet and folder name crashes --------\r\nlibrary(tidyverse) \r\ndata <- tibble(\r\n  x = 1:10,\r\n  g = floor(0:9\/5))\r\nif(fs::file_exists(\"\/tmp\/test\")) fs::dir_delete(\"\/tmp\/test\")\r\n# building subfolders\r\nwalk(0:1, ~{\r\n  dd <- data |> filter(g==.x)\r\n  fn <- str_c(\"\/tmp\/test\/g=\", .x, \"\/part-0.parquet\")\r\n  fs::dir_create(str_c(\"\/tmp\/test\/g=\", .x))\r\n  arrow::write_parquet(dd, fn)})\r\narrow::open_dataset(\"\/tmp\/test\/g=1\") |> collect() # ok\r\narrow::open_dataset(\"\/tmp\/test\/g=0\") |> collect() # ok\r\narrow::open_dataset(\"\/tmp\/test\/\")  |> collect()  # crashes\r\n\r\n# test two hive field in parquet and not in folder name ok --------\r\nlibrary(tidyverse) \r\ndata <- tibble(\r\n  x = 1:10,\r\n  g = floor(0:9\/5))\r\nif(fs::file_exists(\"\/tmp\/test\")) fs::dir_delete(\"\/tmp\/test\")\r\nwalk(0:1, ~{\r\n  dd <- data |> filter(g==.x)\r\n  fn <- str_c(\"\/tmp\/test\/\", .x, \"\/part-0.parquet\")\r\n  fs::dir_create(str_c(\"\/tmp\/test\/\", .x))\r\n  arrow::write_parquet(dd, fn)})\r\narrow::open_dataset(\"\/tmp\/test\/1\") |> collect() # pass\r\narrow::open_dataset(\"\/tmp\/test\/0\") |> collect() # pass\r\narrow::open_dataset(\"\/tmp\/test\/\")  |> collect()  # pass\r\n\r\n# test three hive field not in parquet but in folder name ok --------\r\nlibrary(tidyverse) \r\ndata <- tibble(\r\n  x = 1:10,\r\n  g = floor(0:9\/5))\r\nif(fs::file_exists(\"\/tmp\/test\")) fs::dir_delete(\"\/tmp\/test\")\r\nwalk(0:1, ~{\r\n  dd <- data |> filter(g==.x) |> select(-g)\r\n  fn <- str_c(\"\/tmp\/test\/g=\", .x, \"\/part-0.parquet\")\r\n  fs::dir_create(str_c(\"\/tmp\/test\/g=\", .x))\r\n  arrow::write_parquet(dd, fn)})\r\narrow::open_dataset(\"\/tmp\/test\/g=1\") |> collect() # pass\r\narrow::open_dataset(\"\/tmp\/test\/g=0\") |> collect() # pass\r\narrow::open_dataset(\"\/tmp\/test\/\")  |> collect() # pass\r\n```\r\n\r\nUsing arrow 15.0.1, MacOS 14.4, RStudio, R 4.3.3\r\n\n\n### Component(s)\n\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"[Python][Docs] API docs are broken for dev version","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nDevelopment version of the docs seems to be broken for generated Python API Reference documentation.\r\n\r\nExample: https:\/\/arrow.apache.org\/docs\/dev\/python\/generated\/pyarrow.Schema.html\r\n![Screenshot 2024-03-18 at 09 13 57](https:\/\/github.com\/apache\/arrow\/assets\/16418547\/030cb557-7088-492e-b24a-7966b7be4e39)\r\n\r\nDid a hard refresh, hope it is not just me?\n\n### Component(s)\n\nDocumentation, Python","comments":["cc @raulcd ","This started with the update https:\/\/github.com\/apache\/arrow-site\/commit\/05cb617c8fe58ff90911945f5a25efa94695ba1f from the nightly build of 2024-03-12, so that matches the timeframe of potential candidate cause https:\/\/github.com\/apache\/arrow\/pull\/40442","#40442 may be related...: https:\/\/github.com\/apache\/arrow\/pull\/40442#issuecomment-2007300966\r\n\r\nBut it has different result:\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/27350\/dab337d5-213d-4918-aeaa-e0f428035e71)\r\n","That looks like the same issue","Starting to run the doc build locally, it seems that the issue is that the generated API files are now generated as .md files (but there content is restructuredtext). \r\n\r\nWith https:\/\/www.sphinx-doc.org\/en\/master\/man\/sphinx-autogen.html it seems there is an option to specify which extension to use, but not sure how that can be tweaked from conf.py\r\n\r\nIt seems that adding `myst_parser` to the list of extensions has (unintended) effect on the default extension that sphinx uses for this ..","Oh... Does this solve?\r\n\r\n```diff\r\ndiff --git a\/docs\/source\/conf.py b\/docs\/source\/conf.py\r\nindex 7915e2c2c4..b67833112b 100644\r\n--- a\/docs\/source\/conf.py\r\n+++ b\/docs\/source\/conf.py\r\n@@ -208,8 +208,8 @@ templates_path = ['_templates']\r\n #\r\n \r\n source_suffix = {\r\n-    '.md': 'markdown',\r\n     '.rst': 'restructuredtext',\r\n+    '.md': 'markdown',\r\n }\r\n \r\n autosummary_generate = True\r\n```","Hmm. Sphinx may have a problem:\r\n\r\n```diff\r\ndiff --git a\/sphinx\/ext\/autosummary\/__init__.py b\/sphinx\/ext\/autosummary\/__init__.py\r\nindex 22e616c3f..9a469d23e 100644\r\n--- a\/sphinx\/ext\/autosummary\/__init__.py\r\n+++ b\/sphinx\/ext\/autosummary\/__init__.py\r\n@@ -770,7 +770,7 @@ class AutoLink(SphinxRole):\r\n \r\n def get_rst_suffix(app: Sphinx) -> str | None:\r\n     def get_supported_format(suffix: str) -> tuple[str, ...]:\r\n-        parser_class = app.registry.get_source_parsers().get(suffix)\r\n+        parser_class = app.registry.get_source_parsers().get(suffix.removeprefix('.'))\r\n         if parser_class is None:\r\n             return ('restructuredtext',)\r\n         return parser_class.supported\r\n```","> Oh... Does this solve?\r\n\r\nThat indeed seems to do the trick ..","* https:\/\/github.com\/sphinx-doc\/sphinx\/issues\/12147\r\n* https:\/\/github.com\/sphinx-doc\/sphinx\/pull\/12149","The fix was merged."],"labels":["Type: bug","Component: Python","Component: Documentation"]},{"title":"[C++] Is there a better way to support 'Any'\/'All' syntax with function expression","body":"### Describe the enhancement requested\r\n\r\nQuery like below in PostgreSQL:\r\n```sql\r\ntpch1g=# explain select n_name from nation where n_nationkey > ANY (array[9,2,8]);\r\n                        QUERY PLAN\r\n-----------------------------------------------------------\r\n Seq Scan on nation  (cost=0.00..12.34 rows=120 width=104)\r\n   Filter: (n_nationkey > ANY ('{9,2,8}'::integer[]))\r\n```\r\n\r\nPostgresql support ANY\/ALL syntax with specific internal operation like `less`,`equal`, `great`, `less_equal` ...\r\n\r\nWe want to use arrow's built-in compute functions to implement this with high performance. Seems there is no suitable function to support it.\r\n- `IsIn`, can only support the `equal` situation by use a simple internal Memotable.\r\n- `Any` and `All` in api_aggregate can only support boolean array as input.\r\n\r\nI know that it's a special syntax in PostgreSQL. For now, the basic implementation is deal the input const array and field_ref array with a nested-loop, but performance is poor!\r\n\r\n Is there any suggestion to implement this kind of syntax with high performance? Or should we support this syntax in compute kernel, the function seems reasonable?\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"pyarrow.Array.from_pandas() sets pandas Timedeltas in object-dtyped Series to 0","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```python\r\n>>> pa.Array.from_pandas(pd.Series([pd.Timedelta(np.timedelta64(1, 'ms'))], dtype=object))\r\n<pyarrow.lib.DurationArray object at 0x7f5ad07abf40>\r\n[\r\n  0\r\n]\r\n```\r\n\r\nThe number `1` doesn't matter, but the time unit has to be `'ms'` or `'s'` and the dtype has to be `object` to trigger the error.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"Running TPCH queries in Acero through Substrait ","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nIt would be nice if Acero could run the tpc-h queries through substrait, which seems not possible at the moment.\r\n\r\nI converted the TPCH Q1 from sql to substrait using DuckDB and then tried running it with Acero. This resulted in the following errors:\r\n- `Invalid: substrait::Expression::Cast::FailureBehavior unspecified; must be FAILURE_BEHAVIOR_RETURN_NULL or FAILURE_BEHAVIOR_THROW_EXCEPTION`: this can be fixed by setting the `FailureBehavior` to `FAILURE_BEHAVIOR_THROW_EXCEPTION` inside the cast relations in substrait.\r\n- `NotImplemented: Unsupported aggregation phase 'AGGREGATION_PHASE_UNSPECIFIED'.  Only INITIAL_TO_RESULT is supported`: this can be fixed by setting the phase to the `INITIAL_TO_RESULT` value. \r\n- `NotImplemented: No conversion function exists to convert the Substrait function * to an Arrow call expression`: this can be fixed by changing the operations names from \"*\"->\"multiply\" and similarly for other arithmetic operations.\r\n- `Invalid: Expected an aggregate call #sum to have a direct reference`: I don't know how to fix this.\r\n\r\nDo you have the tpch queries written in substrait in such a way that Acero can run them? Is the Acero's support for substrait enough to run the tpch queries?\r\n\r\nThank you!\r\n\r\n### Component(s)\r\n\r\nBenchmarking, C++","comments":["@richtia do we have all the TPC-H queries in Substrait format? IIRC there are tests that run these against Acero. Can you give @kabicm a pointer to that? Thanks","> @richtia do we have all the TPC-H queries in Substrait format? IIRC there are tests that run these against Acero. Can you give @kabicm a pointer to that? Thanks\r\n\r\nYeah, here are all the TPC-H queries in substrait format (there are actually a few missing because substrait couldn't convert them at the time, need to see if i that's been fixed): https:\/\/github.com\/substrait-io\/consumer-testing\/tree\/main\/substrait_consumer\/tests\/integration\/queries\/tpch_substrait_plans\r\n\r\nThis is the acero test that runs all of them: https:\/\/github.com\/substrait-io\/consumer-testing\/blob\/main\/substrait_consumer\/tests\/integration\/test_acero_tpch.py\r\n\r\nUnfortunately Acero isn't at a point where it's able to completely run any of these fully through.\r\n\r\nThey all fail for various reasons, which can be seen from running the tests.","Thanks @ianmcook and @richtia! \r\n\r\nGreat to see the substrait plans for the TPC-H queries, thanks for the pointers! However, I am still getting the same type of issues as mentioned before when trying to run them with Acero.\r\n\r\nAt the moment, it seems to me that the only way to run the TPCH queries with Acero in C++ is by creating Acero's exec plan. \r\n\r\nAre there maybe Acero query plans for TPC-H queries in C++? I found the Q1 and Q6 Acero plans in C++, using the Declaration API.","@kabicm, yeah...Acero isn't at the point where it can fully run any of those TPC-H substrait plans.  Most  of them either hit the issue you've been encountering or this one: \r\n```\r\npyarrow.lib.ArrowInvalid: A join rel's expression must be a simple equality between keys but got true\r\n```\r\n\r\nFrom the C++ side, maybe @vibhatha could help you out.  He had been working on a bit of the Acero substrait consumer development previoiusly."],"labels":["Component: C++","Component: Benchmarking","Type: usage"]},{"title":"[Docs][HTTP] Clarify when to use batch-at-a-time vs. one-shot approach for receiving data","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nAmong the simple HTTP GET client examples in [`arrow-experiments\/http\/get_simple`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple):\r\n\r\n- Some iterate over the record batches as they stream in from the server (i.e. \"streaming\" approach).\r\n- Some just make a single function call that collects the full data (i.e. \"one-shot\" approach).\r\n\r\nFor example:\r\n- The [Python client example](https:\/\/github.com\/apache\/arrow-experiments\/blob\/main\/http\/get_simple\/python\/client\/client.py) shows how to iterate over the batches calling `reader.read_next_batch()`, whereas it could have just called `reader.read_all()` which would be simpler.\r\n- The [Ruby client example](https:\/\/github.com\/apache\/arrow-experiments\/blob\/main\/http\/get_simple\/ruby\/client\/client.rb) goes for the simpler all-at-once approach, whereas it could have used a batch-at-a-time approach like in [this example](https:\/\/gist.github.com\/amoeba\/b1ba73a1e863e689d4a2ee65601a18c5).\r\n\r\nFor many use cases, it makes no difference which approach is used, and we should just prioritize whatever is syntactically simplest.\r\n\r\nBut for some use cases, the batch-at-a-time approach will be preferred or needed for specific reasons, such as:\r\n- The receiver wants to start processing batches _before_ the final batch is received.\r\n- The receiver wants to stream the received data to a sink without accumulating it in memory.\r\n\r\nWe should clarify this in the Arrow-over-HTTP conventions doc, and wherever possible we should provide examples showing both approaches.\r\n\r\n### Component(s)\r\n\r\nDocumentation","comments":[],"labels":["Component: Documentation","Type: usage"]},{"title":"[Docs][HTTP] Track status of IPC buffer compression support in Arrow libraries","body":"As noted on the [Implementation Status docs page (dev version)](https:\/\/arrow.apache.org\/docs\/dev\/status.html#ipc-format), some of the Arrow libraries lack (full) support for IPC buffer compression. The Arrow-over-HTTP conventions doc should note this and link to the associated issues. Most notably:\r\n- LZ4 buffer compression in the Arrow Java library is very slow https:\/\/github.com\/apache\/arrow\/issues\/27743\r\n- The Arrow JavaScript library lacks support for IPC buffer compression https:\/\/github.com\/apache\/arrow\/issues\/24833\r\n- The Arrow C# library gains support for _writing_ compressed IPC buffers in version 16.0.0 https:\/\/github.com\/apache\/arrow\/pull\/39871\r\n\r\n### Component(s)\r\n\r\nDocumentation","comments":[],"labels":["Component: Documentation","Type: usage"]},{"title":"[Python] Create simple HTTP server example using Flask","body":"### Describe the enhancement requested\n\nThe HTTP server example at [`arrow-experiments\/http\/get_simple\/python\/server`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple\/python\/server) uses Python's built in `http.server` which lacks many important features and is not recommended for production use. We should create another simple Python HTTP server example using Flask.\r\n\r\n@paleolimbot has an example here showing how to do it: https:\/\/github.com\/paleolimbot\/2023-11-21_arrow-over-http-scratchpad\/blob\/main\/app.py\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[C++] ABI break in patch release 15.0.1","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nAn ABI break was reported between 15.0.0 and 15.0.1 in Fedora Linux ([downstream bug](https:\/\/bugzilla.redhat.com\/show_bug.cgi?id=2269811)).\r\n\r\nIt looks like https:\/\/github.com\/apache\/arrow\/commit\/91be098b56021b1f9569986b038bd46c3ed53701 changed the types of several class method parameters that appear in the public API from `const std::string&` to `std::string_view`.\r\n\r\n```\r\n================ changes of 'libarrow.so.1500.0.0'===============\r\n  Functions changes summary: 6 Removed, 0 Changed (11 filtered out), 6 Added functions\r\n  Variables changes summary: 0 Removed, 0 Changed, 0 Added variable\r\n\r\n  6 Removed functions:\r\n\r\n    [D] 'method bool arrow::KeyValueMetadata::Contains(const std::string&) const'    {_ZNK5arrow16KeyValueMetadata8ContainsERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE}\r\n    [D] 'method arrow::Status arrow::KeyValueMetadata::Delete(const std::string&)'    {_ZN5arrow16KeyValueMetadata6DeleteERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE}\r\n    [D] 'method int arrow::KeyValueMetadata::FindKey(const std::string&) const'    {_ZNK5arrow16KeyValueMetadata7FindKeyERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE}\r\n    [D] 'method arrow::Result<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > arrow::KeyValueMetadata::Get(const std::string&) const'    {_ZNK5arrow16KeyValueMetadata3GetERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE}\r\n    [D] 'method arrow::Status arrow::KeyValueMetadata::Set(const std::string&, const std::string&)'    {_ZN5arrow16KeyValueMetadata3SetERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES8_}\r\n    [D] 'function arrow::Status arrow::compute::internal::FSBFilterExec(arrow::compute::KernelContext*, const arrow::compute::ExecSpan&, arrow::compute::ExecResult*)'    {_ZN5arrow7compute8internal13FSBFilterExecEPNS0_13KernelContextERKNS0_8ExecSpanEPNS0_10ExecResultE}\r\n\r\n  6 Added functions:\r\n\r\n    [A] 'method bool arrow::KeyValueMetadata::Contains(std::string_view) const'    {_ZNK5arrow16KeyValueMetadata8ContainsESt17basic_string_viewIcSt11char_traitsIcEE}\r\n    [A] 'method arrow::Status arrow::KeyValueMetadata::Delete(std::string_view)'    {_ZN5arrow16KeyValueMetadata6DeleteESt17basic_string_viewIcSt11char_traitsIcEE}\r\n    [A] 'method int arrow::KeyValueMetadata::FindKey(std::string_view) const'    {_ZNK5arrow16KeyValueMetadata7FindKeyESt17basic_string_viewIcSt11char_traitsIcEE}\r\n    [A] 'method arrow::Result<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > arrow::KeyValueMetadata::Get(std::string_view) const'    {_ZNK5arrow16KeyValueMetadata3GetB5cxx11ESt17basic_string_viewIcSt11char_traitsIcEE}\r\n    [A] 'method arrow::Status arrow::KeyValueMetadata::Set(std::string, std::string)'    {_ZN5arrow16KeyValueMetadata3SetENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES6_}\r\n    [A] 'function arrow::Status arrow::compute::internal::PrimitiveTakeExec(arrow::compute::KernelContext*, const arrow::compute::ExecSpan&, arrow::compute::ExecResult*)'    {_ZN5arrow7compute8internal17PrimitiveTakeExecEPNS0_13KernelContextERKNS0_8ExecSpanEPNS0_10ExecResultE}\r\n\r\n================ end of changes of 'libarrow.so.1500.0.0'===============\r\n```\n\n### Component(s)\n\nC++","comments":["@pitrou Could you take a look at this? It seems that we should have kept the backward compatibility in the PR to follow the semantic versioning.\r\n","Ow... well, we could revert the KeyValueMetadata changes if we plan to do a new 15.0.x release.","FYI @raulcd ","I labeled this as a breaking change for the moment, since it seems to me that reverting the breaking change would be a breaking change too. If this isn't true (or if we don't revert the change) feel free to remove the label and probably leave a comment.","Yes, reverting would also be a breaking change. Also, we're unlikely to do a 15.0.3 IMHO."],"labels":["Type: bug","Component: C++","Breaking Change"]},{"title":"[Python] Create Python examples of HTTP GET Arrow client and server using HTTP compression","body":"### Describe the enhancement requested\n\nCreate simple Python-based HTTP client and server examples like the ones in [`arrow-experiments\/http\/get_simple` ](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple)  or [`arrow-experiments\/http\/get_range` ](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_range) but using HTTP compression.\r\n\r\nFor example, the client sends a request with header `Accept-Encoding: gzip, deflate` and the server sends a response with header `Content-Encoding: gzip` and with the response body gzip-compressed which the HTTP client library automatically decompresses.\n\n### Component(s)\n\nPython","comments":["watching"],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create Python examples of HTTP GET Arrow client and server using IPC buffer compression","body":"### Describe the enhancement requested\n\nCreate simple Python-based HTTP client and server examples like the ones in [`arrow-experiments\/http\/get_simple\/python` ](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple\/python) but using IPC buffer compression (lz4 or zstd).\r\n\r\nOn the server side, I believe this can be done by passing [`pa.ipc.IpcWriteOptions(compression=...)`](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.ipc.IpcWriteOptions.html) as the `options` argument to [`pa.ipc.new_stream()`](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.ipc.new_stream.html).\r\n\r\nAlso see [`pa.Codec`](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.Codec.html)\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create Python examples of HTTP POST Arrow client\/server supporting multipart\/form-data request ","body":"### Describe the enhancement requested\n\nContribute Python client and server examples to the [HTTP POST multipart examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/post_multipart). This should demonstrate a client sending a POST request to a server with a multipart request body (`Content-Type: multipart\/form-data`) containing JSON data (`Content-Type: application\/json`) and Arrow IPC stream data (`Content-Type: application\/vnd.apache.arrow.stream`).\n\n### Component(s)\n\nPython","comments":["A challenge with this approach is that the part boundaries must not occur in the data, as described here: https:\/\/www.w3.org\/TR\/html401\/interact\/forms.html#h-17.13.4.2\r\n\r\nSo to support all possible sequences of bytes that might occur in Arrow IPC stream data, the client would need to be able to dynamically choose a viable boundary sequence by checking that it is not contained in the IPC stream bytes to be sent in the request.\r\n\r\nOn the other hand, maybe it's sufficient for the client to simply use a part boundary that is highly unlikely to occur in the IPC stream bytes."],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create Python examples of HTTP GET Arrow client\/server supporting multipart\/mixed response","body":"### Describe the enhancement requested\r\n\r\nContribute Python client and server examples to the [HTTP GET multipart examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_multipart). This should demonstrate a server\/client pair that sends\/receives a multipart response (`Content-Type: multipart\/mixed`) containing JSON data (`Content-Type: application\/json`) and Arrow IPC stream data (`Content-Type: application\/vnd.apache.arrow.stream`).\r\n\r\n### Component(s)\r\n\r\nPython","comments":["A challenge with this approach is that the part delimiter must not occur in the data, as described here: https:\/\/www.w3.org\/Protocols\/rfc1341\/7_2_Multipart.html\r\n\r\nSo to support all possible sequences of bytes that might occur in Arrow IPC stream data, the server would need to be able to dynamically choose a viable delimiter sequence by checking that it is not contained in the IPC stream bytes to be sent in the response.\r\n\r\nOn the other hand, maybe it's sufficient for the server to simply use a part delimiter that is highly unlikely to occur in the IPC stream bytes."],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create Python examples of HTTP GET Arrow client\/server supporting range requests","body":"### Describe the enhancement requested\r\n\r\nContribute Python client and server examples to the [HTTP GET range request examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_range). This should demonstrate a client\/server pair that sends\/receives data of known size (`Content-Length` response header) in the Arrow IPC streaming format, and supports range requests (`Accept-Range: bytes` and `Content-Range: bytes` response headers; `Range: bytes` request header).\r\n\r\nThe main purpose of this example should be to demonstrate that range requests can be used to resume interrupted GET requests. See the discussion in the comments below for other possible uses of range requests.\r\n\r\n### Component(s)\r\n\r\nPython","comments":["For convenience, this server example could serve the data in the file contributed by @paleolimbot here: https:\/\/github.com\/apache\/arrow-experiments\/blob\/main\/data\/arrow-commits\/arrow-commits.arrows","Range requests are how HTTP clients resume interrupted downloads. \r\n\r\nTo test whether a server supports range requests, you can use curl like this:\r\n\r\n1. Start the download, limiting the speed so it doesn't finish too quickly.\r\n    ```sh\r\n    curl --limit-rate 10K -o file.arrows http:\/\/localhost:8008\r\n    ```\r\n2. Press `^C` to interrupt the download.\r\n3. Resume the download.\r\n    ```sh\r\n    curl --limit-rate 10K -o file.arrows -C - http:\/\/localhost:8008\r\n    ```\r\n","If we want to add examples for range requests, we may want to use not only the stream format but also the file format.\r\nWe can download only needed record batches (and footer) with the file format and range requests.\r\n","@kou Do you mean that a client could send a range request like `Range: batches=x-y` instead of `Range: bytes=x-y`? In that case: yes, the server would be more efficient retrieving the requested batches if the data on the server side was in the IPC file format, because the footer contains memory offsets and sizes for each record batch.\r\n\r\nBut I am -1 on recommending the use of range requests with units that are not `bytes`. Although this is allowed by HTTP\/1.1 (as described in [RFC 2616 Section 3.12](https:\/\/datatracker.ietf.org\/doc\/html\/rfc2616#section-3.12)) and also by HTTP\/2 (as described in [RFC 7540 Section 8](https:\/\/datatracker.ietf.org\/doc\/html\/rfc7540#section-8)), HTTP clients and servers in general do not support this well. At best it would require overriding classes\/behaviors of the HTTP server libraries that are rarely overridden. At worst it would be altogether incompatible with some HTTP clients and servers.\r\n\r\nI think it is better if we recommend that HTTP APIs should handle requests for specific ranges of batches using whatever higher-level application-specific methods they choose (such as URL query parameters) and restrict the use of range requests to `bytes` units only.","I think the idea with the file format might be more like: grab the last 8 bytes of the file (which I think contains a magic number and the number of footer bytes), then grab the footer (which contains offsets for various message locations), then grab specific batches, perhaps in parallel.\r\n\r\nI don't know if it's worth documenting, but I wonder if APIs would want to serve something like the footer metadata (which includes the offsets) via whatever API the client is calling to get the URI to the data in the first place, then the client could use range requests to read specific batches (or split up the fetch in parallel using a thread pool) in the same way.","Ok, that makes sense in general, but I don't think it helps with handling byte range requests. For byte range requests, we can just treat IPC stream files as opaque bytes.","Re the idea of an API serving metadata similar to the IPC file format footer which the client could use to make range requests:\n\nPerhaps, but I suspect that the real-world usefulness of that would be minimal. I can't envision a compelling case where a client user would want to retrieve a subset of data by looking up an ordinal byte range as opposed to passing more contextually meaningful query parameters.","I can't comment on the utility bit, you are almost certainly correct!\r\n\r\nI had envisioned the utility of range requests for the situation where you have a server set up to serve static files (because this is very easy to do) and you wanted to push the responsibility of issuing a partial (or partitioned) read on to the client. If that's not what you're trying to do here, ignore me!","Ah, ok, I see what you mean now. You have a \"dumb\" HTTP server that  is just serving files from a directory structure and doesn't know anything about Arrow. In that case, if the files are in the Arrow IPC file format and the server supports range requests, then the client can take advantage of the footers to download specific record batches, schemas, or other data blocks from the files.\n\nThis is perhaps a bit of an obscure case; we don't recommend using the IPC file format for archival storage so there are not many large collections of Arrow IPC files as far as I know. Parquet files are vastly more common. But it might be worth describing this in a section of the Arrow-over-HTTP conventions document.","Got it! Perhaps what should be documented for this issue is why one would bother supporting range requests (I'm sure there's a good reason, but it's not clear to me what it is if it's not partial reads aligned on batches).","Resuming interrupted downloads is the main case. And for that I think it boils down to \"just treat the IPC stream data as opaque bytes.\"","I think a cool thing to create for this example would be a Python client that can recover from a network disconnection and resume downloading (using a range request) after the network reconnects.","I should have explained more. Sorry. @paleolimbot explained all what I wanted to say. :-)\r\nThanks!\r\n\r\nI agree with resuming downloading is only enough for this case."],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create Python examples of indirect HTTP GET Arrow client and server","body":"### Describe the enhancement requested\r\n\r\nContribute Python client and server examples to the [indirect HTTP GET examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_indirect). This should demonstrate how to use a two-step sequence to retrieve Arrow data:\r\n1. The client sends a GET request to a server and receives a JSON response from the server containing one or more server URIs.\r\n2. The client sends GET requests to each of those URIs (in parallel) and receives a response from each server containing an Arrow IPC stream of record batches (exactly as in the [simple GET examples](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple)).\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Create simple example of Python HTTP POST Arrow client and server","body":"### Describe the enhancement requested\n\nContribute Python client and server examples to the [minimal HTTP POST examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/post_simple).\r\n\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"GH-40592: [C++][Parquet] Implement SizeStatistics","body":"### Rationale for this change\r\n\r\nParquet format 2.10.0 has introduced SizeStatistics. parquet-mr has also implemented this: https:\/\/github.com\/apache\/parquet-mr\/pull\/1177. Now it is time for parquet-cpp to pick the ball.\r\n\r\n### What changes are included in this PR?\r\n\r\nImplement reading and writing size statistics for parquet-cpp.\r\n\r\n### Are these changes tested?\r\n\r\nTBD\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes, now parquet users are able to read and write size statistics.\r\n\r\n\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40592","comments":[],"labels":["Component: Parquet","Component: C++","awaiting change review"]},{"title":"[R] crash when reading with duckdb","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nUsing arrow 15 in R through dplyr, I got a crash (R session is crashed) when reading a dataset with to_duckdb(). On my dataset, pretty large composed of some parquet files -- I can send a dropbox link if needed, there are 2 columns that provoque the crash. There are encoded as factors (dictionary) and I suspect that different dictionaries for the different parquet are the cause of the crash. \r\nso `open_dataset(mydts) |> select(field1) |> to_duckdb() |> collect()` crash everything\r\n`open_dataset(mydts) |> select(field1) |> mutate(field1 = as.character(field1) |> to_duckdb() |> collect()` works\r\nand \r\n`open_dataset(mydts) |> select(field1) |> collect()` works too.\r\n\r\nI tried to rewrite the dataset loading it first (without duckdb) and then using write_dataset with no succes.\r\n\r\nusing arrow 15.0.1 and duckdb 0.10\n\n### Component(s)\n\nR","comments":["Hi @xtimbeau, can you share the Dropbox link? I tried reproducing locally based off your idea about the differing dictionaries and wasn't able to.","Hi @amoeba. I tried also to reproduce it with mild success. Playing around with datasets and arrow 15, I think I found 2 other bugs:\r\n1. trying to write a parquet to a non existing path (on a dropbox); I know this a an unforgiveable error, but crashing R where an error message would be nice is a bit hard on us.\r\n2. producing a dataset with a hierarchy of folders works well, unless you name the subfolder VAR=value and that there is in the parquet data file a field VAR. In that last case, it crashes R. An error message would be nice (or even renaming one of the 2 VAR to something else)\r\n\r\nThen I sent you by PM a link to my faulty dataset (as doing it on simple cases would not reproduce the error). This should be related to different dictionaries or factor wrangling for large datasets ? Changing the type of CCONLC solves the crashes. Here is a reprex :\r\n\r\n```\r\nlibrary(tidyverse)\r\nlibrary(arrow)\r\n# link is in your mail box\r\ntest <- glue(\"{link}\/SOURCEFF=2022\")\r\nalt <- glue(\"{link}\/alt\")\r\ndir.create(alt)\r\n\r\nopen_dataset(test) |> select(CCONLC) |> collect() # works fine\r\nopen_dataset(test) |> to_duckdb() |> select(CCONLC) |> collect() # crashes ungracefully\r\n\r\ndeps <- arrow::open_dataset(test) |> distinct(CCODEP) |> collect() |>  pull()\r\nwalk(deps, ~{\r\n  unlink(str_c(alt, \"\/CCODEP=\", .x), recursive = TRUE)\r\n  dir.create(str_c(alt, \"\/CCODEP=\", .x))\r\n  read_parquet(str_c(test, \"\/CCODEP=\", .x, \"\/part-0.parquet\")) |> \r\n    mutate(CCONLC = as.character(CCONLC)) |> \r\n    write_parquet(str_c(alt, \"\/CCODEP=\", .x, \"\/part-0.parquet\"))})    \r\n\r\nopen_dataset(alt) |> select(CCONLC) |> collect() # still works\r\nopen_dataset(test) |> to_duckdb() |> select(CCONLC) |> collect() # works too !\r\n```\r\nyou can also revert alt$CCONLC to factor, parquet file by parquet file, and reproduce the crash when collecting it with duckdb.\r\n\r\nHopes this clarifies.","Thanks @xtimbeau, I'll have a look.\r\n\r\nRegarding the other two bugs, I'm not able to reproduce either of those on my system. If you're able to file reprexes for those, please feel new issues for each. We definitely don't want to be crashing in either of those scenarios.","I'm able to reproduce locally (thanks!) and it's crashing inside duckdb. Two threads look to be of interest (one duckdb, one arrow):\r\n\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)\r\n  * frame #0: 0x000000018a5cf1d4 libsystem_platform.dylib`_platform_memmove + 52\r\n    frame #1: 0x000000013cd966dc duckdb.so`duckdb::StringHeap::AddBlob(duckdb::string_t const&) [inlined] duckdb::StringHeap::AddBlob(this=<unavailable>, data=<unavailable>, len=<unavailable>) at string_heap.cpp:43:2 [opt]\r\n    frame #2: 0x000000013cd9665c duckdb.so`duckdb::StringHeap::AddBlob(this=<unavailable>, data=<unavailable>) at string_heap.cpp:49:9 [opt]\r\n    frame #3: 0x000000013ce0b1a4 duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] duckdb::StringValueCopy::Operation(meta_data=0x000000016ae307d0, input=string_t @ 0x000000016ae30710) at column_data_collection.cpp:346:62 [opt]\r\n    frame #4: 0x000000013ce0b188 duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] void duckdb::BaseValueCopy<duckdb::string_t>::Assign<duckdb::StringValueCopy>(meta_data=0x000000016ae307d0, target=\"\\U00000002\", source=<unavailable>, target_idx=<unavailable>, source_idx=<unavailable>) at column_data_collection.cpp:333:29 [opt]\r\n```\r\n\r\n```\r\n  thread #38\r\n    frame #0: 0x00000001266b6618 libparquet.1500.1.0.dylib`int arrow::util::RleDecoder::GetBatchWithDict<double>(double const*, int, double*, int) + 624\r\n    frame #1: 0x00000001266b6718 libparquet.1500.1.0.dylib`int arrow::util::RleDecoder::GetBatchWithDictSpaced<double>(double const*, int, double*, int, int, unsigned char const*, long long) + 144\r\n    frame #2: 0x00000001266b6238 libparquet.1500.1.0.dylib`virtual thunk to parquet::(anonymous namespace)::DictDecoderImpl<parquet::PhysicalType<(parquet::Type::type)5>>::DecodeSpaced(double*, int, int, unsigned char const*, long long) + 88\r\n```\r\n\r\nI'm going to set up a debug duckdb build and see if I can get more info.\r\n\r\n<details>\r\n<summary>Expand for complete lldb bt all output<\/summary>\r\n\r\n```\r\n(lldb) bt all\r\nwarning: duckdb.so was compiled with optimization - stepping may behave oddly; variables may not be available.\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)\r\n  * frame #0: 0x000000018a5cf1d4 libsystem_platform.dylib`_platform_memmove + 52\r\n    frame #1: 0x000000013cd966dc duckdb.so`duckdb::StringHeap::AddBlob(duckdb::string_t const&) [inlined] duckdb::StringHeap::AddBlob(this=<unavailable>, data=<unavailable>, len=<unavailable>) at string_heap.cpp:43:2 [opt]\r\n    frame #2: 0x000000013cd9665c duckdb.so`duckdb::StringHeap::AddBlob(this=<unavailable>, data=<unavailable>) at string_heap.cpp:49:9 [opt]\r\n    frame #3: 0x000000013ce0b1a4 duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] duckdb::StringValueCopy::Operation(meta_data=0x000000016ae307d0, input=string_t @ 0x000000016ae30710) at column_data_collection.cpp:346:62 [opt]\r\n    frame #4: 0x000000013ce0b188 duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] void duckdb::BaseValueCopy<duckdb::string_t>::Assign<duckdb::StringValueCopy>(meta_data=0x000000016ae307d0, target=\"\\U00000002\", source=<unavailable>, target_idx=<unavailable>, source_idx=<unavailable>) at column_data_collection.cpp:333:29 [opt]\r\n    frame #5: 0x000000013ce0b17c duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) at column_data_collection.cpp:406:5 [opt]\r\n    frame #6: 0x000000013ce0b060 duckdb.so`void duckdb::ColumnDataCopy<duckdb::string_t>(meta_data=0x000000016ae307d0, source_data=0x00000001300adf80, source=<unavailable>, offset=<unavailable>, copy_count=<unavailable>) at column_data_collection.cpp:439:3 [opt]\r\n    frame #7: 0x000000013ce0cabc duckdb.so`duckdb::ColumnDataCollection::Append(this=0x00000001340c6bd0, state=<unavailable>, input=<unavailable>) at column_data_collection.cpp:737:5 [opt]\r\n    frame #8: 0x000000013cd7cd58 duckdb.so`duckdb::BatchedDataCollection::Append(this=0x00000001061a7ff8, input=0x00000001061ab458, batch_index=139) at batched_data_collection.cpp:37:14 [opt]\r\n    frame #9: 0x000000013d32f6b0 duckdb.so`duckdb::PhysicalBatchCollector::Sink(this=<unavailable>, context=<unavailable>, chunk=<unavailable>, input=<unavailable>) const at physical_batch_collector.cpp:36:13 [opt]\r\n    frame #10: 0x000000013d7ae3cc duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) [inlined] duckdb::PipelineExecutor::Sink(this=0x00000001061ab380, chunk=0x00000001061ab458, input=0x000000016ae308e8) at pipeline_executor.cpp:547:24 [opt]\r\n    frame #11: 0x000000013d7ae3a4 duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(this=0x00000001061ab380, input=0x00000001061ab458, initial_idx=0) at pipeline_executor.cpp:287:23 [opt]\r\n    frame #12: 0x000000013d7ae894 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x00000001061ab380, max_chunks=<unavailable>) at pipeline_executor.cpp:0 [opt]\r\n    frame #13: 0x000000013d7b3738 duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x000000010619a0d0, mode=PROCESS_PARTIAL) at pipeline.cpp:39:34 [opt]\r\n    frame #14: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x000000010619a0d0, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #15: 0x000000013d7a94b4 duckdb.so`duckdb::Executor::ExecuteTask(this=0x0000000107eff880) at executor.cpp:479:24 [opt]\r\n    frame #16: 0x000000013d69778c duckdb.so`duckdb::ClientContext::ExecuteTaskInternal(this=0x000000013404a408, lock=<unavailable>, result=0x0000000107a3a9b0) at client_context.cpp:439:41 [opt]\r\n    frame #17: 0x000000013d6b2838 duckdb.so`duckdb::PendingQueryResult::ExecuteTask() [inlined] duckdb::PendingQueryResult::ExecuteTaskInternal(this=0x0000000107a3a9b0, lock=<unavailable>) at pending_query_result.cpp:53:18 [opt]\r\n    frame #18: 0x000000013d6b2824 duckdb.so`duckdb::PendingQueryResult::ExecuteTask(this=0x0000000107a3a9b0) at pending_query_result.cpp:48:9 [opt]\r\n    frame #19: 0x000000013cbd021c duckdb.so`rapi_execute(stmt=<unavailable>, arrow=false, integer64=false) at statement.cpp:338:37 [opt]\r\n    frame #20: 0x000000013cc042d8 duckdb.so`_duckdb_rapi_execute(stmt=<unavailable>, arrow=<unavailable>, integer64=0x00000001398c2000) at cpp11.cpp:356:27 [opt]\r\n    frame #21: 0x0000000106895274 libR.dylib`R_doDotCall(fun=<unavailable>, nargs=3, cargs=0x000000016ae335b0, call=0x0000000117dd1e38) at dotcode.c:874:17 [opt]\r\n    frame #22: 0x00000001068dcf6c libR.dylib`bcEval(body=0x0000000117dd1d90, rho=<unavailable>, useCache=<unavailable>) at eval.c:8002:21 [opt]\r\n    frame #23: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x0000000117dd1d90, rho=0x0000000139b356d0) at eval.c:1013:8 [opt]\r\n    frame #24: 0x00000001068e074c libR.dylib`R_execClosure(call=0x0000000117dd4d88, newrho=0x0000000139b356d0, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000117dd5840) at eval.c:0 [opt]\r\n    frame #25: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x0000000117dd4d88, op=0x0000000117dd5840, arglist=0x0000000139b35890, rho=0x0000000139b35900, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #26: 0x00000001068cac84 libR.dylib`bcEval(body=0x0000000117dd4c38, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #27: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x0000000117dd4c38, rho=0x0000000139b35900) at eval.c:1013:8 [opt]\r\n    frame #28: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013c1409c0, newrho=0x0000000139b35900, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000117dd48b8) at eval.c:0 [opt]\r\n    frame #29: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x000000013c1409c0, op=0x0000000117dd48b8, arglist=0x0000000139b35970, rho=0x0000000139b1cba8, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #30: 0x00000001068cac84 libR.dylib`bcEval(body=0x000000013c139440, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #31: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x000000013c139440, rho=0x0000000139b1cba8) at eval.c:1013:8 [opt]\r\n    frame #32: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013ad08a38, newrho=0x0000000139b1cba8, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x000000013c139830) at eval.c:0 [opt]\r\n    frame #33: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x000000013ad08a38, op=0x000000013c139830, arglist=0x0000000139b1cd68, rho=0x000000013929d890, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #34: 0x00000001068c3d9c libR.dylib`Rf_eval(e=0x000000013ad08a38, rho=0x000000013929d890) at eval.c:1140:12 [opt]\r\n    frame #35: 0x00000001068e3428 libR.dylib`do_set(call=<unavailable>, op=0x000000011080a008, args=0x000000013ad08aa8, rho=0x000000013929d890) at eval.c:3250:8 [opt]\r\n    frame #36: 0x00000001068c3cc8 libR.dylib`Rf_eval(e=0x000000013ad08ae0, rho=0x000000013929d890) at eval.c:1092:12 [opt]\r\n    frame #37: 0x00000001068e2ecc libR.dylib`do_begin(call=0x000000013ad09170, op=0x0000000110815520, args=0x000000013ad08b18, rho=0x000000013929d890) at eval.c:2798:10 [opt]\r\n    frame #38: 0x00000001068c3cc8 libR.dylib`Rf_eval(e=0x000000013ad09170, rho=0x000000013929d890) at eval.c:1092:12 [opt]\r\n    frame #39: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013ad093a0, newrho=0x000000013929d890, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x000000013ad092c0) at eval.c:0 [opt]\r\n    frame #40: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x000000013ad093a0, op=0x000000013ad092c0, arglist=0x000000013929da50, rho=0x000000013929dd28, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #41: 0x00000001068cac84 libR.dylib`bcEval(body=0x000000013ad0a248, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #42: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x000000013ad0a248, rho=0x000000013929dd28) at eval.c:1013:8 [opt]\r\n    frame #43: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013929b120, newrho=0x000000013929dd28, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x000000013709a1c0) at eval.c:0 [opt]\r\n    frame #44: 0x00000001068e155c libR.dylib`R_execMethod(op=0x000000013709a1c0, rho=0x000000013929e070) at eval.c:2363:11 [opt]\r\n    frame #45: 0x0000000105f07ce8 methods.so`R_dispatchGeneric + 2408\r\n    frame #46: 0x00000001069307f8 libR.dylib`do_standardGeneric(call=<unavailable>, op=<unavailable>, args=<unavailable>, env=0x000000013929e070) at objects.c:1271:13 [opt]\r\n    frame #47: 0x00000001068cafe0 libR.dylib`bcEval(body=0x00000001254a42e8, rho=<unavailable>, useCache=<unavailable>) at eval.c:7403:12 [opt]\r\n    frame #48: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001254a42e8, rho=0x000000013929e070) at eval.c:1013:8 [opt]\r\n    frame #49: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013929b120, newrho=0x000000013929e070, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x000000012549d438) at eval.c:0 [opt]\r\n    frame #50: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x000000013929b120, op=0x000000012549d438, arglist=0x000000013929a2b0, rho=0x000000013929a320, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #51: 0x00000001068cac84 libR.dylib`bcEval(body=0x000000013929b270, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #52: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x000000013929b270, rho=0x000000013929a320) at eval.c:1013:8 [opt]\r\n    frame #53: 0x00000001068e074c libR.dylib`R_execClosure(call=0x000000013929a470, newrho=0x000000013929a320, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x000000013929b740) at eval.c:0 [opt]\r\n    frame #54: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x000000013929a470, op=0x000000013929b740, arglist=0x00000001392964a8, rho=0x0000000139296208, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #55: 0x000000010692f3f4 libR.dylib`applyMethod(call=<unavailable>, op=<unavailable>, args=<unavailable>, rho=<unavailable>, newvars=<unavailable>) at objects.c:118:8 [opt] [artificial]\r\n    frame #56: 0x000000010692dd40 libR.dylib`dispatchMethod(op=0x0000000116ee4ea0, sxp=0x000000013929b740, dotClass=0x0000000137161d08, cptr=0x000000016ae39180, method=0x000000012504ac00, generic=\"db_collect\", rho=0x0000000139296208, callrho=0x00000001390f3f58, defrho=0x0000000110e92350) at objects.c:399:16 [opt]\r\n    frame #57: 0x000000010692d908 libR.dylib`Rf_usemethod(generic=\"db_collect\", obj=<unavailable>, call=<unavailable>, args=<unavailable>, rho=0x0000000139296208, callrho=0x00000001390f3f58, defrho=0x0000000110e92350, ans=0x000000016ae38860) at objects.c:435:10 [opt]\r\n    frame #58: 0x000000010692e014 libR.dylib`do_usemethod(call=0x0000000116ee4848, op=<unavailable>, args=<unavailable>, env=0x0000000139296208) at objects.c:505:9 [opt]\r\n    frame #59: 0x00000001068cb67c libR.dylib`bcEval(body=0x0000000116ee4960, rho=<unavailable>, useCache=<unavailable>) at eval.c:7466:15 [opt]\r\n    frame #60: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x0000000116ee4960, rho=0x0000000139296208) at eval.c:1013:8 [opt]\r\n    frame #61: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001390f1708, newrho=0x0000000139296208, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000116ee4ea0) at eval.c:0 [opt]\r\n    frame #62: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001390f1708, op=0x0000000116ee4ea0, arglist=0x00000001392964a8, rho=0x00000001390f3f58, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #63: 0x00000001068cac84 libR.dylib`bcEval(body=0x00000001390f0a90, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #64: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001390f0a90, rho=0x00000001390f3f58) at eval.c:1013:8 [opt]\r\n    frame #65: 0x00000001068de6e4 libR.dylib`forcePromise(e=0x0000000139297698) at eval.c:833:8 [opt]\r\n    frame #66: 0x00000001068eb530 libR.dylib`getvar [inlined] FORCE_PROMISE(value=0x0000000139297698, symbol=0x000000013801db78, rho=0x00000001392974d8, keepmiss=FALSE) at eval.c:5467:15 [opt]\r\n    frame #67: 0x00000001068eb528 libR.dylib`getvar(symbol=0x000000013801db78, rho=0x00000001392974d8, dd=<unavailable>, keepmiss=FALSE, vcache=<unavailable>, sidx=<unavailable>) at eval.c:5508:14 [opt]\r\n    frame #68: 0x00000001068c7fb8 libR.dylib`bcEval(body=0x00000001382b4bc0, rho=<unavailable>, useCache=<unavailable>) at eval.c:7198:20 [opt]\r\n    frame #69: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b4bc0, rho=0x00000001392974d8) at eval.c:1013:8 [opt]\r\n    frame #70: 0x00000001068de6e4 libR.dylib`forcePromise(e=0x00000001392970b0) at eval.c:833:8 [opt]\r\n    frame #71: 0x00000001068eb530 libR.dylib`getvar [inlined] FORCE_PROMISE(value=0x00000001392970b0, symbol=0x000000013801db78, rho=0x0000000139296e10, keepmiss=FALSE) at eval.c:5467:15 [opt]\r\n    frame #72: 0x00000001068eb528 libR.dylib`getvar(symbol=0x000000013801db78, rho=0x0000000139296e10, dd=<unavailable>, keepmiss=FALSE, vcache=<unavailable>, sidx=<unavailable>) at eval.c:5508:14 [opt]\r\n    frame #73: 0x00000001068c7fb8 libR.dylib`bcEval(body=0x00000001382b2a20, rho=<unavailable>, useCache=<unavailable>) at eval.c:7198:20 [opt]\r\n    frame #74: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b2a20, rho=0x0000000139296e10) at eval.c:1013:8 [opt]\r\n    frame #75: 0x00000001068de6e4 libR.dylib`forcePromise(e=0x0000000139296d68) at eval.c:833:8 [opt]\r\n    frame #76: 0x00000001068eb530 libR.dylib`getvar [inlined] FORCE_PROMISE(value=0x0000000139296d68, symbol=0x000000013801db78, rho=0x0000000139296ac8, keepmiss=FALSE) at eval.c:5467:15 [opt]\r\n    frame #77: 0x00000001068eb528 libR.dylib`getvar(symbol=0x000000013801db78, rho=0x0000000139296ac8, dd=<unavailable>, keepmiss=FALSE, vcache=<unavailable>, sidx=<unavailable>) at eval.c:5508:14 [opt]\r\n    frame #78: 0x00000001068c7fb8 libR.dylib`bcEval(body=0x00000001382b4f08, rho=<unavailable>, useCache=<unavailable>) at eval.c:7198:20 [opt]\r\n    frame #79: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b4f08, rho=0x0000000139296ac8) at eval.c:1013:8 [opt]\r\n    frame #80: 0x00000001068de6e4 libR.dylib`forcePromise(e=0x0000000139296a20) at eval.c:833:8 [opt]\r\n    frame #81: 0x00000001068eb530 libR.dylib`getvar [inlined] FORCE_PROMISE(value=0x0000000139296a20, symbol=0x000000013801db78, rho=0x0000000139296780, keepmiss=FALSE) at eval.c:5467:15 [opt]\r\n    frame #82: 0x00000001068eb528 libR.dylib`getvar(symbol=0x000000013801db78, rho=0x0000000139296780, dd=<unavailable>, keepmiss=FALSE, vcache=<unavailable>, sidx=<unavailable>) at eval.c:5508:14 [opt]\r\n    frame #83: 0x00000001068c7fb8 libR.dylib`bcEval(body=0x00000001382b5288, rho=<unavailable>, useCache=<unavailable>) at eval.c:7198:20 [opt]\r\n    frame #84: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b5288, rho=0x0000000139296780) at eval.c:1013:8 [opt]\r\n    frame #85: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001382b6558, newrho=0x0000000139296780, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000139296a90) at eval.c:0 [opt]\r\n    frame #86: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001382b6558, op=0x0000000139296a90, arglist=0x00000001392969e8, rho=0x0000000139296ac8, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #87: 0x00000001068cac84 libR.dylib`bcEval(body=0x00000001382b2748, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #88: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b2748, rho=0x0000000139296ac8) at eval.c:1013:8 [opt]\r\n    frame #89: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001382b2f98, newrho=0x0000000139296ac8, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000139297430) at eval.c:0 [opt]\r\n    frame #90: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001382b2f98, op=0x0000000139297430, arglist=0x0000000139296d30, rho=0x0000000139296e10, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #91: 0x00000001068cac84 libR.dylib`bcEval(body=0x00000001382b3a18, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #92: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382b3a18, rho=0x0000000139296e10) at eval.c:1013:8 [opt]\r\n    frame #93: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001382b3c10, newrho=0x0000000139296e10, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x00000001392974a0) at eval.c:0 [opt]\r\n    frame #94: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001382b3c10, op=0x00000001392974a0, arglist=0x0000000139297078, rho=0x00000001392974d8, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #95: 0x00000001068cac84 libR.dylib`bcEval(body=0x00000001382a6d80, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #96: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001382a6d80, rho=0x00000001392974d8) at eval.c:1013:8 [opt]\r\n    frame #97: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001390f1820, newrho=0x00000001392974d8, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x00000001382a6ed0) at eval.c:0 [opt]\r\n    frame #98: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001390f1820, op=0x00000001382a6ed0, arglist=0x0000000139297660, rho=0x00000001390f3f58, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #99: 0x00000001068cac84 libR.dylib`bcEval(body=0x00000001390f20e0, rho=<unavailable>, useCache=<unavailable>) at eval.c:7414:12 [opt]\r\n    frame #100: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x00000001390f20e0, rho=0x00000001390f3f58) at eval.c:1013:8 [opt]\r\n    frame #101: 0x00000001068e074c libR.dylib`R_execClosure(call=0x00000001390f40a8, newrho=0x00000001390f3f58, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x00000001390ec630) at eval.c:0 [opt]\r\n    frame #102: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x00000001390f40a8, op=0x00000001390ec630, arglist=0x0000000117d1df18, rho=0x0000000117d1dfc0, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #103: 0x000000010692f3f4 libR.dylib`applyMethod(call=<unavailable>, op=<unavailable>, args=<unavailable>, rho=<unavailable>, newvars=<unavailable>) at objects.c:118:8 [opt] [artificial]\r\n    frame #104: 0x000000010692dd40 libR.dylib`dispatchMethod(op=0x0000000115de4ba0, sxp=0x00000001390ec630, dotClass=0x0000000133222038, cptr=0x000000016ae3fec0, method=0x0000000125046618, generic=\"collect\", rho=0x0000000117d1dfc0, callrho=0x0000000110844d88, defrho=0x00000001158ddcf0) at objects.c:399:16 [opt]\r\n    frame #105: 0x000000010692d908 libR.dylib`Rf_usemethod(generic=\"collect\", obj=<unavailable>, call=<unavailable>, args=<unavailable>, rho=0x0000000117d1dfc0, callrho=0x0000000110844d88, defrho=0x00000001158ddcf0, ans=0x000000016ae3f5a0) at objects.c:435:10 [opt]\r\n    frame #106: 0x000000010692e014 libR.dylib`do_usemethod(call=0x0000000115de4740, op=<unavailable>, args=<unavailable>, env=0x0000000117d1dfc0) at objects.c:505:9 [opt]\r\n    frame #107: 0x00000001068cb67c libR.dylib`bcEval(body=0x0000000115de47e8, rho=<unavailable>, useCache=<unavailable>) at eval.c:7466:15 [opt]\r\n    frame #108: 0x00000001068c3ac8 libR.dylib`Rf_eval(e=0x0000000115de47e8, rho=0x0000000117d1dfc0) at eval.c:1013:8 [opt]\r\n    frame #109: 0x00000001068e074c libR.dylib`R_execClosure(call=0x0000000117d1dd90, newrho=0x0000000117d1dfc0, sysparent=<unavailable>, rho=<unavailable>, arglist=<unavailable>, op=0x0000000115de4ba0) at eval.c:0 [opt]\r\n    frame #110: 0x00000001068defcc libR.dylib`Rf_applyClosure(call=0x0000000117d1dd90, op=0x0000000115de4ba0, arglist=0x0000000117d1df18, rho=0x0000000110844d88, suppliedvars=<unavailable>) at eval.c:2113:16 [opt]\r\n    frame #111: 0x00000001068c3d9c libR.dylib`Rf_eval(e=0x0000000117d1dd90, rho=0x0000000110844d88) at eval.c:1140:12 [opt]\r\n    frame #112: 0x0000000106917e34 libR.dylib`Rf_ReplIteration(rho=0x0000000110844d88, savestack=<unavailable>, browselevel=<unavailable>, state=0x000000016ae40420) at main.c:262:2 [opt]\r\n    frame #113: 0x00000001069193a8 libR.dylib`R_ReplConsole(rho=0x0000000110844d88, savestack=0, browselevel=0) at main.c:314:11 [opt]\r\n    frame #114: 0x00000001069192e4 libR.dylib`run_Rmainloop at main.c:1200:5 [opt]\r\n    frame #115: 0x0000000106919450 libR.dylib`Rf_mainloop at main.c:1207:5 [opt]\r\n    frame #116: 0x0000000105a2ea08 rsession-arm64`___lldb_unnamed_symbol29807 + 424\r\n    frame #117: 0x0000000105a05084 rsession-arm64`___lldb_unnamed_symbol29158 + 4432\r\n    frame #118: 0x0000000105108f80 rsession-arm64`___lldb_unnamed_symbol5105 + 19772\r\n    frame #119: 0x000000018a2160e0 dyld`start + 2360\r\n  thread #2\r\n    frame #0: 0x000000018a569d00 libsystem_kernel.dylib`__sigwait + 8\r\n    frame #1: 0x000000018a59f764 libsystem_pthread.dylib`sigwait + 40\r\n    frame #2: 0x000000010596adb4 rsession-arm64`___lldb_unnamed_symbol27296 + 48\r\n    frame #3: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #3\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x0000000105060740 rsession-arm64`___lldb_unnamed_symbol3002 + 328\r\n    frame #3: 0x0000000105060440 rsession-arm64`___lldb_unnamed_symbol3001 + 256\r\n    frame #4: 0x000000010576a50c rsession-arm64`rstudio_boost::asio::io_context::run() + 36\r\n    frame #5: 0x000000010512162c rsession-arm64`___lldb_unnamed_symbol5413 + 40\r\n    frame #6: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #7: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #4\r\n    frame #0: 0x000000018a569d00 libsystem_kernel.dylib`__sigwait + 8\r\n    frame #1: 0x000000018a59f764 libsystem_pthread.dylib`sigwait + 40\r\n    frame #2: 0x000000010596adb4 rsession-arm64`___lldb_unnamed_symbol27296 + 48\r\n    frame #3: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #5\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x0000000104fdb324 rsession-arm64`___lldb_unnamed_symbol1664 + 84\r\n    frame #3: 0x00000001059377d0 rsession-arm64`___lldb_unnamed_symbol26616 + 340\r\n    frame #4: 0x0000000105937b50 rsession-arm64`___lldb_unnamed_symbol26617 + 172\r\n    frame #5: 0x0000000105997338 rsession-arm64`___lldb_unnamed_symbol27845 + 100\r\n    frame #6: 0x0000000105933578 rsession-arm64`___lldb_unnamed_symbol26591 + 124\r\n    frame #7: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #8: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #6\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x00000001050b9cb4 rsession-arm64`___lldb_unnamed_symbol3993 + 84\r\n    frame #3: 0x00000001051217d0 rsession-arm64`___lldb_unnamed_symbol5414 + 372\r\n    frame #4: 0x000000010510b868 rsession-arm64`___lldb_unnamed_symbol5111 + 36\r\n    frame #5: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #6: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #7\r\n    frame #0: 0x000000018a569358 libsystem_kernel.dylib`__select + 8\r\n    frame #1: 0x000000010505f7a0 rsession-arm64`___lldb_unnamed_symbol2986 + 616\r\n    frame #2: 0x000000010506077c rsession-arm64`___lldb_unnamed_symbol3002 + 388\r\n    frame #3: 0x0000000105060440 rsession-arm64`___lldb_unnamed_symbol3001 + 256\r\n    frame #4: 0x000000010576a50c rsession-arm64`rstudio_boost::asio::io_context::run() + 36\r\n    frame #5: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #6: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #8\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x0000000104fdb324 rsession-arm64`___lldb_unnamed_symbol1664 + 84\r\n    frame #3: 0x0000000104fd820c rsession-arm64`___lldb_unnamed_symbol1591 + 288\r\n    frame #4: 0x0000000104fddb1c rsession-arm64`___lldb_unnamed_symbol1692 + 1276\r\n    frame #5: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #6: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #9\r\n    frame #0: 0x000000018a55fea4 libsystem_kernel.dylib`__workq_kernreturn + 8\r\n  thread #10\r\n    frame #0: 0x000000018a569358 libsystem_kernel.dylib`__select + 8\r\n    frame #1: 0x0000000105964ee8 rsession-arm64`___lldb_unnamed_symbol27219 + 408\r\n    frame #2: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #3: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #11\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x00000001050b9cb4 rsession-arm64`___lldb_unnamed_symbol3993 + 84\r\n    frame #3: 0x00000001050b65f8 rsession-arm64`___lldb_unnamed_symbol3962 + 220\r\n    frame #4: 0x00000001052255f4 rsession-arm64`___lldb_unnamed_symbol8868 + 752\r\n    frame #5: 0x0000000105ac0c20 rsession-arm64`___lldb_unnamed_symbol31959 + 176\r\n    frame #6: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #12\r\n    frame #0: 0x000000018a5617e8 libsystem_kernel.dylib`__semwait_signal + 8\r\n    frame #1: 0x000000018a442274 libsystem_c.dylib`nanosleep + 220\r\n    frame #2: 0x000000010655ecb0 cli.so`clic_thread_func(arg=<unavailable>) at thread.c:37:5 [opt]\r\n    frame #3: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #13\r\n    frame #0: 0x000000013d647ef8 duckdb.so`duckdb::SetSelectionVector(duckdb::SelectionVector&, unsigned char*, duckdb::LogicalType&, unsigned long long, duckdb::ValidityMask*, unsigned long long) at arrow_conversion.cpp:729:23 [opt]\r\n    frame #1: 0x000000013d647ee4 duckdb.so`duckdb::SetSelectionVector(sel=0x000000016b4ceb40, indices_p=\"\", logical_type=0x000000016b4ceb58, size=2048, mask=0x000000016b4ceb20, last_element_pos=2) at arrow_conversion.cpp:763:4 [opt]\r\n    frame #2: 0x000000013d63019c duckdb.so`duckdb::ColumnArrowToDuckDBDictionary(vector=0x0000000138f70b00, array=0x00000001331ce680, array_state=0x0000000127f701f0, size=2048, arrow_type=0x0000000107ef6c70, nested_offset=<unavailable>, parent_mask=<unavailable>, parent_offset=<unavailable>) at arrow_conversion.cpp:846:3 [opt]\r\n    frame #3: 0x000000013d62ee70 duckdb.so`duckdb::ArrowTableFunction::ArrowToDuckDB(scan_state=0x0000000106645d50, arrow_convert_data=size=38, output=0x0000000127fa6c78, start=<unavailable>, arrow_scan_is_projected=true) at arrow_conversion.cpp:889:4 [opt]\r\n    frame #4: 0x000000013d62e98c duckdb.so`duckdb::ArrowTableFunction::ArrowScanFunction(context=<unavailable>, data_p=<unavailable>, output=0x0000000127fa6c78) at arrow.cpp:369:3 [opt]\r\n    frame #5: 0x000000013d38e7bc duckdb.so`duckdb::PhysicalTableScan::GetData(this=<unavailable>, context=<unavailable>, chunk=0x0000000127fa6c78, input=<unavailable>) const at physical_table_scan.cpp:74:2 [opt]\r\n    frame #6: 0x000000013d7aead0 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) [inlined] duckdb::PipelineExecutor::GetData(this=0x0000000127fa6ba0, chunk=0x0000000127fa6c78, input=0x000000016b4ced00) at pipeline_executor.cpp:528:26 [opt]\r\n    frame #7: 0x000000013d7aeaa8 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(this=0x0000000127fa6ba0, result=0x0000000127fa6c78) at pipeline_executor.cpp:554:13 [opt]\r\n    frame #8: 0x000000013d7ae840 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000127fa6ba0, max_chunks=<unavailable>) at pipeline_executor.cpp:197:21 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x00000001061a3ff0, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x00000001061a3ff0, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137ead920) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x00000001340437f8, __args=0x0000000134043800, __args=0x0000000134043808) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x00000001340437f0) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #14\r\n    frame #0: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] duckdb::TemplatedValidityMask<unsigned long long>::SetAllValid(this=<unavailable>, count=<unavailable>) at validity_mask.hpp:279:35 [opt]\r\n    frame #1: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) at column_data_collection.cpp:401:20 [opt]\r\n    frame #2: 0x000000013ce0a8a4 duckdb.so`void duckdb::ColumnDataCopy<double>(meta_data=0x000000016b55abc0, source_data=0x00000001252b3d00, source=<unavailable>, offset=<unavailable>, copy_count=<unavailable>) at column_data_collection.cpp:428:2 [opt]\r\n    frame #3: 0x000000013ce0cabc duckdb.so`duckdb::ColumnDataCollection::Append(this=0x0000000106008bc0, state=<unavailable>, input=<unavailable>) at column_data_collection.cpp:737:5 [opt]\r\n    frame #4: 0x000000013cd7cd58 duckdb.so`duckdb::BatchedDataCollection::Append(this=0x0000000106637618, input=0x0000000106646348, batch_index=130) at batched_data_collection.cpp:37:14 [opt]\r\n    frame #5: 0x000000013d32f6b0 duckdb.so`duckdb::PhysicalBatchCollector::Sink(this=<unavailable>, context=<unavailable>, chunk=<unavailable>, input=<unavailable>) const at physical_batch_collector.cpp:36:13 [opt]\r\n    frame #6: 0x000000013d7ae3cc duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) [inlined] duckdb::PipelineExecutor::Sink(this=0x0000000106646270, chunk=0x0000000106646348, input=0x000000016b55acd8) at pipeline_executor.cpp:547:24 [opt]\r\n    frame #7: 0x000000013d7ae3a4 duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(this=0x0000000106646270, input=0x0000000106646348, initial_idx=0) at pipeline_executor.cpp:287:23 [opt]\r\n    frame #8: 0x000000013d7ae894 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000106646270, max_chunks=<unavailable>) at pipeline_executor.cpp:0 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x000000010619a150, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x000000010619a150, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137e350e0) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134047318, __args=0x0000000134047320, __args=0x0000000134047328) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134047310) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #15\r\n    frame #0: 0x000000018a5cf35c libsystem_platform.dylib`_platform_memmove + 444\r\n    frame #1: 0x000000013d631234 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) [inlined] duckdb::string_t::string_t(this=<unavailable>, data=<unavailable>, len=12) at string_type.hpp:50:4 [opt]\r\n    frame #2: 0x000000013d631214 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) [inlined] duckdb::string_t::string_t(this=<unavailable>, data=<unavailable>, len=12) at string_type.hpp:39:43 [opt]\r\n    frame #3: 0x000000013d631214 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) at arrow_conversion.cpp:265:22 [opt]\r\n    frame #4: 0x000000013d631190 duckdb.so`duckdb::ColumnArrowToDuckDB(vector=0x00000001307c4260, array=<unavailable>, array_state=<unavailable>, size=2048, arrow_type=<unavailable>, nested_offset=<unavailable>, parent_mask=0x0000000000000000, parent_offset=<unavailable>) at arrow_conversion.cpp:430:4 [opt]\r\n    frame #5: 0x000000013d62eaac duckdb.so`duckdb::ArrowTableFunction::ArrowToDuckDB(scan_state=0x000000010601b2f0, arrow_convert_data=size=38, output=0x00000001071a06f8, start=<unavailable>, arrow_scan_is_projected=true) at arrow_conversion.cpp:892:4 [opt]\r\n    frame #6: 0x000000013d62e98c duckdb.so`duckdb::ArrowTableFunction::ArrowScanFunction(context=<unavailable>, data_p=<unavailable>, output=0x00000001071a06f8) at arrow.cpp:369:3 [opt]\r\n    frame #7: 0x000000013d38e7bc duckdb.so`duckdb::PhysicalTableScan::GetData(this=<unavailable>, context=<unavailable>, chunk=0x00000001071a06f8, input=<unavailable>) const at physical_table_scan.cpp:74:2 [opt]\r\n    frame #8: 0x000000013d7aead0 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) [inlined] duckdb::PipelineExecutor::GetData(this=0x00000001071a0620, chunk=0x00000001071a06f8, input=0x000000016b5e6d00) at pipeline_executor.cpp:528:26 [opt]\r\n    frame #9: 0x000000013d7aeaa8 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(this=0x00000001071a0620, result=0x00000001071a06f8) at pipeline_executor.cpp:554:13 [opt]\r\n    frame #10: 0x000000013d7ae840 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x00000001071a0620, max_chunks=<unavailable>) at pipeline_executor.cpp:197:21 [opt]\r\n    frame #11: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #12: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x000000010619a110, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #13: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x000000010619a110, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #14: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137ed0ef0) at task_scheduler.cpp:139:32 [opt]\r\n    frame #15: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134048718, __args=0x0000000134048720, __args=0x0000000134048728) at type_traits:3747:1 [opt]\r\n    frame #16: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #17: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134048710) at thread:291:5 [opt]\r\n    frame #18: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #16\r\n    frame #0: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] duckdb::TemplatedValidityMask<unsigned long long>::SetAllValid(this=<unavailable>, count=<unavailable>) at validity_mask.hpp:279:35 [opt]\r\n    frame #1: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) at column_data_collection.cpp:401:20 [opt]\r\n    frame #2: 0x000000013ce0a8a4 duckdb.so`void duckdb::ColumnDataCopy<double>(meta_data=0x000000016b672bc0, source_data=0x000000013059aac0, source=<unavailable>, offset=<unavailable>, copy_count=<unavailable>) at column_data_collection.cpp:428:2 [opt]\r\n    frame #3: 0x000000013ce0cabc duckdb.so`duckdb::ColumnDataCollection::Append(this=0x0000000132d41930, state=<unavailable>, input=<unavailable>) at column_data_collection.cpp:737:5 [opt]\r\n    frame #4: 0x000000013cd7cd58 duckdb.so`duckdb::BatchedDataCollection::Append(this=0x00000001071a0d88, input=0x00000001071a0cd8, batch_index=133) at batched_data_collection.cpp:37:14 [opt]\r\n    frame #5: 0x000000013d32f6b0 duckdb.so`duckdb::PhysicalBatchCollector::Sink(this=<unavailable>, context=<unavailable>, chunk=<unavailable>, input=<unavailable>) const at physical_batch_collector.cpp:36:13 [opt]\r\n    frame #6: 0x000000013d7ae3cc duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) [inlined] duckdb::PipelineExecutor::Sink(this=0x00000001071a0c00, chunk=0x00000001071a0cd8, input=0x000000016b672cd8) at pipeline_executor.cpp:547:24 [opt]\r\n    frame #7: 0x000000013d7ae3a4 duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(this=0x00000001071a0c00, input=0x00000001071a0cd8, initial_idx=0) at pipeline_executor.cpp:287:23 [opt]\r\n    frame #8: 0x000000013d7ae894 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x00000001071a0c00, max_chunks=<unavailable>) at pipeline_executor.cpp:0 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x0000000107a4d6a0, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x0000000107a4d6a0, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137eff7f0) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x00000001340487a8, __args=0x00000001340487b0, __args=0x00000001340487b8) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x00000001340487a0) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #17\r\n    frame #0: 0x000000013ce0a9ac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] void duckdb::BaseValueCopy<double>::Assign<duckdb::StandardValueCopy<double>>(meta_data=0x000000016b6febc0, target=\"\", source=<unavailable>, target_idx=1912, source_idx=1912) at column_data_collection.cpp:333:54 [opt]\r\n    frame #1: 0x000000013ce0a9ac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) at column_data_collection.cpp:406:5 [opt]\r\n    frame #2: 0x000000013ce0a8a4 duckdb.so`void duckdb::ColumnDataCopy<double>(meta_data=0x000000016b6febc0, source_data=0x0000000117190f00, source=<unavailable>, offset=<unavailable>, copy_count=<unavailable>) at column_data_collection.cpp:428:2 [opt]\r\n    frame #3: 0x000000013ce0cabc duckdb.so`duckdb::ColumnDataCollection::Append(this=0x0000000132a4bd30, state=<unavailable>, input=<unavailable>) at column_data_collection.cpp:737:5 [opt]\r\n    frame #4: 0x000000013cd7cd58 duckdb.so`duckdb::BatchedDataCollection::Append(this=0x0000000127eb0808, input=0x0000000127eaf5f8, batch_index=138) at batched_data_collection.cpp:37:14 [opt]\r\n    frame #5: 0x000000013d32f6b0 duckdb.so`duckdb::PhysicalBatchCollector::Sink(this=<unavailable>, context=<unavailable>, chunk=<unavailable>, input=<unavailable>) const at physical_batch_collector.cpp:36:13 [opt]\r\n    frame #6: 0x000000013d7ae3cc duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) [inlined] duckdb::PipelineExecutor::Sink(this=0x0000000127eaf520, chunk=0x0000000127eaf5f8, input=0x000000016b6fecd8) at pipeline_executor.cpp:547:24 [opt]\r\n    frame #7: 0x000000013d7ae3a4 duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(this=0x0000000127eaf520, input=0x0000000127eaf5f8, initial_idx=0) at pipeline_executor.cpp:287:23 [opt]\r\n    frame #8: 0x000000013d7ae894 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000127eaf520, max_chunks=<unavailable>) at pipeline_executor.cpp:0 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x0000000107a4d640, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x0000000107a4d640, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137ee9240) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134047948, __args=0x0000000134047950, __args=0x0000000134047958) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134047940) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #18\r\n    frame #0: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) [inlined] duckdb::TemplatedValidityMask<unsigned long long>::SetAllValid(this=<unavailable>, count=<unavailable>) at validity_mask.hpp:279:35 [opt]\r\n    frame #1: 0x000000013ce0aaac duckdb.so`void duckdb::ColumnDataCopy<double>(duckdb::ColumnDataMetaData&, duckdb::UnifiedVectorFormat const&, duckdb::Vector&, unsigned long long, unsigned long long) at column_data_collection.cpp:401:20 [opt]\r\n    frame #2: 0x000000013ce0a8a4 duckdb.so`void duckdb::ColumnDataCopy<double>(meta_data=0x000000016b78abc0, source_data=0x0000000110086680, source=<unavailable>, offset=<unavailable>, copy_count=<unavailable>) at column_data_collection.cpp:428:2 [opt]\r\n    frame #3: 0x000000013ce0cabc duckdb.so`duckdb::ColumnDataCollection::Append(this=0x0000000107e80d50, state=<unavailable>, input=<unavailable>) at column_data_collection.cpp:737:5 [opt]\r\n    frame #4: 0x000000013cd7cd58 duckdb.so`duckdb::BatchedDataCollection::Append(this=0x0000000127eaf458, input=0x0000000127eaf3a8, batch_index=131) at batched_data_collection.cpp:37:14 [opt]\r\n    frame #5: 0x000000013d32f6b0 duckdb.so`duckdb::PhysicalBatchCollector::Sink(this=<unavailable>, context=<unavailable>, chunk=<unavailable>, input=<unavailable>) const at physical_batch_collector.cpp:36:13 [opt]\r\n    frame #6: 0x000000013d7ae3cc duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(duckdb::DataChunk&, unsigned long long) [inlined] duckdb::PipelineExecutor::Sink(this=0x0000000127eaf2d0, chunk=0x0000000127eaf3a8, input=0x000000016b78acd8) at pipeline_executor.cpp:547:24 [opt]\r\n    frame #7: 0x000000013d7ae3a4 duckdb.so`duckdb::PipelineExecutor::ExecutePushInternal(this=0x0000000127eaf2d0, input=0x0000000127eaf3a8, initial_idx=0) at pipeline_executor.cpp:287:23 [opt]\r\n    frame #8: 0x000000013d7ae894 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000127eaf2d0, max_chunks=<unavailable>) at pipeline_executor.cpp:0 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x000000010619a190, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x000000010619a190, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137ea1130) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134048758, __args=0x0000000134048760, __args=0x0000000134048768) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134048750) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #19\r\n    frame #0: 0x000000013d6482fc duckdb.so`duckdb::SetSelectionVector(duckdb::SelectionVector&, unsigned char*, duckdb::LogicalType&, unsigned long long, duckdb::ValidityMask*, unsigned long long) at arrow_conversion.cpp:707:22 [opt]\r\n    frame #1: 0x000000013d6482ec duckdb.so`duckdb::SetSelectionVector(sel=0x000000016b816b40, indices_p=\"\", logical_type=0x000000016b816b58, size=2048, mask=0x0000000000000000, last_element_pos=0) at arrow_conversion.cpp:802:4 [opt]\r\n    frame #2: 0x000000013d6301ec duckdb.so`duckdb::ColumnArrowToDuckDBDictionary(vector=0x00000001330ba180, array=0x0000000110052f40, array_state=0x0000000127efbf40, size=2048, arrow_type=0x0000000107e52820, nested_offset=<unavailable>, parent_mask=<unavailable>, parent_offset=<unavailable>) at arrow_conversion.cpp:848:3 [opt]\r\n    frame #3: 0x000000013d62ee70 duckdb.so`duckdb::ArrowTableFunction::ArrowToDuckDB(scan_state=0x00000001061ea790, arrow_convert_data=size=38, output=0x0000000127fa7228, start=<unavailable>, arrow_scan_is_projected=true) at arrow_conversion.cpp:889:4 [opt]\r\n    frame #4: 0x000000013d62e98c duckdb.so`duckdb::ArrowTableFunction::ArrowScanFunction(context=<unavailable>, data_p=<unavailable>, output=0x0000000127fa7228) at arrow.cpp:369:3 [opt]\r\n    frame #5: 0x000000013d38e7bc duckdb.so`duckdb::PhysicalTableScan::GetData(this=<unavailable>, context=<unavailable>, chunk=0x0000000127fa7228, input=<unavailable>) const at physical_table_scan.cpp:74:2 [opt]\r\n    frame #6: 0x000000013d7aead0 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) [inlined] duckdb::PipelineExecutor::GetData(this=0x0000000127fa7150, chunk=0x0000000127fa7228, input=0x000000016b816d00) at pipeline_executor.cpp:528:26 [opt]\r\n    frame #7: 0x000000013d7aeaa8 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(this=0x0000000127fa7150, result=0x0000000127fa7228) at pipeline_executor.cpp:554:13 [opt]\r\n    frame #8: 0x000000013d7ae840 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000127fa7150, max_chunks=<unavailable>) at pipeline_executor.cpp:197:21 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x0000000106183340, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x0000000106183340, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137e37b40) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134048738, __args=0x0000000134048740, __args=0x0000000134048748) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134048730) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #20\r\n    frame #0: 0x000000013d647ef8 duckdb.so`duckdb::SetSelectionVector(duckdb::SelectionVector&, unsigned char*, duckdb::LogicalType&, unsigned long long, duckdb::ValidityMask*, unsigned long long) at arrow_conversion.cpp:729:23 [opt]\r\n    frame #1: 0x000000013d647ee4 duckdb.so`duckdb::SetSelectionVector(sel=0x000000016b8a2b40, indices_p=\"\", logical_type=0x000000016b8a2b58, size=2048, mask=0x000000016b8a2b20, last_element_pos=637) at arrow_conversion.cpp:763:4 [opt]\r\n    frame #2: 0x000000013d63019c duckdb.so`duckdb::ColumnArrowToDuckDBDictionary(vector=0x00000003b189f5c0, array=0x0000000117088f20, array_state=0x0000000107ec74b0, size=2048, arrow_type=0x0000000107e710b0, nested_offset=<unavailable>, parent_mask=<unavailable>, parent_offset=<unavailable>) at arrow_conversion.cpp:846:3 [opt]\r\n    frame #3: 0x000000013d62ee70 duckdb.so`duckdb::ArrowTableFunction::ArrowToDuckDB(scan_state=0x0000000132d2fc00, arrow_convert_data=size=38, output=0x0000000127eaedc8, start=<unavailable>, arrow_scan_is_projected=true) at arrow_conversion.cpp:889:4 [opt]\r\n    frame #4: 0x000000013d62e98c duckdb.so`duckdb::ArrowTableFunction::ArrowScanFunction(context=<unavailable>, data_p=<unavailable>, output=0x0000000127eaedc8) at arrow.cpp:369:3 [opt]\r\n    frame #5: 0x000000013d38e7bc duckdb.so`duckdb::PhysicalTableScan::GetData(this=<unavailable>, context=<unavailable>, chunk=0x0000000127eaedc8, input=<unavailable>) const at physical_table_scan.cpp:74:2 [opt]\r\n    frame #6: 0x000000013d7aead0 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) [inlined] duckdb::PipelineExecutor::GetData(this=0x0000000127eaecf0, chunk=0x0000000127eaedc8, input=0x000000016b8a2d00) at pipeline_executor.cpp:528:26 [opt]\r\n    frame #7: 0x000000013d7aeaa8 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(this=0x0000000127eaecf0, result=0x0000000127eaedc8) at pipeline_executor.cpp:554:13 [opt]\r\n    frame #8: 0x000000013d7ae840 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000127eaecf0, max_chunks=<unavailable>) at pipeline_executor.cpp:197:21 [opt]\r\n    frame #9: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #10: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x00000001061f3eb0, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #11: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x00000001061f3eb0, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #12: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137e5c590) at task_scheduler.cpp:139:32 [opt]\r\n    frame #13: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134047a78, __args=0x0000000134047a80, __args=0x0000000134047a88) at type_traits:3747:1 [opt]\r\n    frame #14: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #15: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134047a70) at thread:291:5 [opt]\r\n    frame #16: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #21\r\n    frame #0: 0x000000013d6311b4 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) [inlined] duckdb::string_t::string_t(this=<unavailable>, data=<unavailable>, len=15) at string_type.hpp:54:4 [opt]\r\n    frame #1: 0x000000013d6311b4 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) [inlined] duckdb::string_t::string_t(this=<unavailable>, data=<unavailable>, len=15) at string_type.hpp:39:43 [opt]\r\n    frame #2: 0x000000013d6311b4 duckdb.so`duckdb::ColumnArrowToDuckDB(duckdb::Vector&, ArrowArray&, duckdb::ArrowArrayScanState&, unsigned long long, duckdb::ArrowType const&, long long, duckdb::ValidityMask*, unsigned long long) at arrow_conversion.cpp:265:22 [opt]\r\n    frame #3: 0x000000013d631190 duckdb.so`duckdb::ColumnArrowToDuckDB(vector=0x000000012563b4c0, array=<unavailable>, array_state=<unavailable>, size=2048, arrow_type=<unavailable>, nested_offset=<unavailable>, parent_mask=<unavailable>, parent_offset=<unavailable>) at arrow_conversion.cpp:430:4 [opt]\r\n    frame #4: 0x000000013d6304b4 duckdb.so`duckdb::ColumnArrowToDuckDB(vector=0x000000012563b4c0, array=0x00000000000005a6, array_state=0x0000000512c9a1c0, size=2048, arrow_type=0x0000020300000025, nested_offset=1447, parent_mask=<unavailable>, parent_offset=15) at arrow_conversion.cpp:417:31 [opt]\r\n    frame #5: 0x000000013d62eaac duckdb.so`duckdb::ArrowTableFunction::ArrowToDuckDB(scan_state=0x0000000107a48890, arrow_convert_data=size=38, output=0x0000000107e50238, start=<unavailable>, arrow_scan_is_projected=true) at arrow_conversion.cpp:892:4 [opt]\r\n    frame #6: 0x000000013d62e98c duckdb.so`duckdb::ArrowTableFunction::ArrowScanFunction(context=<unavailable>, data_p=<unavailable>, output=0x0000000107e50238) at arrow.cpp:369:3 [opt]\r\n    frame #7: 0x000000013d38e7bc duckdb.so`duckdb::PhysicalTableScan::GetData(this=<unavailable>, context=<unavailable>, chunk=0x0000000107e50238, input=<unavailable>) const at physical_table_scan.cpp:74:2 [opt]\r\n    frame #8: 0x000000013d7aead0 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(duckdb::DataChunk&) [inlined] duckdb::PipelineExecutor::GetData(this=0x0000000107e50160, chunk=0x0000000107e50238, input=0x000000016b92ed00) at pipeline_executor.cpp:528:26 [opt]\r\n    frame #9: 0x000000013d7aeaa8 duckdb.so`duckdb::PipelineExecutor::FetchFromSource(this=0x0000000107e50160, result=0x0000000107e50238) at pipeline_executor.cpp:554:13 [opt]\r\n    frame #10: 0x000000013d7ae840 duckdb.so`duckdb::PipelineExecutor::Execute(this=0x0000000107e50160, max_chunks=<unavailable>) at pipeline_executor.cpp:197:21 [opt]\r\n    frame #11: 0x000000013d7b3754 duckdb.so`duckdb::PipelineTask::ExecuteTask(duckdb::TaskExecutionMode) [inlined] duckdb::PipelineExecutor::Execute(this=<unavailable>) at pipeline_executor.cpp:243:9 [opt]\r\n    frame #12: 0x000000013d7b374c duckdb.so`duckdb::PipelineTask::ExecuteTask(this=0x00000001061a3fb0, mode=PROCESS_ALL) at pipeline.cpp:50:34 [opt]\r\n    frame #13: 0x000000013d7a49e4 duckdb.so`duckdb::ExecutorTask::Execute(this=0x00000001061a3fb0, mode=<unavailable>) at executor_task.cpp:28:10 [opt]\r\n    frame #14: 0x000000013d7b10c8 duckdb.so`duckdb::TaskScheduler::ExecuteForever(this=0x0000000137fbdc20, marker=0x0000000137e6cc10) at task_scheduler.cpp:139:32 [opt]\r\n    frame #15: 0x000000013d7b64b0 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] decltype(std::__1::forward<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*)>(fp)(std::__1::forward<duckdb::TaskScheduler*>(fp0), std::__1::forward<std::__1::atomic<bool>*>(fp0))) std::__1::__invoke<void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>(__f=0x0000000134047ac8, __args=0x0000000134047ad0, __args=0x0000000134047ad8) at type_traits:3747:1 [opt]\r\n    frame #16: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*, 2ul, 3ul>(__t=size=4, (null)=<unavailable>) at thread:280:5 [opt]\r\n    frame #17: 0x000000013d7b64a4 duckdb.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, void (*)(duckdb::TaskScheduler*, std::__1::atomic<bool>*), duckdb::TaskScheduler*, std::__1::atomic<bool>*>>(__vp=0x0000000134047ac0) at thread:291:5 [opt]\r\n    frame #18: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #22\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #23\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #24\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #25\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #26\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #27\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #28\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #29\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #30\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #31\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #32\r\n    frame #0: 0x00000001266b828c libparquet.1500.1.0.dylib`parquet::(anonymous namespace)::DictDecoderImpl<parquet::PhysicalType<(parquet::Type::type)6>>::DecodeIndicesSpaced(int, int, unsigned char const*, long long, arrow::ArrayBuilder*) + 400\r\n    frame #1: 0x0000000126621478 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)6>>::ReadRecordData(long long) + 756\r\n    frame #2: 0x00000001266199b0 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)6>>::ReadRecords(long long) + 180\r\n    frame #3: 0x00000001265bce48 libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::LeafReader::LoadBatch(long long) + 260\r\n    frame #4: 0x00000001265bc7b0 libparquet.1500.1.0.dylib`parquet::arrow::ColumnReaderImpl::NextBatch(long long, std::__1::shared_ptr<arrow::ChunkedArray>*) + 52\r\n    frame #5: 0x00000001265c2fec libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::FileReaderImpl::ReadColumn(int, std::__1::vector<int, std::__1::allocator<int>> const&, parquet::arrow::ColumnReader*, std::__1::shared_ptr<arrow::ChunkedArray>*) + 272\r\n    frame #6: 0x00000001265ce08c libparquet.1500.1.0.dylib`arrow::internal::FnOnce<void ()>::FnImpl<std::__1::__bind<arrow::detail::ContinueFuture, arrow::Future<std::__1::shared_ptr<arrow::ChunkedArray>>&, parquet::arrow::(anonymous namespace)::FileReaderImpl::DecodeRowGroups(std::__1::shared_ptr<parquet::arrow::(anonymous namespace)::FileReaderImpl>, std::__1::vector<int, std::__1::allocator<int>> const&, std::__1::vector<int, std::__1::allocator<int>> const&, arrow::internal::Executor*)::$_3&, unsigned long&, std::__1::shared_ptr<parquet::arrow::ColumnReaderImpl>>>::invoke() + 108\r\n    frame #7: 0x000000013489eefc libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 488\r\n    frame #8: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #33\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #34\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #35\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #36\r\n    frame #0: 0x000000018a561c08 libsystem_kernel.dylib`__munmap + 8\r\n    frame #1: 0x000000013554a618 libarrow.1500.1.0.dylib`os_pages_unmap + 36\r\n    frame #2: 0x0000000135542df0 libarrow.1500.1.0.dylib`je_arrow_private_je_extent_dalloc_mmap + 36\r\n    frame #3: 0x0000000135541ea4 libarrow.1500.1.0.dylib`je_arrow_private_je_extent_dalloc_wrapper + 532\r\n    frame #4: 0x0000000135549ff0 libarrow.1500.1.0.dylib`pac_decay_to_limit + 484\r\n    frame #5: 0x0000000135520228 libarrow.1500.1.0.dylib`arena_decay_impl + 152\r\n    frame #6: 0x000000013551cb18 libarrow.1500.1.0.dylib`je_arrow_private_je_arena_handle_deferred_work + 92\r\n    frame #7: 0x000000013554767c libarrow.1500.1.0.dylib`large_dalloc_finish_impl + 64\r\n    frame #8: 0x00000001355476d8 libarrow.1500.1.0.dylib`je_arrow_private_je_large_dalloc + 76\r\n    frame #9: 0x00000001355176c0 libarrow.1500.1.0.dylib`je_arrow_private_je_sdallocx_default + 1120\r\n    frame #10: 0x000000013474339c libarrow.1500.1.0.dylib`arrow::BaseMemoryPoolImpl<arrow::memory_pool::internal::JemallocAllocator>::Free(unsigned char*, long long, long long) + 36\r\n    frame #11: 0x0000000134743a84 libarrow.1500.1.0.dylib`arrow::PoolBuffer::~PoolBuffer() + 72\r\n    frame #12: 0x0000000134743610 libarrow.1500.1.0.dylib`arrow::PoolBuffer::~PoolBuffer() + 12\r\n    frame #13: 0x0000000126615828 libparquet.1500.1.0.dylib`parquet::(anonymous namespace)::SerializedPageReader::~SerializedPageReader() + 284\r\n    frame #14: 0x0000000126615948 libparquet.1500.1.0.dylib`parquet::(anonymous namespace)::SerializedPageReader::~SerializedPageReader() + 12\r\n    frame #15: 0x000000012661b290 libparquet.1500.1.0.dylib`parquet::(anonymous namespace)::ColumnReaderImplBase<parquet::PhysicalType<(parquet::Type::type)6>>::~ColumnReaderImplBase() + 192\r\n    frame #16: 0x0000000126622a30 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::ByteArrayChunkedRecordReader::~ByteArrayChunkedRecordReader() + 232\r\n    frame #17: 0x00000001265bcbd0 libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::LeafReader::~LeafReader() + 68\r\n    frame #18: 0x00000001265bcca8 libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::LeafReader::~LeafReader() + 12\r\n    frame #19: 0x00000001265cdfc0 libparquet.1500.1.0.dylib`arrow::internal::FnOnce<void ()>::FnImpl<std::__1::__bind<arrow::detail::ContinueFuture, arrow::Future<std::__1::shared_ptr<arrow::ChunkedArray>>&, parquet::arrow::(anonymous namespace)::FileReaderImpl::DecodeRowGroups(std::__1::shared_ptr<parquet::arrow::(anonymous namespace)::FileReaderImpl>, std::__1::vector<int, std::__1::allocator<int>> const&, std::__1::vector<int, std::__1::allocator<int>> const&, arrow::internal::Executor*)::$_3&, unsigned long&, std::__1::shared_ptr<parquet::arrow::ColumnReaderImpl>>>::~FnImpl() + 120\r\n    frame #20: 0x000000013489ef0c libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 504\r\n    frame #21: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #37\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #38\r\n    frame #0: 0x00000001266b6618 libparquet.1500.1.0.dylib`int arrow::util::RleDecoder::GetBatchWithDict<double>(double const*, int, double*, int) + 624\r\n    frame #1: 0x00000001266b6718 libparquet.1500.1.0.dylib`int arrow::util::RleDecoder::GetBatchWithDictSpaced<double>(double const*, int, double*, int, int, unsigned char const*, long long) + 144\r\n    frame #2: 0x00000001266b6238 libparquet.1500.1.0.dylib`virtual thunk to parquet::(anonymous namespace)::DictDecoderImpl<parquet::PhysicalType<(parquet::Type::type)5>>::DecodeSpaced(double*, int, int, unsigned char const*, long long) + 88\r\n    frame #3: 0x000000012663f2e0 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)5>>::ReadValuesSpaced(long long, long long) + 108\r\n    frame #4: 0x000000012663ff28 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)5>>::ReadRecordData(long long) + 756\r\n    frame #5: 0x000000012663e704 libparquet.1500.1.0.dylib`parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)5>>::ReadRecords(long long) + 180\r\n    frame #6: 0x00000001265bce48 libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::LeafReader::LoadBatch(long long) + 260\r\n    frame #7: 0x00000001265bc7b0 libparquet.1500.1.0.dylib`parquet::arrow::ColumnReaderImpl::NextBatch(long long, std::__1::shared_ptr<arrow::ChunkedArray>*) + 52\r\n    frame #8: 0x00000001265c2fec libparquet.1500.1.0.dylib`parquet::arrow::(anonymous namespace)::FileReaderImpl::ReadColumn(int, std::__1::vector<int, std::__1::allocator<int>> const&, parquet::arrow::ColumnReader*, std::__1::shared_ptr<arrow::ChunkedArray>*) + 272\r\n    frame #9: 0x00000001265ce08c libparquet.1500.1.0.dylib`arrow::internal::FnOnce<void ()>::FnImpl<std::__1::__bind<arrow::detail::ContinueFuture, arrow::Future<std::__1::shared_ptr<arrow::ChunkedArray>>&, parquet::arrow::(anonymous namespace)::FileReaderImpl::DecodeRowGroups(std::__1::shared_ptr<parquet::arrow::(anonymous namespace)::FileReaderImpl>, std::__1::vector<int, std::__1::allocator<int>> const&, std::__1::vector<int, std::__1::allocator<int>> const&, arrow::internal::Executor*)::$_3&, unsigned long&, std::__1::shared_ptr<parquet::arrow::ColumnReaderImpl>>>::invoke() + 108\r\n    frame #10: 0x000000013489eefc libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 488\r\n    frame #11: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n  thread #39\r\n    frame #0: 0x000000018a5619ec libsystem_kernel.dylib`__psynch_cvwait + 8\r\n    frame #1: 0x000000018a59f55c libsystem_pthread.dylib`_pthread_cond_wait + 1228\r\n    frame #2: 0x000000018a4c4b14 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 28\r\n    frame #3: 0x000000013489efa8 libarrow.1500.1.0.dylib`void* std::__1::__thread_proxy[abi:v160006]<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct>>, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_6>>(void*) + 660\r\n    frame #4: 0x000000018a59ef94 libsystem_pthread.dylib`_pthread_start + 136\r\n```\r\n<\/details>\r\n"],"labels":["Type: bug","Component: R"]},{"title":"[C++][Parquet] Implement SizeStatistics","body":"### Describe the enhancement requested\n\nParquet format 2.10.0 has introduced SizeStatistics. parquet-mr has also implemented this: https:\/\/github.com\/apache\/parquet-mr\/pull\/1177. Now it is time for parquet-cpp to pick the ball.\n\n### Component(s)\n\nC++, Parquet","comments":[],"labels":["Type: enhancement","Component: Parquet","Component: C++"]},{"title":"[C++] S3FileSystem generates a significant amount of redundant requests","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI am using the S3 Filesystem abstractions to process a large Parquet dataset. The job reads data in batches and incrementally produces a new set of Parquet files. During the execution it checkpoints it's work.\r\n\r\nThe checkpointing code is fairly rudamentary: we incrementally write a Parquet file to the `checkpoint\/` prefix, and periodically copy it to the `output\/` directory using `fs.move()`. We then write a `.json` file to the checkpoint directory detailing the current progress. Not perfect, but works well enough for our needs.\r\n\r\nAfter implementing this I analyzed the requests to the bucket and found a surprising number of requests where being made. For one job, `520,752` requests where made to the `checkpointing\/` prefix in S3, whereas only `74,331` requests where made to read + write the dataset. This is a significant discrepency - nearly 7x the number of requests!\r\n\r\nThe following is a table of requests to the checkpoint prefix broken down by type:\r\n\r\n| operation | total |\r\n| :--- | :--- |\r\n| REST.GET.OBJECT | 91871 |\r\n| REST.POST.UPLOADS | 84285 |\r\n| REST.PUT.PART | 84262 |\r\n| REST.POST.UPLOAD | 84261 |\r\n| REST.COPY.OBJECT | 42131 |\r\n| REST.COPY.OBJECT\\_GET | 42131 |\r\n| REST.PUT.OBJECT | 42129 |\r\n| REST.DELETE.OBJECT | 42129 |\r\n| REST.HEAD.OBJECT | 7553 |\r\n\r\nI've dug into this and here are the reasons why:\r\n\r\n`126,388` of the `REST.PUT.PART`, `REST.POST.UPLOADS` and `REST.POST.UPLOAD` requests are made while writing the small `.json` checkpoint files. Due [to this issue](https:\/\/github.com\/apache\/arrow\/issues\/40557), despite being less than 1kb in size and few in number each creation of the `.json` checkpoint files requires 3 requests to S3.\r\n\r\nA further `126,420` requests are also multipart uploads, whilst creating Parquet files. Looking at the statistics, ~50% of the multipart upload requests could have been avoided with an initial 30 megabyte buffer before initiating a multipart upload.\r\n\r\nThe `42,129` requests with the type `REST.PUT.OBJECT` are due to the implementation of `move()` and `delete()`: it currently attempts to [re-create the parent directory](https:\/\/github.com\/apache\/arrow\/blob\/b448b33808f2dd42866195fa4bb44198e2fc26b9\/cpp\/src\/arrow\/filesystem\/s3fs.cc#L2849) after a copy or delete - this is because if there is only a single file in the prefix and we move\/delete it, then the prefix will no longer exist. The workaround as implemented is to create a 0-sized object with the name of the prefix, ensuring that it still \"exists\".\r\n\r\nThe `7,553` requests with the type `REST.HEAD.OBJECT` comes in part from from the implementation of `DeleteObject`, where [we make a HeadObject request before deleting a key](https:\/\/github.com\/apache\/arrow\/blob\/b448b33808f2dd42866195fa4bb44198e2fc26b9\/cpp\/src\/arrow\/filesystem\/s3fs.cc#L2805).\r\n\r\n# Performance with versioned buckets\r\n\r\nWhile it's noble to attempt to create a proper \"filesystem\" facade over S3, there are inherent issues with this in terms of cost and performance.\r\n\r\nOne major thing that worries me [is the `EnsureparentExists()`](https:\/\/github.com\/apache\/arrow\/blob\/b448b33808f2dd42866195fa4bb44198e2fc26b9\/cpp\/src\/arrow\/filesystem\/s3fs.cc#L2521) that is called from `DeleteDir`, `DeleteFile` and `Move` methods. In a versioned bucket, this will repeatedly create empty keys to mimic a directory.\r\n\r\nGiven a pathalogical case where you do something like this from multiple processes\/threads:\r\n\r\n```python\r\nsfs = fs.S3FileSystem()\r\n\r\nfor _ in range(10000):\r\n    path = f\"a_bucket\/some_directory\/{uuid.uuid4()}\"\r\n    with sfs.open_output_stream(path) as fd:\r\n        fd.write(b'hi')\r\n    sfs.move(path, f\"a_bucket\/some_other_directory\/{uuid.uuid4()}\")\r\n```\r\n\r\nThen we will end up with many tens of thousands of versioned objects with the key `some_directory\/`.\r\n\r\nOne lesser-known thing about S3 versioned buckets is that `list_objects_v2` (and `list_objects`) calls have to skip over deleted\/noncurrent versions of objects. While this is fast, it can get _very_ slow - I've seen prefixes take over a minute to list objects due to the number of noncurrent\/deleted objects _within those prefixes_ (i.e listing an outer \"directory\" takes longer if there are many deleted\/noncurrent children within sub-directories). Obviously lifecycle policies can clean these out, but that only happens once a day or so. \r\n\r\nYou will also almost certainly run into S3 request limits when you repeatedly call `PutObject` on a single key at high volume - requests limits in aggregate are very high for S3, but for a single key they can be much lower. \r\n\r\n### Component(s)\r\n\r\nC++, Python","comments":["> Then we will end up with many tens of thousands of versioned objects with the key some_directory\/.\r\n\r\nThe solution here may be just checking that the empty dir marker exists instead of re-creating it unconditionally.","> > Then we will end up with many tens of thousands of versioned objects with the key some_directory\/.\r\n> \r\n> The solution here may be just checking that the empty dir marker exists instead of re-creating it unconditionally.\r\n\r\nThat\u2019s no guarantee though, and would generate a lot of redundant requests.\r\n\r\nAnother idea would be to stop trying to emulate directories on S3. This would need to be feature gated in some way as there may be conditional code that relies on the existence of a directory, but if the feature is enabled (via a flag or a s3 filesystem subclass) then the \u201cdirectory exists\u201d function always returns True?\r\n\r\nI think it\u2019s a fare trade-off if you know you want it?","> ... then the \u201cdirectory exists\u201d function always returns True?\r\n\r\nI suggested something like this months ago while implementing `AzureFileSystem` but quickly learned that these \"simplifications\" break a lot of the invariants `arrow::FileSystem` interface is supposed to guarantee. The reason S3 and other blob stores don't offer FS semantics is efficiency and ability to distribute the system without breaking the filesystem invariants.\r\n\r\nThe true fix would be to have datasets loaded from an `arrow::ObjectStore` abstraction that is emulated on local-filesystem instead of going through an `arrow::FileSystem` abstraction that is emulated on top of object storage systems like S3."],"labels":["Type: bug","Component: C++","Component: Python"]},{"title":"GH-40586: [Dev][C++][Python][R] Use pre-commit for clang-format","body":"### Rationale for this change\r\n\r\nWe can run `clang-format` easily than `archery lint` by using `pre-commit`:\r\n\r\n* We don't need to install `clang-format-14` separately because `pre-commit` prepare it automatically.\r\n* We don't need to run `cmake` to run `clang-format-14`. \r\n\r\n### What changes are included in this PR?\r\n\r\nAdd `clang-format` related `pre-commit` configurations.\r\n\r\nThis doesn't change `archery lint` because our `pre-commit` configurations can't replace `archery lint` entirely yet.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\n* GitHub Issue: #40586","comments":[":warning: GitHub issue #40586 **has been automatically assigned in GitHub** to PR creator.","I'll merge this in a few days if nobody objects this."],"labels":["Component: C++","awaiting committer review"]},{"title":"[Dev][C++][Python][R] Use pre-commit for clang-format","body":"### Describe the enhancement requested\n\nWe can use https:\/\/github.com\/pre-commit\/mirrors-clang-format for this.\n\n### Component(s)\n\nC++, Developer Tools, Python, R","comments":[],"labels":["Type: enhancement","Component: R","Component: C++","Component: Python","Component: Developer Tools"]},{"title":"[Python]Converting a list composed of multiple multi-dimensional halffloat numpy arrays of different shapes into pyarrow.Array.","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\n> The following content uses version 15.0.0 of pyarrow.\r\n\r\nFirst try the following:\r\n```\r\nimport numpy as np\r\nimport pyarrow as pa\r\n\r\nnp_dtype = np.float16\r\nnp_values = np.random.rand(2, 3, 3).astype(np_dtype)\r\nnp_values2 = np.random.rand(3, 3, 3).astype(np_dtype)\r\n\r\npa_values = pa.array([np_values, np_values2])\r\n```\r\nbut error occured:\r\n```\r\npa_values = pa.array([np_values, np_values2])\r\n  File \"pyarrow\\array.pxi\", line 344, in pyarrow.lib.array\r\n  File \"pyarrow\\array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow\\error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Can only convert 1-dimensional array values\r\n```\r\nso I try another way:\r\n```\r\npa_fix_shape_tensor = pa.FixedShapeTensorArray.from_numpy_ndarray(np_values)\r\npa_fix_shape_tensor2 = pa.FixedShapeTensorArray.from_numpy_ndarray(np_values2)\r\n\r\npa_values = pa.array([pa_fix_shape_tensor, pa_fix_shape_tensor2])\r\n```\r\nanother error comes up, it looks like the FixedShapeTensorArray is not recognized:\r\n```\r\npyarrow.lib.ArrowInvalid: Could not convert <pyarrow.lib.FixedShapeTensorArray object at 0x0000023A4DA5C880>\r\n[ ... ]\r\nwith type FixedShapeTensorArray: did not recognize Python value type when inferring an Arrow data type\r\n```\r\n\r\nFinally tried to implement it indirectly using list:\r\n```\r\nvalue_list = np_values.tolist()\r\nvalue_list2 = np_values2.tolist()\r\npa_dtype = pa.list_(pa.list_(pa.list_(pa.float16())))\r\n\r\npa_values = pa.array([value_list, value_list2], type=pa_dtype)\r\n```\r\nbut it looks like there was a problem converting to float16 (I tried float32 and it worked fine):\r\n```\r\n...\r\n   pa_values = pa.array([value_list, value_list2], type=pa_dtype)\r\nFile \"pyarrow\\array.pxi\", line 344, in pyarrow.lib.array\r\n   File \"pyarrow\\array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n   File \"pyarrow\\error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n   File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowTypeError: Expected np.float16 instance\r\n```\r\n\r\n\r\nTherefore, I want to know if there is a direct and efficient way to achieve the requirement: Converting a list composed of multiple multi-dimensional halffloat numpy arrays of different shapes into pyarrow.Array.\n\n### Component(s)\n\nPython","comments":[],"labels":["Component: Python","Type: usage"]},{"title":"[C++][Skyhook] Refactor Skyhook file format to use a higher level interface","body":"### Describe the enhancement requested\n\nThe enhancement is to refactor the Skyhook file format (a custom target of the Arrow dataset API) to loosen the coupling with the Arrow dataset API.\r\n\r\nThe request was originally made via the mailing list ([link][ml-refactor-skyhook]) and mentions that improvements to the dataset API are not possible while maintaining the Skyhook file format in its current form. A reference is also made to Acero and its usage of substrait, perhaps to contextualize a preferred approach.\r\n\r\n\r\n<!-- resources -->\r\n[ml-refactor-skyhook]: https:\/\/lists.apache.org\/thread\/77jkh04d06brs6l0j1vqzgx5yncp69g1\n\n### Component(s)\n\nC++","comments":["The basic features of skyhook are a configurable server side scan\/compute, and efficient transport of resulting buffers to the client. This does not correspond to a file format; file formats compartmentalize reading data files independent of I\/O. A CSV formatted file might reside in a string literal or an S3 bucket, but skyhook's files are actually IPC or parquet and must reside on a ceph server. Writing skyhook as a file format therefore breaks conventions and contracts relied on by the dataset API.\r\n\r\nHowever in the context of acero skyhook has a very natural structure:\r\n- A source `ExecNode` on the client which requests specific scan\/compute from the server, forwards returned batches into an `ExecPlan`, and mediates back pressure.\r\n- A ceph CLS which maintains a catalog of data files for scanning, instantiates server side `ExecPlan`s to fulfil client requests, and responds to back pressure from the client.\r\n\r\nSome design notes which follow from the above:\r\n\r\nCurrently skyhook supports only predicate and projection pushdown, but there's no reason to forbid specifying arbitrary computation to the server. For example an aggregation performed on the server could save significant network overhead when the result is small. Although Arrow's `Expression`s are serializable this feature isn't designed for stability. Substrait is a more capable and stable tool intended for transmitting arbitrary execution plans. It'd be worthwhile to replace projection\/filter `Expression` serialization with a substrait subplan to be executed on the server. This should actually simplify skyhook's server side code since it can reuse `arrow::engine::SerializePlan\/DeserializePlan`.\r\n\r\nFurthermore, skyhook need not be bound to a single file format or to output small enough to be materialized as a single `Table`. On the server an arbitrary collection of data files can be wrapped into a catalog of datasets. For example, a collection of csv and parquet data files could be wrapped in a UnionDataset, cataloged as a single table `nyc-taxi`. The server side complexity of a dataset can be invisible to client configuring a scan; clients should only need to reference these datasets by name and schema, using a substrait `NamedTable` message.\r\n\r\nInstead of having each skyhook scan negotiate a new connection, the connection to a ceph server should be factored out to an independent client side object. This way validation of the connection, checking for registration of a compatible CLS, and caching of the server's catalog can all be compartmentalized into a single construction.\r\n\r\nThe purpose of the arrow monorepo is cross platform library code, whereas skyhook is comprised of a specialized service and client pair. Skyhook should therefore probably be relocated outside the arrow monorepo. Moreover skyhook is compelling as a demonstration of both the extensibility and usability of acero from third party code, which will be emphasized by keeping it in an independent project.\r\n\r\nSome example code illustrating what using a skyhook source node on the client could look like:\r\n\r\n```c++\r\n\/\/ This only needs to be done once:\r\nARROW_ASSIGN_OR_RAISE(auto skyhook_connection, MakeSkyhookConnection(\r\n    config_path, data_pool, user_name, cluster_name));\r\n\r\n\/\/ Display the catalog:\r\nfor (auto [name, schema] : skyhook_connection.catalog()) {\r\n  std::cout << name << \": \" << schema->ToString() << std::endl;\r\n}\r\n\r\nResult<std::shared_ptr<Table>> GetTripCostsForYear(int year) {\r\n  \/\/ Declare a server side plan which will aggregate trip costs\r\n  \/\/ grouped by tag in the specified year:\r\n  auto server_side_plan = Declaration::Sequence({\r\n      {\"named_table\", skyhook_connection.MakeNamedTableOptions(\"nyc-taxi\")},\r\n      {\"filter\", FilterNodeOptions(equal(field_ref(\"year\"), literal(year)))},\r\n      {\"aggregate\", AggregateNodeOptions({{\"hash_sum\", \"cost\", \"cost_sum\"}}, {\"tag\"})},\r\n  });\r\n  \r\n  \/\/ The client side plan then uses the server side plan to configure its source node:\r\n  std::shared_ptr<Table> output_table;\r\n  auto client_side_plan = Declaration::Sequence({\r\n      {\"skyhook_source\", SkyhookSourceNodeOptions(server_side_plan, skyhook_connection)},\r\n      {\"table_sink\", TableSinkNodeOptions(&output_table)},\r\n  });\r\n\r\n  \/\/ Run the plan, return the resulting table\r\n  ARROW_ASSIGN_OR_RAISE(auto plan, ExecPlan::Make());\r\n  ARROW_RETURN_NOT_OK(client_side_plan.AddToPlan(plan.get()));\r\n  auto finished = plan->finished();\r\n  ARROW_RETURN_NOT_OK(finished.status());\r\n  return output_table;\r\n}\r\n```\r\n\r\nA summary of the additions necessary to make that work, client side:\r\n- SkyhookSourceNode (subclass of ExecNode)\r\n  - in StartProducing() override: serialize `server_side_plan` to substrait and send to server\r\n  - receives `ceph::bufferlist`s asynchronously, converts to `ExecBatch`es, then pushes to the next node with `this->output()->InputReceived()`\r\n  - forwards PauseProducing() and ResumeProducing() calls to the server\r\n- SkyhookSourceNodeOptions (subclass of ExecNodeOptions)\r\n- in client initialization: register SkyhookSourceNode factory named \"skyhook_source\"\r\n\r\n... and on the server side:\r\n- Maintain a catalog mapping table names to datasets, accessible as a NamedTableProvider\r\n- Respond to new connections with a summary of the catalog, mapping names to schemas\r\n- Receive and excecute `server_side_plan`s from clients\r\n  - deserialize from substrait\r\n  - construct and run `ExecPlan`\r\n  - convert each generated `ExecBatch` to `ceph::bufferlist` and transmit to the client\r\n  - pause and resume when indicated by the client\r\n","Wow, thanks so much for the write up: it is well written and gives me clear ideas of the direction you're proposing and where the drawbacks of the Skyhook file format are.\r\n\r\nI want to clarify (or maybe pushback?) on a few things, but overall I see a path forward with minimal changes to your suggestions.\r\n\r\n> The basic features of skyhook are a configurable server side scan\/compute, and efficient transport of resulting buffers to the client. This does not correspond to a file format; file formats compartmentalize reading data files independent of I\/O.\r\n\r\nI believe that the Skyhook file format (as implemented) is a contract that when files are written in a given format (arrow IPC) using a standard posix filesystem, they can be read using a different interface (arrow dataset). Because two separate I\/O interfaces are used, the file format accommodates that in the contract. It is akin to saying that if you write files in a particular way, you can process the blocks of the file without changing how you write files.\r\n\r\n> Writing skyhook as a file format therefore breaks conventions and contracts relied on by the dataset API.\r\n\r\nYes, this part I understand. I would just add that this is because ceph (which skyhook is an extension for) has a broader definition of what a \"file\" is in order to map it to object storage. Any system that shims a filesystem interface over the actual storage interface is going to have a similar impedance mismatch (e.g. s3fs or anything like that).\r\n\r\nAll that being said, I agree that nearly all of the changes you've proposed are good; the only one I think we won't do is grouping of files (unless we find a way to shim a \"special directory\" as a grouping, but that sounds hack-y).\r\n\r\nTo rephrase your proposal, I believe you're suggesting we implement a custom operator (`SkyhookSourceNode`) to facilitate data flow between the client and server over the network. The input to the custom operator is executed by Skyhook and the results are handled by the client node.\r\n\r\nI think the catalog can still be resolved on the client side and the custom operator can be used in many connections (every FileFragment is an independent access to a potentially distinct ceph server) and this would only require very simple ExecPlan transformations.","I also want to note that pushing any more operators below the `SkyhookSourceNode` would be done blindly without some type of optimizer making that decision. So, the new operator makes sense when inserted above `Read` operators and that would be the sensible first step.\r\n\r\n@bkietz would you know how Acero serializes custom operators into substrait? Does it use a message that is only known internally? I have been working with custom substrait messages and `SkyhookSourceNode` is a perfect example of where it seems to make sense to use one (and I have something similar I can use [here](https:\/\/github.com\/drin\/mohair-protocol\/blob\/mainline\/proto\/mohair\/algebra.proto#L20-L43)","> I believe that the Skyhook file format (as implemented) is a contract that when files are written in a given format (arrow IPC) using a standard posix filesystem, they can be read using a different interface (arrow dataset).\r\n\r\nThe problem is that the contract of skyhook as written is not the contract of a file format. File formats are intended to be an orthogonal detail to I\/O. It would be possible to write a subclass of `arrow::Array` which is mutable, but although that is possible the broken contract of immutability will severely restrict its usage in the arrow library.\r\n\r\n> To rephrase your proposal, I believe you're suggesting we implement a custom operator (SkyhookSourceNode) to facilitate data flow between the client and server over the network. The input to the custom operator is executed by Skyhook and the results are handled by the client node.\r\n\r\nYes, SkyhookSourceNode is a client side node which proxies the server side plan in the client side plan.\r\n\r\n> (every FileFragment is an independent access to a potentially distinct ceph server)\r\n\r\nI would recommend instead having a 1:1 relationship between SkyhookSourceNodes and ceph servers. If multiple ceph servers are in play, a UnionNode can be used in the client side plan to concatenate their streams.\r\n\r\n> would you know how Acero serializes custom operators into substrait?\r\n\r\nThe design above does not require serializing custom nodes, since it only requires serialization of the server side plan. The SkyhookSourceNode will only appear in the client side plan.\r\n\r\n> I also want to note that pushing any more operators below the SkyhookSourceNode would be done blindly without some type of optimizer\r\n\r\nOptimization and other restructuring of plans is not currently in scope for acero. Instead the exact plan specified is what will be executed. The ~~roadmap~~ hope is that acero's substrait support will continue to improve and that cross-engine plan optimizers will be written against substrait","> The hope is that acero's substrait support will continue to improve and that cross-engine plan optimizers will be written against substrait\r\n\r\nI am working on enabling cross-engine plan optimizers, so that's not a general issue, just a necessary mechanism to enable specifying arbitrary computation to skyhook.\r\n\r\n> I would recommend instead having a 1:1 relationship between SkyhookSourceNodes and ceph servers. If multiple ceph servers are in play, a UnionNode can be used in the client side plan to concatenate their streams\r\n\r\nEdit: I realize I misread this a bit earlier. 1:1 between SkyhookSourceNodes and ceph servers is what I was mentioning, but that still requires many connections unless it's possible to reuse one pipe for many servers.\r\n\r\n> The design above does not require serializing custom nodes, since it only requires serialization of the server side plan\r\n\r\nThen I may be misunderstanding. If SkyhookSourceNode is a client-only node, then it seems like you're proposing execution of 2 independent ExecPlans, and not the use of SkyhookSourceNode for facilitating network communication in the execution of a single ExecPlan, correct? In which case I suppose I can understand the recommendation of a UnionNode, but I am not sure it would even be needed if execution on the server side and client side are independent.\r\n","> File formats are intended to be an orthogonal detail to I\/O.\r\n\r\nI won't belabor the point because I understand you to mean this for the Arrow library in particular. I was just mentioning that file format is not orthogonal if a storage system wants to improve performance. Parquet is an example where it is not orthogonal, the point of RowGroups is to allow portions of a file to be accessible independent of the larger file.\r\n\r\nBut this was meant to be a broad comment, not on how the dataset API should be designed; I totally agree that we want to remove the skyhook file format if it's hindering Arrow's development.\r\n\r\nActually, it occurs to me that skyhook file format should really have been more like a file reader and the difference from the parquet reader is that it can execute compute, etc. on each fragment. I guess another anecdote that the abstractions are not very clean cut.","> If SkyhookSourceNode is a client-only node, then it seems like you're proposing execution of 2 independent ExecPlans\r\n\r\nPrecisely; a server side plan which reads files and performs pushed-down compute finally pushing batches to the client, and a client side plan which receives these batches and performs other computation on them. This is analogous to the current structure of Skyhook which uses arrow datasets on the [server side](https:\/\/github.com\/bkietz\/arrow\/blob\/46758bc3c6321d8b7013acf52bff7761a8b33eda\/cpp\/src\/skyhook\/cls\/cls_skyhook.cc#L153) to scan\/filter\/project\/transmit a Table to the client, and on the [client side](https:\/\/github.com\/bkietz\/arrow\/blob\/46758bc3c6321d8b7013acf52bff7761a8b33eda\/cpp\/src\/skyhook\/client\/file_skyhook.h#L58) to act as a data source.\r\n\r\nFor example: the server side plan might read a set of parquet files with a pushed down filter and perform aggregation while the client side plan includes a UnionNode and collects batches from the skyhook server and from a local dataset, collecting into a single Table:\r\n\r\n```\r\n# server side:\r\nScanNode(nyc-taxi\/*.parquet)\r\n  -> FilterNode(year==2016)\r\n    -> AggregateNode(cost ON tag)\r\n       -> SinkNode(push ceph::bufferlist to client)\r\n\r\n# client side:\r\nScanNode(local-addenda.parquet) ---------------------------v\r\nSkyhookSourceNode(receive ceph::bufferlist from server) -> UnionNode -> TableSinkNode\r\n```\r\n\r\nIn this example, the client side plan includes a SkyhookSourceNode to facilitate network communication by forwarding batches from the server into the UnionNode. For another example, the client side plan could union streams from three different skyhook servers (since we have UnionNode, there's no need to complicate SkyhookSourceNode by forcing it to deal with multiple servers):\r\n\r\n```\r\n# client side:\r\nSkyhookSourceNode(receive ceph::bufferlist from server A) ---v\r\nSkyhookSourceNode(receive ceph::bufferlist from server B) -> UnionNode -> TableSinkNode\r\nSkyhookSourceNode(receive ceph::bufferlist from server C) ---^\r\n```","> Actually, it occurs to me that skyhook file format should really have been more like a file reader and the difference from the parquet reader is that it can execute compute, etc. on each fragment. I guess another anecdote that the abstractions are not very clean cut.\r\n\r\nThis isn't explicitly documented, it's true. It might be useful to note that file readers and the file formats which wrap them can be pointed at a path on a file system or a specific buffer (FileSource) and are guaranteed to do no IO other than reading from that file system or that buffer.\r\n","one last thing I'm curious about, I assume that the sink node on the server side can't stream batches to the client? or is there a way to access ExecBatches from the sink node in a streaming manner?","The default sink node does allow access to the stream of batches as a [`AsyncGenerator<std::optional<ExecBatch>>`](https:\/\/github.com\/bkietz\/arrow\/blob\/46758bc3c6321d8b7013acf52bff7761a8b33eda\/cpp\/src\/arrow\/acero\/hash_aggregate_test.cc#L315-L325). `AsyncGenerator<T>` is a sequence of `Future<T>`, and we have several utilities to consume and compose these. The simplest approach is probably to wrap the generator into a synchronous sequence with `MakeGeneratorIterator` and dedicate a thread to iterate through the synchronous sequence, pushing each batch"],"labels":["Type: enhancement","Component: C++"]},{"title":"Change in how Spaces are handled in file names","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nHello,\r\n\r\nLong time `{arrow}` user.\r\n\r\nI have a production project (using R) which uses `arrow@10.0.0`. I'm performing a migration activity to update packages, and am trying out v15.\r\n\r\nOne breaking change on my end is how spaces are handled in file names. Unsure which version this changed along the way, but, for a grouped dataset:\r\n\r\nIn v10 :\r\n\r\n<img width=\"139\" alt=\"image\" src=\"https:\/\/github.com\/apache\/arrow\/assets\/6344050\/e4c602e6-f512-4aef-80fa-29789d6416c1\">\r\n\r\nIn v15:\r\n\r\n<img width=\"134\" alt=\"image\" src=\"https:\/\/github.com\/apache\/arrow\/assets\/6344050\/f27035b0-f48f-479c-921d-074c60ee5242\">\r\n\r\nUnfortunately, this has become a breaking change, since I have a storage system which cannot accept `%` in the filename.\r\n\r\nWould you know if this is a change in `arrow` or a change in some dependent library like `dplyr`? \r\n\r\nIs there a way to _not_ convert the spaces to `%20` in the file names?\r\n\r\nThanks!\n\n### Component(s)\n\nR","comments":["Hi @rsangole, thanks for letting us know. This is caused by the breaking change made in v11 which was made to address https:\/\/github.com\/apache\/arrow\/issues\/33448. Some discussion in a related issue, https:\/\/github.com\/apache\/arrow\/issues\/34905, lead to a proposal to add a toggle to turn this new routine on and off so I think that will do the trick for you.\r\n\r\nFeel free to subscribe to it for the time being, I see it's had some recent activity so hopefully a PR will be put up soon."],"labels":["Component: R","Type: usage"]},{"title":"[Python] Can't convert object-dtyped NumPy arrays containing np.datetime64 objects to Arrow","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```python\r\n>>> pa.array(np.array([np.datetime64('2024-01-01')]))  # works\r\n<pyarrow.lib.Date32Array object at 0x7f60596d2140>\r\n[\r\n  2024-01-01\r\n]\r\n>>> pa.array(np.array([np.timedelta64(1, 'ns')], dtype=object))  # works\r\n<pyarrow.lib.DurationArray object at 0x7f60596d2500>\r\n[\r\n  1\r\n]\r\n>>> pa.array(np.array([np.datetime64('2024-01-01')], dtype=object))  # fails\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow\/array.pxi\", line 340, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 86, in pyarrow.lib._ndarray_to_array\r\n  File \"pyarrow\/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118\r\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'datetime.date'\r\n```\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[Python][C++] Optimize ListView conversion to pandas\/numpy","body":"### Describe the enhancement requested\n\nThe original implementation in https:\/\/github.com\/apache\/arrow\/pull\/40482 uses a helper API to convert a ListView type to the equivalent List type before converting to pandas\/numpy. This creates an additional copy of the data that is not required. Let's add a custom conversion for ListView type that avoids the additional copy.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[Python][Docs] Max batch size for Dataset","body":"### Describe the enhancement requested\n\nI'm using `pyarrow.Dataset.to_table(batch_size)` and the maximum batch size seems to be hard-coded to 1048576. It'd be good to make it more flexible (in my case I'm interested in numbers like 100 million), however, if it's there for a reason, mentioning it in documentation would already be helpful\n\n### Component(s)\n\nDocumentation, Python","comments":["Can you link to where you're seeing that it's hard-coded and to that number? I see [pyarrow.Dataset.to_table](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.dataset.Dataset.html#pyarrow.dataset.Dataset.to_table). That said, the batches here are [RecordBatches](https:\/\/arrow.apache.org\/docs\/cpp\/tutorials\/basic_arrow.html#making-a-recordbatch) which have an upper limit on row count based on your column types due to size limits on Arrow Arrays.\r\n\r\nCan you give a bit more context as to why you originally became interested in increasing batch size?","@amoeba I've noticed this magic number appearing in multiple places within the source code, but I'm not sure which occurrence is the actual cause of the issue. When I set the batch size to less than 1048576, it is adhered to; however, any number above that is capped at 1048576.\r\n\r\nIn my use case, I'm dealing (locally, on a single big-ass machine) with a large number of huge parquet files as input, each containing up to a billion records, and these files need to be re-partitioned based on the values in certain columns. The number of partitions can be as many as 5,000. Currently, if the data is processed in batches of 1 million (due to the cap), this results in a very large number of files per partition, while each file remains tiny. Consequently, the overall performance drops to below acceptable levels.","_>> I've noticed this magic number appearing in multiple places within the source code_\r\n\r\nI see this number mostly in C++ code, so perhaps the ticket labels should be adjusted.","Can you share some code to show how you're approaching this? The ultimate thing you're trying to do is possible and part of the design of the Datasets module, see https:\/\/arrow.apache.org\/docs\/python\/dataset.html#writing-large-amounts-of-data. Also be aware of the considerations in https:\/\/arrow.apache.org\/docs\/python\/dataset.html#partitioning-performance-considerations.\r\n\r\nWhat you most likely don't want to be doing here is converting one or more Parquet files into an Arrow Table, then converting that Arrow Table to one or more Parquet files.\r\n\r\n","Here's how I do it:\r\n\r\n```\r\nimport pyarrow.dataset as ds\r\n\r\ndef yield_batches(dir_path: str, cols: list[str], batch_size: int = 50_000_000) -> Generator[pd.DataFrame, None, None]:\r\n    dataset = ds.dataset(dir_path, format=\"parquet\")\r\n    for b in dataset.to_batches(columns=cols, batch_size=batch_size):\r\n                yield b.to_pandas()\r\n```\r\n\r\nExperimenting with different batch sizes, you can see that the maximum attainable value is 1048576, even if a larger number is specified."],"labels":["Type: enhancement","Component: Python","Component: Documentation"]},{"title":"[Python] Relink `\/lib\/x86_64-linux-gnu\/libsystemd.so.0' with `\/lib\/x86_64-linux-gnu\/librt.so.1' for IFUNC symbol `clock_gettime'","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nI packaged a wheel installation package through the source package, the installation package is normal output, but after I installed, after using import pyarrow in Python, the following error occurred: \r\n\r\n```text\r\npython:  Relink `\/lib\/x86_64-linux-gnu\/libsystemd.so.0' with `\/lib\/x86_64-linux-gnu\/librt.so.1' for IFUNC symbol `clock_gettime'\r\npython:  Relink `\/lib\/x86_64-linux-gnu\/libudev.so.1' with `\/lib\/x86_64-linux-gnu\/librt.so.1' for IFUNC symbol `clock_gettime'.\r\n```\r\n\r\nOS version: Ubuntu18.04\r\nPython version: 3.7.3\r\narrow Version: 11.0\r\nPlease somebody show me this problem.\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Component: Python","Type: usage"]},{"title":"[Python][FS][Azure] Expose all the configuration options to python","body":"### Describe the enhancement requested\r\n\r\nChild of: https:\/\/github.com\/apache\/arrow\/issues\/39317\r\nblocked by https:\/\/github.com\/apache\/arrow\/issues\/38598 - probably best to wait for all the planned configuration options to be implemented on the C++ side.\r\n\r\nThe C++ file-system implementation has lots of configuration options, e.g. for all the different Azure authentication methods. https:\/\/github.com\/apache\/arrow\/issues\/39968 added the python bindings but was deliberately minimal in terms of the configuration options it exposed.  \r\n\r\nShould bear in mind consistency and code re-use with `AzureOptions::FromUri()` implemented by https:\/\/github.com\/apache\/arrow\/pull\/40325. See discussion at https:\/\/github.com\/apache\/arrow\/pull\/40021#discussion_r1525465089\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[CI] Default environment jobs to Ubuntu 22.04 instead of Ubuntu 20.04","body":"### Describe the enhancement requested\n\nUbuntu 24.04 is about to be available.\r\n\r\nI think it's about time we move our default .env to `UBUNTU=22.04`.\r\n\r\nThis task is to cover that work.\n\n### Component(s)\n\nContinuous Integration","comments":[],"labels":["Type: enhancement","Component: Continuous Integration"]},{"title":"[FlightRPC] Are large RecordBatches chunked into smaller size messages ?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nBased on the docs the RecordBatch in rust is split into approximately 2MB chunks to comply with the recommendation of gRPC of messages not exceeding 4 MB. Is this true across the board in any language that has support for ArrowFlight ?\n\n### Component(s)\n\nFlightRPC","comments":["For other implementations, you would probably have to do this yourself.","For C++\/Python, for a client writing to a server, you can set an optimistic limit with `write_size_limit_bytes`: https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.flight.FlightClient.html#pyarrow.flight.FlightClient\r\n\r\nIf a batch is too big, you get an exception you can catch, and you can retry the write after splitting it.","I\u2019m specifically interested in https:\/\/arrow.apache.org\/rust\/arrow_flight\/encode\/struct.FlightDataEncoderBuilder.html\r\nequivalent in cpp\/python - with_max_flight_data_size\r\n\r\nMy question is about a client reading from a server.","Then it's not available. The server needs to do it."],"labels":["Component: FlightRPC","Type: usage"]},{"title":"GH-39069: [C++][FS][Azure] Use the generic filesystem tests","body":"### Rationale for this change\r\n\r\nWe should provide common spec for all filesystem API.\r\n\r\n### What changes are included in this PR?\r\n\r\nEnable the generic filesystem tests.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\n* GitHub Issue: #39069","comments":[":warning: GitHub issue #39069 **has been automatically assigned in GitHub** to PR creator.","We need to fix some methods:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/actions\/runs\/8292331910\/job\/22693446087?pr=40567#step:6:6004\r\n\r\n```text\r\n[----------] 26 tests from TestAzureFileSystemGeneric\r\n...\r\n[ RUN      ] TestAzureFileSystemGeneric.CreateDir\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:244: Failure\r\nFailed\r\nExpected 'fs->CreateDir(\"AB\/def\/EF\/GH\", true )' to fail with IOError, but got OK\r\n[  FAILED  ] TestAzureFileSystemGeneric.CreateDir (107 ms)\r\n[ RUN      ] TestAzureFileSystemGeneric.DeleteDir\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:77: Failure\r\nExpected equality of these values:\r\n  paths\r\n    Which is: { \"AB\" }\r\n  expected_paths\r\n    Which is: { \"AB\", \"AB\/GH\" }\r\nErrors while running CTest\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:77: Failure\r\nExpected equality of these values:\r\n  paths\r\n    Which is: { \"AB\" }\r\n  expected_paths\r\n    Which is: { \"AB\", \"AB\/GH\" }\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:77: Failure\r\nExpected equality of these values:\r\n  paths\r\n    Which is: { \"AB\" }\r\n  expected_paths\r\n    Which is: { \"AB\", \"AB\/GH\" }\r\n[  FAILED  ] TestAzureFileSystemGeneric.DeleteDir (153 ms)\r\n[ RUN      ] TestAzureFileSystemGeneric.DeleteDirContents\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:307: Failure\r\nFailed\r\nExpected 'fs->DeleteDirContents(\"abc\", true)' to fail with IOError, but got OK\r\n[  FAILED  ] TestAzureFileSystemGeneric.DeleteDirContents (149 ms)\r\n...\r\n[ RUN      ] TestAzureFileSystemGeneric.MoveFile\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:396: Failure\r\nFailed\r\n'fs->Move(\"abc\", \"def\")' failed with NotImplemented: FileSystem::Move() is not implemented for Azure Storage accounts without Hierarchical Namespace support (see arrow\/issues\/40405).\r\n[  FAILED  ] TestAzureFileSystemGeneric.MoveFile (34 ms)\r\n...\r\n[ RUN      ] TestAzureFileSystemGeneric.CopyFile\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:570: Failure\r\nFailed\r\nExpected 'fs->CopyFile(\"AB\/abc\", \"def\/mno\")' to fail with IOError, but got OK\r\n[  FAILED  ] TestAzureFileSystemGeneric.CopyFile (164 ms)\r\n...\r\n[ RUN      ] TestAzureFileSystemGeneric.OpenOutputStream\r\n\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:922: Failure\r\nExpected equality of these values:\r\n  \"x-arrow\/filesystem-test\"\r\n  _actual\r\n    Which is: \"application\/octet-stream\"\r\n[  FAILED  ] TestAzureFileSystemGeneric.OpenOutputStream (71 ms)\r\n...\r\n[----------] 26 tests from TestAzureFileSystemGeneric (2670 ms total)\r\n```","I've changed the current implementation to pass the generic filesystem tests but it's still dirty..."],"labels":["Component: C++","awaiting changes"]},{"title":"[Go] Unable to JSON marshal float64 arrays which contain a NaN value","body":"`array.Float64.MarshallJSON()` returns the following error for the example code below:\r\n\r\n```\r\njson: unsupported value: NaN\r\n```\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"math\"\r\n\r\n\t\"github.com\/apache\/arrow\/go\/v16\/arrow\/array\"\r\n\t\"github.com\/apache\/arrow\/go\/v16\/arrow\/memory\"\r\n)\r\n\r\nfunc main() {\r\n\r\n\tbuilder := array.NewFloat64Builder(memory.DefaultAllocator)\r\n\tbuilder.Append(math.NaN())\r\n\r\n\tarray_ := builder.NewArray()\r\n\r\n\tbs, err := array_.MarshalJSON()\r\n\tif err != nil {\r\n\t\tfmt.Println(\"error:\", err)\r\n\t}\r\n\r\n\tfmt.Println(string(bs))\r\n}\r\n```\r\n\r\nConsider updating the marshalling code for Float64 and other floating point array types with something like:\r\n\r\n```\r\nfunc (a *array.Float64) MarshalJSON() ([]byte, error) {\r\n\tvals := make([]any, a.Len())\r\n\tfor i := 0; i < a.Len(); i++ {\r\n\t\tif !a.IsValid(i) {\r\n\t\t\tvals[i] = nil\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tf := a.Value(i)\r\n\t\tswitch {\r\n\t\tcase math.IsNaN(f):\r\n\t\t\tvals[i] = \"NaN\"\r\n\t\tcase math.IsInf(f, 1):\r\n\t\t\tvals[i] = \"+Inf\"\r\n\t\tcase math.IsInf(f, -1):\r\n\t\t\tvals[i] = \"-Inf\"\r\n\t\tdefault:\r\n\t\t\tvals[i] = f\r\n\t\t}\r\n\t}\r\n\r\n\treturn json.Marshal(vals)\r\n}\r\n```\r\n\r\n### Component(s)\r\n\r\nGo","comments":["Hmm, if we do this we'd probably need to also update the corresponding `UnmarshalJSON` code to properly handle NaN so that we're still symmetrical. I'll mark this as a \"good first issue\" for someone to pick up\r\n"],"labels":["Type: bug","Component: Go","good-first-issue"]},{"title":"[Python][JS] Examples of serializing arrow from python through ipywidgets to JS","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI do not know how to use ipywidgets traitlets.Bytes or even base64 encoding to generate an arrow table that will reify in JS.  \r\n\r\nlets start with Base64 \r\n\r\n```python\r\nimport pyarrow as pa\r\nimport base64\r\ndf = pd.DataFrame({'a':[10,50], 'b': ['paddy', 'margaret']})\r\n\r\ntable = pa.Table.from_pandas(df)\r\nbatch = table.to_batches()[0]\r\npabuffer = batch.serialize()\r\npabuffer\r\n\r\nb64_table = base64.b64encode(pabuffer.to_pybytes())\r\nprint(base_64_table)\r\n```\r\n\r\ngiven the following JS code, that string doesn't reify properly\r\n```typescript\r\nexport function base64ToBytes(base64:string) {\r\n    const binString = atob(base64);\r\n    \/\/@ts-ignore\r\n    return Uint8Array.from(binString, (m) => m.codePointAt(0));\r\n}\r\n\r\n    const b64bytes = base64ToBytes(base64table)\r\n    const t2 = tableFromIPC(b64bytes);\r\n```\r\nI get an error of `TypeError: this.schema is undefined`\r\n\r\nI can get an arrow table to reify when I do base64 in JS and use that string.\r\n\r\n\r\n\r\nI have made a sample repo to play with, benchmark, and document dataframe serialization.  https:\/\/github.com\/paddymul\/df_cereal\r\n\r\nI'm eager to collaborate and help with documentation for using arrow-js.  I think it solves a lot of serialization problems for performance and typing vs json.  It's a little hard to approach as is, because the docs are written for someone who understands arrow, vs someone who knows less about arrow, but wants to use it for serialization.\r\n\r\n\r\n\r\n\n\n### Component(s)\n\nJavaScript, Python","comments":["cc @kylebarron in case you have done things like this or might have pointers","https:\/\/github.com\/developmentseed\/lonboard serializes all array or table data via Arrow and Parquet. The Python side [defines traits to validate and store](https:\/\/github.com\/developmentseed\/lonboard\/blob\/8e97480d79790df1b35f14b758fe1635f0450b8b\/lonboard\/traits.py#L113) `Table`, `Array`, and `ChunkedArray` objects. Each of those has [custom serializers](https:\/\/github.com\/developmentseed\/lonboard\/blob\/8e97480d79790df1b35f14b758fe1635f0450b8b\/lonboard\/traits.py#L142) which [serialize to Parquet](https:\/\/github.com\/developmentseed\/lonboard\/blob\/8e97480d79790df1b35f14b758fe1635f0450b8b\/lonboard\/_serialization.py#L70-L72) (for my uses, a list of Parquet files rather than one Parquet file with many chunks). Then in JS it [parses those Parquet buffers to an Arrow JS table](https:\/\/github.com\/developmentseed\/lonboard\/blob\/8e97480d79790df1b35f14b758fe1635f0450b8b\/src\/model\/layer.ts#L48).","To be clear, you can also use Arrow IPC, but I use Parquet so that file sizes are smaller for when users are in remote Python environments."],"labels":["Component: Python","Component: JavaScript","Type: usage"]},{"title":"[Python] RunEndEncodedArray.from_arrays fails if run_ends are pyarrow.Array","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nIf `run_ends` argument in the `pa.RunEndEncodedArray.from_arrays` is `pyarrow.Array` object  the conversion errors with `TypeError`:\r\n\r\n```python\r\nIn [1]: import pyarrow as pa\r\n   ...: \r\n   ...: run_ends = pa.array([1, 3, 6], type=pa.int32())\r\n   ...: values = pa.array([1, 2, 3], type=pa.int64())\r\n   ...: ree_type = pa.run_end_encoded(pa.int32(), pa.int64())\r\n   ...: expected = pa.RunEndEncodedArray.from_arrays(run_ends, values, ree_type)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 6\r\n      4 values = pa.array([1, 2, 3], type=pa.int64())\r\n      5 ree_type = pa.run_end_encoded(pa.int32(), pa.int64())\r\n----> 6 expected = pa.RunEndEncodedArray.from_arrays(run_ends, values, ree_type)\r\n\r\nFile ~\/repos\/arrow\/python\/pyarrow\/array.pxi:4039, in pyarrow.lib.RunEndEncodedArray.from_arrays()\r\n   4037 \"\"\"\r\n   4038 logical_length = run_ends[-1] if len(run_ends) > 0 else 0\r\n-> 4039 return RunEndEncodedArray._from_arrays(type, True, logical_length,\r\n   4040                                        run_ends, values, 0)\r\n   4041 \r\n\r\nFile ~\/repos\/arrow\/python\/pyarrow\/array.pxi:3999, in pyarrow.lib.RunEndEncodedArray._from_arrays()\r\n   3997     shared_ptr[CRunEndEncodedArray] ree_array\r\n   3998 \r\n-> 3999 _logical_length = <int64_t>logical_length\r\n   4000 _logical_offset = <int64_t>logical_offset\r\n   4001 \r\n\r\nTypeError: an integer is required\r\n```\r\n\r\nThe issue seems to be that the `logical_length` variable is then a pyarrow scalar and it can't be converted to `int64_t` integer.\r\n\r\n```\r\nIn [3]: run_ends[-1]\r\nOut[3]: <pyarrow.Int32Scalar: 6>\r\n```\r\n\r\nIt works well for python lists:\r\n\r\n```python\r\nIn [6]: run_ends = [1, 3, 6]\r\n   ...: values = [1, 2, 3]\r\n   ...: pa.RunEndEncodedArray.from_arrays(run_ends, values, ree_type)\r\nOut[6]: \r\n<pyarrow.lib.RunEndEncodedArray object at 0x104ccd780>\r\n\r\n-- run_ends:\r\n  [\r\n    1,\r\n    3,\r\n    6\r\n  ]\r\n-- values:\r\n  [\r\n    1,\r\n    2,\r\n    3\r\n  ]\r\n```\r\n\r\nwhere `logical_length` is an integer:\r\n\r\n```\r\nIn [7]: run_ends[-1]\r\nOut[7]: 6\r\n```\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python","good-first-issue"]},{"title":"S3Filesystem always initiates multipart uploads, regardless of input size","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nRunning the following snippet shows that `open_output_stream()` initiates a multipart upload immediately, before anything is written.\r\n\r\nThis is quite unexpected: I would expect that the `buffer_size` argument would ensure that a multipart upload is not initiated until at least 1,000 bytes are written. The issue with the current behaviour is that writing a single byte results in three requests to s3: one to create the multipart upload, one to upload the 1-byte part, and one to finish the multipart upload.\r\n\r\nThis is very inefficient if you are writing a small file to S3, where a simple put object (without multipart uploading) would suffice. Using `background_writes=False` and `fs.copy_files(...)` with a local, \"known-sized\" small file also results in a multipart upload.\r\n\r\nWhile this behaviour keeps the implementation simple, it is surprising and I couldn't find [it described in the documentation anywhere](https:\/\/arrow.apache.org\/docs\/python\/filesystems.html).\r\n\r\n```python\r\nimport time\r\n\r\nfrom pyarrow import fs\r\n\r\nfs.initialize_s3(fs.S3LogLevel.Debug)\r\n\r\nsfs = fs.S3FileSystem()\r\nwith sfs.open_output_stream(\"a_bucket\/test\", buffer_size=1000):\r\n    time.sleep(10)\r\n```\r\n\r\n### Component(s)\r\n\r\nPython 3.10\r\n\r\nPlatform: MacOS and Linux\r\n\r\nVersion: 15.0.0","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[Python] Is it possible to enable logging with Python\/PyArrow ?","body":"### Describe the enhancement requested\n\nHi,\r\n\r\nwe've encountered quite a few times a situation, where more debug logs would help to understand performance discrepancies when reading parquet files. It turns out that various APIs use different default values for performance-relevant parameters. \r\nThus if there was a possibility to enable debug logs and e.g. see what parameters are used to create a `ParquetFileReader` it would help a lot in understanding these differences.\r\n\r\nWould you be open to contributions that:\r\n- Add the possibility of enabling logging with PyArrow (As far I can tell it is only possible with C++ at the moment)\r\n- Extend logging around `ParquetFileReader`\n\n### Component(s)\n\nParquet, Python","comments":["\ud83e\udd14What kind of logging would you like to have? A parquet file, if storing on object storage, the bottleneck might be reading from object store. And if living on disk, it might be decompress, decoding ... etc.","For now, I'm thinking about adding a code to log the content of `parquet::ReaderProperties` and `parquet::ArrowReaderProperties`. \r\nFor example, the value of `pre_buffer` parameter has high impact with high-latency file systems. But sometimes that value is not explicitly set and user relies on the implicit default value for it. If a user then uses different API then it might end up using different value for the `pre_buffer` parameter and be unaware of it. With the logs I'm thinking about it would be much easier to track these situations down.\r\n","Existing code has some \"tracing\", would that works?","> Existing code has some \"tracing\", would that works?\r\n\r\nThe existing tracing\/logging infrastructure is ok but:\r\n- I don't see the way to call `arrow::util::ArrowLog::StartArrowLog` from PyArrow\r\n- The traces I'm thinking about are not yet implemented (there is in fact very little tracing in the entire library)"],"labels":["Type: enhancement","Component: Parquet","Component: Python"]},{"title":"[CI] Spark integration tests are failing (\"org.apache.arrow.flatbuf.Message not found\")","body":"Failures from last night:\r\n\r\n- [test-conda-python-3.10-spark-v3.5.0](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8273491801\/job\/22637377160)\r\n- [test-conda-python-3.11-spark-master](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8273491617\/job\/22637376907)\r\n- [test-conda-python-3.8-spark-v3.5.0](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8273492251\/job\/22637415067)\r\n\r\n```\r\nError: ] \/spark\/sql\/core\/src\/main\/scala\/org\/apache\/spark\/sql\/execution\/arrow\/ArrowConverters.scala:26: object flatbuf is not a member of package org.apache.arrow\r\nError: ] \/spark\/sql\/core\/src\/main\/scala\/org\/apache\/spark\/sql\/execution\/arrow\/ArrowConverters.scala:456: Class org.apache.arrow.flatbuf.Message not found - continuing with a stub.\r\nError: [ERROR] two errors found\r\nError:  Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.8.0:compile (scala-compile-first) on project spark-sql_2.12: Execution scala-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.8.0:compile failed: org.apache.commons.exec.ExecuteException: Process exited with an error: 255 (Exit value: 255) -> [Help 1]\r\nError:  \r\nError:  To see the full stack trace of the errors, re-run Maven with the -e switch.\r\nError:  Re-run Maven using the -X switch to enable full debug logging.\r\nError:  \r\nError:  For more information about the errors and possible solutions, please read the following articles:\r\nError:  [Help 1] http:\/\/cwiki.apache.org\/confluence\/display\/MAVEN\/PluginExecutionException\r\nError:  \r\nError:  After correcting the problems, you can resume the build with the command\r\nError:    mvn <args> -rf :spark-sql_2.12\r\n1\r\nError: `docker-compose --file \/home\/runner\/work\/crossbow\/crossbow\/arrow\/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=16.0.0.dev283 conda-python-spark` exited with a non-zero exit code 1, see the process log above.\r\n```\r\n\r\ncc @BryanCutler @kiszk\r\n","comments":["Succeeded: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8225867077\r\nFailed: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8241600644\r\n\r\nBoth look similar until here.\r\nhttps:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8225867077\/job\/22491512891#step:6:18864\r\nhttps:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8241600644\/job\/22539165101#step:6:17463 ","Those are the commits that were introduced on the first failure (from the crossbow report) \r\nhttps:\/\/github.com\/apache\/arrow\/compare\/4de08748afa7dc2b7f36017ed608bfee3cab70e1...605f8a792c388afb2230b1f19e0f3e4df90d5abe\r\nI can't see anything that seems to be the cause but I am not familiar with those changes. @kou any idea if some of those commits might have introduced the issue?","Hmm, maven-shade-plugin https:\/\/github.com\/apache\/arrow\/commit\/8ee9679d401183220a4566681ca7ef9e887ba4d2 may be related...?\r\n\r\nIt seems that the arrow-format package can't be found by Spark.\r\n\r\n@kiszk Do you know when it's happen? ","https:\/\/github.com\/apache\/arrow\/pull\/40462#issuecomment-2013700840 has this problem.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/40461#issuecomment-2013702239 doesn't have this problem.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/40462 must be related."],"labels":["Component: Continuous Integration"]},{"title":"[C++][Parquet] Add support for writing bloom filter to a Parquet file","body":"Hi,\r\n\r\nIt appears to me that the C++ implementation only allows creating, serialising and deserialising a Bloom filter to and from Parquet and reading a bloom filter from a Parquet file, but not writing the bloom filter to Parquet file. As far as I can tell the Rust implementation allows writing a bloom filter to a Parquet file.\r\n\r\nThis issue aims to add support for writing the bloom filter to a Parquet file. Since readers are likely to try filtering out row groups based on the bloom filter, I'd propose storing the bloom filter data before the page indices similar to the first suggestion in the bloom filter proposal.\r\n\r\nLooking forward to hearing opinions on this!\r\n\r\nThank you!\r\n\r\n### Component(s)\r\n\r\nC++, Parquet","comments":["Aha previously I'm working on this, but forgot to move it forward, so sorry for that. I'll working on it then: https:\/\/github.com\/apache\/arrow\/pull\/37400","And besides, this is \"pure parquet\" part. Do you need these flags in Dataset writer and reader?","Hey @mapleFU  thanks a lot for looking at this again! I'd only need the pure parquet part for the moment. \r\n\r\nI'll try reviewing the PR soon, but I have never contributed to the project before so it might take me a bit of time!"],"labels":["Type: enhancement","Component: Parquet","Component: C++"]},{"title":"[R][Docs] Add a non-technical introduction to the functioning of arrow","body":"### Describe the enhancement requested\r\n\r\n__TL;DR: I suggest to complement the `arrow` documentation with an introduction to the functioning of `arrow`, specifically designed for R users with limited computer science background. I wrote such an [introduction in French](https:\/\/www.book.utilitr.org\/03_fiches_thematiques\/fiche_arrow) ([link to automated English translation](https:\/\/www-book-utilitr-org.translate.goog\/03_fiches_thematiques\/fiche_arrow?_x_tr_sl=fr&_x_tr_tl=en&_x_tr_hl=fr&_x_tr_pto=wapp)); it's intentionally written in plain and sometimes imprecise language, avoiding most technical terms that usually puzzle newcomers. I'm ready to translate it to English, provided that the arrow team agrees with adding it to the documentation.__\r\n\r\n## Context\r\n\r\nMy employer (a major European statistical organization) decided to move away from proprietary statistical softwares (mostly SAS) to embrace open source alternatives (mostly R and Python), and decided recently that data should be stored as Parquet files, making `arrow\/dplyr` the standard approach to data processing when working with R.\r\n\r\nI noticed repeatedly that `R` users who are new to `arrow` do not use `arrow` properly, because of an imperfect understanding of the way `arrow` works. For instance, they typically use `collect()` on large Parquet files (resulting in RAM saturation), because they do not understand the difference between `compute()` and `collect()`, or they write extremely long arrow\/dplyr queries, resulting in session crashes.\r\n\r\nImportantly, most of my colleagues have a strong background in statistics and data processing and an intermediate level in R, but a limited background in computer science.\r\n\r\n## The arrow documentation\r\n\r\nIn my experience, this imperfect understanding comes from two causes:\r\n\r\n- Some parts of the documentation are quite difficult to understand for newcomers because of unknown technical terms. For instance, my colleagues are often unable to understand the [following paragraph from the arrow documentation](https:\/\/arrow.apache.org\/docs\/r\/) because they do not really know what the words interface, API or backend mean: \"The arrow R package exposes an interface to the Arrow C++ library, enabling access to many of its features in R. It provides low-level access to the Arrow C++ library API and higher-level access through a [dplyr](https:\/\/dplyr.tidyverse.org\/) backend and familiar R functions.\" As for myself, I struggled for months before understanding these notions.\r\n\r\n- The [Apache Arrow R Cookbook](https:\/\/arrow.apache.org\/cookbook\/r\/index.html) does not really give a general overview of the functioning of `arrow`. For instance, the differences between `compute()` and `collect()` and between the dplyr and acero execution engines, or the limitations of lazy evaluation are not really explained, although they are essential when working on large datasets.\r\n\r\nDisclaimer: I want to stress that the point made here is not a criticism of the Arrow documentation; I just want to point that this documentation may not be well suited for newcomers with a limited background in computer science.\r\n\r\n## My suggestion\r\n\r\nI noticed that my colleagues were able to use `arrow` properly, once they were explained the functioning of `arrow` in plain, non-technical terms. That's why I wrote a [long and gentle introduction to `arrow` in French](https:\/\/www.book.utilitr.org\/03_fiches_thematiques\/fiche_arrow) ([link to automated English translation](https:\/\/www-book-utilitr-org.translate.goog\/03_fiches_thematiques\/fiche_arrow?_x_tr_sl=fr&_x_tr_tl=en&_x_tr_hl=fr&_x_tr_pto=wapp)), specifically designed for R users with limited computer science background. This introduction explains the functioning of `arrow` in plain language, avoiding most technical terms that usually puzzle newcomers. I explain important notions (for instance: lazy evaluation, execution engine) in intuitive and somewhat imprecise terms, so that newcomers get a first rough understanding of how things work with `arrow`, before switching to the current documentation.\r\n\r\nI now think that this introduction could be a valuable contribution to the `arrow` documentation. I'm ready to translate\/adapt it in English, for instance as a vignette for the `R` package, or as an overview chapter in the Apache Arrow R Cookbook. However, I'm very aware that this introduction could be considered as subpar, not precise enough or even misleading. That's why I would like to know if the arrow team is ready to consider this suggestion.\r\n\r\n\r\n### Component(s)\r\n\r\nDocumentation, R","comments":["Hi @oliviermeslin, thank you for filing this.\r\n\r\nWhat you've learned from helping others learn how to be effective with the arrow R package is very welcome here. I think the structure you use in your guide is very useful and the content makes the most sense to me as a new vignette. There's [existing work](https:\/\/github.com\/apache\/arrow\/issues\/35875) an making the top-level readme more newcomer-friendly so maybe some of what you've already written could help with that work too.\r\n\r\nI know @thisisnic and @stephhazlitt have spent a lot of time teaching arrow to others so their insight would be useful too.\r\n","I love this suggestion @oliviermeslin!  I think that you are much better placed than the experienced arrow devs (who are so exposed to this terminology that we no longer realise it's confusing to newcomers), and I'd be delighted to review any PRs you want to make to the arrow cookbook or vignettes.\r\n\r\nI think the best way to proceed would be for you to go ahead with any pull requests you'd like to make, and we can address any concerns about precision about language in the discussion on the pull request (seeing as most PRs have a bit of back and forth before they're merged anyway!).\r\n\r\nThere is a PR open to update the README, so it might be better for this to have merged first so we can iterate on the improvements that have been made there so far: https:\/\/github.com\/apache\/arrow\/pull\/40148\/"],"labels":["Type: enhancement","Component: R","Component: Documentation"]},{"title":"[Python] AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 6, Couldn't resolve host name","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nHi there!\r\n\r\nWe are using the PyArrow library to read files from an S3 bucket, and we're encountering an intermittent error:\r\n\r\n`OSError: When reading information for key '<REDACTED>' in bucket '<REDACTED>': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 6, Couldn't resolve host name`\r\n\r\nPlease note that this error doesn't occur consistently, and the S3 bucket path is valid.\r\n\r\nThe reference code we're using is as follows:\r\n```\r\nimport pyarrow as pa\r\nimport pyarrow.json as pj\r\n\r\nuri = \"s3:\/\/my-bucket\/my-prefix\/foo.json\"\r\n\r\nfs, path = pa.fs.FileSystem.from_uri(uri)  \r\n\r\nwith fs.open_input_file(path) as f:\r\n       tbl = pj.read_json(f)\r\n```\r\n\r\n\r\n\r\nError Details:\r\n```\r\n2024-03-12T13:06:05.616-07:00    [7]: with fs.open_input_file(path) as f:\r\n\r\n2024-03-12T13:06:05.616-07:00    [7]: File \"pyarrow\/_fs.pyx\", line 780, in pyarrow._fs.FileSystem.open_input_file\r\n\r\n2024-03-12T13:06:05.616-07:00    [7]: File \"pyarrow\/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n\r\n2024-03-12T13:06:05.616-07:00    [7]: File \"pyarrow\/error.pxi\", line 91, in pyarrow.lib.check_status\r\n\r\n2024-03-12T13:06:05.616-07:00    [7]:OSError: When reading information for key '<REDACTED>' in bucket '<REDACTED>': AWS Error NETWORK_CONNECTION during HeadObject operation: curlCode: 6, Couldn't resolve host name\r\n\r\n```\r\n\r\nCould you please provide any suggestions on how to handle such intermittent network connectivity errors while reading from S3?\n\n### Component(s)\n\nParquet, Python","comments":["Are you using AWS or do you set `endpoint_override`? \"Couldn't resolve host name\" probably means you're having issues with DNS resolution of the S3 server(s) hostnames, which is quite unexpected with AWS...","@pitrou  Yes, we are using AWS, and not explicitly overriding `endpoint_override `\r\n\r\n>you're having issues with DNS resolution of the S3 server(s) hostnames, which is quite unexpected with AWS...\r\n\r\n\r\nWe've only encountered this problem once among our numerous runs, so it's not a frequent occurrence. Currently, our approach to dealing with this issue is by increasing the default number of S3 retry attempts from 3 to a higher value. \r\n\r\n```\r\nif isinstance(fs, S3FileSystem):\r\n    fs = pa.fs.S3FileSystem(\r\n        region=fs.region, retry_strategy=AwsStandardS3RetryStrategy(max_attempts=6)\r\n    )\r\n```\r\n\r\nHowever, we're uncertain if this is the most effective approach.\r\n\r\nDo you have any recommendations on how we can better handle this on the client side?","Sorry, I don't have any recommandation. If increasing `max_attempts` works, then it seems ok to me."],"labels":["Type: bug","Component: Parquet","Component: Python"]},{"title":"[Python] pa.Array.from_pandas converts empty timestamp[s][pyarrow, UTC] pandas Series to ChunkedArray, not TimestampArray","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\n```python\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))))  # correct\r\n<pyarrow.lib.TimestampArray object at 0x7fb665f77fa0>\r\n[]\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))).dt.tz_localize('UTC'))  # incorrect\r\n<pyarrow.lib.ChunkedArray object at 0x7fb665fd8680>\r\n[\r\n\r\n]\r\n```\r\n\r\nSame issue with `pa.array()` instead of `pa.Array.from_pandas()`.\r\n\r\n### Component(s)\r\n\r\nPython","comments":["@Wainberg this is actually controlled by pandas itself, so can you report this to https:\/\/github.com\/pandas-dev\/pandas\/issues\/ ?","Ping-ponging back here: Looks like `assume_timezone` on empty `pa.ChunkedArray[pa.TImestampArray]` drops the empty `pa.TimestampArray`\r\n\r\n```python\r\nIn [3]: import pyarrow as pa\r\n\r\nIn [4]: arr = pa.chunked_array([pa.array([], type=pa.timestamp(\"ns\"))])\r\n\r\nIn [5]: arr\r\nOut[5]: \r\n<pyarrow.lib.ChunkedArray object at 0x10791b600>\r\n[\r\n  []\r\n]\r\n\r\nIn [6]: import pyarrow.compute as pc\r\n\r\nIn [7]: pc.assume_timezone(arr, \"UTC\")\r\nOut[7]: \r\n<pyarrow.lib.ChunkedArray object at 0x10789bc40>\r\n[\r\n\r\n]\r\n\r\nIn [8]: pa.__version__\r\nOut[8]: '15.0.1'\r\n\r\nIn [9]: pc.assume_timezone(arr, \"UTC\").chunks\r\nOut[9]: []\r\n\r\nIn [10]: arr.chunks\r\nOut[10]: \r\n[<pyarrow.lib.TimestampArray object at 0x10797ee00>\r\n []]\r\n```","To be honest I would also expect it to preserve the chunking structure, but it seems this something we don't do in general (I tried several other scalar kernels on a chunked array with one empty chunk, and they all return a chunked array with no chunks), so that is not specific to `assume_timezone`","But indeed it's because pandas returns a ChunkedArray with 0 chunks to pyarrow, that we don't unpack that to the single chunk Array, because of:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/54ff758a4570d9eeed9f6195dccdc21dcfe8d6d7\/python\/pyarrow\/array.pxi#L116-L118\r\n\r\nin the code handling objects with an `__arrow_array__` method.\r\n"],"labels":["Type: bug","Component: Python"]},{"title":"MINOR: [Java] Bump org.apache.maven.plugins:maven-surefire-plugin from 3.2.3 to 3.2.5 in \/java","body":"Bumps [org.apache.maven.plugins:maven-surefire-plugin](https:\/\/github.com\/apache\/maven-surefire) from 3.2.3 to 3.2.5.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/apache\/maven-surefire\/releases\">org.apache.maven.plugins:maven-surefire-plugin's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>3.2.5<\/h2>\n<p><a href=\"https:\/\/issues.apache.org\/jira\/secure\/ReleaseNote.jspa?projectId=12317927&amp;version=12354100\">JIRA link<\/a><\/p>\n<h1>Release Notes - Maven Surefire - Version 3.2.5<\/h1>\n<!-- raw HTML omitted -->\n<!-- raw HTML omitted -->\n<hr \/>\n<h2>What's Changed<\/h2>\n<ul>\n<li>Bump org.htmlunit:htmlunit from 3.8.0 to 3.9.0 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/695\">apache\/maven-surefire#695<\/a><\/li>\n<li>Bump org.fusesource.jansi:jansi from 2.4.0 to 2.4.1 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/684\">apache\/maven-surefire#684<\/a><\/li>\n<li>Bump doxiaVersion from 1.11.1 to 1.12.0 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/609\">apache\/maven-surefire#609<\/a><\/li>\n<li>[SUREFIRE-2221] Document minimum supported Java version for Toolchains by <a href=\"https:\/\/github.com\/sbernard31\"><code>@\u200bsbernard31<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/701\">apache\/maven-surefire#701<\/a><\/li>\n<li>[SUREFIRE-2224] StatelessXmlReporter#getTestProblems() does not properly reflect report schema structure  by <a href=\"https:\/\/github.com\/michael-o\"><code>@\u200bmichael-o<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/702\">apache\/maven-surefire#702<\/a><\/li>\n<li>[SUREFIRE-2223] Surefire evaluates parameter jvm before skip by <a href=\"https:\/\/github.com\/michael-o\"><code>@\u200bmichael-o<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/703\">apache\/maven-surefire#703<\/a><\/li>\n<li>Use uppercase convention for enum member names by <a href=\"https:\/\/github.com\/michael-o\"><code>@\u200bmichael-o<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/704\">apache\/maven-surefire#704<\/a><\/li>\n<li>[SUREFIRE-2225] Surefire ITs fail when project directory contains space by <a href=\"https:\/\/github.com\/michael-o\"><code>@\u200bmichael-o<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/705\">apache\/maven-surefire#705<\/a><\/li>\n<li>Run CI tests also with Java 21 by <a href=\"https:\/\/github.com\/slachiewicz\"><code>@\u200bslachiewicz<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/707\">apache\/maven-surefire#707<\/a><\/li>\n<li>Bump org.apache.maven.wagon:wagon-http-lightweight from 3.5.1 to 3.5.3 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/699\">apache\/maven-surefire#699<\/a><\/li>\n<li>Bump org.htmlunit:htmlunit from 3.8.0 to 3.9.0 in \/maven-failsafe-plugin\/src\/it\/jetty-war-test-failing by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/694\">apache\/maven-surefire#694<\/a><\/li>\n<li>Bump org.htmlunit:htmlunit from 3.8.0 to 3.9.0 in \/maven-failsafe-plugin\/src\/it\/jetty-war-test-passing by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/693\">apache\/maven-surefire#693<\/a><\/li>\n<li>Bump commons-io:commons-io from 2.15.0 to 2.15.1 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/712\">apache\/maven-surefire#712<\/a><\/li>\n<li>Bump net.java.dev.javacc:javacc from 7.0.12 to 7.0.13 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/711\">apache\/maven-surefire#711<\/a><\/li>\n<li>Bump org.apache.maven.plugins:maven-docck-plugin from 1.1 to 1.2 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/713\">apache\/maven-surefire#713<\/a><\/li>\n<li>[SUREFIRE-2231] JaCoCo 0.8.11 fails with old TestNG releases on Java 17+ by <a href=\"https:\/\/github.com\/michael-o\"><code>@\u200bmichael-o<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/710\">apache\/maven-surefire#710<\/a><\/li>\n<li>Bump org.assertj:assertj-core from 3.24.2 to 3.25.1 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/714\">apache\/maven-surefire#714<\/a><\/li>\n<li>Bump org.codehaus.plexus:plexus-component-metadata from 2.1.1 to 2.2.0 by <a href=\"https:\/\/github.com\/dependabot\"><code>@\u200bdependabot<\/code><\/a> in <a href=\"https:\/\/redirect.github.com\/apache\/maven-surefire\/pull\/715\">apache\/maven-surefire#715<\/a><\/li>\n<\/ul>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/4b3a2719d80613f2ed304fc34144fed81c3043cd\"><code>4b3a271<\/code><\/a> [maven-release-plugin] prepare release surefire-3.2.5<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/eb3f1d946fbb9d1dbaba2fc7113f408f15a60f62\"><code>eb3f1d9<\/code><\/a> Bump org.codehaus.plexus:plexus-component-metadata from 2.1.1 to 2.2.0<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/430c406756df4e6bfad462426544d71a0d5e5867\"><code>430c406<\/code><\/a> Bump org.assertj:assertj-core from 3.24.2 to 3.25.1<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/2d92f2d422f07e75c188312cd2371127508a8e10\"><code>2d92f2d<\/code><\/a> [SUREFIRE-2231] JaCoCo 0.8.11 fails with old TestNG releases on Java 17+<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/3290740be8d81351331d5cd320e56346713ed2c2\"><code>3290740<\/code><\/a> Bump org.apache.maven.plugins:maven-docck-plugin from 1.1 to 1.2<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/25a9776c0e1d0c445a1ef5dbcb7ee27483bb029d\"><code>25a9776<\/code><\/a> Bump net.java.dev.javacc:javacc from 7.0.12 to 7.0.13<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/7752f7e62bf6710616e231c9a0cf6cd7e574416f\"><code>7752f7e<\/code><\/a> Bump commons-io:commons-io from 2.15.0 to 2.15.1<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/8874add5bb1b32e65eb5022f02d7bb01add9a3a8\"><code>8874add<\/code><\/a> Revert &quot;Bump jacocoVersion from 0.8.8 to 0.8.11&quot;<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/c0f775569b4ddd603a1c5c96e16abc78aa794173\"><code>c0f7755<\/code><\/a> Fix formatting<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-surefire\/commit\/e5f45452728fe78753e713b15ce4743634db01a2\"><code>e5f4545<\/code><\/a> Bump jacocoVersion from 0.8.8 to 0.8.11<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/apache\/maven-surefire\/compare\/surefire-3.2.3...surefire-3.2.5\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=org.apache.maven.plugins:maven-surefire-plugin&package-manager=maven&previous-version=3.2.3&new-version=3.2.5)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":["@github-actions crossbow submit -g java","Revision: cdfa84d373d73e8f3fbc840ef16bcf6aa353704e\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-90e994af5b](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-90e994af5b)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729389\/job\/22640554201)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729360\/job\/22640551600)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729539\/job\/22640551723)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729495\/job\/22640551868)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729503\/job\/22640551971)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-90e994af5b-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274729561\/job\/22640552021)|","This is failing because of: https:\/\/github.com\/apache\/arrow\/issues\/40568\r\nProbably needs to wait until https:\/\/github.com\/apache\/arrow\/issues\/40568 fixed","@dependabot rebase","Sorry, only users with push access can use that command.","@kou we need help to rebase this PR and re-run the failing docs CI to make sure it works. Also after rebasing better to rerun the Java CIs. ","@dependabot\u00a0rebase","@github-actions crossbow submit -g java","Revision: 3e0145f5a89a45226578576b3f392724809facb3\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-cbe5b41a2d](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-cbe5b41a2d)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119510\/job\/23004818897)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119509\/job\/23004818612)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119508\/job\/23004818622)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119545\/job\/23004818750)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119522\/job\/23004818641)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cbe5b41a2d-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399119579\/job\/23004818954)|"],"labels":["Component: Java","dependencies","java","awaiting review"]},{"title":"MINOR: [Java] Bump org.apache.maven.plugin-tools:maven-plugin-annotations from 3.6.0 to 3.11.0 in \/java","body":"Bumps [org.apache.maven.plugin-tools:maven-plugin-annotations](https:\/\/github.com\/apache\/maven-plugin-tools) from 3.6.0 to 3.11.0.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/releases\">org.apache.maven.plugin-tools:maven-plugin-annotations's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>3.11.0<\/h2>\n<h2><a href=\"https:\/\/issues.apache.org\/jira\/secure\/ReleaseNote.jspa?projectId=12317820&amp;version=12353824\">Release Notes - Maven Plugin Tools - Version 3.11.0<\/a><\/h2>\n<h2>Bug<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-496\">MPLUGIN-496<\/a>] - Translation for keys report.plugin.goal.yes,no are missing<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-499\">MPLUGIN-499<\/a>] - Deprecate descriptions are missing in description table<\/li>\n<\/ul>\n<h2>Improvement<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-450\">MPLUGIN-450<\/a>] - Make goal prefix mandatory by default<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-474\">MPLUGIN-474<\/a>] - Improve descriptor docs for requiredJavaVersion<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-492\">MPLUGIN-492<\/a>] - Documentation for plugins in general: Goals comprises more than that<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-495\">MPLUGIN-495<\/a>] - WARNINGs based on usage of <code>@Component<\/code> for MavenSession\/MavenProject instead of <code>@Parameter<\/code><\/li>\n<\/ul>\n<h2>Task<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-493\">MPLUGIN-493<\/a>] - Consistently evaluate skip parameter in MavenReport#canGenerateReport()<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-498\">MPLUGIN-498<\/a>] - Move section rendering to separate methods<\/li>\n<\/ul>\n<h2>Dependency upgrade<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-494\">MPLUGIN-494<\/a>] - Upgrade to Parent 41<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-497\">MPLUGIN-497<\/a>] - Upgrade components<\/li>\n<\/ul>\n<h2>3.10.2<\/h2>\n<h2><a href=\"https:\/\/issues.apache.org\/jira\/secure\/ReleaseNote.jspa?projectId=12317820&amp;version=12353719\">Release Notes - Maven Plugin Tools - Version 3.10.2<\/a><\/h2>\n<h2>Bug<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-484\">MPLUGIN-484<\/a>] - Downgrade plexus-xml to 3.0.0<\/li>\n<\/ul>\n<h2>Dependency upgrade<\/h2>\n<ul>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-485\">MPLUGIN-485<\/a>] - Upgrade Parent to 40<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-487\">MPLUGIN-487<\/a>] - Bump org.codehaus.plexus:plexus-java from 1.1.2 to 1.2.0<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-488\">MPLUGIN-488<\/a>] - Bump asmVersion from 9.5 to 9.6<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-489\">MPLUGIN-489<\/a>] - Bump antVersion from 1.10.13 to 1.10.14<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-490\">MPLUGIN-490<\/a>] - Bump org.jsoup:jsoup from 1.16.1 to 1.16.2<\/li>\n<li>[<a href=\"https:\/\/issues.apache.org\/jira\/browse\/MPLUGIN-491\">MPLUGIN-491<\/a>] - Bump org.codehaus.plexus:plexus-testing from 1.1.0 to 1.2.0<\/li>\n<\/ul>\n<h2>3.10.1<\/h2>\n<!-- raw HTML omitted -->\n<\/blockquote>\n<p>... (truncated)<\/p>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/4178d33ea1121a73114caa94983b0e4c425f3b2d\"><code>4178d33<\/code><\/a> [maven-release-plugin] prepare release maven-plugin-tools-3.11.0<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/25d920f53e12ef20a1d01bf9aae3a4c1ce738964\"><code>25d920f<\/code><\/a> [MNG-5695] document Maven 3.2.5+ scoped components usage<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/641849004597c74304b9d7e06379190130bdaf3e\"><code>6418490<\/code><\/a> [MPLUGIN-495] WARNINGs based on usage of <a href=\"https:\/\/github.com\/Component\"><code>@\u200bComponent<\/code><\/a> for MavenSession\/MavenPro...<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/8b93d125d212c1cd1bc5d5d682604395408aabd4\"><code>8b93d12<\/code><\/a> Bump org.jsoup:jsoup from 1.17.1 to 1.17.2<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/f4973acb0326d222ce5fd23eabbb82cbd7cddef6\"><code>f4973ac<\/code><\/a> Bump org.assertj:assertj-core from 3.24.2 to 3.25.1<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/7dd3a259585b50f3d450d68e480eb3da8d19e70b\"><code>7dd3a25<\/code><\/a> [MPLUGIN-499] Add deprecate description in parameters table (<a href=\"https:\/\/redirect.github.com\/apache\/maven-plugin-tools\/issues\/250\">#250<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/9bb13f0427d8795d2c47c5068fc4c8ba552892e2\"><code>9bb13f0<\/code><\/a> [MPLUGIN-492] Documentation for plugins in general: Goals comprises more than...<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/fc412185093fcb24430191c4697e3217f822a967\"><code>fc41218<\/code><\/a> [MPLUGIN-498] Move section rendering to separate methods<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/ed4774bcd8b8d2d1f7ff1196cf7644054cb3ae14\"><code>ed4774b<\/code><\/a> [MPLUGIN-450] Require goalPrefix to be valid (<a href=\"https:\/\/redirect.github.com\/apache\/maven-plugin-tools\/issues\/240\">#240<\/a>)<\/li>\n<li><a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/commit\/331cf42ba758c79ad3c4fca0464c8cfee8255e41\"><code>331cf42<\/code><\/a> [MPLUGIN-497] Upgrade components<\/li>\n<li>Additional commits viewable in <a href=\"https:\/\/github.com\/apache\/maven-plugin-tools\/compare\/maven-plugin-tools-3.6.0...maven-plugin-tools-3.11.0\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=org.apache.maven.plugin-tools:maven-plugin-annotations&package-manager=maven&previous-version=3.6.0&new-version=3.11.0)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n\n\n<\/details>","comments":["@github-actions crossbow submit -g java","Revision: f33ca269b8837b61296597ff015b8d1bcef2516c\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-0f4b793a4a](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-0f4b793a4a)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728833\/job\/22640552275)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728936\/job\/22640551100)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728773\/job\/22640539515)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728759\/job\/22640539444)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728943\/job\/22640551221)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0f4b793a4a-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274728807\/job\/22640542588)|","This is failing because of: https:\/\/github.com\/apache\/arrow\/issues\/40568\r\nProbably needs to wait until https:\/\/github.com\/apache\/arrow\/issues\/40568 fixed and a docs CI is failing, needs further evaluation. ","@dependabot rebase","Sorry, only users with push access can use that command.","@github-actions crossbow submit -g java","@kou \r\nNeeds to rebase this and could we please re-run the failing CI `Docs \/ AMD64 Ubuntu 22.04 Complete Documentation` to make sure it is not a random failure? ","Revision: f33ca269b8837b61296597ff015b8d1bcef2516c\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-0183ee2bc7](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-0183ee2bc7)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870263\/job\/23001904548)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870231\/job\/23001903474)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870194\/job\/23001903502)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870275\/job\/23001903713)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870233\/job\/23001903506)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0183ee2bc7-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8397870282\/job\/23001903765)|","@dependabot rebase","@github-actions crossbow submit -g java","Revision: 0b47c297212724ce4ace2d29d5be25aff6310549\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-a72517dacc](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-a72517dacc)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121893\/job\/23004825392)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121855\/job\/23004825064)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121981\/job\/23004825304)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121883\/job\/23004825124)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121851\/job\/23004825038)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-a72517dacc-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8399121861\/job\/23004825112)|"],"labels":["Component: Java","dependencies","java","awaiting review"]},{"title":"[C++] Update vendored FlatBuffers to 24","body":"### Describe the enhancement requested\r\n\r\nSee https:\/\/github.com\/apache\/arrow\/blob\/main\/cpp\/thirdparty\/flatbuffers\/README.md how to do this.\r\n\r\n### Component(s)\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40018: [CI][Archery] Archery linking should also check for undefined symbols","body":"### Rationale for this change\r\n\r\nThis PR includes a check to find out if there are undefined symbols associated with the libraries. \r\n\r\n### What changes are included in this PR?\r\n\r\nIncludes a util functions which uses `nm` and `ldconfig` tools to find out libraries and their symbols and determine\r\nif there are undefined symbols excluding the symbols in the allowed dependencies. \r\n\r\n### Are these changes tested?\r\n\r\nLocally tested with a faulty shared library, but this needs validation on versions as there is a version mismatch in the local libraries. \r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\n* GitHub Issue: #40018","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","@github-actions crossbow submit java-jars","Revision: 090275e8918144d1270f5ad8697bd744d1119be8\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-7ff12b2218](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-7ff12b2218)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ff12b2218-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8273573283\/job\/22637574633)|",":warning: GitHub issue #40018 **has been automatically assigned in GitHub** to PR creator.","@github-actions crossbow submit java-jars","Revision: 090275e8918144d1270f5ad8697bd744d1119be8\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-e8a43da78d](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-e8a43da78d)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-e8a43da78d-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274012442\/job\/22638708916)|","seems like `nm` version issue: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274012442\/job\/22638708916#step:8:4124\r\nI am looking into this. ","@github-actions crossbow submit java-jars","Revision: 803932a4063ffb41ee1031a5c53349035d5c3609\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-cf41726ea3](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-cf41726ea3)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-cf41726ea3-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8274794485\/job\/22640720210)|","@github-actions crossbow submit java-jars","Revision: b9fb4f27c7ff8c2afd15b8305bdb55b19e41436b\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-2164940a0a](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-2164940a0a)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-2164940a0a-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8277851844\/job\/22649007450)|","@github-actions crossbow submit java-jars","Revision: 407c5b89e4633e8e222692438d224b9cef3731bf\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-56fd17bf3d](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-56fd17bf3d)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-56fd17bf3d-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8277970726\/job\/22649382272)|","@github-actions crossbow submit java-jars","Revision: b3a0f5da1cef2559ab6bb8d7784f78090105c197\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-06d495c097](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-06d495c097)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-06d495c097-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8278345898\/job\/22650494820)|","Locally I have the following `nm`\r\n\r\n```bash\r\nnm --version\r\nGNU nm (GNU Binutils) 2.40\r\nCopyright (C) 2023 Free Software Foundation, Inc.\r\nThis program is free software; you may redistribute it under the terms of\r\nthe GNU General Public License version 3 or (at your option) any later version.\r\nThis program has absolutely no warranty.\r\n```\r\n\r\nbut in the CI we have \r\n\r\n```bash\r\nnm: just-symbols: invalid output format\r\nGNU nm version 2.35-5.el7.4\r\nCopyright (C) 2020 Free Software Foundation, Inc.\r\nThis program is free software; you may redistribute it under the terms of\r\nthe GNU General Public License version 3 or (at your option) any later version.\r\nThis program has absolutely no warranty.\r\n```\r\nWould it be possible to upgrade the `nm` version in the CI containers? \r\n\r\ncc @raulcd ","If we run `archery linking check-dependencies` in `package-jars` job not `build-cpp-ubuntu`, we can use newer `nm`.\r\nBut we can't change `nm` version for `java-jni-manylinux-2014`. We need to use CentOS 7 based image for `java-jni-manylinux-2014`.\r\nIf we use `manylinux_2_28` not `manylinux-2014`, we can use newer `nm`. (Can we drop support for CentOS 7?)","I see, it seems a bit complicated approach to update nm. Then should we target on more work on Python rather than nm update? ","Yes. Could you try the approach?","@kou I will try this. ","@github-actions crossbow submit java-jars","@github-actions crossbow submit -g java","Revision: ccb5841c7a84d7d9b8ffdec42cfbd909f8bf1bc1\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-fbfd5d9768](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-fbfd5d9768)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-fbfd5d9768-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352021877\/job\/22861426527)|","Revision: ccb5841c7a84d7d9b8ffdec42cfbd909f8bf1bc1\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-92964d88ab](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-92964d88ab)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024126\/job\/22861433034)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024060\/job\/22861432179)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024068\/job\/22861432261)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024071\/job\/22861432263)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024117\/job\/22861432418)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-92964d88ab-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352024118\/job\/22861432400)|","@kou, I just checked with the https:\/\/repo1.maven.org\/maven2\/org\/apache\/arrow\/arrow-dataset\/15.0.2\/arrow-dataset-15.0.2.jar\r\n\r\n```bash\r\nError: Undefined symbols found in \/home\/vibhatha\/Documents\/Work\/Apache_Arrow\/issues\/gh-40018\/arrow-dataset-15.0.2\/x86_64\/libarrow_dataset_jni.so:\r\nEVP_MD_CTX_create\r\nEVP_MD_CTX_destroy\r\nHMAC_CTX_cleanup\r\nHMAC_CTX_init\r\n```\r\nThis PR fixes the protobuf issue, but it seems like we still do have openssl related stuff here, which is not in the allowed list right? ","Also looking at the CI failures\r\n\r\n### Build C++ libraries Ubuntu x86_64\r\n\r\n```\r\n=== Checking shared dependencies for libraries ===\r\n\/arrow\/java-dist \/\r\nError: Undefined symbols found in arrow_dataset_jni\/x86_64\/libarrow_dataset_jni.so:\r\ngetentropy\r\n_ITM_addUserCommitAction\r\n_ITM_memcpyRnWt\r\n_ITM_memcpyRtWn\r\n_ITM_RU1\r\n_ITM_RU8\r\n_ZGTtdlPv\r\n_ZGTtnam\r\nZSTD_trace_compress_begin\r\nZSTD_trace_compress_end\r\nZSTD_trace_decompress_begin\r\nZSTD_trace_decompress_end\r\n```\r\n\r\n### Build C++ libraries Ubuntu aarch_64\r\n\r\n```\r\n=== Checking shared dependencies for libraries ===\r\n\/arrow\/java-dist \/\r\nError: Undefined symbols found in arrow_dataset_jni\/aarch_64\/libarrow_dataset_jni.so:\r\ngetentropy\r\n_ITM_addUserCommitAction\r\n_ITM_memcpyRnWt\r\n_ITM_memcpyRtWn\r\n_ITM_RU1\r\n_ITM_RU8\r\n_ZGTtdlPv\r\n_ZGTtnam\r\nZSTD_trace_compress_begin\r\nZSTD_trace_compress_end\r\nZSTD_trace_decompress_begin\r\nZSTD_trace_decompress_end\r\n```\r\n\r\n### Build C++ libraries macOS x86_64\r\n\r\n```\r\n2024-03-20T00:44:56.0094070Z FileNotFoundError: [Errno 2] No such file or directory: 'ldconfig'\r\n```\r\n\r\n### Build C++ libraries macOS aarch_64\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: 'ldconfig'\r\n```\r\n\r\nNeed to customize a bit for macOS too. ","@github-actions crossbow submit -g java","Revision: f426c9c0daedc8069800a698152306f2b65819fa\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-7ddbdef22e](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-7ddbdef22e)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798202\/job\/22863445306)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798274\/job\/22863444610)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798207\/job\/22863444385)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798208\/job\/22863444372)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798256\/job\/22863444568)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-7ddbdef22e-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8352798205\/job\/22863444371)|","You can just submit the `java-jars` job: `@github-actions crossbow submit java-jars`","@github-actions crossbow submit java-jars","It seems that `libstdc++.so` has the following symbols:\r\n\r\n```text\r\ngetentropy\r\n_ITM_addUserCommitAction\r\n_ITM_memcpyRnWt\r\n_ITM_memcpyRtWn\r\n_ITM_RU1\r\n_ITM_RU8\r\n_ZGTtdlPv\r\n_ZGTtnam\r\n```","Revision: 6df06b2997f4bb0c2527431488af8c15fb29b84a\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-e0a3533424](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-e0a3533424)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-e0a3533424-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353191897\/job\/22864520105)|","> _ITM_RU1\r\n\r\nI agree. In local tests, I don't get these as missing symbols though. \r\n\r\n```bash\r\nnm -D --format=just-symbols \/lib\/x86_64-linux-gnu\/libstdc++.so.6 | grep _ITM      \r\n_ITM_addUserCommitAction\r\n_ITM_deregisterTMCloneTable\r\n_ITM_memcpyRnWt\r\n_ITM_memcpyRtWn\r\n_ITM_registerTMCloneTable\r\n_ITM_RU1\r\n_ITM_RU8\r\n```","It seems that they are defined as week symbols. So we can ignore them. (It seems that `nm` has `--no-week` option.)\r\n\r\n```text\r\nZSTD_trace_compress_begin\r\nZSTD_trace_compress_end\r\nZSTD_trace_decompress_begin\r\nZSTD_trace_decompress_end\r\n```","> It seems that they are defined as week symbols. So we can ignore them. (It seems that `nm` has `--no-week` option.)\r\n> \r\n> ```\r\n> ZSTD_trace_compress_begin\r\n> ZSTD_trace_compress_end\r\n> ZSTD_trace_decompress_begin\r\n> ZSTD_trace_decompress_end\r\n> ```\r\n\r\n@kou I cannot find `--no-week` in my `nm` \r\n\r\n```bash\r\nnm --help\r\nUsage: nm [option(s)] [file(s)]\r\n List symbols in [file(s)] (a.out by default).\r\n The options are:\r\n  -a, --debug-syms       Display debugger-only symbols\r\n  -A, --print-file-name  Print name of the input file before every symbol\r\n  -B                     Same as --format=bsd\r\n  -C, --demangle[=STYLE] Decode mangled\/processed symbol names\r\n                           STYLE can be \"none\", \"auto\", \"gnu-v3\", \"java\",\r\n                           \"gnat\", \"dlang\", \"rust\"\r\n      --no-demangle      Do not demangle low-level symbol names\r\n      --recurse-limit    Enable a demangling recursion limit.  (default)\r\n      --no-recurse-limit Disable a demangling recursion limit.\r\n  -D, --dynamic          Display dynamic symbols instead of normal symbols\r\n      --defined-only     Display only defined symbols\r\n  -e                     (ignored)\r\n  -f, --format=FORMAT    Use the output format FORMAT.  FORMAT can be `bsd',\r\n                           `sysv', `posix' or 'just-symbols'.\r\n                           The default is `bsd'\r\n  -g, --extern-only      Display only external symbols\r\n    --ifunc-chars=CHARS  Characters to use when displaying ifunc symbols\r\n  -j, --just-symbols     Same as --format=just-symbols\r\n  -l, --line-numbers     Use debugging information to find a filename and\r\n                           line number for each symbol\r\n  -n, --numeric-sort     Sort symbols numerically by address\r\n  -o                     Same as -A\r\n  -p, --no-sort          Do not sort the symbols\r\n  -P, --portability      Same as --format=posix\r\n  -r, --reverse-sort     Reverse the sense of the sort\r\n      --plugin NAME      Load the specified plugin\r\n  -S, --print-size       Print size of defined symbols\r\n  -s, --print-armap      Include index for symbols from archive members\r\n      --quiet            Suppress \"no symbols\" diagnostic\r\n      --size-sort        Sort symbols by size\r\n      --special-syms     Include special symbols in the output\r\n      --synthetic        Display synthetic symbols as well\r\n  -t, --radix=RADIX      Use RADIX for printing symbol values\r\n      --target=BFDNAME   Specify the target object format as BFDNAME\r\n  -u, --undefined-only   Display only undefined symbols\r\n  -U {d|s|i|x|e|h}       Specify how to treat UTF-8 encoded unicode characters\r\n      --unicode={default|show|invalid|hex|escape|highlight}\r\n      --with-symbol-versions  Display version strings after symbol names\r\n  -X 32_64               (ignored)\r\n  @FILE                  Read options from FILE\r\n  -h, --help             Display this information\r\n  -V, --version          Display this program's version number\r\nnm: supported targets: elf64-x86-64 elf32-i386 elf32-iamcu elf32-x86-64 pei-i386 pe-x86-64 pei-x86-64 elf64-l1om elf64-k1om elf64-little elf64-big elf32-little elf32-big pe-bigobj-x86-64 pe-i386 srec symbolsrec verilog tekhex binary ihex plugin\r\nReport bugs to <https:\/\/sourceware.org\/bugzilla\/>.\r\n```\r\n","Though this is interesting: https:\/\/www.ibm.com\/docs\/en\/aix\/7.2?topic=n-nm-command#nm__row-d3e46469\r\n","You can check `w`\/`W` symbol type to detect weak symbols.\r\n\r\nSee also: https:\/\/man7.org\/linux\/man-pages\/man1\/nm.1.html\r\n\r\n(`--no-week` was typo... `--no-weak` is correct...)","> You can check `w`\/`W` symbol type to detect weak symbols.\r\n> \r\n> See also: https:\/\/man7.org\/linux\/man-pages\/man1\/nm.1.html\r\n> \r\n> (`--no-week` was typo... `--no-weak` is correct...)\r\n\r\nI see it in the man page, I don't have that option locally, a bit odd though. But I will probably try with the CIs.\r\n\r\n```bash\r\nnm -h\r\nUsage: nm [option(s)] [file(s)]\r\n List symbols in [file(s)] (a.out by default).\r\n The options are:\r\n  -a, --debug-syms       Display debugger-only symbols\r\n  -A, --print-file-name  Print name of the input file before every symbol\r\n  -B                     Same as --format=bsd\r\n  -C, --demangle[=STYLE] Decode mangled\/processed symbol names\r\n                           STYLE can be \"none\", \"auto\", \"gnu-v3\", \"java\",\r\n                           \"gnat\", \"dlang\", \"rust\"\r\n      --no-demangle      Do not demangle low-level symbol names\r\n      --recurse-limit    Enable a demangling recursion limit.  (default)\r\n      --no-recurse-limit Disable a demangling recursion limit.\r\n  -D, --dynamic          Display dynamic symbols instead of normal symbols\r\n      --defined-only     Display only defined symbols\r\n  -e                     (ignored)\r\n  -f, --format=FORMAT    Use the output format FORMAT.  FORMAT can be `bsd',\r\n                           `sysv', `posix' or 'just-symbols'.\r\n                           The default is `bsd'\r\n  -g, --extern-only      Display only external symbols\r\n    --ifunc-chars=CHARS  Characters to use when displaying ifunc symbols\r\n  -j, --just-symbols     Same as --format=just-symbols\r\n  -l, --line-numbers     Use debugging information to find a filename and\r\n                           line number for each symbol\r\n  -n, --numeric-sort     Sort symbols numerically by address\r\n  -o                     Same as -A\r\n  -p, --no-sort          Do not sort the symbols\r\n  -P, --portability      Same as --format=posix\r\n  -r, --reverse-sort     Reverse the sense of the sort\r\n      --plugin NAME      Load the specified plugin\r\n  -S, --print-size       Print size of defined symbols\r\n  -s, --print-armap      Include index for symbols from archive members\r\n      --quiet            Suppress \"no symbols\" diagnostic\r\n      --size-sort        Sort symbols by size\r\n      --special-syms     Include special symbols in the output\r\n      --synthetic        Display synthetic symbols as well\r\n  -t, --radix=RADIX      Use RADIX for printing symbol values\r\n      --target=BFDNAME   Specify the target object format as BFDNAME\r\n  -u, --undefined-only   Display only undefined symbols\r\n  -U {d|s|i|x|e|h}       Specify how to treat UTF-8 encoded unicode characters\r\n      --unicode={default|show|invalid|hex|escape|highlight}\r\n      --with-symbol-versions  Display version strings after symbol names\r\n  -X 32_64               (ignored)\r\n  @FILE                  Read options from FILE\r\n  -h, --help             Display this information\r\n  -V, --version          Display this program's version number\r\nnm: supported targets: elf64-x86-64 elf32-i386 elf32-iamcu elf32-x86-64 pei-i386 pe-x86-64 pei-x86-64 elf64-l1om elf64-k1om elf64-little elf64-big elf32-little elf32-big pe-bigobj-x86-64 pe-i386 srec symbolsrec verilog tekhex binary ihex plugin\r\nReport bugs to <https:\/\/sourceware.org\/bugzilla\/>.\r\n```\r\n","```bash\r\nnm --version\r\nGNU nm (GNU Binutils for Ubuntu) 2.38\r\nCopyright (C) 2022 Free Software Foundation, Inc.\r\nThis program is free software; you may redistribute it under the terms of\r\nthe GNU General Public License version 3 or (at your option) any later version.\r\nThis program has absolutely no warranty.\r\n\r\n```","My `nm` is 2.42. Anyway, we should not use `--no-weak` because it's not portable for now.","Also the one in CI containers is way older. Probably catch W from symbols and write another util? ","Still the uncaptured libstdc++ ones are a question though, locally those are captured and filtered \ud83e\udd14","> Probably catch W from symbols and write another util?\r\n\r\nYes. (Both of `W` and `w`.)","> Still the uncaptured libstdc++ ones are a question though, locally those are captured and filtered \ud83e\udd14\r\n\r\nIt seems that they are also weak symbols:\r\n\r\n```console\r\n$ nm arrow-dataset-15.0.0\/x86_64\/libarrow_dataset_jni.so | grep ITM\r\n                 w _ITM_RU1\r\n                 w _ITM_RU8\r\n                 w _ITM_addUserCommitAction\r\n                 w _ITM_deregisterTMCloneTable\r\n                 w _ITM_memcpyRnWt\r\n                 w _ITM_memcpyRtWn\r\n                 w _ITM_registerTMCloneTable\r\n```\r\n\r\nSo implementing the weak symbols related routine may solve all remained issues.","@github-actions crossbow submit java-jars","> > Still the uncaptured libstdc++ ones are a question though, locally those are captured and filtered \ud83e\udd14\r\n> \r\n> It seems that they are also weak symbols:\r\n> \r\n> ```\r\n> $ nm arrow-dataset-15.0.0\/x86_64\/libarrow_dataset_jni.so | grep ITM\r\n>                  w _ITM_RU1\r\n>                  w _ITM_RU8\r\n>                  w _ITM_addUserCommitAction\r\n>                  w _ITM_deregisterTMCloneTable\r\n>                  w _ITM_memcpyRnWt\r\n>                  w _ITM_memcpyRtWn\r\n>                  w _ITM_registerTMCloneTable\r\n> ```\r\n> \r\n> So implementing the weak symbols related routine may solve all remained issues.\r\n\r\nYou're correct. I just saw your comment, but in parallel I learnt the same thing. \r\nLocally it seems to be working fine. Checking the CIs now. ","Revision: e996847fc0f11241b24abc181d085fc9599a98f5\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-798345d75c](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-798345d75c)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-798345d75c-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8360596677\/job\/22886605141)|","@kou btw, the macOS related change I added is not functioning as expected, need to check with a Mac locally. ","@github-actions crossbow submit java-jars","Revision: bb507967227c06aca6cd931d7162a09e1a607e27\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-56cd013c20](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-56cd013c20)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-56cd013c20-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8362530075\/job\/22893334470)|"],"labels":["awaiting change review"]},{"title":"[C#] IPC stream writer should write slices of buffers when writing sliced arrays","body":"### Describe the enhancement requested\r\n\r\nCurrently the C# `ArrowStreamWriter` always writes all data in a buffer. This behaviour differs to the Python\/C++ implementation, which only writes slices of the buffers when an array has a nonzero offset or size in bytes less than the buffer length. This can be observed by looking at the sizes of IPC files for a whole RecordBatch, compared to slices of the data.\r\n\r\nPython:\r\n```python\r\nimport pyarrow as pa\r\nimport numpy as np\r\n\r\n\r\nnum_rows = 400\r\nrows_per_batch = 100\r\n\r\nints = pa.array(np.arange(0, num_rows, 1, dtype=np.int32))\r\nfloats = pa.array(np.arange(0, num_rows \/ 10.0, 0.1, dtype=np.float32))\r\n\r\nall_data = pa.RecordBatch.from_arrays([ints, floats], names=[\"a\", \"b\"])\r\n\r\nsink = pa.BufferOutputStream()\r\nwith pa.ipc.new_stream(sink, all_data.schema) as writer:\r\n    writer.write_batch(all_data)\r\nbuf = sink.getvalue()\r\nprint(f\"Size of serialized full batch = {buf.size}\")\r\n\r\nfor offset in range(0, num_rows, rows_per_batch):\r\n    slice = all_data.slice(offset, rows_per_batch)\r\n    sink = pa.BufferOutputStream()\r\n    with pa.ipc.new_stream(sink, slice.schema) as writer:\r\n        writer.write_batch(slice)\r\n    buf = sink.getvalue()\r\n    print(f\"Size of serialized slice at offset {offset} = {buf.size}\")\r\n```\r\n\r\nThis outputs:\r\n```\r\nSize of serialized full batch = 3576\r\nSize of serialized slice at offset 0 = 1176\r\nSize of serialized slice at offset 100 = 1176\r\nSize of serialized slice at offset 200 = 1176\r\nSize of serialized slice at offset 300 = 1176\r\n```\r\n\r\nThe size of the full batch is 1\/4 the full batch after accounting for the overhead of metadata.\r\n\r\nDoing the same in C#:\r\n```C#\r\nconst int numRows = 400;\r\nconst int rowsPerBatch = 100;\r\n\r\nvar allData = new RecordBatch.Builder()\r\n    .Append(\"a\", false, col => col.Int32(array => array.AppendRange(Enumerable.Range(0, numRows))))\r\n    .Append(\"b\", false, col => col.Float(array => array.AppendRange(Enumerable.Range(0, numRows).Select(i => 0.1f * i))))\r\n    .Build();\r\n\r\n{\r\n    using var ms = new MemoryStream();\r\n    using var writer = new ArrowFileWriter(ms, allData.Schema, false, new IpcOptions());\r\n    await writer.WriteStartAsync();\r\n    await writer.WriteRecordBatchAsync(allData);\r\n    await writer.WriteEndAsync();\r\n\r\n    Console.WriteLine($\"Size of serialized full batch = {ms.Length}\");\r\n}\r\n\r\nfor (var offset = 0; offset < allData.Length; offset += rowsPerBatch)\r\n{\r\n    var arraySlices = allData.Arrays\r\n        .Select(arr => ArrowArrayFactory.Slice(arr, offset, rowsPerBatch))\r\n        .ToArray();\r\n    var slice = new RecordBatch(allData.Schema, arraySlices, arraySlices[0].Length);\r\n\r\n    using var ms = new MemoryStream();\r\n    using var writer = new ArrowFileWriter(ms, slice.Schema, false, new IpcOptions());\r\n    await writer.WriteStartAsync();\r\n    await writer.WriteRecordBatchAsync(slice);\r\n    await writer.WriteEndAsync();\r\n\r\n    Console.WriteLine($\"Size of serialized slice at offset {offset} = {ms.Length}\");\r\n}\r\n```\r\n\r\nThis outputs:\r\n```\r\nSize of serialized full batch = 3802\r\nSize of serialized slice at offset 0 = 3802\r\nSize of serialized slice at offset 100 = 3802\r\nSize of serialized slice at offset 200 = 3802\r\nSize of serialized slice at offset 300 = 3802\r\n```\r\n\r\nWriting a slice of the data results in the same file size as writing the full data, but we'd like to be able to break IPC data into smaller slices in order to send it over a transport that has a message size limit. We're currently working around this by copying the data after slicing.\r\n\r\nFrom a quick look at the C++ implementation, one complication is dealing with null bitmaps, which need to be copied to ensure the start is aligned with a byte boundary.\r\n\r\n### Component(s)\r\n\r\nC#","comments":["I initially thought that the problem was just that the files were bigger than they need to be, and that the array offsets would be correctly round-tripped. But on further investigation, it looks like the IPC files written from sliced arrays are invalid and can't be read back by the C# library or Python library.\r\n\r\nReading these sliced batches back from C# fails with:\r\n```\r\nSystem.IO.InvalidDataException\r\nNull count length must be >= 0\r\n   at Apache.Arrow.Ipc.ArrowReaderImplementation.LoadField(MetadataVersion version, RecordBatchEnumerator& recordBatchEnumerator, Field field, FieldNode& fieldNode, ByteBuffer bodyData, IBufferCreator bufferCreator)\r\n```\r\n\r\nand Python displays the arrays as invalid:\r\n```\r\n<Invalid array: Buffer #0 too small in array of type int32 and length 100: expected at least 13 byte(s), got 0>\r\n```","There's code which suggests a null count of -1 was intended to mean \"RecalculateNullCount\" but apparently there's nothing which actually does this.","Your example works if I add a quick hack e.g.\r\n\r\n```\r\n+            if (nullCount == RecalculateNullCount)\r\n+            {\r\n+                NullCount = Length - CalculateValidCount();\r\n+            }\r\n```\r\nand\r\n\r\n```\r\n+        public int CalculateValidCount()\r\n+        {\r\n+            switch (DataType)\r\n+            {\r\n+                case FixedWidthType fixedWidthType:\r\n+                    ArrowBuffer nullBuffer = Buffers[0];\r\n+                    return nullBuffer.IsEmpty ? 0 : BitUtility.CountBits(nullBuffer.Span);\r\n+                default:\r\n+                    \/\/ TODO:\r\n+                    throw new NotImplementedException();\r\n+            }\r\n+        }\r\n```\r\n\r\nI can finish implementing this over the coming weekend, or someone else could take it on.","When you say it works, I guess you mean that the data can be round-tripped and read correctly (at least from .NET), but the files for a slice are still as large as writing the unsliced data?\r\n\r\nI should be able to work on fixing this eventually, but I have other more urgent work at the moment so am not sure when I will get around to this.","Yeah, it was late and I only had time for a quick look. This avoids the exception but ends up producing the wrong results. The entire ArrowBuffer is being serialized instead of just the part needed for the slice, and while the length is recorded correctly there's (of course) no offset being serialized so the data being read back starts at the beginning of the buffer for each slice -- meaning it's the wrong data. \r\n\r\nThe root of this problem is that ArrowStreamWriter.ArrowRecordBatchFlatBufferBuilder is serializing entire buffers instead of just the parts of the buffer associated with the slice. I suspect this will not be a trivial fix."],"labels":["Type: enhancement","Component: C#"]},{"title":"[JS] Avoid circular dependencies in apache-arrow","body":"### Describe the enhancement requested\r\n\r\nCurrently the apache-arrow library has circular dependencies which cause warnings in tools like rollup:\r\n\r\n```\r\nsrc\/example.js \u2192 dist\/example.js...\r\n(!) Circular dependencies\r\nnode_modules\/apache-arrow\/vector.mjs -> node_modules\/apache-arrow\/util\/vector.mjs -> node_modules\/apache-arrow\/vector.mjs\r\nnode_modules\/apache-arrow\/vector.mjs -> node_modules\/apache-arrow\/util\/vector.mjs -> node_modules\/apache-arrow\/row\/map.mjs -> node_modules\/apache-arrow\/vector.mjs\r\nnode_modules\/apache-arrow\/vector.mjs -> node_modules\/apache-arrow\/util\/vector.mjs -> node_modules\/apache-arrow\/row\/map.mjs -> node_modules\/apache-arrow\/visitor\/get.mjs -> node_modules\/apache-arrow\/vector.mjs\r\n...and 10 more\r\ncreated dist\/example.js in 890ms\r\n```\r\n\r\nSee the following stackblitz which reproduces that build warning: https:\/\/stackblitz.com\/edit\/apache-arrow-circular-dependencies?file=src%2Fexample.js\r\n\r\nIt would be nice if the library was organized such that circular dependencies could be avoided.\r\n\r\nTo workaround the warnings in rollup one can use an [onwarn](https:\/\/rollupjs.org\/configuration-options\/#onwarn) handler in the rollup configuration with an implementation like:\r\n\r\n```js\r\nconst onwarn = (warning, defaultHandler) => {\r\n    const ignoredWarnings = [\r\n        {\r\n            code: 'CIRCULAR_DEPENDENCY',\r\n            file: 'node_modules\/apache-arrow'\r\n        }\r\n    ];\r\n\r\n    if (\r\n        !ignoredWarnings.some(\r\n            ({ code, file }) => warning.code === code && warning.message.includes(file)\r\n        )\r\n    ) {\r\n        defaultHandler(warning);\r\n    }\r\n};\r\n```\r\n\r\n### Component(s)\r\n\r\nJavaScript","comments":["@domoritz Could you take a look at this?","Thanks for the issue report. I've looked at circular deps before and luckily I think all of these are just warning and the bundler can easily serialize the dependencies. However, it would be nice to clean this up. We need to be careful not to make the library less readable by breaking up code that makes sense together into separate files.\r\n\r\nAnd easy reproduction in the repo itself is to remove https:\/\/github.com\/apache\/arrow\/blob\/6b1e254f3b62924f216e06e9e563e92c69f9efd3\/js\/gulp\/bundle-task.js#L86 and then running `yarn gulp bundle:rollup`. Then you get warnings for all the sample bundles we have. \r\n\r\nI don't know how much I can make it a priority compared to other things I have on my plate. This seems like a good issue for someone else to contribute. @rajsite would you be open to making a pull request? "],"labels":["Type: enhancement","Component: JavaScript"]},{"title":"[C++] Pure ScalarFunctions called with no arguments should return scalar","body":"### Describe the enhancement requested\n\nWhen called with no arguments, the desired output length of scalar functions cannot be determined in general. (For some functions such as `random` we can exploit default preallocation of fixed width output to inform the kernel how much output to generate, but this approach can't work for all functions.) However for the majority of scalar functions which are pure (their output is completely determined by their arguments), there is exactly one value corresponding to empty arguments. In the context of expression evaluation, returning this value as a scalar indicates that the value should be broadcast to whatever length is required for the enclosing expression. I think this is a more reasonable return value than an empty null array, which is what is returned by for example `pa.max_element_wise()`.\r\n\r\nNote: explicit Function::is_impure flag added in https:\/\/github.com\/apache\/arrow\/pull\/40396\n\n### Component(s)\n\nC++","comments":["Note that I opened a PR renaming that to `is_pure()` https:\/\/github.com\/apache\/arrow\/pull\/40608","> I think this is a more reasonable return value than an empty null array, which is what is returned by for example `pa.max_element_wise()`.\r\n\r\nAgree, we should return scalar for pure functions with no arguments. For the example `max_element_wise()`, we should return null scalar, right?","@bkietz @felipecrv  We can have more discussion in the PR?"],"labels":["Type: enhancement","Component: C++"]},{"title":"[CI] Consider installing `azurite` and `minio` for Mac OS python tests","body":"### Describe the enhancement requested\r\n\r\nCurrently the Mac OS python tests skip most of the test cases for Azure and S3 filesystems. This is because `azurite` and `minio` are not installed. \r\n\r\n```\r\nSKIPPED [2] ..\/..\/..\/..\/..\/Library\/Frameworks\/Python.framework\/Versions\/3.11\/lib\/python3.11\/site-packages\/pyarrow\/tests\/test_fs.py:607: `minio` command cannot be located\r\n```\r\n```\r\nSKIPPED [1] ..\/..\/..\/..\/..\/Library\/Frameworks\/Python.framework\/Versions\/3.11\/lib\/python3.11\/site-packages\/pyarrow\/tests\/test_fs.py:607: Command ['azurite-blob', '--location', local('\/private\/var\/folders\/24\/8k48jl6d249_n_qfxwsl6xvm0000gn\/T\/pytest-of-runner\/pytest-0'), '--blobPort', '49265'] failed to execute: [Errno 2] No such file or directory: 'azurite-blob'\r\n```\r\nhttps:\/\/github.com\/apache\/arrow\/actions\/runs\/8263216830\/job\/22604137976\r\n\r\nI believe these need to be installed [here](https:\/\/github.com\/apache\/arrow\/blob\/main\/.github\/workflows\/python.yml#L158-L183)\r\n\r\nThis is coming from some discussion on another PR https:\/\/github.com\/apache\/arrow\/pull\/40021#issuecomment-1971476506\r\n\r\n\r\n### Component(s)\r\n\r\nContinuous Integration","comments":[],"labels":["Type: enhancement","Component: Continuous Integration"]},{"title":"GH-40507: [C++][ORC] Upgrade ORC to 2.0.0","body":"### Rationale for this change\r\n\r\nThis PR aims to upgrade to a new major version of Apache ORC: https:\/\/orc.apache.org\/news\/2024\/03\/08\/ORC-2.0.0\/\r\n\r\n### What changes are included in this PR?\r\n\r\nThis PR upgrades ORC dependency from 1.9.2 to 2.0.0.\r\n\r\n### Are these changes tested?\r\n\r\nPass the CIs.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\n* GitHub Issue: #40507","comments":["The Apache ORC java library has dropped Java 8 support from v2.0.0. However, I don't think this is an issue here as the orc-core java dependency is only used in the test scope. Am I correct? @lidavidm @jduo ","OK, I'm wrong:\r\n\r\n```\r\n[INFO] Arrow Performance Benchmarks ....................... SKIPPED\r\n[INFO] Arrow Java C Data Interface ........................ SUCCESS [ 57.354 s]\r\n[INFO] Arrow Orc Adapter .................................. FAILURE [ 35.491 s]\r\n[INFO] Arrow Gandiva ...................................... SUCCESS [01:26 min]\r\n[INFO] Arrow Java Dataset ................................. SKIPPED\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  06:03 min (Wall Clock)\r\n[INFO] Finished at: 2024-03-13T15:23:50Z\r\n[INFO] ------------------------------------------------------------------------\r\nError:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.12.1:testCompile (default-testCompile) on project arrow-orc: Compilation failure\r\nError:  \/arrow\/java\/adapter\/orc\/src\/test\/java\/org\/apache\/arrow\/adapter\/orc\/OrcReaderTest.java:[40,21] error: cannot access OrcFile\r\nError:    bad class file: \/root\/.m2\/repository\/org\/apache\/orc\/orc-core\/2.0.0\/orc-core-2.0.0.jar(\/org\/apache\/orc\/OrcFile.class)\r\nError:      class file has wrong version 61.0, should be 53.0\r\nError:      Please remove or make sure it appears in the correct subdirectory of the classpath.\r\nError:  -> [Help 1]\r\nError:  \r\nError:  To see the full stack trace of the errors, re-run Maven with the -e switch.\r\nError:  Re-run Maven using the -X switch to enable full debug logging.\r\nError:  \r\nError:  For more information about the errors and possible solutions, please read the following articles:\r\nError:  [Help 1] http:\/\/cwiki.apache.org\/confluence\/display\/MAVEN\/MojoFailureException\r\nError:  \r\nError:  After correcting the problems, you can resume the build with the command\r\nError:    mvn <args> -rf :arrow-orc\r\n```","Hmm, that is test scope, and the other builds pass. So I guess that pipeline uses Java 8 and we could maybe exclude the file?","That said, I think what we've done is to hold off on package updates requiring Java 8 for now, since we're still supporting Java 8...We were trying to drop Java 8 support but some people spoke out against. Maybe we need to revisit that.","As `orc-core` is only used in the test scope, perhaps we can leave it to stay at 1.9.x which is the last major version with java 8 support.","I added this to the list at https:\/\/github.com\/apache\/arrow\/issues\/38051","I think what's going to have to happen is that if someone really needs Java 8 support, they are going to have to maintain the fork or otherwise come up with a strategy, because eventually these CVE fixes pile up and we won't be able to defer the issue any longer","@github-actions crossbow submit wheel-windows-*","Revision: 7203fe6fd4575fd8cf7d562c1c731ad379d26d20\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-c8f70cc49c](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-c8f70cc49c)\n\n|Task|Status|\n|----|------|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-c8f70cc49c-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8289254609\/job\/22685344209)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-c8f70cc49c-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8289254682\/job\/22685344555)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-c8f70cc49c-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8289254570\/job\/22685343947)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-c8f70cc49c-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8289254808\/job\/22685344574)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-c8f70cc49c-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8289254730\/job\/22685344565)|","I think we're going to revisit Java 8 in the next 3-6 months so we can include this PR in that effort.","You can probably split the Java and C++ parts of this PR."],"labels":["Component: Java","Component: C++","awaiting review"]},{"title":"[C++][ORC] Upgrade ORC to 2.0.0","body":"### Describe the enhancement requested\r\n\r\nUse ORC to 2.0.0\r\n- https:\/\/orc.apache.org\/news\/2024\/03\/08\/ORC-2.0.0\/\r\n\r\n### Component(s)\r\n\r\nC++, Java","comments":["This might allow us to enable on Windows for pyarrow solving the segmentation fault: https:\/\/github.com\/apache\/arrow\/pull\/37382","@raulcd TBH, I don't think so. I managed to build it on Windows and will try to reproduce that issue later. It will take some time as I'm not that familiar with python and conda."],"labels":["Type: enhancement","Component: Java","Component: C++"]},{"title":"[Python] Provide a way to close a NativeFile without writing the contents","body":"### Describe the enhancement requested\r\n\r\nThe following code creates an object in S3, even if the exception is raised:\r\n\r\n```python\r\nfrom pyarrow.fs import S3FileSystem\r\n\r\nfs = S3FileSystem()\r\nwith fs.open_output_stream(\"bucket\/key\") as fd:\r\n    fd.write(b\"hi\")\r\n    raise RuntimeError()\r\n```\r\n\r\nThis is usually what you want (I guess?), but there is no way to change this behaviour. If you have a long-running task that writes to a `NativeFile`, you may not want partially written data to be written to S3 in the case of an error. The workaround is annoying - you need to write to a separate key, then move the file (or delete it):\r\n\r\n```python\r\nfs = S3FileSystem()\r\ntry:\r\n    with fs.open_output_stream(\"bucket\/another_key\") as fd:\r\n        do_something_that_may_explode(fd)\r\nexcept Exception:\r\n    fs.delete(\"bucket\/another_key\")\r\nelse:\r\n    fs.move(\"bucket\/another_key\", \"bucket\/actual_key\")\r\n```\r\n\r\nThis works, but it's less performant than it should be: data is essentially written to S3 twice, and S3 natively provides a way to prevent this issue: just abort the MultipartUpload (or, don't complete it).\r\n\r\nI thought of just avoiding the context manager entirely:\r\n\r\n```python\r\nfd =  fs.open_output_stream(\"bucket\/another_key\")\r\ntry:\r\n    do_something_that_may_explode(fd)\r\nexcept Exception:\r\n    pass\r\nelse:\r\n   fd.close()\r\n```\r\n\r\nBut upon `fd` being dropped the file is still written. Also, if a reference to `fd` is left alive and unclosed (i.e in a traceback object), then your process will never terminate.\r\n\r\nI'd love a way to do something like this:\r\n\r\n```python\r\nfs = S3FileSystem()\r\nwith fs.open_output_stream(\"bucket\/key\") as fd:\r\n    try:\r\n        do_something_that_may_explode(fd)\r\n    except Exception:\r\n        fd.abort()\r\n# The object should now not exist if abort() is called.\r\n```\r\n\r\nI've tried using `close()` to no avail, and the `background_writes` parameter on the S3FileSystem() seems to have no effect.\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"GH-40494: [Go] add support for protobuf messages","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nSupport for protobuf messages\r\n\r\n### What changes are included in this PR?\r\n\r\nAbility to create a schema from a protobuf message\r\nAbility to create a record from a protobuf message\r\nSome customisations\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nYes, couple of unit tests included\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40494","comments":[":warning: GitHub issue #40494 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #40494 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #40494 **has been automatically assigned in GitHub** to PR creator.","Please fix the build failures, they appear to be caused by unused functions","@zeroshade CI is passing now, please take a look when you can.","lots of checks happened after I said that, looks like the json check includes, dict order differences. I'll fix that"],"labels":["Component: Go","Component: Documentation","awaiting change review"]},{"title":"[Go] Add support for protobuf messages","body":"### Describe the enhancement requested\n\n**Context**\r\n\r\nMy team recieve protobuf messages from a kafka topic, we want to deserialise them and write them to S3 as parquet.\r\n\r\nAt the moment the suggested way is to convert them via JSON i.e. https:\/\/github.com\/apache\/arrow\/issues\/37807\r\n\r\nI would like more support for protobuf features such as any, or one of.\n\n### Component(s)\n\nGo","comments":[],"labels":["Type: enhancement","Component: Go"]},{"title":"[GLib] Add GArrowStreamDecoder","body":"### Describe the enhancement requested\n\nIt's the bindings of `arrow::ipc::StreamDecoder`.\n\n### Component(s)\n\nGLib","comments":[],"labels":["Type: enhancement","Component: GLib"]},{"title":"[Docs] Bug: Wrong property keys in the Arrow Flight JDBC Driver documentation","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nIn the documentation page of the Arrow Flight SQL JDBC Driver, there is an issue where the parameters table and the example are contradicting each other w.r.t the authentication property keys.\r\n\r\nThe example will use `username` in the Connection String meanwhile the table says that `user` is the property key for authentication.\r\n\r\n`username` is the correct one and the table has outdated\/wrong information.\n\n### Component(s)\n\nDocumentation","comments":[],"labels":["Type: bug","Component: Documentation"]},{"title":"MINOR: [Java] Bump Netty to 4.1.107.Final","body":"### Rationale for this change\r\n\r\n[Java] bump to latest version of Netty\r\n\r\nhttps:\/\/netty.io\/news\/2024\/02\/13\/4-1-107-Final.html\r\n\r\n### What changes are included in this PR?\r\n\r\nmodified Java pom.xml\r\n\r\n### Are these changes tested?\r\n\r\nGitHub Actions CI build\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo\r\n\r\n","comments":["Is there a particular fix we should be aware of, and should we bump the gRPC requirement at the same time?","@github-actions crossbow submit *java*","Revision: b3e2ad2b3de30ae0cbcd4c2d6cf6c5f31e99783b\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-faf1d52899](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-faf1d52899)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132907\/job\/22587123804)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132900\/job\/22587122954)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132899\/job\/22587122950)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132867\/job\/22587122333)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132864\/job\/22587122327)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-faf1d52899-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8257132903\/job\/22587122965)|","@github-actions crossbow submit -g java","Revision: b3e2ad2b3de30ae0cbcd4c2d6cf6c5f31e99783b\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-75fa8e9099](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-75fa8e9099)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136973\/job\/22864375558)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136903\/job\/22864374858)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136881\/job\/22864374926)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136906\/job\/22864374881)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136929\/job\/22864374989)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-75fa8e9099-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8353136905\/job\/22864374906)|"],"labels":["Component: Java","awaiting review"]},{"title":"[MATLAB] Create simple example of MATLAB HTTP GET Arrow client","body":"### Describe the enhancement requested\n\nContribute a MATLAB client example to the [set of minimal HTTP GET examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple).\n\n### Component(s)\n\nMATLAB","comments":[],"labels":["Type: enhancement","Component: MATLAB"]},{"title":"[Swift] Create simple example of Swift HTTP GET Arrow client","body":"### Describe the enhancement requested\n\nContribute a Swift client example to the [set of minimal HTTP GET examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple).\n\n### Component(s)\n\nSwift","comments":[],"labels":["Type: enhancement","Component: Swift"]},{"title":"GH-40431: [C++] Try to check\/alloc the TempVectorStack size as HashBatch needed","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nTry to check\/alloc the TempVectorStack size as HashBatch needed.\r\nFor now, HashBatch api need more TempVectorStack space if we only input small recordbatch rows.\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nAdd estimate size for TempVectorStack's init or alloced size's check before we go into HashMultiColumn.\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40431","comments":["cc @kou , it's a temporary fix. And i haven't add ut for now. PTAL?"],"labels":["Component: C++","awaiting change review"]},{"title":"GH-40205: [Python] ListView arrow-to-pandas conversion","body":"### Rationale for this change\r\n\r\nListView should support converting to pandas\/numpy in pyarrow.\r\n\r\n### What changes are included in this PR?\r\n\r\n* `.to_pandas()` successfully creates a pandas series\r\n* `.to_numpy()` successfully creates a numpy array\r\n\r\n### Are these changes tested?\r\n\r\n* Yes, unit tests\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo, just adding support for existing APIs `to_pandas()` `to_numpy()`.\r\n* GitHub Issue: #40205","comments":[":warning: GitHub issue #40205 **has been automatically assigned in GitHub** to PR creator.","One of the problems with the `Flatten` APIs as far as I understand (and that I am not sure you tackled in the current version of the PR?), is that you would then also have to change the offsets to keep those matching the flattened values. \r\n\r\nFrom a comment in `ConvertListsLike`:\r\n```\r\n    \/\/ We can't use Flatten(), because it removes the values behind a null list\r\n    \/\/ value, and that makes the offsets into original list values and our\r\n    \/\/ flattened_values array different.\r\n```\r\n\r\nSo that might be a reason to keep something more similar to the logic we have in arrow_to_pandas.cc right now for ListArray","I am wondering if we could get around having to concatenate the slices for ListView, and rather update the logic that is now in `ConvertListsLikeChunks` to have a specialized version of that for ListView (knowing that the slices we make there might be out of order)","Thanks for the feedback @jorisvandenbossche ! I went ahead and reused the `FromListView()` APIs.","The appveyor error is a known issue on main and already has a GH issue for it. "],"labels":["Component: Python","awaiting merge"]},{"title":"[JS] Create simple example of JavaScript Node.js HTTP GET Arrow server","body":"### Describe the enhancement requested\n\nContribute a Node.js server example to the [set of minimal HTTP GET examples in the arrow-experiments repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple).\n\n### Component(s)\n\nJavaScript","comments":[],"labels":["Type: enhancement","Component: JavaScript"]},{"title":"[C++] Create simple example of C++ HTTP GET Arrow server","body":"### Describe the enhancement requested\n\nContribute a C++ server example to the [set of minimal HTTP GET examples in the `arrow-experiments` repo](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple).\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[Docs] Document conventions for sending and receiving Arrow data over HTTP APIs","body":"### Describe the enhancement requested\r\n\r\nThe Arrow developer community intends to publish a set of conventions in the Arrow docs for how to send and receive Arrow-format data over HTTP APIs. There is a related discussion on the Arrow developer mailing list at https:\/\/lists.apache.org\/thread\/886cnx6ytjst3smmytz4r4ddcbv95191.\r\n\r\n### Tasks\r\n\r\nThis issue is an umbrella for tasks that are a part of this effort.\r\n\r\n-----\r\n\r\n#### Simple HTTP GET client and server examples\r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/get_simple`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_simple)\r\n\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40466\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40467\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40472\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40480\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40481\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40468\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40469\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40470\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40471\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40473\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40474\r\n- [ ] https:\/\/github.com\/apache\/arrow-julia\/issues\/502\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40489\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40475\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40476\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40611\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40477\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40478\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40479\r\n- [x] https:\/\/github.com\/apache\/arrow-rs\/issues\/5496\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40488\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40613\r\n\r\n-----\r\n\r\n#### HTTP GET client and server examples demonstrating range requests\r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/get_range`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_range)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40597\r\n\r\n-----\r\n\r\n#### Indirect response HTTP GET client and server examples \r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/get_indirect`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_indirect)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40596\r\n\r\n-----\r\n\r\n#### Multipart\/mixed response HTTP GET client and server examples \r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/get_multipart`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_multipart)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40598\r\n\r\n-----\r\n\r\n#### HTTP GET examples to test different compression options\r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/get_compressed`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/get_compressed)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40600\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40601\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40612\r\n\r\n-----\r\n\r\n#### Simple HTTP PUT \/ POST client and server examples\r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/post_simple`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/post_simple)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40595\r\n\r\n-----\r\n\r\n#### Multipart\/form-data request HTTP PUT \/ POST client and server examples\r\n\r\n:file_folder: [`arrow-experiments\/tree\/main\/http\/post_multipart`](https:\/\/github.com\/apache\/arrow-experiments\/tree\/main\/http\/post_multipart)\r\n\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40599\r\n\r\n-----\r\n\r\n#### General issues and questions\r\n\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40581\r\n\r\n-----\r\n\r\n### Component(s)\r\n\r\nDocumentation","comments":["Is this the main issue where I can track your HTTP+Arrow initiative @ianmcook?","> Is this the main issue where I can track your HTTP+Arrow initiative @ianmcook?\n\nYes"],"labels":["Type: enhancement","Component: Documentation"]},{"title":"GH-39386: [C++] Add floordiv compute kernel","body":"\n* GitHub Issue: #39386","comments":[":warning: GitHub issue #39386 **has been automatically assigned in GitHub** to PR creator.","I don't think the current test failures are related?"],"labels":["Component: C++","Component: Documentation","awaiting committer review"]},{"title":"GH-40393: [Java] add precondition for NullVector constructor to check field type","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nSee https:\/\/github.com\/apache\/arrow\/issues\/40393\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nThis changes constructors of NullVector and ZeroVector to always set the field type to the Null type, and marks constructors that accept a field or a field type as deprecated.\r\n\r\n### Are these changes tested?\r\n\r\nThis change should be covered by existing tests.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\nYes\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40393","comments":["Looks like the formatter is unhappy:\r\n\r\n```\r\nWarning:  src\/main\/java\/org\/apache\/arrow\/vector\/NullVector.java:[96] (sizes) LineLength: Line is longer than 120 characters (found 133).\r\n```","Hmm. \r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/b448b33808f2dd42866195fa4bb44198e2fc26b9\/java\/vector\/src\/main\/java\/org\/apache\/arrow\/vector\/ZeroVector.java#L28-L31\r\n\r\n@barronw I guess the optimization you wanted originally does exist, it's just a subclass of NulLVector...? That also means the check here isn't possible to do. (This is making every test fail. I didn't realize we had this.)","Since ZeroVector is just a subclass of NullVector,\r\n1. I think this means that NullVector should also accept any field type.\r\n2. Is ZeroVector any different than NullVector with a value count of 0?","I don't think they should have been subclasses, since Null IS a distinct type, and this was just an implementation convenience. Maybe other Java maintainers have context, @jduo or @emkornfield?"],"labels":["Component: Java","awaiting change review"]},{"title":"GH-40308: [C++][Gandiva] Keep decimal divide rules same with compute module's rules","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n Keep gandiva decimal divide rules same with normal expressions.\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nSupport an option argument in GetResultType for compatabilty with  **compute module's decimal promotion rules**.\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nYes\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nNo\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #40308","comments":[":warning: GitHub issue #40308 **has been automatically assigned in GitHub** to PR creator.","@niyue @js8544 Could you take a look at this?","@kou Can this PR be moved forward?"],"labels":["Component: C++","Component: C++ - Gandiva","awaiting merge"]},{"title":"[C++] Crashed at TempStack alloc when use Hashing32::HashBatch independently","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe issue is similar to https:\/\/github.com\/apache\/arrow\/pull\/40007, but they are different.\r\nI want to use the `Hashing32::HashBatch` api   for produce a hash-array for a batch. Although the `Hashing32` and `Hashing64` are used in join based codes, but they can be used independently.\r\n\r\nLike below codes:\r\n```c\r\n  auto arr = arrow::ArrayFromJSON(arrow::int32(), \"[9,2,6]\");\r\n  const int batch_len = arr->length();\r\n  arrow::compute::ExecBatch exec_batch({arr}, batch_len);\r\n  auto ctx = arrow::compute::default_exec_context();\r\n  arrow::util::TempVectorStack stack;\r\n  ASSERT_OK(stack.Init(ctx->memory_pool(), batch_len * sizeof(uint32_t))); \/\/ I just alloc the stack size as i needed.\r\n\r\n  std::vector<uint32_t> hashes(batch_len);\r\n  std::vector<arrow::compute::KeyColumnArray> temp_column_arrays;\r\n  ASSERT_OK(arrow::compute::Hashing32::HashBatch(\r\n      exec_batch, hashes.data(), temp_column_arrays,\r\n      ctx->cpu_info()->hardware_flags(), &stack, 0, batch_len));\r\n```\r\n\r\nThe crash stack in `HashBatch` is:\r\n```shell\r\narrow::compute::Hashing32::HashBatch\r\n  arrow::compute::Hashing32::HashMultiColumn\r\n      arrow::util::TempVectorHolder<unsigned int>::TempVectorHolder\r\n        arrow::util::TempVectorStack::alloc\r\n          ARROW_DCHECK(top_ <= buffer_size_); \/\/ top_=4176, buffer_size_=160\r\n```\r\n\r\nThe reason is blow codes:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/7e286dd004a8fcf2de0f58615793338076741208\/cpp\/src\/arrow\/compute\/key_hash.cc#L385-L387\r\n\r\nThe holder use the `max_batch_size` which is `1024` as it's num_elements, it's far more than the temp stack's init `buffer_size`.\r\n\r\nI know that the `HashBatch` is only used in hash-join or related codes. For join, they have already done line clipping at the upper level, ensuring that each input batch size is less_equal to `kMiniBatchLength` and the stack size is bigger enough.\r\n\r\nBut it can be used independently. So maybe we could use the `num_rows`  rather than `util::MiniBatch::kMiniBatchLength` in `HashBatch` related apis?\r\n\n\n### Component(s)\n\nC++","comments":["cc @kou PTAL this issue?","Can you provide a buildable C++ code that reproduces this problem?","> Can you provide a buildable C++ code that reproduces this problem?\r\n\r\nOf course.\r\n```c\r\n#include <arrow\/compute\/exec.h>\r\n#include <arrow\/compute\/util.h>\r\n#include <arrow\/testing\/gtest_util.h>\r\n#include <arrow\/testing\/random.h>\r\n#include <arrow\/type_fwd.h>\r\n#include <arrow\/compute\/light_array.h>\r\n#include <arrow\/compute\/key_hash.h>\r\n#include <arrow\/util\/async_util.h>\r\n#include <arrow\/util\/future.h>\r\n#include <arrow\/util\/task_group.h>\r\n#include <arrow\/util\/thread_pool.h>\r\n#include <arrow\/util\/logging.h>\r\n#include <arrow\/acero\/options.h>\r\n#include <arrow\/compute\/api_vector.h>\r\n#include <arrow\/memory_pool.h>\r\n#include <arrow\/record_batch.h>\r\n#include <arrow\/builder.h>\r\n#include <arrow\/result.h>\r\n#include <arrow\/array\/diff.h>\r\n\r\n#include <mutex>\r\n#include <thread>\r\n#include <unordered_map>\r\n#include \"gtest\/gtest.h\"\r\n\r\nTEST(HashBatch, BasicTest) {\r\n  auto arr = arrow::ArrayFromJSON(arrow::int32(), \"[9,2,6]\");\r\n  const int batch_len = arr->length();\r\n  arrow::compute::ExecBatch exec_batch({arr}, batch_len);\r\n  auto ctx = arrow::compute::default_exec_context();\r\n  arrow::util::TempVectorStack stack;\r\n  ASSERT_OK(stack.Init(ctx->memory_pool(), batch_len * sizeof(uint32_t)));\r\n\r\n  std::vector<uint32_t> hashes(batch_len);\r\n  std::vector<arrow::compute::KeyColumnArray> temp_column_arrays;\r\n  ASSERT_OK(arrow::compute::Hashing32::HashBatch(\r\n      exec_batch, hashes.data(), temp_column_arrays,\r\n      ctx->cpu_info()->hardware_flags(), &stack, 0, batch_len));\r\n\r\n  for (int i = 0; i < batch_len; i++) {\r\n    std::cout << hashes[i] << \" \";\r\n  }\r\n}\r\n```\r\ncc @kou, do you think this problem needs to be solved?","Sorry. I missed this.\r\n\r\nThanks. I could run the code:\r\n\r\n```diff\r\ndiff --git a\/cpp\/src\/arrow\/compute\/key_hash_test.cc b\/cpp\/src\/arrow\/compute\/key_hash_test.cc\r\nindex c998df7169..ccfddaa645 100644\r\n--- a\/cpp\/src\/arrow\/compute\/key_hash_test.cc\r\n+++ b\/cpp\/src\/arrow\/compute\/key_hash_test.cc\r\n@@ -311,5 +311,24 @@ TEST(VectorHash, FixedLengthTailByteSafety) {\r\n   HashFixedLengthFrom(\/*key_length=*\/19, \/*num_rows=*\/64, \/*start_row=*\/63);\r\n }\r\n \r\n+TEST(HashBatch, BasicTest) {\r\n+  auto arr = arrow::ArrayFromJSON(arrow::int32(), \"[9,2,6]\");\r\n+  const int batch_len = arr->length();\r\n+  arrow::compute::ExecBatch exec_batch({arr}, batch_len);\r\n+  auto ctx = arrow::compute::default_exec_context();\r\n+  arrow::util::TempVectorStack stack;\r\n+  ASSERT_OK(stack.Init(ctx->memory_pool(), batch_len * sizeof(uint32_t)));\r\n+\r\n+  std::vector<uint32_t> hashes(batch_len);\r\n+  std::vector<arrow::compute::KeyColumnArray> temp_column_arrays;\r\n+  ASSERT_OK(arrow::compute::Hashing32::HashBatch(\r\n+      exec_batch, hashes.data(), temp_column_arrays,\r\n+      ctx->cpu_info()->hardware_flags(), &stack, 0, batch_len));\r\n+\r\n+  for (int i = 0; i < batch_len; i++) {\r\n+    std::cout << hashes[i] << \" \";\r\n+  }\r\n+}\r\n+\r\n }  \/\/ namespace compute\r\n }  \/\/ namespace arrow\r\n```\r\n\r\nIn general, allocating only required size is preferred. So using the `num_rows` rather than `util::MiniBatch::kMiniBatchLength` in `HashBatch` related apis may be better. (Sorry, I can't determine whether the `num_rows` is the correct size for it right now.)\r\n\r\nBut it seems that `stack.Init(ctx->memory_pool(), batch_len * sizeof(uint32_t))` isn't enough for this case. Because we need at least 3 allocations for `HashMultiColumn()`:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/605f8a792c388afb2230b1f19e0f3e4df90d5abe\/cpp\/src\/arrow\/compute\/key_hash.cc#L387-L395\r\n\r\nAnd we also need 16 bytes metadata:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/605f8a792c388afb2230b1f19e0f3e4df90d5abe\/cpp\/src\/arrow\/compute\/util.cc#L35-L44\r\n\r\nAnyway, could you try this?"],"labels":["Type: bug","Component: C++"]},{"title":"[C++] String manipulation on a dictionary column","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI was trying to write up an R example using a dataset that has lots of dictionary columns, but am unable to do string manipulation on it.  Here's a slightly contrived examples using mtcars:\r\n\r\n``` r\r\nlibrary(arrow)\r\nlibrary(dplyr)\r\nmtcars |>\r\n  mutate(cyl = as.factor(as.character(cyl))) |>\r\n  arrow_table() |>\r\n  mutate(cyl6 = str_detect(cyl, \"6\")) |>\r\n  collect()\r\n#> Error in `compute.arrow_dplyr_query()`:\r\n#> ! NotImplemented: Function 'match_substring_regex' has no kernel matching input types (dictionary<values=string, indices=int8, ordered=0>)\r\n```\r\n\r\nI'm wondering if this is something we could enable from the R package side, perhaps looking at the type of the column and doing some casting if we are trying to do this type of operation?  Or even in the C++ using the dictionary values? I'm not sure how complicated this would be though.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[DISCUSS] [FlightSQL] FlightSQL versioning \/ compatibility levels","body":"### Describe the enhancement requested\n\nI would like to discuss adding \"versions\" or \"compatibility levels\" to FlightSQL\r\n\r\n## Usecase\r\n\r\nThe usecase is now that FlightSQL is being adopted across products, there is an increasing number of installed clients. As we make changes \/ add optional features to the spec such as:\r\n* https:\/\/github.com\/apache\/arrow\/issues\/37720\r\n\r\nThe question arises \"how will clients\/servers know what to expect\" -- \r\n\r\n\r\nAt the moment, the spec defines certain features that are \"optional\" such as XXX (and a new optional behavior such as https:\/\/github.com\/apache\/arrow\/pull\/40243). This means:\r\n1. Updates to the spec must carefully define what should happen when the client and\/or server do not use the optional feature\r\n2. Clients and Servers must similarly handle a variety of potentially present and missing features\r\n\r\n\r\nAn example of this subtlety is shown in the context of  https:\/\/github.com\/apache\/arrow\/issues\/37720 where we are discussing  \"how should the server tell if the client is using the updated semantics\"  on  https:\/\/github.com\/apache\/arrow\/pull\/40243  with @erratic-pattern  @lidavidm and @pitrou \r\n https:\/\/github.com\/apache\/arrow\/pull\/40243#pullrequestreview-1912699524 \r\n\r\n\r\n\r\n## Possible solutions\r\nOne possible\r\n\n\n### Component(s)\n\nFlightRPC","comments":["The SqlInfo enum was supposed to\/does contain this info","That said it doesn't help here when the server needs to know what the client expects","That said for this specific case, I'm somewhat inclined to punt instead of trying to solve feature\/version negotiation. Would InfluxDB want to support a client that doesn't recognize the updated prepared statement handle? If not, then as long as it detects and errors when the handle is used, that should be enough. The client would just get the error a bit later.","I'm skeptical about the usefulness of this, even more when adding it somewhat late in the process like this.\r\n\r\nPast experience on related situations:\r\n1. the Arrow IPC formay has a `features` field that nobody uses AFAIK\r\n2. the Parquet format has a `version` field that is not used for any particular purpose AFAIK\r\n\r\nA version field could be useful to convey potential incompatible changes, like the `metadata_version` in Arrow IPC does, but that's not what the discussion in https:\/\/github.com\/apache\/arrow\/pull\/40243 is about.","I pretty much agree with everything that's been said. I don't think that such a field would end up providing anything particularly useful over what exists or that would actually get used.",">  Would InfluxDB want to support a client that doesn't recognize the updated prepared statement handle? If not, then as long as it detects and errors when the handle is used, that should be enough. The client would just get the error a bit later.\r\n\r\nYes, this is likely what we would do (return an error). One potential issue raised by  @erratic-pattern is that it would not be possible to distinguish between the cases:\r\n1. The client is old and doesn't know to send the updated handle\r\n2. The client simply never sent the parameter values\r\n\r\nI personally think the error could be made clear enough, but I wanted to mention this potential issue","I do think the concerns about \"spec bloat\" are valid. If there aren't enough of these sort of \"opt-in\" behaviors then it may not be worth complicating the handshake. \r\n\r\nEchoing what Andrew said, even though we would only return an error in the case of a client that doesn't recognize the new handle, we would still want to provide clear and unambiguous error messages about what went wrong. \r\n\r\nCurrently we can get close but it's something like \"No parameters provided for this prepared statement. If parameters were provided, then this FlightSQL client is possibly out of date.\" This is informing the user about 2 distinct possible problems instead of 1. \r\n\r\nAnother problem with this is that you could still get this error message even on the *latest* version of your client, simply because your client has not been updated to support it. So \"out-of-date\" is not necessarily true either. What actually happened is that the client does not support a capability that the server needs.\r\n\r\n\"out of date\" is also a vague explanation that doesn't tell the user what went wrong. You could instead explain in more detail in the error message, but what should it say? \"Your client does not support stateless prepared statements\" Most users will not know what that means. Do you give a vague response or do you explain the full context of the problem? That's a lot of context to fit into a single error message in a way that's easy to understand for someone who doesn't have the related background information about the spec change. Having a concept of capabilities in the specification and documented somewhere would make it easier to explain what is happening and link to relevant information. But there are other possible solutions to this problem, for example having links to online error documentation somewhere.  \r\n\r\nThere is already a sort of concept of a \"feature table\" according to this [table](https:\/\/arrow.apache.org\/docs\/status.html#flight-sql) in the docs. I assume this is currently resolved via the `GetFlightDescriptor` call? Maybe versioning at the command level as an optional metadata field on the `FlightDescriptor` is an option instead? \r\n\r\nVersioning, either at the spec level or the command level, provides a straightforward error message. \"Your client cannot add parameters to this prepared statement because it uses version X of this command\/action\/spec instead of version Y\" This provides the necessary information to either research the specific root cause or come up with a solution (i.e. update your client)\r\n\r\n\r\n","Maybe the message could be \r\n\r\n\"Error: Your client either didn't send parameter values or doesn't support stateless prepared statements\" \r\n\r\nAnd then we could make sure the flight sql spec has a clearly labeled header \/ text \"stateless prepared statements\" that was easy for someone to discover when searching\r\n\r\n\ud83e\udd14 ","I suppose you could also introduce a separate endpoint for binding stateless prepared statements, which itself would be a kind of versioning. ","> I suppose you could also introduce a separate endpoint for binding stateless prepared statements, which itself would be a kind of versioning.\r\n\r\nIndeed, but it would likely also require more substantial changes to clients (which would now have to know how to try both endpoints \ud83e\udd14 )"],"labels":["Type: enhancement","Component: FlightRPC"]},{"title":"`Invalid metadata$r` warning when feeding parquet file into dplyr","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nHi\r\nI have a parquet file (https:\/\/www.dropbox.com\/scl\/fi\/lsg2xxe565dfa88e9plo4\/part-0.parquet?rlkey=3w2sjc6xewaz9lxd4cwcvf65b&dl=0) which is causing an `Invalid metadata$r` warning. It seems to be working fine, but the warning is annoying. \r\n\r\nThe file is written from R as part of a partitioning database, and the error occurs with others as well. Please find the code and the link to the file at the end.\r\n\r\n```\r\n> devtools::session_info()\r\n\u2500 Session info \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n setting  value\r\n version  R version 4.3.3 (2024-02-29)\r\n os       macOS Sonoma 14.4\r\n system   aarch64, darwin20\r\n ui       X11\r\n language (EN)\r\n collate  en_US.UTF-8\r\n ctype    en_US.UTF-8\r\n tz       Europe\/Zurich\r\n date     2024-03-08\r\n pandoc   3.1.12.2 @ \/opt\/homebrew\/bin\/pandoc\r\n\r\n\u2500 Packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n package     * version  date (UTC) lib source\r\n arrow       * 14.0.0.2 2023-12-02 [1] CRAN (R 4.3.1)\r\n assertthat    0.2.1    2019-03-21 [1] CRAN (R 4.3.0)\r\n bit           4.0.5    2022-11-15 [1] CRAN (R 4.3.0)\r\n bit64         4.0.5    2020-08-30 [1] CRAN (R 4.3.0)\r\n cachem        1.0.8    2023-05-01 [1] CRAN (R 4.3.0)\r\n cli           3.6.2    2023-12-11 [1] CRAN (R 4.3.1)\r\n devtools      2.4.5    2022-10-11 [1] CRAN (R 4.3.0)\r\n digest        0.6.34   2024-01-11 [1] CRAN (R 4.3.1)\r\n ellipsis      0.3.2    2021-04-29 [1] CRAN (R 4.3.0)\r\n fastmap       1.1.1    2023-02-24 [1] CRAN (R 4.3.0)\r\n fs            1.6.3    2023-07-20 [1] CRAN (R 4.3.0)\r\n glue          1.7.0    2024-01-09 [1] CRAN (R 4.3.1)\r\n htmltools     0.5.7    2023-11-03 [1] CRAN (R 4.3.1)\r\n htmlwidgets   1.6.4    2023-12-06 [1] CRAN (R 4.3.1)\r\n httpuv        1.6.14   2024-01-26 [1] CRAN (R 4.3.1)\r\n jsonlite      1.8.8    2023-12-04 [1] CRAN (R 4.3.1)\r\n later         1.3.2    2023-12-06 [1] CRAN (R 4.3.1)\r\n lifecycle     1.0.4    2023-11-07 [1] CRAN (R 4.3.1)\r\n magrittr      2.0.3    2022-03-30 [1] CRAN (R 4.3.0)\r\n memoise       2.0.1    2021-11-26 [1] CRAN (R 4.3.0)\r\n mime          0.12     2021-09-28 [1] CRAN (R 4.3.0)\r\n miniUI        0.1.1.1  2018-05-18 [1] CRAN (R 4.3.0)\r\n pkgbuild      1.4.3    2023-12-10 [1] CRAN (R 4.3.1)\r\n pkgload       1.3.4    2024-01-16 [1] CRAN (R 4.3.1)\r\n profvis       0.3.8    2023-05-02 [1] CRAN (R 4.3.0)\r\n promises      1.2.1    2023-08-10 [1] CRAN (R 4.3.0)\r\n purrr         1.0.2    2023-08-10 [1] CRAN (R 4.3.0)\r\n R6            2.5.1    2021-08-19 [1] CRAN (R 4.3.0)\r\n Rcpp          1.0.12   2024-01-09 [1] CRAN (R 4.3.1)\r\n remotes       2.4.2.1  2023-07-18 [1] CRAN (R 4.3.0)\r\n rlang         1.1.3    2024-01-10 [1] CRAN (R 4.3.1)\r\n sessioninfo   1.2.2    2021-12-06 [1] CRAN (R 4.3.0)\r\n shiny         1.8.0    2023-11-17 [1] CRAN (R 4.3.1)\r\n stringi       1.8.3    2023-12-11 [1] CRAN (R 4.3.1)\r\n stringr       1.5.1    2023-11-14 [1] CRAN (R 4.3.1)\r\n tidyselect    1.2.0    2022-10-10 [1] CRAN (R 4.3.0)\r\n urlchecker    1.0.1    2021-11-30 [1] CRAN (R 4.3.0)\r\n usethis       2.2.3    2024-02-19 [1] CRAN (R 4.3.1)\r\n vctrs         0.6.5    2023-12-01 [1] CRAN (R 4.3.1)\r\n xtable        1.8-4    2019-04-21 [1] CRAN (R 4.3.0)\r\n\r\n [1] \/Users\/rainerkrug\/R\/library\/aarch64-apple-darwin20\/4.3\r\n [2] \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\r\n\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```\r\n\r\n```\r\narrow::write_dataset(\r\n            data, \r\n            path = arrow_dir,\r\n            partitioning = \"publication_year\" ,\r\n            format = \"parquet\",\r\n            existing_data_behavior = \"overwrite\"\r\n        )\r\n```\r\n\r\n```\r\n> arrow::open_dataset(\".\/data\/corpus\/publication_year=1500\/part-0.parquet\") |> dplyr::group_by(author_abbr)\r\nFileSystemDataset (query)\r\nid: string\r\nauthor: string\r\nab: string\r\ndoi: string\r\ntopics: string\r\nauthor_abbr: string\r\n\r\n* Grouped by author_abbr\r\nSee $.data for the source Arrow object\r\nWarning message:\r\nInvalid metadata$r \r\n> \r\n```\r\n\r\nThe Parquet file can be downloaded from: https:\/\/www.dropbox.com\/scl\/fi\/lsg2xxe565dfa88e9plo4\/part-0.parquet?rlkey=3w2sjc6xewaz9lxd4cwcvf65b&dl=0\r\n\r\n### Component(s)\r\n\r\nR","comments":["Thanks @rkrug, I was able to reproduce the error with the data you provided. I'll have a look soon and report back."],"labels":["Type: bug","Component: R"]},{"title":"R: convert arrow table or data.frame to parquet raw vector","body":"\r\nI couldn't find it in the docs. Is it possible to convert arrow table to parquet raw vector without writing to disk? Or maybe it's possible to `write_parquet` directly to http url? Thank you in advance!\r\n\r\n### Component(s)\r\n\r\nR","comments":[],"labels":["Component: R","Type: usage"]},{"title":"[Python\/C++] Make upload size per part configurable when uploading to S3","body":"### Describe the enhancement requested\n\nCurrently, the size of every part in a multipart upload is hard-coded here: https:\/\/github.com\/apache\/arrow\/blob\/d2970e1d047f1bd31c31995c35450a7e5bfce3c0\/cpp\/src\/arrow\/filesystem\/s3fs.cc#L1394-L1400\r\n\r\nWe've run into issues where the request rate to s3 is too high when uploading form a bigger cluster and S3 errors. \r\n\r\n```\r\nOSError(\"When completing multiple part upload for key '***' in bucket '***': AWS Error SLOW_DOWN during CompleteMultipartUpload operation: Please reduce your request rate.\")\r\n```\r\n\r\nWe've tried alleviating this with different s3 bucket prefixes, but this didn't solve the problem completely.\r\n\r\nWould you be open to expose an option that makes the part size configurable so that we can configure the chunk size?\n\n### Component(s)\n\nC++, Python","comments":["Hi @phofl,\r\n\r\nThanks for bringing up this topic. This limitation of the fixed size parts also prevents uploading objects larger than 100GB which I think is quite low compared to the maximum object size(5 TB) supported by AWS S3. I would be very interested for a solution to this issue.\r\n\r\nMy preferred solution would be having the part size configurable per output stream. I guess that could still work for Cloudflare R2.\r\n\r\nBehcet"],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++] IpcWriter to memory stream race condition","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nArrow version: 15.0.1\r\nPlatform: x64-windows\r\n\r\nMy Acero use case:\r\n\r\ndataset `scan` node\r\n\\\\\/\r\nfilter\/projection (not relevant)\r\n\\\\\/\r\n`consuming_sink`\r\n\\\\\/\r\nCustom `SinkNodeConsumer` writting batches to `RecordBatchWriter`\r\n`RecordBatchWriter` is an `ipc::MakeStreamWriter` with `BufferOutputStream`\r\n\r\nThis use case creates multiple threads writting to `BufferOutputStream` which causes race in `BufferOutputStream::Write `\r\n\r\nSinkNodeConsumer:\r\n\r\n```\r\nstruct RecordBatchWriterNodeConsumer : public ac::SinkNodeConsumer {\r\n    RecordBatchWriterNodeConsumer(std::shared_ptr<ar::ipc::RecordBatchWriter> _writer)\r\n        : writer(_writer){}\r\n    arrow::Status Init(const std::shared_ptr<arrow::Schema>& _schema,\r\n        ac::BackpressureControl* backpressure_control,\r\n        ac::ExecPlan* plan) override {\r\n        schema = _schema;\r\n        return arrow::Status::OK();\r\n    }\r\n\r\n    arrow::Status Consume(cp::ExecBatch batch) override {\r\n        ARROW_ASSIGN_OR_RAISE(auto rb, batch.ToRecordBatch(schema));\r\n        \/\/const std::lock_guard<std::mutex> lock(consume_mutex);\r\n        return writer->WriteRecordBatch(*rb);\r\n    }\r\n\r\n    arrow::Future<> Finish() override {\r\n        return writer->Close();\r\n    }\r\n\r\n    std::shared_ptr<ar::ipc::RecordBatchWriter> writer;\r\n    std::shared_ptr<arrow::Schema> schema;\r\n    \/\/std::mutex consume_mutex;\r\n};\r\n```\r\n\r\nAddressSanitizer dump:\r\n\r\n```\r\n=================================================================\r\n==22728==ERROR: AddressSanitizer: attempting double-free on 0x038f001c4800 in thread T48:\r\n    #0 0x7ffc9ae00798 in _asan_wrap_RtlValidateHeap+0x288 (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x180040798)\r\n    #1 0x7ffc99f31a7b in arrow::BaseMemoryPoolImpl<arrow::`anonymous namespace'::SystemAllocator>::Reallocate C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:487\r\n    #2 0x7ffc99f321e7 in arrow::PoolBuffer::Resize C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:891\r\n    #3 0x7ffc99ff2db2 in arrow::io::BufferOutputStream::Reserve C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:123\r\n    #4 0x7ffc99ff3544 in arrow::io::BufferOutputStream::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:105\r\n    #5 0x7ffc99ff0770 in arrow::io::Writable::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\interfaces.cc:203\r\n    #6 0x7ffc9a9d7f93 in arrow::ipc::WriteIpcPayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:757\r\n    #7 0x7ffc9a9d82ab in arrow::ipc::internal::PayloadStreamWriter::WritePayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1406\r\n    #8 0x7ffc9a9d8756 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1195\r\n    #9 0x7ffc9a9d8375 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1176\r\n    #10 0x7ffcadac9af5 in RecordBatchWriterNodeConsumer::Consume C:\\Users\\USER\\project\\src\\mxp\\electron\\cmake\\src\\analysis\\PdwProcessing.cpp:358\r\n    #11 0x7ffcac3365e5 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::Process C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:399\r\n    #12 0x7ffcac334d67 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:390\r\n    #13 0x7ffcac32851a in arrow::acero::MapNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\map_node.cc:79\r\n    #14 0x7ffcac344069 in <lambda_7d84ce2741d5a383b7c7277f0b787d5c>::operator() C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\source_node.cc:157\r\n    #15 0x7ffcac349121 in std::_Func_impl_no_alloc<<lambda_7d84ce2741d5a383b7c7277f0b787d5c>,arrow::Status>::_Do_call C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:822\r\n    #16 0x7ffcac331b8f in std::_Call_binder<std::_Unforced,0,1,arrow::detail::ContinueFuture,std::tuple<arrow::Future<arrow::internal::Empty>,std::function<arrow::Status __cdecl(void)> >,std::tuple<> > C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:1307\r\n    #17 0x7ffcac33309d in arrow::internal::FnOnce<void __cdecl(void)>::FnImpl<std::_Binder<std::_Unforced,arrow::detail::ContinueFuture,arrow::Future<arrow::internal::Empty> &,std::function<arrow::Status __cdecl(void)> > >::invoke C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\util\\functional.h:152\r\n    #18 0x7ffc9a056b64 in std::thread::_Invoke<std::tuple<<lambda_72d791419a94ce2df79e1afeb11637d7> >,0> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread:55\r\n    #19 0x7ffd30571bb1 in configthreadlocale+0x91 (C:\\WINDOWS\\System32\\ucrtbase.dll+0x180021bb1)\r\n    #20 0x7ffc9ae0ebde in _asan_default_suppressions__dll+0x122e (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x18004ebde)\r\n    #21 0x7ffd32227343 in BaseThreadInitThunk+0x13 (C:\\WINDOWS\\System32\\KERNEL32.DLL+0x180017343)\r\n    #22 0x7ffd328226b0 in RtlUserThreadStart+0x20 (C:\\WINDOWS\\SYSTEM32\\ntdll.dll+0x1800526b0)\r\n\r\n0x038f001c4800 is located 0 bytes inside of 65536-byte region [0x038f001c4800,0x038f001d4800)\r\nfreed by thread T52 here:\r\n    #0 0x7ffc9ae00798 in _asan_wrap_RtlValidateHeap+0x288 (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x180040798)\r\n    #1 0x7ffc99f31a7b in arrow::BaseMemoryPoolImpl<arrow::`anonymous namespace'::SystemAllocator>::Reallocate C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:487\r\n    #2 0x7ffc99f321e7 in arrow::PoolBuffer::Resize C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:891\r\n    #3 0x7ffc99ff2db2 in arrow::io::BufferOutputStream::Reserve C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:123\r\n    #4 0x7ffc99ff3544 in arrow::io::BufferOutputStream::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:105\r\n    #5 0x7ffc99ff0770 in arrow::io::Writable::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\interfaces.cc:203\r\n    #6 0x7ffc9a9d7f93 in arrow::ipc::WriteIpcPayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:757\r\n    #7 0x7ffc9a9d82ab in arrow::ipc::internal::PayloadStreamWriter::WritePayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1406\r\n    #8 0x7ffc9a9d8756 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1195\r\n    #9 0x7ffc9a9d8375 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1176\r\n    #10 0x7ffcadac9af5 in RecordBatchWriterNodeConsumer::Consume C:\\Users\\USER\\project\\src\\mxp\\electron\\cmake\\src\\analysis\\PdwProcessing.cpp:358\r\n    #11 0x7ffcac3365e5 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::Process C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:399\r\n    #12 0x7ffcac334d67 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:390\r\n    #13 0x7ffcac32851a in arrow::acero::MapNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\map_node.cc:79\r\n    #14 0x7ffcac344069 in <lambda_7d84ce2741d5a383b7c7277f0b787d5c>::operator() C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\source_node.cc:157\r\n    #15 0x7ffcac349121 in std::_Func_impl_no_alloc<<lambda_7d84ce2741d5a383b7c7277f0b787d5c>,arrow::Status>::_Do_call C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:822\r\n    #16 0x7ffcac331b8f in std::_Call_binder<std::_Unforced,0,1,arrow::detail::ContinueFuture,std::tuple<arrow::Future<arrow::internal::Empty>,std::function<arrow::Status __cdecl(void)> >,std::tuple<> > C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:1307\r\n    #17 0x7ffcac33309d in arrow::internal::FnOnce<void __cdecl(void)>::FnImpl<std::_Binder<std::_Unforced,arrow::detail::ContinueFuture,arrow::Future<arrow::internal::Empty> &,std::function<arrow::Status __cdecl(void)> > >::invoke C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\util\\functional.h:152\r\n    #18 0x7ffc9a056b64 in std::thread::_Invoke<std::tuple<<lambda_72d791419a94ce2df79e1afeb11637d7> >,0> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread:55\r\n    #19 0x7ffd30571bb1 in configthreadlocale+0x91 (C:\\WINDOWS\\System32\\ucrtbase.dll+0x180021bb1)\r\n    #20 0x7ffc9ae0ebde in _asan_default_suppressions__dll+0x122e (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x18004ebde)\r\n    #21 0x7ffd32227343 in BaseThreadInitThunk+0x13 (C:\\WINDOWS\\System32\\KERNEL32.DLL+0x180017343)\r\n    #22 0x7ffd328226b0 in RtlUserThreadStart+0x20 (C:\\WINDOWS\\SYSTEM32\\ntdll.dll+0x1800526b0)\r\n\r\npreviously allocated by thread T52 here:\r\n    #0 0x7ffc9ae00a1a in _asan_wrap_RtlValidateHeap+0x50a (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x180040a1a)\r\n    #1 0x7ffc99f3197c in arrow::BaseMemoryPoolImpl<arrow::`anonymous namespace'::SystemAllocator>::Reallocate C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:487\r\n    #2 0x7ffc99f321e7 in arrow::PoolBuffer::Resize C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\memory_pool.cc:891\r\n    #3 0x7ffc99ff2db2 in arrow::io::BufferOutputStream::Reserve C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:123\r\n    #4 0x7ffc99ff3544 in arrow::io::BufferOutputStream::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\memory.cc:105\r\n    #5 0x7ffc99ff0770 in arrow::io::Writable::Write C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\io\\interfaces.cc:203\r\n    #6 0x7ffc9a9d7f93 in arrow::ipc::WriteIpcPayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:757\r\n    #7 0x7ffc9a9d82ab in arrow::ipc::internal::PayloadStreamWriter::WritePayload C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1406\r\n    #8 0x7ffc9a9d8756 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1195\r\n    #9 0x7ffc9a9d8375 in arrow::ipc::internal::IpcFormatWriter::WriteRecordBatch C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\ipc\\writer.cc:1176\r\n    #10 0x7ffcadac9af5 in RecordBatchWriterNodeConsumer::Consume C:\\Users\\USER\\project\\src\\mxp\\electron\\cmake\\src\\analysis\\PdwProcessing.cpp:358\r\n    #11 0x7ffcac3365e5 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::Process C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:399\r\n    #12 0x7ffcac334d67 in arrow::acero::`anonymous namespace'::ConsumingSinkNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\sink_node.cc:390\r\n    #13 0x7ffcac32851a in arrow::acero::MapNode::InputReceived C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\map_node.cc:79\r\n    #14 0x7ffcac344069 in <lambda_7d84ce2741d5a383b7c7277f0b787d5c>::operator() C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\acero\\source_node.cc:157\r\n    #15 0x7ffcac349121 in std::_Func_impl_no_alloc<<lambda_7d84ce2741d5a383b7c7277f0b787d5c>,arrow::Status>::_Do_call C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:822\r\n    #16 0x7ffcac331b8f in std::_Call_binder<std::_Unforced,0,1,arrow::detail::ContinueFuture,std::tuple<arrow::Future<arrow::internal::Empty>,std::function<arrow::Status __cdecl(void)> >,std::tuple<> > C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\functional:1307\r\n    #17 0x7ffcac33309d in arrow::internal::FnOnce<void __cdecl(void)>::FnImpl<std::_Binder<std::_Unforced,arrow::detail::ContinueFuture,arrow::Future<arrow::internal::Empty> &,std::function<arrow::Status __cdecl(void)> > >::invoke C:\\Users\\USER\\project\\vcpkg\\buildtrees\\arrow\\src\\e-arrow-15-f337ad0391.clean\\cpp\\src\\arrow\\util\\functional.h:152\r\n    #18 0x7ffc9a056b64 in std::thread::_Invoke<std::tuple<<lambda_72d791419a94ce2df79e1afeb11637d7> >,0> C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include\\thread:55\r\n    #19 0x7ffd30571bb1 in configthreadlocale+0x91 (C:\\WINDOWS\\System32\\ucrtbase.dll+0x180021bb1)\r\n    #20 0x7ffc9ae0ebde in _asan_default_suppressions__dll+0x122e (\\\\?\\C:\\Users\\USER\\project\\dist_MXP_web\\libs\\clang_rt.asan_dynamic-x86_64.dll+0x18004ebde)\r\n    #21 0x7ffd32227343 in BaseThreadInitThunk+0x13 (C:\\WINDOWS\\System32\\KERNEL32.DLL+0x180017343)\r\n    #22 0x7ffd328226b0 in RtlUserThreadStart+0x20 (C:\\WINDOWS\\SYSTEM32\\ntdll.dll+0x1800526b0)\r\n```\r\n\r\nThis can be fixed by adding lock in RecordBatchWriterNodeConsumer::Consume, however this not seem like a good solution. I think either:\r\narrow::io::BufferOutputStream::Write should have write lock\r\nor: \r\nStreamWriter should have a write lock (as it writes to stream)\r\nor maybe both?\r\n\r\n\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"[Dev] Use pre-commit-hook instead of \"archery lint\" for all lints","body":"### Describe the enhancement requested\r\n\r\nWe use `archery lint ...` to run linters. It's convenient because we don't need to know linters details but we need to install `archery[lint]` and we need to write Python code to add a new linter. They're a bit annoying. \r\n\r\nHow about using pre-commit https:\/\/pre-commit.com\/ for all linters? If we use pre-commit:\r\n\r\n* We don't need to write Python code for new linter and changing linter behavior (We need to write YAML instead)\r\n* C++: We don't need to install `clang-format-14` separately (pre-commit can do it automatically)\r\n* C++: We don't need to run CMake to run `clang-format-14`\r\n* We can get easy to read lint failure messages (`archery docker run ubuntu-lint` outputs many unrelated messages)\r\n\r\nNote that we already have some pre-commit configuration: https:\/\/github.com\/apache\/arrow\/blob\/main\/.pre-commit-config.yaml\r\n\r\nSub issues:\r\n\r\n* #40448\r\n* #40544\r\n* #40586\r\n* #35941\r\n\r\n### Component(s)\r\n\r\nDeveloper Tools","comments":["+1 for pre-commit, which is at least reasonably common across projects these days IMO"],"labels":["Type: enhancement","Component: Developer Tools"]},{"title":"GH-40415: [C++] Extract the primitive operations implementing Take and make them instantiable in multiple scenarios","body":"### Rationale for this change\r\n\r\nThis can help fixing #39798 and #39566, but is also improving the performance of `Take` and `Filter` (specially in the no-nulls case).\r\n\r\n### What changes are included in this PR?\r\n\r\n - Addition of `OptionalValidity<>` \u2014 a template class that allows tracking the presence of the validity bitmap at the type level\r\n - Introduce the Gather class helper which uses `OptionalValidity<>` to unlock type-safe optimization\r\n - Specialized `Take` implementation for values\/indices without nulls\r\n - Unify the boolean and numeric implementations of `Take` in terms of the `Gather` template\r\n - Skip validity bitmap allocation when inputs (values and indices) have no nulls\r\n\r\n### Are these changes tested?\r\n\r\n - Existing tests\r\n - New test assertions that check that `Take` guarantees null values are zeroed out\r\n\n* GitHub Issue: #40415","comments":[":warning: GitHub issue #40415 **has been automatically assigned in GitHub** to PR creator.","@ursabot please benchmark","Benchmark runs are scheduled for commit d972946c31d22c710310ec5a914753bcaaefb4c1. Watch https:\/\/buildkite.com\/apache-arrow and https:\/\/conbench.ursa.dev for updates. A comment will be posted here when the runs are complete.","Results of benchmarks I ran on my laptop:\r\n\r\n`archery benchmark diff --benchmark-filter=\"(Filter(Int64|Bool|Primi))|(Take(Int64|Chunked))\"`\r\n\r\nMy goal was to factor out the abstractions for use in multiple contexts, but the performance results are very good.\r\n\r\n<details><summary>Details<\/summary>\r\n<p>\r\n\r\n```\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nNon-regressions: (37)\r\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                                   benchmark           baseline          contender  change %                                                                                                                                                                                                                                    counters\r\n          FilterInt64FilterNoNulls\/4194304\/2     16.507 GiB\/sec     61.929 GiB\/sec   275.178      {'family_index': 0, 'per_family_instance_index': 2, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2966, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n          FilterInt64FilterNoNulls\/4194304\/1      1.671 GiB\/sec      2.722 GiB\/sec    62.875      {'family_index': 0, 'per_family_instance_index': 1, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 299, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n          FilterInt64FilterNoNulls\/4194304\/0      9.188 GiB\/sec     10.926 GiB\/sec    18.906     {'family_index': 0, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1634, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n         TakeInt64MonotonicIndices\/4194304\/0 769.298M items\/sec 884.929M items\/sec    15.031                                       {'family_index': 4, 'per_family_instance_index': 4, 'run_name': 'TakeInt64MonotonicIndices\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 129, 'null_percent': 0.0}\r\n         FilterInt64FilterNoNulls\/4194304\/14     14.655 GiB\/sec     16.258 GiB\/sec    10.940   {'family_index': 0, 'per_family_instance_index': 14, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2640, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n         FilterInt64FilterNoNulls\/4194304\/11     14.705 GiB\/sec     16.270 GiB\/sec    10.646   {'family_index': 0, 'per_family_instance_index': 11, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2636, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n          FilterInt64FilterNoNulls\/4194304\/3      8.576 GiB\/sec      8.977 GiB\/sec     4.672     {'family_index': 0, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1546, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n         TakeInt64MonotonicIndices\/4194304\/2 227.037M items\/sec 236.976M items\/sec     4.378                                       {'family_index': 4, 'per_family_instance_index': 2, 'run_name': 'TakeInt64MonotonicIndices\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 38, 'null_percent': 50.0}\r\n       FilterInt64FilterWithNulls\/4194304\/11      9.845 GiB\/sec     10.155 GiB\/sec     3.147 {'family_index': 1, 'per_family_instance_index': 11, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1762, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n       FilterInt64FilterWithNulls\/4194304\/14      9.827 GiB\/sec     10.136 GiB\/sec     3.143 {'family_index': 1, 'per_family_instance_index': 14, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1759, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n         TakeInt64MonotonicIndices\/4194304\/1 788.652M items\/sec 810.698M items\/sec     2.795                                     {'family_index': 4, 'per_family_instance_index': 3, 'run_name': 'TakeInt64MonotonicIndices\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 132, 'null_percent': 100.0}\r\n   TakeInt64RandomIndicesWithNulls\/4194304\/2 101.947M items\/sec 104.586M items\/sec     2.589                                 {'family_index': 3, 'per_family_instance_index': 2, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 17, 'null_percent': 50.0}\r\n        FilterInt64FilterWithNulls\/4194304\/5      9.986 GiB\/sec     10.238 GiB\/sec     2.521    {'family_index': 1, 'per_family_instance_index': 5, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1789, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n  TakeInt64RandomIndicesWithNulls\/4194304\/10 103.565M items\/sec 106.136M items\/sec     2.483                                {'family_index': 3, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 17, 'null_percent': 10.0}\r\n          FilterInt64FilterNoNulls\/4194304\/8     15.003 GiB\/sec     15.372 GiB\/sec     2.463      {'family_index': 0, 'per_family_instance_index': 8, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2681, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n        FilterInt64FilterWithNulls\/4194304\/8      9.684 GiB\/sec      9.907 GiB\/sec     2.304    {'family_index': 1, 'per_family_instance_index': 8, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1732, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n        FilterInt64FilterWithNulls\/4194304\/2      9.990 GiB\/sec     10.174 GiB\/sec     1.844    {'family_index': 1, 'per_family_instance_index': 2, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1790, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\nTakeInt64RandomIndicesWithNulls\/4194304\/1000 126.972M items\/sec 129.189M items\/sec     1.746                               {'family_index': 3, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21, 'null_percent': 0.1}\r\n    TakeInt64RandomIndicesNoNulls\/4194304\/10 131.612M items\/sec 133.602M items\/sec     1.512                                  {'family_index': 2, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'null_percent': 10.0}\r\n      TakeInt64MonotonicIndices\/4194304\/1000 345.231M items\/sec 349.957M items\/sec     1.369                                     {'family_index': 4, 'per_family_instance_index': 0, 'run_name': 'TakeInt64MonotonicIndices\/4194304\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 58, 'null_percent': 0.1}\r\n     TakeInt64RandomIndicesNoNulls\/4194304\/2 150.052M items\/sec 152.070M items\/sec     1.345                                   {'family_index': 2, 'per_family_instance_index': 2, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 25, 'null_percent': 50.0}\r\n          FilterInt64FilterNoNulls\/4194304\/6      7.164 GiB\/sec      7.253 GiB\/sec     1.233     {'family_index': 0, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1316, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n        TakeInt64MonotonicIndices\/4194304\/10 347.102M items\/sec 351.008M items\/sec     1.125                                      {'family_index': 4, 'per_family_instance_index': 1, 'run_name': 'TakeInt64MonotonicIndices\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 58, 'null_percent': 10.0}\r\n  TakeInt64RandomIndicesNoNulls\/4194304\/1000 126.014M items\/sec 127.032M items\/sec     0.808                                 {'family_index': 2, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21, 'null_percent': 0.1}\r\n        FilterInt64FilterWithNulls\/4194304\/4      1.385 GiB\/sec      1.394 GiB\/sec     0.699    {'family_index': 1, 'per_family_instance_index': 4, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 251, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n   TakeInt64RandomIndicesWithNulls\/4194304\/0 337.964M items\/sec 340.261M items\/sec     0.680                                  {'family_index': 3, 'per_family_instance_index': 4, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 56, 'null_percent': 0.0}\r\n     TakeInt64RandomIndicesNoNulls\/4194304\/1   1.171G items\/sec   1.178G items\/sec     0.633                                 {'family_index': 2, 'per_family_instance_index': 3, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 196, 'null_percent': 100.0}\r\n   TakeInt64RandomIndicesWithNulls\/4194304\/1   5.113G items\/sec   5.143G items\/sec     0.578                               {'family_index': 3, 'per_family_instance_index': 3, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 859, 'null_percent': 100.0}\r\n          FilterInt64FilterNoNulls\/4194304\/5     16.210 GiB\/sec     16.232 GiB\/sec     0.135      {'family_index': 0, 'per_family_instance_index': 5, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2910, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n     TakeInt64RandomIndicesNoNulls\/4194304\/0 335.021M items\/sec 334.969M items\/sec    -0.016                                    {'family_index': 2, 'per_family_instance_index': 4, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 56, 'null_percent': 0.0}\r\n        FilterInt64FilterWithNulls\/4194304\/1      1.388 GiB\/sec      1.387 GiB\/sec    -0.127    {'family_index': 1, 'per_family_instance_index': 1, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 248, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n        FilterInt64FilterWithNulls\/4194304\/7      1.340 GiB\/sec      1.330 GiB\/sec    -0.694    {'family_index': 1, 'per_family_instance_index': 7, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 239, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n          FilterInt64FilterNoNulls\/4194304\/9      6.934 GiB\/sec      6.883 GiB\/sec    -0.733    {'family_index': 0, 'per_family_instance_index': 9, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1234, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n         FilterInt64FilterNoNulls\/4194304\/12      6.757 GiB\/sec      6.626 GiB\/sec    -1.936  {'family_index': 0, 'per_family_instance_index': 12, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1211, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n          FilterInt64FilterNoNulls\/4194304\/4      1.667 GiB\/sec      1.623 GiB\/sec    -2.668      {'family_index': 0, 'per_family_instance_index': 4, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 297, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n       FilterInt64FilterWithNulls\/4194304\/13      1.339 GiB\/sec      1.292 GiB\/sec    -3.492 {'family_index': 1, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 240, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n       FilterInt64FilterWithNulls\/4194304\/10      1.341 GiB\/sec      1.282 GiB\/sec    -4.405 {'family_index': 1, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 239, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nRegressions: (8)\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                            benchmark      baseline     contender  change %                                                                                                                                                                                                                                    counters\r\n   FilterInt64FilterNoNulls\/4194304\/7 1.601 GiB\/sec 1.521 GiB\/sec    -5.006      {'family_index': 0, 'per_family_instance_index': 7, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 290, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\nFilterInt64FilterWithNulls\/4194304\/12 2.593 GiB\/sec 2.409 GiB\/sec    -7.114 {'family_index': 1, 'per_family_instance_index': 12, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 478, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterInt64FilterWithNulls\/4194304\/9 2.601 GiB\/sec 2.408 GiB\/sec    -7.395   {'family_index': 1, 'per_family_instance_index': 9, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 465, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n  FilterInt64FilterNoNulls\/4194304\/10 1.583 GiB\/sec 1.463 GiB\/sec    -7.562   {'family_index': 0, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 282, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n  FilterInt64FilterNoNulls\/4194304\/13 1.586 GiB\/sec 1.462 GiB\/sec    -7.785   {'family_index': 0, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 283, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n FilterInt64FilterWithNulls\/4194304\/6 2.758 GiB\/sec 2.440 GiB\/sec   -11.514    {'family_index': 1, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 491, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterInt64FilterWithNulls\/4194304\/0 2.964 GiB\/sec 2.532 GiB\/sec   -14.581    {'family_index': 1, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 525, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterInt64FilterWithNulls\/4194304\/3 2.963 GiB\/sec 2.520 GiB\/sec   -14.939    {'family_index': 1, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 532, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n ```\r\n \r\n<\/p>\r\n<\/details> ","Thanks for your patience. Conbench analyzed the 7 benchmarking runs that have been run so far on PR commit d972946c31d22c710310ec5a914753bcaaefb4c1.\n\nThere were 21 benchmark results indicating a performance regression:\n\n- Pull Request Run on `ursa-i9-9960x` at [2024-03-08 05:04:48Z](https:\/\/conbench.ursa.dev\/compare\/runs\/f896633a158f4f22adae816cd86d1fe4...bf5381dae48b44d2a8a7bd603fc7bacf\/)\n  - [`tpch` (R) with engine=arrow, format=native, language=R, memory_map=False, query_id=TPCH-06, scale_factor=1](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065e93587e177bf48000368e890107fb...065eab083b607c1e800041bc03486ada)\n  - [`tpch` (R) with engine=arrow, format=parquet, language=R, memory_map=False, query_id=TPCH-02, scale_factor=1](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065e93462d3a7853800025ae91af3abd...065eaaf4dd62781a8000ce271595dd19)\n- and 19 more (see the report linked below)\n\nThe [full Conbench report](https:\/\/github.com\/apache\/arrow\/runs\/22428450324) has more details.","Ran the benchmarks locally (gcc 12.3.0, AMD Ryzen 9 3900X). There are a number of improved benchmarks and also a couple sizable regressions:\r\n```\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nNon-regressions: (368)\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                                                 benchmark           baseline          contender  change %                                                                                                                                                                                                                                                                                counters\r\n             TakeChunkedFlatInt64MonotonicIndices\/524288\/0 189.336M items\/sec 689.817M items\/sec   264.335                                                                        {'family_index': 33, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedFlatInt64MonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 253, 'null_percent': 0.0}\r\n         TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/0 169.936M items\/sec 552.985M items\/sec   225.408                                                                    {'family_index': 31, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 226, 'null_percent': 0.0}\r\n             TakeChunkedFlatInt64MonotonicIndices\/524288\/1 193.174M items\/sec 576.805M items\/sec   198.593                                                                      {'family_index': 33, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedFlatInt64MonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 258, 'null_percent': 100.0}\r\n         TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/1 187.343M items\/sec 531.243M items\/sec   183.567                                                                  {'family_index': 31, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 249, 'null_percent': 100.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/0\/8 160.887M items\/sec 453.258M items\/sec   181.725                                              {'family_index': 25, 'per_family_instance_index': 8, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 215, 'byte_width': 8.0, 'null_percent': 0.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1\/8 171.711M items\/sec 423.873M items\/sec   146.852                                            {'family_index': 25, 'per_family_instance_index': 6, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 229, 'byte_width': 8.0, 'null_percent': 100.0}\r\n          TakeChunkedFlatInt64MonotonicIndices\/524288\/1000 159.198M items\/sec 368.108M items\/sec   131.227                                                                     {'family_index': 33, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedFlatInt64MonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 213, 'null_percent': 0.1}\r\n    TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/1000 124.191M items\/sec 262.363M items\/sec   111.257                                                               {'family_index': 32, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 166, 'null_percent': 0.1}\r\n            TakeChunkedFlatInt64MonotonicIndices\/524288\/10 145.143M items\/sec 305.834M items\/sec   110.712                                                                      {'family_index': 33, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedFlatInt64MonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 194, 'null_percent': 10.0}\r\n        TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/10 122.809M items\/sec 242.708M items\/sec    97.630                                                                  {'family_index': 31, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 321, 'null_percent': 10.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/0\/9  99.879M items\/sec 186.926M items\/sec    87.153                                              {'family_index': 25, 'per_family_instance_index': 9, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 133, 'byte_width': 9.0, 'null_percent': 0.0}\r\n             TakeChunkedFlatInt64MonotonicIndices\/524288\/2 122.268M items\/sec 219.657M items\/sec    79.652                                                                       {'family_index': 33, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedFlatInt64MonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 166, 'null_percent': 50.0}\r\n      TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/10 101.269M items\/sec 173.332M items\/sec    71.160                                                                {'family_index': 32, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 135, 'null_percent': 10.0}\r\n                           FilterRecordBatchWithNulls\/50\/1      4.010 GiB\/sec      6.736 GiB\/sec    67.991     {'family_index': 9, 'per_family_instance_index': 16, 'run_name': 'FilterRecordBatchWithNulls\/50\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 79, 'data null%': 0.0, 'extracted_size': 37950000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1\/9  98.984M items\/sec 162.975M items\/sec    64.647                                            {'family_index': 25, 'per_family_instance_index': 7, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 129, 'byte_width': 9.0, 'null_percent': 100.0}\r\n                             FilterRecordBatchNoNulls\/50\/7      4.757 GiB\/sec      7.724 GiB\/sec    62.367       {'family_index': 8, 'per_family_instance_index': 22, 'run_name': 'FilterRecordBatchNoNulls\/50\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 91, 'data null%': 1.0, 'extracted_size': 39935600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n         TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/2 102.450M items\/sec 166.201M items\/sec    62.227                                                                   {'family_index': 31, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 137, 'null_percent': 50.0}\r\n                          FilterRecordBatchWithNulls\/100\/4      5.615 GiB\/sec      8.398 GiB\/sec    49.548  {'family_index': 9, 'per_family_instance_index': 34, 'run_name': 'FilterRecordBatchWithNulls\/100\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 112, 'data null%': 0.1, 'extracted_size': 37841600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n       TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/2  76.751M items\/sec 111.961M items\/sec    45.875                                                                 {'family_index': 32, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 102, 'null_percent': 50.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/2\/9  63.716M items\/sec  91.090M items\/sec    42.963                                              {'family_index': 25, 'per_family_instance_index': 5, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 86, 'byte_width': 9.0, 'null_percent': 50.0}\r\n                          FilterRecordBatchWithNulls\/100\/7      5.930 GiB\/sec      8.406 GiB\/sec    41.756  {'family_index': 9, 'per_family_instance_index': 37, 'run_name': 'FilterRecordBatchWithNulls\/100\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 106, 'data null%': 1.0, 'extracted_size': 37841600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                         FilterRecordBatchWithNulls\/100\/13      4.018 GiB\/sec      5.622 GiB\/sec    39.918 {'family_index': 9, 'per_family_instance_index': 43, 'run_name': 'FilterRecordBatchWithNulls\/100\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 77, 'data null%': 90.0, 'extracted_size': 37841600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                           FilterRecordBatchWithNulls\/50\/4      3.859 GiB\/sec      5.187 GiB\/sec    34.404     {'family_index': 9, 'per_family_instance_index': 19, 'run_name': 'FilterRecordBatchWithNulls\/50\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 76, 'data null%': 0.1, 'extracted_size': 37950000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1000\/8 116.086M items\/sec 150.911M items\/sec    29.999                                         {'family_index': 26, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 155, 'byte_width': 8.0, 'null_percent': 0.1}\r\n                          FilterRecordBatchWithNulls\/50\/13      3.341 GiB\/sec      4.330 GiB\/sec    29.626   {'family_index': 9, 'per_family_instance_index': 28, 'run_name': 'FilterRecordBatchWithNulls\/50\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 66, 'data null%': 90.0, 'extracted_size': 37950000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n                            FilterRecordBatchNoNulls\/100\/7      4.709 GiB\/sec      6.090 GiB\/sec    29.306     {'family_index': 8, 'per_family_instance_index': 37, 'run_name': 'FilterRecordBatchNoNulls\/100\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 88, 'data null%': 1.0, 'extracted_size': 39840800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                             FilterRecordBatchNoNulls\/50\/1      3.518 GiB\/sec      4.371 GiB\/sec    24.244       {'family_index': 8, 'per_family_instance_index': 16, 'run_name': 'FilterRecordBatchNoNulls\/50\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 63, 'data null%': 0.0, 'extracted_size': 39935600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n                            FilterRecordBatchNoNulls\/100\/6      3.135 GiB\/sec      3.866 GiB\/sec    23.341     {'family_index': 8, 'per_family_instance_index': 36, 'run_name': 'FilterRecordBatchNoNulls\/100\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 30, 'data null%': 1.0, 'extracted_size': 79928800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                           FilterRecordBatchWithNulls\/50\/6      3.218 GiB\/sec      3.810 GiB\/sec    18.399     {'family_index': 9, 'per_family_instance_index': 21, 'run_name': 'FilterRecordBatchWithNulls\/50\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 31, 'data null%': 1.0, 'extracted_size': 75932400.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\n                            FilterRecordBatchNoNulls\/100\/1      4.353 GiB\/sec      5.149 GiB\/sec    18.274     {'family_index': 8, 'per_family_instance_index': 31, 'run_name': 'FilterRecordBatchNoNulls\/100\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 79, 'data null%': 0.0, 'extracted_size': 39840800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                         FilterRecordBatchWithNulls\/100\/12      3.360 GiB\/sec      3.951 GiB\/sec    17.581 {'family_index': 9, 'per_family_instance_index': 42, 'run_name': 'FilterRecordBatchWithNulls\/100\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 90.0, 'extracted_size': 75973600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/0\/8 789.348M items\/sec 926.435M items\/sec    17.367                                                   {'family_index': 15, 'per_family_instance_index': 8, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1057, 'byte_width': 8.0, 'null_percent': 0.0}\r\n                          FilterRecordBatchWithNulls\/100\/3      3.432 GiB\/sec      4.017 GiB\/sec    17.031   {'family_index': 9, 'per_family_instance_index': 33, 'run_name': 'FilterRecordBatchWithNulls\/100\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 34, 'data null%': 0.1, 'extracted_size': 75973600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n          TakeChunkedChunkedInt64MonotonicIndices\/524288\/0 464.239M items\/sec 540.673M items\/sec    16.464                                                                     {'family_index': 24, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedInt64MonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 620, 'null_percent': 0.0}\r\n                        TakeInt64MonotonicIndices\/524288\/0 782.418M items\/sec 903.981M items\/sec    15.537                                                                                  {'family_index': 12, 'per_family_instance_index': 4, 'run_name': 'TakeInt64MonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1052, 'null_percent': 0.0}\r\n                             FilterRecordBatchNoNulls\/50\/0      3.049 GiB\/sec      3.453 GiB\/sec    13.241       {'family_index': 8, 'per_family_instance_index': 15, 'run_name': 'FilterRecordBatchNoNulls\/50\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 29, 'data null%': 0.0, 'extracted_size': 79930000.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n                            FilterRecordBatchNoNulls\/100\/3      3.405 GiB\/sec      3.814 GiB\/sec    12.025     {'family_index': 8, 'per_family_instance_index': 33, 'run_name': 'FilterRecordBatchNoNulls\/100\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 30, 'data null%': 0.1, 'extracted_size': 79928800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                         FilterRecordBatchWithNulls\/100\/11      2.036 GiB\/sec      2.276 GiB\/sec    11.795  {'family_index': 9, 'per_family_instance_index': 41, 'run_name': 'FilterRecordBatchWithNulls\/100\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1977, 'data null%': 10.0, 'extracted_size': 776800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                           FilterRecordBatchNoNulls\/100\/11      2.549 GiB\/sec      2.845 GiB\/sec    11.623    {'family_index': 8, 'per_family_instance_index': 41, 'run_name': 'FilterRecordBatchNoNulls\/100\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2324, 'data null%': 10.0, 'extracted_size': 827200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                           FilterRecordBatchWithNulls\/50\/0      3.146 GiB\/sec      3.501 GiB\/sec    11.297     {'family_index': 9, 'per_family_instance_index': 15, 'run_name': 'FilterRecordBatchWithNulls\/50\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 31, 'data null%': 0.0, 'extracted_size': 75932400.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/0\/9 252.049M items\/sec 279.188M items\/sec    10.768                                                    {'family_index': 15, 'per_family_instance_index': 9, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 338, 'byte_width': 9.0, 'null_percent': 0.0}\r\n                          FilterRecordBatchWithNulls\/100\/9      3.553 GiB\/sec      3.931 GiB\/sec    10.630  {'family_index': 9, 'per_family_instance_index': 39, 'run_name': 'FilterRecordBatchWithNulls\/100\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 10.0, 'extracted_size': 75973600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                          FilterRecordBatchWithNulls\/100\/2      2.065 GiB\/sec      2.285 GiB\/sec    10.613    {'family_index': 9, 'per_family_instance_index': 32, 'run_name': 'FilterRecordBatchWithNulls\/100\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1982, 'data null%': 0.0, 'extracted_size': 776800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                            FilterRecordBatchNoNulls\/100\/5      2.532 GiB\/sec      2.799 GiB\/sec    10.527      {'family_index': 8, 'per_family_instance_index': 35, 'run_name': 'FilterRecordBatchNoNulls\/100\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2229, 'data null%': 0.1, 'extracted_size': 827200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                          FilterRecordBatchWithNulls\/100\/8      2.044 GiB\/sec      2.250 GiB\/sec    10.116    {'family_index': 9, 'per_family_instance_index': 38, 'run_name': 'FilterRecordBatchWithNulls\/100\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1984, 'data null%': 1.0, 'extracted_size': 776800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                           FilterRecordBatchNoNulls\/100\/14      2.550 GiB\/sec      2.803 GiB\/sec     9.920    {'family_index': 8, 'per_family_instance_index': 44, 'run_name': 'FilterRecordBatchNoNulls\/100\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2322, 'data null%': 90.0, 'extracted_size': 827200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                            FilterRecordBatchNoNulls\/100\/2      2.541 GiB\/sec      2.789 GiB\/sec     9.776      {'family_index': 8, 'per_family_instance_index': 32, 'run_name': 'FilterRecordBatchNoNulls\/100\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2235, 'data null%': 0.0, 'extracted_size': 827200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 1.0}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/1\/9 144.794M items\/sec 158.852M items\/sec     9.709                                                {'family_index': 27, 'per_family_instance_index': 7, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 193, 'byte_width': 9.0, 'null_percent': 100.0}\r\n                         FilterRecordBatchWithNulls\/100\/14      2.047 GiB\/sec      2.244 GiB\/sec     9.633  {'family_index': 9, 'per_family_instance_index': 44, 'run_name': 'FilterRecordBatchWithNulls\/100\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2000, 'data null%': 90.0, 'extracted_size': 776800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                            FilterRecordBatchNoNulls\/100\/8      2.555 GiB\/sec      2.795 GiB\/sec     9.371      {'family_index': 8, 'per_family_instance_index': 38, 'run_name': 'FilterRecordBatchNoNulls\/100\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2323, 'data null%': 1.0, 'extracted_size': 827200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 1.0}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/0\/9 201.390M items\/sec 219.792M items\/sec     9.138                                                  {'family_index': 27, 'per_family_instance_index': 9, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 269, 'byte_width': 9.0, 'null_percent': 0.0}\r\n                            FilterRecordBatchNoNulls\/50\/11      3.398 GiB\/sec      3.694 GiB\/sec     8.694      {'family_index': 8, 'per_family_instance_index': 26, 'run_name': 'FilterRecordBatchNoNulls\/50\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3069, 'data null%': 10.0, 'extracted_size': 831200.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 1.0}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/0\/8 472.590M items\/sec 512.743M items\/sec     8.496                                                  {'family_index': 27, 'per_family_instance_index': 8, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 630, 'byte_width': 8.0, 'null_percent': 0.0}\r\n                            FilterRecordBatchNoNulls\/10\/11      2.841 GiB\/sec      3.074 GiB\/sec     8.222      {'family_index': 8, 'per_family_instance_index': 11, 'run_name': 'FilterRecordBatchNoNulls\/10\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2751, 'data null%': 10.0, 'extracted_size': 811840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                          FilterRecordBatchWithNulls\/100\/0      3.730 GiB\/sec      4.032 GiB\/sec     8.080   {'family_index': 9, 'per_family_instance_index': 30, 'run_name': 'FilterRecordBatchWithNulls\/100\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 0.0, 'extracted_size': 75973600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                            FilterRecordBatchNoNulls\/100\/0      3.374 GiB\/sec      3.638 GiB\/sec     7.827     {'family_index': 8, 'per_family_instance_index': 30, 'run_name': 'FilterRecordBatchNoNulls\/100\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 29, 'data null%': 0.0, 'extracted_size': 79928800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                          FilterRecordBatchWithNulls\/100\/5      2.124 GiB\/sec      2.288 GiB\/sec     7.769    {'family_index': 9, 'per_family_instance_index': 35, 'run_name': 'FilterRecordBatchWithNulls\/100\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2055, 'data null%': 0.1, 'extracted_size': 776800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 1.0}\r\n                             FilterRecordBatchNoNulls\/10\/5      2.813 GiB\/sec      3.029 GiB\/sec     7.672         {'family_index': 8, 'per_family_instance_index': 5, 'run_name': 'FilterRecordBatchNoNulls\/10\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2792, 'data null%': 0.1, 'extracted_size': 811840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/2\/8     72.009 GiB\/sec     77.451 GiB\/sec     7.557                  {'family_index': 2, 'per_family_instance_index': 4, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 103000, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n    TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/0 418.965M items\/sec 450.615M items\/sec     7.554                                                               {'family_index': 23, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 559, 'null_percent': 0.0}\r\n      TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/0 428.137M items\/sec 459.524M items\/sec     7.331                                                                 {'family_index': 22, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 565, 'null_percent': 0.0}\r\n                          FilterRecordBatchWithNulls\/50\/14      2.156 GiB\/sec      2.314 GiB\/sec     7.321    {'family_index': 9, 'per_family_instance_index': 29, 'run_name': 'FilterRecordBatchWithNulls\/50\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2049, 'data null%': 90.0, 'extracted_size': 790800.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 1.0}\r\n                             FilterRecordBatchNoNulls\/50\/5      3.402 GiB\/sec      3.651 GiB\/sec     7.319        {'family_index': 8, 'per_family_instance_index': 20, 'run_name': 'FilterRecordBatchNoNulls\/50\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3081, 'data null%': 0.1, 'extracted_size': 831200.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 1.0}\r\n                      TakeStringMonotonicIndices\/524288\/10  72.856M items\/sec  78.182M items\/sec     7.310                                                                                 {'family_index': 21, 'per_family_instance_index': 1, 'run_name': 'TakeStringMonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 98, 'null_percent': 10.0}\r\n                             FilterRecordBatchNoNulls\/50\/3      3.189 GiB\/sec      3.417 GiB\/sec     7.172       {'family_index': 8, 'per_family_instance_index': 18, 'run_name': 'FilterRecordBatchNoNulls\/50\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 30, 'data null%': 0.1, 'extracted_size': 79930000.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n                             FilterRecordBatchNoNulls\/50\/2      3.420 GiB\/sec      3.654 GiB\/sec     6.819        {'family_index': 8, 'per_family_instance_index': 17, 'run_name': 'FilterRecordBatchNoNulls\/50\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3062, 'data null%': 0.0, 'extracted_size': 831200.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 1.0}\r\n         TakeFixedSizeBinaryMonotonicIndices\/524288\/1000\/9 185.020M items\/sec 197.550M items\/sec     6.772                                                 {'family_index': 15, 'per_family_instance_index': 1, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 247, 'byte_width': 9.0, 'null_percent': 0.1}\r\n                             FilterRecordBatchNoNulls\/50\/8      3.387 GiB\/sec      3.607 GiB\/sec     6.514        {'family_index': 8, 'per_family_instance_index': 23, 'run_name': 'FilterRecordBatchNoNulls\/50\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3083, 'data null%': 1.0, 'extracted_size': 831200.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 1.0}\r\n   TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/0  29.992M items\/sec  31.893M items\/sec     6.335                                                               {'family_index': 29, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 40, 'null_percent': 0.0}\r\n       TakeChunkedChunkedFSBMonotonicIndices\/524288\/1000\/9 154.495M items\/sec 164.233M items\/sec     6.303                                               {'family_index': 27, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 207, 'byte_width': 9.0, 'null_percent': 0.1}\r\n                       TakeStringMonotonicIndices\/524288\/2  49.626M items\/sec  52.657M items\/sec     6.108                                                                                  {'family_index': 21, 'per_family_instance_index': 2, 'run_name': 'TakeStringMonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 66, 'null_percent': 50.0}\r\n                TakeFSLInt64RandomIndicesNoNulls\/524288\/10  62.614M items\/sec  66.407M items\/sec     6.057                                                                           {'family_index': 16, 'per_family_instance_index': 1, 'run_name': 'TakeFSLInt64RandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 84, 'null_percent': 10.0}\r\n            TakeFSLInt64RandomIndicesWithNulls\/524288\/1000  73.663M items\/sec  78.054M items\/sec     5.961                                                                        {'family_index': 17, 'per_family_instance_index': 0, 'run_name': 'TakeFSLInt64RandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 99, 'null_percent': 0.1}\r\n         TakeChunkedChunkedFSBMonotonicIndices\/524288\/10\/9 136.878M items\/sec 144.908M items\/sec     5.866                                                {'family_index': 27, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 183, 'byte_width': 9.0, 'null_percent': 10.0}\r\n                            FilterRecordBatchNoNulls\/50\/14      3.414 GiB\/sec      3.613 GiB\/sec     5.826      {'family_index': 8, 'per_family_instance_index': 29, 'run_name': 'FilterRecordBatchNoNulls\/50\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3101, 'data null%': 90.0, 'extracted_size': 831200.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 1.0}\r\n         TakeChunkedChunkedFSBMonotonicIndices\/524288\/10\/8 257.208M items\/sec 272.156M items\/sec     5.812                                                {'family_index': 27, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 344, 'byte_width': 8.0, 'null_percent': 10.0}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/1\/9 188.515M items\/sec 199.330M items\/sec     5.737                                                  {'family_index': 15, 'per_family_instance_index': 7, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 252, 'byte_width': 9.0, 'null_percent': 100.0}\r\n              TakeFSLInt64RandomIndicesNoNulls\/524288\/1000  74.033M items\/sec  78.255M items\/sec     5.704                                                                          {'family_index': 16, 'per_family_instance_index': 0, 'run_name': 'TakeFSLInt64RandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 99, 'null_percent': 0.1}\r\n                            FilterRecordBatchNoNulls\/50\/12      2.974 GiB\/sec      3.143 GiB\/sec     5.699     {'family_index': 8, 'per_family_instance_index': 27, 'run_name': 'FilterRecordBatchNoNulls\/50\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 28, 'data null%': 90.0, 'extracted_size': 79930000.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n   TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1000\/8 281.490M items\/sec 297.503M items\/sec     5.689                                           {'family_index': 14, 'per_family_instance_index': 0, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 372, 'byte_width': 8.0, 'null_percent': 0.1}\r\n         TakeChunkedChunkedStringMonotonicIndices\/524288\/1 214.388M items\/sec 226.436M items\/sec     5.619                                                                  {'family_index': 30, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedStringMonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 285, 'null_percent': 100.0}\r\n              TakeFSLInt64RandomIndicesWithNulls\/524288\/10  53.474M items\/sec  56.397M items\/sec     5.466                                                                         {'family_index': 17, 'per_family_instance_index': 1, 'run_name': 'TakeFSLInt64RandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 72, 'null_percent': 10.0}\r\n                          FilterRecordBatchWithNulls\/50\/11      2.188 GiB\/sec      2.307 GiB\/sec     5.424    {'family_index': 9, 'per_family_instance_index': 26, 'run_name': 'FilterRecordBatchWithNulls\/50\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2065, 'data null%': 10.0, 'extracted_size': 790800.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 1.0}\r\n          TakeChunkedChunkedInt64MonotonicIndices\/524288\/1 427.847M items\/sec 450.859M items\/sec     5.378                                                                   {'family_index': 24, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedInt64MonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 573, 'null_percent': 100.0}\r\n         TakeFixedSizeBinaryMonotonicIndices\/524288\/1000\/8 421.593M items\/sec 444.155M items\/sec     5.352                                                 {'family_index': 15, 'per_family_instance_index': 0, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 563, 'byte_width': 8.0, 'null_percent': 0.1}\r\n         TakeChunkedChunkedInt64MonotonicIndices\/524288\/10 257.542M items\/sec 270.864M items\/sec     5.173                                                                   {'family_index': 24, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedInt64MonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 344, 'null_percent': 10.0}\r\n                         FilterInt64FilterNoNulls\/524288\/2     74.186 GiB\/sec     77.992 GiB\/sec     5.131                                                 {'family_index': 0, 'per_family_instance_index': 2, 'run_name': 'FilterInt64FilterNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 105559, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                    TakeInt64RandomIndicesNoNulls\/524288\/0 699.164M items\/sec 734.735M items\/sec     5.088                                                                               {'family_index': 10, 'per_family_instance_index': 4, 'run_name': 'TakeInt64RandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 882, 'null_percent': 0.0}\r\n         TakeChunkedChunkedStringMonotonicIndices\/524288\/2  82.966M items\/sec  87.160M items\/sec     5.056                                                                   {'family_index': 30, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedStringMonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 111, 'null_percent': 50.0}\r\n                          FilterRecordBatchWithNulls\/50\/12      3.785 GiB\/sec      3.973 GiB\/sec     4.958   {'family_index': 9, 'per_family_instance_index': 27, 'run_name': 'FilterRecordBatchWithNulls\/50\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 90.0, 'extracted_size': 75932400.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\n   TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/1000 231.926M items\/sec 243.062M items\/sec     4.802                                                              {'family_index': 22, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 312, 'null_percent': 0.1}\r\n                  TakeStringRandomIndicesNoNulls\/524288\/10  27.757M items\/sec  29.065M items\/sec     4.713                                                                             {'family_index': 19, 'per_family_instance_index': 1, 'run_name': 'TakeStringRandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 37, 'null_percent': 10.0}\r\n      TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/1000 262.098M items\/sec 274.412M items\/sec     4.698                                                                 {'family_index': 31, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedFlatInt64RandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 350, 'null_percent': 0.1}\r\n               TakeFSLInt64RandomIndicesWithNulls\/524288\/2  35.239M items\/sec  36.889M items\/sec     4.683                                                                          {'family_index': 17, 'per_family_instance_index': 2, 'run_name': 'TakeFSLInt64RandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 47, 'null_percent': 50.0}\r\n                           FilterRecordBatchWithNulls\/50\/2      2.164 GiB\/sec      2.266 GiB\/sec     4.675      {'family_index': 9, 'per_family_instance_index': 17, 'run_name': 'FilterRecordBatchWithNulls\/50\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2063, 'data null%': 0.0, 'extracted_size': 790800.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 1.0}\r\n                       TakeStringMonotonicIndices\/524288\/1  55.909M items\/sec  58.475M items\/sec     4.590                                                                                 {'family_index': 21, 'per_family_instance_index': 3, 'run_name': 'TakeStringMonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 75, 'null_percent': 100.0}\r\n                     TakeFSLInt64MonotonicIndices\/524288\/2  49.181M items\/sec  51.435M items\/sec     4.584                                                                                {'family_index': 18, 'per_family_instance_index': 2, 'run_name': 'TakeFSLInt64MonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 66, 'null_percent': 50.0}\r\n     TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/0  30.818M items\/sec  32.222M items\/sec     4.557                                                                 {'family_index': 28, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 42, 'null_percent': 0.0}\r\n                    TakeStringMonotonicIndices\/524288\/1000  90.367M items\/sec  94.438M items\/sec     4.506                                                                               {'family_index': 21, 'per_family_instance_index': 0, 'run_name': 'TakeStringMonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 122, 'null_percent': 0.1}\r\n               TakeInt64RandomIndicesWithNulls\/524288\/1000 278.693M items\/sec 290.996M items\/sec     4.415                                                                          {'family_index': 11, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 369, 'null_percent': 0.1}\r\n       TakeChunkedChunkedInt64MonotonicIndices\/524288\/1000 306.797M items\/sec 320.326M items\/sec     4.410                                                                  {'family_index': 24, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedInt64MonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 407, 'null_percent': 0.1}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/2\/9 109.883M items\/sec 114.720M items\/sec     4.402                                                 {'family_index': 27, 'per_family_instance_index': 5, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 147, 'byte_width': 9.0, 'null_percent': 50.0}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1\/9 191.954M items\/sec 200.255M items\/sec     4.325                                              {'family_index': 13, 'per_family_instance_index': 7, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 256, 'byte_width': 9.0, 'null_percent': 100.0}\r\n                 TakeFSLInt64RandomIndicesNoNulls\/524288\/2  41.117M items\/sec  42.859M items\/sec     4.237                                                                            {'family_index': 16, 'per_family_instance_index': 2, 'run_name': 'TakeFSLInt64RandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 55, 'null_percent': 50.0}\r\n                             FilterRecordBatchNoNulls\/50\/6      3.023 GiB\/sec      3.151 GiB\/sec     4.227       {'family_index': 8, 'per_family_instance_index': 21, 'run_name': 'FilterRecordBatchNoNulls\/50\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 28, 'data null%': 1.0, 'extracted_size': 79930000.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n   TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1000\/8 232.230M items\/sec 241.904M items\/sec     4.166                                           {'family_index': 25, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 309, 'byte_width': 8.0, 'null_percent': 0.1}\r\n                 TakeInt64RandomIndicesNoNulls\/524288\/1000 291.688M items\/sec 303.143M items\/sec     3.927                                                                            {'family_index': 10, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 390, 'null_percent': 0.1}\r\n                           FilterRecordBatchWithNulls\/50\/8      2.143 GiB\/sec      2.227 GiB\/sec     3.917      {'family_index': 9, 'per_family_instance_index': 23, 'run_name': 'FilterRecordBatchWithNulls\/50\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2043, 'data null%': 1.0, 'extracted_size': 790800.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 1.0}\r\n   TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/2  55.385M items\/sec  57.539M items\/sec     3.889                                                              {'family_index': 29, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 73, 'null_percent': 50.0}\r\n                  TakeInt64RandomIndicesWithNulls\/524288\/1   7.038G items\/sec   7.309G items\/sec     3.852                                                                          {'family_index': 11, 'per_family_instance_index': 3, 'run_name': 'TakeInt64RandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 9290, 'null_percent': 100.0}\r\n           TakeFixedSizeBinaryMonotonicIndices\/524288\/10\/8 335.658M items\/sec 348.356M items\/sec     3.783                                                  {'family_index': 15, 'per_family_instance_index': 2, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 448, 'byte_width': 8.0, 'null_percent': 10.0}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/1\/8 432.503M items\/sec 448.584M items\/sec     3.718                                                {'family_index': 27, 'per_family_instance_index': 6, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 575, 'byte_width': 8.0, 'null_percent': 100.0}\r\n TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/1000 222.050M items\/sec 230.071M items\/sec     3.612                                                            {'family_index': 23, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 297, 'null_percent': 0.1}\r\n                             FilterRecordBatchNoNulls\/10\/8      2.963 GiB\/sec      3.069 GiB\/sec     3.574         {'family_index': 8, 'per_family_instance_index': 8, 'run_name': 'FilterRecordBatchNoNulls\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2731, 'data null%': 1.0, 'extracted_size': 811840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                       FilterInt64FilterWithNulls\/524288\/0      3.141 GiB\/sec      3.253 GiB\/sec     3.562                                                {'family_index': 1, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4739, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n      TakeChunkedChunkedStringMonotonicIndices\/524288\/1000  56.733M items\/sec  58.744M items\/sec     3.544                                                                  {'family_index': 30, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedStringMonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 73, 'null_percent': 0.1}\r\n                   TakeStringRandomIndicesNoNulls\/524288\/1 246.857M items\/sec 255.589M items\/sec     3.537                                                                            {'family_index': 19, 'per_family_instance_index': 3, 'run_name': 'TakeStringRandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 331, 'null_percent': 100.0}\r\n                    TakeFSLInt64MonotonicIndices\/524288\/10  73.739M items\/sec  76.336M items\/sec     3.522                                                                               {'family_index': 18, 'per_family_instance_index': 1, 'run_name': 'TakeFSLInt64MonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 99, 'null_percent': 10.0}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/2\/9 124.660M items\/sec 129.009M items\/sec     3.489                                                   {'family_index': 15, 'per_family_instance_index': 5, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 166, 'byte_width': 9.0, 'null_percent': 50.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/2\/8 113.291M items\/sec 117.244M items\/sec     3.489                                             {'family_index': 14, 'per_family_instance_index': 4, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 150, 'byte_width': 8.0, 'null_percent': 50.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/0\/8 723.600M items\/sec 748.813M items\/sec     3.484                                              {'family_index': 14, 'per_family_instance_index': 8, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 967, 'byte_width': 8.0, 'null_percent': 0.0}\r\n                          FilterRecordBatchWithNulls\/50\/10      5.149 GiB\/sec      5.327 GiB\/sec     3.457  {'family_index': 9, 'per_family_instance_index': 25, 'run_name': 'FilterRecordBatchWithNulls\/50\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 101, 'data null%': 10.0, 'extracted_size': 37950000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n         TakeChunkedChunkedStringMonotonicIndices\/524288\/0  55.345M items\/sec  57.250M items\/sec     3.441                                                                     {'family_index': 30, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedStringMonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 70, 'null_percent': 0.0}\r\n     TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/10\/8 181.104M items\/sec 187.335M items\/sec     3.441                                            {'family_index': 14, 'per_family_instance_index': 2, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 239, 'byte_width': 8.0, 'null_percent': 10.0}\r\n                 TakeInt64RandomIndicesWithNulls\/524288\/10 178.200M items\/sec 184.304M items\/sec     3.425                                                                           {'family_index': 11, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 240, 'null_percent': 10.0}\r\n   TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1000\/9 134.461M items\/sec 138.974M items\/sec     3.356                                           {'family_index': 14, 'per_family_instance_index': 1, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 180, 'byte_width': 9.0, 'null_percent': 0.1}\r\n    TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/1 969.075M items\/sec   1.001G items\/sec     3.273                                                            {'family_index': 23, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1286, 'null_percent': 100.0}\r\n                            FilterRecordBatchNoNulls\/10\/14      2.959 GiB\/sec      3.054 GiB\/sec     3.206      {'family_index': 8, 'per_family_instance_index': 14, 'run_name': 'FilterRecordBatchNoNulls\/10\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2750, 'data null%': 90.0, 'extracted_size': 811840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                         FilterInt64FilterNoNulls\/524288\/0     32.655 GiB\/sec     33.698 GiB\/sec     3.191                                                 {'family_index': 0, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 46508, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1000\/9  76.376M items\/sec  78.790M items\/sec     3.160                                         {'family_index': 26, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 102, 'byte_width': 9.0, 'null_percent': 0.1}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/3\/8      3.014 GiB\/sec      3.108 GiB\/sec     3.118                 {'family_index': 3, 'per_family_instance_index': 6, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/3\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4464, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n                       TakeInt64MonotonicIndices\/524288\/10 330.253M items\/sec 340.527M items\/sec     3.111                                                                                 {'family_index': 12, 'per_family_instance_index': 1, 'run_name': 'TakeInt64MonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 445, 'null_percent': 10.0}\r\n                  TakeInt64RandomIndicesWithNulls\/524288\/2 112.439M items\/sec 115.926M items\/sec     3.101                                                                            {'family_index': 11, 'per_family_instance_index': 2, 'run_name': 'TakeInt64RandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 150, 'null_percent': 50.0}\r\n  TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/10  33.596M items\/sec  34.626M items\/sec     3.065                                                             {'family_index': 29, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 45, 'null_percent': 10.0}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/0\/9 224.834M items\/sec 231.694M items\/sec     3.051                                                {'family_index': 13, 'per_family_instance_index': 9, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 297, 'byte_width': 9.0, 'null_percent': 0.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1\/8   7.021G items\/sec   7.232G items\/sec     3.007                                           {'family_index': 14, 'per_family_instance_index': 6, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 9428, 'byte_width': 8.0, 'null_percent': 100.0}\r\n                           FilterRecordBatchWithNulls\/50\/5      2.171 GiB\/sec      2.236 GiB\/sec     2.984      {'family_index': 9, 'per_family_instance_index': 20, 'run_name': 'FilterRecordBatchWithNulls\/50\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2074, 'data null%': 0.1, 'extracted_size': 790800.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 1.0}\r\n                          FilterRecordBatchWithNulls\/100\/1      8.118 GiB\/sec      8.359 GiB\/sec     2.962  {'family_index': 9, 'per_family_instance_index': 31, 'run_name': 'FilterRecordBatchWithNulls\/100\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 158, 'data null%': 0.0, 'extracted_size': 37841600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                            FilterRecordBatchNoNulls\/100\/9      3.425 GiB\/sec      3.522 GiB\/sec     2.842    {'family_index': 8, 'per_family_instance_index': 39, 'run_name': 'FilterRecordBatchNoNulls\/100\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 10.0, 'extracted_size': 79928800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                             FilterRecordBatchNoNulls\/10\/6      2.351 GiB\/sec      2.417 GiB\/sec     2.808        {'family_index': 8, 'per_family_instance_index': 6, 'run_name': 'FilterRecordBatchNoNulls\/10\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'data null%': 1.0, 'extracted_size': 79921920.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                             FilterRecordBatchNoNulls\/50\/9      3.111 GiB\/sec      3.198 GiB\/sec     2.807      {'family_index': 8, 'per_family_instance_index': 24, 'run_name': 'FilterRecordBatchNoNulls\/50\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 28, 'data null%': 10.0, 'extracted_size': 79930000.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/1\/8 764.111M items\/sec 784.536M items\/sec     2.673                                                 {'family_index': 15, 'per_family_instance_index': 6, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1016, 'byte_width': 8.0, 'null_percent': 100.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/0\/9 225.371M items\/sec 231.343M items\/sec     2.650                                              {'family_index': 14, 'per_family_instance_index': 9, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 301, 'byte_width': 9.0, 'null_percent': 0.0}\r\n                   TakeStringRandomIndicesNoNulls\/524288\/2  61.844M items\/sec  63.482M items\/sec     2.649                                                                              {'family_index': 19, 'per_family_instance_index': 2, 'run_name': 'TakeStringRandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 82, 'null_percent': 50.0}\r\n        TakeChunkedChunkedStringMonotonicIndices\/524288\/10  58.972M items\/sec  60.504M items\/sec     2.597                                                                   {'family_index': 30, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedStringMonotonicIndices\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 79, 'null_percent': 10.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1\/9 419.930M items\/sec 430.614M items\/sec     2.544                                            {'family_index': 14, 'per_family_instance_index': 7, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 555, 'byte_width': 9.0, 'null_percent': 100.0}\r\n                 TakeStringRandomIndicesWithNulls\/524288\/2  62.404M items\/sec  63.983M items\/sec     2.531                                                                            {'family_index': 20, 'per_family_instance_index': 2, 'run_name': 'TakeStringRandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 82, 'null_percent': 50.0}\r\n                 TakeStringRandomIndicesWithNulls\/524288\/0  24.880M items\/sec  25.506M items\/sec     2.517                                                                             {'family_index': 20, 'per_family_instance_index': 4, 'run_name': 'TakeStringRandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'null_percent': 0.0}\r\n           TakeFixedSizeBinaryMonotonicIndices\/524288\/10\/9 160.607M items\/sec 164.606M items\/sec     2.490                                                  {'family_index': 15, 'per_family_instance_index': 3, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 212, 'byte_width': 9.0, 'null_percent': 10.0}\r\n                       FilterStringFilterNoNulls\/524288\/12    384.328 MiB\/sec    393.735 MiB\/sec     2.448                                               {'family_index': 6, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterNoNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 544, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/5\/8     14.181 GiB\/sec     14.522 GiB\/sec     2.407                  {'family_index': 2, 'per_family_instance_index': 10, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/5\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20382, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/2\/9     81.507 GiB\/sec     83.432 GiB\/sec     2.362                  {'family_index': 2, 'per_family_instance_index': 5, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 116686, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                            FilterRecordBatchNoNulls\/10\/12      2.313 GiB\/sec      2.367 GiB\/sec     2.361     {'family_index': 8, 'per_family_instance_index': 12, 'run_name': 'FilterRecordBatchNoNulls\/10\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20, 'data null%': 90.0, 'extracted_size': 79921920.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                       TakeStringMonotonicIndices\/524288\/0 148.236M items\/sec 151.732M items\/sec     2.358                                                                                  {'family_index': 21, 'per_family_instance_index': 4, 'run_name': 'TakeStringMonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 194, 'null_percent': 0.0}\r\n                TakeStringRandomIndicesNoNulls\/524288\/1000  26.201M items\/sec  26.791M items\/sec     2.254                                                                            {'family_index': 19, 'per_family_instance_index': 0, 'run_name': 'TakeStringRandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 35, 'null_percent': 0.1}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/8\/9     14.584 GiB\/sec     14.909 GiB\/sec     2.228                  {'family_index': 2, 'per_family_instance_index': 17, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/8\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21421, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n     TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/2  54.172M items\/sec  55.355M items\/sec     2.184                                                                {'family_index': 28, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 72, 'null_percent': 50.0}\r\n       TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/1 265.408M items\/sec 271.104M items\/sec     2.146                                                                {'family_index': 32, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 354, 'null_percent': 100.0}\r\n     TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/10\/9 111.689M items\/sec 114.081M items\/sec     2.142                                            {'family_index': 25, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 149, 'byte_width': 9.0, 'null_percent': 10.0}\r\n                 TakeFSLInt64RandomIndicesNoNulls\/524288\/1  57.107M items\/sec  58.314M items\/sec     2.113                                                                           {'family_index': 16, 'per_family_instance_index': 3, 'run_name': 'TakeFSLInt64RandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 76, 'null_percent': 100.0}\r\n      TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/2\/9  73.932M items\/sec  75.470M items\/sec     2.080                                              {'family_index': 14, 'per_family_instance_index': 5, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 99, 'byte_width': 9.0, 'null_percent': 50.0}\r\n     TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/1 219.554M items\/sec 224.111M items\/sec     2.076                                                              {'family_index': 28, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 293, 'null_percent': 100.0}\r\n                      FilterInt64FilterWithNulls\/524288\/14     11.042 GiB\/sec     11.267 GiB\/sec     2.040                                             {'family_index': 1, 'per_family_instance_index': 14, 'run_name': 'FilterInt64FilterWithNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16065, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n   TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/10 153.779M items\/sec 156.908M items\/sec     2.035                                                             {'family_index': 23, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 207, 'null_percent': 10.0}\r\n                        FilterStringFilterNoNulls\/524288\/7      1.663 GiB\/sec      1.696 GiB\/sec     1.975                                                 {'family_index': 6, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterNoNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2397, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n   TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/1   1.138G items\/sec   1.161G items\/sec     1.969                                                           {'family_index': 29, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1516, 'null_percent': 100.0}\r\n                   TakeStringRandomIndicesNoNulls\/524288\/0  25.010M items\/sec  25.501M items\/sec     1.961                                                                               {'family_index': 19, 'per_family_instance_index': 4, 'run_name': 'TakeStringRandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'null_percent': 0.0}\r\n                       FilterInt64FilterWithNulls\/524288\/9      2.489 GiB\/sec      2.538 GiB\/sec     1.947                                               {'family_index': 1, 'per_family_instance_index': 9, 'run_name': 'FilterInt64FilterWithNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3541, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1\/8 707.767M items\/sec 720.680M items\/sec     1.825                                              {'family_index': 13, 'per_family_instance_index': 6, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 945, 'byte_width': 8.0, 'null_percent': 100.0}\r\n     TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/10\/9 107.303M items\/sec 109.259M items\/sec     1.823                                            {'family_index': 14, 'per_family_instance_index': 3, 'run_name': 'TakeFixedSizeBinaryRandomIndicesWithNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 142, 'byte_width': 9.0, 'null_percent': 10.0}\r\n                        FilterInt64FilterNoNulls\/524288\/10      1.596 GiB\/sec      1.625 GiB\/sec     1.807                                               {'family_index': 0, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2293, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                           FilterRecordBatchWithNulls\/50\/3      3.104 GiB\/sec      3.159 GiB\/sec     1.779     {'family_index': 9, 'per_family_instance_index': 18, 'run_name': 'FilterRecordBatchWithNulls\/50\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 31, 'data null%': 0.1, 'extracted_size': 75932400.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\n       TakeChunkedChunkedFSBMonotonicIndices\/524288\/1000\/8 306.771M items\/sec 312.197M items\/sec     1.769                                               {'family_index': 27, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 410, 'byte_width': 8.0, 'null_percent': 0.1}\r\n                         FilterInt64FilterNoNulls\/524288\/7      1.688 GiB\/sec      1.717 GiB\/sec     1.754                                                  {'family_index': 0, 'per_family_instance_index': 7, 'run_name': 'FilterInt64FilterNoNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2414, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                           FilterRecordBatchWithNulls\/10\/6      2.276 GiB\/sec      2.315 GiB\/sec     1.699      {'family_index': 9, 'per_family_instance_index': 6, 'run_name': 'FilterRecordBatchWithNulls\/10\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 23, 'data null%': 1.0, 'extracted_size': 75915760.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 99.9}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/0\/8 419.071M items\/sec 425.866M items\/sec     1.621                                            {'family_index': 26, 'per_family_instance_index': 8, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 561, 'byte_width': 8.0, 'null_percent': 0.0}\r\n                        TakeInt64MonotonicIndices\/524288\/1 753.504M items\/sec 765.527M items\/sec     1.596                                                                                {'family_index': 12, 'per_family_instance_index': 3, 'run_name': 'TakeInt64MonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1002, 'null_percent': 100.0}\r\n          TakeChunkedChunkedInt64MonotonicIndices\/524288\/2 198.101M items\/sec 201.208M items\/sec     1.568                                                                    {'family_index': 24, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedInt64MonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 264, 'null_percent': 50.0}\r\nTakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/1000  31.109M items\/sec  31.593M items\/sec     1.557                                                            {'family_index': 29, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedStringRandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 41, 'null_percent': 0.1}\r\n                            FilterRecordBatchNoNulls\/10\/13      2.111 GiB\/sec      2.144 GiB\/sec     1.539     {'family_index': 8, 'per_family_instance_index': 13, 'run_name': 'FilterRecordBatchNoNulls\/10\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 36, 'data null%': 90.0, 'extracted_size': 39957840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                        FilterStringFilterNoNulls\/524288\/8     27.632 GiB\/sec     28.056 GiB\/sec     1.535                                                 {'family_index': 6, 'per_family_instance_index': 8, 'run_name': 'FilterStringFilterNoNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39586, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                   FilterFSLInt64FilterWithNulls\/524288\/10    743.880 MiB\/sec    755.087 MiB\/sec     1.507                                          {'family_index': 5, 'per_family_instance_index': 10, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1035, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n     TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/10 213.572M items\/sec 216.732M items\/sec     1.480                                                               {'family_index': 22, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 285, 'null_percent': 10.0}\r\n                           FilterRecordBatchNoNulls\/100\/10      4.420 GiB\/sec      4.482 GiB\/sec     1.410   {'family_index': 8, 'per_family_instance_index': 40, 'run_name': 'FilterRecordBatchNoNulls\/100\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 82, 'data null%': 10.0, 'extracted_size': 39840800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/13\/8      1.584 GiB\/sec      1.606 GiB\/sec     1.409                {'family_index': 2, 'per_family_instance_index': 26, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/13\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2273, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/0      1.427 GiB\/sec      1.447 GiB\/sec     1.377                                               {'family_index': 7, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2066, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n          TakeChunkedChunkedFSBMonotonicIndices\/524288\/2\/8 198.179M items\/sec 200.905M items\/sec     1.376                                                 {'family_index': 27, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedFSBMonotonicIndices\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 264, 'byte_width': 8.0, 'null_percent': 50.0}\r\n    TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/10  34.712M items\/sec  35.171M items\/sec     1.323                                                               {'family_index': 28, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 46, 'null_percent': 10.0}\r\n                         FilterInt64FilterNoNulls\/524288\/3     14.600 GiB\/sec     14.792 GiB\/sec     1.314                                                 {'family_index': 0, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterNoNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21016, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n              TakeStringRandomIndicesWithNulls\/524288\/1000  25.891M items\/sec  26.230M items\/sec     1.308                                                                          {'family_index': 20, 'per_family_instance_index': 0, 'run_name': 'TakeStringRandomIndicesWithNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 35, 'null_percent': 0.1}\r\n                             FilterRecordBatchNoNulls\/10\/3      2.358 GiB\/sec      2.388 GiB\/sec     1.259        {'family_index': 8, 'per_family_instance_index': 3, 'run_name': 'FilterRecordBatchNoNulls\/10\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'data null%': 0.1, 'extracted_size': 79921920.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/0\/8     32.700 GiB\/sec     33.108 GiB\/sec     1.247                  {'family_index': 2, 'per_family_instance_index': 0, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 47041, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                      FilterStringFilterWithNulls\/524288\/6      1.348 GiB\/sec      1.364 GiB\/sec     1.208                                               {'family_index': 7, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterWithNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1934, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/2\/9  98.491M items\/sec  99.680M items\/sec     1.207                                               {'family_index': 13, 'per_family_instance_index': 5, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 132, 'byte_width': 9.0, 'null_percent': 50.0}\r\n                           FilterRecordBatchWithNulls\/50\/7      5.161 GiB\/sec      5.223 GiB\/sec     1.201     {'family_index': 9, 'per_family_instance_index': 22, 'run_name': 'FilterRecordBatchWithNulls\/50\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 82, 'data null%': 1.0, 'extracted_size': 37950000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n       TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/10\/9 126.560M items\/sec 128.050M items\/sec     1.177                                              {'family_index': 13, 'per_family_instance_index': 3, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 171, 'byte_width': 9.0, 'null_percent': 10.0}\r\n                        FilterStringFilterNoNulls\/524288\/5     27.468 GiB\/sec     27.790 GiB\/sec     1.173                                                 {'family_index': 6, 'per_family_instance_index': 5, 'run_name': 'FilterStringFilterNoNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39724, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n                 TakeFSLInt64RandomIndicesNoNulls\/524288\/0 128.815M items\/sec 130.315M items\/sec     1.165                                                                            {'family_index': 16, 'per_family_instance_index': 4, 'run_name': 'TakeFSLInt64RandomIndicesNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 173, 'null_percent': 0.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/9    608.620 MiB\/sec    615.616 MiB\/sec     1.150                                             {'family_index': 5, 'per_family_instance_index': 9, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 867, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                      FilterStringFilterWithNulls\/524288\/9      1.197 GiB\/sec      1.211 GiB\/sec     1.144                                              {'family_index': 7, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterWithNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1729, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n     TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1000\/8 296.502M items\/sec 299.895M items\/sec     1.144                                             {'family_index': 13, 'per_family_instance_index': 0, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1000\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 392, 'byte_width': 8.0, 'null_percent': 0.1}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/0\/9     32.684 GiB\/sec     33.042 GiB\/sec     1.097                  {'family_index': 2, 'per_family_instance_index': 1, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 46503, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                            FilterRecordBatchNoNulls\/10\/10      2.186 GiB\/sec      2.209 GiB\/sec     1.085     {'family_index': 8, 'per_family_instance_index': 10, 'run_name': 'FilterRecordBatchNoNulls\/10\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39, 'data null%': 10.0, 'extracted_size': 39957840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                       FilterStringFilterNoNulls\/524288\/11     24.744 GiB\/sec     25.009 GiB\/sec     1.070                                              {'family_index': 6, 'per_family_instance_index': 11, 'run_name': 'FilterStringFilterNoNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 35792, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n      TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/1 424.762M items\/sec 429.271M items\/sec     1.062                                                               {'family_index': 22, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 571, 'null_percent': 100.0}\r\n               TakeFSLInt64RandomIndicesWithNulls\/524288\/1  67.986M items\/sec  68.707M items\/sec     1.061                                                                         {'family_index': 17, 'per_family_instance_index': 3, 'run_name': 'TakeFSLInt64RandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 91, 'null_percent': 100.0}\r\n            TakeFixedSizeBinaryMonotonicIndices\/524288\/2\/8 239.950M items\/sec 242.494M items\/sec     1.060                                                   {'family_index': 15, 'per_family_instance_index': 4, 'run_name': 'TakeFixedSizeBinaryMonotonicIndices\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 320, 'byte_width': 8.0, 'null_percent': 50.0}\r\n                   TakeInt64RandomIndicesNoNulls\/524288\/10 267.288M items\/sec 270.121M items\/sec     1.060                                                                             {'family_index': 10, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 356, 'null_percent': 10.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/11\/9     14.555 GiB\/sec     14.709 GiB\/sec     1.059                {'family_index': 2, 'per_family_instance_index': 23, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/11\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20790, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/8\/8     13.978 GiB\/sec     14.126 GiB\/sec     1.056                  {'family_index': 2, 'per_family_instance_index': 16, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/8\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 19960, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1\/9 295.107M items\/sec 298.203M items\/sec     1.049                                          {'family_index': 26, 'per_family_instance_index': 7, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 394, 'byte_width': 9.0, 'null_percent': 100.0}\r\n                       FilterStringFilterNoNulls\/524288\/14      2.676 GiB\/sec      2.704 GiB\/sec     1.044                                               {'family_index': 6, 'per_family_instance_index': 14, 'run_name': 'FilterStringFilterNoNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3873, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                         FilterInt64FilterNoNulls\/524288\/8     14.246 GiB\/sec     14.391 GiB\/sec     1.014                                                  {'family_index': 0, 'per_family_instance_index': 8, 'run_name': 'FilterInt64FilterNoNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20293, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/4\/9      1.457 GiB\/sec      1.471 GiB\/sec     0.973                 {'family_index': 3, 'per_family_instance_index': 9, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/4\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2084, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n                   FilterFSLInt64FilterWithNulls\/524288\/11      8.577 GiB\/sec      8.658 GiB\/sec     0.945                                          {'family_index': 5, 'per_family_instance_index': 11, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12227, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/2\/8 102.567M items\/sec 103.520M items\/sec     0.929                                           {'family_index': 26, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 136, 'byte_width': 8.0, 'null_percent': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/1\/8      1.759 GiB\/sec      1.775 GiB\/sec     0.915                 {'family_index': 3, 'per_family_instance_index': 2, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2515, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                           FilterRecordBatchWithNulls\/10\/9      2.281 GiB\/sec      2.301 GiB\/sec     0.890     {'family_index': 9, 'per_family_instance_index': 9, 'run_name': 'FilterRecordBatchWithNulls\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'data null%': 10.0, 'extracted_size': 75915760.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                          FilterRecordBatchWithNulls\/10\/12      2.281 GiB\/sec      2.302 GiB\/sec     0.888   {'family_index': 9, 'per_family_instance_index': 12, 'run_name': 'FilterRecordBatchWithNulls\/10\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 23, 'data null%': 90.0, 'extracted_size': 75915760.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 99.9}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/10\/8      1.590 GiB\/sec      1.603 GiB\/sec     0.840                {'family_index': 2, 'per_family_instance_index': 20, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2290, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n   TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/10\/8 154.942M items\/sec 156.186M items\/sec     0.803                                          {'family_index': 26, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 207, 'byte_width': 8.0, 'null_percent': 10.0}\r\n                  TakeFSLInt64MonotonicIndices\/524288\/1000  90.641M items\/sec  91.364M items\/sec     0.798                                                                             {'family_index': 18, 'per_family_instance_index': 0, 'run_name': 'TakeFSLInt64MonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 121, 'null_percent': 0.1}\r\n                     FilterStringFilterWithNulls\/524288\/11     21.447 GiB\/sec     21.618 GiB\/sec     0.797                                            {'family_index': 7, 'per_family_instance_index': 11, 'run_name': 'FilterStringFilterWithNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 30400, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                TakeStringRandomIndicesWithNulls\/524288\/10  39.137M items\/sec  39.442M items\/sec     0.780                                                                           {'family_index': 20, 'per_family_instance_index': 1, 'run_name': 'TakeStringRandomIndicesWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 52, 'null_percent': 10.0}\r\n                             FilterRecordBatchNoNulls\/10\/7      2.172 GiB\/sec      2.189 GiB\/sec     0.780        {'family_index': 8, 'per_family_instance_index': 7, 'run_name': 'FilterRecordBatchNoNulls\/10\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 41, 'data null%': 1.0, 'extracted_size': 39957840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/8     24.238 GiB\/sec     24.426 GiB\/sec     0.776                                               {'family_index': 7, 'per_family_instance_index': 8, 'run_name': 'FilterStringFilterWithNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 34515, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/4\/8      1.800 GiB\/sec      1.813 GiB\/sec     0.690                   {'family_index': 2, 'per_family_instance_index': 8, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/4\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2588, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n                     TakeInt64MonotonicIndices\/524288\/1000 417.958M items\/sec 420.763M items\/sec     0.671                                                                                {'family_index': 12, 'per_family_instance_index': 0, 'run_name': 'TakeInt64MonotonicIndices\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 554, 'null_percent': 0.1}\r\n                      FilterStringFilterWithNulls\/524288\/1      1.698 GiB\/sec      1.709 GiB\/sec     0.653                                               {'family_index': 7, 'per_family_instance_index': 1, 'run_name': 'FilterStringFilterWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2421, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                          FilterRecordBatchWithNulls\/100\/6      3.445 GiB\/sec      3.467 GiB\/sec     0.642   {'family_index': 9, 'per_family_instance_index': 36, 'run_name': 'FilterRecordBatchWithNulls\/100\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 33, 'data null%': 1.0, 'extracted_size': 75973600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                      FilterInt64FilterWithNulls\/524288\/13      1.568 GiB\/sec      1.578 GiB\/sec     0.638                                             {'family_index': 1, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterWithNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2274, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                     FilterStringFilterWithNulls\/524288\/10      1.477 GiB\/sec      1.486 GiB\/sec     0.608                                            {'family_index': 7, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2079, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/2     25.131 GiB\/sec     25.284 GiB\/sec     0.607                                               {'family_index': 7, 'per_family_instance_index': 2, 'run_name': 'FilterStringFilterWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 35665, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                         FilterInt64FilterNoNulls\/524288\/4      1.818 GiB\/sec      1.829 GiB\/sec     0.598                                                  {'family_index': 0, 'per_family_instance_index': 4, 'run_name': 'FilterInt64FilterNoNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2608, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n                  TakeInt64RandomIndicesWithNulls\/524288\/0 710.967M items\/sec 715.156M items\/sec     0.589                                                                             {'family_index': 11, 'per_family_instance_index': 4, 'run_name': 'TakeInt64RandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 967, 'null_percent': 0.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/9\/9      1.798 GiB\/sec      1.809 GiB\/sec     0.564               {'family_index': 3, 'per_family_instance_index': 19, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/9\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2646, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                       FilterInt64FilterWithNulls\/524288\/6      2.749 GiB\/sec      2.765 GiB\/sec     0.559                                                {'family_index': 1, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterWithNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3915, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                           FilterRecordBatchWithNulls\/10\/0      2.280 GiB\/sec      2.293 GiB\/sec     0.533      {'family_index': 9, 'per_family_instance_index': 0, 'run_name': 'FilterRecordBatchWithNulls\/10\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 23, 'data null%': 0.0, 'extracted_size': 75915760.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                    TakeInt64RandomIndicesNoNulls\/524288\/1 701.535M items\/sec 704.893M items\/sec     0.479                                                                             {'family_index': 10, 'per_family_instance_index': 3, 'run_name': 'TakeInt64RandomIndicesNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 949, 'null_percent': 100.0}\r\n                        FilterStringFilterNoNulls\/524288\/4      1.706 GiB\/sec      1.714 GiB\/sec     0.468                                                 {'family_index': 6, 'per_family_instance_index': 4, 'run_name': 'FilterStringFilterNoNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2447, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n                      FilterInt64FilterWithNulls\/524288\/10      1.575 GiB\/sec      1.583 GiB\/sec     0.465                                             {'family_index': 1, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterWithNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2269, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                     TakeFSLInt64MonotonicIndices\/524288\/1  56.525M items\/sec  56.784M items\/sec     0.458                                                                               {'family_index': 18, 'per_family_instance_index': 3, 'run_name': 'TakeFSLInt64MonotonicIndices\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 75, 'null_percent': 100.0}\r\n                         FilterInt64FilterNoNulls\/524288\/5     14.515 GiB\/sec     14.581 GiB\/sec     0.454                                                  {'family_index': 0, 'per_family_instance_index': 5, 'run_name': 'FilterInt64FilterNoNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20681, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n                             FilterRecordBatchNoNulls\/10\/2      3.031 GiB\/sec      3.043 GiB\/sec     0.402         {'family_index': 8, 'per_family_instance_index': 2, 'run_name': 'FilterRecordBatchNoNulls\/10\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2783, 'data null%': 0.0, 'extracted_size': 811840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                             FilterRecordBatchNoNulls\/10\/9      2.360 GiB\/sec      2.369 GiB\/sec     0.369       {'family_index': 8, 'per_family_instance_index': 9, 'run_name': 'FilterRecordBatchNoNulls\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'data null%': 10.0, 'extracted_size': 79921920.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/5\/9     14.833 GiB\/sec     14.885 GiB\/sec     0.355                  {'family_index': 2, 'per_family_instance_index': 11, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/5\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21103, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n                     FilterStringFilterWithNulls\/524288\/12    369.240 MiB\/sec    370.434 MiB\/sec     0.323                                             {'family_index': 7, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterWithNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 516, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                   FilterFSLInt64FilterWithNulls\/524288\/14      7.925 GiB\/sec      7.950 GiB\/sec     0.313                                          {'family_index': 5, 'per_family_instance_index': 14, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 11243, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/8\/8     10.905 GiB\/sec     10.938 GiB\/sec     0.300                {'family_index': 3, 'per_family_instance_index': 16, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/8\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15594, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/7\/9      1.516 GiB\/sec      1.521 GiB\/sec     0.287                  {'family_index': 2, 'per_family_instance_index': 15, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/7\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2162, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/7      1.661 GiB\/sec      1.665 GiB\/sec     0.257                                               {'family_index': 7, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterWithNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2389, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                         FilterInt64FilterNoNulls\/524288\/9      9.068 GiB\/sec      9.088 GiB\/sec     0.219                                                {'family_index': 0, 'per_family_instance_index': 9, 'run_name': 'FilterInt64FilterNoNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12998, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                         FilterInt64FilterNoNulls\/524288\/6     10.518 GiB\/sec     10.540 GiB\/sec     0.216                                                 {'family_index': 0, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterNoNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15028, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/7\/9      1.417 GiB\/sec      1.420 GiB\/sec     0.180                {'family_index': 3, 'per_family_instance_index': 15, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/7\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2031, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/4      1.696 GiB\/sec      1.699 GiB\/sec     0.157                                               {'family_index': 7, 'per_family_instance_index': 4, 'run_name': 'FilterStringFilterWithNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2440, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n                     FilterStringFilterWithNulls\/524288\/14      1.862 GiB\/sec      1.864 GiB\/sec     0.140                                             {'family_index': 7, 'per_family_instance_index': 14, 'run_name': 'FilterStringFilterWithNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2681, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/2\/9  68.672M items\/sec  68.757M items\/sec     0.123                                            {'family_index': 26, 'per_family_instance_index': 5, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 92, 'byte_width': 9.0, 'null_percent': 50.0}\r\n                          FilterRecordBatchWithNulls\/10\/14    926.972 MiB\/sec    928.101 MiB\/sec     0.122     {'family_index': 9, 'per_family_instance_index': 14, 'run_name': 'FilterRecordBatchWithNulls\/10\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 892, 'data null%': 90.0, 'extracted_size': 770000.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                   FilterFSLInt64FilterWithNulls\/524288\/12    436.039 MiB\/sec    436.533 MiB\/sec     0.113                                           {'family_index': 5, 'per_family_instance_index': 12, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 612, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/0\/8      3.103 GiB\/sec      3.106 GiB\/sec     0.096                 {'family_index': 3, 'per_family_instance_index': 0, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4675, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                        TakeInt64MonotonicIndices\/524288\/2 235.876M items\/sec 236.079M items\/sec     0.086                                                                                  {'family_index': 12, 'per_family_instance_index': 2, 'run_name': 'TakeInt64MonotonicIndices\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 316, 'null_percent': 50.0}\r\n                      FilterStringFilterWithNulls\/524288\/3      1.429 GiB\/sec      1.430 GiB\/sec     0.040                                               {'family_index': 7, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterWithNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2027, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n       TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/0 169.206M items\/sec 169.257M items\/sec     0.030                                                                  {'family_index': 32, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedFlatInt64RandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 226, 'null_percent': 0.0}\r\n                           FilterRecordBatchWithNulls\/10\/8    928.017 MiB\/sec    928.270 MiB\/sec     0.027        {'family_index': 9, 'per_family_instance_index': 8, 'run_name': 'FilterRecordBatchWithNulls\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 883, 'data null%': 1.0, 'extracted_size': 770000.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/4\/9      1.562 GiB\/sec      1.563 GiB\/sec     0.026                   {'family_index': 2, 'per_family_instance_index': 9, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/4\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2223, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n                             FilterRecordBatchNoNulls\/10\/0      2.375 GiB\/sec      2.375 GiB\/sec    -0.015        {'family_index': 8, 'per_family_instance_index': 0, 'run_name': 'FilterRecordBatchNoNulls\/10\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'data null%': 0.0, 'extracted_size': 79921920.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                   FilterFSLInt64FilterWithNulls\/524288\/13    591.785 MiB\/sec    591.583 MiB\/sec    -0.034                                           {'family_index': 5, 'per_family_instance_index': 13, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 829, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                      FilterInt64FilterWithNulls\/524288\/11     11.131 GiB\/sec     11.126 GiB\/sec    -0.040                                             {'family_index': 1, 'per_family_instance_index': 11, 'run_name': 'FilterInt64FilterWithNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15965, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/14\/9     14.551 GiB\/sec     14.543 GiB\/sec    -0.057                {'family_index': 2, 'per_family_instance_index': 29, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/14\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21092, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1\/8 943.667M items\/sec 943.101M items\/sec    -0.060                                         {'family_index': 26, 'per_family_instance_index': 6, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1287, 'byte_width': 8.0, 'null_percent': 100.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/7\/8      1.696 GiB\/sec      1.695 GiB\/sec    -0.066                  {'family_index': 2, 'per_family_instance_index': 14, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/7\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2413, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                       FilterInt64FilterWithNulls\/524288\/3      3.189 GiB\/sec      3.185 GiB\/sec    -0.106                                                {'family_index': 1, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterWithNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4594, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n                         FilterInt64FilterNoNulls\/524288\/1      3.513 GiB\/sec      3.509 GiB\/sec    -0.112                                                  {'family_index': 0, 'per_family_instance_index': 1, 'run_name': 'FilterInt64FilterNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 5029, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                        FilterStringFilterNoNulls\/524288\/9      1.280 GiB\/sec      1.278 GiB\/sec    -0.177                                                {'family_index': 6, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterNoNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1895, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/6\/8      2.785 GiB\/sec      2.778 GiB\/sec    -0.271                {'family_index': 3, 'per_family_instance_index': 12, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/6\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4000, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                      FilterInt64FilterWithNulls\/524288\/12      2.495 GiB\/sec      2.488 GiB\/sec    -0.287                                             {'family_index': 1, 'per_family_instance_index': 12, 'run_name': 'FilterInt64FilterWithNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3561, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/1\/9      1.467 GiB\/sec      1.463 GiB\/sec    -0.302                 {'family_index': 3, 'per_family_instance_index': 3, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2101, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                          FilterRecordBatchWithNulls\/10\/10      1.952 GiB\/sec      1.946 GiB\/sec    -0.329   {'family_index': 9, 'per_family_instance_index': 10, 'run_name': 'FilterRecordBatchWithNulls\/10\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39, 'data null%': 10.0, 'extracted_size': 37945920.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/5\/9     11.588 GiB\/sec     11.548 GiB\/sec    -0.341                {'family_index': 3, 'per_family_instance_index': 11, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/5\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16274, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n                        FilterStringFilterNoNulls\/524288\/1      3.532 GiB\/sec      3.519 GiB\/sec    -0.363                                                 {'family_index': 6, 'per_family_instance_index': 1, 'run_name': 'FilterStringFilterNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 5042, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                       FilterInt64FilterWithNulls\/524288\/2     11.583 GiB\/sec     11.539 GiB\/sec    -0.375                                                {'family_index': 1, 'per_family_instance_index': 2, 'run_name': 'FilterInt64FilterWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16519, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/1\/8      3.487 GiB\/sec      3.473 GiB\/sec    -0.401                   {'family_index': 2, 'per_family_instance_index': 2, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/1\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4974, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n      TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/2 154.080M items\/sec 153.403M items\/sec    -0.439                                                                {'family_index': 22, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 206, 'null_percent': 50.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/9\/8      9.008 GiB\/sec      8.963 GiB\/sec    -0.494                {'family_index': 2, 'per_family_instance_index': 18, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/9\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12888, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                       FilterStringFilterNoNulls\/524288\/10      1.496 GiB\/sec      1.489 GiB\/sec    -0.508                                              {'family_index': 6, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2129, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                 TakeStringRandomIndicesWithNulls\/524288\/1   4.008G items\/sec   3.987G items\/sec    -0.527                                                                         {'family_index': 20, 'per_family_instance_index': 3, 'run_name': 'TakeStringRandomIndicesWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 5311, 'null_percent': 100.0}\r\n     TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1000\/9 138.472M items\/sec 137.685M items\/sec    -0.569                                             {'family_index': 13, 'per_family_instance_index': 1, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 184, 'byte_width': 9.0, 'null_percent': 0.1}\r\n                           FilterRecordBatchWithNulls\/10\/7      1.949 GiB\/sec      1.938 GiB\/sec    -0.575      {'family_index': 9, 'per_family_instance_index': 7, 'run_name': 'FilterRecordBatchWithNulls\/10\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 38, 'data null%': 1.0, 'extracted_size': 37945920.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 50.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/9\/9      8.692 GiB\/sec      8.641 GiB\/sec    -0.581                {'family_index': 2, 'per_family_instance_index': 19, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/9\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12412, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                       FilterInt64FilterWithNulls\/524288\/8     11.033 GiB\/sec     10.967 GiB\/sec    -0.602                                                {'family_index': 1, 'per_family_instance_index': 8, 'run_name': 'FilterInt64FilterWithNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15604, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/4\/8      1.774 GiB\/sec      1.764 GiB\/sec    -0.618                 {'family_index': 3, 'per_family_instance_index': 8, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/4\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2545, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/7\/8      1.682 GiB\/sec      1.672 GiB\/sec    -0.623                {'family_index': 3, 'per_family_instance_index': 14, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/7\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2414, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/0\/9      2.141 GiB\/sec      2.127 GiB\/sec    -0.666                 {'family_index': 3, 'per_family_instance_index': 1, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3087, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                      FilterStringFilterWithNulls\/524288\/5     24.179 GiB\/sec     24.004 GiB\/sec    -0.721                                               {'family_index': 7, 'per_family_instance_index': 5, 'run_name': 'FilterStringFilterWithNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 34153, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/6    874.636 MiB\/sec    868.118 MiB\/sec    -0.745                                               {'family_index': 4, 'per_family_instance_index': 6, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1231, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/2\/9     11.853 GiB\/sec     11.760 GiB\/sec    -0.779                 {'family_index': 3, 'per_family_instance_index': 5, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/2\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 17060, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\n      TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/2\/8 155.262M items\/sec 154.002M items\/sec    -0.812                                             {'family_index': 25, 'per_family_instance_index': 4, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 207, 'byte_width': 8.0, 'null_percent': 50.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/12\/9      8.700 GiB\/sec      8.628 GiB\/sec    -0.828               {'family_index': 2, 'per_family_instance_index': 25, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/12\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12517, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/3\/9      2.101 GiB\/sec      2.083 GiB\/sec    -0.847                 {'family_index': 3, 'per_family_instance_index': 7, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/3\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3040, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/3\/8     14.468 GiB\/sec     14.344 GiB\/sec    -0.861                  {'family_index': 2, 'per_family_instance_index': 6, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/3\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20881, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/1\/9      3.755 GiB\/sec      3.722 GiB\/sec    -0.868                   {'family_index': 2, 'per_family_instance_index': 3, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/1\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 5376, 'byte_width': 9.0, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                        FilterInt64FilterNoNulls\/524288\/13      1.598 GiB\/sec      1.584 GiB\/sec    -0.878                                               {'family_index': 0, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterNoNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2282, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                    TakeInt64RandomIndicesNoNulls\/524288\/2 179.177M items\/sec 177.437M items\/sec    -0.971                                                                              {'family_index': 10, 'per_family_instance_index': 2, 'run_name': 'TakeInt64RandomIndicesNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 242, 'null_percent': 50.0}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/6\/8     10.520 GiB\/sec     10.416 GiB\/sec    -0.987                 {'family_index': 2, 'per_family_instance_index': 12, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/6\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15131, 'byte_width': 8.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n       TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/10\/8 268.292M items\/sec 265.460M items\/sec    -1.056                                              {'family_index': 13, 'per_family_instance_index': 2, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 359, 'byte_width': 8.0, 'null_percent': 10.0}\r\n                       FilterInt64FilterWithNulls\/524288\/5     11.418 GiB\/sec     11.297 GiB\/sec    -1.066                                                {'family_index': 1, 'per_family_instance_index': 5, 'run_name': 'FilterInt64FilterWithNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16291, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n                     FilterStringFilterWithNulls\/524288\/13    267.920 MiB\/sec    264.952 MiB\/sec    -1.108                                             {'family_index': 7, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterWithNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 372, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                        FilterInt64FilterNoNulls\/524288\/12      9.052 GiB\/sec      8.949 GiB\/sec    -1.138                                              {'family_index': 0, 'per_family_instance_index': 12, 'run_name': 'FilterInt64FilterNoNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 12903, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/3\/9     13.008 GiB\/sec     12.857 GiB\/sec    -1.160                  {'family_index': 2, 'per_family_instance_index': 7, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/3\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 18546, 'byte_width': 9.0, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n   TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/10\/9  95.518M items\/sec  94.383M items\/sec    -1.187                                          {'family_index': 26, 'per_family_instance_index': 3, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 125, 'byte_width': 9.0, 'null_percent': 10.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/10\/9      1.508 GiB\/sec      1.490 GiB\/sec    -1.196                {'family_index': 2, 'per_family_instance_index': 21, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2142, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n    TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/0\/9 181.391M items\/sec 179.086M items\/sec    -1.271                                            {'family_index': 26, 'per_family_instance_index': 9, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesWithNulls\/524288\/0\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 242, 'byte_width': 9.0, 'null_percent': 0.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/6\/9      1.969 GiB\/sec      1.944 GiB\/sec    -1.305                {'family_index': 3, 'per_family_instance_index': 13, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/6\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2819, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n   TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1000\/9 118.839M items\/sec 117.239M items\/sec    -1.346                                           {'family_index': 25, 'per_family_instance_index': 1, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/1000\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 158, 'byte_width': 9.0, 'null_percent': 0.1}\r\n                       FilterInt64FilterWithNulls\/524288\/1      1.783 GiB\/sec      1.758 GiB\/sec    -1.360                                                {'family_index': 1, 'per_family_instance_index': 1, 'run_name': 'FilterInt64FilterWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2545, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                       FilterStringFilterNoNulls\/524288\/13    293.771 MiB\/sec    289.620 MiB\/sec    -1.413                                               {'family_index': 6, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterNoNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 404, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/9\/8      2.547 GiB\/sec      2.510 GiB\/sec    -1.431               {'family_index': 3, 'per_family_instance_index': 18, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/9\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3639, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                       FilterInt64FilterWithNulls\/524288\/7      1.686 GiB\/sec      1.661 GiB\/sec    -1.479                                                {'family_index': 1, 'per_family_instance_index': 7, 'run_name': 'FilterInt64FilterWithNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2423, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/8\/9     11.414 GiB\/sec     11.235 GiB\/sec    -1.570                {'family_index': 3, 'per_family_instance_index': 17, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/8\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16199, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                          FilterRecordBatchWithNulls\/10\/11    940.510 MiB\/sec    925.278 MiB\/sec    -1.620     {'family_index': 9, 'per_family_instance_index': 11, 'run_name': 'FilterRecordBatchWithNulls\/10\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 887, 'data null%': 10.0, 'extracted_size': 770000.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                       FilterInt64FilterWithNulls\/524288\/4      1.776 GiB\/sec      1.746 GiB\/sec    -1.667                                                {'family_index': 1, 'per_family_instance_index': 4, 'run_name': 'FilterInt64FilterWithNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2519, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/12\/8      9.091 GiB\/sec      8.931 GiB\/sec    -1.761               {'family_index': 2, 'per_family_instance_index': 24, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/12\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 13060, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/6    754.577 MiB\/sec    740.597 MiB\/sec    -1.853                                             {'family_index': 5, 'per_family_instance_index': 6, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1053, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/13\/9      1.504 GiB\/sec      1.475 GiB\/sec    -1.891                {'family_index': 2, 'per_family_instance_index': 27, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/13\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2140, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/5\/8     11.343 GiB\/sec     11.118 GiB\/sec    -1.988                {'family_index': 3, 'per_family_instance_index': 10, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/5\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15996, 'byte_width': 8.0, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n     TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/10\/8 214.381M items\/sec 209.986M items\/sec    -2.050                                            {'family_index': 25, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedFSBRandomIndicesNoNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 287, 'byte_width': 8.0, 'null_percent': 10.0}\r\n                        FilterStringFilterNoNulls\/524288\/6      2.132 GiB\/sec      2.087 GiB\/sec    -2.066                                                 {'family_index': 6, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterNoNulls\/524288\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3012, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n             FilterFixedSizeBinaryFilterNoNulls\/524288\/6\/9      9.965 GiB\/sec      9.751 GiB\/sec    -2.148                 {'family_index': 2, 'per_family_instance_index': 13, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/6\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 14387, 'byte_width': 9.0, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/14\/9     11.809 GiB\/sec     11.550 GiB\/sec    -2.195              {'family_index': 3, 'per_family_instance_index': 29, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/14\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16994, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/9    640.749 MiB\/sec    626.489 MiB\/sec    -2.226                                               {'family_index': 4, 'per_family_instance_index': 9, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 883, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/4      1.013 GiB\/sec   1013.697 MiB\/sec    -2.246                                               {'family_index': 4, 'per_family_instance_index': 4, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1453, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n                     FilterFSLInt64FilterNoNulls\/524288\/12    445.948 MiB\/sec    435.845 MiB\/sec    -2.266                                             {'family_index': 4, 'per_family_instance_index': 12, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 627, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/3      1.013 GiB\/sec   1012.775 MiB\/sec    -2.395                                               {'family_index': 4, 'per_family_instance_index': 3, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1450, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n                           FilterRecordBatchWithNulls\/10\/5    940.331 MiB\/sec    917.625 MiB\/sec    -2.415        {'family_index': 9, 'per_family_instance_index': 5, 'run_name': 'FilterRecordBatchWithNulls\/10\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 884, 'data null%': 0.1, 'extracted_size': 770000.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/0      1.308 GiB\/sec      1.276 GiB\/sec    -2.465                                               {'family_index': 4, 'per_family_instance_index': 0, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1884, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/7    931.532 MiB\/sec    908.252 MiB\/sec    -2.499                                               {'family_index': 4, 'per_family_instance_index': 7, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1297, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/3    792.209 MiB\/sec    771.912 MiB\/sec    -2.562                                             {'family_index': 5, 'per_family_instance_index': 3, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1115, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n           FilterFixedSizeBinaryFilterWithNulls\/524288\/2\/8     11.572 GiB\/sec     11.273 GiB\/sec    -2.579                 {'family_index': 3, 'per_family_instance_index': 4, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16510, 'byte_width': 8.0, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/5      9.128 GiB\/sec      8.892 GiB\/sec    -2.592                                             {'family_index': 5, 'per_family_instance_index': 5, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 13083, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 1.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/1      1.149 GiB\/sec      1.118 GiB\/sec    -2.630                                               {'family_index': 4, 'per_family_instance_index': 1, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1643, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/0\/8 707.832M items\/sec 689.138M items\/sec    -2.641                                                {'family_index': 13, 'per_family_instance_index': 8, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/0\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 945, 'byte_width': 8.0, 'null_percent': 0.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/8      9.075 GiB\/sec      8.834 GiB\/sec    -2.652                                             {'family_index': 5, 'per_family_instance_index': 8, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 13008, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 1.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/7    862.481 MiB\/sec    838.890 MiB\/sec    -2.735                                             {'family_index': 5, 'per_family_instance_index': 7, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1207, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                     FilterFSLInt64FilterNoNulls\/524288\/13    600.656 MiB\/sec    582.994 MiB\/sec    -2.940                                             {'family_index': 4, 'per_family_instance_index': 13, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 842, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/5     11.969 GiB\/sec     11.598 GiB\/sec    -3.104                                               {'family_index': 4, 'per_family_instance_index': 5, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16968, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/14\/8     14.492 GiB\/sec     14.038 GiB\/sec    -3.127                {'family_index': 2, 'per_family_instance_index': 28, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/14\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 21115, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                           FilterRecordBatchWithNulls\/10\/2    929.225 MiB\/sec    899.277 MiB\/sec    -3.223        {'family_index': 9, 'per_family_instance_index': 2, 'run_name': 'FilterRecordBatchWithNulls\/10\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 891, 'data null%': 0.0, 'extracted_size': 770000.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 1.0}\r\n                     FilterFSLInt64FilterNoNulls\/524288\/14     10.090 GiB\/sec      9.761 GiB\/sec    -3.261                                            {'family_index': 4, 'per_family_instance_index': 14, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 14372, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                     FilterFSLInt64FilterNoNulls\/524288\/10    770.094 MiB\/sec    743.686 MiB\/sec    -3.429                                            {'family_index': 4, 'per_family_instance_index': 10, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1079, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n                        FilterStringFilterNoNulls\/524288\/2     95.515 GiB\/sec     92.215 GiB\/sec    -3.455                                                {'family_index': 6, 'per_family_instance_index': 2, 'run_name': 'FilterStringFilterNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 135480, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                           FilterRecordBatchWithNulls\/10\/3      2.294 GiB\/sec      2.213 GiB\/sec    -3.502      {'family_index': 9, 'per_family_instance_index': 3, 'run_name': 'FilterRecordBatchWithNulls\/10\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 23, 'data null%': 0.1, 'extracted_size': 75915760.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 99.9}\r\n                          FilterRecordBatchWithNulls\/10\/13      1.944 GiB\/sec      1.875 GiB\/sec    -3.557   {'family_index': 9, 'per_family_instance_index': 13, 'run_name': 'FilterRecordBatchWithNulls\/10\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39, 'data null%': 90.0, 'extracted_size': 37945920.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 50.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/10\/8      1.628 GiB\/sec      1.569 GiB\/sec    -3.622              {'family_index': 3, 'per_family_instance_index': 20, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/10\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2188, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                     FilterFSLInt64FilterNoNulls\/524288\/11     11.121 GiB\/sec     10.718 GiB\/sec    -3.624                                            {'family_index': 4, 'per_family_instance_index': 11, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15843, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/4    886.152 MiB\/sec    853.899 MiB\/sec    -3.640                                             {'family_index': 5, 'per_family_instance_index': 4, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1236, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/2     12.917 GiB\/sec     12.429 GiB\/sec    -3.777                                               {'family_index': 4, 'per_family_instance_index': 2, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 18441, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                      FilterFSLInt64FilterNoNulls\/524288\/8     11.470 GiB\/sec     11.028 GiB\/sec    -3.852                                               {'family_index': 4, 'per_family_instance_index': 8, 'run_name': 'FilterFSLInt64FilterNoNulls\/524288\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16526, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/13\/8      1.633 GiB\/sec      1.569 GiB\/sec    -3.953              {'family_index': 3, 'per_family_instance_index': 26, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/13\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2341, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                     TakeFSLInt64MonotonicIndices\/524288\/0 145.149M items\/sec 139.401M items\/sec    -3.960                                                                                {'family_index': 18, 'per_family_instance_index': 4, 'run_name': 'TakeFSLInt64MonotonicIndices\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 194, 'null_percent': 0.0}\r\n                    FilterFSLInt64FilterWithNulls\/524288\/2      9.853 GiB\/sec      9.459 GiB\/sec    -4.004                                             {'family_index': 5, 'per_family_instance_index': 2, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 13968, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 1.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/12\/9      1.882 GiB\/sec      1.803 GiB\/sec    -4.182              {'family_index': 3, 'per_family_instance_index': 25, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/12\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2704, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n        TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/2\/8 179.819M items\/sec 172.117M items\/sec    -4.283                                               {'family_index': 13, 'per_family_instance_index': 4, 'run_name': 'TakeFixedSizeBinaryRandomIndicesNoNulls\/524288\/2\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 239, 'byte_width': 8.0, 'null_percent': 50.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/14\/8     11.463 GiB\/sec     10.955 GiB\/sec    -4.430              {'family_index': 3, 'per_family_instance_index': 28, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/14\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16387, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 1.0}\r\n            FilterFixedSizeBinaryFilterNoNulls\/524288\/11\/8     14.555 GiB\/sec     13.889 GiB\/sec    -4.580                {'family_index': 2, 'per_family_instance_index': 22, 'run_name': 'FilterFixedSizeBinaryFilterNoNulls\/524288\/11\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20975, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/11\/8     11.505 GiB\/sec     10.968 GiB\/sec    -4.673              {'family_index': 3, 'per_family_instance_index': 22, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/11\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 16267, 'byte_width': 8.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n    TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/2 102.309M items\/sec  97.416M items\/sec    -4.783                                                              {'family_index': 23, 'per_family_instance_index': 2, 'run_name': 'TakeChunkedChunkedInt64RandomIndicesWithNulls\/524288\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 136, 'null_percent': 50.0}\r\n                        FilterInt64FilterNoNulls\/524288\/14     14.564 GiB\/sec     13.862 GiB\/sec    -4.817                                               {'family_index': 0, 'per_family_instance_index': 14, 'run_name': 'FilterInt64FilterNoNulls\/524288\/14', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20755, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 1.0}\r\n          FilterFixedSizeBinaryFilterWithNulls\/524288\/11\/9     11.985 GiB\/sec     11.390 GiB\/sec    -4.963              {'family_index': 3, 'per_family_instance_index': 23, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/11\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 17179, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 1.0}\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nRegressions: (22)\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                                               benchmark           baseline          contender  change %                                                                                                                                                                                                                                                                                 counters\r\n                  FilterFSLInt64FilterWithNulls\/524288\/0    947.840 MiB\/sec    899.346 MiB\/sec    -5.116                                              {'family_index': 5, 'per_family_instance_index': 0, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1331, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n             TakeFSLInt64RandomIndicesWithNulls\/524288\/0 128.668M items\/sec 121.940M items\/sec    -5.229                                                                           {'family_index': 17, 'per_family_instance_index': 4, 'run_name': 'TakeFSLInt64RandomIndicesWithNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 172, 'null_percent': 0.0}\r\n        FilterFixedSizeBinaryFilterWithNulls\/524288\/10\/9      1.462 GiB\/sec      1.385 GiB\/sec    -5.260               {'family_index': 3, 'per_family_instance_index': 21, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/10\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2106, 'byte_width': 9.0, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                           FilterRecordBatchNoNulls\/10\/4      2.412 GiB\/sec      2.284 GiB\/sec    -5.317         {'family_index': 8, 'per_family_instance_index': 4, 'run_name': 'FilterRecordBatchNoNulls\/10\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 46, 'data null%': 0.1, 'extracted_size': 39957840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n        FilterFixedSizeBinaryFilterWithNulls\/524288\/12\/8      2.617 GiB\/sec      2.477 GiB\/sec    -5.328               {'family_index': 3, 'per_family_instance_index': 24, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/12\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3751, 'byte_width': 8.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n                       FilterRecordBatchWithNulls\/100\/10      6.530 GiB\/sec      6.174 GiB\/sec    -5.458 {'family_index': 9, 'per_family_instance_index': 40, 'run_name': 'FilterRecordBatchWithNulls\/100\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 109, 'data null%': 10.0, 'extracted_size': 37841600.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                          FilterRecordBatchNoNulls\/50\/10      6.229 GiB\/sec      5.882 GiB\/sec    -5.561      {'family_index': 8, 'per_family_instance_index': 25, 'run_name': 'FilterRecordBatchNoNulls\/50\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 98, 'data null%': 10.0, 'extracted_size': 39935600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n        FilterFixedSizeBinaryFilterWithNulls\/524288\/13\/9      1.477 GiB\/sec      1.393 GiB\/sec    -5.665               {'family_index': 3, 'per_family_instance_index': 27, 'run_name': 'FilterFixedSizeBinaryFilterWithNulls\/524288\/13\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2102, 'byte_width': 9.0, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                  FilterFSLInt64FilterWithNulls\/524288\/1   1012.983 MiB\/sec    949.123 MiB\/sec    -6.304                                              {'family_index': 5, 'per_family_instance_index': 1, 'run_name': 'FilterFSLInt64FilterWithNulls\/524288\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1415, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n                           FilterRecordBatchNoNulls\/10\/1      2.366 GiB\/sec      2.216 GiB\/sec    -6.352         {'family_index': 8, 'per_family_instance_index': 1, 'run_name': 'FilterRecordBatchNoNulls\/10\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 46, 'data null%': 0.0, 'extracted_size': 39957840.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                         FilterRecordBatchNoNulls\/100\/12      3.863 GiB\/sec      3.592 GiB\/sec    -7.003    {'family_index': 8, 'per_family_instance_index': 42, 'run_name': 'FilterRecordBatchNoNulls\/100\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 36, 'data null%': 90.0, 'extracted_size': 79928800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n                      FilterInt64FilterNoNulls\/524288\/11     14.511 GiB\/sec     13.435 GiB\/sec    -7.413                                                {'family_index': 0, 'per_family_instance_index': 11, 'run_name': 'FilterInt64FilterNoNulls\/524288\/11', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 20624, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 1.0}\r\n                         FilterRecordBatchWithNulls\/50\/9      3.432 GiB\/sec      3.173 GiB\/sec    -7.532     {'family_index': 9, 'per_family_instance_index': 24, 'run_name': 'FilterRecordBatchWithNulls\/50\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 34, 'data null%': 10.0, 'extracted_size': 75932400.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\nTakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/1000  31.239M items\/sec  28.606M items\/sec    -8.428                                                               {'family_index': 28, 'per_family_instance_index': 0, 'run_name': 'TakeChunkedChunkedStringRandomIndicesNoNulls\/524288\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 40, 'null_percent': 0.1}\r\n                          FilterRecordBatchNoNulls\/100\/4      8.481 GiB\/sec      7.718 GiB\/sec    -9.002     {'family_index': 8, 'per_family_instance_index': 34, 'run_name': 'FilterRecordBatchNoNulls\/100\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 161, 'data null%': 0.1, 'extracted_size': 39840800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                         FilterRecordBatchWithNulls\/10\/1      1.957 GiB\/sec      1.758 GiB\/sec   -10.179       {'family_index': 9, 'per_family_instance_index': 1, 'run_name': 'FilterRecordBatchWithNulls\/10\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 39, 'data null%': 0.0, 'extracted_size': 37945920.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                         FilterRecordBatchWithNulls\/10\/4      2.072 GiB\/sec      1.836 GiB\/sec   -11.406       {'family_index': 9, 'per_family_instance_index': 4, 'run_name': 'FilterRecordBatchWithNulls\/10\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 42, 'data null%': 0.1, 'extracted_size': 37945920.0, 'mask null%': 5.0, 'num_cols': 10.0, 'select%': 50.0}\r\n                      FilterStringFilterNoNulls\/524288\/3      5.123 GiB\/sec      4.520 GiB\/sec   -11.772                                                  {'family_index': 6, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterNoNulls\/524288\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 7234, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n                      FilterStringFilterNoNulls\/524288\/0     11.172 GiB\/sec      9.735 GiB\/sec   -12.864                                                 {'family_index': 6, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterNoNulls\/524288\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 15929, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n                           FilterRecordBatchNoNulls\/50\/4      8.021 GiB\/sec      4.471 GiB\/sec   -44.263       {'family_index': 8, 'per_family_instance_index': 19, 'run_name': 'FilterRecordBatchNoNulls\/50\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 109, 'data null%': 0.1, 'extracted_size': 39935600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n                         FilterRecordBatchNoNulls\/100\/13      8.611 GiB\/sec      4.709 GiB\/sec   -45.312   {'family_index': 8, 'per_family_instance_index': 43, 'run_name': 'FilterRecordBatchNoNulls\/100\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 162, 'data null%': 90.0, 'extracted_size': 39840800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n                          FilterRecordBatchNoNulls\/50\/13      8.247 GiB\/sec      3.588 GiB\/sec   -56.493     {'family_index': 8, 'per_family_instance_index': 28, 'run_name': 'FilterRecordBatchNoNulls\/50\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 155, 'data null%': 90.0, 'extracted_size': 39935600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n```\r\n","I spent a lot of time trying to figure out what could possibly be causing these regressions, but it turns out to be randomness caused by memory operations and branch predictions.\r\n\r\nWhen running the benchmarks on a commit that doesn't even touch the code I get sizeable regressions too.\r\n\r\n```\r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nRegressions: (42)\r\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                                   benchmark           baseline          contender  change %                                                                                                                                                                                                                                                                                counters\r\n       FilterFSLInt64FilterNoNulls\/4194304\/8     12.655 GiB\/sec     11.998 GiB\/sec    -5.188                                               {'family_index': 2, 'per_family_instance_index': 8, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/8', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2257, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 1.0}\r\n            FilterRecordBatchWithNulls\/100\/9      5.371 GiB\/sec      5.078 GiB\/sec    -5.448  {'family_index': 7, 'per_family_instance_index': 39, 'run_name': 'FilterRecordBatchWithNulls\/100\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 50, 'data null%': 10.0, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n          FilterInt64FilterNoNulls\/4194304\/7      1.658 GiB\/sec      1.566 GiB\/sec    -5.562                                                  {'family_index': 0, 'per_family_instance_index': 7, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 299, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n             FilterRecordBatchWithNulls\/50\/7      3.781 GiB\/sec      3.570 GiB\/sec    -5.596     {'family_index': 7, 'per_family_instance_index': 22, 'run_name': 'FilterRecordBatchWithNulls\/50\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 68, 'data null%': 1.0, 'extracted_size': 37909600.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n           FilterRecordBatchWithNulls\/100\/10      3.966 GiB\/sec      3.733 GiB\/sec    -5.883 {'family_index': 7, 'per_family_instance_index': 40, 'run_name': 'FilterRecordBatchWithNulls\/100\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 69, 'data null%': 10.0, 'extracted_size': 37952800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n  TakeInt64RandomIndicesWithNulls\/4194304\/10 109.672M items\/sec 102.977M items\/sec    -6.104                                                                            {'family_index': 9, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 18, 'null_percent': 10.0}\r\nTakeInt64RandomIndicesWithNulls\/4194304\/1000 134.146M items\/sec 125.634M items\/sec    -6.346                                                                           {'family_index': 9, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 22, 'null_percent': 0.1}\r\n    TakeInt64RandomIndicesNoNulls\/4194304\/10 142.865M items\/sec 133.343M items\/sec    -6.665                                                                              {'family_index': 8, 'per_family_instance_index': 1, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 24, 'null_percent': 10.0}\r\n         FilterStringFilterNoNulls\/4194304\/7      1.261 GiB\/sec      1.177 GiB\/sec    -6.730                                                 {'family_index': 4, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterNoNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 224, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n             FilterRecordBatchWithNulls\/50\/6      6.255 GiB\/sec      5.830 GiB\/sec    -6.799     {'family_index': 7, 'per_family_instance_index': 21, 'run_name': 'FilterRecordBatchWithNulls\/50\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 58, 'data null%': 1.0, 'extracted_size': 75924000.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 99.9}\r\n  TakeInt64RandomIndicesNoNulls\/4194304\/1000 136.858M items\/sec 127.408M items\/sec    -6.906                                                                             {'family_index': 8, 'per_family_instance_index': 0, 'run_name': 'TakeInt64RandomIndicesNoNulls\/4194304\/1000', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 23, 'null_percent': 0.1}\r\n            FilterRecordBatchWithNulls\/100\/0      5.548 GiB\/sec      5.157 GiB\/sec    -7.059   {'family_index': 7, 'per_family_instance_index': 30, 'run_name': 'FilterRecordBatchWithNulls\/100\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 51, 'data null%': 0.0, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n         FilterInt64FilterNoNulls\/4194304\/10      1.625 GiB\/sec      1.505 GiB\/sec    -7.385                                               {'family_index': 0, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 294, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n           FilterRecordBatchWithNulls\/100\/12      5.554 GiB\/sec      5.144 GiB\/sec    -7.385 {'family_index': 7, 'per_family_instance_index': 42, 'run_name': 'FilterRecordBatchWithNulls\/100\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 52, 'data null%': 90.0, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n         FilterStringFilterNoNulls\/4194304\/3      2.469 GiB\/sec      2.285 GiB\/sec    -7.461                                                 {'family_index': 4, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterNoNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 446, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n            FilterRecordBatchWithNulls\/100\/3      5.536 GiB\/sec      5.100 GiB\/sec    -7.866   {'family_index': 7, 'per_family_instance_index': 33, 'run_name': 'FilterRecordBatchWithNulls\/100\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 50, 'data null%': 0.1, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n         FilterStringFilterNoNulls\/4194304\/0      2.847 GiB\/sec      2.617 GiB\/sec    -8.056                                                 {'family_index': 4, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterNoNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 521, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n         FilterInt64FilterNoNulls\/4194304\/13      1.636 GiB\/sec      1.501 GiB\/sec    -8.242                                               {'family_index': 0, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 294, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n       FilterFSLInt64FilterNoNulls\/4194304\/4      1.101 GiB\/sec      1.004 GiB\/sec    -8.853                                               {'family_index': 2, 'per_family_instance_index': 4, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 197, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n            FilterRecordBatchWithNulls\/100\/7      3.947 GiB\/sec      3.593 GiB\/sec    -8.964   {'family_index': 7, 'per_family_instance_index': 37, 'run_name': 'FilterRecordBatchWithNulls\/100\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 75, 'data null%': 1.0, 'extracted_size': 37952800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n       FilterFSLInt64FilterNoNulls\/4194304\/5     13.319 GiB\/sec     12.088 GiB\/sec    -9.246                                               {'family_index': 2, 'per_family_instance_index': 5, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2365, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 1.0}\r\n        FilterInt64FilterWithNulls\/4194304\/6      2.960 GiB\/sec      2.685 GiB\/sec    -9.291                                                {'family_index': 1, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 526, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n   TakeInt64RandomIndicesWithNulls\/4194304\/1   1.451G items\/sec   1.315G items\/sec    -9.434                                                                           {'family_index': 9, 'per_family_instance_index': 3, 'run_name': 'TakeInt64RandomIndicesWithNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 242, 'null_percent': 100.0}\r\n       FilterFSLInt64FilterNoNulls\/4194304\/2     13.907 GiB\/sec     12.463 GiB\/sec   -10.381                                               {'family_index': 2, 'per_family_instance_index': 2, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/2', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2485, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 1.0}\r\n       FilterFSLInt64FilterNoNulls\/4194304\/1      1.274 GiB\/sec      1.132 GiB\/sec   -11.141                                               {'family_index': 2, 'per_family_instance_index': 1, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 230, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n        FilterStringFilterNoNulls\/4194304\/10      1.245 GiB\/sec      1.100 GiB\/sec   -11.619                                              {'family_index': 4, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 227, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n         FilterStringFilterNoNulls\/4194304\/9    850.490 MiB\/sec    749.768 MiB\/sec   -11.843                                                {'family_index': 4, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterNoNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 147, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n        FilterInt64FilterWithNulls\/4194304\/3      3.212 GiB\/sec      2.822 GiB\/sec   -12.143                                                {'family_index': 1, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 570, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n         FilterStringFilterNoNulls\/4194304\/6      1.360 GiB\/sec      1.192 GiB\/sec   -12.345                                                 {'family_index': 4, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterNoNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 243, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n       FilterStringFilterWithNulls\/4194304\/4      1.252 GiB\/sec      1.085 GiB\/sec   -13.320                                               {'family_index': 5, 'per_family_instance_index': 4, 'run_name': 'FilterStringFilterWithNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 220, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\n        FilterStringFilterNoNulls\/4194304\/13    277.736 MiB\/sec    240.290 MiB\/sec   -13.483                                               {'family_index': 4, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterNoNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 48, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n       FilterStringFilterWithNulls\/4194304\/3    976.622 MiB\/sec    844.007 MiB\/sec   -13.579                                               {'family_index': 5, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterWithNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 172, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n        FilterInt64FilterWithNulls\/4194304\/0      3.268 GiB\/sec      2.789 GiB\/sec   -14.677                                                {'family_index': 1, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 567, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n       FilterStringFilterWithNulls\/4194304\/1      1.272 GiB\/sec      1.084 GiB\/sec   -14.788                                               {'family_index': 5, 'per_family_instance_index': 1, 'run_name': 'FilterStringFilterWithNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 221, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n       FilterStringFilterWithNulls\/4194304\/6    952.154 MiB\/sec    800.961 MiB\/sec   -15.879                                               {'family_index': 5, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterWithNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 165, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n      FilterStringFilterWithNulls\/4194304\/10      1.236 GiB\/sec      1.038 GiB\/sec   -16.001                                            {'family_index': 5, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterWithNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 223, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n       FilterStringFilterWithNulls\/4194304\/7      1.254 GiB\/sec      1.052 GiB\/sec   -16.104                                               {'family_index': 5, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterWithNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 223, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\n       FilterStringFilterWithNulls\/4194304\/0    995.479 MiB\/sec    831.870 MiB\/sec   -16.435                                               {'family_index': 5, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 174, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n       FilterStringFilterWithNulls\/4194304\/9    897.171 MiB\/sec    739.897 MiB\/sec   -17.530                                              {'family_index': 5, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterWithNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 157, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n      FilterStringFilterWithNulls\/4194304\/13    258.601 MiB\/sec    203.408 MiB\/sec   -21.343                                             {'family_index': 5, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterWithNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 45, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\n      FilterStringFilterWithNulls\/4194304\/12    370.972 MiB\/sec    278.844 MiB\/sec   -24.834                                             {'family_index': 5, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterWithNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 65, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n        FilterStringFilterNoNulls\/4194304\/12    442.746 MiB\/sec    298.930 MiB\/sec   -32.483                                               {'family_index': 4, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterNoNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 77, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n```\r\n        \r\nUPDATE: with all my changes applied:\r\n        \r\n```\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nRegressions: (44)\r\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                             benchmark        baseline        contender  change %                                                                                                                                                                                                                                                                                counters\r\n      FilterRecordBatchWithNulls\/100\/6   5.531 GiB\/sec    5.254 GiB\/sec    -5.019   {'family_index': 7, 'per_family_instance_index': 36, 'run_name': 'FilterRecordBatchWithNulls\/100\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 51, 'data null%': 1.0, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n        FilterRecordBatchNoNulls\/50\/12   6.004 GiB\/sec    5.692 GiB\/sec    -5.197     {'family_index': 6, 'per_family_instance_index': 27, 'run_name': 'FilterRecordBatchNoNulls\/50\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 56, 'data null%': 90.0, 'extracted_size': 79921600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n FilterInt64FilterWithNulls\/4194304\/12   2.834 GiB\/sec    2.685 GiB\/sec    -5.275                                             {'family_index': 1, 'per_family_instance_index': 12, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 501, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n      FilterRecordBatchWithNulls\/100\/3   5.333 GiB\/sec    5.047 GiB\/sec    -5.361   {'family_index': 7, 'per_family_instance_index': 33, 'run_name': 'FilterRecordBatchWithNulls\/100\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 47, 'data null%': 0.1, 'extracted_size': 75952000.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 99.9}\r\n      FilterRecordBatchWithNulls\/100\/4   3.996 GiB\/sec    3.777 GiB\/sec    -5.481   {'family_index': 7, 'per_family_instance_index': 34, 'run_name': 'FilterRecordBatchWithNulls\/100\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 68, 'data null%': 0.1, 'extracted_size': 37952800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n         FilterRecordBatchNoNulls\/50\/9   6.132 GiB\/sec    5.796 GiB\/sec    -5.482      {'family_index': 6, 'per_family_instance_index': 24, 'run_name': 'FilterRecordBatchNoNulls\/50\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 52, 'data null%': 10.0, 'extracted_size': 79921600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 99.9}\r\n   FilterStringFilterNoNulls\/4194304\/3   2.427 GiB\/sec    2.292 GiB\/sec    -5.568                                                 {'family_index': 4, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterNoNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 428, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 99.9}\r\n       FilterRecordBatchNoNulls\/100\/10   4.213 GiB\/sec    3.973 GiB\/sec    -5.689   {'family_index': 6, 'per_family_instance_index': 40, 'run_name': 'FilterRecordBatchNoNulls\/100\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 71, 'data null%': 10.0, 'extracted_size': 39960800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n  FilterInt64FilterWithNulls\/4194304\/9   2.852 GiB\/sec    2.689 GiB\/sec    -5.729                                               {'family_index': 1, 'per_family_instance_index': 9, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 506, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterFSLInt64FilterNoNulls\/4194304\/4   1.101 GiB\/sec    1.036 GiB\/sec    -5.839                                               {'family_index': 2, 'per_family_instance_index': 4, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 197, 'data null%': 0.1, 'mask null%': 0.0, 'select%': 50.0}\r\n     FilterRecordBatchWithNulls\/100\/10   4.036 GiB\/sec    3.795 GiB\/sec    -5.974 {'family_index': 7, 'per_family_instance_index': 40, 'run_name': 'FilterRecordBatchWithNulls\/100\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 71, 'data null%': 10.0, 'extracted_size': 37952800.0, 'mask null%': 5.0, 'num_cols': 100.0, 'select%': 50.0}\r\n      FilterRecordBatchWithNulls\/50\/13   3.882 GiB\/sec    3.647 GiB\/sec    -6.046   {'family_index': 7, 'per_family_instance_index': 28, 'run_name': 'FilterRecordBatchWithNulls\/50\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 69, 'data null%': 90.0, 'extracted_size': 37909600.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n   FilterStringFilterNoNulls\/4194304\/7   1.259 GiB\/sec    1.181 GiB\/sec    -6.199                                                 {'family_index': 4, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterNoNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 228, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 50.0}\r\n        FilterRecordBatchNoNulls\/100\/6   5.905 GiB\/sec    5.500 GiB\/sec    -6.863     {'family_index': 6, 'per_family_instance_index': 36, 'run_name': 'FilterRecordBatchNoNulls\/100\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 50, 'data null%': 1.0, 'extracted_size': 79927200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n      FilterRecordBatchWithNulls\/50\/10   3.870 GiB\/sec    3.598 GiB\/sec    -7.024   {'family_index': 7, 'per_family_instance_index': 25, 'run_name': 'FilterRecordBatchWithNulls\/50\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 68, 'data null%': 10.0, 'extracted_size': 37909600.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n        FilterRecordBatchNoNulls\/100\/0   5.850 GiB\/sec    5.432 GiB\/sec    -7.136     {'family_index': 6, 'per_family_instance_index': 30, 'run_name': 'FilterRecordBatchNoNulls\/100\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 49, 'data null%': 0.0, 'extracted_size': 79927200.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 99.9}\r\n         FilterRecordBatchNoNulls\/10\/7   4.047 GiB\/sec    3.757 GiB\/sec    -7.166        {'family_index': 6, 'per_family_instance_index': 7, 'run_name': 'FilterRecordBatchNoNulls\/10\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 68, 'data null%': 1.0, 'extracted_size': 39981680.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 50.0}\r\n        FilterRecordBatchNoNulls\/100\/4   4.225 GiB\/sec    3.913 GiB\/sec    -7.384     {'family_index': 6, 'per_family_instance_index': 34, 'run_name': 'FilterRecordBatchNoNulls\/100\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 74, 'data null%': 0.1, 'extracted_size': 39960800.0, 'mask null%': 0.0, 'num_cols': 100.0, 'select%': 50.0}\r\n   FilterInt64FilterNoNulls\/4194304\/10   1.627 GiB\/sec    1.501 GiB\/sec    -7.798                                               {'family_index': 0, 'per_family_instance_index': 10, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 294, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n   FilterInt64FilterNoNulls\/4194304\/13   1.634 GiB\/sec    1.501 GiB\/sec    -8.119                                               {'family_index': 0, 'per_family_instance_index': 13, 'run_name': 'FilterInt64FilterNoNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 294, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n       FilterRecordBatchWithNulls\/50\/4   3.768 GiB\/sec    3.459 GiB\/sec    -8.197     {'family_index': 7, 'per_family_instance_index': 19, 'run_name': 'FilterRecordBatchWithNulls\/50\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 69, 'data null%': 0.1, 'extracted_size': 37909600.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n   FilterStringFilterNoNulls\/4194304\/0   2.802 GiB\/sec    2.564 GiB\/sec    -8.508                                                 {'family_index': 4, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterNoNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 493, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 99.9}\r\n FilterFSLInt64FilterNoNulls\/4194304\/1   1.286 GiB\/sec    1.176 GiB\/sec    -8.533                                               {'family_index': 2, 'per_family_instance_index': 1, 'run_name': 'FilterFSLInt64FilterNoNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 230, 'data null%': 0.0, 'mask null%': 0.0, 'select%': 50.0}\r\n       FilterRecordBatchWithNulls\/50\/7   3.767 GiB\/sec    3.419 GiB\/sec    -9.231     {'family_index': 7, 'per_family_instance_index': 22, 'run_name': 'FilterRecordBatchWithNulls\/50\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 67, 'data null%': 1.0, 'extracted_size': 37909600.0, 'mask null%': 5.0, 'num_cols': 50.0, 'select%': 50.0}\r\n   FilterStringFilterNoNulls\/4194304\/6   1.345 GiB\/sec    1.214 GiB\/sec    -9.736                                                 {'family_index': 4, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterNoNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 240, 'data null%': 1.0, 'mask null%': 0.0, 'select%': 99.9}\r\n   FilterStringFilterNoNulls\/4194304\/9 833.919 MiB\/sec  751.049 MiB\/sec    -9.937                                                {'family_index': 4, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterNoNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 148, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 99.9}\r\n  FilterInt64FilterWithNulls\/4194304\/6   3.008 GiB\/sec    2.684 GiB\/sec   -10.771                                                {'family_index': 1, 'per_family_instance_index': 6, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 531, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n         FilterRecordBatchNoNulls\/10\/5   1.589 GiB\/sec    1.415 GiB\/sec   -10.967         {'family_index': 6, 'per_family_instance_index': 5, 'run_name': 'FilterRecordBatchNoNulls\/10\/5', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 1429, 'data null%': 0.1, 'extracted_size': 790480.0, 'mask null%': 0.0, 'num_cols': 10.0, 'select%': 1.0}\r\n        FilterRecordBatchNoNulls\/50\/13   4.327 GiB\/sec    3.844 GiB\/sec   -11.150     {'family_index': 6, 'per_family_instance_index': 28, 'run_name': 'FilterRecordBatchNoNulls\/50\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 71, 'data null%': 90.0, 'extracted_size': 39913600.0, 'mask null%': 0.0, 'num_cols': 50.0, 'select%': 50.0}\r\n FilterStringFilterWithNulls\/4194304\/1   1.225 GiB\/sec    1.086 GiB\/sec   -11.316                                               {'family_index': 5, 'per_family_instance_index': 1, 'run_name': 'FilterStringFilterWithNulls\/4194304\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 216, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 50.0}\r\n  FilterInt64FilterWithNulls\/4194304\/3   3.221 GiB\/sec    2.820 GiB\/sec   -12.452                                                {'family_index': 1, 'per_family_instance_index': 3, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 565, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n  FilterStringFilterNoNulls\/4194304\/10   1.268 GiB\/sec    1.110 GiB\/sec   -12.512                                              {'family_index': 4, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterNoNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 227, 'data null%': 10.0, 'mask null%': 0.0, 'select%': 50.0}\r\n  FilterStringFilterNoNulls\/4194304\/13 275.253 MiB\/sec  239.649 MiB\/sec   -12.935                                               {'family_index': 4, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterNoNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 49, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 50.0}\r\n  FilterInt64FilterWithNulls\/4194304\/0   3.290 GiB\/sec    2.838 GiB\/sec   -13.730                                                {'family_index': 1, 'per_family_instance_index': 0, 'run_name': 'FilterInt64FilterWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 577, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterStringFilterWithNulls\/4194304\/0 981.662 MiB\/sec  828.349 MiB\/sec   -15.618                                               {'family_index': 5, 'per_family_instance_index': 0, 'run_name': 'FilterStringFilterWithNulls\/4194304\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 168, 'data null%': 0.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterStringFilterWithNulls\/4194304\/3 992.118 MiB\/sec  836.884 MiB\/sec   -15.647                                               {'family_index': 5, 'per_family_instance_index': 3, 'run_name': 'FilterStringFilterWithNulls\/4194304\/3', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 175, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterStringFilterWithNulls\/4194304\/4   1.225 GiB\/sec    1.023 GiB\/sec   -16.474                                               {'family_index': 5, 'per_family_instance_index': 4, 'run_name': 'FilterStringFilterWithNulls\/4194304\/4', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 223, 'data null%': 0.1, 'mask null%': 5.0, 'select%': 50.0}\r\nFilterStringFilterWithNulls\/4194304\/10   1.242 GiB\/sec    1.021 GiB\/sec   -17.775                                            {'family_index': 5, 'per_family_instance_index': 10, 'run_name': 'FilterStringFilterWithNulls\/4194304\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 218, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 50.0}\r\n FilterStringFilterWithNulls\/4194304\/9 904.356 MiB\/sec  729.773 MiB\/sec   -19.305                                              {'family_index': 5, 'per_family_instance_index': 9, 'run_name': 'FilterStringFilterWithNulls\/4194304\/9', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 157, 'data null%': 10.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterStringFilterWithNulls\/4194304\/6 968.160 MiB\/sec  776.877 MiB\/sec   -19.757                                               {'family_index': 5, 'per_family_instance_index': 6, 'run_name': 'FilterStringFilterWithNulls\/4194304\/6', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 170, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 99.9}\r\n FilterStringFilterWithNulls\/4194304\/7   1.239 GiB\/sec 1010.961 MiB\/sec   -20.301                                               {'family_index': 5, 'per_family_instance_index': 7, 'run_name': 'FilterStringFilterWithNulls\/4194304\/7', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 221, 'data null%': 1.0, 'mask null%': 5.0, 'select%': 50.0}\r\nFilterStringFilterWithNulls\/4194304\/13 259.727 MiB\/sec  206.531 MiB\/sec   -20.481                                             {'family_index': 5, 'per_family_instance_index': 13, 'run_name': 'FilterStringFilterWithNulls\/4194304\/13', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 45, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 50.0}\r\nFilterStringFilterWithNulls\/4194304\/12 369.387 MiB\/sec  279.425 MiB\/sec   -24.354                                             {'family_index': 5, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterWithNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 65, 'data null%': 90.0, 'mask null%': 5.0, 'select%': 99.9}\r\n  FilterStringFilterNoNulls\/4194304\/12 442.561 MiB\/sec  299.625 MiB\/sec   -32.297                                               {'family_index': 4, 'per_family_instance_index': 12, 'run_name': 'FilterStringFilterNoNulls\/4194304\/12', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 76, 'data null%': 90.0, 'mask null%': 0.0, 'select%': 99.9}\r\n  ```"],"labels":["Component: C++","awaiting committer review"]},{"title":"[C++] Extract the primitive operations implementing Take and make them instanciable in multiple scenarios","body":"### Describe the enhancement requested\n\nThe first thing that can be achieved is unifying the implementation of Take for numeric and boolean arrays.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[C++]  How to get row group sizes for a parquet file in remote FS","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nHi team. I want to get the metadate of a parquet file which is stored in remote HDFS. Specifically I'd like to know how many rows  of all row_groups with C++ api without pulling the whole parquet file. I wonder if it's feasible. And if yes, is there any example? Thanks!\r\n\r\n### Component(s)\r\n\r\nParquet","comments":[],"labels":["Component: Parquet","Type: usage"]},{"title":"[CI][C++] Fix arrow-s3fs-test timeouts on macOS C++ job","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThe `arrow-s3fs-test` has been failing due to what appears to be a timeout on the \"AMD64 macOS 12 C++\" job for a while.\r\n\r\n\r\nA recent example is: https:\/\/github.com\/apache\/arrow\/actions\/runs\/8179809257\/job\/22366589254?pr=40373#step:13:265 and the relevant output is:\r\n\r\n```\r\n2024-03-06T23:05:01.0379130Z 97\/97 Test #73: arrow-s3fs-test ..............................***Timeout 300.04 sec\r\n2024-03-06T23:05:01.0381760Z Running arrow-s3fs-test, redirecting output into \/Users\/runner\/work\/arrow\/arrow\/build\/cpp\/build\/test-logs\/arrow-s3fs-test.txt (attempt 1\/1)\r\n2024-03-06T23:05:01.0383120Z \r\n2024-03-06T23:05:01.0383360Z       Start 73: arrow-s3fs-test\r\n2024-03-06T23:10:01.0805520Z     Test #73: arrow-s3fs-test ..............................***Timeout 300.04 sec\r\n2024-03-06T23:10:01.0808220Z Running arrow-s3fs-test, redirecting output into \/Users\/runner\/work\/arrow\/arrow\/build\/cpp\/build\/test-logs\/arrow-s3fs-test.txt (attempt 1\/1)\r\n2024-03-06T23:10:01.0810650Z \r\n2024-03-06T23:10:01.0810890Z       Start 73: arrow-s3fs-test\r\n2024-03-06T23:15:01.1167650Z     Test #73: arrow-s3fs-test ..............................***Timeout 300.04 sec\r\n2024-03-06T23:15:01.1171300Z Running arrow-s3fs-test, redirecting output into \/Users\/runner\/work\/arrow\/arrow\/build\/cpp\/build\/test-logs\/arrow-s3fs-test.txt (attempt 1\/1)\r\n2024-03-06T23:15:01.1172580Z \r\n2024-03-06T23:15:01.1175960Z \r\n2024-03-06T23:15:01.1176500Z 99% tests passed, 1 tests failed out of 97\r\n2024-03-06T23:15:01.1176970Z \r\n2024-03-06T23:15:01.1177690Z Label Time Summary:\r\n2024-03-06T23:15:01.1178690Z arrow-compute-tests    =  89.33 sec*proc (13 tests)\r\n2024-03-06T23:15:01.1179450Z arrow-tests            = 449.67 sec*proc (41 tests)\r\n2024-03-06T23:15:01.1183940Z arrow_acero            = 148.43 sec*proc (13 tests)\r\n2024-03-06T23:15:01.1184610Z arrow_dataset          = 101.76 sec*proc (14 tests)\r\n2024-03-06T23:15:01.1185230Z Errors while running CTest\r\n2024-03-06T23:15:01.1185750Z arrow_flight           =  14.60 sec*proc (2 tests)\r\n2024-03-06T23:15:01.1186400Z arrow_substrait        =   1.58 sec*proc (1 test)\r\n2024-03-06T23:15:01.1187050Z filesystem             = 334.44 sec*proc (5 tests)\r\n2024-03-06T23:15:01.1187830Z gandiva-tests          =  28.20 sec*proc (4 tests)\r\n2024-03-06T23:15:01.1188560Z parquet-tests          =  59.08 sec*proc (9 tests)\r\n2024-03-06T23:15:01.1189210Z unittest               = 892.66 sec*proc (97 tests)\r\n2024-03-06T23:15:01.1189650Z \r\n2024-03-06T23:15:01.1189820Z Total Test time (real) = 1008.26 sec\r\n2024-03-06T23:15:01.1190170Z \r\n2024-03-06T23:15:01.1190330Z The following tests FAILED:\r\n2024-03-06T23:15:01.1190830Z \t 73 - arrow-s3fs-test (Timeout)\r\n```\r\n\r\n[gist](https:\/\/gist.github.com\/amoeba\/e14175676bc8ce84d4c4aa4ac8100155) for posterity.\r\n\r\nI expect this causes a lot of noise for maintainers so figuring out why this test is timing out would be good.\r\n\r\n### Component(s)\r\n\r\nC++, Continuous Integration","comments":["When running this test locally on my non-AMD64 Mac, I actually get three failures:\r\n\r\n```\r\n[ RUN      ] TestS3FS.CreateDir\r\n\/Users\/bryce\/src\/apache\/arrow\/cpp\/src\/arrow\/filesystem\/s3fs_test.cc:934: Failure\r\nFailed\r\nExpected 'fs_->CreateDir(\"bucket\/somefile\")' to fail with IOError, but got OK\r\n[  FAILED  ] TestS3FS.CreateDir (37 ms)\r\n[ RUN      ] TestS3FSGeneric.CopyFile\r\nWarning: The standard parity is set to 0. This can lead to data loss.\r\n\/Users\/bryce\/src\/apache\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:570: Failure\r\nFailed\r\nExpected 'fs->CopyFile(\"AB\/abc\", \"def\/mno\")' to fail with IOError, but got OK\r\n[  FAILED  ] TestS3FSGeneric.CopyFile (66 ms)\r\n[ RUN      ] TestS3FSGeneric.MoveFile\r\nWarning: The standard parity is set to 0. This can lead to data loss.\r\n\/Users\/bryce\/src\/apache\/arrow\/cpp\/src\/arrow\/filesystem\/test_util.cc:448: Failure\r\nFailed\r\nExpected 'fs->Move(\"AB\/pqr\", \"xxx\/mno\")' to fail with IOError, but got OK\r\n[  FAILED  ] TestS3FSGeneric.MoveFile (81 ms)\r\n```\r\n\r\nI see in this job we run ctest with,\r\n\r\n```\r\nctest --label-regex unittest --output-on-failure --parallel 4 --repeat until-pass:3 --timeout 300\r\n```\r\n\r\nMy hunch is that it's possible the test is failing and ctest is retrying. My local machine is fast enough that it can get 3 repeats in before the timeout but it's possible GitHub's runner is slow enough that the timeout happens first.\r\n\r\nAnother thing I noticed is that, when ctest automatically re-attempts the test as requested, it still prints \"attempt 1\/1\" each time:\r\n\r\n```\r\nRunning arrow-s3fs-test, redirecting output into \/Users\/bryce\/src\/apache\/arrow\/cpp\/build\/build\/test-logs\/arrow-s3fs-test.txt (attempt 1\/1)\r\n```\r\n\r\n","I reverted my minio and mc binaries to the ones we pin for the tests and the above failures go away so I don't think that's what's going on. Though the above errors may be something to be aware of we ever need to upgrade minio\/mc.","Relevant link -> https:\/\/github.com\/apache\/arrow\/pull\/34671","I haven't been able to reproduce locally (without modifying anything) but I have been able to reproduce once on CI with a stripped-down workflow in this run https:\/\/github.com\/amoeba\/arrow\/actions\/runs\/8194916281\/job\/22416431654 so I'm going to try debugging that with tmate.","Good luck with that, my attempts at reproducing while logging in using tmate actually failed here:\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/40401","I'm having success reproducing the timeout by SSH'ing into GHA so this has now turned into me figuring out how to get useful information out of the hanging test with dtrace. A did a bit of searching to find out how to get thread stack traces from a running process and ended up trying `sudo dtrace -p $PID -n 'profile-997 \/pid == '$PID'\/ { @[ustack(100)] = count();}' -o somefile` which I don'k think is close but not quite what I'm after. It behaves like it captures just a single stack trace so I'm going to keep experimenting.\r\n\r\nBut so far, it produces this more often than not, which is interesting,\r\n\r\n```\r\nlibsystem_kernel.dylib`__pipe+0xa\r\nlibcurl.4.dylib`curl_easy_perform+0x77\r\nlibaws-cpp-sdk-core.dylib`Aws::Http::CurlHttpClient::MakeRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::Utils::RateLimits::RateLimiterInterface*, Aws::Utils::RateLimits::RateLimiterInterface*) const+0xda6\r\nlibaws-cpp-sdk-core.dylib`std::__1::__function::__func<Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const::$_24, std::__1::allocator<Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const::$_24>, std::__1::shared_ptr<Aws::Http::HttpResponse> ()>::operator()()+0x26\r\nlibaws-cpp-sdk-core.dylib`std::__1::shared_ptr<Aws::Http::HttpResponse> smithy::components::tracing::TracingUtils::MakeCallWithTiming<std::__1::shared_ptr<Aws::Http::HttpResponse> >(std::__1::function<std::__1::shared_ptr<Aws::Http::HttpResponse> ()>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, smithy::components::tracing::Meter const&, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)+0x48\r\nlibaws-cpp-sdk-core.dylib`Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const+0x779\r\nlibaws-cpp-sdk-core.dylib`Aws::Client::AWSClient::AttemptExhaustively(Aws::Http::URI const&, Aws::AmazonWebServiceRequest const&, Aws::Http::HttpMethod, char const*, char const*, char const*) const+0x43b\r\nlibaws-cpp-sdk-core.dylib`Aws::Client::AWSXMLClient::MakeRequest(Aws::Http::URI const&, Aws::AmazonWebServiceRequest const&, Aws::Http::HttpMethod, char const*, char const*, char const*) const+0x42\r\nlibarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::S3Client::GetBucketRegion(Aws::S3::Model::HeadBucketRequest const&)+0xf2\r\nlibarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::S3Client::GetBucketRegion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)+0xb8\r\nlibarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::RegionResolver::ResolveRegionUncached(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)+0xda\r\nlibarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::RegionResolver::ResolveRegion(d::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)+0x12c\r\nlibarrow.1600.0.0.dylib`arrow::fs::ResolveS3BucketRegion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)+0x265a\r\nlibarrow_gtestd.1.11.0.dylib`void testing::internal::HandleSehExceptionsInMethodIfSuppord<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)+0x\r\nlibarrow_gtestd.1.11.0.dylib`void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)+0x6a\r\nlibarrow_gtestd.1.11.0.dylib`testing::TestInfo::Run()+0xlibarrow_gtestd.1.11.0.dylib`testing::TestSuite::Run()+0x104\r\n```","More easily perhaps, you can use `gdb -p` to take control of a running process and debug it.\r\nlldb has [something similar](https:\/\/lldb.llvm.org\/use\/map.html#attach-to-the-process-with-process-id-123).","Thanks. A `bt all` gives something similar. In an attempt to catch when the test was approaching timeout, I got a baseline for how long the test takes to complete normally which was pretty consistently ~25s. I ran the test by itself until it took longer and waited until ~60s and attached using `lldb -p` and then ran `bt all`:\r\n\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP\r\n  * frame #0: 0x00007ff8179a40aa libsystem_kernel.dylib`__ulock_wait + 10\r\n    frame #1: 0x00007ff8179e0c95 libsystem_pthread.dylib`_pthread_join + 358\r\n    frame #2: 0x00007ff82c299cda libcurl.4.dylib`Curl_thread_join + 23\r\n    frame #3: 0x00007ff82c28c45c libcurl.4.dylib`Curl_resolver_kill + 41\r\n    frame #4: 0x00007ff82c2c21a0 libcurl.4.dylib`multi_done + 75\r\n    frame #5: 0x00007ff82c2c5876 libcurl.4.dylib`multi_handle_timeout + 291\r\n    frame #6: 0x00007ff82c2c4254 libcurl.4.dylib`multi_runsingle + 3542\r\n    frame #7: 0x00007ff82c2c3391 libcurl.4.dylib`curl_multi_perform + 212\r\n    frame #8: 0x00007ff82c29e7eb libcurl.4.dylib`curl_easy_perform + 299\r\n    frame #9: 0x0000000102139df2 libaws-cpp-sdk-core.dylib`Aws::Http::CurlHttpClient::MakeRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::Utils::RateLimits::RateLimiterInterface*, Aws::Utils::RateLimits::RateLimiterInterface*) const + 3494\r\n    frame #10: 0x000000010212b61c libaws-cpp-sdk-core.dylib`std::__1::__function::__func<Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const::$_24, std::__1::allocator<Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const::$_24>, std::__1::shared_ptr<Aws::Http::HttpResponse> ()>::operator()() + 38\r\n    frame #11: 0x0000000102116e78 libaws-cpp-sdk-core.dylib`std::__1::shared_ptr<Aws::Http::HttpResponse> smithy::components::tracing::TracingUtils::MakeCallWithTiming<std::__1::shared_ptr<Aws::Http::HttpResponse> >(std::__1::function<std::__1::shared_ptr<Aws::Http::HttpResponse> ()>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, smithy::components::tracing::Meter const&, std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 72\r\n    frame #12: 0x0000000102113883 libaws-cpp-sdk-core.dylib`Aws::Client::AWSClient::AttemptOneRequest(std::__1::shared_ptr<Aws::Http::HttpRequest> const&, Aws::AmazonWebServiceRequest const&, char const*, char const*, char const*) const + 1913\r\n    frame #13: 0x00000001021116af libaws-cpp-sdk-core.dylib`Aws::Client::AWSClient::AttemptExhaustively(Aws::Http::URI const&, Aws::AmazonWebServiceRequest const&, Aws::Http::HttpMethod, char const*, char const*, char const*) const + 1083\r\n    frame #14: 0x000000010211c612 libaws-cpp-sdk-core.dylib`Aws::Client::AWSXMLClient::MakeRequest(Aws::Http::URI const&, Aws::AmazonWebServiceRequest const&, Aws::Http::HttpMethod, char const*, char const*, char const*) const + 66\r\n    frame #15: 0x0000000120bbb4d2 libarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::S3Client::GetBucketRegion(Aws::S3::Model::HeadBucketRequest const&) + 242\r\n    frame #16: 0x0000000120bbb368 libarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::S3Client::GetBucketRegion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 184\r\n    frame #17: 0x0000000120bbac9a libarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::RegionResolver::ResolveRegionUncached(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 218\r\n    frame #18: 0x0000000120b56d3c libarrow.1600.0.0.dylib`arrow::fs::(anonymous namespace)::RegionResolver::ResolveRegion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 300\r\n    frame #19: 0x0000000120b49ab5 libarrow.1600.0.0.dylib`arrow::fs::ResolveS3BucketRegion(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 613\r\n    frame #20: 0x0000000100c7cfd4 arrow-s3fs-test`arrow::fs::S3RegionResolutionTest_NonExistentBucket_Test::TestBody() + 68\r\n    frame #21: 0x00000001026db47b libarrow_gtestd.1.11.0.dylib`void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) + 123\r\n    frame #22: 0x000000010269dcba libarrow_gtestd.1.11.0.dylib`void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) + 106\r\n    frame #23: 0x000000010269dc03 libarrow_gtestd.1.11.0.dylib`testing::Test::Run() + 195\r\n    frame #24: 0x000000010269ed81 libarrow_gtestd.1.11.0.dylib`testing::TestInfo::Run() + 241\r\n    frame #25: 0x000000010269fe24 libarrow_gtestd.1.11.0.dylib`testing::TestSuite::Run() + 260\r\n    frame #26: 0x00000001026af16b libarrow_gtestd.1.11.0.dylib`testing::internal::UnitTestImpl::RunAllTests() + 987\r\n    frame #27: 0x00000001026e04bb libarrow_gtestd.1.11.0.dylib`bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) + 123\r\n    frame #28: 0x00000001026aeb2a libarrow_gtestd.1.11.0.dylib`bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) + 106\r\n    frame #29: 0x00000001026aea15 libarrow_gtestd.1.11.0.dylib`testing::UnitTest::Run() + 197\r\n    frame #30: 0x00000001011e0f21 libarrow_gtest_maind.1.11.0.dylib`RUN_ALL_TESTS() + 17\r\n    frame #31: 0x00000001011e0efd libarrow_gtest_maind.1.11.0.dylib`main + 61\r\n    frame #32: 0x000000010afe352e dyld`start + 462\r\n  thread #2\r\n    frame #0: 0x00007ff8179a401a libsystem_kernel.dylib`__workq_kernreturn + 10\r\n    frame #1: 0x00007ff8179dc034 libsystem_pthread.dylib`_pthread_wqthread + 426\r\n    frame #2: 0x00007ff8179daf57 libsystem_pthread.dylib`start_wqthread + 15\r\n  thread #3, name = 'AwsEventLoop 1'\r\n    frame #0: 0x00007ff8179a730e libsystem_kernel.dylib`kevent + 10\r\n    frame #1: 0x0000000101fe99ec libaws-c-io.1.0.0.dylib`aws_event_loop_thread + 404\r\n    frame #2: 0x00000001024ffd3d libaws-c-common.1.0.0.dylib`thread_fn + 345\r\n    frame #3: 0x00007ff8179df4e1 libsystem_pthread.dylib`_pthread_start + 125\r\n    frame #4: 0x00007ff8179daf6b libsystem_pthread.dylib`thread_start + 15\r\n  thread #4\r\n    frame #0: 0x00007ff8179a730e libsystem_kernel.dylib`kevent + 10\r\n    frame #1: 0x00007ff817a052ee libsystem_info.dylib`_mdns_search_ex + 1217\r\n    frame #2: 0x00007ff817a07c1c libsystem_info.dylib`mdns_addrinfo + 483\r\n    frame #3: 0x00007ff817a079e4 libsystem_info.dylib`search_addrinfo + 164\r\n    frame #4: 0x00007ff817a00659 libsystem_info.dylib`si_addrinfo + 1811\r\n    frame #5: 0x00007ff8179ffeac libsystem_info.dylib`getaddrinfo + 176\r\n    frame #6: 0x00007ff82c296186 libcurl.4.dylib`Curl_getaddrinfo_ex + 36\r\n    frame #7: 0x00007ff82c28ca24 libcurl.4.dylib`getaddrinfo_thread + 83\r\n    frame #8: 0x00007ff82c299c8d libcurl.4.dylib`curl_thread_create_thunk + 29\r\n    frame #9: 0x00007ff8179df4e1 libsystem_pthread.dylib`_pthread_start + 125\r\n    frame #10: 0x00007ff8179daf6b libsystem_pthread.dylib`thread_start + 15\r\n```\r\n","Thanks a lot @amoeba . It appears therefore that it is timing out when trying to join a thread using `Curl_thread_join`. <s>This is quite unexpected, though there seems to be a similar issue reported on the Internet:\r\nhttps:\/\/github.com\/nim-lang\/Nim\/issues\/15019#issuecomment-661208069<\/s>\r\n\r\n*Edit:* at second sight, the sentence above is incorrect. There is a thread pending on `Curl_getaddrinfo_ex`, which means that the DNS request is still ongoing. Presumably DNS requests on macOS can take a lot of time to time out?","It would be nice if you could try again to catch another backtrace, so that we can see if it's always the same test timing out. If it is, we could just disable that test on macOS...","Even better: can you try getting a traceback _later_? It seems DNS resolution timeout on macOS is [30 seconds](https:\/\/discussions.apple.com\/thread\/254138037?sortBy=best)."],"labels":["Type: bug","Component: C++","Component: Continuous Integration"]},{"title":"GH-40407: [JS] Fix string coercion in MapRowProxyHandler.ownKeys","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nThe `ProxyHandler.ownKeys` implementation must return strings or symbols. Because of this bug, it was returning numbers, causing the `in` operator to crash when trying to iterate over the keys of a `MapRow` object.\r\n\r\nAn example of this is a DuckDB SQL query using the `HISTOGRAM` operator:\r\n\r\n```sql\r\nSELECT HISTOGRAM(phot_g_mean_mag) FROM gaia\r\n```\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nInstead of calling `array.map(String)`, which returns a typed array of non-strings when `array` is a typed array, call `Array.from` which is guaranteed to return strings.\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\nApologies, but I don\u2019t know how to test this.\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\nThis fixes a crash when using the `in` operator on a `MapRow` object.\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40407","comments":[":warning: GitHub issue #40407 **has been automatically assigned in GitHub** to PR creator.","LGTM, thanks @mbostock!","Woot, thanks for the approval @trxcllnt. I\u2019m excited to contribute. \ud83d\ude01 ","IIRC the Map keys are required to be strings (tho I'm not sure we prevent someone from using a different type). Or has newer Arrow has loosened this requirement? That's the only reason I can think we'd have made the assumption the keys would always be strings here.","I personally prefer supporting maps with keys of any Arrow dtype, but not sure how compatible with other implementations that will be. I do see the [integration tests](https:\/\/github.com\/apache\/arrow\/blob\/40d4c54401c3c4f6f6c0e6116e32dc20050e67b7\/dev\/archery\/archery\/integration\/datagen.py#L1672-L1691) seem to only test maps with string keys.","Python (which uses C++ which could be considered the canonical implementation) only allows string keys.\r\n\r\n```py\r\nTraceback (most recent call last):\r\n  File \"\/Users\/dominik\/Code\/ramsch\/map.py\", line 8, in <module>\r\n    table = pa.table({'a': range(10), 'b': np.random.randn(10), 'c': [{x: x} for x in range(10)]})\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\/table.pxi\", line 5204, in pyarrow.lib.table\r\n  File \"pyarrow\/table.pxi\", line 1813, in pyarrow.lib._Tabular.from_pydict\r\n  File \"pyarrow\/table.pxi\", line 5339, in pyarrow.lib._from_pydict\r\n  File \"pyarrow\/array.pxi\", line 374, in pyarrow.lib.asarray\r\n  File \"pyarrow\/array.pxi\", line 344, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow\/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowTypeError: Expected dict key of type str or bytes, got 'int'\r\n```\r\n\r\nRust doesn't seem to enforce string keys and we couldn't find anything in the spec so it seems to be a gray area right now. ","Yeah, I'm inclined to tell DuckDB their Map implementation is non-conformant. Generally whatever `libarrow`\/`pyarrow` does (and what's in the integration tests) is the source of truth.\r\n\r\nEven if JS allows non-string keys, other implementations probably won't. This has been a significant source of confusion in the past, so we try to align with C++ as much as possible.","It is absolutely not the case that maps must have string keys. As just one example, part of the definition of [ADBC metadata](https:\/\/arrow.apache.org\/docs\/format\/ADBC\/C.html) includes a `map<int32, list<int32>>`.","You're right, I don't see C++ checking the keys type in [`MapArray::ValidateChildData()`](https:\/\/github.com\/apache\/arrow\/blob\/40d4c54401c3c4f6f6c0e6116e32dc20050e67b7\/cpp\/src\/arrow\/array\/array_nested.cc#L826-L845). Maybe the string keys is just a thing pyarrow enforces. Either way, it'd be good to get some integration tests to settle any disagreements.\r\n","This is not how you would create a Map array with PyArrow. Example:\r\n```python\r\n>>> ty = pa.map_(pa.int8(), pa.int32())\r\n>>> a = pa.array([{1: 1000, 2: 10000}, {3: -1000}], type=ty)\r\n>>> a\r\n<pyarrow.lib.MapArray object at 0x7f7ab948dd80>\r\n[\r\n  keys:\r\n  [\r\n    1,\r\n    2\r\n  ]\r\n  values:\r\n  [\r\n    1000,\r\n    10000\r\n  ],\r\n  keys:\r\n  [\r\n    3\r\n  ]\r\n  values:\r\n  [\r\n    -1000\r\n  ]\r\n]\r\n```","Oh, `pa.table` doesn't recognize the dictionary. Thanks for the corrected code!","Anything else I can do to help this land? Seems like we still want this fix."],"labels":["Component: JavaScript","awaiting committer review"]},{"title":"[JS] MapRowProxyHandler.ownKeys can return a typed array","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThe MapRowProxyHandler ownKeys implementation currently calls array.map:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/40d4c54401c3c4f6f6c0e6116e32dc20050e67b7\/js\/src\/row\/map.ts#L109-L111\r\n\r\nWhen `row[kKeys].toArray()` returns a typed array such as `Float32Array`, this means that `ownKeys` returns a `Float32Array`, which is a bug because a `ProxyHandler` is required to return an iterable of `string` or `Symbol`. (See [`Proxy.ownKeys`](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/JavaScript\/Reference\/Global_Objects\/Proxy\/Proxy\/ownKeys).) This results in a crash when trying to use the `in` operator with a `MapRow` object.\r\n\r\n### Component(s)\r\n\r\nJavaScript","comments":[],"labels":["Type: bug","Component: JavaScript"]},{"title":"[C++] Add support for LLD","body":"### Describe the enhancement requested\n\nhttps:\/\/lld.llvm.org\/\r\n\r\nLLD is the LLVM Linker. It's faster than GNU ld.\n\n### Component(s)\n\nC++","comments":["> LLD is a drop-in replacement for the GNU linkers that accepts the same command line arguments and linker scripts as GNU.\r\n\r\nDo we even need to add anything explicitly if this is the case?","We need to pass `-fuse-ld=lld` argument to `gcc`\/`g++`.\r\nSee also: https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Link-Options.html","Ah I see, I thought it had a setup like mold that pretends to be ld ^^"],"labels":["Type: enhancement","Component: C++"]},{"title":"[Java] NullVector constructor should not accept an arbitrary field type","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nNullVector constructor should not accept an arbitrary field type.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/main\/java\/vector\/src\/main\/java\/org\/apache\/arrow\/vector\/NullVector.java#L70-L98\r\n\r\nSee discussion: https:\/\/github.com\/apache\/arrow\/issues\/40383#issuecomment-1981141014\r\n\n\n### Component(s)\n\nJava","comments":["take"],"labels":["Type: bug","Component: Java"]},{"title":"GH-40361:  [C++] Make flatbuffers serialization more deterministic","body":"### Rationale for this change\r\n\r\nThis is the start of a PR to address #40361, and in turn #40202, to make metadata in parquet files written by arrow to be identical irrespective of the platform configuration.  This is limited, as platform-specific differences in R or Python versions or compression libraries could still result in differences.\r\n\r\n### What changes are included in this PR?\r\n\r\nSo far I have only made a partial change to part of the metadata serialization.  I need to look at whether other calls to flatbuffers require similar treatment.\r\n\r\n### Are these changes tested?\r\n\r\nNot yet, this is a draft PR\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo \r\n\r\n\r\n* GitHub Issue: #40361","comments":[":warning: GitHub issue #40361 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #40361 **has no components**, please add labels for components.",":warning: GitHub issue #40361 **has no components**, please add labels for components.","Thanks @noamross!\r\n\r\n@kou can you please approve all workflows to run?","Approved!",":warning: GitHub issue #40361 **has no components**, please add labels for components.","As discussed on the GH issue, it would be useful to add a test using output from your machine as a reference.\r\n(also, perhaps open a separate PR with just the test to validate that it would fail on CI)"],"labels":["Component: C++","awaiting changes"]},{"title":"[R] create_package_with_all_dependencies not working when created on Windows and tested on macOS","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nIn https:\/\/github.com\/apache\/arrow\/pull\/40232 I found that I couldn't install an offline build created with `create_package_with_all_dependencies` that I created on Windows and tried to install on macOS. When I did this, I got:\r\n\r\n```sh\r\n** testing if installed package can be loaded from temporary location\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  dlopen(\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so, 0x0006): symbol not found in flat namespace '__ZN3re212re2_internal5ParseINSt3__117basic_string_viewIcNS2_11char_traitsIcEEEEEEbPKcmPT_'\r\nError: loading failed\r\n```\r\n\r\n<details>\r\n<summary>full R CMD INSTALL output<\/summary>\r\n\r\n```sh\r\n$ R CMD INSTALL arrow_14.0.2.1_with_deps.tar.gz\r\n* installing to library \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\u2019\r\n* installing *source* package \u2018arrow\u2019 ...\r\n** package \u2018arrow\u2019 successfully unpacked and MD5 sums checked\r\n** using staged installation\r\n*** pkg-config found.\r\n*** Trying Arrow C++ in ARROW_HOME: \/Users\/bryce\/builds\/arrow-arm64\r\n**** Not using: C++ library version (15.0.0): not supported by R package version 14.0.2.1\r\ntrying URL 'https:\/\/apache.jfrog.io\/artifactory\/arrow\/r\/'\r\ndownloaded 906 bytes\r\n\r\n*** Found local C++ source: 'tools\/cpp'\r\n*** Building libarrow from source\r\n    For build options and troubleshooting, see the install guide:\r\n    https:\/\/arrow.apache.org\/docs\/r\/articles\/install.html\r\n*** Building with MAKEFLAGS=-j8\r\n**** cmake 3.28.3: \/opt\/homebrew\/bin\/cmake\r\n**** arrow with SOURCE_DIR='tools\/cpp' BUILD_DIR='\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/\/RtmpUiyqGS\/filebc7d400cb95d' DEST_DIR='libarrow\/arrow-14.0.2' CMAKE='\/opt\/homebrew\/bin\/cmake' EXTRA_CMAKE_FLAGS='' CC='clang -arch arm64' CXX='clang++ -arch arm64 -std=gnu++17' LDFLAGS='-L\/opt\/R\/arm64\/lib' N_JOBS='2' Boost_SOURCE='BUNDLED' lz4_SOURCE='BUNDLED' ARROW_S3='OFF' ARROW_GCS='OFF' ARROW_ABSL_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/absl-20211102.0.tar.gz' ARROW_AWS_C_AUTH_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-auth-v0.6.22.tar.gz' ARROW_AWS_C_CAL_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-cal-v0.5.20.tar.gz' ARROW_AWS_C_COMMON_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-common-v0.8.9.tar.gz' ARROW_AWS_C_COMPRESSION_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-compression-v0.2.16.tar.gz' ARROW_AWS_C_EVENT_STREAM_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-event-stream-v0.2.18.tar.gz' ARROW_AWS_C_HTTP_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-http-v0.7.3.tar.gz' ARROW_AWS_C_IO_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-io-v0.13.14.tar.gz' ARROW_AWS_C_MQTT_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-mqtt-v0.8.4.tar.gz' ARROW_AWS_C_S3_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-s3-v0.2.3.tar.gz' ARROW_AWS_C_SDKUTILS_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-c-sdkutils-v0.1.6.tar.gz' ARROW_AWS_CHECKSUMS_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-checksums-v0.1.13.tar.gz' ARROW_AWS_CRT_CPP_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-crt-cpp-v0.18.16.tar.gz' ARROW_AWS_LC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-lc-v1.3.0.tar.gz' ARROW_AWSSDK_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/aws-sdk-cpp-1.10.55.tar.gz' ARROW_BOOST_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/boost-1.81.0.tar.gz' ARROW_BROTLI_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/brotli-v1.0.9.tar.gz' ARROW_BZIP2_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/bzip2-1.0.8.tar.gz' ARROW_CARES_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/cares-1.17.2.tar.gz' ARROW_CRC32C_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/crc32c-1.1.2.tar.gz' ARROW_GBENCHMARK_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/gbenchmark-v1.7.1.tar.gz' ARROW_GFLAGS_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/gflags-v2.2.2.tar.gz' ARROW_GLOG_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/glog-v0.5.0.tar.gz' ARROW_GOOGLE_CLOUD_CPP_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/google-cloud-cpp-v2.12.0.tar.gz' ARROW_GRPC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/grpc-v1.46.3.tar.gz' ARROW_GTEST_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/gtest-1.11.0.tar.gz' ARROW_JEMALLOC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/jemalloc-5.3.0.tar.bz2' ARROW_LZ4_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/lz4-v1.9.4.tar.gz' ARROW_MIMALLOC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/mimalloc-v2.0.6.tar.gz' ARROW_NLOHMANN_JSON_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/nlohmann-json-v3.10.5.tar.gz' ARROW_OPENTELEMETRY_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/opentelemetry-cpp-v1.8.1.tar.gz' ARROW_OPENTELEMETRY_PROTO_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/opentelemetry-proto-v0.17.0.tar.gz' ARROW_ORC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/orc-1.9.0.tar.gz' ARROW_PROTOBUF_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/protobuf-v21.3.tar.gz' ARROW_RAPIDJSON_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/rapidjson-232389d4f1012dddec4ef84861face2d2ba85709.tar.gz' ARROW_RE2_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/re2-2022-06-01.tar.gz' ARROW_S2N_TLS_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/s2n-v1.3.35.tar.gz' ARROW_SNAPPY_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/snappy-1.1.10.tar.gz' ARROW_THRIFT_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/thrift-0.16.0.tar.gz' ARROW_UCX_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/ucx-1.12.1.tar.gz' ARROW_UTF8PROC_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/utf8proc-v2.7.0.tar.gz' ARROW_XSIMD_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/xsimd-9.0.1.tar.gz' ARROW_ZLIB_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/zlib-1.2.13.tar.gz' ARROW_ZSTD_URL='\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/thirdparty_dependencies\/zstd-1.5.5.tar.gz' ARROW_VERBOSE_THIRDPARTY_BUILD='ON'\r\n+ : \/Users\/bryce\/builds\/arrow-arm64\r\n+ : tools\/cpp\r\n+ : \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/\/RtmpUiyqGS\/filebc7d400cb95d\r\n+ : libarrow\/arrow-14.0.2\r\n+ : \/opt\/homebrew\/bin\/cmake\r\n++ cd tools\/cpp\r\n++ pwd\r\n+ SOURCE_DIR=\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\r\n++ mkdir -p libarrow\/arrow-14.0.2\r\n++ cd libarrow\/arrow-14.0.2\r\n++ pwd\r\n+ DEST_DIR=\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/libarrow\/arrow-14.0.2\r\n+ '[' 2 = '' ']'\r\n+ '[' '' '!=' '' ']'\r\n+ '[' '' = false ']'\r\n+ ARROW_DEFAULT_PARAM=OFF\r\n+ case \"$CXX\" in\r\n+ mkdir -p \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/\/RtmpUiyqGS\/filebc7d400cb95d\r\n+ pushd \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/\/RtmpUiyqGS\/filebc7d400cb95d\r\n\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpUiyqGS\/filebc7d400cb95d \/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\r\n+ \/opt\/homebrew\/bin\/cmake -DARROW_BOOST_USE_SHARED=OFF -DARROW_BUILD_TESTS=OFF -DARROW_BUILD_SHARED=OFF -DARROW_BUILD_STATIC=ON -DARROW_ACERO=ON -DARROW_COMPUTE=ON -DARROW_CSV=ON -DARROW_DATASET=ON -DARROW_DEPENDENCY_SOURCE=AUTO -DAWSSDK_SOURCE= -DBoost_SOURCE=BUNDLED -Dlz4_SOURCE=BUNDLED -DARROW_FILESYSTEM=ON -DARROW_GCS=OFF -DARROW_JEMALLOC=OFF -DARROW_MIMALLOC=ON -DARROW_JSON=ON -DARROW_PARQUET=ON -DARROW_S3=OFF -DARROW_WITH_BROTLI=OFF -DARROW_WITH_BZ2=OFF -DARROW_WITH_LZ4=ON -DARROW_WITH_RE2=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_UTF8PROC=ON -DARROW_WITH_ZLIB=OFF -DARROW_WITH_ZSTD=OFF -DARROW_VERBOSE_THIRDPARTY_BUILD=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_FIND_DEBUG_MODE=OFF -DCMAKE_INSTALL_LIBDIR=lib -DCMAKE_INSTALL_PREFIX=\/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/libarrow\/arrow-14.0.2 -DCMAKE_EXPORT_NO_PACKAGE_REGISTRY=ON -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_UNITY_BUILD=OFF -Dre2_SOURCE=BUNDLED -Dxsimd_SOURCE= -Dzstd_SOURCE= -G 'Unix Makefiles' \/private\/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\r\n-- Building using CMake version: 3.28.3\r\n-- The C compiler identification is AppleClang 15.0.0.15000100\r\n-- The CXX compiler identification is AppleClang 15.0.0.15000100\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: \/Applications\/Xcode.app\/Contents\/Developer\/Toolchains\/XcodeDefault.xctoolchain\/usr\/bin\/clang - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: \/Applications\/Xcode.app\/Contents\/Developer\/Toolchains\/XcodeDefault.xctoolchain\/usr\/bin\/clang++ - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Arrow version: 14.0.2 (full: '14.0.2')\r\n-- Arrow SO version: 1400 (full: 1400.2.0)\r\n-- clang-tidy found at \/opt\/homebrew\/opt\/llvm@14\/bin\/clang-tidy\r\n-- clang-format found at \/opt\/homebrew\/opt\/llvm@14\/bin\/clang-format\r\n-- Found ClangTools: \/opt\/homebrew\/opt\/llvm@14\/bin\/clang-format\r\n-- infer not found\r\n-- Found Python3: \/opt\/homebrew\/bin\/python3.12 (found version \"3.12.2\") found components: Interpreter\r\nfatal: not a git repository (or any of the parent directories): .git\r\n-- Using ccache: \/opt\/homebrew\/bin\/ccache\r\nCMake Warning (dev) at CMakeLists.txt:277 (find_program):\r\n  Policy CMP0109 is not set: find_program() requires permission to execute\r\n  but not to read.  Run \"cmake --help-policy CMP0109\" for policy details.\r\n  Use the cmake_policy command to set the policy and suppress this warning.\r\n\r\n  The file\r\n\r\n    \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\/build-support\/cpplint.py\r\n\r\n  is readable but not executable.  CMake is using it for compatibility.\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found cpplint executable at \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\/build-support\/cpplint.py\r\n-- System processor: arm64\r\n-- Performing Test CXX_SUPPORTS_SVE\r\n-- Performing Test CXX_SUPPORTS_SVE - Success\r\n-- Arrow build warning level: PRODUCTION\r\n-- Build Type: RELEASE\r\n-- Performing Test CXX_LINKER_SUPPORTS_VERSION_SCRIPT\r\n-- Performing Test CXX_LINKER_SUPPORTS_VERSION_SCRIPT - Failed\r\n-- Using AUTO approach to find dependencies\r\n-- ARROW_ABSL_BUILD_VERSION: 20211102.0\r\n-- ARROW_ABSL_BUILD_SHA256_CHECKSUM: dcf71b9cba8dc0ca9940c4b316a0c796be8fab42b070bb6b7cab62b48f0e66c4\r\n-- ARROW_AWS_C_AUTH_BUILD_VERSION: v0.6.22\r\n-- ARROW_AWS_C_AUTH_BUILD_SHA256_CHECKSUM: 691a6b4418afcd3dc141351b6ad33fccd8e3ff84df0e9e045b42295d284ee14c\r\n-- ARROW_AWS_C_CAL_BUILD_VERSION: v0.5.20\r\n-- ARROW_AWS_C_CAL_BUILD_SHA256_CHECKSUM: acc352359bd06f8597415c366cf4ec4f00d0b0da92d637039a73323dd55b6cd0\r\n-- ARROW_AWS_C_COMMON_BUILD_VERSION: v0.8.9\r\n-- ARROW_AWS_C_COMMON_BUILD_SHA256_CHECKSUM: 2f3fbaf7c38eae5a00e2a816d09b81177f93529ae8ba1b82dc8f31407565327a\r\n-- ARROW_AWS_C_COMPRESSION_BUILD_VERSION: v0.2.16\r\n-- ARROW_AWS_C_COMPRESSION_BUILD_SHA256_CHECKSUM: 044b1dbbca431a07bde8255ef9ec443c300fc60d4c9408d4b862f65e496687f4\r\n-- ARROW_AWS_C_EVENT_STREAM_BUILD_VERSION: v0.2.18\r\n-- ARROW_AWS_C_EVENT_STREAM_BUILD_SHA256_CHECKSUM: 310ca617f713bf664e4c7485a3d42c1fb57813abd0107e49790d107def7cde4f\r\n-- ARROW_AWS_C_HTTP_BUILD_VERSION: v0.7.3\r\n-- ARROW_AWS_C_HTTP_BUILD_SHA256_CHECKSUM: 07e16c6bf5eba6f0dea96b6f55eae312a7c95b736f4d2e4a210000f45d8265ae\r\n-- ARROW_AWS_C_IO_BUILD_VERSION: v0.13.14\r\n-- ARROW_AWS_C_IO_BUILD_SHA256_CHECKSUM: 12b66510c3d9a4f7e9b714e9cfab2a5bf835f8b9ce2f909d20ae2a2128608c71\r\n-- ARROW_AWS_C_MQTT_BUILD_VERSION: v0.8.4\r\n-- ARROW_AWS_C_MQTT_BUILD_SHA256_CHECKSUM: 232eeac63e72883d460c686a09b98cdd811d24579affac47c5c3f696f956773f\r\n-- ARROW_AWS_C_S3_BUILD_VERSION: v0.2.3\r\n-- ARROW_AWS_C_S3_BUILD_SHA256_CHECKSUM: a00b3c9f319cd1c9aa2c3fa15098864df94b066dcba0deaccbb3caa952d902fe\r\n-- ARROW_AWS_C_SDKUTILS_BUILD_VERSION: v0.1.6\r\n-- ARROW_AWS_C_SDKUTILS_BUILD_SHA256_CHECKSUM: 8a2951344b2fb541eab1e9ca17c18a7fcbfd2aaff4cdd31d362d1fad96111b91\r\n-- ARROW_AWS_CHECKSUMS_BUILD_VERSION: v0.1.13\r\n-- ARROW_AWS_CHECKSUMS_BUILD_SHA256_CHECKSUM: 0f897686f1963253c5069a0e495b85c31635ba146cd3ac38cc2ea31eaf54694d\r\n-- ARROW_AWS_CRT_CPP_BUILD_VERSION: v0.18.16\r\n-- ARROW_AWS_CRT_CPP_BUILD_SHA256_CHECKSUM: 9e69bc1dc4b50871d1038aa9ff6ddeb4c9b28f7d6b5e5b1b69041ccf50a13483\r\n-- ARROW_AWS_LC_BUILD_VERSION: v1.3.0\r\n-- ARROW_AWS_LC_BUILD_SHA256_CHECKSUM: ae96a3567161552744fc0cae8b4d68ed88b1ec0f3d3c98700070115356da5a37\r\n-- ARROW_AWSSDK_BUILD_VERSION: 1.10.55\r\n-- ARROW_AWSSDK_BUILD_SHA256_CHECKSUM: 2d552fb1a84bef4a9b65e34aa7031851ed2aef5319e02cc6e4cb735c48aa30de\r\n-- ARROW_AZURE_SDK_BUILD_VERSION: azure-core_1.10.3\r\n-- ARROW_AZURE_SDK_BUILD_SHA256_CHECKSUM: dd624c2f86adf474d2d0a23066be6e27af9cbd7e3f8d9d8fd7bf981e884b7b48\r\n-- ARROW_BOOST_BUILD_VERSION: 1.81.0\r\n-- ARROW_BOOST_BUILD_SHA256_CHECKSUM: 9e0ffae35528c35f90468997bc8d99500bf179cbae355415a89a600c38e13574\r\n-- ARROW_BROTLI_BUILD_VERSION: v1.0.9\r\n-- ARROW_BROTLI_BUILD_SHA256_CHECKSUM: f9e8d81d0405ba66d181529af42a3354f838c939095ff99930da6aa9cdf6fe46\r\n-- ARROW_BZIP2_BUILD_VERSION: 1.0.8\r\n-- ARROW_BZIP2_BUILD_SHA256_CHECKSUM: ab5a03176ee106d3f0fa90e381da478ddae405918153cca248e682cd0c4a2269\r\n-- ARROW_CARES_BUILD_VERSION: 1.17.2\r\n-- ARROW_CARES_BUILD_SHA256_CHECKSUM: 4803c844ce20ce510ef0eb83f8ea41fa24ecaae9d280c468c582d2bb25b3913d\r\n-- ARROW_CRC32C_BUILD_VERSION: 1.1.2\r\n-- ARROW_CRC32C_BUILD_SHA256_CHECKSUM: ac07840513072b7fcebda6e821068aa04889018f24e10e46181068fb214d7e56\r\n-- ARROW_GBENCHMARK_BUILD_VERSION: v1.7.1\r\n-- ARROW_GBENCHMARK_BUILD_SHA256_CHECKSUM: 6430e4092653380d9dc4ccb45a1e2dc9259d581f4866dc0759713126056bc1d7\r\n-- ARROW_GFLAGS_BUILD_VERSION: v2.2.2\r\n-- ARROW_GFLAGS_BUILD_SHA256_CHECKSUM: 34af2f15cf7367513b352bdcd2493ab14ce43692d2dcd9dfc499492966c64dcf\r\n-- ARROW_GLOG_BUILD_VERSION: v0.5.0\r\n-- ARROW_GLOG_BUILD_SHA256_CHECKSUM: eede71f28371bf39aa69b45de23b329d37214016e2055269b3b5e7cfd40b59f5\r\n-- ARROW_GOOGLE_CLOUD_CPP_BUILD_VERSION: v2.12.0\r\n-- ARROW_GOOGLE_CLOUD_CPP_BUILD_SHA256_CHECKSUM: 8cda870803925c62de8716a765e03eb9d34249977e5cdb7d0d20367e997a55e2\r\n-- ARROW_GRPC_BUILD_VERSION: v1.46.3\r\n-- ARROW_GRPC_BUILD_SHA256_CHECKSUM: d6cbf22cb5007af71b61c6be316a79397469c58c82a942552a62e708bce60964\r\n-- ARROW_GTEST_BUILD_VERSION: 1.11.0\r\n-- ARROW_GTEST_BUILD_SHA256_CHECKSUM: b4870bf121ff7795ba20d20bcdd8627b8e088f2d1dab299a031c1034eddc93d5\r\n-- ARROW_JEMALLOC_BUILD_VERSION: 5.3.0\r\n-- ARROW_JEMALLOC_BUILD_SHA256_CHECKSUM: 2db82d1e7119df3e71b7640219b6dfe84789bc0537983c3b7ac4f7189aecfeaa\r\n-- ARROW_LZ4_BUILD_VERSION: v1.9.4\r\n-- ARROW_LZ4_BUILD_SHA256_CHECKSUM: 0b0e3aa07c8c063ddf40b082bdf7e37a1562bda40a0ff5272957f3e987e0e54b\r\n-- ARROW_MIMALLOC_BUILD_VERSION: v2.0.6\r\n-- ARROW_MIMALLOC_BUILD_SHA256_CHECKSUM: 9f05c94cc2b017ed13698834ac2a3567b6339a8bde27640df5a1581d49d05ce5\r\n-- ARROW_NLOHMANN_JSON_BUILD_VERSION: v3.10.5\r\n-- ARROW_NLOHMANN_JSON_BUILD_SHA256_CHECKSUM: 5daca6ca216495edf89d167f808d1d03c4a4d929cef7da5e10f135ae1540c7e4\r\n-- ARROW_OPENTELEMETRY_BUILD_VERSION: v1.8.1\r\n-- ARROW_OPENTELEMETRY_BUILD_SHA256_CHECKSUM: 3d640201594b07f08dade9cd1017bd0b59674daca26223b560b9bb6bf56264c2\r\n-- ARROW_OPENTELEMETRY_PROTO_BUILD_VERSION: v0.17.0\r\n-- ARROW_OPENTELEMETRY_PROTO_BUILD_SHA256_CHECKSUM: f269fbcb30e17b03caa1decd231ce826e59d7651c0f71c3b28eb5140b4bb5412\r\n-- ARROW_ORC_BUILD_VERSION: 1.9.0\r\n-- ARROW_ORC_BUILD_SHA256_CHECKSUM: 0dca8bbccdb2ee87e59ba964933436beebd02ea78c4134424828a8127fbc4faa\r\n-- ARROW_PROTOBUF_BUILD_VERSION: v21.3\r\n-- ARROW_PROTOBUF_BUILD_SHA256_CHECKSUM: 2f723218f6cb709ae4cdc4fb5ed56a5951fc5d466f0128ce4c946b8c78c8c49f\r\n-- ARROW_RAPIDJSON_BUILD_VERSION: 232389d4f1012dddec4ef84861face2d2ba85709\r\n-- ARROW_RAPIDJSON_BUILD_SHA256_CHECKSUM: b9290a9a6d444c8e049bd589ab804e0ccf2b05dc5984a19ed5ae75d090064806\r\n-- ARROW_RE2_BUILD_VERSION: 2022-06-01\r\n-- ARROW_RE2_BUILD_SHA256_CHECKSUM: f89c61410a072e5cbcf8c27e3a778da7d6fd2f2b5b1445cd4f4508bee946ab0f\r\n-- ARROW_SNAPPY_BUILD_VERSION: 1.1.10\r\n-- ARROW_SNAPPY_BUILD_SHA256_CHECKSUM: 49d831bffcc5f3d01482340fe5af59852ca2fe76c3e05df0e67203ebbe0f1d90\r\n-- ARROW_SUBSTRAIT_BUILD_VERSION: v0.27.0\r\n-- ARROW_SUBSTRAIT_BUILD_SHA256_CHECKSUM: 4ed375f69d972a57fdc5ec406c17003a111831d8640d3f1733eccd4b3ff45628\r\n-- ARROW_S2N_TLS_BUILD_VERSION: v1.3.35\r\n-- ARROW_S2N_TLS_BUILD_SHA256_CHECKSUM: 9d32b26e6bfcc058d98248bf8fc231537e347395dd89cf62bb432b55c5da990d\r\n-- ARROW_THRIFT_BUILD_VERSION: 0.16.0\r\n-- ARROW_THRIFT_BUILD_SHA256_CHECKSUM: f460b5c1ca30d8918ff95ea3eb6291b3951cf518553566088f3f2be8981f6209\r\n-- ARROW_UCX_BUILD_VERSION: 1.12.1\r\n-- ARROW_UCX_BUILD_SHA256_CHECKSUM: 9bef31aed0e28bf1973d28d74d9ac4f8926c43ca3b7010bd22a084e164e31b71\r\n-- ARROW_UTF8PROC_BUILD_VERSION: v2.7.0\r\n-- ARROW_UTF8PROC_BUILD_SHA256_CHECKSUM: 4bb121e297293c0fd55f08f83afab6d35d48f0af4ecc07523ad8ec99aa2b12a1\r\n-- ARROW_XSIMD_BUILD_VERSION: 9.0.1\r\n-- ARROW_XSIMD_BUILD_SHA256_CHECKSUM: b1bb5f92167fd3a4f25749db0be7e61ed37e0a5d943490f3accdcd2cd2918cc0\r\n-- ARROW_ZLIB_BUILD_VERSION: 1.2.13\r\n-- ARROW_ZLIB_BUILD_SHA256_CHECKSUM: b3a24de97a8fdbc835b9833169501030b8977031bcb54b3b3ac13740f846ab30\r\n-- ARROW_ZSTD_BUILD_VERSION: 1.5.5\r\n-- ARROW_ZSTD_BUILD_SHA256_CHECKSUM: 9c4396cc829cfae319a6e2615202e82aad41372073482fce286fac78646d3ee4\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\r\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\r\n-- Found Threads: TRUE\r\n-- Looking for _M_ARM64\r\n-- Looking for _M_ARM64 - not found\r\n-- Looking for __SIZEOF_INT128__\r\n-- Looking for __SIZEOF_INT128__ - found\r\n-- Boost include dir:\r\n-- Providing CMake module for SnappyAlt as part of Arrow CMake package\r\n-- pkg-config package for snappy that is used by arrow for static link isn't found\r\n-- Building without OpenSSL support. Minimum OpenSSL version 1.0.2 required.\r\nCMake Warning at cmake_modules\/FindThriftAlt.cmake:56 (find_package):\r\n  By not providing \"FindThrift.cmake\" in CMAKE_MODULE_PATH this project has\r\n  asked CMake to find a package configuration file provided by \"Thrift\", but\r\n  CMake did not find one.\r\n\r\n  Could not find a package configuration file provided by \"Thrift\" (requested\r\n  version 0.11.0) with any of the following names:\r\n\r\n    ThriftConfig.cmake\r\n    thrift-config.cmake\r\n\r\n  Add the installation prefix of \"Thrift\" to CMAKE_PREFIX_PATH or set\r\n  \"Thrift_DIR\" to a directory containing one of the above files.  If \"Thrift\"\r\n  provides a separate development package or SDK, be sure it has been\r\n  installed.\r\nCall Stack (most recent call first):\r\n  cmake_modules\/ThirdpartyToolchain.cmake:277 (find_package)\r\n  cmake_modules\/ThirdpartyToolchain.cmake:1697 (resolve_dependency)\r\n  CMakeLists.txt:542 (include)\r\n\r\n\r\n-- Checking for module 'thrift'\r\n--   Found thrift, version 0.19.0\r\n-- Found ThriftAlt: \/opt\/homebrew\/Cellar\/thrift\/0.19.0\/lib\/libthrift.dylib (found suitable version \"0.19.0\", minimum required is \"0.11.0\")\r\n-- Providing CMake module for ThriftAlt as part of Parquet CMake package\r\n-- Using pkg-config package for thrift that is used by parquet for static link\r\n-- Building (vendored) mimalloc from source\r\n-- RapidJSON found. Headers: \/opt\/homebrew\/Cellar\/rapidjson\/1.1.0\/include\r\n-- Using pkg-config package for xsimd that is used by arrow for static link\r\n-- xsimd found. Headers: \/opt\/homebrew\/include\r\n-- Building LZ4 from source\r\n-- Building RE2 from source\r\n-- Found utf8proc: \/opt\/homebrew\/lib\/libutf8proc.dylib (found suitable version \"2.9.0\", minimum required is \"2.2.0\")\r\n-- Providing CMake module for utf8proc as part of Arrow CMake package\r\n-- Using pkg-config package for libutf8proc that is used by arrow for static link\r\n-- Found hdfs.h at: \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\/thirdparty\/hadoop\/include\/hdfs.h\r\n-- All bundled static libraries: mimalloc::mimalloc;LZ4::lz4;re2::re2\r\n-- CMAKE_C_FLAGS: -falign-functions=64 -Wall -g -O2 -Qunused-arguments  -Wall -Wno-unknown-warning-option -Wno-pass-failed -march=armv8-a\r\n-- CMAKE_CXX_FLAGS:  -fno-aligned-new -falign-functions=64 -Wall -g -O2 -Qunused-arguments -fcolor-diagnostics  -Wall -Wno-unknown-warning-option -Wno-pass-failed -march=armv8-a\r\n-- CMAKE_C_FLAGS_RELEASE: -O3 -DNDEBUG -O2\r\n-- CMAKE_CXX_FLAGS_RELEASE: -O3 -DNDEBUG -O2\r\n-- Creating bundled static library target arrow_bundled_dependencies at \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpUiyqGS\/filebc7d400cb95d\/release\/libarrow_bundled_dependencies.a\r\n-- Looking for backtrace\r\n-- Looking for backtrace - found\r\n-- backtrace facility detected in default set of libraries\r\n-- Found Backtrace: \/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX14.2.sdk\/usr\/include\r\n-- ---------------------------------------------------------------------\r\n-- Arrow version:                                 14.0.2\r\n--\r\n-- Build configuration summary:\r\n--   Generator: Unix Makefiles\r\n--   Build type: RELEASE\r\n--   Source directory: \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/tools\/cpp\r\n--   Install prefix: \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpnW2mtk\/R.INSTALLb7604ef963da\/arrow\/libarrow\/arrow-14.0.2\r\n--   Compile commands: \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpUiyqGS\/filebc7d400cb95d\/compile_commands.json\r\n--\r\n-- Compile and link options:\r\n--\r\n--   ARROW_CXXFLAGS=\"\" [default=\"\"]\r\n--       Compiler flags to append when compiling Arrow\r\n--   ARROW_BUILD_STATIC=ON [default=ON]\r\n--       Build static libraries\r\n--   ARROW_BUILD_SHARED=OFF [default=ON]\r\n--       Build shared libraries\r\n--   ARROW_PACKAGE_KIND=\"\" [default=\"\"]\r\n--       Arbitrary string that identifies the kind of package\r\n--       (for informational purposes)\r\n--   ARROW_GIT_ID=\"\" [default=\"\"]\r\n--       The Arrow git commit id (if any)\r\n--   ARROW_GIT_DESCRIPTION=\"\" [default=\"\"]\r\n--       The Arrow git commit description (if any)\r\n--   ARROW_NO_DEPRECATED_API=OFF [default=OFF]\r\n--       Exclude deprecated APIs from build\r\n--   ARROW_POSITION_INDEPENDENT_CODE=ON [default=ON]\r\n--       Whether to create position-independent target\r\n--   ARROW_USE_CCACHE=ON [default=ON]\r\n--       Use ccache when compiling (if available)\r\n--   ARROW_USE_SCCACHE=ON [default=ON]\r\n--       Use sccache when compiling (if available),\r\n--       takes precedence over ccache if a storage backend is configured\r\n--   ARROW_USE_LD_GOLD=OFF [default=OFF]\r\n--       Use ld.gold for linking on Linux (if available)\r\n--   ARROW_USE_PRECOMPILED_HEADERS=OFF [default=OFF]\r\n--       Use precompiled headers when compiling\r\n--   ARROW_SIMD_LEVEL=NEON [default=DEFAULT|NONE|SSE4_2|AVX2|AVX512|NEON|SVE|SVE128|SVE256|SVE512]\r\n--       Compile-time SIMD optimization level\r\n--   ARROW_RUNTIME_SIMD_LEVEL=MAX [default=MAX|NONE|SSE4_2|AVX2|AVX512]\r\n--       Max runtime SIMD optimization level\r\n--   ARROW_ALTIVEC=ON [default=ON]\r\n--       Build with Altivec if compiler has support\r\n--   ARROW_RPATH_ORIGIN=OFF [default=OFF]\r\n--       Build Arrow libraries with RATH set to $ORIGIN\r\n--   ARROW_INSTALL_NAME_RPATH=ON [default=ON]\r\n--       Build Arrow libraries with install_name set to @rpath\r\n--   ARROW_GGDB_DEBUG=ON [default=ON]\r\n--       Pass -ggdb flag to debug builds\r\n--   ARROW_WITH_MUSL=OFF [default=OFF]\r\n--       Whether the system libc is musl or not\r\n--   ARROW_ENABLE_THREADING=ON [default=ON]\r\n--       Enable threading in Arrow core\r\n--\r\n-- Test and benchmark options:\r\n--\r\n--   ARROW_BUILD_EXAMPLES=OFF [default=OFF]\r\n--       Build the Arrow examples\r\n--   ARROW_BUILD_TESTS=OFF [default=OFF]\r\n--       Build the Arrow googletest unit tests\r\n--   ARROW_ENABLE_TIMING_TESTS=ON [default=ON]\r\n--       Enable timing-sensitive tests\r\n--   ARROW_BUILD_INTEGRATION=OFF [default=OFF]\r\n--       Build the Arrow integration test executables\r\n--   ARROW_BUILD_BENCHMARKS=OFF [default=OFF]\r\n--       Build the Arrow micro benchmarks\r\n--   ARROW_BUILD_BENCHMARKS_REFERENCE=OFF [default=OFF]\r\n--       Build the Arrow micro reference benchmarks\r\n--   ARROW_BUILD_OPENMP_BENCHMARKS=OFF [default=OFF]\r\n--       Build the Arrow benchmarks that rely on OpenMP\r\n--   ARROW_BUILD_DETAILED_BENCHMARKS=OFF [default=OFF]\r\n--       Build benchmarks that do a longer exploration of performance\r\n--   ARROW_TEST_LINKAGE=static [default=static|shared]\r\n--       Linkage of Arrow libraries with unit tests executables.\r\n--   ARROW_FUZZING=OFF [default=OFF]\r\n--       Build Arrow Fuzzing executables\r\n--   ARROW_LARGE_MEMORY_TESTS=OFF [default=OFF]\r\n--       Enable unit tests which use large memory\r\n--\r\n-- Lint options:\r\n--\r\n--   ARROW_ONLY_LINT=OFF [default=OFF]\r\n--       Only define the lint and check-format targets\r\n--   ARROW_VERBOSE_LINT=OFF [default=OFF]\r\n--       If off, 'quiet' flags will be passed to linting tools\r\n--   ARROW_GENERATE_COVERAGE=OFF [default=OFF]\r\n--       Build with C++ code coverage enabled\r\n--\r\n-- Checks options:\r\n--\r\n--   ARROW_TEST_MEMCHECK=OFF [default=OFF]\r\n--       Run the test suite using valgrind --tool=memcheck\r\n--   ARROW_USE_ASAN=OFF [default=OFF]\r\n--       Enable Address Sanitizer checks\r\n--   ARROW_USE_TSAN=OFF [default=OFF]\r\n--       Enable Thread Sanitizer checks\r\n--   ARROW_USE_UBSAN=OFF [default=OFF]\r\n--       Enable Undefined Behavior sanitizer checks\r\n--\r\n-- Project component options:\r\n--\r\n--   ARROW_ACERO=ON [default=OFF]\r\n--       Build the Arrow Acero Engine Module\r\n--   ARROW_AZURE=OFF [default=OFF]\r\n--       Build Arrow with Azure support (requires the Azure SDK for C++)\r\n--   ARROW_BUILD_UTILITIES=OFF [default=OFF]\r\n--       Build Arrow commandline utilities\r\n--   ARROW_COMPUTE=ON [default=OFF]\r\n--       Build all Arrow Compute kernels\r\n--   ARROW_CSV=ON [default=OFF]\r\n--       Build the Arrow CSV Parser Module\r\n--   ARROW_CUDA=OFF [default=OFF]\r\n--       Build the Arrow CUDA extensions (requires CUDA toolkit)\r\n--   ARROW_DATASET=ON [default=OFF]\r\n--       Build the Arrow Dataset Modules\r\n--   ARROW_FILESYSTEM=ON [default=OFF]\r\n--       Build the Arrow Filesystem Layer\r\n--   ARROW_FLIGHT=OFF [default=OFF]\r\n--       Build the Arrow Flight RPC System (requires GRPC, Protocol Buffers)\r\n--   ARROW_FLIGHT_SQL=OFF [default=OFF]\r\n--       Build the Arrow Flight SQL extension\r\n--   ARROW_GANDIVA=OFF [default=OFF]\r\n--       Build the Gandiva libraries\r\n--   ARROW_GCS=OFF [default=OFF]\r\n--       Build Arrow with GCS support (requires the GCloud SDK for C++)\r\n--   ARROW_HDFS=OFF [default=OFF]\r\n--       Build the Arrow HDFS bridge\r\n--   ARROW_IPC=ON [default=ON]\r\n--       Build the Arrow IPC extensions\r\n--   ARROW_JEMALLOC=OFF [default=ON]\r\n--       Build the Arrow jemalloc-based allocator\r\n--   ARROW_JSON=ON [default=OFF]\r\n--       Build Arrow with JSON support (requires RapidJSON)\r\n--   ARROW_MIMALLOC=ON [default=OFF]\r\n--       Build the Arrow mimalloc-based allocator\r\n--   ARROW_PARQUET=ON [default=OFF]\r\n--       Build the Parquet libraries\r\n--   ARROW_ORC=OFF [default=OFF]\r\n--       Build the Arrow ORC adapter\r\n--   ARROW_PYTHON=OFF [default=OFF]\r\n--       Build some components needed by PyArrow.\r\n--       (This is a deprecated option. Use CMake presets instead.)\r\n--   ARROW_S3=OFF [default=OFF]\r\n--       Build Arrow with S3 support (requires the AWS SDK for C++)\r\n--   ARROW_SKYHOOK=OFF [default=OFF]\r\n--       Build the Skyhook libraries\r\n--   ARROW_SUBSTRAIT=OFF [default=OFF]\r\n--       Build the Arrow Substrait Consumer Module\r\n--   ARROW_TENSORFLOW=OFF [default=OFF]\r\n--       Build Arrow with TensorFlow support enabled\r\n--   ARROW_TESTING=OFF [default=OFF]\r\n--       Build the Arrow testing libraries\r\n--\r\n-- Thirdparty toolchain options:\r\n--\r\n--   ARROW_DEPENDENCY_SOURCE=AUTO [default=AUTO|BUNDLED|SYSTEM|CONDA|VCPKG|BREW]\r\n--       Method to use for acquiring arrow's build dependencies\r\n--   ARROW_VERBOSE_THIRDPARTY_BUILD=ON [default=OFF]\r\n--       Show output from ExternalProjects rather than just logging to files\r\n--   ARROW_DEPENDENCY_USE_SHARED=ON [default=ON]\r\n--       Link to shared libraries\r\n--   ARROW_BOOST_USE_SHARED=OFF [default=ON]\r\n--       Rely on Boost shared libraries where relevant\r\n--   ARROW_BROTLI_USE_SHARED=ON [default=ON]\r\n--       Rely on Brotli shared libraries where relevant\r\n--   ARROW_BZ2_USE_SHARED=ON [default=ON]\r\n--       Rely on Bz2 shared libraries where relevant\r\n--   ARROW_GFLAGS_USE_SHARED=ON [default=ON]\r\n--       Rely on GFlags shared libraries where relevant\r\n--   ARROW_GRPC_USE_SHARED=ON [default=ON]\r\n--       Rely on gRPC shared libraries where relevant\r\n--   ARROW_JEMALLOC_USE_SHARED=ON [default=ON]\r\n--       Rely on jemalloc shared libraries where relevant\r\n--   ARROW_LLVM_USE_SHARED=ON [default=ON]\r\n--       Rely on LLVM shared libraries where relevant\r\n--   ARROW_LZ4_USE_SHARED=ON [default=ON]\r\n--       Rely on lz4 shared libraries where relevant\r\n--   ARROW_OPENSSL_USE_SHARED=ON [default=ON]\r\n--       Rely on OpenSSL shared libraries where relevant\r\n--   ARROW_PROTOBUF_USE_SHARED=ON [default=ON]\r\n--       Rely on Protocol Buffers shared libraries where relevant\r\n--   ARROW_SNAPPY_USE_SHARED=ON [default=ON]\r\n--       Rely on snappy shared libraries where relevant\r\n--   ARROW_THRIFT_USE_SHARED=ON [default=ON]\r\n--       Rely on thrift shared libraries where relevant\r\n--   ARROW_UTF8PROC_USE_SHARED=ON [default=ON]\r\n--       Rely on utf8proc shared libraries where relevant\r\n--   ARROW_ZSTD_USE_SHARED=ON [default=ON]\r\n--       Rely on zstd shared libraries where relevant\r\n--   ARROW_USE_GLOG=OFF [default=OFF]\r\n--       Build libraries with glog support for pluggable logging\r\n--   ARROW_WITH_BACKTRACE=ON [default=ON]\r\n--       Build with backtrace support\r\n--   ARROW_WITH_OPENTELEMETRY=OFF [default=OFF]\r\n--       Build libraries with OpenTelemetry support for distributed tracing\r\n--   ARROW_WITH_BROTLI=OFF [default=OFF]\r\n--       Build with Brotli compression\r\n--   ARROW_WITH_BZ2=OFF [default=OFF]\r\n--       Build with BZ2 compression\r\n--   ARROW_WITH_LZ4=ON [default=OFF]\r\n--       Build with lz4 compression\r\n--   ARROW_WITH_SNAPPY=ON [default=OFF]\r\n--       Build with Snappy compression\r\n--   ARROW_WITH_ZLIB=OFF [default=OFF]\r\n--       Build with zlib compression\r\n--   ARROW_WITH_ZSTD=OFF [default=OFF]\r\n--       Build with zstd compression\r\n--   ARROW_WITH_UCX=OFF [default=OFF]\r\n--       Build with UCX transport for Arrow Flight\r\n--       (only used if ARROW_FLIGHT is ON)\r\n--   ARROW_WITH_UTF8PROC=ON [default=ON]\r\n--       Build with support for Unicode properties using the utf8proc library\r\n--       (only used if ARROW_COMPUTE is ON or ARROW_GANDIVA is ON)\r\n--   ARROW_WITH_RE2=ON [default=ON]\r\n--       Build with support for regular expressions using the re2 library\r\n--       (only used if ARROW_COMPUTE or ARROW_GANDIVA is ON)\r\n--\r\n-- Parquet options:\r\n--\r\n--   PARQUET_MINIMAL_DEPENDENCY=OFF [default=OFF]\r\n--       Depend only on Thirdparty headers to build libparquet.\r\n--       Always OFF if building binaries\r\n--   PARQUET_BUILD_EXECUTABLES=OFF [default=OFF]\r\n--       Build the Parquet executable CLI tools. Requires static libraries to be built.\r\n--   PARQUET_BUILD_EXAMPLES=OFF [default=OFF]\r\n--       Build the Parquet examples. Requires static libraries to be built.\r\n--   PARQUET_REQUIRE_ENCRYPTION=OFF [default=OFF]\r\n--       Build support for encryption. Fail if OpenSSL is not found\r\n--\r\n-- Gandiva options:\r\n--\r\n--   ARROW_GANDIVA_STATIC_LIBSTDCPP=OFF [default=OFF]\r\n--       Include -static-libstdc++ -static-libgcc when linking with\r\n--       Gandiva static libraries\r\n--   ARROW_GANDIVA_PC_CXX_FLAGS=\"\" [default=\"\"]\r\n--       Compiler flags to append when pre-compiling Gandiva operations\r\n--\r\n-- Advanced developer options:\r\n--\r\n--   ARROW_EXTRA_ERROR_CONTEXT=OFF [default=OFF]\r\n--       Compile with extra error context (line numbers, code)\r\n--   ARROW_OPTIONAL_INSTALL=OFF [default=OFF]\r\n--       If enabled install ONLY targets that have already been built. Please be\r\n--       advised that if this is enabled 'install' will fail silently on components\r\n--       that have not been built\r\n--   ARROW_GDB_INSTALL_DIR=\"\" [default=\"\"]\r\n--       Use a custom install directory for GDB plugin.\r\n--       In general, you don't need to specify this because the default\r\n--       (CMAKE_INSTALL_FULL_BINDIR on Windows, CMAKE_INSTALL_FULL_LIBDIR otherwise)\r\n--       is reasonable.\r\n--   Outputting build configuration summary to \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpUiyqGS\/filebc7d400cb95d\/cmake_summary.json\r\n-- Configuring done (3.8s)\r\n-- Generating done (0.2s)\r\n-- Build files have been written to: \/var\/folders\/vm\/bbcxtl3s63l7gn9h1tvfh1mm0000gn\/T\/RtmpUiyqGS\/filebc7d400cb95d\r\n+ \/opt\/homebrew\/bin\/cmake --build . --target install -- -j 2\r\n-----SNIP------\r\ninstalling to \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\r\n** R\r\n** inst\r\n** byte-compile and prepare package for lazy loading\r\n** help\r\n*** installing help indices\r\n** building package indices\r\n** testing if installed package can be loaded from temporary location\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  dlopen(\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so, 0x0006): symbol not found in flat namespace '__ZN3re212re2_internal5ParseINSt3__117basic_string_viewIcNS2_11char_traitsIcEEEEEEbPKcmPT_'\r\nError: loading failed\r\nExecution halted\r\nERROR: loading failed\r\n* removing \u2018\/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/library\/arrow\u2019\r\n```\r\n<\/details>\r\n\r\nThat looks like something to do with re2. Creating an offline archive on Windows then building on Linux does work as confirmed in https:\/\/github.com\/apache\/arrow\/pull\/40232#issuecomment-1979985703.\r\n\r\nIt's not clear to me if there's any reason why this shouldn't work so it'd be good (albeit low priority) to investigate what's going on.\n\n### Component(s)\n\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"GH-40384: [Python] Expand the the C Device Interface bindings to support import on CUDA device","body":"### Rationale for this change\r\n\r\nFollow-up on https:\/\/github.com\/apache\/arrow\/issues\/39979 which added `_export_to_c_device`\/`_import_from_c_device` methods, but for now only for CPU devices.\r\n\r\n### What changes are included in this PR?\r\n\r\n* Use the `arrow::cuda::DefaultMemoryMapper` instead of `arrow::DefaultDeviceMapper` in the python bindings, when the `pyarrow.cuda` module is available\r\n* Add tests for exporting\/importing with the device interface on CUDA\r\n\r\n\r\n### Are these changes tested?\r\n\r\nYes, added tests for CUDA.\r\n\r\n* GitHub Issue: #40384","comments":[":warning: GitHub issue #40384 **has been automatically assigned in GitHub** to PR creator.","@github-actions crossbow submit test-cuda-python","Revision: 05dd7a5d3796636bc263826388653d2ebc57c804\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-b585ced356](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-b585ced356)\n\n|Task|Status|\n|----|------|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b585ced356-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8343980442\/job\/22835346348)|","@github-actions crossbow submit test-cuda-python","Revision: 83b0d587f62f5def2e213d094daee4e8545bbab0\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-17a7524191](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-17a7524191)\n\n|Task|Status|\n|----|------|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-17a7524191-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8345754091\/job\/22841389263)|"],"labels":["Component: Python","awaiting changes"]},{"title":" [Python] Expand the the C Device Interface bindings to support CUDA device","body":"Follow-up on https:\/\/github.com\/apache\/arrow\/issues\/39979 which added `_export_to_c_device`\/`_import_from_c_device` methods, but for now only for CPU devices. Expand this to support the CUDA device.","comments":[],"labels":["Component: Python"]},{"title":"[Python] Does pyarrow support reading avro files?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI'm looking for a way to load a local avro file into pyarrow and then dump it out as a parquet file. Pandas is taking a long time for me and was wondering whether pyarrow would be a better fit.\n\n### Component(s)\n\nPython","comments":["I do not think Arrow C++ supports Avro file formats and after looking at the docs I can confirm that: https:\/\/arrow.apache.org\/docs\/dev\/status.html#third-party-data-formats"],"labels":["Component: Python","Type: usage"]},{"title":"[DO NOT MERGE][C++] Reduce the number of logical index resolutions that might require a binary search","body":"### Rationale for this change\r\n\r\nTODO\r\n\r\n### What changes are included in this PR?\r\n\r\nTODO\r\n\r\n### Are these changes tested?\r\n\r\nTODO\r\n","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","@ursabot please benchmark","Benchmark runs are scheduled for commit f09f3b3445eed79539fad885ac2e46d03e1bbb53. Watch https:\/\/buildkite.com\/apache-arrow and https:\/\/conbench.ursa.dev for updates. A comment will be posted here when the runs are complete.","Thanks for your patience. Conbench analyzed the 7 benchmarking runs that have been run so far on PR commit f09f3b3445eed79539fad885ac2e46d03e1bbb53.\n\nThere were 2 benchmark results with an error:\n\n- Pull Request Run on `ursa-i9-9960x` at [2024-03-05 17:27:28Z](https:\/\/conbench.ursa.dev\/compare\/runs\/a47f05bda64a4cbaa7b2c8c1114374cd...629b4e780d7941db8970f9983b4094b3\/)\n  - [`csv-read` (Python) with compression=gzip, dataset=nyctaxi_2010-01, output_format=arrow_table, streaming=file](https:\/\/conbench.ursa.dev\/benchmark-results\/065e755da4687b8d800092d3f39d8433)\n  - [`csv-read` (Python) with compression=gzip, dataset=nyctaxi_2010-01, output_format=arrow_table, streaming=file](https:\/\/conbench.ursa.dev\/benchmark-results\/065e755da53372658000dcac75668f87)\n\nThere were no benchmark performance regressions. \ud83c\udf89\n\nThe [full Conbench report](https:\/\/github.com\/apache\/arrow\/runs\/22317616962) has more details."],"labels":["Component: C++","awaiting review"]},{"title":"[Python][CI] Nightly pandas\/numpy builds are crashing","body":"The last run of the pandas nightly\/upstream_devel builds have been segfaulting. The run of 2024-03-01 was still working (one actual error in the pandas nightly, green in upstream_devel). Now both are crashing, also after some other tests failing.\r\n\r\nExample: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8134146382\/job\/22226672519\r\n\r\nMy suspicion is that this is more likely related to numpy main (they have been merging a lot of changes the last days ramping up to branching 2.0)","comments":["Hmm, it seems the crashes were in the one but last build, the ones from last night are failing \"normally\" again. Maybe there was a brief incompatibility between the numpy and pandas nightly wheels, or just something on numpy's side that was fixed in the meantime.\r\n\r\nAnyway, we still get a bunch of failures because of changes to `copy` behaviour in `np.asarray`. I added a bullet point about this in https:\/\/github.com\/apache\/arrow\/issues\/39532. We might also need to wait until things are fixed for this in pandas to get a fully green build (https:\/\/github.com\/pandas-dev\/pandas\/pull\/57172)"],"labels":["Component: Python","Component: Continuous Integration"]},{"title":"[C++] Make Flatbuffers serialization more deterministic","body":"### Describe the enhancement requested\n\nIn https:\/\/github.com\/apache\/arrow\/issues\/40202#issuecomment-1978589865 it was determined that Flatbuffers serialization of a Arrow schema did not always result in the same binary encoding.\r\n\r\nA [changeset in Flatbuffers](https:\/\/github.com\/google\/flatbuffers\/commit\/f575b02fda04fe579fb23442234feb8129b77ee2) led us to a likely explanation, as there's a place in our code where serialization of strings depends on argument evaluation order:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/3ba6d286caad328b8572a3b9228045da8c8d2043\/cpp\/src\/arrow\/ipc\/metadata_internal.cc#L478-L481\r\n\r\nBinary inspection of the data files provided in that issue seems to confirm that hypothesis.\r\n\r\nThis is obviously a very minor issue, but should also be easy to fix.\r\n\n\n### Component(s)\n\nC++","comments":["cc @felipecrv ","Any thoughts on how one could add a test when submitting a PR for this? I'm not much of a C++ programmer but I know enough to follow the change from flatbuffers and attempt to apply it. A test, however requires a way to trigger different evaluation order in the current code that would be corrected in the change.","I think a regression test would be enough, so long as you can reproduce the non-determinism in that test before making the code change.","What I mean is that I'm not sure how to reproduce the non-determinism in the original.  In my understanding, the order is actually determined at compile-time and could differ across compilers or platforms.","Right. There might be a better way than this but adding a test that exercises `SchemaToFlatbuffer` and asserts the resulting Flatbuffer schema is byte-identical to some value you get, that test should fail on CI one one or more platforms. You could put up a draft PR and let CI exercise it to save you work. @felipecrv will probably have a better idea though.","> Any thoughts on how one could add a test when submitting a PR for this? I'm not much of a C++ programmer but I know enough to follow the change from flatbuffers and attempt to apply it. A test, however requires a way to trigger different evaluation order in the current code that would be corrected in the change.\r\n\r\nI wouldn't worry to much about adding a test. The non-determinism here doesn't even lead to bugs. But it creates non-determinism in output that can make testing and file comparisons harder. In a way, by fixing this you are already contributing to testability itself.","I disagree and think we should try to add a test for this, if only to validate that we are actually fixing something."],"labels":["Type: enhancement","good-second-issue"]},{"title":"[Python][FlightRPC] Arrow Flight RPC handling concurrent requests","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nContinuing the [discussion](https:\/\/github.com\/apache\/arrow\/issues\/34607) around concurrent requests in `Arrow Flight RPC` on a new thread.\r\n\r\n@lidavidm stated the following regarding concurrent requests in `Arrow Flight RPC` in Python.\r\n\r\n> If you make 10 concurrent requests to a Python service, the service can only handle one at a time due to the GIL. So the concurrency will not get you any wall-clock-time speedup. @pvardanis notes that they got the same time whether they used a thread pool or ran the requests sequentially. That is the question I am answering. They've simply hijacked this thread to ask whether asyncio would make a difference: I contend it would not.\r\n\r\n(cc @pitrou) I'm still confused about what happens when I send concurrent requests using a `ThreadPoolExecutor` with `max_workers=16` since my server seems to handle requests in parallel, in contrast to what you're stating:\r\n```\r\n05-Mar-24 11:58:29 - root - INFO: [\ud83d\udce1] Starting server on `grpc:\/\/0.0.0.0:8080`...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - INFO: Processing data...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n05-Mar-24 11:58:43 - mac.service.arrow_flight.server - DEBUG: Starting batch processing...\r\n```\r\nThe server is hosting a ML model of around 6GB in size and the prediction step is CPU-bound. It's still not clear to me what happens with the GIL and whether Arrow Flight RPC can process requests in parallel or not. \r\n\r\nWhat I've also noticed is that the higher the `max_workers` value and the number of concurrent requests I'm sending (e.g. 100) the more likely the server would hang for a bit during the prediction step, probably because CPU usage is at maximum. FWIW the results are not lost and I'm still getting what I'm supposed to, it just gets delayed a little bit more which ofc compared to handling the requests sequentially is way faster.\r\n\r\nHere are some benchmarks I did:\r\n<img width=\"302\" alt=\"image\" src=\"https:\/\/github.com\/apache\/arrow\/assets\/37624791\/99b43e12-39ef-4759-a040-a84a7b2c5997\">\r\n`max_workers=1` shows the exact same behavior as sending the requests sequentially. In all benchmarks, parallel requests are significantly faster than sequential ones (i.e. with `max_workers=1`), but it seems there's a sweet spot of `max_workers` (which I assume corresponds to the number of concurrent requests I'm sending at once) where allocating too many workers pushes the CPU to its limits, thus the server hanging which results to slower elapsed time.\r\n\r\nI have some other questions in addition:\r\n- Can I configure the number of concurrent requests my server handles at a time via Arrow Flight RPC?\r\n- Is it possible to split the server to multiple workers as if I'd have different instances of the server (including the model) running in parallel?\r\n\r\n### Component(s)\r\n\r\nFlightRPC, Python","comments":["Sorry, I was being a little unclear. I meant that for a CPU-bound service, effectively only one RPC can get serviced at a time (but they will run concurrently on separate threads) due to the GIL. However, if your CPU-bound operation is releasing the GIL, then that doesn't apply.","> Can I configure the number of concurrent requests my server handles at a time via Arrow Flight RPC?\r\n\r\nNo. gRPC doesn't expose meaningful control over this. Handle it at the application layer (e.g. a semaphore or a mutex+condition variable+counter).\r\n\r\n> Is it possible to split the server to multiple workers as if I'd have different instances of the server (including the model) running in parallel?\r\n\r\nNot really. If you host on say Kubernetes or otherwise have control over DNS, if you have your client connect to a host that resolves to multiple IP addresses, gRPC can distribute requests over those hosts. https:\/\/github.com\/grpc\/grpc\/blob\/master\/doc\/load-balancing.md#pick_first\r\n\r\n(The wording is extremely confusing because it's overly abstracted and they also tried to invent their own load balancing API standard that everything is written in terms of, despite being deprecated and never really used)\r\n\r\nOtherwise, use a proxy\/load balancer in the middle.","@lidavidm Can I have a `ServerMiddleware` that uses an `asyncio.Semaphore` configured to a desired number for `max_concurrency`and run `do_exchange()` on the server side based on whether the semaphore is locked or not?","In this case, the semaphore would simply block the RPC entirely, the RPC will not continue until the middleware stops blocking. ","Right, and also, **\"asyncio primitives are not thread-safe, therefore they should not be used for OS thread synchronization\"**. Coming straight from the documentation here:\r\nhttps:\/\/docs.python.org\/3\/library\/asyncio-sync.html\r\n","> In this case, the semaphore would simply block the RPC entirely, the RPC will not continue until the middleware stops blocking.\r\n\r\nIf my intention is to have a limit on the concurrent requests I'm processing at once, I guess that's fine, no? In any case, if you think there's a more appropriate way to do something similar on the server side I'm open to suggestions.","It should be fine, I can't say I've tried this before but I don't think you have many other alternatives. You may want to consider something more sophisticated if you expect load to be wildly variable, e.g. rejecting requests with UNAVAILABLE and having clients retry later (or on a different host), since the request will still sit there and take up a server thread."],"labels":["Component: Python","Component: FlightRPC","Type: usage"]},{"title":"GH-40060: [C++][Python] Basic conversion of RecordBatch to Arrow Tensor - add support for different data types","body":"### What changes are included in this PR?\r\n\r\n- Added support for `RecordBatches` with fields of different type in the conversion `RecordBatch` \u2192 `Tensor`.\r\n- Added detail of the constraints to the `RecordBatch.to_tensor()` docstrings, see https:\/\/github.com\/apache\/arrow\/pull\/40064#discussion_r1512307964.\r\n\r\n### Are these changes tested?\r\n\r\nYes.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\n* GitHub Issue: #40060","comments":[":warning: GitHub issue #40060 **has been automatically assigned in GitHub** to PR creator.","@zeroshade would you mind reviewing this feature also?"],"labels":["Component: C++","Component: Python","awaiting changes"]},{"title":"GH-40357: [C++] Add benchmark for ToTensor conversions","body":"### Rationale for this change\r\n\r\nWe should add benchmarks to be sure not to cause regressions while working on additional implementations of `RecordBatch::ToTensor` and `Table::ToTensor`.\r\n\r\n### What changes are included in this PR?\r\n\r\nNew `cpp\/src\/arrow\/to_tensor_benchmark.cc file`.\n* GitHub Issue: #40357","comments":["Can you show the result of running them? And we might want to use some more data to get a more reliable result?","This was the result output:\r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-ahcnq1ah\/WORKSPACE\/build\/release\/arrow-to-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 17.32, 18.72, 16.18\r\n----------------------------------------------------------------------------------------\r\nBenchmark                              Time             CPU   Iterations UserCounters...\r\n----------------------------------------------------------------------------------------\r\nRecordBatchUniformTypesSimple        624 ns          624 ns      1125492 bytes_per_second=1.29039Gi\/s items_per_second=43.2982M\/s\r\n```\r\n\r\nWIll use `RandomArrayGenerator` to generate more data and add the result here.","The result from running `archery benchmark diff --benchmark-filter=BatchToTensorSimple` on the second commit (but with arrays of length `100`, not `500`):\r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-jun4cokj\/WORKSPACE\/build\/release\/arrow-to-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 24.95, 25.04, 19.14\r\n---------------------------------------------------------------------------------------------\r\nBenchmark                                   Time             CPU   Iterations UserCounters...\r\n---------------------------------------------------------------------------------------------\r\nBatchToTensorSimple<UInt8Type>            550 ns          550 ns      1254345 bytes_per_second=4.06699Gi\/s items_per_second=545.863M\/s\r\nBatchToTensorSimple<UInt16Type>           555 ns          553 ns      1235570 bytes_per_second=8.08251Gi\/s items_per_second=542.408M\/s\r\nBatchToTensorSimple<UInt32Type>           569 ns          568 ns      1253335 bytes_per_second=15.7341Gi\/s items_per_second=527.949M\/s\r\nBatchToTensorSimple<UInt64Type>           580 ns          580 ns      1237449 bytes_per_second=30.8253Gi\/s items_per_second=517.163M\/s\r\nBatchToTensorSimple<Int8Type>             548 ns          548 ns      1249732 bytes_per_second=4.07944Gi\/s items_per_second=547.533M\/s\r\nBatchToTensorSimple<Int16Type>            623 ns          568 ns      1233654 bytes_per_second=7.87246Gi\/s items_per_second=528.312M\/s\r\nBatchToTensorSimple<Int32Type>            565 ns          564 ns      1204923 bytes_per_second=15.8461Gi\/s items_per_second=531.706M\/s\r\nBatchToTensorSimple<Int64Type>            585 ns          585 ns      1269059 bytes_per_second=30.5699Gi\/s items_per_second=512.878M\/s\r\nBatchToTensorSimple<HalfFloatType>        545 ns          544 ns      1217900 bytes_per_second=8.21219Gi\/s items_per_second=551.111M\/s\r\nBatchToTensorSimple<FloatType>            575 ns          574 ns      1239991 bytes_per_second=15.5835Gi\/s items_per_second=522.896M\/s\r\nBatchToTensorSimple<DoubleType>           567 ns          566 ns      1152074 bytes_per_second=31.5943Gi\/s items_per_second=530.065M\/s\r\n```","Current output when running `archery benchmark diff --benchmark-filter=BatchToTensorSimple`:\r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-e8lvkw1g\/WORKSPACE\/build\/release\/arrow-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 27.50, 28.87, 23.74\r\n-----------------------------------------------------------------------------------------------------------\r\nBenchmark                                                 Time             CPU   Iterations UserCounters...\r\n-----------------------------------------------------------------------------------------------------------\r\nBatchToTensorSimple<UInt8Type>\/65536\/10000             4121 us         4107 us          171 bytes_per_second=15.217Mi\/s items_per_second=12.765G\/s null_percent=0.01 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/65536\/100               4273 us         4219 us          170 bytes_per_second=14.8143Mi\/s items_per_second=12.4271G\/s null_percent=1 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/65536\/10                4019 us         4003 us          173 bytes_per_second=15.6149Mi\/s items_per_second=13.0988G\/s null_percent=10 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/65536\/2                 4100 us         4083 us          136 bytes_per_second=15.3084Mi\/s items_per_second=12.8416G\/s null_percent=50 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/65536\/1                 3972 us         3894 us          178 bytes_per_second=16.0516Mi\/s items_per_second=13.465G\/s null_percent=100 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/65536\/0                 3953 us         3927 us          178 bytes_per_second=15.9142Mi\/s items_per_second=13.3498G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/4194304\/10000       15398661 us      1947088 us            1 bytes_per_second=2.05435Mi\/s items_per_second=1.72331G\/s null_percent=0.01 size=4.1943M\r\n.\r\n.\r\n.\r\n```","Output from running the benchmarks on the latest commit:\r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-y9o8zv4d\/WORKSPACE\/build\/release\/arrow-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 20.67, 17.39, 10.95\r\n-----------------------------------------------------------------------------------------------------\r\nBenchmark                                           Time             CPU   Iterations UserCounters...\r\n-----------------------------------------------------------------------------------------------------\r\nBatchToTensorSimple<UInt8Type>\/65536           443099 ns       442863 ns         1580 bytes_per_second=141.127Mi\/s items_per_second=14.7983G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<UInt8Type>\/4194304       38391076 ns     35795222 ns           18 bytes_per_second=111.747Mi\/s items_per_second=11.7175G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<UInt16Type>\/65536          882040 ns       881129 ns          747 bytes_per_second=70.9318Mi\/s items_per_second=7.43773G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<UInt16Type>\/4194304     118462838 ns     81059222 ns            9 bytes_per_second=49.3466Mi\/s items_per_second=5.17437G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<UInt32Type>\/65536         1937139 ns      1933673 ns          361 bytes_per_second=32.3219Mi\/s items_per_second=3.3892G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<UInt32Type>\/4194304    1271556625 ns    651396000 ns            1 bytes_per_second=6.14066Mi\/s items_per_second=643.895M\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<UInt64Type>\/65536         4440503 ns      4344614 ns          166 bytes_per_second=14.3856Mi\/s items_per_second=1.50844G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<UInt64Type>\/4194304    1.1486e+10 ns   1742537000 ns            1 bytes_per_second=2.2955Mi\/s items_per_second=240.701M\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<Int8Type>\/65536            415187 ns       410957 ns         1710 bytes_per_second=152.084Mi\/s items_per_second=15.9472G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<Int8Type>\/4194304        34241740 ns     33962150 ns           20 bytes_per_second=117.778Mi\/s items_per_second=12.3499G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<Int16Type>\/65536           812298 ns       810349 ns          917 bytes_per_second=77.1273Mi\/s items_per_second=8.08738G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<Int16Type>\/4194304       75301182 ns     70352375 ns            8 bytes_per_second=56.8566Mi\/s items_per_second=5.96185G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<Int32Type>\/65536          2033466 ns      2026663 ns          329 bytes_per_second=30.8389Mi\/s items_per_second=3.23369G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<Int32Type>\/4194304     1233238541 ns    562396000 ns            1 bytes_per_second=7.11243Mi\/s items_per_second=745.792M\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<Int64Type>\/65536          3969188 ns      3959770 ns          178 bytes_per_second=15.7837Mi\/s items_per_second=1.65505G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<Int64Type>\/4194304     1.5188e+10 ns   1823171000 ns            1 bytes_per_second=2.19398Mi\/s items_per_second=230.055M\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<HalfFloatType>\/65536       899771 ns       888509 ns          749 bytes_per_second=70.3426Mi\/s items_per_second=7.37595G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<HalfFloatType>\/4194304   71104797 ns     69327375 ns            8 bytes_per_second=57.6973Mi\/s items_per_second=6.05G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<FloatType>\/65536          2025175 ns      2021084 ns          347 bytes_per_second=30.924Mi\/s items_per_second=3.24262G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<FloatType>\/4194304     1087905188 ns    395840500 ns            2 bytes_per_second=10.1051Mi\/s items_per_second=1.05959G\/s null_percent=0 size=4.1943M\r\nBatchToTensorSimple<DoubleType>\/65536         4118269 ns      4089947 ns          170 bytes_per_second=15.2814Mi\/s items_per_second=1.60237G\/s null_percent=0 size=65.536k\r\nBatchToTensorSimple<DoubleType>\/4194304    9901101750 ns   1684713000 ns            1 bytes_per_second=2.37429Mi\/s items_per_second=248.963M\/s null_percent=0 size=4.1943M\r\n```","Looking better after your last suggestions Joris \ud83c\udf89 \r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-w9c6kiee\/WORKSPACE\/build\/release\/arrow-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 16.45, 17.62, 11.63\r\n-----------------------------------------------------------------------------------------------------\r\nBenchmark                                           Time             CPU   Iterations UserCounters...\r\n-----------------------------------------------------------------------------------------------------\r\nBatchToTensorSimple<UInt8Type>\/65536           458536 ns       458333 ns         1504 bytes_per_second=13.3168Gi\/s items_per_second=14.2988G\/s\r\nBatchToTensorSimple<UInt8Type>\/4194304       51650596 ns     40859385 ns           13 bytes_per_second=9.56023Gi\/s items_per_second=10.2652G\/s\r\nBatchToTensorSimple<UInt16Type>\/65536          443072 ns       441767 ns         1327 bytes_per_second=13.8161Gi\/s items_per_second=7.41748G\/s\r\nBatchToTensorSimple<UInt16Type>\/4194304      37997653 ns     36128500 ns           18 bytes_per_second=10.8121Gi\/s items_per_second=5.8047G\/s\r\nBatchToTensorSimple<UInt32Type>\/65536          556504 ns       525753 ns         1625 bytes_per_second=11.6091Gi\/s items_per_second=3.11629G\/s\r\nBatchToTensorSimple<UInt32Type>\/4194304      47726554 ns     38831059 ns           17 bytes_per_second=10.0596Gi\/s items_per_second=2.70035G\/s\r\nBatchToTensorSimple<UInt64Type>\/65536          543929 ns       510296 ns         1071 bytes_per_second=11.9607Gi\/s items_per_second=1.60534G\/s\r\nBatchToTensorSimple<UInt64Type>\/4194304     104417887 ns     55937176 ns           17 bytes_per_second=6.98328Gi\/s items_per_second=937.28M\/s\r\nBatchToTensorSimple<Int8Type>\/65536            542291 ns       530495 ns         1000 bytes_per_second=11.5053Gi\/s items_per_second=12.3537G\/s\r\nBatchToTensorSimple<Int8Type>\/4194304        55069580 ns     44818231 ns           13 bytes_per_second=8.71576Gi\/s items_per_second=9.35848G\/s\r\nBatchToTensorSimple<Int16Type>\/65536           472947 ns       466738 ns         1604 bytes_per_second=13.077Gi\/s items_per_second=7.02065G\/s\r\nBatchToTensorSimple<Int16Type>\/4194304       45937775 ns     40318200 ns           15 bytes_per_second=9.68855Gi\/s items_per_second=5.2015G\/s\r\nBatchToTensorSimple<Int32Type>\/65536           439955 ns       438705 ns         1351 bytes_per_second=13.9126Gi\/s items_per_second=3.73463G\/s\r\nBatchToTensorSimple<Int32Type>\/4194304       38181667 ns     36099833 ns           18 bytes_per_second=10.8207Gi\/s items_per_second=2.90466G\/s\r\nBatchToTensorSimple<Int64Type>\/65536           440425 ns       439585 ns         1583 bytes_per_second=13.8847Gi\/s items_per_second=1.86358G\/s\r\nBatchToTensorSimple<Int64Type>\/4194304       51548936 ns     39940333 ns           15 bytes_per_second=9.78021Gi\/s items_per_second=1.31268G\/s\r\nBatchToTensorSimple<HalfFloatType>\/65536       435417 ns       434107 ns         1526 bytes_per_second=14.0599Gi\/s items_per_second=7.54836G\/s\r\nBatchToTensorSimple<HalfFloatType>\/4194304   48649122 ns     38652385 ns           13 bytes_per_second=10.1061Gi\/s items_per_second=5.42567G\/s\r\nBatchToTensorSimple<FloatType>\/65536           432115 ns       430647 ns         1522 bytes_per_second=14.1729Gi\/s items_per_second=3.80451G\/s\r\nBatchToTensorSimple<FloatType>\/4194304       42923344 ns     38628000 ns           16 bytes_per_second=10.1125Gi\/s items_per_second=2.71455G\/s\r\nBatchToTensorSimple<DoubleType>\/65536          442113 ns       441402 ns         1304 bytes_per_second=13.8276Gi\/s items_per_second=1.85591G\/s\r\nBatchToTensorSimple<DoubleType>\/4194304      60867021 ns     44292875 ns           16 bytes_per_second=8.81914Gi\/s items_per_second=1.18368G\/s\r\n```","Thanks for this @AlenkaF . I have two general suggestions here:\r\n1) given that the types are purely physical here (i.e. float32 should use the same conversion code as int32 and uint32), we don't need to benchmark all numeric data types, we can limit ourselves to four integer types: int8, int16, int32, int64\r\n2) on the other hand, it would be nice to exercise different numbers of columns, because that could affect conversion performance: for example 3, 30, 300?\r\n\r\nDoes it make sense @AlenkaF @jorisvandenbossche ?","It does! Will update \ud83d\udc4d ","@pitrou I have included your suggestions.\r\nThis is the output with the latest changes:\r\n\r\n```\r\nRunning \/var\/folders\/gw\/q7wqd4tx18n_9t4kbkd0bj1m0000gn\/T\/arrow-archery-s2l7kna2\/WORKSPACE\/build\/release\/arrow-tensor-benchmark\r\nRun on (8 X 24 MHz CPU s)\r\nCPU Caches:\r\n  L1 Data 64 KiB\r\n  L1 Instruction 128 KiB\r\n  L2 Unified 4096 KiB (x8)\r\nLoad Average: 19.25, 15.77, 10.23\r\n-----------------------------------------------------------------------------------------------------\r\nBenchmark                                           Time             CPU   Iterations UserCounters...\r\n-----------------------------------------------------------------------------------------------------\r\nBatchToTensorSimple<Int8Type>\/65536\/3            3803 ns         3802 ns       179651 bytes_per_second=48.1626Gi\/s items_per_second=51.7142G\/s\r\nBatchToTensorSimple<Int8Type>\/65536\/30         141770 ns       140390 ns         5332 bytes_per_second=13.0426Gi\/s items_per_second=14.0044G\/s\r\nBatchToTensorSimple<Int8Type>\/65536\/300       1509271 ns      1488588 ns          471 bytes_per_second=12.3006Gi\/s items_per_second=13.2077G\/s\r\nBatchToTensorSimple<Int8Type>\/4194304\/3        892806 ns       890394 ns          792 bytes_per_second=13.1613Gi\/s items_per_second=14.1318G\/s\r\nBatchToTensorSimple<Int8Type>\/4194304\/30     10833571 ns     10319294 ns           68 bytes_per_second=11.3562Gi\/s items_per_second=12.1936G\/s\r\nBatchToTensorSimple<Int8Type>\/4194304\/300  1128155000 ns    551951000 ns            1 bytes_per_second=2.12315Gi\/s items_per_second=2.27972G\/s\r\nBatchToTensorSimple<Int16Type>\/65536\/3           3781 ns         3769 ns       185929 bytes_per_second=48.5878Gi\/s items_per_second=26.0854G\/s\r\nBatchToTensorSimple<Int16Type>\/65536\/30        129794 ns       129615 ns         5636 bytes_per_second=14.1269Gi\/s items_per_second=7.58431G\/s\r\nBatchToTensorSimple<Int16Type>\/65536\/300      1553976 ns      1550687 ns          435 bytes_per_second=11.808Gi\/s items_per_second=6.33938G\/s\r\nBatchToTensorSimple<Int16Type>\/4194304\/3       824934 ns       822791 ns          882 bytes_per_second=14.2427Gi\/s items_per_second=7.64648G\/s\r\nBatchToTensorSimple<Int16Type>\/4194304\/30     9991414 ns      9954623 ns           69 bytes_per_second=11.7722Gi\/s items_per_second=6.32013G\/s\r\nBatchToTensorSimple<Int16Type>\/4194304\/300  791524063 ns    310795500 ns            2 bytes_per_second=3.77057Gi\/s items_per_second=2.02431G\/s\r\nBatchToTensorSimple<Int32Type>\/65536\/3           3717 ns         3712 ns       183626 bytes_per_second=49.3228Gi\/s items_per_second=13.24G\/s\r\nBatchToTensorSimple<Int32Type>\/65536\/30        135493 ns       134325 ns         5035 bytes_per_second=13.6315Gi\/s items_per_second=3.65918G\/s\r\nBatchToTensorSimple<Int32Type>\/65536\/300      1607824 ns      1600713 ns          436 bytes_per_second=11.439Gi\/s items_per_second=3.07063G\/s\r\nBatchToTensorSimple<Int32Type>\/4194304\/3       863068 ns       860123 ns          782 bytes_per_second=13.6245Gi\/s items_per_second=3.6573G\/s\r\nBatchToTensorSimple<Int32Type>\/4194304\/30    10307080 ns     10272412 ns           68 bytes_per_second=11.408Gi\/s items_per_second=3.06231G\/s\r\nBatchToTensorSimple<Int32Type>\/4194304\/300  261872267 ns    147986600 ns            5 bytes_per_second=7.91879Gi\/s items_per_second=2.12568G\/s\r\nBatchToTensorSimple<Int64Type>\/65536\/3           3725 ns         3722 ns       183079 bytes_per_second=49.1992Gi\/s items_per_second=6.60341G\/s\r\nBatchToTensorSimple<Int64Type>\/65536\/30        126616 ns       126444 ns         5720 bytes_per_second=14.4811Gi\/s items_per_second=1.94362G\/s\r\nBatchToTensorSimple<Int64Type>\/65536\/300      1508292 ns      1506162 ns          445 bytes_per_second=12.1571Gi\/s items_per_second=1.6317G\/s\r\nBatchToTensorSimple<Int64Type>\/4194304\/3       837330 ns       835840 ns          833 bytes_per_second=14.0203Gi\/s items_per_second=1.88178G\/s\r\nBatchToTensorSimple<Int64Type>\/4194304\/30     9866716 ns      9823261 ns           69 bytes_per_second=11.9296Gi\/s items_per_second=1.60116G\/s\r\nBatchToTensorSimple<Int64Type>\/4194304\/300  745086639 ns    325750333 ns            3 bytes_per_second=3.59746Gi\/s items_per_second=482.843M\/s\r\n```"],"labels":["Component: C++","awaiting change review"]},{"title":"[C++] Add benchmark for ToTensor conversions","body":"### Describe the enhancement requested\n\nWe should add benchmarks to be sure not to cause regressions while working on additional implementations of `RecordBatch::ToTensor` and `Table::ToTensor`.\r\n\r\nThe first step is to create a benchmark file with a basic benchmark for a simple uniform case of `RecordBatch` to `Tensor` conversion (all fields same type, no missing values). With follow-up features being added we should also add more cases to this benchmark file.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":" GH-40342: [C++] move LocalFileSystem to the registry ","body":"### Rationale for this change\r\n\r\nMoving LocalFileSystem into the registry is a good first usage and will help us hammer out which aspects of built in file systems should remain public.\r\n\r\n\r\n### What changes are included in this PR?\r\n\r\nA factory for LocalFileSystem is added to the registry. `FileSystem::MakeUri` ( https:\/\/github.com\/apache\/arrow\/issues\/18316 ) is added to enable roundtripping filesystems through URIs. `file:\/\/` URIs now support a use_mmap query parameter, and `local:\/\/` URIs are also supported as an alias.\r\n\r\n<h6 id=\"reduce\">Reducing the set of bound classes<\/h6>\r\n\r\nSome unnecessary bindings to the LocalFileSystem concrete class are removed. This demonstrates that with a registered factory pattern, it is no longer necessary to keep a class hierarchy public for binding. Eventually (if desired), we can move concrete subclasses of FileSystem entirely out of public headers.\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nYes, all existing tests for file:\/\/ URIs continue to pass\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nFor consistency, local:\/\/ URIs will now be considered equivalent to corresponding file:\/\/ URIs\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->","comments":[],"labels":["Component: R","Component: C++","Component: Python","Component: Documentation","awaiting merge"]},{"title":"GH-40347: [C#] ArrowReaderImplementation:  fix for out of range exception","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nAdds an additional check for processing the loop to ensure it doesn't go longer than the FieldsList in the schema. \r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\nAll tests pass with the change included\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40347","comments":[":warning: GitHub issue #40347 **has been automatically assigned in GitHub** to PR creator."],"labels":["Component: C#","awaiting review"]},{"title":"GH-40348: [Python] Add Python wrapper for VariableShapeTensor","body":"### Rationale for this change\r\n\r\nOnce C++ implementation of VariableShapeTensor is complete (#38007) we want a Python wrapper for `VariableShapeTensor`.\r\n\r\n### What changes are included in this PR?\r\n\r\nThis adds a Python wrapper for `VariableShapeTensor`.\r\n\r\n### Are these changes tested?\r\n\r\nThis adds appropriate Python tests.\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes, a new extension and array type are exposed.\n* GitHub Issue: #40348","comments":[],"labels":["Component: C++","Component: Python","Component: Documentation","awaiting committer review"]},{"title":"[C++][FlightRPC] How to decode a pure gRPC response from an ArrowFlight to an arrow table?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI have some middleware client that wraps the gRPC client library and dispatches an arrow do_get request.\r\n\r\nThe response is a stream of ArrowFlight objects. How do I decode this into an Arrow table, assuming that the service returns a RecordBatchStream ?\r\n\r\nI tried to read the docs but I cannot seem to be able to put this together. \r\n\r\nI'm interested in Python\/cpp.\r\n\r\nThank you!\n\n### Component(s)\n\nFlightRPC","comments":["https:\/\/stackoverflow.com\/questions\/70629168\/how-to-decode-arrow-flight-flightdata-with-a-pure-grpc-client\/70629249#70629249","That said, I would like to expose this directly (see https:\/\/github.com\/apache\/arrow\/issues\/34607#issuecomment-1959517462) instead of making people jump through hoops. Consider raising some noise so I can justify this to other developers :)","Ok. Is this reliable to do ? Is this going to work for any endpoints ? Eg flightInfo ?\r\n\r\nIt's unlikely that people will use the Arrowflight client all the time, in my case I have to use a middleware that performs routing based on various things that exposes a different api and I only have access to the FlightData objects.","For FlightInfo, you can use FlightInfo.deserialize or if you generate the Protobuf code you can use it directly as well.","Would you mind providing a small snippet , please ?","If you have the byte representation, you can just pass that to FlightInfo.deserialize. If you have the Protobuf on hand then you'll have to get it back into bytes."],"labels":["Component: FlightRPC","Type: usage"]},{"title":"[Python] Add Python wrapper for VariableShapeTensor","body":"### Describe the enhancement requested\n\nOnce C++ implementation of VariableShapeTensor is complete (#38007) we want a Python wrapper for it.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[C#] csharp\/src\/Apache.Arrow\/Ipc\/ArrowReaderImplementation: Out of range exception when schema.GetFieldByIndex is called","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nIn some scenarios, the loop moves past the length of the schema index, causing an out of range exception when `schema.GetFieldByIndex` is called. In particular, this was observed when calling: \r\n\r\n```\r\nArrowFileReader reader = new ArrowFileReader(fs))\r\nint batches = reader.RecordBatchCountAsync().Result; \/\/ does not get the error on this call, even though the same BuildArrays call is made\r\n\r\nfor (int i = 0; i < batches; i++)\r\n{\r\n    RecordBatch recordBatch = reader.ReadNextRecordBatch();\r\n    \/\/ ...\r\n}\r\n``` \r\n\r\nThe error occurs on the first call to `ReadNextRecordBatch`\r\n\r\nThe specific stack trace is:\r\n\r\n``` \r\nStack Trace:\u2009\r\nList`1.get_Item(Int32 index)\r\nSchema.GetFieldByIndex(Int32 i)\u2009line\u200971\r\nArrowReaderImplementation.BuildArrays(MetadataVersion version, Schema schema, ByteBuffer messageBuffer, RecordBatch recordBatchMessage)\u2009line\u2009191\r\nArrowReaderImplementation.CreateArrowObjectFromMessage(Message message, ByteBuffer bodyByteBuffer, IMemoryOwner`1 memoryOwner)\u2009line\u2009123\r\nArrowStreamReaderImplementation.ReadMessage()\u2009line\u2009143\r\nArrowStreamReaderImplementation.ReadRecordBatch()\u2009line\u2009111\r\nArrowFileReaderImplementation.ReadRecordBatch(Int32 index)\u2009line\u2009176\r\nArrowFileReaderImplementation.ReadNextRecordBatch()\u2009line\u2009205\r\nArrowStreamReader.ReadNextRecordBatch()\u2009line\u2009107\r\nReplayableUtils.LoadRecordBatches(String location)\u2009line\u200979\r\nReplayableStatement.ExecuteQuery()\u2009line\u200962\r\nDriverTests.CanRun()\u2009line\u200967\r\n```\r\n\r\n### Component(s)\r\n\r\nC#","comments":["[Repro.zip](https:\/\/github.com\/apache\/arrow\/files\/14484870\/Repro.zip)\r\nHere is a simple repro, including an Arrow file with 5 record batches in it. "],"labels":["Type: bug","Component: C#"]},{"title":"[C++] Move fsspec FileSystem to a separate module","body":"### Describe the enhancement requested\n\nhttps:\/\/github.com\/apache\/arrow\/issues\/38309 adds the capability to register FileSystems not defined in libarrow. Python objects which satisfy fsspec are suitable for registration as such a filesystem.\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++] Move S3FileSystem to a separate module","body":"\r\n### Describe the enhancement requested\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/38309 adds the capability to build FileSystems separately. Apply this to S3FileSystem\r\n\r\n### Component(s)\r\n\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[C++] Move LocalFileSystem to a separate module","body":"### Describe the enhancement requested\n\nhttps:\/\/github.com\/apache\/arrow\/issues\/38309 adds the capability to build FileSystems separately. Apply this to LocalFileSystem\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"GH-40339: [Java] StringView Initial Implementation","body":"### Rationale for this change (DO NOT REVIEW: WORK IN PROGRESS)\r\n\r\nStringView implementation in Java. \r\n\r\n### What changes are included in this PR?\r\n\r\nN\/A\r\n\r\n### Are these changes tested?\r\n\r\nN\/A\r\n\r\n### Are there any user-facing changes?\r\n\r\nN\/A\r\n* GitHub Issue: #40339","comments":[],"labels":["Component: Java","awaiting review"]},{"title":"[Java] StringView Initial Implementation","body":"### Describe the enhancement requested\n\nStringView is already supported in C++, and providing support for Java users is important. Providing core StringView functionality via `ViewVarCharVector` and `ViewVarBinaryVector` is the objective. \n\n### Component(s)\n\nJava","comments":[],"labels":["Type: enhancement","Component: Java"]},{"title":"[Python][CI] Failing test_dateutil_tzinfo_to_string due to new release of python-dateutil","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\n`test_dateutil_tzinfo_to_string` started failing with:\r\n\r\n```python\r\n         tz = dateutil.tz.gettz('Europe\/Paris')\r\n>       assert pa.lib.tzinfo_to_string(tz) == 'Europe\/Paris'\r\nE       AssertionError: assert 'Europe\/Monaco' == 'Europe\/Paris'\r\nE         - Europe\/Paris\r\nE         + Europe\/Monaco\r\n```\r\n\r\nsee: https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8103088008\/job\/22164107523#step:6:4305.\r\n\r\nThis is most probably due to new release of `python-dateutil` package.\r\n\r\nThe test is failing on the CI now also, with dateutil `2.9.0`: https:\/\/ci.appveyor.com\/project\/ApacheSoftwareFoundation\/arrow\/builds\/49322144?fullLog=true\r\n\r\n### Component(s)\r\n\r\nContinuous Integration, Python","comments":["cc @raulcd ","I can't reproduce this on Linux, so most probably a Windows issue (or related to the exact tzdata being used there, since for me locally it's using the system provided data)\r\n\r\nMight be good to report upstream?","The logic we use to convert a dateutil tz to a string:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/2b194ad222f4dc8ecf2eb73539ab8cab5b1fc5e7\/python\/pyarrow\/src\/arrow\/python\/datetime.cc#L554-L568\r\n\r\n(so it might also be that something changed under the hood in dateutil that messes that u. It would be good to verify if the actual `tz` object in the snippet above, before conversion to a string by pyarrow, also shows \"Europe\/Monaco\")","Agree with you Joris. I would first like to check the behaviour of dateutil, as you mention, but also can't do that on my M1. I could in any case open an issue upstream before, if we think that makes sense.","The same issue is being reproduced on my PR on the conda feedstock _on Windows_:\r\nhttps:\/\/dev.azure.com\/conda-forge\/feedstock-builds\/_build\/results?buildId=888527&view=logs&j=ab98fee7-5bc6-5c2f-a410-3ab9b2f2e8ca&t=472408ae-fc68-5791-981c-69ea41d2d692&l=38399\r\n```\r\n    def test_dateutil_tzinfo_to_string():\r\n        pytest.importorskip(\"dateutil\")\r\n        import dateutil.tz\r\n    \r\n        tz = dateutil.tz.UTC\r\n        assert pa.lib.tzinfo_to_string(tz) == 'UTC'\r\n        tz = dateutil.tz.gettz('Europe\/Paris')\r\n>       assert pa.lib.tzinfo_to_string(tz) == 'Europe\/Paris'\r\nE       AssertionError: assert 'Europe\/Monaco' == 'Europe\/Paris'\r\nE         - Europe\/Paris\r\nE         + Europe\/Monaco\r\n```\r\nEdit: _added on Windows_","A temporary skip was added in https:\/\/github.com\/apache\/arrow\/pull\/40486"],"labels":["Type: bug","Component: Python","Component: Continuous Integration"]},{"title":"[Java] JDBC Flight SQL driver not connect  to individual endpoints specified by GetFlightInfo","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nFlightEndpoint returned by GetFlightInfoPreparedStatement is another flight server, but JDBC Flight SQL driver still fetch results from main flight server connection.\r\n\r\nversion: 15.0.0 release and 15.0.0 nightlies 2024.2.20\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/issues\/38785, In fact I found this problem in arrow 14.0.1, It was solved using 15.0.0 nightlies at that time, but today I tried again and found problem occurred again.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/38521 This PR seems to actually solve this problem.\r\n\r\n### Component(s)\r\n\r\nFlightRPC","comments":["Hmm, there were definitely other changes but looking at the core it should definitely be using all locations:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/2b194ad222f4dc8ecf2eb73539ab8cab5b1fc5e7\/java\/flight\/flight-sql-jdbc-core\/src\/main\/java\/org\/apache\/arrow\/driver\/jdbc\/client\/ArrowFlightSqlClientHandler.java#L111-L177","> Hmm, there were definitely other changes but looking at the core it should definitely be using all locations:\r\n\r\nThanks for your reply, i will debug this part of the code"],"labels":["Type: bug","Component: Java","Component: FlightRPC"]},{"title":"[R] Package Error: package arrow\/libs\/arrow.so: undefined symbol: _ZN5arrow7dataset14JsonFileFormatC1Ev","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nHello, I'm trying to build R package with a built Arrow using EasyBuild I get the following error:\r\n\r\n\r\n```R\r\n[1] \"C\"\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/home\/user\/R\/x86_64-pc-linux-gnu-library\/4.3\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  \/home\/user\/R\/x86_64-pc-linux-gnu-library\/4.3\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so: undefined symbol: _ZN5arrow7dataset14JsonFileFormatC1Ev\r\nError: loading failed\r\nExecution halted\r\nERROR: loading failed\r\n```\r\n\r\nEasyConfig file used: https:\/\/github.com\/easybuilders\/easybuild-easyconfigs\/blob\/develop\/easybuild\/easyconfigs\/a\/Arrow\/Arrow-14.0.1-gfbf-2023a.eb\r\n\r\nCan you help me to solve it ? Am i missing an option? Thanks\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nR","comments":["Could you show all install logs?","Sure! There it is :\r\n\r\n```R\r\nR version 4.3.3 (2024-02-29) -- \"Angel Food Cake\"\r\nCopyright (C) 2024 The R Foundation for Statistical Computing\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\n\r\nR is free software and comes with ABSOLUTELY NO WARRANTY.\r\nYou are welcome to redistribute it under certain conditions.\r\nType 'license()' or 'licence()' for distribution details.\r\n\r\n  Natural language support but running in an English locale\r\n\r\nR is a collaborative project with many contributors.\r\nType 'contributors()' for more information and\r\n'citation()' on how to cite R or R packages in publications.\r\n\r\nType 'demo()' for some demos, 'help()' for on-line help, or\r\n'help.start()' for an HTML browser interface to help.\r\nType 'q()' to quit R.\r\n\r\n[1] \"C\"\r\n[Previously saved workspace restored]\r\n\r\n> options(repos = c(CRAN_mirror = \"https:\/\/myrepo.com\/prod-cran\/latest\"))\r\n> install.packages('arrow')\r\nInstalling package into \u2018\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\u2019\r\n(as \u2018lib\u2019 is unspecified)\r\ntrying URL 'https:\/\/myrepo.com\/prod-cran\/latest\/src\/contrib\/arrow_14.0.2.1.tar.gz'\r\nContent type 'application\/x-gzip' length 4278652 bytes (4.1 MB)\r\n==================================================\r\ndownloaded 4.1 MB\r\n\r\n[1] \"C\"\r\n* installing *source* package \u2018arrow\u2019 ...\r\n** package \u2018arrow\u2019 successfully unpacked and MD5 sums checked\r\n** using staged installation\r\n*** pkg-config found.\r\n*** Trying Arrow C++ found by pkg-config: \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\r\n[1] \"C\"\r\n**** C++ library version 14.0.1 is supported by R version 14.0.2.1\r\nPKG_CFLAGS=-DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON\r\nPKG_LIBS=-L\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib   -larrow_dataset -lparquet -larrow_acero -larrow\r\n** libs\r\nusing C++ compiler: \u2018g++ (GCC) 12.2.0\u2019\r\nusing C++17\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c RTasks.cpp -o RTasks.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c altrep.cpp -o altrep.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c array.cpp -o array.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c array_to_vector.cpp -o array_to_vector.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c arraydata.cpp -o arraydata.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c arrowExports.cpp -o arrowExports.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c bridge.cpp -o bridge.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c buffer.cpp -o buffer.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c chunkedarray.cpp -o chunkedarray.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c compression.cpp -o compression.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c compute-exec.cpp -o compute-exec.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c compute.cpp -o compute.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c config.cpp -o config.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c csv.cpp -o csv.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c dataset.cpp -o dataset.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c datatype.cpp -o datatype.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c expression.cpp -o expression.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c extension-impl.cpp -o extension-impl.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c feather.cpp -o feather.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c field.cpp -o field.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c filesystem.cpp -o filesystem.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c io.cpp -o io.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c json.cpp -o json.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c memorypool.cpp -o memorypool.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c message.cpp -o message.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c parquet.cpp -o parquet.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c r_to_arrow.cpp -o r_to_arrow.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c recordbatch.cpp -o recordbatch.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c recordbatchreader.cpp -o recordbatchreader.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c recordbatchwriter.cpp -o recordbatchwriter.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c safe-call-into-r-impl.cpp -o safe-call-into-r-impl.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c scalar.cpp -o scalar.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c schema.cpp -o schema.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c symbols.cpp -o symbols.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c table.cpp -o table.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c threadpool.cpp -o threadpool.o\r\ng++ -std=gnu++17 -I\"\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/include\" -DNDEBUG -DUTF8PROC_EXPORTS -I\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/include -I\/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/include -I\/usr\/local\/include -I\/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include     -DARROW_R_WITH_PARQUET -DARROW_R_WITH_DATASET -DARROW_R_WITH_ACERO -DARROW_R_WITH_JSON -I'\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/cpp11\/include' -I\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/include -I\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/include -I\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/include -I\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Java\/11.0.20\/include -I\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/include -I\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/include -I\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/include -I\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/include -I\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas    -fpic  -O2 -ftree-vectorize -march=native -fno-math-errno  -c type_infer.cpp -o type_infer.o\r\ng++ -std=gnu++17 -shared -L\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/lib -L\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/OpenSSL\/1.1.1q-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libgit2\/1.5.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/MPFR\/4.2.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/lib64 -L\/home\/easybuild\/software\/GDAL\/3.6.2-foss-2022b\/lib -L\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/nodejs\/18.12.1-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/GLPK\/5.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/ImageMagick\/7.1.0-53-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/lib64 -L\/home\/easybuild\/software\/GSL\/2.7-GCC-12.2.0\/lib -L\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/UDUNITS\/2.2.28-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/lib64 -L\/home\/easybuild\/software\/HDF5\/1.14.0-gompi-2022b\/lib -L\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/ICU\/72.1-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libsndfile\/1.2.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/lib64 -L\/home\/easybuild\/software\/FFTW\/3.3.10-GCC-12.2.0\/lib -L\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/NLopt\/2.7.1-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/GMP\/6.2.1-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libxml2\/2.10.3-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/cURL\/7.86.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/Tk\/8.6.12-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/Java\/11.0.20\/lib64 -L\/home\/easybuild\/software\/Java\/11.0.20\/lib -L\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/LibTIFF\/4.4.0-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libjpeg-turbo\/2.1.4-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libpng\/1.6.38-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/PCRE2\/10.40-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/SQLite\/3.39.4-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/XZ\/5.2.7-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/ncurses\/6.3-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libreadline\/8.2-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/cairo\/1.17.4-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libGLU\/9.0.2-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/Mesa\/22.2.4-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/lib64 -L\/home\/easybuild\/software\/Arrow\/11.0.0-gfbf-2022b\/lib -L\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/HarfBuzz\/5.3.1-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/PostgreSQL\/15.2-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/lib64 -L\/home\/easybuild\/software\/JAGS\/4.3.2-foss-2022b\/lib -L\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/libsodium\/1.0.18-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/FriBidi\/1.0.12-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/lib64 -L\/home\/easybuild\/software\/unixODBC\/2.3.11-foss-2022b\/lib -L\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/lib64 -L\/home\/easybuild\/software\/wkhtmltopdf\/0.12.3-foss-2022b\/lib -L\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/X11\/20221110-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/Xvfb\/21.1.6-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/lib64 -L\/home\/easybuild\/software\/pkgconf\/1.9.3-GCCcore-12.2.0\/lib -L\/home\/easybuild\/software\/ScaLAPACK\/2.2.0-gompi-2022b-fb\/lib64 -L\/home\/easybuild\/software\/ScaLAPACK\/2.2.0-gompi-2022b-fb\/lib -L\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/lib64 -L\/home\/easybuild\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/lib -L\/home\/easybuild\/software\/GCCcore\/12.2.0\/lib64 -L\/home\/easybuild\/software\/GCCcore\/12.2.0\/lib -o arrow.so RTasks.o altrep.o array.o array_to_vector.o arraydata.o arrowExports.o bridge.o buffer.o chunkedarray.o compression.o compute-exec.o compute.o config.o csv.o dataset.o datatype.o expression.o extension-impl.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o r_to_arrow.o recordbatch.o recordbatchreader.o recordbatchwriter.o safe-call-into-r-impl.o scalar.o schema.o symbols.o table.o threadpool.o type_infer.o -L\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib -larrow_dataset -lparquet -larrow_acero -larrow -L\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/lib -lR\r\ninstalling to \/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/00LOCK-arrow\/00new\/arrow\/libs\r\n** R\r\n** inst\r\n** byte-compile and prepare package for lazy loading\r\n[1] \"C\"\r\n** help\r\n*** installing help indices\r\n** building package indices\r\n[1] \"C\"\r\n** testing if installed package can be loaded from temporary location\r\nlibgcc_s.so.1 must be installed for pthread_cancel to work\r\nsh: line 1:  8423 Aborted                 R_TESTS= '\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/bin\/R' --no-save --no-restore --no-echo 2>&1 < '\/tmp\/RtmpDfUEVQ\/file17ad435812d2'\r\n[1] \"C\"\r\nError: package or namespace load failed for \u2018arrow\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so':\r\n  \/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/00LOCK-arrow\/00new\/arrow\/libs\/arrow.so: undefined symbol: _ZN5arrow7dataset14JsonFileFormatC1Ev\r\nError: loading failed\r\nExecution halted\r\nERROR: loading failed\r\n* removing \u2018\/home\/myuser\/R\/x86_64-pc-linux-gnu-library\/4.3\/arrow\u2019\r\n\r\nThe downloaded source packages are in\r\n        \u2018\/tmp\/RtmpvY7CC8\/downloaded_packages\u2019\r\nWarning message:\r\nIn install.packages(\"arrow\") :\r\n  installation of package \u2018arrow\u2019 had non-zero exit status\r\n```","Could you also show `\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/cmake\/Arrow\/ArrowOptions.cmake`?","Sure!\r\n```\r\n# Options used to build arrow:\r\n\r\n## Compile and link options:\r\n### Compiler flags to append when compiling Arrow\r\nset(ARROW_CXXFLAGS \"\")\r\n### Build static libraries\r\nset(ARROW_BUILD_STATIC \"ON\")\r\n### Build shared libraries\r\nset(ARROW_BUILD_SHARED \"ON\")\r\n### Arbitrary string that identifies the kind of package\r\n### (for informational purposes)\r\nset(ARROW_PACKAGE_KIND \"\")\r\n### The Arrow git commit id (if any)\r\nset(ARROW_GIT_ID \"\")\r\n### The Arrow git commit description (if any)\r\nset(ARROW_GIT_DESCRIPTION \"\")\r\n### Exclude deprecated APIs from build\r\nset(ARROW_NO_DEPRECATED_API \"OFF\")\r\n### Whether to create position-independent target\r\nset(ARROW_POSITION_INDEPENDENT_CODE \"ON\")\r\n### Use ccache when compiling (if available)\r\nset(ARROW_USE_CCACHE \"ON\")\r\n### Use sccache when compiling (if available),\r\n### takes precedence over ccache if a storage backend is configured\r\nset(ARROW_USE_SCCACHE \"ON\")\r\n### Use ld.gold for linking on Linux (if available)\r\nset(ARROW_USE_LD_GOLD \"OFF\")\r\n### Use precompiled headers when compiling\r\nset(ARROW_USE_PRECOMPILED_HEADERS \"OFF\")\r\n### Compile-time SIMD optimization level\r\nset(ARROW_SIMD_LEVEL \"SSE4_2\")\r\n### Max runtime SIMD optimization level\r\nset(ARROW_RUNTIME_SIMD_LEVEL \"MAX\")\r\n### Build with Altivec if compiler has support\r\nset(ARROW_ALTIVEC \"ON\")\r\n### Build Arrow libraries with RATH set to $ORIGIN\r\nset(ARROW_RPATH_ORIGIN \"OFF\")\r\n### Build Arrow libraries with install_name set to @rpath\r\nset(ARROW_INSTALL_NAME_RPATH \"ON\")\r\n### Pass -ggdb flag to debug builds\r\nset(ARROW_GGDB_DEBUG \"ON\")\r\n### Whether the system libc is musl or not\r\nset(ARROW_WITH_MUSL \"OFF\")\r\n### Enable threading in Arrow core\r\nset(ARROW_ENABLE_THREADING \"ON\")\r\n\r\n## Test and benchmark options:\r\n### Build the Arrow examples\r\nset(ARROW_BUILD_EXAMPLES \"OFF\")\r\n### Build the Arrow googletest unit tests\r\nset(ARROW_BUILD_TESTS \"OFF\")\r\n### Enable timing-sensitive tests\r\nset(ARROW_ENABLE_TIMING_TESTS \"ON\")\r\n### Build the Arrow integration test executables\r\nset(ARROW_BUILD_INTEGRATION \"OFF\")\r\n### Build the Arrow micro benchmarks\r\nset(ARROW_BUILD_BENCHMARKS \"OFF\")\r\n### Build the Arrow micro reference benchmarks\r\nset(ARROW_BUILD_BENCHMARKS_REFERENCE \"OFF\")\r\n### Build the Arrow benchmarks that rely on OpenMP\r\nset(ARROW_BUILD_OPENMP_BENCHMARKS \"OFF\")\r\n### Build benchmarks that do a longer exploration of performance\r\nset(ARROW_BUILD_DETAILED_BENCHMARKS \"OFF\")\r\n### Linkage of Arrow libraries with unit tests executables.\r\nset(ARROW_TEST_LINKAGE \"shared\")\r\n### Build Arrow Fuzzing executables\r\nset(ARROW_FUZZING \"OFF\")\r\n### Enable unit tests which use large memory\r\nset(ARROW_LARGE_MEMORY_TESTS \"OFF\")\r\n\r\n## Lint options:\r\n### Only define the lint and check-format targets\r\nset(ARROW_ONLY_LINT \"OFF\")\r\n### If off, 'quiet' flags will be passed to linting tools\r\nset(ARROW_VERBOSE_LINT \"OFF\")\r\n### Build with C++ code coverage enabled\r\nset(ARROW_GENERATE_COVERAGE \"OFF\")\r\n\r\n## Checks options:\r\n### Run the test suite using valgrind --tool=memcheck\r\nset(ARROW_TEST_MEMCHECK \"OFF\")\r\n### Enable Address Sanitizer checks\r\nset(ARROW_USE_ASAN \"OFF\")\r\n### Enable Thread Sanitizer checks\r\nset(ARROW_USE_TSAN \"OFF\")\r\n### Enable Undefined Behavior sanitizer checks\r\nset(ARROW_USE_UBSAN \"OFF\")\r\n\r\n## Project component options:\r\n### Build the Arrow Acero Engine Module\r\nset(ARROW_ACERO \"ON\")\r\n### Build Arrow with Azure support (requires the Azure SDK for C++)\r\nset(ARROW_AZURE \"OFF\")\r\n### Build Arrow commandline utilities\r\nset(ARROW_BUILD_UTILITIES \"OFF\")\r\n### Build all Arrow Compute kernels\r\nset(ARROW_COMPUTE \"ON\")\r\n### Build the Arrow CSV Parser Module\r\nset(ARROW_CSV \"ON\")\r\n### Build the Arrow CUDA extensions (requires CUDA toolkit)\r\nset(ARROW_CUDA \"OFF\")\r\n### Build the Arrow Dataset Modules\r\nset(ARROW_DATASET \"ON\")\r\n### Build the Arrow Filesystem Layer\r\nset(ARROW_FILESYSTEM \"ON\")\r\n### Build the Arrow Flight RPC System (requires GRPC, Protocol Buffers)\r\nset(ARROW_FLIGHT \"OFF\")\r\n### Build the Arrow Flight SQL extension\r\nset(ARROW_FLIGHT_SQL \"OFF\")\r\n### Build the Gandiva libraries\r\nset(ARROW_GANDIVA \"OFF\")\r\n### Build Arrow with GCS support (requires the GCloud SDK for C++)\r\nset(ARROW_GCS \"OFF\")\r\n### Build the Arrow HDFS bridge\r\nset(ARROW_HDFS \"ON\")\r\n### Build the Arrow IPC extensions\r\nset(ARROW_IPC \"ON\")\r\n### Build the Arrow jemalloc-based allocator\r\nset(ARROW_JEMALLOC \"ON\")\r\n### Build Arrow with JSON support (requires RapidJSON)\r\nset(ARROW_JSON \"ON\")\r\n### Build the Arrow mimalloc-based allocator\r\nset(ARROW_MIMALLOC \"OFF\")\r\n### Build the Parquet libraries\r\nset(ARROW_PARQUET \"ON\")\r\n### Build the Arrow ORC adapter\r\nset(ARROW_ORC \"OFF\")\r\n### Build some components needed by PyArrow.\r\n### (This is a deprecated option. Use CMake presets instead.)\r\nset(ARROW_PYTHON \"on\")\r\n### Build Arrow with S3 support (requires the AWS SDK for C++)\r\nset(ARROW_S3 \"OFF\")\r\n### Build the Skyhook libraries\r\nset(ARROW_SKYHOOK \"OFF\")\r\n### Build the Arrow Substrait Consumer Module\r\nset(ARROW_SUBSTRAIT \"OFF\")\r\n### Build Arrow with TensorFlow support enabled\r\nset(ARROW_TENSORFLOW \"OFF\")\r\n### Build the Arrow testing libraries\r\nset(ARROW_TESTING \"OFF\")\r\n\r\n## Thirdparty toolchain options:\r\n### Method to use for acquiring arrow's build dependencies\r\nset(ARROW_DEPENDENCY_SOURCE \"AUTO\")\r\n### Show output from ExternalProjects rather than just logging to files\r\nset(ARROW_VERBOSE_THIRDPARTY_BUILD \"OFF\")\r\n### Link to shared libraries\r\nset(ARROW_DEPENDENCY_USE_SHARED \"ON\")\r\n### Rely on Boost shared libraries where relevant\r\nset(ARROW_BOOST_USE_SHARED \"ON\")\r\n### Rely on Brotli shared libraries where relevant\r\nset(ARROW_BROTLI_USE_SHARED \"ON\")\r\n### Rely on Bz2 shared libraries where relevant\r\nset(ARROW_BZ2_USE_SHARED \"ON\")\r\n### Rely on GFlags shared libraries where relevant\r\nset(ARROW_GFLAGS_USE_SHARED \"ON\")\r\n### Rely on gRPC shared libraries where relevant\r\nset(ARROW_GRPC_USE_SHARED \"ON\")\r\n### Rely on jemalloc shared libraries where relevant\r\nset(ARROW_JEMALLOC_USE_SHARED \"OFF\")\r\n### Rely on LLVM shared libraries where relevant\r\nset(ARROW_LLVM_USE_SHARED \"ON\")\r\n### Rely on lz4 shared libraries where relevant\r\nset(ARROW_LZ4_USE_SHARED \"ON\")\r\n### Rely on OpenSSL shared libraries where relevant\r\nset(ARROW_OPENSSL_USE_SHARED \"ON\")\r\n### Rely on Protocol Buffers shared libraries where relevant\r\nset(ARROW_PROTOBUF_USE_SHARED \"ON\")\r\n### Rely on snappy shared libraries where relevant\r\nset(ARROW_SNAPPY_USE_SHARED \"ON\")\r\n### Rely on thrift shared libraries where relevant\r\nset(ARROW_THRIFT_USE_SHARED \"ON\")\r\n### Rely on utf8proc shared libraries where relevant\r\nset(ARROW_UTF8PROC_USE_SHARED \"ON\")\r\n### Rely on zstd shared libraries where relevant\r\nset(ARROW_ZSTD_USE_SHARED \"ON\")\r\n### Build libraries with glog support for pluggable logging\r\nset(ARROW_USE_GLOG \"OFF\")\r\n### Build with backtrace support\r\nset(ARROW_WITH_BACKTRACE \"ON\")\r\n### Build libraries with OpenTelemetry support for distributed tracing\r\nset(ARROW_WITH_OPENTELEMETRY \"OFF\")\r\n### Build with Brotli compression\r\nset(ARROW_WITH_BROTLI \"OFF\")\r\n### Build with BZ2 compression\r\nset(ARROW_WITH_BZ2 \"ON\")\r\n### Build with lz4 compression\r\nset(ARROW_WITH_LZ4 \"ON\")\r\n### Build with Snappy compression\r\nset(ARROW_WITH_SNAPPY \"ON\")\r\n### Build with zlib compression\r\nset(ARROW_WITH_ZLIB \"ON\")\r\n### Build with zstd compression\r\nset(ARROW_WITH_ZSTD \"ON\")\r\n### Build with UCX transport for Arrow Flight\r\n### (only used if ARROW_FLIGHT is ON)\r\nset(ARROW_WITH_UCX \"OFF\")\r\n### Build with support for Unicode properties using the utf8proc library\r\n### (only used if ARROW_COMPUTE is ON or ARROW_GANDIVA is ON)\r\nset(ARROW_WITH_UTF8PROC \"ON\")\r\n### Build with support for regular expressions using the re2 library\r\n### (only used if ARROW_COMPUTE or ARROW_GANDIVA is ON)\r\nset(ARROW_WITH_RE2 \"ON\")\r\n\r\n## Parquet options:\r\n### Depend only on Thirdparty headers to build libparquet.\r\n### Always OFF if building binaries\r\nset(PARQUET_MINIMAL_DEPENDENCY \"OFF\")\r\n### Build the Parquet executable CLI tools. Requires static libraries to be built.\r\nset(PARQUET_BUILD_EXECUTABLES \"OFF\")\r\n### Build the Parquet examples. Requires static libraries to be built.\r\nset(PARQUET_BUILD_EXAMPLES \"OFF\")\r\n### Build support for encryption. Fail if OpenSSL is not found\r\nset(PARQUET_REQUIRE_ENCRYPTION \"OFF\")\r\n\r\n## Gandiva options:\r\n### Include -static-libstdc++ -static-libgcc when linking with\r\n### Gandiva static libraries\r\nset(ARROW_GANDIVA_STATIC_LIBSTDCPP \"OFF\")\r\n### Compiler flags to append when pre-compiling Gandiva operations\r\nset(ARROW_GANDIVA_PC_CXX_FLAGS \"\")\r\n\r\n## Advanced developer options:\r\n### Compile with extra error context (line numbers, code)\r\nset(ARROW_EXTRA_ERROR_CONTEXT \"OFF\")\r\n### If enabled install ONLY targets that have already been built. Please be\r\n### advised that if this is enabled 'install' will fail silently on components\r\n### that have not been built\r\nset(ARROW_OPTIONAL_INSTALL \"OFF\")\r\n### Use a custom install directory for GDB plugin.\r\n### In general, you don't need to specify this because the default\r\n### (CMAKE_INSTALL_FULL_BINDIR on Windows, CMAKE_INSTALL_FULL_LIBDIR otherwise)\r\n### is reasonable.\r\nset(ARROW_GDB_INSTALL_DIR \"\/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\")\r\n```\r\n","Could you show `ldd \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/libarrow_dataset.so`?","There it is:\r\n```\r\nldd \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/libarrow_dataset.so\r\n\r\n        linux-vdso.so.1 =>  (0x00007ff0d9a2a000)\r\n        libparquet.so.1400 => \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/libparquet.so.1400 (0x00007ff0d9496000)\r\n        libarrow_acero.so.1400 => \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/libarrow_acero.so.1400 (0x00007ff0d92ff000)\r\n        libarrow.so.1400 => \/home\/easybuild\/software\/Arrow\/14.0.1-gfbf-2022b\/lib\/libarrow.so.1400 (0x00007ff0d790e000)\r\n        libutf8proc.so.2 => \/home\/easybuild\/software\/utf8proc\/2.8.0-GCCcore-12.2.0\/lib\/libutf8proc.so.2 (0x00007ff0d78b7000)\r\n        libdl.so.2 => \/lib64\/libdl.so.2 (0x00007ff0d76b3000)\r\n        librt.so.1 => \/lib64\/librt.so.1 (0x00007ff0d74ab000)\r\n        libstdc++.so.6 => \/home\/easybuild\/software\/GCCcore\/12.2.0\/lib64\/libstdc++.so.6 (0x00007ff0d7288000)\r\n        libm.so.6 => \/lib64\/libm.so.6 (0x00007ff0d6f86000)\r\n        libgcc_s.so.1 => \/home\/easybuild\/software\/GCCcore\/12.2.0\/lib64\/libgcc_s.so.1 (0x00007ff0d6f65000)\r\n        libpthread.so.0 => \/lib64\/libpthread.so.0 (0x00007ff0d6d49000)\r\n        libc.so.6 => \/lib64\/libc.so.6 (0x00007ff0d697b000)\r\n        \/lib64\/ld-linux-x86-64.so.2 (0x00007ff0d980b000)\r\n        libbz2.so.1.0 => \/home\/easybuild\/software\/bzip2\/1.0.8-GCCcore-12.2.0\/lib\/libbz2.so.1.0 (0x00007ff0d6966000)\r\n        liblz4.so.1 => \/home\/easybuild\/software\/lz4\/1.9.4-GCCcore-12.2.0\/lib\/liblz4.so.1 (0x00007ff0d6933000)\r\n        libsnappy.so.1 => \/home\/easybuild\/software\/snappy\/1.1.9-GCCcore-12.2.0\/lib\/libsnappy.so.1 (0x00007ff0d6926000)\r\n        libz.so.1 => \/home\/easybuild\/software\/zlib\/1.2.12-GCCcore-12.2.0\/lib\/libz.so.1 (0x00007ff0d690b000)\r\n        libzstd.so.1 => \/home\/easybuild\/software\/zstd\/1.5.2-GCCcore-12.2.0\/lib\/libzstd.so.1 (0x00007ff0d6859000)\r\n```","```text\r\n        libgcc_s.so.1 => \/home\/easybuild\/software\/GCCcore\/12.2.0\/lib64\/libgcc_s.so.1 (0x00007ff0d6f65000)\r\n```\r\n\r\nThis may be a problem.\r\n\r\nIt seems that your `libgcc_s.so.1` has a problem:\r\n\r\n```text\r\nlibgcc_s.so.1 must be installed for pthread_cancel to work\r\nsh: line 1:  8423 Aborted                 R_TESTS= '\/home\/easybuild\/software\/R\/4.3.3-foss-2022b\/lib64\/R\/bin\/R' --no-save --no-restore --no-echo 2>&1 < '\/tmp\/RtmpDfUEVQ\/file17ad435812d2'\r\n```\r\n\r\nCould you check it?","Thanks for your diagnostic, going to work on it :) "],"labels":["Type: bug","Component: R"]},{"title":"[Python][Parquet] element_type is nullable despite being non-null","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI have a column where each element is an array of floats (vector embeddings). Those floats are never null. I need to persist this column in this exact format but pyarrow will always set the inner element_type to `optional`. FastParquet seems to have an option `has_nulls=False` for similar purposes but FastParquet does not seem to be able to write nested array types at all.\r\n\r\nThis is the pyarrow schema type of the column:\r\n```\r\n ('embedding', pyarrow.list_(pyarrow.float32(), list_size=1024, False),\r\n```\r\nAnd this is the resulting parquet schema\r\n```\r\n  required group field_id=-1 embedding (List) {\r\n    repeated group field_id=-1 list {\r\n      optional float field_id=-1 element;\r\n    }\r\n  }\r\n```\r\n\r\nNo matter what I try, I don't find a way to set the `optional` qualifier of the inner element to `required`. For my purposes, I need this exact schema and it seems problematic that pyarrow cannot create it despite the pyarrow documentation explicitly acknowledging that the inner element type may be either `required` or `optional`.\n\n### Component(s)\n\nPython","comments":["using `pyarrow.field` and set nullable with `pyarrow.float32()`, I think This should be set to non-nullable.","@mapleFU Thanks, that seems to work. But it seems very strange to use a Field inside a list. Fields should be part of a struct, right? When I use them inside a list, I *have* to provide a field name which at the same time is ignored.","Hmm this can be a workaround\ud83e\udd14 I'm not familiar with Python, the list is defined as below:\r\n\r\n```c++\r\nclass ARROW_EXPORT ListType : public BaseListType {\r\n public:\r\n  static constexpr Type::type type_id = Type::LIST;\r\n  using offset_type = int32_t;\r\n\r\n  static constexpr const char* type_name() { return \"list\"; }\r\n\r\n  \/\/ List can contain any other logical value type\r\n  explicit ListType(const std::shared_ptr<DataType>& value_type)\r\n      : ListType(std::make_shared<Field>(\"item\", value_type)) {}\r\n```\r\n\r\nIt contains a `Field` here, so I think it's ok, but I agree it's a bit weird for user. But actually I'm not familiar with how to create in convinient... Maybe other can help"],"labels":["Type: bug","Component: Python"]},{"title":"[Python] Creating an array from a null list scalar fails","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nCurrently pyarrow fails to construct an array from a null list scalar:\r\n```\r\n>>> import pyarrow as pa\r\n>>> pa.__version__\r\n'15.0.0'\r\n>>> pa.array([pa.scalar(None, pa.list_(pa.int64()))])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow\/array.pxi\", line 344, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow\/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118\r\n  File \"pyarrow\/scalar.pxi\", line 685, in pyarrow.lib.ListScalar.__len__\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\nThis may be worked around by directly constructing the array and specifying the type when this case is detected:\r\n```\r\n>>> pa.array([None], type=pa.list_(pa.int64()))\r\n<pyarrow.lib.ListArray object at 0x7f1d8e37f160>\r\n[\r\n  null\r\n]\r\n```\n\n### Component(s)\n\nPython","comments":["Thanks for reporting @vyasr!\r\n\r\nI am actually getting a segfault when running the code locally. Unfortunately lldb is not giving me much extra information than this:\r\n\r\n```python\r\nIn [15]: pa.array([pa.scalar(None, pa.list_(pa.int64()))])\r\n\/Users\/alenkafrim\/repos\/arrow\/python\/pyarrow\/src\/arrow\/python\/python_to_arrow.cc:532:  Check failed: (size) >= (offset) \r\nzsh: abort      ipython\r\n```\r\n\r\n(from lldb)\r\n```\r\n\/Users\/alenkafrim\/repos\/arrow\/python\/pyarrow\/src\/arrow\/python\/python_to_arrow.cc:532:  Check failed: (size) >= (offset) \r\n```\r\n\r\nSo the issue in my case happens in this part of the code:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/ee1a8c39a55f3543a82fed900dadca791f6e9f88\/python\/pyarrow\/src\/arrow\/python\/python_to_arrow.cc#L532"],"labels":["Type: bug","Component: Python"]},{"title":"GH-40314: [R] Return plan generated when calling write_dataset  ","body":"### Rationale for this change\r\n\r\n\r\n\r\n### What changes are included in this PR?\r\n\r\n\r\n### Are these changes tested?\r\n\r\n\r\n### Are there any user-facing changes?\r\n\r\n\n* GitHub Issue: #40314","comments":[],"labels":["Component: R","awaiting committer review"]},{"title":"[R] Return plan generated when calling write_dataset","body":"### Describe the enhancement requested\n\nReturn the generated ExecPlan; this will make it easier to debug \n\n### Component(s)\n\nR","comments":[],"labels":["Type: enhancement","Component: R"]},{"title":"GH-37720: [Go][FlightSQL] Implement stateless prepared statements","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nSee discussion on https:\/\/github.com\/apache\/arrow\/issues\/37720 and mailing list: https:\/\/lists.apache.org\/thread\/3kb82ypx99q96g84qv555l6x8r0bppyq\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\nChanges the Go FlightSQL client and server implementations to support returning an updated prepared statement handle to the client as part of the `DoPut(PreparedStatement)` RPC call.\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nSee parent issue and docs PR #40243  for details of user facing changes.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n**This PR includes breaking changes to public APIs.**\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #37720","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n","I mentioned [here](https:\/\/github.com\/apache\/arrow-rs\/pull\/5433#discussion_r1511392897) in the Rust PR that I would like to expand the Go tests to cover both the stateful and stateless cases, but I didn't spend much time learning how the Go test suite is structured, and I am less familiar with Go as a language in general.\r\n\r\nI will take a look at it this week, but would appreciate recommendations of where to look from anyone.","@erratic-pattern thanks for doing this. As far as how the tests are structured, there's three primary files specific to flightsql:\r\n\r\n- `client_test.go`: contains a mocked `GrpcClientStream` object and a mock `FlightServiceClient`  object to isolate the FlightSQL logic. Everything is part of the `FlightSqlClientSuite` test object, so adding a new test is just adding a new method named using the pattern `TestXXXXX()` and it will automatically get picked up. You can look at `SetupTest` and `TearDownTest` for the setup and teardown which currently just consist of setting up the mocks and asserting the expectations happen. \r\n- `server_test.go`: You can look at `FlightSqlServerSuite` and its methods for the basic test suite that starts up a flight server using the `testServer` object in that file. Adding proper tests there would require simply adding any needed functionality to the `testServer` methods and corresponding test methods in the `FlightSqlServerSuite` and the `FlightSqlServerSessionSuite` if its relevant to test the Session based interactions too.\r\n- `sqlite_server_test.go` is a test suite which just tests the example SQLite FlightSQL server. You could add the stateless prepared statement support to the sqlite server in the `example` subdir, and then test it in this file.\r\n\r\nHope that helps! If you want me to look closely at this while it's still a draft let me know, otherwise I'll wait until you mark it as `ready for review`\r\n\r\n","@zeroshade A review would be much appreciated. The PR is in draft state until the spec is voted on, but the code change is more or less \"Done\"","@erratic-pattern in general this looks okay, but the build needs to be fixed as I'm seeing \r\n\r\n```\r\n# github.com\/apache\/arrow\/go\/v16\/arrow\/flight\/flightsql\r\nError: flight\\flightsql\\server.go:979:16: undefined: flight.DoPutPreparedStatementResult\r\n```\r\n\r\nIn the CI","I must have forgotten to commit the protobuf generated code in this branch. I've run `go generate` locally on [the format PR](https:\/\/github.com\/apache\/arrow\/pull\/40243) and committed the changes to this PR. ","@zeroshade  Not sure how to interpret this CI failure. It seems to involve a Java client integration test. Can you take a look? https:\/\/github.com\/apache\/arrow\/actions\/runs\/8352550401\/job\/22934920078?pr=40311 \r\n"],"labels":["Component: Go","awaiting review"]},{"title":"[R] How to convert the timestamp without time zone type to the POSIXct type ideally?","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThe POSIXct type in R seems to behave in the absence of time zone information in such a way that it is interpreted as the time zone time in which the R session is running.\r\n\r\nHowever, the arrow package does not consider the time zone of the R session when converting from the Arrow timestamp type to the POSIXct type.\r\n\r\n``` r\r\nlibrary(arrow)\r\n#>\r\n#> Attaching package: 'arrow'\r\n#> The following object is masked from 'package:utils':\r\n#>\r\n#>     timestamp\r\n\r\nSys.setenv(TZ = \"UTC\")\r\n\r\nx <- \"1900-01-01\"\r\na_a <- chunked_array(as.POSIXct(x), type = timestamp())\r\na_a\r\n#> ChunkedArray\r\n#> <timestamp[s]>\r\n#> [\r\n#>   [\r\n#>     1900-01-01 00:00:00\r\n#>   ]\r\n#> ]\r\n\r\nout <- as.vector(a_a)\r\n\r\nas.POSIXct(x)\r\n#> [1] \"1900-01-01 UTC\"\r\nunclass(as.POSIXct(x))\r\n#> [1] -2208988800\r\n#> attr(,\"tzone\")\r\n#> [1] \"\"\r\nout\r\n#> [1] \"1900-01-01 UTC\"\r\nunclass(out)\r\n#> [1] -2208988800\r\n\r\nSys.setenv(TZ = \"Europe\/Paris\")\r\n\r\nout <- as.vector(a_a)\r\n\r\nas.POSIXct(x)\r\n#> [1] \"1900-01-01 PMT\"\r\nunclass(as.POSIXct(x))\r\n#> [1] -2208989361\r\n#> attr(,\"tzone\")\r\n#> [1] \"\"\r\nout\r\n#> [1] \"1900-01-01 00:09:21 PMT\"\r\nunclass(out)\r\n#> [1] -2208988800\r\n```\r\n\r\n<sup>Created on 2024-03-01 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\nIs there a reason why this behavior was chosen? Or is this a bug and should be fixed? (which is, of course, a breaking change).\r\n\r\nI found this behavior in this issue (pola-rs\/r-polars#875)\r\n\r\n### Component(s)\r\n\r\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"[C++][Gandiva] A question about Gandiva's GetResultType for DecimalType","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nBelow codes calculate the decimal type's precision and scale.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/30e6d72242e376baa598b2e8f1d9b80d800a974c\/cpp\/src\/gandiva\/decimal_type_util.cc#L62\r\n\r\nIt seems different from our **Redshift\u2019s decimal promotion rules**, is there any relevant background here?\r\n```c\r\nscale = max(4, s1 + p2 - s2 + 1)\r\nprecision = p1 - s1 + s2 + scale\r\n```\r\nIt may cause the decimal divide results different from normal expression calculate.\r\n\r\n### Component(s)\r\n\r\nC++ - Gandiva","comments":["@pravindra Could you take a look at this?","I think it's a problem and we should fix it.\r\nBelow codes could prove this problem:\r\n```c\r\n\/\/ Normal expression\r\nTEST(DecimalTest, Divide) {\r\n  auto expr = arrow::compute::call(\"divide\", {arrow::compute::field_ref(\"f1\"),\r\n                                              arrow::compute::field_ref(\"f2\")});\r\n  auto s = arrow::schema({arrow::field(\"f1\", arrow::decimal128(11, 3)),\r\n                          arrow::field(\"f2\", arrow::decimal128(20, 9))});\r\n  ASSERT_OK_AND_ASSIGN(expr, expr.Bind(*s));\r\n  arrow::AssertTypeEqual(*expr.type(), *arrow::decimal128(32, 15));\r\n\r\n  auto b = arrow::RecordBatchFromJSON(s, R\"([\r\n    [\"1.982\", \"1.000000001\"]\r\n   ])\");\r\n  ASSERT_OK_AND_ASSIGN(auto input, arrow::compute::MakeExecBatch(*s, b));\r\n  ASSERT_OK_AND_ASSIGN(auto res,\r\n                       arrow::compute::ExecuteScalarExpression(expr, input));\r\n  arrow::AssertArraysEqual(\r\n      *res.make_array(),\r\n      *arrow::ArrayFromJSON(arrow::decimal128(32, 15), R\"([\"1.981999998018000\"])\"));\r\n}\r\n\r\n\/\/ gandiva test\r\nTEST_F(TestDecimalOps, TestDivide) {\r\n  DivideAndVerify(decimal_literal(\"1982\", 11, 3),\r\n                  decimal_literal(\"1000000001\", 20, 9),\r\n                  decimal_literal(\"1981999998018000001982\", 38, 21));\r\n}\r\n```\r\n\r\nWe will have different precision and scale for the same data.","> Below codes could prove this problem:\r\n\r\nI'm not sure what is the important part in the codes. Could you explain it?\r\n\r\n> our Redshift\u2019s decimal promotion rules\r\n\r\nCould you show the URL that describes the rule?\r\n\r\n> is there any relevant background here?\r\n\r\nBased on the `kMinAdjustedScale` comment, it seems that the current precision doesn't have strong reason but it's compatible with SQLServer and Impala:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/6b1e254f3b62924f216e06e9e563e92c69f9efd3\/cpp\/src\/gandiva\/decimal_type_util.h#L54-L58\r\n","> I'm not sure what is the important part in the codes. Could you explain it?\r\n\r\nThe test codes are the decimal divide operation in compute expression and gandiva expression.\r\nWe input the save decimal type (11, 3), (20, 9) with same input arr-data, but the results are different.\r\n\r\nFor compute expression, divide result's decimal type is (32,15); but gandiva expression's result is (38, 21).\r\n\r\n> Could you show the URL that describes the rule?\r\n\r\nBelow rules is our compute expression's rules:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/9f0a28f61a068059a6e53f25a3ffcb1689d701ec\/cpp\/src\/arrow\/compute\/kernels\/codegen_internal.cc#L421\r\n\r\n\r\nAnd the compute doc:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/4fe364efa4be98b35964509e0e3d57a421a48039\/docs\/source\/cpp\/compute.rst#L501-L520.\r\n\r\n\r\n\r\n> Based on the `kMinAdjustedScale` comment, it seems that the current precision doesn't have strong reason but it's compatible with SQLServer and Impala:\r\n\r\nYes, but maybe we should unify these rules between compute expression and gandiva expression. Or the results will be different when user do optimize expression with gandiva?","Ah, I understand. You referred our compute module's behavior by \"our Redshift\u2019s decimal promotion rules\". I thought that you refer Redshift's rules not our compute module's one.\r\n\r\nI don't object that Gandiva can also use the rules but it breaks backward compatibility. Can we support both rules and use the current rule by default for compatibility?","> I thought that you refer Redshift's rules not our compute module's one.\r\n\r\nActually our compute module's decimal rules is the same with Redshift's rules. Compute module implement the rules by two step:\r\n1.  Implicit cast. The logic is in `CastBinaryDecimalArgs`.\r\n2. Output resolve. The logic is in `ResolveDecimalDivisionOutput`.\r\n\r\nAll these two steps are in expression `Bind`, and form the rules same with `Redshift's numeric rules`.\r\n\r\n\r\n\r\n\r\n> I don't object that Gandiva can also use the rules but it breaks backward compatibility. Can we support both rules and use the current rule by default for compatibility?\r\n\r\nAgree.","@kou Added the compatibility in the pr. PTAL?"],"labels":["Type: bug","Component: C++ - Gandiva"]},{"title":"`pyarrow.fs.HadoopFileSystem` throws OSError: Unable to load libhdfs","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI'm trying to create an HDFS Connection via `pyarrow.fs.HadoopFileSystem`, but unfortunately I get an error:\r\n\r\n```{python}\r\nfrom pyarrow.fs import HadoopFileSystem\r\nhdfs = HadoopFileSystem(\r\n    host=\"localhost\",\r\n    port=8001,\r\n)\r\n```\r\n> OSError: Unable to load libhdfs: Das angegebene Modul wurde nicht gefunden.\r\n\r\nFrom https:\/\/arrow.apache.org\/docs\/python\/filesystems.html#hadoop-distributed-file-system-hdfs I understand that `libhdfs.so` should be located in `%HADOOP_HOME%lib\/native\/`, which is the case. I also set the `CLASSPATH` environment variable to `%HADOOP_HOME%\/bin\/hadoop`. \r\n\r\nWhat am I missing?\r\n\r\nI use pyarrow==15.0.0.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[R]: Unnamed columns cause issues when used in dplyr queries ","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nWhen using the defaults for `write.csv()` the rownames are written and the column header is ``. If we were to read this in with R we would get a column name of X since an empty string is not a valid column name. But Arrow doesn't do this, and presents an empty string, which then causes issues with `rlang:::env_bind0`\r\n\r\n\r\n```\r\n> write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\r\n> open_dataset(\"matcars.csv\", format = \"csv\") |> mutate(gear_one = gear + 1) |> collect()\r\nError: IOError: Cannot list directory 'matcars.csv'. Detail: [errno 2] No such file or directory\r\n> write.csv(mtcars, \"mtcars.csv\")\r\n> open_dataset(\"mtcars.csv\", format = \"csv\") |> mutate(gear_one = gear + 1) |> collect()\r\nError in env_bind0(env, data) : attempt to use zero-length variable name\r\n> open_dataset(\"mtcars.csv\", format = \"csv\") |> collect()\r\n# A tibble: 32 \u00d7 12\r\n   ``            mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\r\n   <chr>       <dbl> <int> <dbl> <int> <dbl> <dbl> <dbl> <int> <int> <int> <int>\r\n 1 Mazda RX4    21       6  160    110  3.9   2.62  16.5     0     1     4     4\r\n 2 Mazda RX4 \u2026  21       6  160    110  3.9   2.88  17.0     0     1     4     4\r\n 3 Datsun 710   22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\r\n 4 Hornet 4 D\u2026  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\r\n 5 Hornet Spo\u2026  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\r\n 6 Valiant      18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\r\n 7 Duster 360   14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\r\n 8 Merc 240D    24.4     4  147.    62  3.69  3.19  20       1     0     4     2\r\n 9 Merc 230     22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\r\n10 Merc 280     19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\r\n# \u2139 22 more rows\r\n# \u2139 Use `print(n = ...)` to see more rows\r\n```\r\n\r\nOr, if we specifically select all but the column that is the empty string, it works:\r\n```\r\n> open_dataset(\"mtcars.csv\", format = \"csv\") |> select(mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb) |> mutate(gear_one = gear + 1) |> collect()\r\n# A tibble: 32 \u00d7 12\r\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb gear_one\r\n   <dbl> <int> <dbl> <int> <dbl> <dbl> <dbl> <int> <int> <int> <int>    <int>\r\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4        5\r\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4        5\r\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1        5\r\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1        4\r\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2        4\r\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1        4\r\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4        4\r\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2        5\r\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2        5\r\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4        5\r\n# \u2139 22 more rows\r\n# \u2139 Use `print(n = ...)` to see more rows\r\n```\r\n\r\nBut, if we don't write the un-named column, this works just fine:\r\n```\r\n> write.csv(mtcars, \"mtcars.csv\", row.names = FALSE)\r\n> open_dataset(\"mtcars.csv\", format = \"csv\") |> mutate(gear_one = gear + 1) |> collect()\r\n# A tibble: 32 \u00d7 12\r\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb gear_one\r\n   <dbl> <int> <dbl> <int> <dbl> <dbl> <dbl> <int> <int> <int> <int>    <int>\r\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4        5\r\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4        5\r\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1        5\r\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1        4\r\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2        4\r\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1        4\r\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4        4\r\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2        5\r\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2        5\r\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4        5\r\n# \u2139 22 more rows\r\n# \u2139 Use `print(n = ...)` to see more rows\r\n```\n\n### Component(s)\n\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"[C#]  How to serialize POCOs to a Table?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nArrow seems like a great system for working with data _once you already have some Arrow-format data to work with,_ but trying to reach that point seems needlessly complicated.  For a system dedicated to batch processing of data, you'd expect that one of the most fundamental operations would be to take a list of POCO data objects and serialize it to a `Table`.  But if there's any serializer in the `Apache.Arrow` library, or even any helpers that would be useful for building one, I can't find them anywhere.\r\n\r\nWhat's the recommended way to serialize a list of objects to a `Table`?\n\n### Component(s)\n\nC#","comments":[],"labels":["Component: C#","Type: usage"]},{"title":"[Python] Only convert in parallel for the ConsolidatedBlockCreator class for large data","body":"### Describe the enhancement requested\n\nThe [Consolidated Block Creator](https:\/\/github.com\/apache\/arrow\/blob\/a6e577d031d20a1a7d3dd01536b9a77db5d1bff8\/python\/pyarrow\/src\/arrow\/python\/arrow_to_pandas.cc#L2276) runs the column conversion in parallel, creating a [Scalar Memo Table](https:\/\/github.com\/apache\/arrow\/blob\/a6e577d031d20a1a7d3dd01536b9a77db5d1bff8\/python\/pyarrow\/src\/arrow\/python\/arrow_to_pandas.cc#L609) for each column up until `pa.cpu_count()`. \r\n\r\nFor performance reasons, [jemalloc](https:\/\/engineering.fb.com\/2011\/01\/03\/core-infra\/scalable-memory-allocation-using-jemalloc\/) and [mimalloc](https:\/\/github.com\/microsoft\/mimalloc\/issues\/351) maintain allocations on a per-memory segment level to reduce contention between threads. \r\n\r\nWhat this means is that if a user calls `table.to_pandas(split_blocks=False)` for a small table, a disproportionately large amount of memory gets allocated to build the `Scalar Memo Table`. Both `jemalloc` and `mimalloc` will essentially allocate a chunk of memory per column.\r\n\r\nHere is some code:\r\n\r\n```\r\nimport pyarrow as pa\r\n\r\ndef test_memory_usage():\r\n    table = pa.table({'A': 'a', 'B': 'b', 'C': 'c', 'D': 'd', 'E': 'e', 'F': 'f', 'G': 'g'}) #'H':'h', 'I': 'i', 'J': 'j', 'K':'k', 'L':'l', 'M':'m', 'N':'n', 'O':'o', 'P':'p', 'Q':'q', 'R':'r', 'S':'s', 'T':'t', 'U':'u', 'V':'v', 'W':'w', 'X':'x','Y':'y','Z':'z'})\r\n    df = table.to_pandas()\r\n\r\nif __name__ == '__main__':\r\n    test_memory_usage()\r\n```\r\n\r\nHere are the resulting memory allocations summarised with [memray](https:\/\/github.com\/bloomberg\/memray):\r\n\r\njemalloc with 7 columns:\r\n\r\n```\r\n\ud83d\udce6 Total memory allocated:\r\n        178.127MB\r\n\r\n\ud83d\udcca Histogram of allocation size:\r\n        min: 1.000B\r\n        --------------------------------------------\r\n        < 4.000B   :  3548 \u2587\u2587\r\n        < 24.000B  :  1253 \u2587\r\n        < 119.000B : 14686 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 588.000B :  3746 \u2587\u2587\r\n        < 2.827KB  : 58959 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 13.924KB :  2550 \u2587\u2587\r\n        < 68.569KB :   403 \u2587\r\n        < 337.661KB:    84 \u2587\r\n        < 1.624MB  :    24 \u2587\r\n        <=7.996MB  :    20 \u2587\r\n        --------------------------------------------\r\n        max: 7.996MB\r\n```\r\n\r\njemalloc with 26 columns:\r\n\r\n```\r\n\ud83d\udce6 Total memory allocated:\r\n        238.229MB\r\n\r\n\ud83d\udcca Histogram of allocation size:\r\n        min: 1.000B\r\n        --------------------------------------------\r\n        < 4.000B   :  3545 \u2587\u2587\r\n        < 24.000B  :  1627 \u2587\r\n        < 119.000B : 15086 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 588.000B :  3882 \u2587\u2587\r\n        < 2.828KB  : 58971 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 13.929KB :  2552 \u2587\u2587\r\n        < 68.593KB :   403 \u2587\r\n        < 337.794KB:    84 \u2587\r\n        < 1.625MB  :    24 \u2587\r\n        <=8.000MB  :    47 \u2587\r\n        --------------------------------------------\r\n        max: 8.000MB\r\n```\r\n\r\nmimalloc with 7 columns:\r\n\r\n```\r\n\ud83d\udce6 Total memory allocated:\r\n        380.166MB\r\n\r\n\ud83d\udcca Histogram of allocation size:\r\n        min: 1.000B\r\n        --------------------------------------------\r\n        < 6.000B   :  3548 \u2587\u2587\r\n        < 36.000B  :  7470 \u2587\u2587\u2587\u2587\r\n        < 222.000B :  9524 \u2587\u2587\u2587\u2587\u2587\r\n        < 1.319KB  : 57271 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 7.999KB  :  6492 \u2587\u2587\u2587\r\n        < 48.503KB :   775 \u2587\r\n        < 294.066KB:   150 \u2587\r\n        < 1.741MB  :    26 \u2587\r\n        < 10.556MB :     1 \u2587\r\n        <=64.000MB :     4 \u2587\r\n        --------------------------------------------\r\n        max: 64.000MB\r\n```\r\n\r\nmimalloc with 26 columns:\r\n\r\n```\r\n\ud83d\udce6 Total memory allocated:\r\n        1.434GB\r\n\r\n\ud83d\udcca Histogram of allocation size:\r\n        min: 1.000B\r\n        --------------------------------------------\r\n        < 6.000B   :  3545 \u2587\u2587\r\n        < 36.000B  :  7845 \u2587\u2587\u2587\u2587\r\n        < 222.000B : 10001 \u2587\u2587\u2587\u2587\u2587\r\n        < 1.319KB  : 57332 \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\r\n        < 7.999KB  :  6501 \u2587\u2587\u2587\r\n        < 48.503KB :   794 \u2587\r\n        < 294.066KB:   150 \u2587\r\n        < 1.741MB  :    26 \u2587\r\n        < 10.556MB :     1 \u2587\r\n        <=64.000MB :    21 \u2587\r\n        --------------------------------------------\r\n        max: 64.000MB\r\n```\r\n\r\nYou can see how dramatically the memory increases even for a very small table.\r\n\r\nMy proposal is that we only do the conversion in parallel when it might make a substantial performance difference for a table of a certain size. I'm not quite sure which size, but once the code has been refactored, we can run experiments to come to a data-informed decision. \n\n### Component(s)\n\nC++","comments":["I\u2019ve taken a look at the code.\r\n\r\n`OptionalParallelFor` is used when `PandasOptions::use_threads` is set. Internally, it uses the default CPU thread pool which can allocate CPU-count threads at a time. That\u2019s why it allocates CPU-count threads as @anjakefala described.\r\n\r\nMy first solution\/idea: add another option to `PandasOptions` \u2014 `int threads`. Then if that\u2019s different than 0, create a different `ThreadPool` with that as capacity when calling the parallel-for.\r\n\r\nEasy, but it\u2019s an extra option that users have to remember to set.\r\n\r\n### Alternative\r\n\r\nAdd more parameters to parallel-for(columns) that modify the loop that submits tasks to the thread-pool:\r\n\r\nStart with a low minimum number of submitted tasks (eg 2. each column is a task). ~After that, tasks are added with a small delay (microseconds) as to give the chance of the initially submitted tasks to finish (because cols are small) and make these initially used threads to be re-used.~\r\n\r\nThis might require internal `ThreadPool` changes to avoid waits that wouldn\u2019t lead to a global reduction in time taken to convert all columns.\r\n\r\nEDIT: the elegant solution is **work-stealing**. Now you know why work-stealing was invented.","@felipecrv Is modifying `ThreadPool` better than an option where we use an approach similar to [the SplitBlockCreator class](https:\/\/github.com\/apache\/arrow\/blob\/a6e577d031d20a1a7d3dd01536b9a77db5d1bff8\/python\/pyarrow\/src\/arrow\/python\/arrow_to_pandas.cc#L2422) for tables under a certain size? That's more along the line of what I was thinking of. \r\n\r\nHowever, if you think `work-stealing` would be the most robust solution, that other functions would benefit from, I'd be game for approaching this. \r\n\r\nI prefer the work-stealing approach because, ideally, we wouldn't require the user to know about the existence of an option to set. Folks might not know that the memory usage has to do with the spawning of individual threads. They might not even know why `to_pandas` spawns multiple threads. "],"labels":["Type: enhancement","Component: Python"]},{"title":"GH-40279: [C++] Reduce S3Client initialization time","body":"### Rationale for this change\r\n\r\nBy default, S3Client instantiation is extremely slow (around 1ms for every instance). Investigation led to the conclusion that most of this time was spent inside the AWS SDK, parsing a hardcoded piece of JSON data when instantiating a AWS rule engine.\r\n\r\nPython benchmarks show this repeated initiatlization cost:\r\n```python\r\n>>> from pyarrow.fs import S3FileSystem\r\n\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 21.1 ms, sys: 0 ns, total: 21.1 ms\r\nWall time: 20.9 ms\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 2.37 ms, sys: 0 ns, total: 2.37 ms\r\nWall time: 2.18 ms\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 2.42 ms, sys: 0 ns, total: 2.42 ms\r\nWall time: 2.23 ms\r\n\r\n>>> %timeit s = S3FileSystem()\r\n1.28 ms \u00b1 4.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n>>> %timeit s = S3FileSystem()\r\n1.28 ms \u00b1 2.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n>>> %timeit s = S3FileSystem(anonymous=True)\r\n1.26 ms \u00b1 2.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```\r\n\r\n### What changes are included in this PR?\r\n\r\nInstead of letting the AWS SDK create a new S3EndpointProvider for each S3Client, arrange to only create a single S3EndpointProvider per set of endpoint configuration options. This lets the 1ms instantiation cost be paid only when a new set of endpoint configuration options is given.\r\n\r\nPython benchmarks show the initialization cost has become a one-time cost:\r\n```python\r\n>>> from pyarrow.fs import S3FileSystem\r\n\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\r\nWall time: 19.8 ms\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 404 \u00b5s, sys: 49 \u00b5s, total: 453 \u00b5s\r\nWall time: 266 \u00b5s\r\n>>> %time s = S3FileSystem()\r\nCPU times: user 361 \u00b5s, sys: 42 \u00b5s, total: 403 \u00b5s\r\nWall time: 249 \u00b5s\r\n\r\n>>> %timeit s = S3FileSystem()\r\n50.4 \u00b5s \u00b1 227 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n>>> %timeit s = S3FileSystem(anonymous=True)\r\n33.5 \u00b5s \u00b1 306 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n```\r\n\r\n### Are these changes tested?\r\n\r\nBy existing tests.\r\n\r\n### Are there any user-facing changes?\r\n\r\nNo.\r\n* GitHub Issue: #40279","comments":["@github-actions crossbow submit -g cpp -g python -g wheel","Revision: 35c040f089f84578fcb8032ce6a04525fa86ffbb\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-4a39636abd](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-4a39636abd)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100682745\/job\/22139225171)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100682926\/job\/22139225539)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684589\/job\/22139232086)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4a39636abd-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22139228903)|\n|test-conda-python-3.10|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684485\/job\/22139231781)|\n|test-conda-python-3.10-cython2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-cython2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685288\/job\/22139270576)|\n|test-conda-python-3.10-hdfs-2.9.2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-hdfs-2.9.2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100682921\/job\/22139225441)|\n|test-conda-python-3.10-hdfs-3.2.1|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-hdfs-3.2.1)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685292\/job\/22139270552)|\n|test-conda-python-3.10-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684692\/job\/22139232502)|\n|test-conda-python-3.10-pandas-nightly|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-pandas-nightly)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683486\/job\/22139227252)|\n|test-conda-python-3.10-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684978\/job\/22139269749)|\n|test-conda-python-3.10-substrait|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.10-substrait)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684282\/job\/22139230842)|\n|test-conda-python-3.11|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684982\/job\/22139270008)|\n|test-conda-python-3.11-dask-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11-dask-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684743\/job\/22139268997)|\n|test-conda-python-3.11-dask-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11-dask-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685015\/job\/22139270286)|\n|test-conda-python-3.11-hypothesis|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11-hypothesis)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684319\/job\/22139231082)|\n|test-conda-python-3.11-pandas-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11-pandas-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684950\/job\/22139270048)|\n|test-conda-python-3.11-spark-master|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.11-spark-master)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684626\/job\/22139232219)|\n|test-conda-python-3.12|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.12)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683489\/job\/22139227571)|\n|test-conda-python-3.8|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.8)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683119\/job\/22139226060)|\n|test-conda-python-3.8-pandas-1.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.8-pandas-1.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683064\/job\/22139225947)|\n|test-conda-python-3.8-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.8-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684660\/job\/22139232402)|\n|test-conda-python-3.9|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.9)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684529\/job\/22139231989)|\n|test-conda-python-3.9-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-conda-python-3.9-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684280\/job\/22139231010)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685118\/job\/22139270451)|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683635\/job\/22139228389)|\n|test-debian-11-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-debian-11-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684029\/job\/22139230025)|\n|test-debian-11-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-debian-11-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683270\/job\/22139226615)|\n|test-debian-11-python-3-amd64|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4a39636abd-azure-test-debian-11-python-3-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22139232725)|\n|test-debian-11-python-3-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-debian-11-python-3-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684904\/job\/22139269420)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683620\/job\/22139228262)|\n|test-fedora-39-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4a39636abd-azure-test-fedora-39-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22139229809)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684321\/job\/22139231091)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684363\/job\/22139231392)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684896\/job\/22139269381)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683280\/job\/22139226625)|\n|test-ubuntu-20.04-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-4a39636abd-azure-test-ubuntu-20.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22139232129)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683532\/job\/22139227699)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684610\/job\/22139232305)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683080\/job\/22139225914)|\n|test-ubuntu-22.04-python-3|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-test-ubuntu-22.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684968\/job\/22139269851)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683756\/job\/22139229044)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683366\/job\/22139226784)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683625\/job\/22139228314)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683461\/job\/22139227184)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684173\/job\/22139230438)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683178\/job\/22139226252)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684510\/job\/22139231780)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685405\/job\/22139270735)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684466\/job\/22139231768)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683929\/job\/22139229556)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683926\/job\/22139229689)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683782\/job\/22139229181)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684077\/job\/22139230257)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684939\/job\/22139270234)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683749\/job\/22139228938)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100685076\/job\/22139270267)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100682740\/job\/22139225217)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684286\/job\/22139230804)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684758\/job\/22139268987)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684735\/job\/22139268954)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683954\/job\/22139229670)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684522\/job\/22139232075)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100682959\/job\/22139225475)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683902\/job\/22139229497)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683604\/job\/22139228179)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683193\/job\/22139226474)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684015\/job\/22139229963)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683741\/job\/22139228722)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683672\/job\/22139228513)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683189\/job\/22139226246)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683533\/job\/22139227784)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683419\/job\/22139226931)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100684390\/job\/22139231464)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683394\/job\/22139226915)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-4a39636abd-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8100683566\/job\/22139227902)|","@github-actions crossbow submit -g cpp -g wheel","Revision: 3ba9dca382f1a752020414e0b8a7a1aefc03a53b\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-f123b375dd](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-f123b375dd)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526503\/job\/22141908599)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526411\/job\/22141907812)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526756\/job\/22141910069)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-f123b375dd-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22141914033)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526508\/job\/22141908727)|\n|test-debian-11-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-debian-11-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527113\/job\/22141912458)|\n|test-debian-11-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-debian-11-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526801\/job\/22141910828)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527038\/job\/22141912432)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527208\/job\/22141912709)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526994\/job\/22141911696)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527501\/job\/22141913778)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526687\/job\/22141909248)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526657\/job\/22141909263)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526874\/job\/22141911068)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527500\/job\/22141913771)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526161\/job\/22141906715)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526987\/job\/22141911479)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526296\/job\/22141907567)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526685\/job\/22141909489)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527367\/job\/22141913619)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526867\/job\/22141910874)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526372\/job\/22141907781)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526803\/job\/22141910810)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526370\/job\/22141908352)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526784\/job\/22141910097)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527503\/job\/22141913845)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526966\/job\/22141911462)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527145\/job\/22141912484)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527155\/job\/22141912688)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526197\/job\/22141906809)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526316\/job\/22141907844)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527734\/job\/22141914556)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526164\/job\/22141906751)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526832\/job\/22141910838)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526407\/job\/22141908030)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527146\/job\/22141912481)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527329\/job\/22141913578)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526274\/job\/22141907461)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526242\/job\/22141907241)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526439\/job\/22141908578)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527484\/job\/22141913764)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526945\/job\/22141911484)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526244\/job\/22141907257)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526930\/job\/22141911482)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526224\/job\/22141906948)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526528\/job\/22141908912)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527011\/job\/22141911749)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526472\/job\/22141908802)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101526632\/job\/22141909253)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-f123b375dd-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101527192\/job\/22141912694)|","Oh, it looks like the RTools 40 build is using a very old AWS SDK version (1.7.365):\r\n```\r\n2024-02-29T19:29:45.9968248Z ==> Making package: mingw-w64-arrow 15.0.0.9000-8000 (Thu, Feb 29, 2024  7:29:45 PM)\r\n2024-02-29T19:29:46.0104530Z ==> Checking runtime dependencies...\r\n2024-02-29T19:29:46.0933579Z ==> Installing missing dependencies...\r\n2024-02-29T19:29:46.1681983Z resolving dependencies...\r\n2024-02-29T19:29:46.1816658Z looking for conflicting packages...\r\n2024-02-29T19:29:46.1842088Z \r\n2024-02-29T19:29:46.1846702Z Packages (12) mingw-w64-ucrt-x86_64-boost-1.67.0-9002  mingw-w64-ucrt-x86_64-libssh2-1.11.0-9801  mingw-w64-ucrt-x86_64-nghttp2-1.51.0-1  mingw-w64-ucrt-x86_64-openssl-3.1.1-9800  mingw-w64-ucrt-x86_64-aws-sdk-cpp-1.7.365-1  mingw-w64-ucrt-x86_64-brotli-1.0.9-4  mingw-w64-ucrt-x86_64-curl-8.1.2-9000  mingw-w64-ucrt-x86_64-libutf8proc-2.4.0-2  mingw-w64-ucrt-x86_64-lz4-1.8.2-1  mingw-w64-ucrt-x86_64-re2-20200801-1  mingw-w64-ucrt-x86_64-snappy-1.1.7-2  mingw-w64-ucrt-x86_64-thrift-0.13.0-1\r\n```\r\n\r\nDo we know why that is @paleolimbot @jonkeane @assignUser  ?","@github-actions crossbow submit -g cpp -g wheel","Revision: 54f0a3453ad801ef38a4df01f1543288dc91536f\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-b72df011bb](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-b72df011bb)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956851\/job\/22143370001)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956133\/job\/22143366790)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956060\/job\/22143366590)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-b72df011bb-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22143374499)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957329\/job\/22143372334)|\n|test-debian-11-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-debian-11-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956559\/job\/22143368884)|\n|test-debian-11-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-debian-11-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957185\/job\/22143371809)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957453\/job\/22143372976)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956216\/job\/22143367231)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957060\/job\/22143371086)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956986\/job\/22143370670)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957885\/job\/22143373710)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956293\/job\/22143367766)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957065\/job\/22143371094)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957024\/job\/22143371757)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956853\/job\/22143370173)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956377\/job\/22143367848)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957226\/job\/22143372076)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101955785\/job\/22143365376)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956209\/job\/22143367214)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957072\/job\/22143371304)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956246\/job\/22143367297)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101955772\/job\/22143365315)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957472\/job\/22143372982)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956373\/job\/22143368014)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957501\/job\/22143373089)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956467\/job\/22143368474)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956664\/job\/22143369387)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101955776\/job\/22143365385)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956396\/job\/22143368236)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956520\/job\/22143368815)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956974\/job\/22143370494)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957447\/job\/22143372671)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956638\/job\/22143369281)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956903\/job\/22143370343)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101955884\/job\/22143365853)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957129\/job\/22143371335)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956656\/job\/22143369443)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956127\/job\/22143367014)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957143\/job\/22143371325)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101955897\/job\/22143365888)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956415\/job\/22143368175)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956348\/job\/22143367859)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956580\/job\/22143368957)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957409\/job\/22143372595)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956394\/job\/22143368195)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956098\/job\/22143366760)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957945\/job\/22143373647)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101956152\/job\/22143366844)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-b72df011bb-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8101957003\/job\/22143370600)|","@github-actions crossbow submit -g cpp -g python -g wheel","Revision: 2be51947448aac17da5eb4e7b284483da72f7f41\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-14c6d115c7](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-14c6d115c7)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830439\/job\/22146157425)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829508\/job\/22146151767)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829771\/job\/22146152526)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-14c6d115c7-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22146156787)|\n|test-conda-python-3.10|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829890\/job\/22146153191)|\n|test-conda-python-3.10-cython2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-cython2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830199\/job\/22146155833)|\n|test-conda-python-3.10-hdfs-2.9.2|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-hdfs-2.9.2)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830282\/job\/22146156642)|\n|test-conda-python-3.10-hdfs-3.2.1|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-hdfs-3.2.1)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829100\/job\/22146150179)|\n|test-conda-python-3.10-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829961\/job\/22146153498)|\n|test-conda-python-3.10-pandas-nightly|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-pandas-nightly)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829481\/job\/22146151543)|\n|test-conda-python-3.10-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830532\/job\/22146157600)|\n|test-conda-python-3.10-substrait|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.10-substrait)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830205\/job\/22146155843)|\n|test-conda-python-3.11|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829263\/job\/22146150826)|\n|test-conda-python-3.11-dask-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11-dask-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830455\/job\/22146157417)|\n|test-conda-python-3.11-dask-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11-dask-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829324\/job\/22146150990)|\n|test-conda-python-3.11-hypothesis|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11-hypothesis)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830158\/job\/22146155024)|\n|test-conda-python-3.11-pandas-upstream_devel|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11-pandas-upstream_devel)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830699\/job\/22146158964)|\n|test-conda-python-3.11-spark-master|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.11-spark-master)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829983\/job\/22146154868)|\n|test-conda-python-3.12|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.12)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829432\/job\/22146151502)|\n|test-conda-python-3.8|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.8)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829986\/job\/22146153902)|\n|test-conda-python-3.8-pandas-1.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.8-pandas-1.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830011\/job\/22146154111)|\n|test-conda-python-3.8-spark-v3.5.0|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.8-spark-v3.5.0)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830376\/job\/22146157231)|\n|test-conda-python-3.9|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.9)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830689\/job\/22146158945)|\n|test-conda-python-3.9-pandas-latest|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-conda-python-3.9-pandas-latest)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830649\/job\/22146158888)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829779\/job\/22146152807)|\n|test-cuda-python|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-cuda-python)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830221\/job\/22146155884)|\n|test-debian-11-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-debian-11-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829768\/job\/22146152826)|\n|test-debian-11-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-debian-11-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830092\/job\/22146154876)|\n|test-debian-11-python-3-amd64|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-14c6d115c7-azure-test-debian-11-python-3-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22146155858)|\n|test-debian-11-python-3-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-debian-11-python-3-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830615\/job\/22146157893)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829561\/job\/22146152111)|\n|test-fedora-39-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-14c6d115c7-azure-test-fedora-39-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22146153689)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830545\/job\/22146157611)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830341\/job\/22146156673)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829760\/job\/22146152823)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830519\/job\/22146157447)|\n|test-ubuntu-20.04-python-3|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-14c6d115c7-azure-test-ubuntu-20.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22146154459)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830038\/job\/22146154142)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830141\/job\/22146155016)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830577\/job\/22146158738)|\n|test-ubuntu-22.04-python-3|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-test-ubuntu-22.04-python-3)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829189\/job\/22146150564)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830098\/job\/22146155022)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830670\/job\/22146158931)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829894\/job\/22146153156)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830704\/job\/22146159079)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830169\/job\/22146155187)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829822\/job\/22146152534)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829471\/job\/22146151801)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830029\/job\/22146154103)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830408\/job\/22146157244)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830492\/job\/22146157470)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829419\/job\/22146151406)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830781\/job\/22146187715)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830720\/job\/22146187647)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829290\/job\/22146150862)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829970\/job\/22146154861)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830617\/job\/22146158783)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830037\/job\/22146154869)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829160\/job\/22146150245)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830155\/job\/22146155034)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829166\/job\/22146150221)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830592\/job\/22146157672)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830842\/job\/22146187826)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829931\/job\/22146153601)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829625\/job\/22146152336)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830551\/job\/22146157695)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830598\/job\/22146157803)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830196\/job\/22146155834)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830800\/job\/22146187651)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830745\/job\/22146187645)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829465\/job\/22146151533)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829946\/job\/22146153559)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830343\/job\/22146157218)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829591\/job\/22146152283)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102830232\/job\/22146156516)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-14c6d115c7-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8102829235\/job\/22146150764)|","> Oh, it looks like the RTools 40 build is using a very old AWS SDK version (1.7.365):\r\n\r\nIt seems that RTools 40 still uses old MSYS2.","@github-actions crossbow submit -g cpp -g wheel","Revision: 09dc9982fc9a7d6446df96a85a18e2079f717e98\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-0ed3ff1b7a](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-0ed3ff1b7a)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551953\/job\/22667011208)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552848\/job\/22667012732)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550775\/job\/22666987832)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-0ed3ff1b7a-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22666991875)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551223\/job\/22666989905)|\n|test-debian-12-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-debian-12-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551175\/job\/22666989871)|\n|test-debian-12-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-debian-12-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551468\/job\/22666990676)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551495\/job\/22666991012)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550922\/job\/22666988222)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552773\/job\/22667012783)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552325\/job\/22667012234)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552435\/job\/22667012479)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550621\/job\/22666987262)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550611\/job\/22666987388)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550955\/job\/22666988336)|\n|test-ubuntu-24.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-24.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550606\/job\/22666987283)|\n|test-ubuntu-24.04-cpp-gcc-14|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-test-ubuntu-24.04-cpp-gcc-14)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551022\/job\/22666988949)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552692\/job\/22667012546)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552519\/job\/22667012531)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551221\/job\/22666989898)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551765\/job\/22667010631)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550986\/job\/22666988558)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551432\/job\/22666990655)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551673\/job\/22667001863)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551850\/job\/22667010688)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551753\/job\/22667010629)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550912\/job\/22666988129)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551944\/job\/22667010970)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551367\/job\/22666990132)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551183\/job\/22666989876)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552127\/job\/22667012212)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550983\/job\/22666988937)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551516\/job\/22667000788)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551923\/job\/22667010928)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551921\/job\/22667010906)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551580\/job\/22667000808)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551427\/job\/22666990699)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552112\/job\/22667012229)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552976\/job\/22667012782)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551507\/job\/22666990724)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550345\/job\/22666986143)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552121\/job\/22667011941)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551650\/job\/22667001848)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552408\/job\/22667012283)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551840\/job\/22667010647)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283552441\/job\/22667012484)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551091\/job\/22666989354)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283550573\/job\/22666987096)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551067\/job\/22666989304)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551085\/job\/22666989353)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551554\/job\/22667000779)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-0ed3ff1b7a-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8283551910\/job\/22667010927)|","@github-actions crossbow submit -g cpp -g wheel","Revision: 6b770c00b4ff80e2717e312cf53c1c901b6ba0d9\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-8946df4af2](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-8946df4af2)\n\n|Task|Status|\n|----|------|\n|test-alpine-linux-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-alpine-linux-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891315\/job\/22944536869)|\n|test-build-cpp-fuzz|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-build-cpp-fuzz)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890178\/job\/22944530635)|\n|test-conda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-conda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890963\/job\/22944534598)|\n|test-conda-cpp-valgrind|[![Azure](https:\/\/dev.azure.com\/ursacomputing\/crossbow\/_apis\/build\/status\/ursacomputing.crossbow?branchName=actions-8946df4af2-azure-test-conda-cpp-valgrind)](https:\/\/github.com\/ursacomputing\/crossbow\/runs\/22944531994)|\n|test-cuda-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-cuda-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891533\/job\/22944537254)|\n|test-debian-12-cpp-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-debian-12-cpp-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891178\/job\/22944535666)|\n|test-debian-12-cpp-i386|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-debian-12-cpp-i386)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891044\/job\/22944534862)|\n|test-fedora-39-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-fedora-39-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891564\/job\/22944537232)|\n|test-ubuntu-20.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-20.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889852\/job\/22944529538)|\n|test-ubuntu-20.04-cpp-bundled|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-20.04-cpp-bundled)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890516\/job\/22944531712)|\n|test-ubuntu-20.04-cpp-minimal-with-formats|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-20.04-cpp-minimal-with-formats)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890709\/job\/22944532235)|\n|test-ubuntu-20.04-cpp-thread-sanitizer|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-20.04-cpp-thread-sanitizer)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889755\/job\/22944528452)|\n|test-ubuntu-22.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-22.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889677\/job\/22944527785)|\n|test-ubuntu-22.04-cpp-20|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-22.04-cpp-20)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890528\/job\/22944531883)|\n|test-ubuntu-22.04-cpp-no-threading|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-22.04-cpp-no-threading)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890079\/job\/22944529994)|\n|test-ubuntu-24.04-cpp|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-24.04-cpp)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889710\/job\/22944527992)|\n|test-ubuntu-24.04-cpp-gcc-14|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-test-ubuntu-24.04-cpp-gcc-14)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889553\/job\/22944527665)|\n|wheel-macos-big-sur-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-big-sur-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889740\/job\/22944528480)|\n|wheel-macos-big-sur-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-big-sur-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891465\/job\/22944536949)|\n|wheel-macos-big-sur-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-big-sur-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890813\/job\/22944533119)|\n|wheel-macos-big-sur-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-big-sur-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891161\/job\/22944535669)|\n|wheel-macos-big-sur-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-big-sur-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891176\/job\/22944535942)|\n|wheel-macos-catalina-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-catalina-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889892\/job\/22944529105)|\n|wheel-macos-catalina-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-catalina-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890600\/job\/22944531946)|\n|wheel-macos-catalina-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-catalina-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891006\/job\/22944534655)|\n|wheel-macos-catalina-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-catalina-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891785\/job\/22944537528)|\n|wheel-macos-catalina-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-macos-catalina-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891185\/job\/22944535964)|\n|wheel-manylinux-2-28-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890719\/job\/22944532099)|\n|wheel-manylinux-2-28-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891597\/job\/22944537286)|\n|wheel-manylinux-2-28-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891786\/job\/22944537642)|\n|wheel-manylinux-2-28-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889595\/job\/22944527511)|\n|wheel-manylinux-2-28-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890215\/job\/22944530645)|\n|wheel-manylinux-2-28-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889878\/job\/22944528944)|\n|wheel-manylinux-2-28-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891016\/job\/22944534691)|\n|wheel-manylinux-2-28-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891235\/job\/22944536086)|\n|wheel-manylinux-2-28-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889581\/job\/22944527508)|\n|wheel-manylinux-2-28-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2-28-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891536\/job\/22944537247)|\n|wheel-manylinux-2014-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889939\/job\/22944529030)|\n|wheel-manylinux-2014-cp310-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp310-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890912\/job\/22944533130)|\n|wheel-manylinux-2014-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891804\/job\/22944537612)|\n|wheel-manylinux-2014-cp311-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp311-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890057\/job\/22944530066)|\n|wheel-manylinux-2014-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890806\/job\/22944533120)|\n|wheel-manylinux-2014-cp312-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp312-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890941\/job\/22944533114)|\n|wheel-manylinux-2014-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891940\/job\/22944538367)|\n|wheel-manylinux-2014-cp38-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp38-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890324\/job\/22944531090)|\n|wheel-manylinux-2014-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891381\/job\/22944536899)|\n|wheel-manylinux-2014-cp39-arm64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-manylinux-2014-cp39-arm64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890706\/job\/22944531941)|\n|wheel-windows-cp310-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-windows-cp310-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890366\/job\/22944531583)|\n|wheel-windows-cp311-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-windows-cp311-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378890043\/job\/22944529976)|\n|wheel-windows-cp312-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-windows-cp312-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891414\/job\/22944536931)|\n|wheel-windows-cp38-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-windows-cp38-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378889875\/job\/22944528976)|\n|wheel-windows-cp39-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8946df4af2-github-wheel-windows-cp39-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8378891706\/job\/22944537523)|",":warning: GitHub issue #40279 **has been automatically assigned in GitHub** to PR creator.","The R failures are resolved by #40710"],"labels":["Component: C++","awaiting merge"]},{"title":"[C++] Add TensorFromJSON helper function","body":"### Describe the enhancement requested\n\nTo make tests easier to write and read, we should create a `TensorFromJSON()` helper function. See https:\/\/github.com\/apache\/arrow\/pull\/40064#discussion_r1507890850\n\n### Component(s)\n\nC++","comments":["It'd be nice to also be able to provide strides or dimension permutation as input."],"labels":["Type: enhancement","Component: C++","good-first-issue"]},{"title":"[Dev] Remove implicit workflow transitions","body":"### Describe the enhancement requested\n\nCurrently, some workflow transitions (using labels such as \"awaiting review\") are implicitly triggered on various events, such as posting a comment on a PR. This is both useless and confusing, because it does not mirror the actual state of a PR (you can post a comment without it being a review, for example).\r\n\r\nWe should disable those implicit transitions. Later we can implement explicit transitions, using dedicated user actions.\r\n\n\n### Component(s)\n\nDeveloper Tools","comments":["Thoughts @jorisvandenbossche @felipecrv @kou @js8544 ?","PR labels are indeed very confusing sometimes. But I'm not sure what would explicit transitions look like? Could you give an example?","It looks like this, though it seems this bot is actually slightly broken:\r\nhttps:\/\/github.com\/python\/cpython\/pull\/115989#issuecomment-1966656381\r\n","I'm not using these labels. (I just ignore these labels.) So I don't have a strong opinion for this.\r\nI'm using \"new comment\" email notifications as review\/answer triggers.","I never look at and never use the labels. My workflow is opening the \"Participating\" and \"Mentioned\" filters, and scanning from top to bottom for items with recent updates."],"labels":["Type: enhancement","Component: Developer Tools"]},{"title":"[Java][ListView] Implementation of the ListView","body":"### Describe the enhancement requested\n\nC++, Go, and Python all support ListView. The goal of this enhancement is to expand the Arrow Java format input for ListView. \r\n\r\nThis could be an initial proposal for ListViewVector based on current LisVector implementation:\r\n\r\n**Using ListViewVector:**\r\n````\r\n...\r\n\/\/ values = [12, -7, 25, 0, -127, 127, 50]\r\n\/\/ offsets = [0, 7, 3, 0]\r\n\/\/ sizes = [3, 0, 4, 0]\r\n\/\/ data to get thru listview: [[12,-7,25], null, [0,-127,127,50], []]\r\n...\r\nlistViewVector.allocateNew();\r\ninVector.allocateNew();\r\n\r\ninVector.setSafe(0, 12);\r\ninVector.setSafe(1, -7);\r\ninVector.setSafe(2, 25);\r\ninVector.setSafe(3, 0);\r\ninVector.setSafe(4, -127);\r\ninVector.setSafe(5, 127);\r\ninVector.setSafe(6, 50);\r\n\r\nlistViewVector.startNewValue(0, 0); \/\/ (0=index, 0=offset)\r\nlistViewVector.endValue(0, 3); \/\/(0=index, 3=number of items)\r\n\r\nlistViewVector.setNull(1);\r\n\r\nlistViewVector.startNewValue(2, 3);\r\nlistViewVector.endValue(2, 4);\r\n\r\nlistViewVector.startNewValue(3, 0);\r\nlistViewVector.endValue(3, 0);\r\n\r\nlistViewVector.setValueCount(4);\r\n\r\n...\r\n\r\nassertEquals(inVector.toString(), \"[12, -7, 25, 0, -127, 127, 50]\");\r\nassertEquals(listViewVector.toString(), \"[[12,-7,25], null, [0,-127,127,50], []]\");\r\n````\r\n\r\n**Using Writers:**\r\n````\r\n\/\/ values = [12, -7, 25, 0, -127, 127, 50]\r\n\/\/ offsets = [0, 7, 3, 0]\r\n\/\/ sizes = [3, 0, 4, 0]\r\n\/\/ data to get thru listview: [[12,-7,25], null, [0,-127,127,50], []]\r\n...\r\nUnionListViewWriter writer = inVector.getWriter();\r\nwriter.allocate();\r\n\r\nwriter.setPosition(0);\r\nwriter.startList(0); \/\/ (0=offset)\r\nwriter.bigInt().writeBigInt(12);\r\nwriter.bigInt().writeBigInt(-7);\r\nwriter.bigInt().writeBigInt(25);\r\nwriter.endList(3); \/\/ (3=number of items)\r\n\r\n\r\nwriter.setPosition(2);\r\nwriter.startList(3);\r\nwriter.bigInt().writeBigInt(0);\r\nwriter.bigInt().writeBigInt(-127);\r\nwriter.bigInt().writeBigInt(127);\r\nwriter.bigInt().writeBigInt(50);\r\nwriter.endList(4);\r\n\r\nwriter.setPosition(3);\r\nwriter.startList(0);\r\nwriter.endList(0);\r\n\r\nwriter.setValueCount(4);\r\n\r\n...\r\n\r\nassertEquals(inVector.getDataVector().toString(), \"[12, -7, 25, 0, -127, 127, 50]\");\r\nassertEquals(inVector.toString(), \"[[12,-7,25], null, [0,-127,127,50], []]\");\r\n````\r\n\n\n### Component(s)\n\nJava","comments":[],"labels":["Type: enhancement","Component: Java"]},{"title":"[R] Document use of arrow 13.0.0 for cran binaries and how to install with up to date libarrow","body":"### Describe the enhancement requested\n\nFor the foreseeable future we will need to use the arrow 13.0.0 binary for the windows build on CRAN. This means people will have a reduced feature set by default. We should document this (docs + maybe rlang::warn with frequency=once?) and how to install with the release jfrog version of libarrow (override via `LIBARROW`?)  \n\n### Component(s)\n\nR","comments":[],"labels":["Type: enhancement","Component: R"]},{"title":"[R] Formalize CRAN only changes to win binary source","body":"### Describe the enhancement requested\n\nFor cran specifically we use rwinlibs arrow 13.0.0 binary since 14.0.2.1 by patching nixlibs.R. It looks like we have to carry this through for the forseable future so we should find a way to add this (vs cherry picking the commit from release branch to release branch) permanently. We still want installs from github to use the jfrog\/nightly binaries, so maybe we make jfrog the default and flip that in the release branch?\n\n### Component(s)\n\nR","comments":[],"labels":["Type: enhancement","Component: R"]},{"title":"[C++] Parse query parameters in util::Uri::Parse","body":"### Describe the enhancement requested\n\nDiscussion in: https:\/\/github.com\/apache\/arrow\/pull\/39067#discussion_r1505853441\r\n\r\nIt'd be advantageous to provide helpers for looking up specific query parameters. `Uri::query_items()` currently re-parses the query string each time it is called and only returns a sequence of key-value pairs which is less useful for lookup.\r\n\r\n<details>\r\nBasic sketch of initial changes:\r\n\r\n<pre>\r\ndiff --git a\/cpp\/src\/arrow\/filesystem\/s3fs.cc b\/cpp\/src\/arrow\/filesystem\/s3fs.cc\r\nindex db134f581..aa846528c 100644\r\n--- a\/cpp\/src\/arrow\/filesystem\/s3fs.cc\r\n+++ b\/cpp\/src\/arrow\/filesystem\/s3fs.cc\r\n@@ -355,11 +355,6 @@ Result<S3Options> S3Options::FromUri(const Uri& uri, std::string* out_path) {\r\n     *out_path = std::string(internal::RemoveTrailingSlash(path));\r\n   }\r\n \r\n-  std::unordered_map<std::string, std::string> options_map;\r\n-  ARROW_ASSIGN_OR_RAISE(const auto options_items, uri.query_items());\r\n-  for (const auto& kv : options_items) {\r\n-    options_map.emplace(kv.first, kv.second);\r\n-  }\r\n \r\n   const auto username = uri.username();\r\n   if (!username.empty()) {\r\n@@ -379,7 +374,8 @@ Result<S3Options> S3Options::FromUri(const Uri& uri, std::string* out_path) {\r\n   }\r\n \r\n   bool region_set = false;\r\n-  for (const auto& kv : options_map) {\r\n+  ARROW_ASSIGN_OR_RAISE(auto query_options, uri.query_items());\r\n+  for (const auto& kv : query_options) {\r\n     if (kv.first == \"region\") {\r\n       options.region = kv.second;\r\n       region_set = true;\r\ndiff --git a\/cpp\/src\/arrow\/util\/uri.cc b\/cpp\/src\/arrow\/util\/uri.cc\r\nindex d1b54e78a..639b3a5b8 100644\r\n--- a\/cpp\/src\/arrow\/util\/uri.cc\r\n+++ b\/cpp\/src\/arrow\/util\/uri.cc\r\n@@ -117,25 +117,19 @@ struct Uri::Impl {\r\n   void Reset() {\r\n     uriFreeUriMembersA(&uri_);\r\n     memset(&uri_, 0, sizeof(uri_));\r\n-    data_.clear();\r\n     string_rep_.clear();\r\n     path_segments_.clear();\r\n     port_ = -1;\r\n   }\r\n \r\n-  const std::string& KeepString(const std::string& s) {\r\n-    data_.push_back(s);\r\n-    return data_.back();\r\n-  }\r\n-\r\n   UriUriA uri_;\r\n-  \/\/ Keep alive strings that uriparser stores pointers to\r\n-  std::vector<std::string> data_;\r\n+  \/\/ uriparser stores pointers into the string representation, so we must keep it alive\r\n   std::string string_rep_;\r\n   int32_t port_ = -1;\r\n   std::vector<std::string_view> path_segments_;\r\n   bool is_file_uri_;\r\n   bool is_absolute_path_;\r\n+  std::optional<std::vector<std::pair<std::string, std::string>>> query_items_;\r\n };\r\n \r\n Uri::Uri() : impl_(new Impl) {}\r\n@@ -214,44 +208,20 @@ std::string Uri::path() const {\r\n \r\n std::string Uri::query_string() const { return TextRangeToString(impl_->uri_.query); }\r\n \r\n-Result<std::vector<std::pair<std::string, std::string>>> Uri::query_items() const {\r\n-  \/\/ XXX would it be worthwhile to fold this parsing into Uri::parse() or maybe\r\n-  \/\/ cache these lazily in an unordered_map? Then we could provide\r\n-  \/\/ Uri::query_item(std::string name)\r\n-  const auto& query = impl_->uri_.query;\r\n-  UriQueryListA* query_list;\r\n-  int item_count;\r\n-  std::vector<std::pair<std::string, std::string>> items;\r\n+Result<util::span<std::pair<std::string, std::string>>> Uri::query_items() const {\r\n+  if (impl_->query_items_) return *impl_->query_items_;\r\n \r\n-  if (query.first == nullptr) {\r\n-    return items;\r\n-  }\r\n-  if (uriDissectQueryMallocA(&query_list, &item_count, query.first, query.afterLast) !=\r\n-      URI_SUCCESS) {\r\n-    return Status::Invalid(\"Cannot parse query string: '\", query_string(), \"'\");\r\n-  }\r\n-  std::unique_ptr<UriQueryListA, decltype(&uriFreeQueryListA)> query_guard(\r\n-      query_list, uriFreeQueryListA);\r\n-\r\n-  items.reserve(item_count);\r\n-  while (query_list != nullptr) {\r\n-    if (query_list->value != nullptr) {\r\n-      items.emplace_back(query_list->key, query_list->value);\r\n-    } else {\r\n-      items.emplace_back(query_list->key, \"\");\r\n-    }\r\n-    query_list = query_list->next;\r\n-  }\r\n-  return items;\r\n+  return Status::Invalid(\"Cannot parse query string: '\", query_string(), \"'\");\r\n }\r\n \r\n const std::string& Uri::ToString() const { return impl_->string_rep_; }\r\n \r\n-Status Uri::Parse(const std::string& uri_string) {\r\n+Status Uri::Parse(std::string uri_string) {\r\n   impl_->Reset();\r\n \r\n-  const auto& s = impl_->KeepString(uri_string);\r\n-  impl_->string_rep_ = s;\r\n+  impl_->string_rep_ = std::move(uri_string);\r\n+  const auto& s = impl_->string_rep_;\r\n+\r\n   const char* error_pos;\r\n   if (uriParseSingleUriExA(&impl_->uri_, s.data(), s.data() + s.size(), &error_pos) !=\r\n       URI_SUCCESS) {\r\n@@ -309,12 +279,39 @@ Status Uri::Parse(const std::string& uri_string) {\r\n     impl_->port_ = port_num;\r\n   }\r\n \r\n+  const auto& query = impl_->uri_.query;\r\n+\r\n+  if (query.first == nullptr) {\r\n+    impl_->query_items_ = std::vector<std::pair<std::string, std::string>>{};\r\n+    return Status::OK();\r\n+  }\r\n+\r\n+  int item_count;\r\n+  UriQueryListA* query_list;\r\n+  if (uriDissectQueryMallocA(&query_list, &item_count, query.first, query.afterLast) !=\r\n+      URI_SUCCESS) {\r\n+    return Status::Invalid(\"Cannot parse query string: '\", query_string(), \"'\");\r\n+  }\r\n+  std::unique_ptr<UriQueryListA, decltype(&uriFreeQueryListA)> query_guard(\r\n+      query_list, uriFreeQueryListA);\r\n+\r\n+  impl_->query_items_ = std::vector<std::pair<std::string, std::string>>(item_count);\r\n+  auto* item = impl_->query_items_->data();\r\n+  while (query_list != nullptr) {\r\n+    if (query_list->value != nullptr) {\r\n+      *item++ = {query_list->key, query_list->value};\r\n+    } else {\r\n+      *item++ = {query_list->key, \"\"};\r\n+    }\r\n+    query_list = query_list->next;\r\n+  }\r\n+\r\n   return Status::OK();\r\n }\r\n \r\n-Result<Uri> Uri::FromString(const std::string& uri_string) {\r\n+Result<Uri> Uri::FromString(std::string uri_string) {\r\n   Uri uri;\r\n-  ARROW_RETURN_NOT_OK(uri.Parse(uri_string));\r\n+  ARROW_RETURN_NOT_OK(uri.Parse(std::move(uri_string)));\r\n   return uri;\r\n }\r\n \r\ndiff --git a\/cpp\/src\/arrow\/util\/uri.h b\/cpp\/src\/arrow\/util\/uri.h\r\nindex 74dbe924f..cdfcd78f5 100644\r\n--- a\/cpp\/src\/arrow\/util\/uri.h\r\n+++ b\/cpp\/src\/arrow\/util\/uri.h\r\n@@ -25,6 +25,7 @@\r\n #include <vector>\r\n \r\n #include \"arrow\/type_fwd.h\"\r\n+#include \"arrow\/util\/span.h\"\r\n #include \"arrow\/util\/visibility.h\"\r\n \r\n namespace arrow::util {\r\n@@ -77,16 +78,16 @@ class ARROW_EXPORT Uri {\r\n   \/\/\/\r\n   \/\/\/ Note this API doesn't allow differentiating between an empty value\r\n   \/\/\/ and a missing value, such in \"a&b=1\" vs. \"a=&b=1\".\r\n-  Result<std::vector<std::pair<std::string, std::string>>> query_items() const;\r\n+  Result<util::span<std::pair<std::string, std::string>>> query_items() const;\r\n \r\n   \/\/\/ Get the string representation of this URI.\r\n   const std::string& ToString() const;\r\n \r\n   \/\/\/ Factory function to parse a URI from its string representation.\r\n-  Status Parse(const std::string& uri_string);\r\n+  Status Parse(std::string uri_string);\r\n \r\n   \/\/\/ Factory function to parse a URI from its string representation.\r\n-  static Result<Uri> FromString(const std::string& uri_string);\r\n+  static Result<Uri> FromString(std::string uri_string);\r\n \r\n  private:\r\n   struct Impl;\r\ndiff --git a\/cpp\/src\/arrow\/util\/uri_test.cc b\/cpp\/src\/arrow\/util\/uri_test.cc\r\nindex 36e09b1b2..9fa436ffe 100644\r\n--- a\/cpp\/src\/arrow\/util\/uri_test.cc\r\n+++ b\/cpp\/src\/arrow\/util\/uri_test.cc\r\n@@ -137,7 +137,7 @@ TEST(Uri, ParseQuery) {\r\n     ASSERT_EQ(uri.query_string(), query_string);\r\n     auto result = uri.query_items();\r\n     ASSERT_OK(result);\r\n-    ASSERT_EQ(*result, items);\r\n+    ASSERT_EQ(*result, util::span{items});\r\n   };\r\n \r\n   check_case(\"unix:\/\/localhost\/tmp\", \"\", {});\r\n<\/pre>\r\n<\/details>\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"[Python] Use C++ type traits in types.py","body":"### Describe the enhancement requested\n\ntypes.py redefines type traits such as `is_nested()` that is already defined in type_traits.h\/cc. Could we import and use the C++ type traits instead?\r\n\r\nSee https:\/\/github.com\/apache\/arrow\/pull\/40265\/files#r1505826814\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python","good-first-issue"]},{"title":"[Python\/C++] `S3FileSystem` slow to deserialize due to AWS rule engine JSON parsing","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nDeserializing a pickled S3FileSystem instance is surprisingly slow\r\n\r\n\r\n```python\r\n\r\nimport boto3\r\nfrom pyarrow.fs import S3FileSystem\r\n\r\n# Going via boto is not strictly necessary but setting all the keys and tokens already avoids one HTTP request during init\r\nsession = boto3.session.Session()\r\ncredentials = session.get_credentials()\r\n\r\nfs = S3FileSystem(\r\n    secret_key=credentials.secret_key,\r\n    access_key=credentials.access_key,\r\n    region=\"us-east-2\",\r\n    session_token=credentials.token,\r\n)\r\n# Note: This can also be seen by using just S3FileSystem() but this then posts one HTTP request and I want to emphasize the slow json parser, see below\r\n```\r\n\r\n```python\r\n%timeit pickle.loads(pickle.dumps(fs))\r\n```\r\ntakes `1.01 ms \u00b1 153 \u00b5s per loop` on my machine\r\n\r\nLooking at a py-spy profile shows that most of the time is spent in some internal JSON parsing. Is there a way to avoid this?\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/8629629\/0c9895a6-6550-4d2d-8936-a1bf193dadb3)\r\n\n\n### Component(s)\n\nPython","comments":["cc @pitrou ","> Looking at a py-spy profile shows that most of the time is spent in some internal JSON parsing. Is there a way to avoid this?\r\n\r\nI have no idea. S3 configuration mechanisms are still a mystery to me.","Similar observations here:\r\n```python\r\n>>> %timeit s = S3FileSystem()\r\n1.29 ms \u00b1 1.27 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n>>> %timeit s = S3FileSystem(anonymous=True)\r\n1.29 ms \u00b1 5.52 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n>>> %timeit s = S3FileSystem(anonymous=True, region='eu-west-1')\r\n1.29 ms \u00b1 4.86 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```","@fjetter Is it the repeated cost that bothers you? As in, if only the first `S3FileSystem` instantiation in a Python process was slow, would you mind? ","If the first instantiation is slow but repeated ones would reuse, etc. that would be fine. I'm running into this issue because those things are de\/serialized as part of dask tasks, i.e. once per interpreter is fine, once per instance is not great","Note to self: this is the construction path we go through currently:\r\n```c++\r\nS3Client::S3Client(const std::shared_ptr<AWSCredentialsProvider>& credentialsProvider,\r\n                   const Client::ClientConfiguration& clientConfiguration,\r\n                   Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy signPayloads \/*= Never*\/,\r\n                   bool useVirtualAddressing \/*= true*\/,\r\n                   Aws::S3::US_EAST_1_REGIONAL_ENDPOINT_OPTION USEast1RegionalEndPointOption) :\r\n  BASECLASS(clientConfiguration,\r\n            Aws::MakeShared<Aws::Auth::S3ExpressSignerProvider>(ALLOCATION_TAG,\r\n                                                                credentialsProvider,\r\n                                                                Aws::MakeShared<DefaultS3ExpressIdentityProvider>(ALLOCATION_TAG, *this),\r\n                                                                SERVICE_NAME,\r\n                                                                Aws::Region::ComputeSignerRegion(clientConfiguration.region),\r\n                                                                signPayloads,\r\n                                                                \/*doubleEncodeValue*\/ false),\r\n            Aws::MakeShared<S3ErrorMarshaller>(ALLOCATION_TAG)),\r\n    m_clientConfiguration(clientConfiguration, signPayloads, useVirtualAddressing, USEast1RegionalEndPointOption),\r\n    m_executor(clientConfiguration.executor),\r\n    m_endpointProvider(Aws::MakeShared<S3EndpointProvider>(ALLOCATION_TAG))\r\n{\r\n  init(m_clientConfiguration);\r\n}\r\n\r\nvoid S3Client::init(const S3::S3ClientConfiguration& config)\r\n{\r\n  AWSClient::SetServiceClientName(\"S3\");\r\n  AWS_CHECK_PTR(SERVICE_NAME, m_endpointProvider);\r\n  m_endpointProvider->InitBuiltInParameters(config);\r\n}\r\n```\r\n\r\nIt's the `Aws::MakeShared<S3EndpointProvider>(ALLOCATION_TAG)` that takes so much time in practice.\r\nWe might cache the endpoint providers based on the config options that are used by `InitBuiltInParameters`, that is:\r\n* in `S3BuiltInParameters::SetFromClientConfiguration`: `config.useUSEast1RegionalEndPointOption`, `config.useArnRegion`, `config.disableMultiRegionAccessPoints`, `config.useVirtualAddressing`\r\n* in `BuiltInParameters::SetFromClientConfiguration`: `config.region`, `config.useFIPS`, `config.useDualStack`, `config.endpointOverride`, `config.scheme`\r\n\r\nWe would need at least the AWS SDK >= 1.10 to have `S3ClientConfiguration` and its additional fields.","What is our minimum required AWS SDK version? @kou would you know the answer?\r\n\r\nEdit: trying to find out in https:\/\/github.com\/apache\/arrow\/pull\/40299","With the caching PoC in https:\/\/github.com\/apache\/arrow\/pull\/40299 I get the following:\r\n```python\r\n>>> %timeit s = S3FileSystem(anonymous=True)\r\n34.2 \u00b5s \u00b1 42.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n>>> %timeit s = S3FileSystem(anonymous=True, region='eu-west-1')\r\n32.8 \u00b5s \u00b1 37.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n>>> %timeit s = S3FileSystem()\r\n52.5 \u00b5s \u00b1 262 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n```","> What is our minimum required AWS SDK version? \r\n\r\nI think that we don't define it. Old AWS SDK can be used but there are some known problems. (I think that we recommend at least the current bundled version (1.10.55) to avoid known problems.)\r\n\r\nIf we want to define it, we can do it like the following:\r\n\r\n```diff\r\ndiff --git a\/cpp\/cmake_modules\/ThirdpartyToolchain.cmake b\/cpp\/cmake_modules\/ThirdpartyToolchain.cmake\r\nindex 951028b699..cf9232f331 100644\r\n--- a\/cpp\/cmake_modules\/ThirdpartyToolchain.cmake\r\n+++ b\/cpp\/cmake_modules\/ThirdpartyToolchain.cmake\r\n@@ -5031,7 +5031,7 @@ macro(build_awssdk)\r\n endmacro()\r\n \r\n if(ARROW_S3)\r\n-  resolve_dependency(AWSSDK HAVE_ALT TRUE)\r\n+  resolve_dependency(AWSSDK HAVE_ALT TRUE REQUIRED_VERSION \"X.Y.Z\")\r\n \r\n   message(STATUS \"Found AWS SDK headers: ${AWSSDK_INCLUDE_DIR}\")\r\n   message(STATUS \"Found AWS SDK libraries: ${AWSSDK_LINK_LIBRARIES}\")\r\n```","I've marked https:\/\/github.com\/apache\/arrow\/pull\/40299 ready for review."],"labels":["Type: bug","Component: Python"]},{"title":"[C++] Support casting string to duration","body":"### Describe the enhancement requested\n\nDuration to string cast is supported, would be nice to support string to duration cast. Would also be nice to allow extracting durations from a csv file. Currently, they can be written to a csv file, but not extracted from a csv file\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"GH-40270: [C++] Use LargeStringArray for casting when writing tables to CSV","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nAvoid casting failures when tables contains too long large string arrays.\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nReplace the usage of `StringArray` to `LargeStringArray`.\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\nNo extra test case is needed (as it is to fix some corner cases).\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\nNo user-facing changes.\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40270","comments":[":warning: GitHub issue #40270 **has been automatically assigned in GitHub** to PR creator.","CI results indicate segfaults, surely something is wrong in this PR?","> CI results indicate segfaults, surely something is wrong in this PR?\r\n\r\nIssue fixed, and has passed the `arrow-csv-test` in local environment.","The timeout error of s3 test shouldn't be caused by this PR: https:\/\/github.com\/apache\/arrow\/actions\/runs\/8081349229\/job\/22079702366","After running the benchmarks locally, it seems that this is triggering a bunch of regressions when writing a StringArray to CSV:\r\n```\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\nRegressions: (8)\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n                   benchmark        baseline       contender  change %                                                                                                                                                                                         counters\r\n    WriteCsvStringNoQuote\/50 692.326 MiB\/sec 621.875 MiB\/sec   -10.176     {'family_index': 1, 'per_family_instance_index': 3, 'run_name': 'WriteCsvStringNoQuote\/50', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2217, 'null_percent': 50.0}\r\nWriteCsvStringRejectQuote\/50 704.688 MiB\/sec 625.617 MiB\/sec   -11.221 {'family_index': 3, 'per_family_instance_index': 3, 'run_name': 'WriteCsvStringRejectQuote\/50', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2418, 'null_percent': 50.0}\r\n    WriteCsvStringNoQuote\/10   1.478 GiB\/sec   1.250 GiB\/sec   -15.420     {'family_index': 1, 'per_family_instance_index': 2, 'run_name': 'WriteCsvStringNoQuote\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 2840, 'null_percent': 10.0}\r\n     WriteCsvStringNoQuote\/0   1.976 GiB\/sec   1.652 GiB\/sec   -16.403       {'family_index': 1, 'per_family_instance_index': 0, 'run_name': 'WriteCsvStringNoQuote\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3384, 'null_percent': 0.0}\r\n     WriteCsvStringNoQuote\/1   1.857 GiB\/sec   1.540 GiB\/sec   -17.098       {'family_index': 1, 'per_family_instance_index': 1, 'run_name': 'WriteCsvStringNoQuote\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3184, 'null_percent': 1.0}\r\nWriteCsvStringRejectQuote\/10   1.829 GiB\/sec   1.465 GiB\/sec   -19.942 {'family_index': 3, 'per_family_instance_index': 2, 'run_name': 'WriteCsvStringRejectQuote\/10', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 3843, 'null_percent': 10.0}\r\n WriteCsvStringRejectQuote\/0   2.878 GiB\/sec   2.300 GiB\/sec   -20.086   {'family_index': 3, 'per_family_instance_index': 0, 'run_name': 'WriteCsvStringRejectQuote\/0', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 5652, 'null_percent': 0.0}\r\n WriteCsvStringRejectQuote\/1   2.660 GiB\/sec   1.966 GiB\/sec   -26.074   {'family_index': 3, 'per_family_instance_index': 1, 'run_name': 'WriteCsvStringRejectQuote\/1', 'repetitions': 1, 'repetition_index': 0, 'threads': 1, 'iterations': 4869, 'null_percent': 1.0}\r\n```\r\n","10% looks a bit significant, how could I replay the regression benchmark? Could you please help show me the command line?","You can build in release mode and pass `-DARROW_BUILD_BENCHMARKS=ON` to CMake. You'll then find some benchmark executables in the target directory, including `arrow-csv-writer-benchmark`.","I think this means that this should be more careful when casting. Only cast to large string if the input data is too large for a regular string array.","> I think this means that this should be more careful when casting. Only cast to large string if the input data is too large for a regular string array.\r\n\r\nThanks for the suggestion, indeed that is exactly what I was considering. I have pushed an implementation which tries to cast to `StringArray` then `LargeStringArray` if failed and dispatches to a templated implementations based on the array's type in runtime. As virtual member method cannot be template, this is the simplest \/ least changed solution I can think of.\r\n\r\nI failed to find how to run regression benchmarks, but I have run the main branch and this PR's branch twice, the numbers looks good now.\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/7144772\/ef1fbebb-6df0-4b23-93dd-7af4128382d9)\r\n","Hi @pitrou, I would like to if there are further comments on this pull request?\r\n\r\nThanks!","@ursabot please benchmark","Benchmark runs are scheduled for commit 7e7e5c681f3cb251508dff9071a273f3c04ca4eb. Watch https:\/\/buildkite.com\/apache-arrow and https:\/\/conbench.ursa.dev for updates. A comment will be posted here when the runs are complete.","Thanks for your patience. Conbench analyzed the 7 benchmarking runs that have been run so far on PR commit 7e7e5c681f3cb251508dff9071a273f3c04ca4eb.\n\nThere was 1 benchmark result indicating a performance regression:\n\n- Pull Request Run on `ursa-thinkcentre-m75q` at [2024-03-01 18:23:09Z](https:\/\/conbench.ursa.dev\/compare\/runs\/5c03b55fbe184a7a8d1cbe0111b90d77...40506078b99f40eb82b501cd38aaff6b\/)\n  - [`UniqueString100bytes` (C++) with params=0, source=cpp-micro, suite=arrow-compute-vector-hash-benchmark](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065defe42950785d8000b8b3291c1f0c...065e20b317f6721f8000752a8bb6bbdd)\n\nThe [full Conbench report](https:\/\/github.com\/apache\/arrow\/runs\/22186385788) has more details.",">   * [`UniqueString100bytes` (C++) with params=0, source=cpp-micro, suite=arrow-compute-vector-hash-benchmark](https:\/\/conbench.ursa.dev\/compare\/benchmarks\/065defe42950785d8000b8b3291c1f0c...065e20b317f6721f8000752a8bb6bbdd)\r\n\r\nThe regression of `arrow-compute-vector-hash-benchmark` should be caused by this PR.","The CI failure shouldn't be caused by this PR:\r\n\r\n```\r\n================================== FAILURES ===================================\r\n_______________________ test_dateutil_tzinfo_to_string ________________________\r\n    def test_dateutil_tzinfo_to_string():\r\n        pytest.importorskip(\"dateutil\")\r\n        import dateutil.tz\r\n    \r\n        tz = dateutil.tz.UTC\r\n        assert pa.lib.tzinfo_to_string(tz) == 'UTC'\r\n        tz = dateutil.tz.gettz('Europe\/Paris')\r\n>       assert pa.lib.tzinfo_to_string(tz) == 'Europe\/Paris'\r\nE       AssertionError: assert 'Europe\/Monaco' == 'Europe\/Paris'\r\nE         - Europe\/Paris\r\nE         + Europe\/Monaco\r\npyarrow\\tests\\test_types.py:355: AssertionError\r\n============================== warnings summary ===============================\r\n```"],"labels":["Component: C++","awaiting changes"]},{"title":"[C++] CSV writer's `WriteTable` failed with: Failed casting from large_string to string: input array too large","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nFor tables which contains `arrow::LargeStringArray` which further contains long string elements, the following code:\r\n\r\n```\r\narrow::csv::WriteCSV(*table, options, ofp_.get())\r\n```\r\n\r\nfailed with the error: `Failed casting from large_string to string: input array too large`.\n\n### Component(s)\n\nC++","comments":[],"labels":["Type: bug","Component: C++"]},{"title":"[Python] Add `pyarrow.dataset.dataset` doesn't accept `RecordBatchReader`","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe documentation suggests that `pyarrow.dataset.dataset` should accept a `RecordBatchReader` as a data source, however, this fails.\r\n\r\n```python\r\nimport pyarrow\r\nimport pyarrow.dataset\r\n\r\nbatch = pyarrow.RecordBatch.from_pydict({'a': [1, 2, 3]})\r\nreader = pyarrow.RecordBatchReader.from_batches(batch.schema, [batch])\r\ndataset = pyarrow.dataset.dataset(reader)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<REDACTED>\/.venv\/lib\/python3.11\/site-packages\/pyarrow\/dataset.py\", line 802, in dataset\r\n    raise TypeError(\r\nTypeError: Expected a path-like, list of path-likes or a list of Datasets instead of the given type: RecordBatchReader\r\n```\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: bug","Component: Python"]},{"title":"[Python] Add FlightSqlServer bindings","body":"### Describe the enhancement requested\n\nAdd python bindings to the Arrow C++ implementation of FlightSqlServer.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[C++] Avoid double initialisation \/ double finalize of Aws SDK","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nI have pre-existing code that uses the C++ AWS SDK and calls `Aws::InitAPI` and `Aws::ShutdownAPI`. Now I am using arrow with s3, and it seems to need me to call _arrow_'s `arrow::fs::InitializeS3()` before it will allow me to use the `S3FileSystem`. I would prefer to handle SDK initialisation myself, but I don't see a way to tell arrow that \"I've already initialised the SDK, don't do it yourself\". I would prefer not to add Arrow awareness to all of my existing code. \r\n\r\nIs there a known workaround for this please?\n\n### Component(s)\n\nC++","comments":["> Is there a known workaround for this please?\r\n\r\nCurrently, no, but that's a reasonable feature request. Perhaps by adding APIs such as:\r\n```c++\r\nvoid MarkS3Initialized();\r\nvoid MarkS3Finalized();\r\n```\r\n\r\nWhat do you think?","Thanks @pitrou. That API makes sense to me.","It seems that in more recent versions of the aws sdk than I was using, there is code that ignores duplicate Init\/Shutdown calls:\r\nhttps:\/\/github.com\/aws\/aws-sdk-cpp\/blob\/5929e202e9b1cd84d6234aa01b64f56fd2208350\/src\/aws-cpp-sdk-core\/source\/Aws.cpp#L188\r\n\r\nPerhaps this avoids the need for this altogether. ","> Perhaps this avoids the need for this altogether.\r\n\r\nCan you take a look and see whether it's good enough for you?"],"labels":["Component: C++","Type: usage"]},{"title":"GH-40069: [C++] Use DCLP to prevent race conditions in reading\/writing scalar scratch space","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40069","comments":[],"labels":["Component: C++","awaiting review"]},{"title":"[Python] FlightServerBase don't support inject grpc options","body":"### Describe the enhancement requested\r\n\r\nI'm using FlightServerBase as a server which acts as an UDFServer.\r\n\r\nBut I did not find another document or places to inject GRPC options in `FlightServerBase`.\r\n\r\n\r\nhttps:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.flight.FlightServerBase.html\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nPython","comments":["Hi @sundy-li, were you thinking about options like we have in the tests?\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/cd06982fddcc0b4327cade6e5429f903dd77fd1a\/python\/pyarrow\/tests\/test_flight.py#L2036-L2039\r\n\r\nIf so, I think we could improve this with:\r\n\r\n1. Adding examples to https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.flight.connect.html#pyarrow-flight-connect\r\n2. Adding a Python cookbook entry for this\r\n\r\nWould you be interested in sending a PR in for either\/both?","@amoeba\r\n\r\nThanks for the reply. But I am not looking about set grpc options on client side.  Let me explain the issue more directly.\r\n\r\nI am using Arrow Flight as a server in [databend-udf](https:\/\/github.com\/datafuselabs\/databend-udf\/blob\/d016db34123a2626966d9d8fc408e57600c997f9\/python\/databend_udf\/udf.py#L188), it's python based.\r\n\r\nAnd I want to make the server handle a long-time-response request (such as `time.sleep(300)`). Now I got the error from client side(it's rust based) after 240 s\uff1a\r\n\r\n```\r\nDecode record batch error: Tonic(Status { code: Unavailable, message: \"Too many pings\", source: None })\r\n```\r\n\r\nI searched the internet, and users [suggested](https:\/\/stackoverflow.com\/questions\/66271810\/grpc-error-on-long-connections-too-many-pings) me to add grpc options on server side rather than client side.\r\n\r\nSo I want to know how to add grpc options in `FlightServerBase` \uff08such as set `GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA` to be zero \uff09.\r\n","Hi @sundy-li, sorry about that. It doesn't look like we expose those options to PyArrow at the moment but it seems useful to expose them. Would you be interested in submitting a PR?","> Hi @sundy-li, sorry about that. It doesn't look like we expose those options to PyArrow at the moment but it seems useful to expose them. Would you be interested in submitting a PR?\r\n\r\nI'm afraid not.  I am new to this repo and I found it will involve lots cpp codes and pyx codes to have this feature . It's not an easy task I think.","No worries @sundy-li. Filing issues like you've done is a great way to contribute and if you ever want to take a crack at a PR, there's lots of good options tagged as [good-first-issue](https:\/\/github.com\/apache\/arrow\/issues?q=is%3Aopen+is%3Aissue+label%3Agood-first-issue)."],"labels":["Type: enhancement","Component: Python"]},{"title":"[R] tidyr::unnest function for arrow dataset object containing many parquet objects","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nIs there a tidyr::unnest equivalent for Arrow datasets with multiple Parquet files? \r\n\r\nI need to handle close to a hundred terabytes of Parquet files. Each file has an attribute with nested tables, and within these tables, there's another attribute containing OpenStreetMap IDs that require filtering. I need to cross-reference these IDs with attributes from another index. If it were a flat file or a long \"tidy\" data frame, it wouldn't be an issue, but the nested structure is complicating matters with the Arrow dataset object.\r\n\r\nCurrently, I employ an iterative approach, loading individual Parquet files into memory for filtering and saving (Actually I do this in parallel with the avaible cores on my computer). However, I've come across Arrow datasets, and the ability to lazily define operations before loading the object could greatly enhance speed.\r\n\r\nSee below images for reference of the data Im working with. \r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/60335544\/90d1ff7f-d83a-40c3-b344-20587496c6e5)\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/60335544\/e769ae66-0762-4a37-b355-186b162c8042)\r\n\r\n\n\n### Component(s)\n\nR","comments":["I think it is related to #24956 and #34762\r\n\r\nI have not tried it, but `polars`' unnest might work well.\r\nhttps:\/\/rpolars.github.io\/man\/LazyFrame_unnest.html\r\n\r\nDuckDB also has unnest function.\r\nhttps:\/\/duckdb.org\/docs\/sql\/query_syntax\/unnest.html"],"labels":["Component: R","Type: usage"]},{"title":"[C++] Enable using the GCS+GRPC plugin with Arrow","body":"### Describe the enhancement requested\r\n\r\nLink to plugin: https:\/\/cloud.google.com\/cpp\/docs\/reference\/storage\/latest\/storage-grpc\r\n\r\nGetting the experimental gRPC+GCS plugin to work would require quite a bit build system (CMake) work and the upgrading of libprotobuf and gRPC dependencies. \r\n\r\nThis is untested and incomplete, but this is [an effort that was made](https:\/\/github.com\/felipecrv\/arrow\/commit\/f1f72da1de388c390ae15a66d56e433666ade9f5) to get it working.\r\n\r\nThis could potentially help with higher throughput with GCSFS. \r\n\r\n### Component(s)\r\n\r\nC++","comments":[],"labels":["Type: enhancement","Component: C++"]},{"title":"MINOR: [JS] Bump es5-ext from 0.10.62 to 0.10.63 in \/js","body":"Bumps [es5-ext](https:\/\/github.com\/medikoo\/es5-ext) from 0.10.62 to 0.10.63.\n<details>\n<summary>Release notes<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/medikoo\/es5-ext\/releases\">es5-ext's releases<\/a>.<\/em><\/p>\n<blockquote>\n<h2>0.10.63 (2024-02-23)<\/h2>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li>Do not rely on problematic regex (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/3551cdd7b2db08b1632841f819d008757d28e8e2\">3551cdd<\/a>), addresses <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/201\">#201<\/a><\/li>\n<li>Support ES2015+ function definitions in <code>function#toStringTokens()<\/code> (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/a52e95736690ad1d465ebcd9791d54570e294602\">a52e957<\/a>), addresses <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/021\">#021<\/a><\/li>\n<li>Ensure postinstall script does not crash on Windows, fixes <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/181\">#181<\/a> (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/bf8ed799d57df53096da9d908ff577f305e1366f\">bf8ed79<\/a>)<\/li>\n<\/ul>\n<h3>Maintenance Improvements<\/h3>\n<ul>\n<li>Simplify the manifest message (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/7855319f41b9736639cf4555bd2c419f17addf55\">7855319<\/a>)<\/li>\n<\/ul>\n<hr \/>\n<p><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/compare\/v0.10.62...v0.10.63\">Comparison since last release<\/a><\/p>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Changelog<\/summary>\n<p><em>Sourced from <a href=\"https:\/\/github.com\/medikoo\/es5-ext\/blob\/main\/CHANGELOG.md\">es5-ext's changelog<\/a>.<\/em><\/p>\n<blockquote>\n<h3><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/compare\/v0.10.62...v0.10.63\">0.10.63<\/a> (2024-02-23)<\/h3>\n<h3>Bug Fixes<\/h3>\n<ul>\n<li>Do not rely on problematic regex (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/3551cdd7b2db08b1632841f819d008757d28e8e2\">3551cdd<\/a>), addresses <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/201\">#201<\/a><\/li>\n<li>Support ES2015+ function definitions in <code>function#toStringTokens()<\/code> (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/a52e95736690ad1d465ebcd9791d54570e294602\">a52e957<\/a>), addresses <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/021\">#021<\/a><\/li>\n<li>Ensure postinstall script does not crash on Windows, fixes <a href=\"https:\/\/redirect.github.com\/medikoo\/es5-ext\/issues\/181\">#181<\/a> (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/bf8ed799d57df53096da9d908ff577f305e1366f\">bf8ed79<\/a>)<\/li>\n<\/ul>\n<h3>Maintenance Improvements<\/h3>\n<ul>\n<li>Simplify the manifest message (<a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/7855319f41b9736639cf4555bd2c419f17addf55\">7855319<\/a>)<\/li>\n<\/ul>\n<\/blockquote>\n<\/details>\n<details>\n<summary>Commits<\/summary>\n<ul>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/de4e03c4776a303284142f73f3f181a070615817\"><code>de4e03c<\/code><\/a> chore: Release v0.10.63<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/3fd53b755ec883be8f119c747f0b04130741e456\"><code>3fd53b7<\/code><\/a> chore: Upgrade<code> lint-staged<\/code> to v13<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/bf8ed799d57df53096da9d908ff577f305e1366f\"><code>bf8ed79<\/code><\/a> chore: Ensure postinstall script does not crash on Windows<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/2cbbb0717bd8de6e38fcba1f0d45bc876e7a1951\"><code>2cbbb07<\/code><\/a> chore: Bump dependencies<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/22d0416ea170000a115609f22a560dfa9193ebb0\"><code>22d0416<\/code><\/a> chore: Bump LICENSE year<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/a52e95736690ad1d465ebcd9791d54570e294602\"><code>a52e957<\/code><\/a> fix: Support ES2015+ function definitions in <code>function#toStringTokens()<\/code><\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/3551cdd7b2db08b1632841f819d008757d28e8e2\"><code>3551cdd<\/code><\/a> fix: Do not rely on problematic regex<\/li>\n<li><a href=\"https:\/\/github.com\/medikoo\/es5-ext\/commit\/7855319f41b9736639cf4555bd2c419f17addf55\"><code>7855319<\/code><\/a> chore: Simplify the manifest message<\/li>\n<li>See full diff in <a href=\"https:\/\/github.com\/medikoo\/es5-ext\/compare\/v0.10.62...v0.10.63\">compare view<\/a><\/li>\n<\/ul>\n<\/details>\n<br \/>\n\n\n[![Dependabot compatibility score](https:\/\/dependabot-badges.githubapp.com\/badges\/compatibility_score?dependency-name=es5-ext&package-manager=npm_and_yarn&previous-version=0.10.62&new-version=0.10.63)](https:\/\/docs.github.com\/en\/github\/managing-security-vulnerabilities\/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[\/\/]: # (dependabot-automerge-start)\n[\/\/]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options<\/summary>\n<br \/>\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https:\/\/github.com\/apache\/arrow\/network\/alerts).\n\n<\/details>","comments":[],"labels":["Component: JavaScript","dependencies","javascript","awaiting review"]},{"title":"[Java] JDBC driver throws error based on SqlInfo values","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```\r\njava.lang.NullPointerException: Cannot invoke \"java.util.List.stream()\" because \"sqlInfoList\" is null\r\n\tat org.apache.arrow.driver.jdbc.ArrowDatabaseMetadata.convertListSqlInfoToString(ArrowDatabaseMetadata.java:757)\r\n\tat org.apache.arrow.driver.jdbc.ArrowDatabaseMetadata.getSQLKeywords(ArrowDatabaseMetadata.java:199)\r\n\tat org.jkiss.dbeaver.model.impl.jdbc.exec.JDBCDatabaseMetaDataImpl.getSQLKeywords(JDBCDatabaseMetaDataImpl.java:452)\r\n\tat org.jkiss.dbeaver.model.impl.jdbc.JDBCSQLDialect.loadDriverKeywords(JDBCSQLDialect.java:384)\r\n\tat org.jkiss.dbeaver.model.impl.jdbc.JDBCSQLDialect.initDriverSettings(JDBCSQLDialect.java:223)\r\n\tat org.jkiss.dbeaver.ext.generic.model.GenericSQLDialect.initDriverSettings(GenericSQLDialect.java:72)\r\n\tat org.jkiss.dbeaver.model.impl.jdbc.JDBCDataSource.initialize(JDBCDataSource.java:484)\r\n\tat org.jkiss.dbeaver.ext.generic.model.GenericDataSource.initialize(GenericDataSource.java:468)\r\n\tat org.jkiss.dbeaver.registry.DataSourceDescriptor.openDataSource(DataSourceDescriptor.java:1322)\r\n\tat org.jkiss.dbeaver.registry.DataSourceDescriptor.connect0(DataSourceDescriptor.java:1163)\r\n\tat org.jkiss.dbeaver.registry.DataSourceDescriptor.connect(DataSourceDescriptor.java:960)\r\n\tat org.jkiss.dbeaver.runtime.jobs.ConnectJob.run(ConnectJob.java:77)\r\n\tat org.jkiss.dbeaver.model.runtime.AbstractJob.run(AbstractJob.java:105)\r\n\tat org.eclipse.core.internal.jobs.Worker.run(Worker.java:63)\r\n```\r\n\r\nThis was with DBeaver. I'm guessing that this SqlInfo value isn't provided and the driver isn't checking for null before manipulating the value\n\n### Component(s)\n\nFlightRPC, Java","comments":[],"labels":["Type: bug","Component: Java","Component: FlightRPC"]},{"title":"[R][CI] Consider using pak to install prerequisites?","body":"### Describe the enhancement requested\n\nWe typically use core R functionality (e.g. `install.packages()`) or remotes to install dependencies in our CI. Should we consider moving (some or all) to pak? \r\n\r\nhttps:\/\/github.com\/r-hub\/containers\/issues\/58#issuecomment-1964376761 indicates this is the way forward when using rhub\/container images. So if we use those, we would either need manage our own system dependencies or we could try pak and see if that helps simplify some of our CI dependency management.\n\n### Component(s)\n\nContinuous Integration, R","comments":[],"labels":["Type: enhancement","Component: R","Component: Continuous Integration"]},{"title":"GH-37720: [Format][Docs][FlightSQL] Document stateless prepared statements","body":"documents changes for stateless management of FlightSQL prepared statement handles based on the design proposal described in apache\/arrow#37720\r\n\r\n* GitHub Issue: #37720\r\n\r\nPRs for language implementations:\r\n* Rust: apache\/arrow-rs#5433\r\n* Go: apache\/arrow#40311\r\n\r\nMailing list discussion: https:\/\/lists.apache.org\/thread\/3kb82ypx99q96g84qv555l6x8r0bppyq\r\n","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n",":warning: GitHub issue #37720 **has been automatically assigned in GitHub** to PR creator.","Will we need to extend the schema of the `PutResult` message in order to encapsulate the `DoPutPreparedStatementResult` payload?\r\n\r\nThis is the spec right now:\r\n```proto\r\nservice FlightService {\r\n  rpc DoPut(stream FlightData) returns (stream PutResult) {}\r\n}\r\n\r\nmessage PutResult {\r\n  bytes app_metadata = 1;\r\n}\r\n```\r\n\r\nI don't think the response would be considered `app_metadata`.","The \"app\" in this case is Flight SQL. (Someday I would like to fuse Flight SQL\/Flight RPC to avoid these odd distinctions, and so that we aren't multiplexing a dozen different calls into the same endpoint...)","With that framing it makes sense, thanks @lidavidm."],"labels":["Component: Documentation","awaiting change review"]},{"title":"GH-40069: [C++] Use relaxed atomic store to write scalar scratch space","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #40069","comments":[":warning: GitHub issue #40069 **has been automatically assigned in GitHub** to PR creator.","Hi @bkietz , I had some bad news about the current approach.\r\n\r\nThe current approach did prevent WW race by using atomic stores. The reading, however, is using plane memory load, thus leads to RW race (observable by TSAN, at this line https:\/\/github.com\/apache\/arrow\/pull\/40237\/files#diff-5af65f049e302f76671ecbfc17b0f9c522ec94be291b2bcb442fa565d3607736R2006). Given that the reading is through `ArraySpan` and spread all over the compute component, it is not feasible to change every read to atomic load.\r\n\r\nSo I have to turn to an alternative approach drafted in #40260, using DCLP which could prevent both WW and RW race conditions. Would you step to #40260 to take a look? (It's of course in draft and lacks tests of many other scalar types, but it would be nice to make agreement on the approach first)\r\n\r\nOnce we have an agreement on the new approach, I'll close this PR and move on with the other.\r\n\r\ncc @pitrou ","Ok, before we introduce even more complexity and ad-hoc synchronization, let's step back a bit and ask ourselves the question: why is a ArraySpan or BufferSpan being initialized concurrently? We should probably fix that, instead of adding bandaids so that it doesn't trigger TSAN warnings.","> why is a ArraySpan or BufferSpan being initialized concurrently?\r\n\r\nThe problem is we want FillFromScalar to act like copying a constant value into the ArraySpan.  It would not be an error for multiple threads to copy a `const int` into a local `int`. However FillFromScalar mutates the constant scalar from which we're copying.\r\n\r\nOne way to resolve this without sync would be setting up the offset scratch space when the scalar is constructed. IIRC we didn't do that in the first place because there are places where we assign to `BinaryScalar::value`, which would invalidate the offsets. Cleaning those up would be annoying but is more in line with Scalar behaving as an immutable value","I think it would be good to move to an immutable-after-instantiation model. I don't think it's worth supporting the mutation of a Scalar after it was constructed (especially as Scalar is not meant to be high performance in the first place).","OK, l can check how much work is needed to move to the immutable-after-instantiation approach.","Just a concern. Will this approach break the API compatibility? Since `BaseBinaryScalar::value` is already a public API and there might already have been user code that is assigning to it.","> Will this approach break the API compatibility?\r\n\r\nYes, this would be a breaking change. I would guess that not much user code would be invalidated by the change"],"labels":["Component: C++","awaiting change review"]},{"title":"GH-34535: [C++] Move `ChunkResolver` to the public API","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nSee https:\/\/github.com\/apache\/arrow\/issues\/34535.\r\n\r\n### What changes are included in this PR?\r\n\r\n- [ ] Check unit test coverage and expand as necessary to handle edge cases\r\n- [x] Move `ChunkResolver` out of the `internal` namespace\r\n  - [x] Refactor current usages of the class in Arrow C++ as necessary\r\n- [ ] Update the documentation\r\n  - [x] Update the API\/reference docs as necessary\r\n  - [ ] Add a section, perhaps in the User Guide or Examples, on how to use this class\r\n\r\nAs [requested by @felipecrv](https:\/\/github.com\/apache\/arrow\/issues\/34535#issuecomment-1927733262), this PR also contains a minor simplification to `ChunkResolver`'s existing interface.\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nOpening PR in draft state, unit tests still need to be implemented.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nYes, users will now be able to use the `ChunkResolver` from the public `::arrow` namespace. There are no breaking changes.\r\n\r\nOpening PR in draft state, some documentation may still need to be added.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #34535","comments":[":warning: GitHub issue #34535 **has been automatically assigned in GitHub** to PR creator.","I implemented `Valid` and `Next` in #40368. Feel free to copy and paste here."],"labels":["Component: R","Component: C++","Component: Documentation","awaiting committer review"]},{"title":"[C++] Repartitioning a large dataset with 3 variables uses all my RAM and kills the process","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI'm trying to repartition a ~10Gb dataset based on a new variable, but I can't work out whether this is a bug or expected behaviour due to how things are implemented internally.  Here's the R code I've been running:\r\n\r\n```r\r\nopen_dataset(\"data\/pums\/person\") |>\r\n  mutate(\r\n    age_group = case_when(\r\n      AGEP < 25 ~ \"Under 25\",\r\n      AGEP < 35 ~ \"25-34\",\r\n      AGEP < 45 ~ \"35-44\",\r\n      AGEP < 55 ~ \"45-54\",\r\n      AGEP < 65 ~ \"55-64\",\r\n      TRUE ~ \"65+\"\r\n    )\r\n  )|>\r\n  write_dataset(\r\n    path = \".\/data\/pums\/person-age-partitions\",\r\n    partitioning = c(\"year\", \"location\", \"age_group\")\r\n  )\r\n```\r\n\r\nThe data is in Parquet format and is already partitioned by \"year\" and \"location\".  When I try to run this, it gradually uses more and more of my RAM until it crashes.\r\n\r\nIf I run it with the debugger attached, it all looks fine, but eventually dies with the message `Program terminated with signal SIGKILL, Killed.`\r\n\r\nThis is using Arrow C++ library version 14.0.2, and R package version 14.0.2.1\r\n\r\nWhen I try this with a different variable that already exists in the dataset, it uses a lot of RAM, but seems to back off before it gets too high, e.g.\r\n\r\n```r\r\nopen_dataset(\"data\/pums\/person\") |>\r\n  write_dataset(\"data\/pums\/person-cow-partition\", partitioning = c(\"year\", \"COW\"))\r\n```\r\n\r\nI *think* I'm missing something here in terms of what's going on, rather than this being a bug?\r\n\r\n### Component(s)\r\n\r\nC++","comments":["@westonpace Reckon this is a bug?","Is it possible to print the query plan that gets generated?  I agree that this should work.","Here's the query plan (the dataset has a lot of columns):\r\n\r\n```\r\nExecPlan with 3 nodes:\r\n2:SinkNode{}\r\n  1:ProjectNode{projection=[SPORDER, RT, SERIALNO, PUMA, ST, ADJUST, PWGTP, AGEP, CIT, COW, DDRS, DEYE, DOUT, DPHY, DREM, DWRK, ENG, FER, GCL, GCM, GCR, INTP, JWMNP, JWRIP, JWTR, LANX, MAR, MIG, MIL, MILY, MLPA, MLPB, MLPC, MLPD, MLPE, MLPF, MLPG, MLPH, MLPI, MLPJ, MLPK, NWAB, NWAV, NWLA, NWLK, NWRE, OIP, PAP, REL, RETP, SCH, SCHG, SCHL, SEMP, SEX, SSIP, SSP, WAGP, WKHP, WKL, WKW, YOEP, UWRK, ANC, ANC1P, ANC2P, DECADE, DRIVESP, DS, ESP, ESR, HISP, INDP, JWAP, JWDP, LANP, MIGPUMA, MIGSP, MSP, NAICSP, NATIVITY, OC, OCCP, PAOC, PERNP, PINCP, POBP, POVPIP, POWPUMA, POWSP, QTRBIR, RAC1P, RAC2P, RAC3P, RACAIAN, RACASN, RACBLK, RACNHPI, RACNUM, RACSOR, RACWHT, RC, SFN, SFR, SOCP, VPS, WAOB, FAGEP, FANCP, FCITP, FCOWP, FDDRSP, FDEYEP, FDOUTP, FDPHYP, FDREMP, FDWRKP, FENGP, FESRP, FFERP, FGCLP, FGCMP, FGCRP, FHISP, FINDP, FINTP, FJWDP, FJWMNP, FJWRIP, FJWTRP, FLANP, FLANXP, FMARP, FMIGP, FMIGSP, FMILPP, FMILSP, FMILYP, FOCCP, FOIP, FPAP, FPOBP, FPOWSP, FRACP, FRELP, FRETP, FSCHGP, FSCHLP, FSCHP, FSEMP, FSEXP, FSSIP, FSSP, FWAGP, FWKHP, FWKLP, FWKWP, FYOEP, PWGTP1, PWGTP2, PWGTP3, PWGTP4, PWGTP5, PWGTP6, PWGTP7, PWGTP8, PWGTP9, PWGTP10, PWGTP11, PWGTP12, PWGTP13, PWGTP14, PWGTP15, PWGTP16, PWGTP17, PWGTP18, PWGTP19, PWGTP20, PWGTP21, PWGTP22, PWGTP23, PWGTP24, PWGTP25, PWGTP26, PWGTP27, PWGTP28, PWGTP29, PWGTP30, PWGTP31, PWGTP32, PWGTP33, PWGTP34, PWGTP35, PWGTP36, PWGTP37, PWGTP38, PWGTP39, PWGTP40, PWGTP41, PWGTP42, PWGTP43, PWGTP44, PWGTP45, PWGTP46, PWGTP47, PWGTP48, PWGTP49, PWGTP50, PWGTP51, PWGTP52, PWGTP53, PWGTP54, PWGTP55, PWGTP56, PWGTP57, PWGTP58, PWGTP59, PWGTP60, PWGTP61, PWGTP62, PWGTP63, PWGTP64, PWGTP65, PWGTP66, PWGTP67, PWGTP68, PWGTP69, PWGTP70, PWGTP71, PWGTP72, PWGTP73, PWGTP74, PWGTP75, PWGTP76, PWGTP77, PWGTP78, PWGTP79, PWGTP80, NOP, ADJINC, CITWP, DEAR, DRAT, DRATX, HINS1, HINS2, HINS3, HINS4, HINS5, HINS6, HINS7, MARHD, MARHM, MARHT, MARHW, MARHYP, DIS, HICOV, PRIVCOV, PUBCOV, FCITWP, FDEARP, FDRATP, FDRATXP, FHINS1P, FHINS2P, FHINS3P, FHINS4P, FHINS5P, FHINS6P, FHINS7P, FMARHDP, FMARHMP, FMARHTP, FMARHWP, FMARHYP, WRK, FOD1P, FOD2P, SCIENGP, SCIENGRLP, FFODP, FHINS3C, FHINS4C, FHINS5C, RELP, FWRKP, FDISP, FPERNP, FPINCP, FPRIVCOVP, FPUBCOVP, RACNH, RACPI, SSPA, MLPCD, MLPFG, FHICOVP, DIVISION, REGION, HIMRKS, JWTRNS, RELSHIPP, WKWN, FHIMRKSP, FJWTRNSP, FRELSHIPP, FWKWNP, MLPIK, year, location, \"age_group\": case_when({1=(AGEP < 25), 2=(AGEP < 35), 3=(AGEP < 45), 4=(AGEP < 55), 5=(AGEP < 65), 6=true}, \"Under 25\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\")]}\r\n    0:SourceNode{}\r\n```","Hmm, could be an R bug or something already solved actually; I ran the following (different query, but similarly problematic in R) with pyarrow:\r\n\r\n```\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as ds\r\n\r\ndataset = ds.dataset(\"data\/pums\/person\", format=\"parquet\")\r\nds.write_dataset(\"data\/pums\/person-puma-partition\", partitioning = [\"year\", \"PUMA\"])\r\n```\r\n\r\nand RAM usage never goes above 80%.\r\n\r\n(It is a newer Arrow C++ version though, so I'll need to check and see if it's the same on 14.0.0)","> The data is in Parquet format and is already partitioned by \"year\" and \"location\". When I try to run this, it gradually uses more and more of my RAM until it crashes.\r\n\r\nHow were you measuring RAM?  Were you looking at the RSS of the process? Or were you looking at the amount of free\/available memory?\r\n\r\nI would also be surprised if that plan itself was leaking \/ accumulating memory.\r\n\r\nIf it is R specific then maybe R is accumulating everything before the call to `write_dataset`?  I seem to remember that being an R fallback at some point when creating plans.\r\n\r\nIn python the `write_dataset` call can take as input a record batch reader.  I think you actually end up with two acero plans.  The first is the one you shared and the second is just `source -> write` (where the first plan's output is the \"source\" node in the second plan).\r\n\r\nHowever, in R, it might be more natural to make a `source -> project -> write` plan instead of a `source -> project -> sink` plan in this situation.","OK, this is the actual plan:\r\n\r\n```\r\nExecPlan with 3 nodes:\r\n2:ConsumingSinkNode{}\r\n  1:ProjectNode{projection=[SPORDER, RT, SERIALNO, PUMA, ST, ADJUST, PWGTP, AGEP, CIT, COW, DDRS, DEYE, DOUT, DPHY, DREM, DWRK, ENG, FER, GCL, GCM, GCR, INTP, JWMNP, JWRIP, JWTR, LANX, MAR, MIG, MIL, MILY, MLPA, MLPB, MLPC, MLPD, MLPE, MLPF, MLPG, MLPH, MLPI, MLPJ, MLPK, NWAB, NWAV, NWLA, NWLK, NWRE, OIP, PAP, REL, RETP, SCH, SCHG, SCHL, SEMP, SEX, SSIP, SSP, WAGP, WKHP, WKL, WKW, YOEP, UWRK, ANC, ANC1P, ANC2P, DECADE, DRIVESP, DS, ESP, ESR, HISP, INDP, JWAP, JWDP, LANP, MIGPUMA, MIGSP, MSP, NAICSP, NATIVITY, OC, OCCP, PAOC, PERNP, PINCP, POBP, POVPIP, POWPUMA, POWSP, QTRBIR, RAC1P, RAC2P, RAC3P, RACAIAN, RACASN, RACBLK, RACNHPI, RACNUM, RACSOR, RACWHT, RC, SFN, SFR, SOCP, VPS, WAOB, FAGEP, FANCP, FCITP, FCOWP, FDDRSP, FDEYEP, FDOUTP, FDPHYP, FDREMP, FDWRKP, FENGP, FESRP, FFERP, FGCLP, FGCMP, FGCRP, FHISP, FINDP, FINTP, FJWDP, FJWMNP, FJWRIP, FJWTRP, FLANP, FLANXP, FMARP, FMIGP, FMIGSP, FMILPP, FMILSP, FMILYP, FOCCP, FOIP, FPAP, FPOBP, FPOWSP, FRACP, FRELP, FRETP, FSCHGP, FSCHLP, FSCHP, FSEMP, FSEXP, FSSIP, FSSP, FWAGP, FWKHP, FWKLP, FWKWP, FYOEP, PWGTP1, PWGTP2, PWGTP3, PWGTP4, PWGTP5, PWGTP6, PWGTP7, PWGTP8, PWGTP9, PWGTP10, PWGTP11, PWGTP12, PWGTP13, PWGTP14, PWGTP15, PWGTP16, PWGTP17, PWGTP18, PWGTP19, PWGTP20, PWGTP21, PWGTP22, PWGTP23, PWGTP24, PWGTP25, PWGTP26, PWGTP27, PWGTP28, PWGTP29, PWGTP30, PWGTP31, PWGTP32, PWGTP33, PWGTP34, PWGTP35, PWGTP36, PWGTP37, PWGTP38, PWGTP39, PWGTP40, PWGTP41, PWGTP42, PWGTP43, PWGTP44, PWGTP45, PWGTP46, PWGTP47, PWGTP48, PWGTP49, PWGTP50, PWGTP51, PWGTP52, PWGTP53, PWGTP54, PWGTP55, PWGTP56, PWGTP57, PWGTP58, PWGTP59, PWGTP60, PWGTP61, PWGTP62, PWGTP63, PWGTP64, PWGTP65, PWGTP66, PWGTP67, PWGTP68, PWGTP69, PWGTP70, PWGTP71, PWGTP72, PWGTP73, PWGTP74, PWGTP75, PWGTP76, PWGTP77, PWGTP78, PWGTP79, PWGTP80, year, location, \"age_group\": case_when({1=(AGEP < 25), 2=(AGEP < 35), 3=(AGEP < 45), 4=(AGEP < 55), 5=(AGEP < 65), 6=true}, \"Under 25\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\")]}\r\n    0:SourceNode{}\r\n```","It would be nice if the ConsumingSinkNode printed the values of the WriteNodeOptions so we could compare with pyarrow. But glancing at the defaults, they look the same (more or less?)","> How were you measuring RAM? Were you looking at the RSS of the process? Or were you looking at the amount of free\/available memory?\r\n\r\nI was just looking at free\/available memory - would RSS of the process be better?\r\n\r\n> If it is R specific then maybe R is accumulating everything before the call to write_dataset? I seem to remember that being an R fallback at some point when creating plans.\r\n\r\nThanks, I'll take a closer look into the code to see if I can find something.\r\n\r\n> In python the write_dataset call can take as input a record batch reader. I think you actually end up with two acero plans. The first is the one you shared and the second is just source -> write (where the first plan's output is the \"source\" node in the second plan).\r\n\r\n> However, in R, it might be more natural to make a source -> project -> write plan instead of a source -> project -> sink plan in this situation.\r\n\r\nSorry, I'm a bit lost here; what are the implications of write versus sink here?","> Sorry, I'm a bit lost here; what are the implications of write versus sink here?\r\n\r\nI suppose it is more about \"two plans\" vs. \"one plan\".\r\n\r\nA plan's output can be a record batch reader (the final node is a sink node).  A plan's input can be a record batch reader.  A write plan has no output (the final node is a write node).\r\n\r\nSo, to scan and rewrite a dataset, you can have two plans:\r\n\r\nPlan 1: Scan(files) -> Project -> Sink\r\nPlan 2: Scan(record_batch_reader) -> Write\r\n\r\nOr you can do it all in one plan:\r\n\r\nCombined: Scan(files) -> Project -> Write\r\n\r\nIn python, since there is no equivalent of dplyr, the user cannot write a \"single statement\" like `open_dataset |> mutate(...) |> write_dataset(...)`.  Instead the user has to do something like...\r\n\r\n```\r\nprojected = dataset.to_reader(...) # Creates an acero plan\r\nds.write_dataset(projected) # Creates a second acero plan\r\n```\r\n\r\nHowever, since you have a single dplyr plan in R, it might be possible to make a single acero plan.  That being said, I don't expect it to have much impact.\r\n\r\n> I was just looking at free\/available memory - would RSS of the process be better?\r\n\r\nWhen writing a dataset the server will generally use all available memory.  This is because a \"write\" call just copies data from the process RSS to the kernel page cache.  It doesn't wait and block until all the data is persisted to disk.  So you will generally see `it gradually uses more and more of my RAM` as expected behavior.  If you are looking at the output of `free`:\r\n\r\n```\r\n               total        used        free      shared  buff\/cache   available\r\nMem:            31Gi       9.8Gi        10Gi       262Mi        11Gi        20Gi\r\nSwap:           63Gi       5.5Gi        58Gi\r\n```\r\n\r\nYou should see `free` drop to 0 and `buff\/cache` grow to consume all RAM.  This is normal.\r\n\r\nHowever, you should not see the RSS of the process increase without bound.   You should not see `used` increase without bound.  You also shouldn't see `Program terminated with signal SIGKILL, Killed.`","Thanks!  And when you say \"increase without bound\", how would I know that's happening?","OK, so I've been experimenting with various combinations of this, and have found that it happens with both Python and R, so looks like a C++ issue.  \r\n\r\nI'm running this in a Docker container I've created based off `ubuntu:latest`, with 8Gb of RAM, 2Gb of swap, and 50% of my CPU.\r\n\r\nHere's what I've found so far:\r\n* everything is fine when I partition on an existing variable and a new one that I've created via projection (so I think the new column thing was a red herring), even if it's slow, it eventually completes\r\n* as soon as I partition on 3 variables, it eventually crashes (both in Python and R)\r\n\r\nHere's an example in pyarrow using the NYC taxi dataset (this should result in 924 partitions):\r\n\r\n```py\r\nimport pyarrow.dataset as ds\r\nimport pyarrow as pa\r\n\r\ndataset = ds.dataset(\"data\", partitioning=\"hive\")\r\n\r\ntarget_dir = \"data2\"\r\n\r\nds.write_dataset(\r\n    dataset,\r\n    target_dir,\r\n    partitioning=[\"year\", \"month\", \"rate_code\"]\r\n)\r\n```\r\n\r\nI was wondering if it was related to the number of partitions, though when I run this example (which should have fewer partitions -  396), it also eats memory until the Python process is killed.\r\n\r\n```py\r\nimport pyarrow.dataset as ds\r\nimport pyarrow as pa\r\n\r\ndataset = ds.dataset(\"data\", partitioning=\"hive\")\r\n\r\ntarget_dir = \"data2\"\r\n\r\nds.write_dataset(\r\n    dataset,\r\n    target_dir,\r\n    partitioning=[\"year\", \"month\", \"vendor_name\"]\r\n)\r\n```\r\n\r\nHappy to try to investigate further and try to get some output, but I wasn't sure whether it'd be more useful to log the memory via `free` or run with the debugger attached and log the output?","I think we might be rehashing some of the conversation already had a long time ago in https:\/\/github.com\/apache\/arrow\/issues\/18944#issuecomment-1377665189","Is the relevant issue number of partition variables? Or is it the number of groups\/partitions that result. I.e. if you partitioned on one variable that had 1000 distinct values, would you have the same problem?","I tried it with `mta_tax` which has 385 distinct values, and it also crashes.  But I'd expect that, seeing as the data isn't already partitioned on that variable and it'd need to all be in memory to work out how many distinct partitions it needs.  Actually, no, perhaps I wouldn't given we can call `distinct()` on a column without having all data in memory, though I don't know if the same mechanism that allows that is used when working out what data goes in what partition.\r\n\r\nBut, I don't know how this works internally here.\r\n\r\nMaybe I'm overthinking this and it is just as simple as number of total partitions...","> OK, so I've been experimenting with various combinations of this, and have found that it happens with both Python and R, so looks like a C++ issue.\r\n> \r\n> I'm running this in a Docker container I've created based off `ubuntu:latest`, with 8Gb of RAM, 2Gb of swap, and 50% of my CPU.\r\n> \r\n> Here's what I've found so far:\r\n> \r\n>     * everything is fine when I partition on an existing variable and a new one that I've created via projection (so I think the new column thing was a red herring), even if it's slow, it eventually completes\r\n> \r\n>     * as soon as I partition on 3 variables, it eventually crashes (both in Python and R)\r\n> \r\n> \r\n> Here's an example in pyarrow using the NYC taxi dataset (this should result in 924 partitions):\r\n> \r\n> ```python\r\n> import pyarrow.dataset as ds\r\n> import pyarrow as pa\r\n> \r\n> dataset = ds.dataset(\"data\", partitioning=\"hive\")\r\n> \r\n> target_dir = \"data2\"\r\n> \r\n> ds.write_dataset(\r\n>     dataset,\r\n>     target_dir,\r\n>     partitioning=[\"year\", \"month\", \"rate_code\"]\r\n> )\r\n> ```\r\n> \r\n> I was wondering if it was related to the number of partitions, though when I run this example (which should have fewer partitions - 396), it also eats memory until the Python process is killed.\r\n> \r\n> ```python\r\n> import pyarrow.dataset as ds\r\n> import pyarrow as pa\r\n> \r\n> dataset = ds.dataset(\"data\", partitioning=\"hive\")\r\n> \r\n> target_dir = \"data2\"\r\n> \r\n> ds.write_dataset(\r\n>     dataset,\r\n>     target_dir,\r\n>     partitioning=[\"year\", \"month\", \"vendor_name\"]\r\n> )\r\n> ```\r\n> \r\n> Happy to try to investigate further and try to get some output, but I wasn't sure whether it'd be more useful to log the memory via `free` or run with the debugger attached and log the output?\r\n\r\nIs the `data` in these examples something I can easily download?  I will try and reproduce \/ study this on the weekend.","Thanks @westonpace !  I set up a private repo with all the Dockerfiles etc I've been using, and notes about the different experiments I've been running - you should have an invitation to that repo now. `data` is the NYC taxi dataset from the VD bucket.","Thank you!  I looked at this today.  It's a bug.  It was probably introduced when we switched the dataset writer over to using some more generic tools for backpressure (the async task scheduler).  Apologies in advance for the long boring explanation :)\r\n\r\nThe dataset writer does not assume the underlying file writer is re-entrant (I can't remember if the parquet writer is reentrant or not).  When a batch comes in for file X and a write task is already running on file X then we queue that batch up.  We call this data \"in flight\" and we have a special throttle for how many rows we can have in flight (it's not configurable and set to 8Mi).  When this throttle is full it sends a signal to the source to pause.\r\n\r\nAll of this is actually working correctly.  The problem is that, when it pauses the source, a few extra tasks leak in because they were already running.  This is kind of ok, but then it unpauses, fills up, and pauses again, and a few more extra tasks leak in. This process repeats...a lot.  By the time it crashed on my machine there was over a thousand extra tasks.  Because all these tasks are getting in the source thinks the data is being consumed and it keeps reading.  This becomes uncontrolled memory growth and everything crashes.\r\n\r\nI suspect this is related to partitioning because you end up with lots of tiny writes and the in flight throttle fills and empties A LOT during execution.  You might be able to get things to pass if you set `min_rows_per_group` to some largish value although then you run into a different kind of throttle (max rows \"staged\") and so it might not help.\r\n\r\nThe proper fix would be to not release the throttle until the extra tasks that snuck in the last time it was paused have been launched.  I am doing some arrow-cpp work this week and will try and get a look at this.","Cheers for looking into it!\r\n\r\nHah, after spending time running various experiments and testing hypotheses based on guesswork mental models, reading the long explanation is certainly not boring - it's satisfying to know these things, so thank you!","@thisisnic I believe I have come up with a fix (https:\/\/github.com\/apache\/arrow\/pull\/40722) if you want to try and test it out on your setup.","Thanks @westonpace, will take a look over the weekend!"],"labels":["Component: C++","Type: usage"]},{"title":"[R] perl operators in regular expressions","body":"### Describe the enhancement requested\n\nR 4.3, arrow 14.0.0.2 (most recent Mac OS binary; apologies in advance if this is already supported in source)\r\n\r\n`arrow` can't handle perl operators, such as negative lookaheads, in regular expressions, at least via `dplyr` and `stringr`: \r\n\r\n```\r\nlibrary(arrow)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\n\r\nar = data.frame(text = c('Lorem ipsum dolor sit amet', \r\n                         'Lorem dolor ipsum sit amet')) |> \r\n    as_arrow_table()\r\n\r\n## Works, returns both rows\r\nar |> \r\n    filter(str_detect(text, 'Lorem [^(ipsum)]')) |> \r\n    collect()\r\n\r\n## Should only return the second row\r\n## Error in `compute.arrow_dplyr_query()`:\r\n## ! Invalid: Invalid regular expression: invalid perl operator: (?!\r\nar |> \r\n    filter(str_detect(text, regex('Lorem(?! ipsum)')))\r\n    collect()\r\n```\n\n### Component(s)\n\nR","comments":["Without looking at the code, so not a definitive answer, but I am pretty sure that `re2` the C++ library used in acero doesn't support lookahead so this is probably not something that can be added."],"labels":["Type: enhancement","Component: R"]},{"title":"[R] Caught segfault on benchmark run for TPCH query 21 (scale factor 10)","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nFrom the performance report for #40197, apparently we get:\r\n\r\n```\r\n ' *** caught segfault ***', \"address 0x3d, cause 'memory not mapped'\", '', 'Traceback:', ' 1: RecordBatchReader__UnsafeDelete(self)', ' 2: reader$.unsafe_delete()', ' 3: as_arrow_table.arrow_dplyr_query(x)',\r\n```\r\n\r\nfor the job:\r\n\r\n```\r\nengine=arrow, format=parquet, language=R, memory_map=False, query_id=TPCH-21, scale_factor=10\r\n```\r\n\r\n`RecordBatchReader__UnsafeDelete()` is something I added but I would have to re-look into its use to ensure it is not getting called twice...I believe it was introduced to ensure that open files were closed promptly since this caused problems on Windows.\r\n\r\nAlso, huge regression last July:\r\n\r\n<img width=\"443\" alt=\"Screenshot 2024-02-23 at 4 32 28\u202fPM\" src=\"https:\/\/github.com\/apache\/arrow\/assets\/10995762\/b6170627-1c00-48ad-bb1c-38a5a37914ca\">\r\n\r\n\r\nhttps:\/\/conbench.ursa.dev\/benchmark-results\/065d8d9e6ab17d1e8000cb6422edfa64\/\r\n\r\nFull dump:\r\n\r\n<details>\r\n\r\n```\r\n                ['', ' *** caught segfault ***', \"address 0x3d, cause 'memory not mapped'\", '', 'Traceback:', ' 1: RecordBatchReader__UnsafeDelete(self)', ' 2: reader$.unsafe_delete()', ' 3: as_arrow_table.arrow_dplyr_query(x)', ' 4: as_arrow_table(x)', ' 5: doTryCatch(return(expr), name, parentenv, handler)', ' 6: tryCatchOne(expr, names, parentenv, handlers[[1L]])', ' 7: tryCatchList(expr, classes, parentenv, handlers)', ' 8: tryCatch(as_arrow_table(x), error = function(e, call = caller_env(n = 4)) {    augment_io_error_msg(e, call, schema = schema())})', ' 9: compute.arrow_dplyr_query(x)', '10: collect.arrow_dplyr_query(.)', '11: collect_func(.)', '12: input_func(\"supplier\") %>% inner_join(line_items, by = c(s_suppkey = \"l_suppkey\")) %>%     filter(l_receiptdate > l_commitdate) %>% inner_join(input_func(\"nation\"),     by = c(s_nationkey = \"n_nationkey\")) %>% filter(n_name ==     \"SAUDI ARABIA\") %>% group_by(s_name) %>% summarise(numwait = n()) %>%     ungroup() %>% arrange(desc(numwait), s_name) %>% head(100) %>%     collect_func()', '13: query(input_func, collect_func, con)', '14: eval(bm$run, envir = ctx)', '15: eval(bm$run, envir = ctx)', '16: eval(expr, p)', '17: eval.parent(...)', '18: as_bench_time(.Call(system_time_, substitute(expr), parent.frame()))', '19: stats::setNames(as_bench_time(.Call(system_time_, substitute(expr),     parent.frame())), c(\"process\", \"real\"))', '20: bench::bench_time(eval.parent(...))', '21: eval(expr, p)', '22: eval.parent(expr)', '23: with_profiling(profiling, {    timings <- bench::bench_time(eval.parent(...))})', '24: force(expr)', '25: with_gc_info({    prof_file <- with_profiling(profiling, {        timings <- bench::bench_time(eval.parent(...))    })})', '26: measure(eval(bm$run, envir = ctx), profiling = profiling, drop_caches = drop_caches)', '27: run_iteration(bm = bm, ctx = ctx, profiling = profiling, drop_caches = global_params[[\"drop_caches\"]])', '28: withCallingHandlers({    results[[i]] <- run_iteration(bm = bm, ctx = ctx, profiling = profiling,         drop_caches = global_params[[\"drop_caches\"]])}, warning = function(w) {    warnings <<- c(warnings, list(list(warning = as.character(w),         stack_trace = vapply(traceback(3), function(x) paste(x,             collapse = \"\\\\n\"), character(1)))))})', '29: doTryCatch(return(expr), name, parentenv, handler)', '30: tryCatchOne(expr, names, parentenv, handlers[[1L]])', '31: tryCatchList(expr, classes, parentenv, handlers)', '32: tryCatch(withCallingHandlers({    results[[i]] <- run_iteration(bm = bm, ctx = ctx, profiling = profiling,         drop_caches = global_params[[\"drop_caches\"]])}, warning = function(w) {    warnings <<- c(warnings, list(list(warning = as.character(w),         stack_trace = vapply(traceback(3), function(x) paste(x,             collapse = \"\\\\n\"), character(1)))))}), error = function(e) {    error <<- list(error = as.character(e), stack_trace = vapply(traceback(3),         function(x) paste(x, collapse = \"\\\\n\"), character(1)))})', '33: run_bm(format = \"parquet\", scale_factor = 10, engine = \"arrow\",     memory_map = FALSE, query_id = 21, bm = structure(list(name = \"tpch\",         setup = function(engine = \"arrow\", query_id = 1:22, format = c(\"native\",             \"parquet\"), scale_factor = c(1, 10), memory_map = FALSE,             output = \"data_frame\", chunk_size = NULL) {            engine <- match.arg(engine, c(\"arrow\", \"duckdb\",                 \"duckdb_sql\", \"dplyr\"))            format <- match.arg(format, c(\"parquet\", \"feather\",                 \"native\"))            stopifnot(`query_id must be an int` = query_id%%1 ==                 0, `query_id must 1-22` = query_id >= 1 & query_id <=                 22)            output <- match.arg(output, c(\"arrow_table\", \"data_frame\"))            library(\"dplyr\", warn.conflicts = FALSE)            collect_func <- collect            if (output == \"data_frame\") {                collect_func <- collect            } else if (output == \"arrow_table\") {                collect_func <- compute            }            con <- NULL            if (engine %in% c(\"duckdb\", \"duckdb_sql\")) {                con <- DBI::dbConnect(duckdb::duckdb())                DBI::dbExecute(con, paste0(\"PRAGMA threads=\",                   getOption(\"Ncpus\")))            }            BenchEnvironment(input_func = get_input_func(engine = engine,                 scale_factor = scale_factor, query_id = query_id,                 format = format, con = con, memory_map = memory_map,                 chunk_size = chunk_size), query = get_query_func(query_id,                 engine), engine = engine, con = con, scale_factor = scale_factor,                 query_id = query_id, collect_func = collect_func)        }, before_each = quote({            result <- NULL        }), run = quote({            result <- query(input_func, collect_func, con)        }), after_each = quote({            if (scale_factor %in% c(0.01, 0.1, 1, 10)) {                answer <- tpch_answer(scale_factor, query_id)                result <- dplyr::as_tibble(result)                all_equal_out <- waldo::compare(result, answer,                   tolerance = 0.01)                if (length(all_equal_out) != 0) {                  warning(paste0(\"\\\\n\", all_equal_out, \"\\\\n\"))                  stop(\"The answer does not match\")                }            } else {                warning(\"There is no validation for scale_factors other than 0.01, 0.1, 1, and 10. Be careful with these results!\")            }            result <- NULL        }), teardown = quote({            if (!is.null(con)) {                DBI::dbDisconnect(con, shutdown = TRUE)            }        }), valid_params = function(params) {            drop <- (params$engine != \"arrow\" & params$format ==                 \"feather\") | (params$engine != \"arrow\" & params$output ==                 \"arrow_table\") | (params$engine != \"arrow\" &                 params$memory_map == TRUE) | (params$engine ==                 \"dplyr\" & params$format == \"native\")            params[!drop, ]        }, case_version = function(params) NULL, batch_id_fun = function(params) {            batch_id <- uuid()            paste0(batch_id, \"-\", params$scale_factor, substr(params$format,                 1, 1))        }, tags_fun = function(params) {            params$query_id <- sprintf(\"TPCH-%02d\", params$query_id)            if (!is.null(params$output) && params$output == \"data_frame\") {                params$output <- NULL            }            params        }, packages_used = function(params) {            c(params$engine, \"dplyr\", \"lubridate\")        }), class = \"Benchmark\"), n_iter = 1, batch_id = NULL,     profiling = FALSE, global_params = list(cpu_count = NULL,         lib_path = \"latest\"), run_id = NULL, run_name = NULL,     run_reason = NULL)', 'An irrecoverable exception occurred. R is aborting now ...', 'Segmentation fault (core dumped)']\r\n```\r\n\r\n<\/details>\n\n### Component(s)\n\nR","comments":[],"labels":["Type: bug","Component: R"]},{"title":"[Python] Uploading nightly wheels to the scientific python nightly channel","body":"### Describe the enhancement requested\n\nIt looks like you already build nightly Python wheels:\r\n- https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/developers\/python.rst#installing-nightly-packages\r\n\r\nWould you be interested\/willing to upload nightly wheels to he [scientific-python nightly channel](https:\/\/anaconda.org\/scientific-python-nightly-wheels) instead:\r\n- https:\/\/anaconda.org\/scientific-python-nightly-wheels\/\r\n\r\nThis would make the wheels easier to discover by hosting them in a \"standard\" place. It will also make it easier for downstream projects to test against multiple packages, since they won't need to specify multiple locations in their CI scripts.\r\n\r\nYou can read more about why and how we are building \/ testing against nightly wheels here:\r\nhttps:\/\/scientific-python.org\/specs\/spec-0004\/\r\n\r\nWe also provide a GitHub action to upload and manage nightly builds:\r\nhttps:\/\/github.com\/scientific-python\/upload-nightly-action\/\r\n\r\nWe @matthewfeickert, @larsoner, @tupui are happy to help.\r\n\n\n### Component(s)\n\nPython","comments":["cc @raulcd \r\n\r\nThis sounds as an interesting idea to me. My understanding is that our nightly wheels are not \"official\" release artifacts anyway, so it shouldn't matter too much where it is hosted? (also not sure if the current hosting location is actually tied to the Apache foundation)\r\n\r\nI suppose the main cost is the cost of switching for current users of this (and for them to first find out that the old link no longer gives them the latest packages, at the time we would decide to stop uploading there as well)"],"labels":["Type: enhancement","Component: Python"]},{"title":"MINOR: [Documentation] Update Java Dictionary snippet","body":"Fixed Dictionary snipped on Java docs","comments":[],"labels":["Component: Documentation","awaiting review"]},{"title":"[Python] Abstract schema visitor for pa.Schema ","body":"### Describe the enhancement requested\r\n\r\nCan arrow provide a abstract class for visiting schema so libraries can make use of it ? \r\n\r\nMany libraries often have their own Schema class for representing the data and often they have to convert them back and forth between arrow and their own Schema. There are about 33 defined for arrow and implementing a visitor is repetitive. \r\n\r\nIt would be great if Arrow could provide a abstract class. \r\n\r\n```python\r\n\r\nclass AbstractSchemaVisitor(T, abc) : \r\n     def visit(schema) -> List[T]:\r\n            ... \r\n\r\n     def visit_field(field): \r\n           # Check the type of the field (one among 33 types) and call visit_<type>(field) method\r\n            ... \r\n     \r\n     @abstractmethod\r\n     def visit_int8(field):\r\n           ...\r\n     \r\n     # Visit fields for other types\r\n     ....\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Add ListView conversion to numpy\/pandas","body":"### Describe the enhancement requested\n\nAdd conversion to numpy and pandas from ListView and LargeListView types in PyArrow.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[R] Platform-dependent hashes of parquet files?","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nMoving this from ROpenSci slack. Our team has Mac, Linux, and Windows users, and we have found that we get three different hashes when saving parquet files. \r\n\r\n```\r\narrow::write_parquet(mtcars, \"mtcars.parquet\")\r\ndigest::digest(\"mtcars.parquet\", file = TRUE)\r\n```\r\nMac \"05be83226acb5d2a673d922ff9f69414\"\r\nLinux \"8bddf47bdbede54d87ec3c4cbec280da\"\r\nWindows \"bef251d299843f07348248416572edab\"\r\n\r\nWhen uncompressed, we get the same hashes for Linux and Windows, different for Mac.\r\n```\r\narrow::write_parquet(mtcars, \"mtcars.parquet\", compression = \"uncompressed\" )\r\ndigest::digest(\"mtcars.parquet\", file = TRUE)\r\n```\r\nMac \"58ec2e7a6d614db15fc2123455a83a7e\"\r\nLinux \"4f3f049ffebdb395c489864e90d5e36b\"\r\nWindows \"4f3f049ffebdb395c489864e90d5e36b\"\r\n\r\n`arrow_info()` for our three systems:\r\n\r\n<details>\r\n<summary> Mac <\/summary>\r\n\r\n```\r\nArrow package version: 14.0.0.2\r\n\r\nCapabilities:\r\n               \r\nacero      TRUE\r\ndataset    TRUE\r\nsubstrait FALSE\r\nparquet    TRUE\r\njson       TRUE\r\ns3         TRUE\r\ngcs        TRUE\r\nutf8proc   TRUE\r\nre2        TRUE\r\nsnappy     TRUE\r\ngzip       TRUE\r\nbrotli     TRUE\r\nzstd       TRUE\r\nlz4        TRUE\r\nlz4_frame  TRUE\r\nlzo       FALSE\r\nbz2        TRUE\r\njemalloc   TRUE\r\nmimalloc   TRUE\r\n\r\nMemory:\r\n                  \r\nAllocator mimalloc\r\nCurrent    0 bytes\r\nMax       50.62 Kb\r\n\r\nRuntime:\r\n                        \r\nSIMD Level          none\r\nDetected SIMD Level none\r\n\r\nBuild:\r\n                                                             \r\nC++ Library Version                                    14.0.0\r\nC++ Compiler                                       AppleClang\r\nC++ Compiler Version                          15.0.0.15000040\r\nGit ID               2dcee3f82c6cf54b53a64729fd81840efa583244\r\n```\r\n<\/details>\r\n\r\n<details>\r\n<summary> Linux <\/summary>\r\n\r\n```\r\nArrow package version: 14.0.0.2\r\n\r\nCapabilities:\r\n               \r\nacero      TRUE\r\ndataset    TRUE\r\nsubstrait FALSE\r\nparquet    TRUE\r\njson       TRUE\r\ns3         TRUE\r\ngcs        TRUE\r\nutf8proc   TRUE\r\nre2        TRUE\r\nsnappy     TRUE\r\ngzip       TRUE\r\nbrotli     TRUE\r\nzstd       TRUE\r\nlz4        TRUE\r\nlz4_frame  TRUE\r\nlzo       FALSE\r\nbz2        TRUE\r\njemalloc   TRUE\r\nmimalloc   TRUE\r\n\r\nMemory:\r\n                  \r\nAllocator jemalloc\r\nCurrent    0 bytes\r\nMax        0 bytes\r\n\r\nRuntime:\r\n                        \r\nSIMD Level          avx2\r\nDetected SIMD Level avx2\r\n\r\nBuild:\r\n                           \r\nC++ Library Version  14.0.0\r\nC++ Compiler            GNU\r\nC++ Compiler Version 11.4.0\r\n```\r\n<\/details>\r\n\r\n<details>\r\n<summary> Windows <\/summary>\r\n\r\n```\r\nArrow package version: 14.0.0.2\r\n\r\nCapabilities:\r\n               \r\nacero      TRUE\r\ndataset    TRUE\r\nsubstrait FALSE\r\nparquet    TRUE\r\njson       TRUE\r\ns3         TRUE\r\ngcs        TRUE\r\nutf8proc   TRUE\r\nre2        TRUE\r\nsnappy     TRUE\r\ngzip       TRUE\r\nbrotli     TRUE\r\nzstd       TRUE\r\nlz4        TRUE\r\nlz4_frame  TRUE\r\nlzo       FALSE\r\nbz2        TRUE\r\njemalloc  FALSE\r\nmimalloc   TRUE\r\n\r\nArrow options():\r\n                       \r\narrow.use_threads FALSE\r\n\r\nMemory:\r\n                  \r\nAllocator mimalloc\r\nCurrent    0 bytes\r\nMax        0 bytes\r\n\r\nRuntime:\r\n                        \r\nSIMD Level          avx2\r\nDetected SIMD Level avx2\r\n\r\nBuild:\r\n                                                             \r\nC++ Library Version                                    14.0.0\r\nC++ Compiler                                              GNU\r\nC++ Compiler Version                                   10.3.0\r\nGit ID               2dcee3f82c6cf54b53a64729fd81840efa583244\r\n```\r\n<\/details>\r\n\r\n\r\n### Component(s)\r\n\r\nParquet, R","comments":["Could you try using [parquet-tools](https:\/\/pypi.org\/project\/parquet-tools\/) or [parquet cli](https:\/\/formulae.brew.sh\/formula\/parquet-tools) to inspect the different files and see if there are any differences (if you can, posting the output here for each would be helpful)\r\n\r\nI suspect there are differences due to compression or differences between default layouts that would cause different hashes to files like these.","Got identical results for the three, other than difference in space saved value.\r\n\r\n<details>\r\n<summary> Mac <\/summary>\r\n\r\n```\r\n############ file meta data ############\r\ncreated_by: parquet-cpp-arrow version 14.0.0\r\nnum_columns: 11\r\nnum_rows: 32\r\nnum_row_groups: 1\r\nformat_version: 2.6\r\nserialized_size: 2823\r\n\r\n\r\n############ Columns ############\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n\r\n############ Column(mpg) ############\r\nname: mpg\r\npath: mpg\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 22%)\r\n\r\n############ Column(cyl) ############\r\nname: cyl\r\npath: cyl\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(disp) ############\r\nname: disp\r\npath: disp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(hp) ############\r\nname: hp\r\npath: hp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(drat) ############\r\nname: drat\r\npath: drat\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 9%)\r\n\r\n############ Column(wt) ############\r\nname: wt\r\npath: wt\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 12%)\r\n\r\n############ Column(qsec) ############\r\nname: qsec\r\npath: qsec\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 12%)\r\n\r\n############ Column(vs) ############\r\nname: vs\r\npath: vs\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(am) ############\r\nname: am\r\npath: am\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(gear) ############\r\nname: gear\r\npath: gear\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(carb) ############\r\nname: carb\r\npath: carb\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 8%\r\n```\r\n<\/details>\r\n\r\n<details>\r\n<summary> Linux <\/summary>\r\n\r\n```\r\n############ file meta data ############\r\ncreated_by: parquet-cpp-arrow version 14.0.0\r\nnum_columns: 11\r\nnum_rows: 32\r\nnum_row_groups: 1\r\nformat_version: 2.6\r\nserialized_size: 2823\r\n\r\n\r\n############ Columns ############\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n\r\n############ Column(mpg) ############\r\nname: mpg\r\npath: mpg\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 22%)\r\n\r\n############ Column(cyl) ############\r\nname: cyl\r\npath: cyl\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(disp) ############\r\nname: disp\r\npath: disp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(hp) ############\r\nname: hp\r\npath: hp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(drat) ############\r\nname: drat\r\npath: drat\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 9%)\r\n\r\n############ Column(wt) ############\r\nname: wt\r\npath: wt\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 12%)\r\n\r\n############ Column(qsec) ############\r\nname: qsec\r\npath: qsec\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 13%)\r\n\r\n############ Column(vs) ############\r\nname: vs\r\npath: vs\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(am) ############\r\nname: am\r\npath: am\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(gear) ############\r\nname: gear\r\npath: gear\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(carb) ############\r\nname: carb\r\npath: carb\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 8%)\r\n```\r\n<\/details>\r\n\r\n<details>\r\n<summary> Windows <\/summary>\r\n\r\n```\r\n############ file meta data ############\r\ncreated_by: parquet-cpp-arrow version 14.0.0\r\nnum_columns: 11\r\nnum_rows: 32\r\nnum_row_groups: 1\r\nformat_version: 2.6\r\nserialized_size: 2823\r\n\r\n\r\n############ Columns ############\r\nmpg\r\ncyl\r\ndisp\r\nhp\r\ndrat\r\nwt\r\nqsec\r\nvs\r\nam\r\ngear\r\ncarb\r\n\r\n############ Column(mpg) ############\r\nname: mpg\r\npath: mpg\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 22%)\r\n\r\n############ Column(cyl) ############\r\nname: cyl\r\npath: cyl\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(disp) ############\r\nname: disp\r\npath: disp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(hp) ############\r\nname: hp\r\npath: hp\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 20%)\r\n\r\n############ Column(drat) ############\r\nname: drat\r\npath: drat\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 9%)\r\n\r\n############ Column(wt) ############\r\nname: wt\r\npath: wt\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 12%)\r\n\r\n############ Column(qsec) ############\r\nname: qsec\r\npath: qsec\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 13%)\r\n\r\n############ Column(vs) ############\r\nname: vs\r\npath: vs\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(am) ############\r\nname: am\r\npath: am\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: -4%)\r\n\r\n############ Column(gear) ############\r\nname: gear\r\npath: gear\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 0%)\r\n\r\n############ Column(carb) ############\r\nname: carb\r\npath: carb\r\nmax_definition_level: 1\r\nmax_repetition_level: 0\r\nphysical_type: DOUBLE\r\nlogical_type: None\r\nconverted_type (legacy): NONE\r\ncompression: SNAPPY (space_saved: 8%)\r\n```\r\n<\/details>","Thanks for the help here, @emmamendelsohn. Could you zip up all three Parquet files and attach them here?","I managed to reproduce getting different checksums for files written using macOS and Linux and am attaching them here in case anyone wants to take a look: [mtcars-parquet.zip](https:\/\/github.com\/apache\/arrow\/files\/14380370\/mtcars-parquet.zip). Both were written with `arrow::write_parquet(mtcars, \"mtcars.parquet\", compression = \"uncompressed\")` using arrow R 14.0.0.2.\r\n\r\nWhen I run parquet-tools inspect on each file with --detail, I get two differences in output. The first is some unlabeled number that's either 262658 or 262914 (diff of 256 which is a bit conspicuous) depending on the file and the second difference is in the KeyValue metadata for the `ARROW:schema` key. I wonder if the two differences are related.","Here are the three files for the compressed example (`arrow::write_parquet(mtcars, \"mtcars.parquet\")`). With `--detail` I see there are differences in file and page offsets. \r\n\r\n[snappy-mtcars-parquet.zip](https:\/\/github.com\/apache\/arrow\/files\/14386357\/snappy-mtcars-parquet.zip)\r\n","I am not surprised by difference in compression depending on the exact version of the compression library (Snappy), which also depends on the platform and the Arrow version numbers.\r\n","Ok, the uncompressed difference is in the R-specific metadata that's stored with Arrow tables. Either @nealrichardson @jonkeane or @paleolimbot would probably be able to explain what it's about, and why it may vary from platform to platform.","And, yeah, the format of the \"r\" metadata is very similar to the example showed in http:\/\/richfitz.github.io\/redux\/reference\/object_to_string.html\r\n\r\nUnder PyArrow:\r\n```python\r\n>>> a = pq.read_table(\"\/home\/antoine\/arrow\/data\/mtcars-linux-uncompressed.parquet\")\r\n>>> b = pq.read_table(\"\/home\/antoine\/arrow\/data\/mtcars-macos-uncompressed.parquet\")\r\n>>> a.schema.metadata\r\n{b'r': b'A\\n3\\n262658\\n197888\\n5\\nUTF-8\\n531\\n1\\n531\\n11\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n1026\\n1\\n262153\\n5\\nnames\\n16\\n11\\n262153\\n3\\nmpg\\n262153\\n3\\ncyl\\n262153\\n4\\ndisp\\n262153\\n2\\nhp\\n262153\\n4\\ndrat\\n262153\\n2\\nwt\\n262153\\n4\\nqsec\\n262153\\n2\\nvs\\n262153\\n2\\nam\\n262153\\n4\\ngear\\n262153\\n4\\ncarb\\n254\\n1026\\n511\\n16\\n1\\n262153\\n7\\ncolumns\\n254\\n'}\r\n>>> b.schema.metadata\r\n{b'r': b'A\\n3\\n262914\\n197888\\n5\\nUTF-8\\n531\\n1\\n531\\n11\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n254\\n1026\\n1\\n262153\\n5\\nnames\\n16\\n11\\n262153\\n3\\nmpg\\n262153\\n3\\ncyl\\n262153\\n4\\ndisp\\n262153\\n2\\nhp\\n262153\\n4\\ndrat\\n262153\\n2\\nwt\\n262153\\n4\\nqsec\\n262153\\n2\\nvs\\n262153\\n2\\nam\\n262153\\n4\\ngear\\n262153\\n4\\ncarb\\n254\\n1026\\n511\\n16\\n1\\n262153\\n7\\ncolumns\\n254\\n'}\r\n>>> a.schema.metadata == b.schema.metadata\r\nFalse\r\n```","By the way, `262658` is 0x40202 while `262914` is 0x40302, so this might very well be dependent on the R version you generated those files with (4.2.2 vs. 4.3.2?). Probably easy to verify.","All files from my example with R 4.3.2. ","@emmamendelsohn Ah, I was talking about the uncompressed example from @amoeba . As I said above, differences in compressed files should not be a surprise. Do you still see differences if you generate uncompressed files?","I see. Yes for uncompressed we found Linux and Windows had the same hash, while macOS was different, all on 4.3.2. Let me know if you'd like me to share those files. ","Thank you! Yes, you can share the Linux and macOS files for example.\r\n\r\n(I suspect the final reason will be similar: slightly different R metadata serialized, for which I'll let R-Arrow experts answer :-))","Thanks for looking at this @pitrou, the R version and metadata causing the issue makes sense. I'll look into what we're doing in that regard next.","Actually, I was mistaken, all three systems have different hashes when uncompressed. This matches @amoeba's example above. \r\n[uncompressed-mtcars-parquet.zip](https:\/\/github.com\/apache\/arrow\/files\/14486090\/uncompressed-mtcars-parquet.zip)\r\n","Thanks @emmamendelsohn . After taking a quick look:\r\n1) all three files differ only in the Parquet metadata, not the actual data\r\n2) once deserialized, the Arrow schema is the same, except for R metadata (depending on R version perhaps: it might have been 4.3.3 on Linux vs. 4.3.2 on Windows and Mac?)\r\n3) hence, most of the difference seems to be in the way the Arrow schema is serialized by [flatbuffers](https:\/\/github.com\/google\/flatbuffers). This is certainly harmless as long as the data is the same once deserialized.\r\n\r\nIs there a particular reason you were wondering about these files being different?\r\n","This is an [interesting flatbuffers commit message](https:\/\/github.com\/google\/flatbuffers\/commit\/f575b02fda04fe579fb23442234feb8129b77ee2) as we do have a [similar piece of code](https:\/\/github.com\/apache\/arrow\/blob\/3ba6d286caad328b8572a3b9228045da8c8d2043\/cpp\/src\/arrow\/ipc\/metadata_internal.cc#L480). And binary inspection of the serialized Flatbuffers metadata seems to match this interpretation.","@pitrou the different hashes became an issue for our team using a collaborative R `targets` workflow. In short, we use a shared S3 bucket for object storage so that each user can easily access the same versioned objects. This is especially useful for things like model objects that take a long time to produce. However, for large raw data files, we've found that the cost of transferring to\/from AWS is too high, so each user saves the files locally as parquets. The `targets` version tracking system needs to register that these local files have the expected hash to be able to run downstream endpoints. When the file hashes differ across systems, `targets` detects a change and invalidates subsequent endpoints. \r\n\r\nAnyway, we're [rethinking](https:\/\/github.com\/ropensci\/targets\/discussions\/1232) some aspects of this approach, and so this may not be relevant in the future. Appreciate you looking into it nonetheless!\r\n","Yes, I think you should probably reconsider, because it is not realistic to expect a sophisticated compression-based format like Parquet to always generate the same bitwise data using slightly different producers.","Makes sense!","Would @nealrichardson @jonkeane or @paleolimbot be able to explain the R-specific metadata that generated maybe point to the code in the package where this occurs? From a quick inspection it looks a summary of the data frame schema in R's ASCII serialization format.","@noamross it looks like we do that here https:\/\/github.com\/apache\/arrow\/blob\/9ca7d787402c715ee84c1bb21cfca0e54ae2f12d\/r\/R\/metadata.R#L19-L33\r\n\r\n(calling into `serialize` as you guessed)","@noamross IIRC the purpose of this is so that object attributes, including R class names, is preserved so that you can round-trip the data to parquet or arrow files and get the same R types back. If you had a bare data.frame and only vanilla R vector types, I would expect the metadata to be empty."],"labels":["Type: bug","Component: R","Component: Parquet"]},{"title":"[Python] Unable to filter datasets on __fragment_index?","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nHey,\r\nI have a dataset that I'd like to filter using `__fragment_index` and\/or `__filename`:\r\n\r\n```python\r\n> from pyarrow import dataset\r\n> ds = dataset.parquet_dataset(\"bucket\/dataset_path\/_metadata\", filesystem=S3_FS)\r\n> ds.scanner(columns=['col_1', '__filename'], filters=dataset.field('__fragment_index') == 1)\r\nArrowInvalid: No match for FieldRef.Name(__fragment_index) in col_1: string\r\ncol_2: string\r\ncol_3: string\r\n```\r\n\r\nThe docstrings for `scanner` _imply_ that this is possible:\r\n\r\n> The list of columns or expressions may use the special fields\r\n> `__batch_index` (the index of the batch within the fragment),\r\n> `__fragment_index` (the index of the fragment within the dataset),\r\n> `__last_in_fragment` (whether the batch is last in fragment), and\r\n> `__filename` (the name of the source file or a description of the\r\n> source fragment).\r\n\r\nHowever I can't construct a `dataset.Expression` that references any of the special fields, and I can't see any tests that utilise this.\r\n\r\nAm I doing something silly?\n\n### Component(s)\n\nPython","comments":["Looking at the docs, _I think_ this is possible only for `columns` parameter, not `filter` (there is a typo in your code: `filters`).  This test provides the best use of it in my opinion: https:\/\/github.com\/apache\/arrow\/blob\/c57115de8d9b844f1929f8395d29c61ca2269b24\/python\/pyarrow\/tests\/test_dataset.py#L449-L494\r\n\r\nWhat if you move the expression to the `columns` parameter, does it work?"],"labels":["Component: Python","Type: usage"]},{"title":"[Release][Ruby] Investigate why we require to install libffi manually on a new M2 to run verification","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nAs discussed here:\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/39960#discussion_r1491520898\r\n\r\nI did require to add:\r\n```\r\n   # libffi required for gobject-introspection gem\r\n   brew install libffi\r\n```\r\nIn order to run verification successfully on a new macOS ARM (M2). Otherwise we got the following error:\r\n```\r\nResolving dependencies...\r\nFetching power_assert 2.0.3\r\nFetching native-package-installer 1.1.9\r\nFetching pkg-config 1.5.6\r\nInstalling power_assert 2.0.3\r\nInstalling native-package-installer 1.1.9\r\nInstalling pkg-config 1.5.6\r\nFetching test-unit 3.6.2\r\nFetching glib2 4.2.0\r\nInstalling test-unit 3.6.2\r\nInstalling glib2 4.2.0 with native extensions\r\nFetching gobject-introspection 4.2.0\r\nInstalling gobject-introspection 4.2.0 with native extensions\r\nGem::Ext::BuildError: ERROR: Failed to build gem native extension.\r\n\r\ncurrent directory:\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/gobject-introspection-4.2.0\/ext\/gobject-introspection\r\n\/opt\/homebrew\/opt\/ruby\/bin\/ruby extconf.rb\r\nchecking for --enable-debug-build option... no\r\nchecking for -Wall option to compiler... yes\r\nchecking for -Wcast-align option to compiler... yes\r\nchecking for -Wextra option to compiler... yes\r\nchecking for -Wformat=2 option to compiler... yes\r\nchecking for -Winit-self option to compiler... yes\r\nchecking for -Wlarger-than-65500 option to compiler... yes\r\nchecking for -Wmissing-declarations option to compiler... yes\r\nchecking for -Wmissing-format-attribute option to compiler... yes\r\nchecking for -Wmissing-include-dirs option to compiler... yes\r\nchecking for -Wmissing-noreturn option to compiler... yes\r\nchecking for -Wmissing-prototypes option to compiler... yes\r\nchecking for -Wnested-externs option to compiler... yes\r\nchecking for -Wold-style-definition option to compiler... yes\r\nchecking for -Wpacked option to compiler... yes\r\nchecking for -Wp,-D_FORTIFY_SOURCE=2 option to compiler... yes\r\nchecking for -Wpointer-arith option to compiler... yes\r\nchecking for -Wundef option to compiler... yes\r\nchecking for -Wout-of-line-declaration option to compiler... yes\r\nchecking for -Wunsafe-loop-optimizations option to compiler... no\r\nchecking for -Wwrite-strings option to compiler... yes\r\nchecking for Homebrew... yes\r\nchecking for gobject-introspection-1.0... yes (1.78.1)\r\ncreating rbgiversion.h\r\ncreating gobject-introspection-enum-types.c\r\ncreating gobject-introspection-enum-types.h\r\ncreating ruby-gobject-introspection.pc\r\ncreating Makefile\r\n\r\ncurrent directory:\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/gobject-introspection-4.2.0\/ext\/gobject-introspection\r\nmake DESTDIR\\= sitearchdir\\=.\/.gem.20240216-45788-684atu sitelibdir\\=.\/.gem.20240216-45788-684atu clean\r\n\r\ncurrent directory:\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/gobject-introspection-4.2.0\/ext\/gobject-introspection\r\nmake DESTDIR\\= sitearchdir\\=.\/.gem.20240216-45788-684atu sitelibdir\\=.\/.gem.20240216-45788-684atu\r\ncompiling gobject-introspection-enum-types.c\r\ncompiling rb-gi-arg-info.c\r\nIn file included from rb-gi-arg-info.c:21:\r\nIn file included from .\/rb-gi-private.h:24:\r\nIn file included from\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/glib2-4.2.0\/ext\/glib2\/..\/..\/lib\/rbgobject.h:27:\r\nIn file included from\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/glib2-4.2.0\/ext\/glib2\/..\/..\/lib\/rbgutil.h:26:\r\nIn file included from \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/include\/ruby-3.3.0\/ruby\/encoding.h:22:\r\n\/opt\/homebrew\/Cellar\/ruby\/3.3.0\/include\/ruby-3.3.0\/ruby\/internal\/encoding\/ctype.h:82:46: warning: unused parameter 'enc' [-Wunused-parameter]\r\nrb_enc_isascii(OnigCodePoint c, rb_encoding *enc)\r\n                                             ^\r\nIn file included from rb-gi-arg-info.c:21:\r\nIn file included from .\/rb-gi-private.h:28:\r\n\/opt\/homebrew\/Cellar\/gobject-introspection\/1.78.1\/include\/gobject-introspection-1.0\/girffi.h:25:10: fatal error: 'ffi.h' file not found\r\n#include <ffi.h>\r\n         ^~~~~~~\r\n1 warning and 1 error generated.\r\nmake: *** [rb-gi-arg-info.o] Error 1\r\n\r\nmake failed, exit code 2\r\n\r\nGem files will remain installed in\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/gems\/gobject-introspection-4.2.0 for inspection.\r\nResults logged to\r\n\/private\/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O\/apache-arrow-15.0.0\/c_glib\/vendor\/bundle\/ruby\/3.3.0\/extensions\/arm64-darwin-23\/3.3.0\/gobject-introspection-4.2.0\/gem_make.out\r\n\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:125:in `run'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:51:in `block in make'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:43:in `each'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:43:in `make'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/ext_conf_builder.rb:42:in `build'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:193:in `build_extension'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:227:in `block in build_extensions'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:224:in `each'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/ext\/builder.rb:224:in `build_extensions'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/rubygems\/installer.rb:852:in `build_extensions'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/rubygems_gem_installer.rb:76:in `build_extensions'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/rubygems_gem_installer.rb:28:in `install'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/source\/rubygems.rb:205:in `install'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/installer\/gem_installer.rb:54:in `install'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/installer\/gem_installer.rb:16:in `install_from_spec'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/installer\/parallel_installer.rb:132:in `do_install'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/installer\/parallel_installer.rb:123:in `block in worker_pool'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/worker.rb:62:in `apply_func'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/worker.rb:57:in `block in process_queue'\r\n  <internal:kernel>:187:in `loop'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/worker.rb:54:in `process_queue'\r\n  \/opt\/homebrew\/Cellar\/ruby\/3.3.0\/lib\/ruby\/3.3.0\/bundler\/worker.rb:90:in `block (2 levels) in create_threads'\r\n\r\nAn error occurred while installing gobject-introspection (4.2.0), and Bundler cannot continue.\r\n\r\nIn Gemfile:\r\n  gobject-introspection\r\nFailed to verify release candidate. See \/var\/folders\/rr\/t_15q7dx4_lf1nv0btypffcw0000gn\/T\/arrow-15.0.0.XXXXX.li0gqOMB3O for details.\r\n\r\n```\r\n\r\nSome info on the same comment thread:\r\n```\r\n% pkg-config --cflags gobject-introspection-1.0\r\n-I\/opt\/homebrew\/Cellar\/gobject-introspection\/1.78.1\/include\/gobject-introspection-1.0 -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/include -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/include\/glib-2.0 -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/lib\/glib-2.0\/include -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/Cellar\/pcre2\/10.42\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi\r\n\r\nraul@raul-MacBook-Pro % ls -l \/Library\/Developer\/CommandLineTools\/SDKs\/\r\ntotal 0\r\ndrwxr-xr-x  7 root  wheel  224 Nov 19  2021 MacOSX12.1.sdk\r\ndrwxr-xr-x  3 root  wheel   96 Feb 10 14:11 MacOSX12.3.sdk\r\nraul@raul-MacBook-Pro % sw_vers -productVersion\r\n14.3.1\r\n```\r\nAfter manually adding the link `sudo ln -s MacOSX12.3.sdk \/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk` it still failed.\n\n### Component(s)\n\nRelease, Ruby","comments":["Could you show the output of the following command line after you install `pkg-config` gem by `PATH=$(brew --prefix ruby)\/bin:$PATH gem install pkg-config`?\r\n\r\n```bash\r\nPATH=$(brew --prefix ruby)\/bin:$PATH ruby -r pkg-config -e 'p PKGConfig.cflags(\"gobject-introspection-1.0\")'\r\n```","Could you also show the output of the following command line?\r\n\r\n```bash\r\npkg-config --debug --cflags libffi\r\n```","Sorry @kou , I forgot about this:\r\n```\r\nraul@raul-MacBook-Pro release % PATH=$(brew --prefix ruby)\/bin:$PATH ruby -r pkg-config -e 'p PKGConfig.cflags(\"gobject-introspection-1.0\")'\r\n\"-I\/opt\/homebrew\/Cellar\/gobject-introspection\/1.78.1\/include\/gobject-introspection-1.0 -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/include\/glib-2.0 -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/lib\/glib-2.0\/include -I\/opt\/homebrew\/opt\/gettext\/include -I\/opt\/homebrew\/Cellar\/pcre2\/10.42\/include -I\/opt\/homebrew\/Cellar\/glib\/2.78.4\/include -I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi\"\r\nraul@raul-MacBook-Pro release % pkg-config --debug --cflags libffi\r\nError printing enabled by default due to use of output options besides --exists, --atleast\/exact\/max-version or --list-all. Value of --silence-errors: 0\r\nError printing enabled\r\nAdding virtual 'pkg-config' package to list of known packages\r\nLooking for package 'libffi'\r\nLooking for package 'libffi-uninstalled'\r\nReading 'libffi' from file '\/opt\/homebrew\/Library\/Homebrew\/os\/mac\/pkgconfig\/12\/libffi.pc'\r\nParsing package file '\/opt\/homebrew\/Library\/Homebrew\/os\/mac\/pkgconfig\/12\/libffi.pc'\r\n  line>homebrew_sdkroot=\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\r\n Variable declaration, 'homebrew_sdkroot' has value '\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk'\r\n  line>prefix=${homebrew_sdkroot}\/usr\r\n Variable declaration, 'prefix' has value '\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr'\r\n  line>exec_prefix=\/usr\r\n Variable declaration, 'exec_prefix' has value '\/usr'\r\n  line>libdir=${exec_prefix}\/lib\r\n Variable declaration, 'libdir' has value '\/usr\/lib'\r\n  line>toolexeclibdir=${libdir}\r\n Variable declaration, 'toolexeclibdir' has value '\/usr\/lib'\r\n  line>includedir=${prefix}\/include\/ffi\r\n Variable declaration, 'includedir' has value '\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi'\r\n  line>\r\n  line>Name: libffi\r\n  line>Description: Library supporting Foreign Function Interfaces\r\n  line>Version: 3.4-rc1\r\n  line>Libs: -L${toolexeclibdir} -lffi\r\n  line>Cflags: -I${includedir}\r\nPath position of 'libffi' is 5\r\nAdding 'libffi' to list of known packages\r\nPackage libffi has -L \/usr\/lib in Libs\r\nRemoving -L \/usr\/lib from libs for libffi\r\n post-recurse: libffi\r\nadding CFLAGS_OTHER string \"\"\r\n post-recurse: libffi\r\n original: libffi\r\n   sorted: libffi\r\nadding CFLAGS_I string \"-I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi \"\r\nreturning flags string \"-I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi\"\r\n-I\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi\r\nraul@raul-MacBook-Pro release % \r\n```","Thanks for remembering this!\r\n\r\nIt seems that detected `-I`s are expected. `ffi.h` may not exist in `\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi`.\r\n\r\nCould you show the output of the following command lines?\r\n\r\n```bash\r\nls -lah \/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\r\nls -lah \/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX12.sdk\/usr\/include\/ffi\r\n```"],"labels":["Type: bug","Component: Ruby","Component: Release"]},{"title":"[R] Arrays containing -2147483648 are converted to NA in R","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nR uses -2147483648 (int32_min) to represent missing integer values (`NA`). When converting Arrow arrays to R using the C API and then casting to R vectors using `as.vector`, arrays containing -2147483649 and below are converted to bit64::integer64, but if the minimum value of the array is exactly -2147483648, all the -2147483648s are converted to NA. This is an edge case but it's an important one, because int32_min is often used as a special sentinel value. \r\n\r\nArrow should update the out-of-range check that decides whether to convert to bit64::integer64, to use -2147483647 as the minimum valid int32 rather than -2147483648.\n\n### Component(s)\n\nR","comments":["Reprex:\r\n\r\n``` r\r\nlibrary(arrow, warn.conflicts = FALSE)\r\n\r\narrow_array(\"-2147483648\")$cast(int32()) |> as.vector()\r\n#> [1] NA\r\n```\r\n\r\nThis conversion happens in one of a few places, depending on whether `options(arrow.use_altrep = TRUE)` and how the caller of the R C API is consuming the ALTREP array (by `INTEGER_ELT()`, `INTEGER_GET_REGION()`, or `DATAPTR_RO()`.\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/214378b522a36fbf6010e3d4f5470abaca7bf92e\/r\/src\/array_to_vector.cpp#L193\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/214378b522a36fbf6010e3d4f5470abaca7bf92e\/r\/src\/altrep.cpp#L282-L283\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/214378b522a36fbf6010e3d4f5470abaca7bf92e\/r\/src\/altrep.cpp#L310-L311\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/214378b522a36fbf6010e3d4f5470abaca7bf92e\/r\/src\/altrep.cpp#L259-L262\r\n\r\nChecking for specific int32 values is potentially expensive (but safer) in the ALTREP scenario...technically there would be an identical problem with int64 conversions to R's integer64 class.\r\n\r\nIt looks like nanoarrow has an identical problem here:\r\n\r\n``` r\r\nlibrary(arrow, warn.conflicts = FALSE)\r\n\r\narrow_array(\"-2147483648\")$cast(int32()) |>\r\n  nanoarrow::as_nanoarrow_array() |> \r\n  as.vector()\r\n#> [1] NA\r\n```\r\n"],"labels":["Type: bug","Component: R"]},{"title":"GH-#40188 :C# Csharp setnull","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nWhen we're filling out columns, we sometimes want to set values as null.  This is currently implemented in other languages, but not in c#.\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nI implemented SetNull in a few places!\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nYes, they seem to work fine.  I'm open to further testing if other people have any ideas.\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nUsers can use SetNull!\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->","comments":["<!--\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\nThanks for opening a pull request!\n\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\n\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\n\nThen could you also rename the pull request title in the following format?\n\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nor\n\n    MINOR: [${COMPONENT}] ${SUMMARY}\n\nIn the case of PARQUET issues on JIRA the title also supports:\n\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\n\nSee also:\n\n  * [Other pull requests](https:\/\/github.com\/apache\/arrow\/pulls\/)\n  * [Contribution Guidelines - Contributing Overview](https:\/\/arrow.apache.org\/docs\/developers\/overview.html)\n"],"labels":["Component: C#","awaiting committer review"]},{"title":"[C#] Add SetNull functionality to classes","body":"### Describe the enhancement requested\n\nCurrently, we find ourselves only able to Set valid values, whereas sometimes we may  find ourselves wanting to set a given index as null instead, and adjust the relevant validity index.\r\n\r\nThe ensuing code implements SetNull on a small set of types, but should get people started.\n\n### Component(s)\n\nC#","comments":["https:\/\/github.com\/apache\/arrow\/pull\/40189","I think you can achieve what you're looking for by using Reserve instead of Resize and growing the builder allocations in chunks. I have a sample program that shows this using a TPC-H table stored in SQL Server. You can see that at [https:\/\/gist.github.com\/CurtHagenlocher\/306865d4b4202906470f4f18fd410c4e](https:\/\/gist.github.com\/CurtHagenlocher\/306865d4b4202906470f4f18fd410c4e).\r\n\r\nI don't think there's anything wrong with adding SetNull to the builders, but on the whole I don't find them very ergonomical. In my sample code, for instance, I have to resize each array directly because Reserve and Resize are defined on a typed array interface instead of a shared base interface. I imagine this was done to allow \"fluent\"-style construction and that's not something I find compelling. In any event, backwards compatibility is very important so changing what's there isn't really an option.\r\n\r\nAn alternative change which might help this scenario is to add constructors to the builders which allow specification of the initial size and then to double the size once the capacity is reached instead of just growing it by one, or to otherwise allow encapsulation of a \"grow builder\" strategy that's a little more practical than the default."],"labels":["Type: enhancement","Component: C#"]},{"title":"GH-31315: [C++][Docs] Document that the strptime kernel ignores %Z","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nDocument that the strptime kernel ignores %Z\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nDocuments that the strptime kernel ignores %Z  for any string\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nNo\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nYes\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\n* GitHub Issue: #31315","comments":["I am guessing the checks are failing because of the clang format.\r\n```\r\n+++ \/arrow\/cpp\/src\/arrow\/compute\/kernels\/scalar_temporal_unary.cc (after clang format)\r\n@@ -1784,8 +1784,7 @@\r\n      \"in StrptimeOptions. Null inputs emit null. If a non-null string\\n\"\r\n      \"fails parsing, an error is returned by default.\\n\"\r\n      \"\\n\"\r\n-     \"**Note:** The strptime kernel currently ignores the %Z specifier for any string.\"\r\n-     ),\r\n+     \"**Note:** The strptime kernel currently ignores the %Z specifier for any string.\"),\r\n     {\"strings\"}, \"StrptimeOptions\", \/*options_required=*\/true);\r\n const FunctionDoc assume_timezone_doc{\r\n     \"Convert naive timestamp to timezone-aware timestamp\",\r\n\/arrow\/cpp\/src\/arrow\/compute\/kernels\/scalar_temporal_unary.cc had clang-format style issues\r\nninja: build stopped: subcommand failed.\r\n``` \r\n@rok when I used `clang-format -i compute.rst` command the following changes are coming and they seem wrong \r\n```diff\r\ndiff --git a\/docs\/source\/cpp\/compute.rst b\/docs\/source\/cpp\/compute.rst\r\nindex 2db5988b8..64660ec23 100644\r\n--- a\/docs\/source\/cpp\/compute.rst\r\n+++ b\/docs\/source\/cpp\/compute.rst\r\n@@ -1,4 +1,4 @@\r\n-.. Licensed to the Apache Software Foundation (ASF) under one\r\n+..Licensed to the Apache Software Foundation(ASF) under one\r\n .. or more contributor license agreements.  See the NOTICE file\r\n .. distributed with this work for additional information\r\n .. regarding copyright ownership.  The ASF licenses this file\r\n@@ -62,44 +62,41 @@ Compute functions can be invoked by name using\r\n :func:`arrow::compute::CallFunction`::\r\n \r\n    std::shared_ptr<arrow::Array> numbers_array = ...;\r\n-   std::shared_ptr<arrow::Scalar> increment = ...;\r\n-   arrow::Datum incremented_datum;\r\n+std::shared_ptr<arrow::Scalar> increment = ...;\r\n+arrow::Datum incremented_datum;\r\n \r\n-   ARROW_ASSIGN_OR_RAISE(incremented_datum,\r\n-                         arrow::compute::CallFunction(\"add\", {numbers_array, increment}));\r\n-   std::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\r\n+ARROW_ASSIGN_OR_RAISE(incremented_datum,\r\n+                      arrow::compute::CallFunction(\"add\", {numbers_array, increment}));\r\n+std::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\r\n \r\n-(note this example uses implicit conversion from ``std::shared_ptr<Array>``\r\n-to ``Datum``)\r\n+(note this example uses implicit conversion from ``std::shared_ptr<Array>`` to ``Datum``)\r\n \r\n-Many compute functions are also available directly as concrete APIs, here\r\n-:func:`arrow::compute::Add`::\r\n+    Many compute functions are also available directly as concrete APIs,\r\n+    here : func :`arrow::compute::Add`::\r\n \r\n-   std::shared_ptr<arrow::Array> numbers_array = ...;\r\n-   std::shared_ptr<arrow::Scalar> increment = ...;\r\n-   arrow::Datum incremented_datum;\r\n+               std::shared_ptr<arrow::Array> numbers_array = ...;\r\n+std::shared_ptr<arrow::Scalar> increment = ...;\r\n+arrow::Datum incremented_datum;\r\n \r\n-   ARROW_ASSIGN_OR_RAISE(incremented_datum,\r\n-                         arrow::compute::Add(numbers_array, increment));\r\n-   std::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\r\n+ARROW_ASSIGN_OR_RAISE(incremented_datum, arrow::compute::Add(numbers_array, increment));\r\n+std::shared_ptr<Array> incremented_array = std::move(incremented_datum).make_array();\r\n \r\n-Some functions accept or require an options structure that determines the\r\n-exact semantics of the function::\r\n+Some functions accept or\r\n+    require an options structure that determines the exact semantics of the function::\r\n \r\n-   ScalarAggregateOptions scalar_aggregate_options;\r\n-   scalar_aggregate_options.skip_nulls = false;\r\n+        ScalarAggregateOptions scalar_aggregate_options;\r\n+scalar_aggregate_options.skip_nulls = false;\r\n \r\n-   std::shared_ptr<arrow::Array> array = ...;\r\n-   arrow::Datum min_max;\r\n+std::shared_ptr<arrow::Array> array = ...;\r\n+arrow::Datum min_max;\r\n \r\n-   ARROW_ASSIGN_OR_RAISE(min_max,\r\n-                         arrow::compute::CallFunction(\"min_max\", {array},\r\n-                                                      &scalar_aggregate_options));\r\n+ARROW_ASSIGN_OR_RAISE(min_max, arrow::compute::CallFunction(\"min_max\", {array},\r\n+                                                            &scalar_aggregate_options));\r\n \r\n-   \/\/ Unpack struct scalar result (a two-field {\"min\", \"max\"} scalar)\r\n-   std::shared_ptr<arrow::Scalar> min_value, max_value;\r\n-   min_value = min_max.scalar_as<arrow::StructScalar>().value[0];\r\n-   max_value = min_max.scalar_as<arrow::StructScalar>().value[1];\r\n+\/\/ Unpack struct scalar result (a two-field {\"min\", \"max\"} scalar)\r\n+std::shared_ptr<arrow::Scalar> min_value, max_value;\r\n+min_value = min_max.scalar_as<arrow::StructScalar>().value[0];\r\n+max_value = min_max.scalar_as<arrow::StructScalar>().value[1];\r\n \r\n However, :ref:`Grouped Aggregations <grouped-aggregations-group-by>` are\r\n not invocable via ``CallFunction``.\r\n@@ -257,12 +254,18 @@ the input to a single output value.\r\n * \\(4) For decimal inputs, the resulting decimal will have the same\r\n   precision and scale. The result is rounded away from zero.\r\n \r\n-* \\(5) Output is a ``{\"min\": input type, \"max\": input type}`` Struct.\r\n+* \\(5) Output is a ``{\r\n+  \"min\" : input type, \"max\" : input type\r\n+}`` Struct.\r\n \r\n-  Of the interval types, only the month interval is supported, as the day-time\r\n-  and month-day-nano types are not sortable.\r\n+    Of the interval types,\r\n+    only the month interval is supported,\r\n+    as the day - time and month - day -\r\n+        nano types are not sortable.\r\n \r\n-* \\(6) Output is an array of ``{\"mode\": input type, \"count\": Int64}`` Struct.\r\n+            * \\(6)Output is an array of ``{\r\n+  \"mode\" : input type, \"count\" : Int64\r\n+}`` Struct.\r\n   It contains the *N* most common elements in the input, in descending\r\n   order, where *N* is given in :member:`ModeOptions::n`.\r\n   If two values have the same count, the smallest one comes first.\r\n@@ -394,7 +397,8 @@ equivalents above and reflects how they are implemented internally.\r\n * \\(4) For decimal inputs, the resulting decimal will have the same\r\n   precision and scale. The result is rounded away from zero.\r\n \r\n-* \\(5) Output is a ``{\"min\": input type, \"max\": input type}`` Struct array.\r\n+* \\(5) Output is a ``{\r\n+  \"min\" : input type, \"max\" : input type}`` Struct array.\r\n \r\n   Of the interval types, only the month interval is supported, as the day-time\r\n   and month-day-nano types are not sortable.\r\n@@ -440,8 +444,8 @@ Arithmetic functions\r\n ~~~~~~~~~~~~~~~~~~~~\r\n \r\n These functions expect inputs of numeric type and apply a given arithmetic\r\n-operation to each element(s) gathered from the input(s).  If any of the\r\n-input element(s) is null, the corresponding output element is null.\r\n+operation to each element(s)\r\n+gathered from the input(s).If any of the input element(s) is null, the corresponding output element is null.\r\n For binary functions, input(s) will be cast to the\r\n :ref:`common numeric type <common-numeric-type>`\r\n (and dictionary decoded, if applicable) before the operation is applied.\r\n@@ -603,13 +607,16 @@ The example values are given for default values of ``ndigits`` and ``multiple``.\r\n +-----------------------+--------------------------------------------------------------+---------------------------+\r\n | ``round_mode``        | Operation performed                                          | Example values            |\r\n +=======================+==============================================================+===========================+\r\n-| DOWN                  | Round to nearest integer less than or equal in magnitude;    | 3.2 -> 3, 3.7 -> 3,       |\r\n-|                       | also known as ``floor(x)``                                   | -3.2 -> -4, -3.7 -> -4    |\r\n-+-----------------------+--------------------------------------------------------------+---------------------------+\r\n-| UP                    | Round to nearest integer greater than or equal in magnitude; | 3.2 -> 4, 3.7 -> 4,       |\r\n-|                       | also known as ``ceil(x)``                                    | -3.2 -> -3, -3.7 -> -3    |\r\n-+-----------------------+--------------------------------------------------------------+---------------------------+\r\n-| TOWARDS_ZERO          | Get the integral part without fractional digits;             | 3.2 -> 3, 3.7 -> 3,       |\r\n+| DOWN                  | Round to nearest integer less than or equal in magnitude;\r\n+| 3.2->3, 3.7->3, | | | also known as ``floor(x)`` | -3.2->- 4,\r\n+    -3.7->- 4 |\r\n+            +-- -- -- -- -- -- -- -- -- -- -- -+-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --+-- -- -- -- -- -- -- -- -- -- -- -- -- -+ |\r\n+            UP | Round to nearest integer greater than\r\n+        or equal in magnitude;\r\n+| 3.2->4, 3.7->4, | | | also known as ``ceil(x)`` | -3.2->- 3,\r\n+    -3.7->- 3 |\r\n+        +-- -- -- -- -- -- -- -- -- -- -- -+-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --+-- -- -- -- -- -- -- -- -- -- -- -- -- -+ |\r\n+        TOWARDS_ZERO | Get the integral part without fractional digits;             | 3.2 -> 3, 3.7 -> 3,       |\r\n |                       | also known as ``trunc(x)``                                   | -3.2 -> -3, -3.7 -> -3    |\r\n +-----------------------+--------------------------------------------------------------+---------------------------+\r\n | TOWARDS_INFINITY      | Round negative values with ``DOWN`` rule,                    | 3.2 -> 4, 3.7 -> 4,       |\r\n@@ -1117,7 +1124,8 @@ String Slicing\r\n This function transforms each sequence of the array to a subsequence, according\r\n to start and stop indices, and a non-zero step (defaulting to 1).  Slicing\r\n semantics follow Python slicing semantics: the start index is inclusive,\r\n-the stop index exclusive; if the step is negative, the sequence is followed\r\n+the stop index exclusive;\r\n+if the step is negative, the sequence is followed\r\n in reverse order.\r\n \r\n +--------------------------+------------+-------------------------+-------------------------+--------------------------+---------+\r\n@@ -1427,26 +1435,30 @@ null input value is converted into a null output value.\r\n   is available).\r\n \r\n * \\(2) The field names of the output type must be the same or a subset of the\r\n-  field names of the input type; they also must have the same order. Casting to\r\n-  a subset of field names \"selects\" those fields such that each output field\r\n-  matches the data of the input field with the same name.\r\n-\r\n-* \\(3) The list offsets are unchanged, the list values are cast from the\r\n-  input value type to the output value type (if a conversion is\r\n-  available).\r\n-\r\n-* \\(4) Offsets are unchanged, the keys and values are cast from respective input\r\n-  to output types (if a conversion is available). If output type is a list of\r\n-  struct, the key field is output as the first field and the value field the\r\n-  second field, regardless of field names chosen.\r\n-\r\n-* \\(5) Any input type that can be cast to the resulting extension's storage type.\r\n-  This excludes extension types, unless being cast to the same extension type.\r\n-\r\n-Temporal component extraction\r\n-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n-\r\n-These functions extract datetime components (year, month, day, etc) from temporal types.\r\n+  field names of the input type;\r\n+they also must have the same order\r\n+    .Casting to a subset of field names \"selects\" those fields such that each output field\r\n+        matches the data of the input field with the same name.\r\n+\r\n+            * \\(3)The list offsets are unchanged,\r\n+    the list values are cast from the input value type to the output value\r\n+    type(if a conversion is available)\r\n+        .\r\n+\r\n+            * \\(4)Offsets are unchanged,\r\n+    the keys and values are cast from respective input to output\r\n+    types(if a conversion is available)\r\n+        .If output type is a list of struct,\r\n+    the key field is output as the first field and the value field the second field,\r\n+    regardless of field names chosen.\r\n+\r\n+        * \\(5)Any input type that can be cast to the resulting\r\n+    extension's storage type. This excludes extension types,\r\n+    unless being cast to the same extension type.\r\n+\r\n+    Temporal component extraction ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n+\r\n+    These functions extract datetime components(year, month, day, etc) from temporal types.\r\n For timestamps inputs with non-empty timezone, localized timestamp components will be returned.\r\n \r\n +--------------------+------------+-------------------+---------------+----------------------------+-------+\r\n@@ -1507,7 +1519,8 @@ For timestamps inputs with non-empty timezone, localized timestamp components wi\r\n   starts with the first ISO week. ISO week starts on Monday.\r\n   See `ISO 8601 week date definition`_ for more details.\r\n \r\n-* \\(3) Output is a ``{\"iso_year\": output type, \"iso_week\": output type, \"iso_day_of_week\":  output type}`` Struct.\r\n+* \\(3) Output is a ``{\r\n+  \"iso_year\" : output type, \"iso_week\" : output type, \"iso_day_of_week\" : output type}`` Struct.\r\n \r\n * \\(4) First US week has the majority (4 or more) of its days in January. US year\r\n   starts with the first US week. US week starts on Sunday.\r\n@@ -1517,9 +1530,12 @@ For timestamps inputs with non-empty timezone, localized timestamp components wi\r\n   If :member:`WeekOptions::count_from_zero` is true, dates from the current year that fall into the last ISO week\r\n   of the previous year are numbered as week 0, else week 52 or 53 if false.\r\n   If :member:`WeekOptions::first_week_is_fully_in_year` is true, the first week (week 1) must fully be in January;\r\n-  else if false, a week that begins on December 29, 30, or 31 is considered the first week of the new year.\r\n+else if false, a week that begins on December 29, 30,\r\n+    or 31 is considered the first week of the new year.\r\n \r\n-* \\(6) Output is a ``{\"year\": int64(), \"month\": int64(), \"day\": int64()}`` Struct.\r\n+            * \\(6)Output is a ``{\r\n+  \"year\" : int64(), \"month\" : int64(), \"day\" : int64()\r\n+}`` Struct.\r\n \r\n .. _ISO 8601 week date definition: https:\/\/en.wikipedia.org\/wiki\/ISO_week_date#First_week\r\n \r\n@@ -1681,7 +1697,8 @@ Associative transforms\r\n * \\(2) Duplicates are removed from the output while the original order is\r\n   maintained.\r\n \r\n-* \\(3) Output is a ``{\"values\": input type, \"counts\": Int64}`` Struct.\r\n+* \\(3) Output is a ``{\r\n+  \"values\" : input type, \"counts\" : Int64}`` Struct.\r\n   Each output element corresponds to a unique value in the input, along\r\n   with the number of times this value has appeared.\r\n \r\n\r\n``` ","I think you can start with the linter error. `clang-format` is probably not the right formatter for `.rst` files.","@github-actions crossbow submit preview-docs","```\nOnly contributors can submit requests to this bot. Please ask someone from the community for help with getting the first commit in.\nThe Archery job run can be found at: https:\/\/github.com\/apache\/arrow\/actions\/runs\/8001286916\n```","@github-actions crossbow submit preview-docs","Revision: 084fa491db8bf63b20d5fb7bce31417dbed593a2\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-8eda212e55](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-8eda212e55)\n\n|Task|Status|\n|----|------|\n|preview-docs|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-8eda212e55-github-preview-docs)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8002001925\/job\/21854327362)|","@github-actions crossbow submit preview-docs","Revision: 296f2e7f52b34f6b8d4e40bd37c962fcc20119f8\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-29833d01de](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-29833d01de)\n\n|Task|Status|\n|----|------|\n|preview-docs|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-29833d01de-github-preview-docs)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8004832371\/job\/21862980760)|","Docs preview was build, here is the [Python doc](http:\/\/crossbow.voltrondata.com\/pr_docs\/40186\/python\/generated\/pyarrow.compute.strptime.html) and the [c++ doc](\r\nhttp:\/\/crossbow.voltrondata.com\/pr_docs\/40186\/cpp\/compute.html#conversions). It seems the Python change came through ok, while you'd probably want to add the same language to the [c++ doc](https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/cpp\/compute.rst#L1330).","> Docs preview was build, here is the [Python doc](http:\/\/crossbow.voltrondata.com\/pr_docs\/40186\/python\/generated\/pyarrow.compute.strptime.html) and the [c++ doc](http:\/\/crossbow.voltrondata.com\/pr_docs\/40186\/cpp\/compute.html#conversions). It seems the Python change came through ok, while you'd probably want to add the same language to the [c++ doc](https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/cpp\/compute.rst#L1330).\r\n\r\n@rok does this change look good to you?\r\n```diff\r\ndiff --git a\/docs\/source\/cpp\/compute.rst b\/docs\/source\/cpp\/compute.rst\r\nindex e7310d2c0..82a8d2a11 100644\r\n--- a\/docs\/source\/cpp\/compute.rst\r\n+++ b\/docs\/source\/cpp\/compute.rst\r\n@@ -1327,7 +1327,7 @@ provided by a concrete function :func:`~arrow::compute::Cast`.\r\n +-----------------+------------+--------------------+------------------+--------------------------------+-------+\r\n | strftime        | Unary      | Temporal           | String           | :struct:`StrftimeOptions`      | \\(1)  |\r\n +-----------------+------------+--------------------+------------------+--------------------------------+-------+\r\n-| strptime        | Unary      | String-like        | Timestamp        | :struct:`StrptimeOptions`      |       |\r\n+| strptime        | Unary      | String-like        | Timestamp        | :struct:`StrptimeOptions`      | \\(2)  |\r\n +-----------------+------------+--------------------+------------------+--------------------------------+-------+\r\n \r\n The conversions available with ``cast`` are listed below.  In all cases, a\r\n@@ -1343,6 +1343,8 @@ null input value is converted into a null output value.\r\n \r\n .. _detailed formatting documentation: https:\/\/howardhinnant.github.io\/date\/date.html#to_stream_formatting\r\n \r\n+* \\(2) The strptime kernel currently ignores the %Z specifier for any string.\r\n+\r\n **Truth value extraction**\r\n \r\n +-----------------------------+------------------------------------+--------------+\r\n\r\n``` ","@rok please review this pr and suggest any changes if needed?\r\ncc @kou","Can you please run the preview docs build command? @kou ","@github-actions crossbow submit preview-docs","Revision: 9168d5b758033372d100125d2e64e7c486355804\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-e541a11760](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-e541a11760)\n\n|Task|Status|\n|----|------|\n|preview-docs|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-e541a11760-github-preview-docs)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/8048755108\/job\/21980762529)|"],"labels":["Component: C++","Component: Documentation","awaiting changes"]},{"title":"[CI][Packaging] Homebrew CPP job has been failing for some days","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nThe [homebrew-cpp](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986115835\/job\/21805858631) job has been failing for some days with the following output:\r\n```\r\n \/Applications\/Xcode_14.2.app\/Contents\/Developer\/Toolchains\/XcodeDefault.xctoolchain\/usr\/bin\/ranlib: file: \/usr\/local\/Cellar\/apache-arrow\/HEAD-29a0581_3\/lib\/libarrow.a(ios.mm.o) has no symbols\r\n...\r\n \/Applications\/Xcode_14.2.app\/Contents\/Developer\/Toolchains\/XcodeDefault.xctoolchain\/usr\/bin\/ranlib: file: \/usr\/local\/Cellar\/apache-arrow\/HEAD-29a0581_3\/lib\/libarrow_acero.a(swiss_join_avx2.cc.o) has no symbols\r\n...\r\nln -s ..\/..\/Cellar\/apache-arrow\/HEAD-29a0581_3\/lib\/pkgconfig\/arrow.pc arrow.pc\r\nln -s ..\/..\/Cellar\/apache-arrow\/HEAD-29a0581_3\/lib\/pkgconfig\/gandiva.pc gandiva.pc\r\nln -s ..\/..\/Cellar\/apache-arrow\/HEAD-29a0581_3\/lib\/pkgconfig\/parquet.pc parquet.pc\r\n==> Summary\r\n\ud83c\udf7a  \/usr\/local\/Cellar\/apache-arrow\/HEAD-29a0581_3: 589 files, 126.8MB, built in 9 minutes 42 seconds\r\nError: Process completed with exit code 1.\r\n```\r\nI couldn't find the error and I am not sure if the above logs say anything.\r\n\r\nThis is also failing on the maint-15.0.1 branch.\r\n\r\n### Component(s)\r\n\r\nContinuous Integration, Packaging","comments":["@kou any idea what is the issue? I can't seem to find what is happening","Hmm. The \"no symbols\" log exists on success build too. e.g.: https:\/\/zulip.clear-code.com\/#narrow\/stream\/6-.E3.82.AF.E3.83.AA.E3.82.A2.E3.82.B3.E3.83.BC.E3.83.89\/topic\/.E6.96.B0.E5.85.A5.E7.A4.BE.E5.93.A1\/near\/291673\r\nSo it may not be related.\r\n\r\nWe may need to add some debug logs... I may be able to work on this in a few days..."],"labels":["Type: bug","Component: Continuous Integration","Component: Packaging"]},{"title":"[Java] JDBC driver too large","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nIf you add flight-sql-jdbc-driver as a dependency to a java appllication it pulls in 70MB of dependencies including 8 platform specific versions of netty and 1000s of files. H2 database which includes a JDBC driver and a database engine is 2.5MB. If you want smaller, here is JDBC implemented as a single file (https:\/\/github.com\/KxSystems\/kdb\/blob\/master\/c\/jdbc.java), this is used by 100s of users globally. \r\n\r\nI admire the idea of a common data format but tying it to a 70MB verbose implementation when someone just wants a java driver isn't convincing me. \r\n\r\nEclipse, apache, google, gson, flatbuffer - it's basically every popular java dependency in the world.\r\n\r\n- **There's a 256kb suffix list text file from mozilla in there.** What's that for?  No attempt has been made at crafting something beautiful. 256kb is 10% of the size of H2 database and JDBC driver.\r\n- 13 MB for platform specific nettys is larger than 80% of other database drivers alone. (screenshot below)\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/7273208\/0a33e1c5-e6f2-48f0-9936-f8b42a11e64e)\r\n\r\n\r\nmozilla\\public-suffix-list.txt\r\n\r\n```\r\n\/\/ This Source Code Form is subject to the terms of the Mozilla Public\r\n\/\/ License, v. 2.0. If a copy of the MPL was not distributed with this\r\n\/\/ file, You can obtain one at https:\/\/mozilla.org\/MPL\/2.0\/.\r\n\r\n\/\/ Please pull this list from, and only from https:\/\/publicsuffix.org\/list\/public_suffix_list.dat,\r\n\/\/ rather than any other VCS sites. Pulling from any other URL is not guaranteed to be supported.\r\n\r\n\/\/ Instructions on pulling and using this list can be found at https:\/\/publicsuffix.org\/list\/.\r\n\r\n\/\/ ===BEGIN ICANN DOMAINS===\r\n\r\n\/\/ ac : http:\/\/nic.ac\/rules.htm\r\nac\r\ncom.ac\r\nedu.ac\r\ngov.ac\r\nnet.ac\r\nmil.ac\r\norg.ac\r\n\r\n\/\/ ad : https:\/\/en.wikipedia.org\/wiki\/.ad\r\nad\r\nnom.ad\r\n\r\n\/\/ ae : https:\/\/tdra.gov.ae\/en\/aeda\/ae-policies\r\nae\r\nco.ae\r\nnet.ae\r\norg.ae\r\nsch.ae\r\nac.ae\r\n```\r\n\r\nPlatform specific netty takes 13MB.\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/7273208\/2cbe8008-b39f-4a37-9e26-b247f0733689)\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nJava","comments":["Would it help to have an unshaded JAR available @ryanhamilton ? At the least, it'd reduce the download size when dependencies are already part of the calling application.\r\n\r\nSee #37892 .","My interest was because I provide a free SQL IDE: https:\/\/www.timestored.com\/qstudio\/\r\nIt bundles common\/small drivers and automatically downloads larger. A user requested support.\r\nSo I myself don't have a strong need and probably will skip supporting InfluxDB automaticaly for now. Users can download the jar themselves.\r\n\r\nI mostly raised this issue to make you aware that some use-cases will care about \r\n1. Deployment size\r\n2. Huge number of dependencies.\r\nI wanted to warn you of this early as it's much harder to reduce size later when your library has become more popular. \r\n\r\nOn the huge number of dependencies, I provide qStudio to a number of banks. They scan qStudio,.jar for CVEs. With as large a number of dependencies as you have, if I bundled your driver it's likely to trigger a CVE alert somewhere. Really I would recommend trying to trim your dependencies.\r\n\r\nGood luck.","I suspect the maintainers would love some help to make PRs to reduce the number of dependencies in the arrow JDBC drivers"],"labels":["Type: enhancement","Component: Java"]},{"title":"GH-18036: [Packaging][Python] Add support for musllinux_1_2","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\nThis change adds pre-building of musllinux wheels, which can be installed on Alpine Linux\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\nI'm only using musllinux_1_2 since I couldn't get musllinux_1_1 to work (and can't say for sure if it's even needed)\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\nTBA\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\nWheels will be added for musllinux\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* GitHub Issue: #18036","comments":[":warning: GitHub issue #18036 **has been automatically assigned in GitHub** to PR creator.","@kou I'm running `archery docker run python-wheel-musllinux-1-2` locally right now and most of it seems to work. Right now it fails locally on the `cmake --build . --target install` step, which seems to be running within the container itself. That step seems to run out of memory locally though, so I'm not sure if I can test everything right now. I'm now trying to run it again with a little more memory allocated to Docker, but if that doesn't help I'll need to run it on my other laptop\/I'd be unable to run it locally.","How much memory did you use?\r\n","I increased the max amount to 10gb, so I'm starting to think it's not actually a memory issue, but it's the only thing I can find when googling the error. I'll try some more troubleshooting steps (like running the build interactively to inspect the environment after the error).","Could you share your logs?","Of course!\r\n```log\r\n+ cmake --build . --target install\r\n[58\/128] Building CXX object src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx.o\r\nFAILED: src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx.o \r\n\/usr\/local\/bin\/ccache \/usr\/bin\/c++ -DARROW_DS_EXPORTING -DARROW_HAVE_RUNTIME_AVX2 -DARROW_HAVE_RUNTIME_AVX512 -DARROW_HAVE_RUNTIME_BMI2 -DARROW_HAVE_RUNTIME_SSE4_2 -DARROW_HAVE_SSE4_2 -DARROW_HDFS -DARROW_MIMALLOC -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_RE2 -DARROW_WITH_SNAPPY -DARROW_WITH_TIMING_TESTS -DARROW_WITH_UTF8PROC -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DAWS_SDK_VERSION_MAJOR=1 -DAWS_SDK_VERSION_MINOR=11 -DAWS_SDK_VERSION_PATCH=201 -DAWS_USE_EPOLL -DAZ_RTTI -DCURL_STATICLIB -DURI_STATIC_BUILD -DUTF8PROC_STATIC -I\/tmp\/arrow-build\/src -I\/arrow\/cpp\/src -I\/arrow\/cpp\/src\/generated -I\/arrow\/cpp\/src\/parquet -isystem \/arrow\/cpp\/thirdparty\/flatbuffers\/include -isystem \/arrow\/cpp\/thirdparty\/hadoop\/include -isystem \/opt\/vcpkg\/installed\/amd64-linux-static-release\/include -isystem \/tmp\/arrow-build\/orc_ep-install\/include -isystem \/opt\/vcpkg\/installed\/amd64-linux-static-release\/share\/rapidjson\/..\/..\/include -isystem \/tmp\/arrow-build\/jemalloc_ep-prefix\/src -isystem \/tmp\/arrow-build\/mimalloc_ep\/src\/mimalloc_ep\/include\/mimalloc-2.0 -Wno-noexcept-type -Wno-subobject-linkage  -fdiagnostics-color=always  -Wall -fno-semantic-interposition -msse4.2  -O3 -DNDEBUG -O2 -ftree-vectorize  -fPIC -DS2N_KYBER512R3_AVX2_BMI2 -DS2N_CPUID_AVAILABLE -DS2N_FEATURES_AVAILABLE -fPIC -DS2N_FALL_THROUGH_SUPPORTED -DS2N___RESTRICT__SUPPORTED -DS2N_MADVISE_SUPPORTED -DS2N_CLONE_SUPPORTED -std=c++17 -MD -MT src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx.o -MF src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx.o.d -o src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx.o -c \/tmp\/arrow-build\/src\/arrow\/dataset\/CMakeFiles\/arrow_dataset_objlib.dir\/Unity\/unity_1_cxx.cxx\r\nc++: fatal error: Killed signal terminated program cc1plus\r\ncompilation terminated.\r\n[71\/128] Building CXX object src\/parquet\/CMakeFiles\/parquet_objlib.dir\/Unity\/unity_1_cxx.cxx.o\r\nninja: build stopped: subcommand failed.\r\n```","Could you disable `CMAKE_UNITY_BUILD` for now?\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/40177\/files#diff-323a5673627df4e855450c3b25e09df91f0b5e1beb654194f2b2bbb6f33f6e90R73","> Could you disable `CMAKE_UNITY_BUILD` for now? https:\/\/github.com\/apache\/arrow\/pull\/40177\/files#diff-323a5673627df4e855450c3b25e09df91f0b5e1beb654194f2b2bbb6f33f6e90R73\r\n\r\nRunning now! Will let you know the result when it's finished","It got a little further and now times out git status:\r\n```\r\n+ python setup.py bdist_wheel\r\nsetup.py:34: DeprecationWarning: pkg_resources is deprecated as an API. See https:\/\/setuptools.pypa.io\/en\/latest\/pkg_resources.html\r\n  import pkg_resources\r\n\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\r\n!!\r\n\r\n        ********************************************************************************\r\n        Requirements should be satisfied by a PEP 517 installer.\r\n        If you are using pip, you can try `pip install --use-pep517`.\r\n        ********************************************************************************\r\n\r\n!!\r\n  dist.fetch_build_eggs(dist.setup_requires)\r\nWARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 475, in <module>\r\n    setup(\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/__init__.py\", line 103, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/_distutils\/core.py\", line 147, in setup\r\n    _setup_distribution = dist = klass(attrs)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/dist.py\", line 303, in __init__\r\n    _Distribution.__init__(self, dist_attrs)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/_distutils\/dist.py\", line 283, in __init__\r\n    self.finalize_options()\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/dist.py\", line 654, in finalize_options\r\n    ep(self)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools\/dist.py\", line 674, in _finalize_setup_keywords\r\n    ep.load()(self, ep.name, value)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_integration\/setuptools.py\", line 101, in version_keyword\r\n    _assign_version(dist, config)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_integration\/setuptools.py\", line 53, in _assign_version\r\n    maybe_version = _get_version(config, force_write_version_files=True)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_get_version_impl.py\", line 93, in _get_version\r\n    parsed_version = parse_version(config)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_get_version_impl.py\", line 56, in parse_version\r\n    or parse_scm_version(config)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_get_version_impl.py\", line 27, in parse_scm_version\r\n    parse_result = config.parse(config.absolute_root, config=config)\r\n  File \"setup.py\", line 430, in parse_git\r\n    return parse(root, **kwargs)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/git.py\", line 211, in parse\r\n    return _git_parse_inner(\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/git.py\", line 267, in _git_parse_inner\r\n    dirty = wd.is_dirty()\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/git.py\", line 94, in is_dirty\r\n    return run_git(\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/git.py\", line 58, in run_git\r\n    return _run(\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/site-packages\/setuptools_scm\/_run_cmd.py\", line 144, in run\r\n    res = subprocess.run(\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/subprocess.py\", line 495, in run\r\n    stdout, stderr = process.communicate(input, timeout=timeout)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/subprocess.py\", line 1028, in communicate\r\n    stdout, stderr = self._communicate(input, endtime, timeout)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/subprocess.py\", line 1885, in _communicate\r\n    self._check_timeout(endtime, orig_timeout, stdout, stderr)\r\n  File \"\/opt\/python\/cp38-cp38\/lib\/python3.8\/subprocess.py\", line 1072, in _check_timeout\r\n    raise TimeoutExpired(\r\nsubprocess.TimeoutExpired: Command '['git', '--git-dir', '\/arrow\/.git', 'status', '--porcelain', '--untracked-files=no']' timed out after 20 seconds\r\n```","If you run the `git status ...` command manually, does it work?","Yes. Not sure if it's because not everything has ran yet (I just started the Docker container using `docker run -v .:\/arrow:delegated -v ${DOCKER_VOLUME_PREFIX}python-wheel-musllinux-1-2-ccache:\/ccache:delegated -it apache\/arrow-dev:amd64-python-3.8-wheel-musllinux-1-2-vcpkg-a42af01b72c28a8e1d7b48107b33e4f286a55ef6 \/bin\/ash`) or something else?","You can run the same configured container by `archery docker run python-wheel-musllinux-1-12 bash`.","It runs fine when I run it manually."],"labels":["awaiting review"]},{"title":"[C++] string created by strftime can't be parsed by strptime","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n>>> pa.compute.strftime(pd.Timestamp.utcnow(), \"%Y-%m-%d %z\")\r\n<pyarrow.StringScalar: '2024-02-21 +0000'>\r\n>>> pa.compute.strptime('2024-02-21 +0000', \"%Y-%m-%d %z\", \"ns\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\pd_dev_1208\\Lib\\site-packages\\pyarrow\\compute.py\", line 263, in wrapper\r\n    return func.call(args, options, memory_pool)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\\_compute.pyx\", line 385, in pyarrow._compute.Function.call\r\n  File \"pyarrow\\error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Failed to parse string: '2024-02-21 +0000' as a scalar of type timestamp[ns]\r\n>>>\n\n### Component(s)\n\nC++, Python","comments":["Which version of pyarrow are you using? For me, using the latest dev version, this seems to work fine:\r\n\r\n```\r\nIn [8]: pa.compute.strftime(pd.Timestamp.utcnow(), \"%Y-%m-%d %z\")\r\nOut[8]: <pyarrow.StringScalar: '2024-02-27 +0000'>\r\n\r\nIn [9]: pa.compute.strptime('2024-02-21 +0000', \"%Y-%m-%d %z\", \"ns\")\r\nOut[9]: <pyarrow.TimestampScalar: '2024-02-21T00:00:00.000000000+0000'>\r\n```","Ah, and potentially the platform is more relevant information. I suppose you are using Windows (from the file paths)?\r\n\r\nFor Windows, we have a vendored implementation for strptime, while for unix we use the system's one, I think","Yes windows with latest pyarrow. I guess it is related to the vendored implementation then."],"labels":["Type: bug","Component: C++","Component: Python"]},{"title":"[Python] Add FlightSql client bindings","body":"### Describe the enhancement requested\n\nPyArrow currently only implements bindings to Arrow Flight RPC (not Arrow Flight SQL). There already exists a python Flight SQL driver in the ADBC repo[1]. We can implement a python Flight SQL client in PyArrow by wrapping the Arrow C++ Flight SQL client. The PyArrow Flight RPC implementation is here[2] for reference.\r\n\r\n[1]https:\/\/github.com\/apache\/arrow-adbc\/tree\/main\/python\/adbc_driver_flightsql\r\n[2]https:\/\/github.com\/apache\/arrow\/blob\/main\/python\/pyarrow\/flight.py\r\n\n\n### Component(s)\n\nPython","comments":["For reference, the C++ Client is defined here:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/40a8a68c7c3903e2bad98605d82339460c5ea930\/cpp\/src\/arrow\/flight\/sql\/client.h#L44-L51\r\n\r\n(added in the original Flight SQL PR https:\/\/github.com\/apache\/arrow\/pull\/12013)"],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Consider splitting _lib module into several parts","body":"### Describe the enhancement requested\n\nWhen reading the logs of a wheel build on Windows I noticed these lines:\r\n```\r\n  lib.cpp\r\nC:\\Python312\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\ndarraytypes.h(1250,38): warning C4200: nonstandard extension used: zero-sized array in struct\/union [C:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.vcxproj]\r\nC:\\Python312\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\ndarraytypes.h(1250,38): message : This member will be ignored by a defaulted constructor or copy\/move assignment operator [C:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.vcxproj]\r\nC:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.cpp(335145,17): warning C4244: '=': conversion from 'Py_ssize_t' to 'long', possible loss of data [C:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.vcxproj]\r\nC:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.cpp(335645,17): warning C4244: '=': conversion from 'Py_ssize_t' to 'long', possible loss of data [C:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.vcxproj]\r\nC:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.cpp(335855,17): warning C4244: '=': conversion from 'Py_ssize_t' to 'long', possible loss of data [C:\\arrow\\python\\build\\temp.win-amd64-cpython-312\\lib.vcxproj]\r\n```\r\n\r\nIgnoring what the warnings say, what stands out is that the `lib.cpp` generated by Cython has *at least* 335000 lines. This is huge and can obviously lead to enormous compile times, especially if the RAM is not large enough for the C++ compiler to hold the entire intermediate representation(s) in memory.\r\n\r\nWe should definitely try to split the `_lib` into smaller parts, in order to alleviate this problem.\r\n\r\n(it is also a Cython problem that so much C++ code is generated, but I'm not sure we can fix that).\n\n### Component(s)\n\nPython","comments":["@AlenkaF @jorisvandenbossche @danepitkin  I think this would be worth looking into, in the \"quality of life\" department.","(I also posted on the Cython users ML: https:\/\/groups.google.com\/g\/cython-users\/c\/hr3cFevY46k)","Note that our Windows wheel builds routinely take 3 hours, and it may very well be because of this:\r\nhttps:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7970517386\/job\/21758260987","This might also be related to the AppVeyor timeouts.","Related: https:\/\/github.com\/apache\/arrow\/pull\/40225","When it comes to \"quality of life\" while developing pyarrow locally, I would personally prioritize improving our build system to have proper rebuilds (https:\/\/github.com\/apache\/arrow\/issues\/36411#issuecomment-1753704373), but of course I am also biased because not using Windows and not seeing this issue locally. And improving build times for Ci is definitely important as well.\r\n\r\nThe previous time we worked on splitting `pyarrow.lib` I brought up the back-compat issue for people (c)importing from there, see https:\/\/github.com\/apache\/arrow\/pull\/10162#issuecomment-831829432 and the comments below. \r\nOf course we can decide to break that once in a release, but I would still prefer we have a clearer story about how we recommend to use pyarrow in those cases.\r\n\r\nThere might also be some smaller things we could already split off that are less controversial \/ less publicly used (for example `benchmark.pxi`, although this is only a tiny one-function file and won't help much. A bigger one might be `tensor.pxi`)","We should maybe also experiment with ways to do this in a less breaking way. For example, can we still include things in `lib.pxd` so `cimport` keeps working, while moving actual implementations out of pyarrow.lib. In pure Python something like that certainly works, but I don't know by heart how cython would deal with that.","> When it comes to \"quality of life\" while developing pyarrow locally, I would personally prioritize improving our build system to have proper rebuilds ([#36411 (comment)](https:\/\/github.com\/apache\/arrow\/issues\/36411#issuecomment-1753704373)),\r\n\r\nI think we should do both :-)","Related to quality of life on incremental builds: https:\/\/github.com\/cython\/cython\/issues\/6070"],"labels":["Type: enhancement","Component: Python"]},{"title":"[Archery] Avoid relying on internal setuptools_scm API","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nThe Crossbow submission routines depend on an unstable setuptools_scm API that forces us to pin the setuptools_scm version:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/47f15b07080d62cd912bfbfd5d067cf70dfe6960\/dev\/archery\/archery\/crossbow\/core.py#L742\r\n\r\nWe should rewrite this function to avoid relying on setuptools_scm and do the version parsing entirely ourselves, instead.\r\n\r\nThis affects the comment bot, example at https:\/\/github.com\/apache\/arrow\/pull\/40162\n\n### Component(s)\n\nArchery, Continuous Integration","comments":["cc @raulcd @kszucs "],"labels":["Type: bug","Component: Archery","Component: Continuous Integration"]},{"title":"[C++][Parquet] Separate encoders and decoder","body":"### Describe the enhancement requested\n\nCurrently, encoders and decoders are defined in a single file `encoding.cc`, which is quite large.\r\n\r\nGiven that their infrastructure is separate, it would probably make maintenance easier to split them into two C++ source files (for example `encoder.cc` and `decoder.cc`). We can add corresponding `.h` files, and also keep `encoding.h` for compatibility.\r\n\n\n### Component(s)\n\nC++, Parquet","comments":["Thoughts @felipecrv @mapleFU @wgtmac ?","I just check that there're few common constant used in `encoding.cc`, so maybe spliting them is not hard.\r\n\r\nBut I'm not sure about this. Current code is also ok to me","> split them into two C++ source files\r\n\r\nI'm in favor, but it will be 3 files (or .h\/.cc pairs) because there has to be one for the shared funcitonality. One risk of the split is exposing the bits that are now fully private in `encoding.cc`.","Perhaps we can add a `encoding_internal.h` file to hold those private but common stuff?","> Perhaps we can add a `encoding_internal.h` file to hold those private but common stuff?\r\n\r\nWe can, though I doubt there's much shared functionality.","Hi, I am new to apache arrow and I would like to take this issue. Can this one be assigned to me? ","You can type \"take\" to take the issue, and create a pull request named \"GH-40154: [C++][Parquet] ...\" when you finished @changkhothuychung ","Assigned!"],"labels":["Type: enhancement","Component: Parquet","Component: C++","good-first-issue"]},{"title":"MINOR: [Java] Bump org.apache.maven.plugins:maven-dependency-plugin from 3.1.2 to 3.6.1 (manual update)","body":"### Rationale for this change\r\n\r\nThe dependabot [PR](https:\/\/github.com\/apache\/arrow\/pull\/39745) includes a modification to upgrade the maven dependency plugin usage from 3.1.2 to 3.6.1. But just updating the versions doesn't do the expected. This is a replacement PR which completes the required upgrade. \r\n\r\n### What changes are included in this PR?\r\n\r\nBump org.apache.maven.plugins:maven-dependency-plugin from 3.1.2 to 3.6.1.\r\nRequired minor refactor of `Integration` util to be moved to tests from source. \r\n\r\n### Are these changes tested?\r\n\r\nTested by existing test cases. \r\n\r\n### Are there any user-facing changes?\r\n\r\nNo","comments":["@github-actions crossbow submit *java*","Revision: f62b7746d31ec3308eb7a6c486db1dd791ccd453\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-25ce96495f](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-25ce96495f)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156071\/job\/21751972306)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156074\/job\/21751971876)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156085\/job\/21751971965)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156096\/job\/21751972059)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156079\/job\/21751971879)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-25ce96495f-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7968156073\/job\/21751971931)|","@github-actions crossbow submit *java*","Revision: 94a94b343c05de7cee39e11d945e7d5505b39786\n\nSubmitted crossbow builds: [ursacomputing\/crossbow @ actions-d328cd87db](https:\/\/github.com\/ursacomputing\/crossbow\/branches\/all?query=actions-d328cd87db)\n\n|Task|Status|\n|----|------|\n|java-jars|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-java-jars)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934572\/job\/21808343799)|\n|verify-rc-source-java-linux-almalinux-8-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-verify-rc-source-java-linux-almalinux-8-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934582\/job\/21808342718)|\n|verify-rc-source-java-linux-conda-latest-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-verify-rc-source-java-linux-conda-latest-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934583\/job\/21808342748)|\n|verify-rc-source-java-linux-ubuntu-20.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-verify-rc-source-java-linux-ubuntu-20.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934611\/job\/21808342776)|\n|verify-rc-source-java-linux-ubuntu-22.04-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-verify-rc-source-java-linux-ubuntu-22.04-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934677\/job\/21808343231)|\n|verify-rc-source-java-macos-amd64|[![GitHub Actions](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/workflows\/crossbow.yml\/badge.svg?branch=actions-d328cd87db-github-verify-rc-source-java-macos-amd64)](https:\/\/github.com\/ursacomputing\/crossbow\/actions\/runs\/7986934880\/job\/21808343523)|","@davisusanibar @danepitkin could you please take look? \r\nThere are 3 CIs failing but the crossbows for Java are passing. I am wondering do we need to include jackson dependencies both test and compile since Python is using the jar? ","We could also stop using ByteBufferBackedInputStream"],"labels":["Component: Java","awaiting review"]},{"title":"[Python] Casting an empty dictionary array loses category information","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nCasting a DictionaryArray to another dictionary type will preserve the original dictionary, unless the array is empty.\r\n\r\n### Minimal example\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\narr = pa.array([\"foo\", \"bar\"], pa.dictionary(pa.int32(), pa.string()))\r\narr_empty = arr[:0]\r\narr_empty_cast = arr_empty.cast(pa.dictionary(pa.int64(), pa.string()))\r\nprint(arr_empty_cast.dictionary)  # []\r\n```\r\n\r\n### Additional examples\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\narr = pa.array([\"foo\", \"bar\"], pa.dictionary(pa.int32(), pa.string()))\r\nprint(arr.dictionary)  # [\"foo\", \"bar\"]\r\n\r\narr_short_cast = arr[:1].cast(pa.dictionary(pa.int64(), pa.string()))\r\nprint(arr_short_cast.dictionary)  # [\"foo\", \"bar\"]\r\n\r\narr_empty = arr[:0]\r\nprint(arr_empty.dictionary)  # [\"foo\", \"bar\"]\r\n\r\narr_empty_cast = arr_empty.cast(pa.dictionary(pa.int64(), pa.string()))\r\nprint(arr_empty_cast.dictionary)  # []\r\n```\r\n\r\n### Expected output\r\n\r\nThe original categories should be preserved when changing the index type, even if the array is empty.\n\n### Component(s)\n\nPython","comments":["Why do we need to do this? Could you explain your use case?","@pitrou Do you have any opinion for this case?","I agree that the original dictionary should probably be preserved for consistency, if it's not too costly to do so.\r\ncc @felipecrv @jorisvandenbossche ","> Why do we need to do this? Could you explain your use case?\r\n\r\nFor Polars, we use `pyarrow` for the converting our data to `pandas`. Our `Enum` data type is similar to a pyarrow Dictionary type with a set number of categories. This converts into a pandas categorical type which also has a set number of categories.\r\n\r\nIn this conversion, we have to cast the index type from `UInt32` (our default) to `Int64` since pandas does not support unsigned indices. If the data is empty, this cast now loses the category information, and data type after the conversion is wrong.\r\n\r\nSee the original issue opened in our repo: https:\/\/github.com\/pola-rs\/polars\/issues\/14582","@stinodego Do you think you would be able to work on a PR?","I'm not familiar with the Arrow code base, but I suppose I could give it a shot.\r\n\r\nEDIT: If the implementation here is in C++ then it's probably better if someone else picks this up - I have no experience programming in that language.","Agreed that we should be consistent for empty arrays. \r\n\r\nHaven't tested this, but it might be caused by shortcutting calling the actual kernel for empty input here:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/4dc3d04ae84d97d02443c0cef555a46535925c2b\/cpp\/src\/arrow\/compute\/exec.cc#L781-L795\r\n\r\nThis creates a generic array of length 0 of the output type, but so in case of a dictionary output type, it will never have an actual dictionary attached to that return value.\r\n\r\nWe might want to generally skip that fast-path in case `output_type_` is a dictionary type?","> We might want to generally skip that fast-path in case output_type_ is a dictionary type?\r\n\r\nIdeally we would also do that for nested types that have a dictionary field somewhere.\r\n\r\nBut we can start with your idea and see what happens (not all scalar kernels might like zero-length inputs?).","I am not sure if we actually have many scalar kernels apart from `cast` that would work for dictionary \u00e0nd return dictionary?\r\n\r\nChecking some non-scalar (vector) kernels, those seem to preserve the dictionary correctly for empty input (using `arr` from the top post):\r\n\r\n```\r\nIn [8]: pc.filter(arr[:0], pa.array([], pa.bool_()))\r\nOut[8]: \r\n<pyarrow.lib.DictionaryArray object at 0x7f2f399f2ff0>\r\n\r\n-- dictionary:\r\n  [\r\n    \"foo\",\r\n    \"bar\"\r\n  ]\r\n-- indices:\r\n  []\r\n\r\nIn [9]: pc.take(arr[:0], pa.array([], pa.int64()))\r\nOut[9]: \r\n<pyarrow.lib.DictionaryArray object at 0x7f2f3b048040>\r\n\r\n-- dictionary:\r\n  [\r\n    \"foo\",\r\n    \"bar\"\r\n  ]\r\n-- indices:\r\n  []\r\n```","We do:\r\n```python\r\n>>> a = pa.array([1,2,3,1]).dictionary_encode()\r\n>>> pc.coalesce(a[:1])\r\n<pyarrow.lib.DictionaryArray object at 0x7f0ef4948ba0>\r\n\r\n-- dictionary:\r\n  [\r\n    1,\r\n    2,\r\n    3\r\n  ]\r\n-- indices:\r\n  [\r\n    0\r\n  ]\r\n>>> pc.coalesce(a[:0])\r\n<pyarrow.lib.DictionaryArray object at 0x7f0ef49d70d0>\r\n\r\n-- dictionary:\r\n  []\r\n-- indices:\r\n  []\r\n```","My take here is that dictionaries are for compression and might be aggressively further-compressed by some operations [1], but `cast` is special and *should preserve* the original dictionary because the goal of a cast is to change only the encoding of the data with as little as possible impact on its semantic properties.\r\n\r\nI would keep the fast path and make only cast kernels the exception. I don't know exactly how without looking at the code.\r\n\r\nMy reasoning: we don't want code accidentally relying on a guarantee that is stronger than \"the dictionary contains the values that are referenced at least once\". The path to full determinism here is probably being more aggressive in removing values, than preserving them.\r\n\r\n[1] many operations act on the `array[0..length)` values and might gather only the dictionary values that have references to them","Workaround in the meantime:\r\n\r\n```python\r\nimport pyarrow as pa\r\narr = pa.array([\"foo\", \"bar\"], pa.dictionary(pa.int32(), pa.string()))\r\narr_empty = arr[:0]\r\narr_empty_cast = pa.DictionaryArray.from_arrays(pa.array([], type=pa.int64()), arr_empty.dictionary)\r\nprint(arr_empty_cast.dictionary)  # ['foo', 'bar']\r\n```"],"labels":["Type: bug","Component: C++","Component: Python","good-second-issue"]},{"title":"[Go][Parquet] Looking for Memory-friendly way to seek & extract data from parquet columns","body":"### Describe the usage question you have. Please include as many useful details as  possible.\r\n\r\n\r\nHi,\r\n\r\nbeeing new to Apache Arrow I'm a little confused about the different options to interact with Parquet files. The documentation in the Go library is in many places very sparse and existing examples from various sources don't seem to match my use case.\r\n\r\n**The question is:** \r\nGiven that you have a parquet file containing serveral thousand rows each with an _ID_ column and a _Data_ column, where the data column holds some larger blob, how do you seek certain rows based on their ID column and extract the data of the _Data_ column in an efficient and memory-friendly way?\r\n\r\nBy 'memory-friendly' I mean that only the relevant values should be read from the parquet files and loaded into memory, not the whole column, rowgroup, batch or chunk. Reading the ID column completely into memory would be fine, but not the blob data.\r\n\r\nI tried the variant with creating a pqarrow.RecordReader based on a pqarrow.FileReader based on parquet.Reader, but it seems that the Record batches always load the the whole batch (incl. all column data) into memory, not just when loading the value of of a column entry by index. While this approach works as desired, it has a very high memory usage due to the large blobs.\r\n\r\nI also tried to extract the relevant row-indexes in a first sweep to then somehow only retrieve these rows from the Data column in a second sweep, but I could not find a way that improved the first approach. \r\n\r\nThere is probably a simple way (without using pqarrow?) by just iterating over the parquet file rowgroups, but the usage of the available datastructures are not reallly documented well (FieldReaders, chunks, etc.)\r\n\r\nBtw, doing the same thing with DuckDb works very well and is noticably lighter on memory than the RecordReader approach, but including that library for the simple seek&extract use case is somewhat overkill and I would prefer to avoid it.\r\n\r\nThanks for any hints\r\n\r\n\r\nJochen Mehlhorn <jochen.mehlhorn@mercedes-benz.com>, Mercedes-Benz Tech Innovation GmbH\r\n\r\n[Provider Information](https:\/\/github.com\/Daimler\/daimler-foss\/blob\/master\/PROVIDER_INFORMATION.md)\r\n\r\n\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nGo","comments":[],"labels":["Component: Go","Type: usage"]},{"title":"[Python] from_pylist should allow a parameter to scan more records for columns","body":"### Describe the enhancement requested\r\n\r\nThis:\r\n\r\n```python \r\nimport pyarrow\r\n\r\nprint(pyarrow.Table.from_pylist([{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4, \"c\": 5}]))\r\n\r\nprint(pyarrow.Table.from_pylist([{\"a\": 1, \"b\": 2, \"c\": 5}, {\"a\": 3, \"b\": 4}]))\r\n```\r\nresults in \r\n```\r\nfirst print:\r\na: int64     \r\nb: int64\r\n----\r\na: [[1,3]]\r\nb: [[2,4]]\r\n\r\nsecond print:\r\na: int64     \r\nb: int64\r\nc: int64\r\n----\r\na: [[1,3]]\r\nb: [[2,4]]\r\nc: [[5,null]]\r\n\r\n```\r\n\r\nI think it's kind of okay, but also a bit surprising. I think there should at least be parameter so that first result would also include column \"c\":\r\n```\r\na: int64     \r\nb: int64\r\nc:  int64\r\n----\r\na: [[1,3]]\r\nb: [[2,4]]\r\nc: [[null,5]]\r\n```\r\n\r\n\r\n\r\nThere are API's that do not return properties in case they are null, if you call from_pylist for such one, you will loose data. In my case the API was [the Power BI API](https:\/\/learn.microsoft.com\/en-us\/rest\/api\/power-bi\/admin\/reports-get-reports-as-admin).\r\n\r\nOne can of you course pass the schema to workaround this, but that's not always a good option (eg, you'll loose additional new properties)\r\n\r\n### Component(s)\r\n\r\nPython","comments":["Btw, Polars does this \"correctly\":\r\n\r\n```python\r\nimport polars\r\n\r\nprint(polars.from_dicts([{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4, \"c\": 5}]))\r\n\r\ngives:\r\n\u2502 a   \u2506 b   \u2506 c    \u2502\r\n\u2502 --- \u2506 --- \u2506 ---  \u2502\r\n\u2502 i64 \u2506 i64 \u2506 i64  \u2502\r\n\u2502 1   \u2506 2   \u2506 null \u2502\r\n\u2502 3   \u2506 4   \u2506 5    \u2502\r\n```\r\n\r\n\r\n","The current behaviour is somewhat documented:\r\n\r\n        schema : Schema, default None\r\n            If not passed, will be inferred from the first row of the\r\n            mapping values.\r\n\r\nBut I agree this might be surprising, or in any case that it can be useful to change that behaviour.\r\n\r\nWorkaround on the short term is to use `pa.array` to infer the list of dicts to a StructArray, which has the desired behaviour, and then this array can be converted zero copy to a batch or table:\r\n\r\n```\r\nIn [4]: arr = pa.array([{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4, \"c\": 5}])\r\n\r\nIn [5]: batch = pa.RecordBatch.from_struct_array(arr)\r\n\r\nIn [6]: batch\r\nOut[6]: \r\npyarrow.RecordBatch\r\na: int64\r\nb: int64\r\nc: int64\r\n----\r\na: [1,3]\r\nb: [2,4]\r\nc: [null,5]\r\n\r\nIn [7]: batch.to_pandas()\r\nOut[7]: \r\n   a  b    c\r\n0  1  2  NaN\r\n1  3  4  5.0\r\n```\r\n\r\nI suppose for larger data, this should actually also be faster, and so we should maybe consider using that under the hood as well."],"labels":["Type: enhancement","Component: Python"]},{"title":"[Python] Incorrect conversion from datetime.datetime and datetime.time objects with non-UTC timezones","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nI may be missing something fundamental, but Pyarrow seems to improperly handle the conversion from datetime.datetime and datetime.time objects with non-UTC timezones.\r\n\r\n**Datetimes**\r\n\r\nThis Python datetime is the turn of the millenium in UTC+1:\r\n\r\n```python\r\n>>> dt = datetime.datetime(2000, 1, 1, 0, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=1)))\r\n```\r\n\r\nConverting to pyarrow makes it 11 PM in UTC+1, which is a different time (not just the same time in a different coordinate system):\r\n\r\n```python\r\n>>> a = pa.array([dt])\r\n>>> a\r\n<pyarrow.lib.TimestampArray object at 0x7f4d0197e3e0>\r\n[\r\n  1999-12-31 23:00:00.000000\r\n]\r\n>>> a.type\r\nTimestampType(timestamp[us, tz=+01:00])\r\n```\r\n\r\nWhen converted to R (using the Arrow C API), this becomes the following POSIXct vector:\r\n```R\r\n> a\r\n[1] \"1999-12-31 23:00:00\"\r\n> attr(a, 'tzone')\r\n[1] \"+01:00\"\r\n```\r\n\r\nwhich is again 11 PM in UTC+1. \r\n\r\nSo the Arrow -> R conversion is consistent, but the Python -> Arrow conversion seems like a bug.\r\n\r\n**Times**\r\n\r\nCreating a pyarrow array from a datetime.time strips timezone info:\r\n\r\n```python\r\n>>> pa.array([datetime.time(12, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=1)))]) == \\\r\n... pa.array([datetime.time(12, 0)])\r\nTrue\r\n```\r\n\r\nwhich seems to be intentional according to https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.Time64Array.html:\r\n\r\n> Localized timestamps will currently be returned as UTC (pandas\u2019s native representation). Timezone-naive data will be implicitly interpreted as UTC.\r\n\r\nBut again, these are two different times (not just the same time in a different coordinate system), so they shouldn't compare equal.\r\n\r\nThis is on pyarrow 14.0.2, Python 3.12, R version 4.3.2, R Arrow version 14.0.1.\r\n\r\n### Component(s)\r\n\r\nPython","comments":["I think everything is behaving as expected, but the repr is confusing.\r\n\r\n> **Datetimes**\r\n> ...\r\n> Converting to pyarrow makes it 11 PM in UTC+1, which is a different time (not just the same time in a different coordinate system):\r\n\r\nSo it actually did convert it to 11 PM in UTC (the UTC value is what is stored under the hood), and thus still representing midnight in UTC+1.  \r\nOne might expect wall-time relative to the type in the repr, but so the repr is showing the UTC values, which is causing the confusion.\r\n\r\nIn the latest pyarrow 15.0, we improved the repr a _little_ bit by adding \"Z\" as an indicator that what you see are UTC values:\r\n\r\n```\r\nIn [4]: a\r\nOut[4]: \r\n<pyarrow.lib.TimestampArray object at 0x7f2fe814ebc0>\r\n[\r\n  1999-12-31 23:00:00.000000Z\r\n]\r\n\r\nIn [5]: a.type\r\nOut[5]: TimestampType(timestamp[us, tz=+01:00])\r\n```\r\n\r\n> **Times**\r\n> \r\n> Creating a pyarrow array from a datetime.time strips timezone info:\r\n\r\nThe Time type in Arrow format does not have a notion of timezones (see [fatbuffer spec](https:\/\/github.com\/apache\/arrow\/blob\/4dc3d04ae84d97d02443c0cef555a46535925c2b\/format\/Schema.fbs#L255-L274)), and pyarrow's conversion code for python->arrow currently just ignores the timezone. \r\nThis is not ideal, and maybe it would be better if pyarrow just raised an error instead? (indicating it doesn't support `datetime.time` objects with a timezone?)\r\n\r\n\r\n\r\n\r\n\r\n\r\n> which seems to be intentional according to https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.Time64Array.html:\r\n> \r\n> > Localized timestamps will currently be returned as UTC (pandas\u2019s native representation). Timezone-naive data will be implicitly interpreted as UTC.\r\n\r\nHmm, that note seems quite outdated (also, it is about timestamps, not about times .. although it appears in the Time64Array doc page, but that's because this `from_pandas` method is implemented on the base class and thus has the same docstring for all concrete Array subclasses). \r\n\r\n","> Hmm, that note seems quite outdated \r\n\r\nThat note seems to have been added almost at the beginning of the project (https:\/\/github.com\/apache\/arrow\/pull\/287), and since then quite some changes were made to timezone handling. \r\n(localized timestamps are _not_ exactly returned as UTC, but the timezone information is preserved; and for timezone-naive data there is an equivalent timestamp without timezone type in Arrow ..)","Makes sense! You're right that the datetime.datetimes are all the same between base Python, pandas, Arrow and base R, they're just displayed differently. I definitely prefer base Python and pandas's display to Arrow's though, maybe version 16 could switch? ;)\r\n\r\n```python\r\n>>> pa.array([datetime.datetime(2000, 1, 1, 0, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=1)))])\r\n<pyarrow.lib.TimestampArray object at 0x7fa7b9084280>\r\n[\r\n  1999-12-31 23:00:00.000000\r\n]\r\n>>> pa.array([datetime.datetime(2000, 1, 1, 0, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=1)))]).to_pandas()\r\n0   2000-01-01 00:00:00+01:00\r\ndtype: datetime64[us, pytz.FixedOffset(60)]\r\n```\r\n\r\nFor the times, I would definitely raise an error on datetime.time objects with a non-default timezone, it seems quite dangerous to truncate the timezone silently."],"labels":["Type: bug","Component: Python"]},{"title":"[CI][FS][Azure] Azurite tests are flaking on `main`","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nFlaky failures like\r\n```\r\nC++ exception with description \"Connection closed before getting full response or response is less than expected. Expected response length = 254. Read until now = 231\" thrown in the test body.\r\n2024-02-18T12:50:20.039Z ada6933e-9c33-47d2-86f6-29e9aa01f713 info: BlobStorageContextMiddleware: RequestMethod=DELETE RequestURL=http:\/\/127.0.0.1\/devstoreaccount1\/container?restype=container RequestHeaders:{\"authorization\":\"SharedKey devstoreaccount1:hYh+JRj5cBYqdqOyM2wB3EZizQ\/s2DiIoDI0CIF2EXM=\",\"host\":\"127.0.0.1:10000\",\"user-agent\":\"azsdk-cpp-storage-blobs\/12.10.0-beta.1 (Linux 6.2.0-1019-azure x86_64 #19~22.04.1-Ubuntu SMP Wed Jan 10 22:57:03 UTC 2024)\",\"x-ms-client-request-id\":\"be6819a2-72b8-4630-8eb0-4a88e7cb3061\",\"x-ms-date\":\"Sun, 18 Feb 2024 12:50:20 GMT\",\"x-ms-version\":\"2022-11-02\"} ClientIP=127.0.0.1 Protocol=http HTTPVersion=1.1\r\n```\r\nI've seen the occur in different test cases and in different test suites. \r\n\r\nExample failures:\r\nI've seen one flake on `main`: https:\/\/github.com\/apache\/arrow\/actions\/runs\/7915689559\/job\/21608061673\r\nFlakes on my recent PRs:\r\nhttps:\/\/github.com\/apache\/arrow\/actions\/runs\/7951594516\/job\/21705210845?pr=40080\r\nhttps:\/\/github.com\/apache\/arrow\/actions\/runs\/7949050250\/job\/21699789831?pr=40080\r\n\r\n\r\n\n\n### Component(s)\n\nC++, Continuous Integration","comments":["Hmm, it seems that failed tests aren't same...\r\nCan we re-run failed tests?\r\n\r\n```diff\r\ndiff --git a\/ci\/scripts\/cpp_test.sh b\/ci\/scripts\/cpp_test.sh\r\nindex 1d685c51a9..a23ea8eb1c 100755\r\n--- a\/ci\/scripts\/cpp_test.sh\r\n+++ b\/ci\/scripts\/cpp_test.sh\r\n@@ -86,6 +86,7 @@ ctest \\\r\n     --label-regex unittest \\\r\n     --output-on-failure \\\r\n     --parallel ${n_jobs} \\\r\n+    --repeat until-pass:3 \\\r\n     --timeout ${ARROW_CTEST_TIMEOUT:-300} \\\r\n     \"${ctest_options[@]}\" \\\r\n     \"$@\"\r\n```","I expect retries would be an effective mitigation. "],"labels":["Type: bug","Component: C++","Component: Continuous Integration"]},{"title":"[Java][FlightRPC] Flight SQL JDBC driver parameter getting an exception: parameter ordinal 1 out of range","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n```\r\nprotected AvaticaParameter getParameter(int param) throws SQLException {\r\n        try {\r\n            return (AvaticaParameter)this.getSignature().parameters.get(param - 1);\r\n        } catch (IndexOutOfBoundsException var3) {\r\n            throw AvaticaConnection.HELPER.toSQLException(AvaticaConnection.HELPER.createException(\"parameter ordinal \" + param + \" out of range\"));\r\n        }\r\n    }\r\n```\r\n\r\nGetting this exception\r\n\r\nMy code:\r\n```\r\npublic static void handleSqlStatements() throws SQLException {\r\n        String url = \"jdbc:arrow-flight:\/\/localhost:5000;useEncryption=false;useServerPrepStmts=false;\";\r\n        String sql = \"select * from MixedTypeDB.`Table` where OID=?\"; \/\/ Example SQL query\r\n        System.out.println(\"SQL query: \" + sql); \/\/ Debugging: Log the query\r\n\r\n        try (var connection = DriverManager.getConnection(url)) {\r\n            ArrowFlightPreparedStatement pstmt = (ArrowFlightPreparedStatement)connection.prepareStatement(sql);\r\n            ParameterMetaData metadata = pstmt.getParameterMetaData();\r\n            pstmt.setInt(1, 12345); \/\/ crashing here\r\n\r\n            System.out.println(\"Executing SQL query: \" + sql);\r\n\r\n            try (final ResultSet rs = pstmt.executeQuery()) {\r\n            }\r\n        } catch (SQLException e) {\r\n            e.printStackTrace();\r\n        }\r\n}\r\n```\r\n\r\n```\r\n<dependency>\r\n            <groupId>org.apache.arrow<\/groupId>\r\n            <artifactId>flight-sql-jdbc-driver<\/artifactId>\r\n            <version>13.0.0<\/version>\r\n        <\/dependency>\r\n```\n\n### Component(s)\n\nJava","comments":["Duplicate of https:\/\/github.com\/apache\/arrow\/issues\/33475\r\n\r\nThis was fixed in 15.0.0, can you upgrade and try again?","> Duplicate of #33475\r\n> \r\n> This was fixed in 15.0.0, can you upgrade and try again?\r\n\r\nHave tried it with 15.0.0 and it's has the same issue\r\n","Can you post all the details with 15.0.0 (stack trace, versions, etc)?","> Can you post all the details with 15.0.0 (stack trace, versions, etc)?\r\n\r\nSir, the error is still the same and the code is exactly the same, if you want I can repaste in here again , including the exception details. I can even add the project it self if needed. Which I will do.\r\n[anybase-inspector-probe.zip](https:\/\/github.com\/apache\/arrow\/files\/14323011\/anybase-inspector-probe.zip)\r\n","I'm just asking for the stack trace, so I can take a quick look. While the project is appreciated, I don't have the time to build this from scratch.","`![image](https:\/\/github.com\/apache\/arrow\/assets\/6758579\/728a1dee-36a7-4312-abf1-6bfef3baecb7)\r\n@lidavidm \r\n\r\n\r\n`Error executing SQL: parameter ordinal 1 out of range\r\njava.sql.SQLException: parameter ordinal 1 out of range\r\n\tat cfjd.org.apache.calcite.avatica.Helper.createException(Helper.java:60)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaPreparedStatement.getParameter(AvaticaPreparedStatement.java:427)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaPreparedStatement.getSite(AvaticaPreparedStatement.java:434)\r\n\tat cfjd.org.apache.calcite.avatica.AvaticaPreparedStatement.setInt(AvaticaPreparedStatement.java:174)\r\n\tat com.ge.core.SQLInterpreterWithParameters.setParameter(SQLInterpreterWithParameters.java:62)\r\n\tat com.ge.core.SQLInterpreterWithParameters.handleSqlStatements(SQLInterpreterWithParameters.java:38)\r\n\tat com.ge.grid.Main.main(Main.java:11)`","Is the server implementation actually returning a parameter schema?","> Is the server implementation actually returning a parameter schema?\r\n\r\nall the column names are back other than that IDK, can you point me where to look please?","What server implementation are you using? It needs to fill in the parameter_schema in the CreatePreparedStatement response https:\/\/github.com\/apache\/arrow\/blob\/a03d957b5b8d0425f9d5b6c98b6ee1efa56a1248\/format\/FlightSql.proto#L1524-L1549","> parameter_schema\r\n\r\nSeems like it's empty: The calcite oriented db\r\n```csharp\r\nvar preparedStatementResult = new ActionCreatePreparedStatementResult\r\n        {\r\n            DatasetSchema = schema.ToByteString(),\r\n            ParameterSchema = ByteString.Empty,","The server needs to return the schema so that the driver knows how many and what type the parameters are. We could perhaps improve the error message on the client, or assume what the type\/number of parameters are based on what the client does, but that is a duplicate of https:\/\/github.com\/apache\/arrow\/issues\/38585","> The server needs to return the schema so that the driver knows how many and what type the parameters are. We could perhaps improve the error message on the client, or assume what the type\/number of parameters are based on what the client does, but that is a duplicate of #38585\r\n\r\nI understand and already opened an issue in our code base, really appreciate you assistance. Awesome responsiveness.","Sounds good. I'll leave this open and link it in that issue to track the error message improvement.","I encountered the same problem\r\n<img width=\"1735\" alt=\"image\" src=\"https:\/\/github.com\/apache\/arrow\/assets\/23352189\/7a47ca96-042d-4029-8b97-1f948c810747\">\r\n\r\n"],"labels":["Type: enhancement","Component: Java"]},{"title":"[R] Initializing an Arrow array from an R difftime or hms::hms rounds down to the nearest time unit","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\n```R\r\n> delta <- as.difftime(c(0.000, 0.001, 0.002, 1, 1.5), units = \"secs\")\r\n> delta\r\nTime differences in secs\r\n[1] 0.000 0.001 0.002 1.000 1.500\r\n> arrow::Array$create(delta)\r\nArray\r\n<duration[s]>\r\n[\r\n  0,\r\n  0,\r\n  0,\r\n  1,\r\n  1\r\n]\r\n```\r\nThis should be [0, 0.001, 0.002, 1, 1.5], not [0, 0, 0, 1, 1]. Same issue with `arrow::Array$create(hms::hms(c(0.000, 0.001, 0.002, 1, 1.5)))`, which becomes `time32[s]`.\r\n\r\nR version 4.3.2, R Arrow version 14.0.1.\r\n\r\n### Component(s)\r\n\r\nR","comments":["It looks like R maps `difftime` objects as an Arrow `DurationType`, but `DurationType` stores the values as an int64, and so the R values are being casted to integers. This is happening on the C++ side: \r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/909f6f90ebdbb2e16b223d768ee10c78e0b37bfb\/cpp\/src\/arrow\/type.h#L1811-L1817 \r\n\r\nOne workaround would be to convert the values to numerics before creating the Array: \r\n\r\n```r\r\n> delta <- as.difftime(c(0.000, 0.001, 0.002, 1, 1.5), units = \"secs\")\r\n> arrow::Array$create(as.numeric(delta))\r\nArray\r\n<double>\r\n[\r\n  0,\r\n  0.001,\r\n  0.002,\r\n  1,\r\n  1.5\r\n]\r\n```\r\n\r\nMaybe the r package could provide a warning about the casting of difftime to integers to the user. ","difftime doesn't get any smaller than seconds, so anything smaller than a second will always be truncated. That needs a fix, not a warning! ","It looks like the actual truncation happens here:\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/214378b522a36fbf6010e3d4f5470abaca7bf92e\/r\/src\/r_to_arrow.cpp#L926\r\n\r\nThe cast to the `c_type` as David noted, is a cast to an int64. On this line, one could check that you're not doing any truncation (I think you can use `std::modf()` for that). You would probably have to do something like count the number of lossy casts (e.g., `this->n_lossy_casts_++`) and issue the warning at the very end of the conversion.\r\n\r\nPerhaps the underlying cause is that we infer the unit of \"seconds\" by default. We could infer \"milliseconds\" or \"microseconds\" which would avoid truncation (or would limit it to thousandths or millionths of a second). I don't know why \"seconds\" is the default but a good fix for this might be to change it to \"ms\" or \"us\" (or add an `options()` to do so, perhaps migrating to a safer default over several versions with some warnings).\r\n\r\n``` r\r\narrow::infer_type(as.difftime(double(), units = \"secs\"))\r\n#> DurationType\r\n#> duration[s]\r\n```\r\n\r\nA workaround could be to specify the type explicitly:\r\n\r\n``` r\r\ndelta <- as.difftime(c(0.000, 0.001, 0.002, 1, 1.5), units = \"secs\")\r\ndelta |> \r\n  arrow::as_arrow_array(type = arrow::duration(\"ms\"))\r\n#> Array\r\n#> <duration[ms]>\r\n#> [\r\n#>   0,\r\n#>   1,\r\n#>   2,\r\n#>   1000,\r\n#>   1500\r\n#> ]\r\n```\r\n\r\nIt looks like I inferred \"microseconds\" by default in nanoarrow although I forget the reasoning:\r\n\r\n``` r\r\nlibrary(nanoarrow)\r\n\r\ndelta <- as.difftime(c(0.000, 0.001, 0.002, 1, 1.5), units = \"secs\")\r\ndelta |> \r\n  as_nanoarrow_array() |> \r\n  arrow::as_arrow_array()\r\n#> Array\r\n#> <duration[us]>\r\n#> [\r\n#>   0,\r\n#>   1000,\r\n#>   2000,\r\n#>   1000000,\r\n#>   1500000\r\n#> ]\r\n```","Not sure where this landed but if it's not going to warn about the type casting it would be helpful to mention it in the documentation. I can submit a PR if you'd like. ","Truncating times to the nearest second isn't ok! That's not reasonable behavior! \r\n\r\nThe default should be to convert to nanoseconds, like `arrow::as_arrow_array(type = arrow::duration(\"ns\"))`. 2^64 ns is ~585 years, so you don't generally have to worry about overflow.","Related issue #32693"],"labels":["Type: bug","Component: R"]},{"title":"[JS] Remove unnecessary production dependencies","body":"### Describe the enhancement requested\n\nThe Apache Arrow JS bundle includes a few unnecessary packages that increase the dependency graph. The culprits are:\r\n\r\n- command-line-usage: used by integration.ts, json-to-arrow.ts, and arrowtocsv.ts. I _think_ these can move to dev dependencies since it looks like the intent of the scripts is integration testing and benchmarking.\r\n- @swc\/helpers: Introduced in https:\/\/github.com\/apache\/arrow\/pull\/38500. `@swc\/helpers` is never imported. I think it's used for https:\/\/github.com\/apache\/arrow\/blob\/9b931af14e5a710cba0aaa6b899e2ca696bfd785\/js\/tsconfig.json#L22. If ts-node is only for development, this can move to devDependencies.\r\n- @types\/node: Should move to devDependencies. I don't think there's any benefit to having it listed as a production dependency.\r\n- @types\/command-line-args: move to devDependencies\r\n- @types\/command-line-usage: move to devDepdencies\r\n- json-bignum: used by integration.ts, json-to-arrow.ts, and arrowtocsv.ts. Can move to dev dependencies.\r\n\r\nHere's the graph of the current dependencies.\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/22385\/5ae3eff1-394a-4b0d-9f5f-8f78167bb635)\r\n\n\n### Component(s)\n\nJavaScript","comments":[],"labels":["Type: enhancement","Component: JavaScript"]},{"title":"[Python] JSON Type Inference + datasets: `null` doesn't fallback to other type between dataset files","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nWhen inferring types in a JSON dataset, `null` doesn't fallback to other types when those types are across files in the dataset.\r\n\r\nFor example, in a JSON lines dataset like so:\r\n\r\nFile 1\r\n> {\"userId\":null}\r\n\r\nFile 2\r\n> {\"userId\": null}\r\n> {\"userId\": \"example-string\"}\r\n\r\nResults in an error:\r\n> Invalid: JSON parse error: Column(\/userId) changed from null to string in row 1\r\n\r\nHowever, [the documentation states](https:\/\/arrow.apache.org\/docs\/python\/json.html#automatic-type-inference) that \"JSON `null` values convert to the null type, but can fall back to any other type.\"\r\n\r\nSo this fallback works _within the file_:\r\n\r\nFile 1\r\n> {\"userId\":null}\r\n> {\"userId\": \"example\"}\r\n\r\nFile 2\r\n> {\"userId\":\"for-example\"}\r\n\r\nThis dataset works! Resulting in:\r\n\r\n> pyarrow.Table\r\n> userId: string\r\n\r\nVersion & Platform: pyarrow 14.0.1 and 15.0.0, Python 3.9.2\r\n\r\nFully runnable reproduction:\r\n\r\n```py\r\nimport tempfile\r\nimport pathlib\r\nimport pyarrow.dataset as ds\r\n\r\nbase = pathlib.Path(tempfile.mkdtemp(prefix=\"pyarrow-\"))\r\n(base \/ \"json_dataset\").mkdir(exist_ok=True)\r\n\r\n# Make file 1\r\nf = open(base \/ \"json_dataset\/data1.json\", \"a\")\r\nf.write('{\"userId\": null}\\n{\"userId\": null}')\r\nf.close()\r\n\r\n# Make file 2\r\nf = open(base \/ \"json_dataset\/data2.json\", \"a\")\r\nf.write('{\"userId\": null}\\n{\"userId\": \"example-string\"}')\r\nf.close()\r\n\r\n# writing it into two parquet files\r\ntable = ds.dataset(base \/ \"json_dataset\", format=\"json\")\r\nprint(table.to_table())\r\n```\r\n\r\nIn this repro, if you change the order of the files, it'll work.\r\n\r\n### Component(s)\r\n\r\nPython","comments":["Interestingly, this doesn't error, but results in an inferred type of `null`\r\n\r\n```\r\n# Make file 1\r\nf = open(base \/ \"json_dataset\/data1.json\", \"a\")\r\nf.write('{\"userId\": null}')\r\nf.close()\r\n\r\n# Make file 2\r\nf = open(base \/ \"json_dataset\/data2.json\", \"a\")\r\nf.write('{\"userId\": \"example-string\"}')\r\nf.close()\r\n```\r\n\r\nResults in:\r\n\r\n> pyarrow.Table\r\n> userId: null\r\n> \r\n> userId: [1 nulls,1 nulls]"],"labels":["Type: bug","Component: Python"]},{"title":"[C++] Support scalar aggregate expressions on ExecuteScalarExpression","body":"### Describe the enhancement requested\r\n\r\nIt would be fantastic to be able to run expressions on a dataset\/table with non-scalar expression. As a dummy example, this would return all the rows in which column \"foo\" is bigger than the last value:\r\n\r\n```\r\nimport pyarrow.compute as pc\r\nimport pyarrow as pa\r\n\r\ntable = pa.Table.from_arrays([pa.array([1, 5, 3, 4])], names=[\"foo\"])\r\nexpr = pc.field('foo') >= pc.last(pc.field('foo'))\r\n\r\n# expected:  pa.Table.from_arrays([pa.array([5, 4])], names=[\"foo\"])\r\n```\r\n\r\nThis is in Python, but the issue is that `ExecuteScalarExpression` gives you:\r\n\r\n`ArrowInvalid: ExecuteScalarExpression cannot Execute non-scalar expression (foo == last(foo))`\r\n\r\nGiven that `SCALAR_AGGREGATE`s compute scalars, I think these should be executable in `ExecuteScalarExpression` too.\r\n\r\n### Component(s)\r\n\r\nC++","comments":["See https:\/\/github.com\/apache\/arrow\/pull\/40103"],"labels":["Type: enhancement","Component: C++"]},{"title":"[Python] Windows fatal exception: access violation","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nHi,\r\n\r\nWhen using the pyarrow flight client, I have a user who occasionally sees a Windows fatal exception error. This involves a query with multiple subqueries across many fields. I do have access to the environment and can reproduce. We have found that there is some sort of correlation between the number of fields and the exception occurring. As we decrease the number of fields the issue can occur less and less consistently.\r\n\r\nI realize that getting an issue without exact steps to reproduce is unhelpful. However, I am more than willing to try out test builds or build a customer version to gather more details if I can get some guidance.\r\n\r\nI was able to easily build a custom version on Linux [per the dev docs](https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/developers\/python.rst), but I tried building a custom pyarrow on Windows and ran into issues right away with detection of the compiler. I have my steps and logs below.\r\n\r\n## Observations\r\n\r\n1. This only occurs on Windows 10 or 11; the same query runs fun on Linux\/macOS\r\n2. This only occurs when running as a Python notebook, running as a script works\r\n3. It reproduces with both Python 3.11 and 3.12\r\n4. Issues occurs with both a pip-only or conda environment\r\n5. Disabling all virus or Windows security detection does not help\r\n6. A windows event occurs calling out `arrow_flight.dll`\r\n\r\n## Windows Event Log Message\r\n\r\n```s\r\nFaulting application name: python3.12.exe, version: 3.12.1150.1013, time stamp: 0x6572422a\r\nFaulting module name: arrow_flight.dll, version: 0.0.0.0, time stamp: 0x65a69ccb\r\nException code: 0xc0000005\r\nFault offset: 0x00000000002dc6b0\r\nFaulting process id: 0x0x4F8\r\nFaulting application start time: 0x0x1DA55FAF308D836\r\nFaulting application path: C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\\python3.12.exe\r\nFaulting module path: C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\pyarrow\\arrow_flight.dll\r\nReport Id: f8313105-2c59-4f1a-a8a6-a4227a8ae7d9\r\nFaulting package full name: PythonSoftwareFoundation.Python.3.12_3.12.496.0_x64__qbz5n2kfra8p0\r\nFaulting package-relative application ID: Python\r\n```\r\n\r\n## Code\r\n\r\n```python\r\nimport json\r\nimport certifi\r\n\r\nfrom pyarrow.flight import FlightClient, Ticket, FlightCallOptions\r\n\r\nimport faulthandler\r\nfaulthandler.enable()\r\n\r\nhost = \"host\"\r\ntoken = \"token\"\r\ndatabase = \"db\"\r\n\r\nwith open(certifi.where(), \"r\", encoding=\"utf-8\") as f_cert:\r\n    cert = f_cert.read()\r\n\r\nwith open(\"kernel-crash.sql\", \"r\", encoding=\"utf-8\") as f_sql:\r\n    query = f_sql.read()\r\n\r\noptions = FlightCallOptions(**{\r\n    \"headers\": [(b\"authorization\", f\"Bearer {token}\".encode('utf-8'))],\r\n    \"timeout\": 300\r\n})\r\nticket_data = {\r\n    \"database\": database,\r\n    \"sql_query\": query,\r\n    \"query_type\": \"sql\",\r\n}\r\nticket = Ticket(json.dumps(ticket_data).encode('utf-8'))\r\nwith FlightClient(f\"grpc+tls:\/\/{host}:443\", tls_root_certs=cert) as client:\r\n    reader = client.do_get(ticket, options)\r\n    print(reader.read_all())\r\n```\r\n\r\n## Traceback\r\n\r\n```s\r\nWindows fatal exception: access violation\r\n\r\nThread 0x000026a8 (most recent call first):\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\parentpoller.py\", line 93 in run\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1073 in _bootstrap_inner\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n\r\nThread 0x00002700 (most recent call first):\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 355 in wait\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 655 in wait\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\history.py\", line 894 in run\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\history.py\", line 60 in only_when_enabled\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\decorator.py\", line 232 in fun\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1073 in _bootstrap_inner\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n\r\nThread 0x00002620 (most recent call first):\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py\", line 314 in _select\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py\", line 323 in select\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1947 in _run_once\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 639 in run_forever\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205 in start\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\control.py\", line 23 in run\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1073 in _bootstrap_inner\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n\r\nThread 0x00001ba8 (most recent call first):\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\heartbeat.py\", line 106 in run\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1073 in _bootstrap_inner\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n\r\nThread 0x00001d80 (most recent call first):\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py\", line 314 in _select\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\selectors.py\", line 323 in select\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1947 in _run_once\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 639 in run_forever\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205 in start\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py\", line 92 in _thread_main\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1010 in run\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1073 in _bootstrap_inner\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n\r\nCurrent thread 0x000025e0 (most recent call first):\r\n  File \"C:\\Users\\powersj\\AppData\\Local\\Temp\\ipykernel_9720\\769077188.py\", line 26 in <module>\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553 in run_code\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493 in run_ast_nodes\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311 in run_cell_async\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129 in _pseudo_sync_runner\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106 in _run_cell\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051 in run_cell\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549 in run_cell\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446 in do_execute\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 775 in execute_request\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359 in execute_request\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437 in dispatch_shell\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 531 in process_one\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 542 in dispatch_queue\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88 in _run\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1985 in _run_once\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 639 in run_forever\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205 in start\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739 in start\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075 in launch_instance\r\n  File \"C:\\Users\\powersj\\v3-ear\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 17 in <module>\r\n  File \"<frozen runpy>\", line 88 in _run_code\r\n  File \"<frozen runpy>\", line 198 in _run_module_as_main\r\n```\r\n\r\n### System Information\r\n\r\n```s\r\n$ python --version\r\nPython 3.11.8\r\n(venv)\r\n$ pip list\r\nPackage           Version\r\n----------------- --------\r\nasttokens         2.4.1\r\ncertifi           2024.2.2\r\ncolorama          0.4.6\r\ncomm              0.2.1\r\ndebugpy           1.8.1\r\ndecorator         5.1.1\r\nexecuting         2.0.1\r\nipdb              0.13.13\r\nipykernel         6.29.2\r\nipython           8.21.0\r\njedi              0.19.1\r\njupyter_client    8.6.0\r\njupyter_core      5.7.1\r\nmatplotlib-inline 0.1.6\r\nnest-asyncio      1.6.0\r\nnumpy             1.26.4\r\npackaging         23.2\r\nparso             0.8.3\r\npip               23.3.1\r\nplatformdirs      4.2.0\r\nprompt-toolkit    3.0.43\r\npsutil            5.9.8\r\npure-eval         0.2.2\r\npyarrow           15.0.0\r\nPygments          2.17.2\r\npython-dateutil   2.8.2\r\npywin32           306\r\npyzmq             25.1.2\r\nsetuptools        69.0.2\r\nsix               1.16.0\r\nstack-data        0.6.3\r\ntornado           6.4\r\ntraitlets         5.14.1\r\nwcwidth           0.2.13\r\nwheel             0.42.0\r\n```\r\n\r\nWhen using conda:\r\n\r\n```\r\nC:\\Users\\powersj>conda info\r\n\r\n     active environment : None\r\n       user config file : C:\\Users\\powersj\\.condarc\r\n populated config files :\r\n          conda version : 23.11.0\r\n    conda-build version : not installed\r\n         python version : 3.11.5.final.0\r\n                 solver : libmamba (default)\r\n       virtual packages : __archspec=1=x86_64\r\n                          __conda=23.11.0=0\r\n                          __win=0=0\r\n       base environment : C:\\ProgramData\\miniconda3  (read only)\r\n      conda av data dir : C:\\ProgramData\\miniconda3\\etc\\conda\r\n  conda av metadata url : None\r\n           channel URLs : https:\/\/repo.anaconda.com\/pkgs\/main\/win-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/main\/noarch\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/win-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/noarch\r\n                          https:\/\/repo.anaconda.com\/pkgs\/msys2\/win-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/msys2\/noarch\r\n          package cache : C:\\ProgramData\\miniconda3\\pkgs\r\n                          C:\\Users\\powersj\\.conda\\pkgs\r\n                          C:\\Users\\powersj\\AppData\\Local\\conda\\conda\\pkgs\r\n       envs directories : C:\\Users\\powersj\\.conda\\envs\r\n                          C:\\ProgramData\\miniconda3\\envs\r\n                          C:\\Users\\powersj\\AppData\\Local\\conda\\conda\\envs\r\n               platform : win-64\r\n             user-agent : conda\/23.11.0 requests\/2.31.0 CPython\/3.11.5 Windows\/10 Windows\/10.0.22621 solver\/libmamba conda-libmamba-solver\/23.12.0 libmambapy\/1.5.3\r\n          administrator : False\r\n             netrc file : None\r\n           offline mode : False\r\n```\r\n\r\n## Build Attempt\r\n\r\n```cmd\r\nC:\\Users\\powersj>conda create -y -n pyarrow-dev -c conda-forge ^\r\nMore?       --file arrow\\ci\\conda_env_cpp.txt ^\r\nMore?       --file arrow\\ci\\conda_env_python.txt ^\r\nMore?       --file arrow\\ci\\conda_env_gandiva.txt ^\r\nMore?       python=3.11\r\n\r\n<snip>\r\n\r\nC:\\Users\\powersj>conda activate pyarrow-dev\r\n\r\n(pyarrow-dev) C:\\Users\\powersj>set ARROW_HOME=%CONDA_PREFIX%\\Library\r\n\r\n(pyarrow-dev) C:\\Users\\powersj>mkdir arrow\\cpp\\build\r\n\r\n(pyarrow-dev) C:\\Users\\powersj>pushd arrow\\cpp\\build\r\n\r\n(pyarrow-dev) C:\\Users\\powersj\\arrow\\cpp\\build>cmake -G \"Ninja\" ^\r\nMore?       -DCMAKE_INSTALL_PREFIX=%ARROW_HOME% ^\r\nMore?       -DCMAKE_UNITY_BUILD=ON ^\r\nMore?       -DARROW_COMPUTE=ON ^\r\nMore?       -DARROW_CSV=ON ^\r\nMore?       -DARROW_CXXFLAGS=\"\/WX \/MP\" ^\r\nMore?       -DARROW_DATASET=ON ^\r\nMore?       -DARROW_FILESYSTEM=ON ^\r\nMore?       -DARROW_HDFS=ON ^\r\nMore?       -DARROW_JSON=ON ^\r\nMore?       -DARROW_PARQUET=ON ^\r\nMore?       -DARROW_WITH_LZ4=ON ^\r\nMore?       -DARROW_WITH_SNAPPY=ON ^\r\nMore?       -DARROW_WITH_ZLIB=ON ^\r\nMore?       -DARROW_WITH_ZSTD=ON ^\r\nMore?       -DARROW_FLIGHT=ON ^\r\nMore?       ..\r\n-- Building using CMake version: 3.28.3\r\n-- The C compiler identification is Clang 17.0.6 with GNU-like command-line\r\n-- The CXX compiler identification is unknown\r\nCMake Error at C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/share\/cmake-3.28\/Modules\/Platform\/Windows-Clang.cmake:170 (message):\r\n  The current configuration mixes Clang and MSVC or some other CL compatible\r\n  compiler tool.  This is not supported.  Use either clang or MSVC as both C,\r\n  C++ and\/or HIP compilers.\r\nCall Stack (most recent call first):\r\n  C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/share\/cmake-3.28\/Modules\/Platform\/Windows-Clang.cmake:180 (__verify_same_language_values)\r\n  C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/share\/cmake-3.28\/Modules\/Platform\/Windows-Clang-C.cmake:1 (include)\r\n  C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/share\/cmake-3.28\/Modules\/CMakeCInformation.cmake:48 (include)\r\n  CMakeLists.txt:95 (project)\r\n\r\n\r\nCMake Error at CMakeLists.txt:95 (project):\r\n  No CMAKE_CXX_COMPILER could be found.\r\n\r\n  Tell CMake where to find the compiler by setting either the environment\r\n  variable \"CXX\" or the CMake cache entry CMAKE_CXX_COMPILER to the full path\r\n  to the compiler, or to the compiler name if it is in the PATH.\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\n(pyarrow-dev) C:\\Users\\powersj\\arrow\\cpp\\build>\r\n```\r\n\r\nIt is not clear to me what compiler I am suppose to use, either something from the conda environment or the locally installed one?\r\n\r\nIf I try setting via the CC and CXX env variables I get:\r\n\r\n```\r\nset CC=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\bin\\Hostx64\\x64\\cl.exe\r\nset CXX=C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.39.33519\\bin\\Hostx64\\x64\\cl.exe\r\n\r\n<snip>\r\n-- Building using CMake version: 3.28.3\r\n-- The C compiler identification is MSVC 19.39.33519.0\r\n-- The CXX compiler identification is MSVC 19.39.33519.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - failed\r\n-- Check for working C compiler: C:\/Program Files\/Microsoft Visual Studio\/2022\/Community\/VC\/Tools\/MSVC\/14.39.33519\/bin\/Hostx64\/x64\/cl.exe\r\n-- Check for working C compiler: C:\/Program Files\/Microsoft Visual Studio\/2022\/Community\/VC\/Tools\/MSVC\/14.39.33519\/bin\/Hostx64\/x64\/cl.exe - broken\r\nCMake Error at C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/share\/cmake-3.28\/Modules\/CMakeTestCCompiler.cmake:67 (message):\r\n  The C compiler\r\n\r\n    \"C:\/Program Files\/Microsoft Visual Studio\/2022\/Community\/VC\/Tools\/MSVC\/14.39.33519\/bin\/Hostx64\/x64\/cl.exe\"\r\n\r\n  is not able to compile a simple test program.\r\n\r\n  It fails with the following output:\r\n\r\n    Change Dir: 'C:\/Users\/powersj\/arrow\/cpp\/build\/CMakeFiles\/CMakeScratch\/TryCompile-j51cjy'\r\n\r\n    Run Build Command(s): C:\/Users\/powersj\/.conda\/envs\/pyarrow-dev\/Library\/bin\/ninja.exe -v cmTC_f4d4d\r\n    [1\/2] C:\\PROGRA~1\\MICROS~2\\2022\\COMMUN~1\\VC\\Tools\\MSVC\\1439~1.335\\bin\\Hostx64\\x64\\cl.exe  \/nologo   \/DWIN32 \/D_WINDOWS  \/Zi \/Ob0 \/Od \/RTC1 -MDd \/showIncludes \/FoCMakeFiles\\cmTC_f4d4d.dir\\testCCompiler.c.obj \/FdCMakeFiles\\cmTC_f4d4d.dir\\ \/FS -c C:\\Users\\powersj\\arrow\\cpp\\build\\CMakeFiles\\CMakeScratch\\TryCompile-j51cjy\\testCCompiler.c\r\n    [2\/2] C:\\WINDOWS\\system32\\cmd.exe \/C \"cd . && C:\\Users\\powersj\\.conda\\envs\\pyarrow-dev\\Library\\bin\\cmake.exe -E vs_link_exe --intdir=CMakeFiles\\cmTC_f4d4d.dir --rc=rc --mt=CMAKE_MT-NOTFOUND --manifests  -- C:\\PROGRA~1\\MICROS~2\\2022\\COMMUN~1\\VC\\Tools\\MSVC\\1439~1.335\\bin\\Hostx64\\x64\\link.exe \/nologo CMakeFiles\\cmTC_f4d4d.dir\\testCCompiler.c.obj  \/out:cmTC_f4d4d.exe \/implib:cmTC_f4d4d.lib \/pdb:cmTC_f4d4d.pdb \/version:0.0 \/machine:x64  \/debug \/INCREMENTAL \/subsystem:console  kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib && cd .\"\r\n    FAILED: cmTC_f4d4d.exe\r\n    C:\\WINDOWS\\system32\\cmd.exe \/C \"cd . && C:\\Users\\powersj\\.conda\\envs\\pyarrow-dev\\Library\\bin\\cmake.exe -E vs_link_exe --intdir=CMakeFiles\\cmTC_f4d4d.dir --rc=rc --mt=CMAKE_MT-NOTFOUND --manifests  -- C:\\PROGRA~1\\MICROS~2\\2022\\COMMUN~1\\VC\\Tools\\MSVC\\1439~1.335\\bin\\Hostx64\\x64\\link.exe \/nologo CMakeFiles\\cmTC_f4d4d.dir\\testCCompiler.c.obj  \/out:cmTC_f4d4d.exe \/implib:cmTC_f4d4d.lib \/pdb:cmTC_f4d4d.pdb \/version:0.0 \/machine:x64  \/debug \/INCREMENTAL \/subsystem:console  kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib && cd .\"\r\n    RC Pass 1: command \"rc \/fo CMakeFiles\\cmTC_f4d4d.dir\/manifest.res CMakeFiles\\cmTC_f4d4d.dir\/manifest.rc\" failed (exit code 0) with the following output:\r\n    The system cannot find the file specified\r\n    ninja: build stopped: subcommand failed.\r\n\r\n\r\n\r\n\r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:95 (project)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\n\r\n(pyarrow-dev) C:\\Users\\powersj\\arrow\\cpp\\build>\r\n```\r\n\n\n### Component(s)\n\nPython","comments":["Hi @powersj. This looks similar to https:\/\/github.com\/apache\/arrow\/issues\/37852 though we weren't able to reproduce in that issue. I couldn't reproduce, though I had to modify your script to run on my system and create a simple server implementation to test. Would it be possible to share a self-contained example of both the client and server code? cc @lidavidm \r\n\r\nOne thing that jumped out at me in your logs are the lines in your traceback like this:\r\n\r\n```\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1030 in _bootstrap\r\n```\r\n\r\nThat looks like references to the Windows Store version of Python but I'd expect all the paths to lead to your conda environment so I wonder if the crash is due to mixing two Python environments. \r\n\r\nHave you tried capturing your crash with [WinDbg Preview](https:\/\/apps.microsoft.com\/detail\/9PGJGD53TN86)?\r\n\r\n@assignUser @kou do either of you have any idea about the build issue at the bottom of the OP? I also get that when I try to do a Windows+conda+clang build.","For Windows, you'll want to use vcvarsall.bat or whatever the modern equivalent is, don't muck with the env vars yourself. Also, possibly try the VS generator for CMake instead of Ninja.\r\n\r\nI don't have any clue about the crash itself. We would need a way to reproduce it.\r\n\r\nYou could also try downloading \"Windbg Preview\" from the Windows Store and running your script as `windbgx -g python myscript.py` to get a traceback.","> For Windows, you'll want to use vcvarsall.bat or whatever the modern equivalent is, don't muck with the env vars yourself.\r\n\r\nI think so too.\r\n\r\n> Also, possibly try the VS generator for CMake instead of Ninja.\r\n\r\nIf you use one of Visual Studio Generators https:\/\/cmake.org\/cmake\/help\/latest\/manual\/cmake-generators.7.html#visual-studio-generators , you don't need to use `vcvarsall.bat`. CMake will find suitable Visual C++.","Thanks @kou. I'll send a PR to make this clearer in the Python docs. It wasn't clear to me which toolchain we were supporting there. I think it's fairly clear in the [C++ docs](https:\/\/arrow.apache.org\/docs\/developers\/cpp\/windows.html#system-setup). I'll try that it tomorrow.","Thanks for all the responses, especially around building the Python libraries on Windows. It does seem that changing the cmake target has allowed me to get further along via `cmake -G \"Visual Studio 17 2022\" -A x64 ...`.\r\n\r\nI did find the Python build requires the older 2017 libraries installed that are already referenced in the Python docs. I had some success with the debugger below though.\r\n\r\n> You could also try downloading \"Windbg Preview\" from the Windows Store and running your script as windbgx -g python myscript.py to get a traceback.\r\n\r\nI launched the notebook and attached to the python process with the time travel option and caught it. How can I better share this with you? Does this collect anything helpful? Would it help to share the time travel capture? fwiw it is 620MB.\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/6453401\/e1a5ec4f-0697-45df-a0bd-119ebb14d753)\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/6453401\/a8929046-7303-411d-894b-6b4ae35d0b2c)\r\n\r\n\r\n","Full stack:\r\n\r\n```\r\n[0x0]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x14d0e0   0x4e3b1eb7f0   0x7ffc3c81e7ca   \r\n[0x1]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x14f1fa   0x4e3b1eb850   0x7ffc3cc191da   \r\n[0x2]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x549c0a   0x4e3b1eb890   0x7ffc3c82110e   \r\n[0x3]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x151b3e   0x4e3b1eb8f0   0x7ffc3c91f46f   \r\n[0x4]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x24fe9f   0x4e3b1eb9a0   0x7ffc3c8370ab   \r\n[0x5]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x167adb   0x4e3b1eb9d0   0x7ffc3c83e219   \r\n[0x6]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x16ec49   0x4e3b1ebac0   0x7ffc3c81b6a5   \r\n[0x7]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x14c0d5   0x4e3b1ebc50   0x7ffc3c7aca49   \r\n[0x8]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0xdd479   0x4e3b1ebcc0   0x7ffc3c869402   \r\n[0x9]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x199e32   0x4e3b1ebd20   0x7ffc3c81bbd5   \r\n[0xa]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0x14c605   0x4e3b1ebdf0   0x7ffc3c7b8c57   \r\n[0xb]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0xe9687   0x4e3b1ebe30   0x7ffc3c7ba351   \r\n[0xc]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0xead81   0x4e3b1ec030   0x7ffc3c6d9f51   \r\n[0xd]   arrow_flight!arrow::flight::FlightWriteSizeStatusDetail::type_id+0xa981   0x4e3b1ec090   0x7ffc3c6b04f7   \r\n[0xe]   arrow_flight!arrow::flight::MakeTracingServerMiddlewareFactory+0x1427   0x4e3b1ec0f0   0x7ffc3c6b30d9   \r\n[0xf]   arrow_flight!arrow::flight::FlightClient::PollFlightInfo+0x2a99   0x4e3b1ec180   0x7ffc3c6c8c93   \r\n[0x10]   arrow_flight!arrow::flight::MakeTracingClientMiddlewareFactory+0xc3   0x4e3b1ec380   0x7ffc3c6ccdcd   \r\n[0x11]   arrow_flight!arrow::flight::FlightServerBase::Shutdown+0x119d   0x4e3b1ec480   0x7ffc3c6cd57d   \r\n[0x12]   arrow_flight!arrow::flight::FlightStreamReader::ToTable+0x3d   0x4e3b1ec590   0x7ffc7329b43f   \r\n[0x13]   _flight_cp311_win_amd64 + 0x5b43f!_flight_cp311_win_amd64+0x5b43f   0x4e3b1ec680   0x7ffc3e845f04   \r\n[0x14]   python311!PyObject_VectorcallMethod+0x1b0   0x4e3b1ec780   0x7ffc3e784c2c   \r\n[0x15]   python311!PyObject_Vectorcall+0x5dc   0x4e3b1ec7d0   0x7ffc3e7860b2   \r\n[0x16]   python311!PyEval_EvalFrameDefault+0x7a2   0x4e3b1ec8e0   0x7ffc3e7e56bf   \r\n[0x17]   python311!PyType_CalculateMetaclass+0xfb   0x4e3b1ecaf0   0x7ffc3e7e70bf   \r\n[0x18]   python311!PyEval_EvalCode+0x97   0x4e3b1ecb30   0x7ffc3e870e50   \r\n[0x19]   python311!Py_GetRecursionLimit+0x53c   0x4e3b1ecbb0   0x7ffc3e870d20   \r\n[0x1a]   python311!Py_GetRecursionLimit+0x40c   0x4e3b1ecc30   0x7ffc3e789c44   \r\n[0x1b]   python311!PyEval_EvalFrameDefault+0x4334   0x4e3b1ecce0   0x7ffc3e7a370f   \r\n[0x1c]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ecef0   0x7ffc3e8d015d   \r\n[0x1d]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ecf40   0x7ffc3e789667   \r\n[0x1e]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1ecf80   0x7ffc3e7a370f   \r\n[0x1f]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ed190   0x7ffc3e8d015d   \r\n[0x20]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ed1e0   0x7ffc3e789667   \r\n[0x21]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1ed220   0x7ffc3e7a370f   \r\n[0x22]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ed430   0x7ffc3e84709b   \r\n[0x23]   python311!PyGen_Finalize+0x263   0x4e3b1ed480   0x7ffc3e7ac0fb   \r\n[0x24]   python311!PySequence_Tuple+0x537   0x4e3b1ed4d0   0x7ffc3e784c2c   \r\n[0x25]   python311!PyObject_Vectorcall+0x5dc   0x4e3b1ed510   0x7ffc3e7860b2   \r\n[0x26]   python311!PyEval_EvalFrameDefault+0x7a2   0x4e3b1ed620   0x7ffc3e7b6f94   \r\n[0x27]   python311!PyFunction_Vectorcall+0x1a4   0x4e3b1ed830   0x7ffc3e7b84cd   \r\n[0x28]   python311!PyFunction_Vectorcall+0x16dd   0x4e3b1ed8c0   0x7ffc3e80036a   \r\n[0x29]   python311!PyObject_CallObject+0x37e   0x4e3b1ed9c0   0x7ffc3e78ac94   \r\n[0x2a]   python311!PyEval_EvalFrameDefault+0x5384   0x4e3b1eda20   0x7ffc3e7a370f   \r\n[0x2b]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1edc30   0x7ffc3e8d015d   \r\n[0x2c]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1edc80   0x7ffc3e789667   \r\n[0x2d]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1edcc0   0x7ffc3e7a370f   \r\n[0x2e]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1eded0   0x7ffc3e8d015d   \r\n[0x2f]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1edf20   0x7ffc3e789667   \r\n[0x30]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1edf60   0x7ffc3e7a370f   \r\n[0x31]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ee170   0x7ffc3e8d015d   \r\n[0x32]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ee1c0   0x7ffc3e789667   \r\n[0x33]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1ee200   0x7ffc3e7a370f   \r\n[0x34]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ee410   0x7ffc3e8d015d   \r\n[0x35]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ee460   0x7ffc3e789667   \r\n[0x36]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1ee4a0   0x7ffc3e7a370f   \r\n[0x37]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ee6b0   0x7ffc3e8d015d   \r\n[0x38]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ee700   0x7ffc3e789667   \r\n[0x39]   python311!PyEval_EvalFrameDefault+0x3d57   0x4e3b1ee740   0x7ffc3e7a370f   \r\n[0x3a]   python311!PyDict_MergeFromSeq2+0x3cf   0x4e3b1ee950   0x7ffc3e8d015d   \r\n[0x3b]   python311!PyLong_AsUnsignedLongMask+0xc1   0x4e3b1ee9a0   0x7ffc6ad558df   \r\n[0x3c]   _asyncio!PyInit__asyncio+0x48df   0x4e3b1ee9e0   0x7ffc6ad55753   \r\n[0x3d]   _asyncio!PyInit__asyncio+0x4753   0x4e3b1eea80   0x7ffc6ad5602f   \r\n[0x3e]   _asyncio!PyInit__asyncio+0x502f   0x4e3b1eeab0   0x7ffc3e7b9c0c   \r\n[0x3f]   python311!PyIter_Send+0x13ec   0x4e3b1eeaf0   0x7ffc3e9b8b31   \r\n[0x40]   python311!PyContext_NewHamtForTests+0x51   0x4e3b1eeb50   0x7ffc3e9b8e11   \r\n[0x41]   python311!PyContext_NewHamtForTests+0x331   0x4e3b1eeb90   0x7ffc3e7ba86c   \r\n[0x42]   python311!PyArg_CheckPositional+0x12c   0x4e3b1eebe0   0x7ffc3e800773   \r\n[0x43]   python311!PyObject_Call+0x5b   0x4e3b1eec20   0x7ffc3e800440   \r\n[0x44]   python311!PyObject_CallObject+0x454   0x4e3b1eec80   0x7ffc3e78ac94   \r\n[0x45]   python311!PyEval_EvalFrameDefault+0x5384   0x4e3b1eece0   0x7ffc3e7e56bf   \r\n[0x46]   python311!PyType_CalculateMetaclass+0xfb   0x4e3b1eeef0   0x7ffc3e7e70bf   \r\n[0x47]   python311!PyEval_EvalCode+0x97   0x4e3b1eef30   0x7ffc3e870e50   \r\n[0x48]   python311!Py_GetRecursionLimit+0x53c   0x4e3b1eefb0   0x7ffc3e870d20   \r\n[0x49]   python311!Py_GetRecursionLimit+0x40c   0x4e3b1ef030   0x7ffc3e7ba86c   \r\n[0x4a]   python311!PyArg_CheckPositional+0x12c   0x4e3b1ef0e0   0x7ffc3e784c2c   \r\n[0x4b]   python311!PyObject_Vectorcall+0x5dc   0x4e3b1ef120   0x7ffc3e7860b2   \r\n[0x4c]   python311!PyEval_EvalFrameDefault+0x7a2   0x4e3b1ef230   0x7ffc3e7b6f94   \r\n[0x4d]   python311!PyFunction_Vectorcall+0x1a4   0x4e3b1ef440   0x7ffc3e800773   \r\n[0x4e]   python311!PyObject_Call+0x5b   0x4e3b1ef4d0   0x7ffc3e835788   \r\n[0x4f]   python311!PyRun_SimpleStringFlags+0x230   0x4e3b1ef530   0x7ffc3e83594b   \r\n[0x50]   python311!Py_RunMain+0x137   0x4e3b1ef580   0x7ffc3e835829   \r\n[0x51]   python311!Py_RunMain+0x15   0x4e3b1ef5f0   0x7ff614fa42ef   \r\n[0x52]   python3_11 + 0x42ef!python3_11+0x42ef   0x4e3b1ef620   0x7ff614fa58b4   \r\n[0x53]   python3_11 + 0x58b4!python3_11+0x58b4   0x4e3b1ef8c0   0x7ffc84a2257d   \r\n[0x54]   KERNEL32!BaseThreadInitThunk+0x1d   0x4e3b1ef900   0x7ffc856caa58   \r\n[0x55]   ntdll!RtlUserThreadStart+0x28   0x4e3b1ef930   0x0   \r\n```","Shoot. I think I've seen this once or twice but was never able to figure it out.\r\n\r\nRight here you basically make an impossible\/nonsensical jump:\r\n\r\n```\r\n[0x11]   arrow_flight!arrow::flight::FlightServerBase::Shutdown+0x119d   0x4e3b1ec480   0x7ffc3c6cd57d   \r\n[0x12]   arrow_flight!arrow::flight::FlightStreamReader::ToTable+0x3d   0x4e3b1ec590   0x7ffc7329b43f   \r\n```\r\n\r\nThat is, ToTable should never call that function. So something is seriously borked. I don't really want to blame a \"compiler bug\" but...\r\n\r\nWell. When you generated this stack, which PyArrow package were you using? (If a wheel, what version _exactly_?) We could disassemble `ToTable` at that offset and see if there's any explanation for how it managed to pull off that jump.","> which PyArrow package were you using?\r\n\r\nI am going to assume `pyarrow-15.0.0-cp311-cp311-win_amd64.whl` based on the following:\r\n\r\n```s\r\n$ pip show pyarrow\r\nName: pyarrow\r\nVersion: 15.0.0\r\nSummary: Python library for Apache Arrow\r\nHome-page: https:\/\/arrow.apache.org\/\r\nAuthor:\r\nAuthor-email:\r\nLicense: Apache License, Version 2.0\r\nLocation: C:\\Users\\powersj\\v3-ear\\venv\\Lib\\site-packages\r\nRequires: numpy\r\nRequired-by:\r\n```\r\n\r\nDigging through the site-packages the pyarrow-15.0.0.dist-info\/WHEEL I see:\r\n\r\n```s\r\nWheel-Version: 1.0\r\nGenerator: bdist_wheel (0.41.1)\r\nRoot-Is-Purelib: false\r\nTag: cp311-cp311-win_amd64\r\n```\r\n\r\n\r\n","Ok. I think it's a virtual call:\r\n\r\n```\r\n                             **************************************************************\r\n                             *                          FUNCTION                          *\r\n                             **************************************************************\r\n                             undefined ?ToTable@FlightStreamReader@flight@arrow@@QEAA\r\n                               assume GS_OFFSET = 0xff00000000\r\n             undefined         AL:1           <RETURN>\r\n                             0x18d540  548  ?ToTable@FlightStreamReader@flight@arrow@@QEAA\r\n                             Ordinal_548                                     XREF[4]:     Entry Point(*), \r\n                             ?ToTable@FlightStreamReader@flight@arrow@@QEAA               FUN_18018d520:18018d52d(c), \r\n                                                                                          18096cad4(*), 1809c53d0(*)  \r\n       18018d540 40 55           PUSH       RBP\r\n       18018d542 53              PUSH       RBX\r\n       18018d543 56              PUSH       RSI\r\n       18018d544 57              PUSH       RDI\r\n       18018d545 41 56           PUSH       R14\r\n       18018d547 48 8d 6c        LEA        RBP,[RSP + -0x37]\r\n                 24 c9\r\n       18018d54c 48 81 ec        SUB        RSP,0xc0\r\n                 c0 00 00 00\r\n       18018d553 48 c7 45        MOV        qword ptr [RBP + -0x29],-0x2\r\n                 d7 fe ff \r\n                 ff ff\r\n       18018d55b 48 8b 05        MOV        RAX,qword ptr [DAT_18098d388]                    = 00002B992DDFA232h\r\n                 26 fe 7f 00\r\n       18018d562 48 33 c4        XOR        RAX,RSP\r\n       18018d565 48 89 45 27     MOV        qword ptr [RBP + 0x27],RAX\r\n       18018d569 48 8b f2        MOV        RSI,RDX\r\n       18018d56c 48 8b d9        MOV        RBX,RCX\r\n       18018d56f 48 89 55 b7     MOV        qword ptr [RBP + -0x49],RDX\r\n       18018d573 48 8b 01        MOV        RAX,qword ptr [RCX]\r\n       18018d576 48 8d 55 df     LEA        RDX,[RBP + -0x21]\r\n                             LAB_18018d57a                                   XREF[1]:     1808cc9e8(*)  \r\n       18018d57a ff 50 30        CALL       qword ptr [RAX + 0x30]\r\n       18018d57d 90              NOP\r\n```\r\n\r\nThat'd make sense given the implementation: https:\/\/github.com\/apache\/arrow\/blob\/11ef68d7dc2e15c81dfc75f4304070021ad42a1e\/cpp\/src\/arrow\/flight\/client.cc#L113-L118\r\n\r\nSo I'd hazard that we have a nullptr or otherwise invalid reader here, and instead of crashing we're just jumping to oblivion. That doesn't explain _how_ we got said reader...","Here's another curious thing.\r\n\r\n```\r\n[0x3c]   _asyncio!PyInit__asyncio+0x48df   0x4e3b1ee9e0   0x7ffc6ad55753   \r\n[0x3d]   _asyncio!PyInit__asyncio+0x4753   0x4e3b1eea80   0x7ffc6ad5602f   \r\n[0x3e]   _asyncio!PyInit__asyncio+0x502f   0x4e3b1eeab0   0x7ffc3e7b9c0c   \r\n```\r\n\r\n...that's supposed to initialize the asyncio native library. How is _that_ in the stack trace? ","Hmm, actually, you mention this only happens in a notebook? Does IPython fork the Python kernel process or something?","> you mention this only happens in a notebook? Does IPython fork the Python kernel process or something?\r\n\r\nCorrect, I seem to be able to run the same code as a python script (e.g. `python script.py`) all day long, but once it is tossed into a notebook it crashes. I have struggled to find any information on debugging the ipython kernel or to break this down any further in the event that it is not actually an issue with pyarrow.\r\n\r\nWhat else could I provide to help dig into this further?","I think we're going to have to replicate it, and then try to track down a debug build, unfortunately. Or if you know the Python stack trace of the crash we could start investigating from that side.","Thanks for looking into this so far.\r\n\r\n> Or if you know the Python stack trace of the crash we could start investigating from that side.\r\n\r\nIn my original comment I used faulthandler to grab a traceback, does that provide any pointers?","If I'm not mistaken, I don't see any Flight RPC frames in that traceback. ","Hmm, or well possibly it's \r\n\r\n```\r\nCurrent thread 0x000025e0 (most recent call first):\r\n  File \"C:\\Users\\powersj\\AppData\\Local\\Temp\\ipykernel_9720\\769077188.py\", line 26 in <module>\r\n```\r\n\r\nbut L26 there is just in the middle of making a dictionary...","@lidavidm is there anything else I could try or provide?","I think either we need a reproducer to look at, or we need to figure out how to produce a debug build and get a backtrace that way. But I've never figured out exactly how to get a debug build working on Windows.","It's also possible that it's something like https:\/\/github.com\/grpc\/grpc\/issues\/29185 which I never managed to track down.","Would getting the debug grpc logs aid to confirm that it might be related? ","We can look, but for that issue, I had to attach a debugger - the debug grpc logs don't really tell much in case of a crash","Sorry, I haven't gotten any time to actually fire up a Windows VM and try to attempt anything - I'm heavily timeboxed these days and anything Windows automatically eats up a good portion of the day :frowning_face: ","> anything Windows automatically eats up a good portion of the day\r\n\r\nCompletely understand, especially since I am unable to provide a direct reproducer. Please do let me know if there is anything else I can provide or help with. Very happy to run some sort of debug build as well.","I've already spent a bit of time on a reproduction (no luck so far) and can also see about a debug build while I'm there. I'll update here with what I find."],"labels":["Type: bug","Component: Python"]},{"title":"[C++][Parquet] Expand ParquetVersion enum values","body":"### Describe the enhancement requested\n\nThe latest released Parquet format version is 2.10.0, but our ParquetVersion enum only goes up to 2.6.0. We should fill in the missing values. For example, 2.8.0 adds the BYTE_STREAM_SPLIT encoding for floats.\n\n### Component(s)\n\nC++, Parquet","comments":["cc @jorisvandenbossche @mapleFU @wgtmac ","@pitrou I've considering this problem before, we talked about it here: https:\/\/github.com\/apache\/arrow\/issues\/35776 , I forgot this previously"],"labels":["Type: enhancement","Component: Parquet","Component: C++"]},{"title":"[Python] mutex lock failed: Invalid argument when importing after tink (M1, macOS 14.2.1)","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n- macOS 14.2.1, M1\r\n- clang 15.0.0 (clang-1500.1.0.2.5, arm64-apple-darwin23.2.0)\r\n- Python 3.8.18\r\n- Pyarrow 15.0.0\r\n- Tink 1.9.0 (install used prebuilt wheel tink-1.9.0-cp38-cp38-macosx_11_0_universal2.whl)\r\n- OpenSSL 3.2.0 23 Nov 2023\r\n\r\n```python\r\nimport tink\r\nimport pyarrow\r\n```\r\n\r\nResults in:\r\n\r\n```\r\nlibc++abi: terminating due to uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument\r\n```\r\n\r\nReversing import order results in the process hanging with:\r\n\r\n```\r\n[mutex.cc : 453] RAW: Lock blocking 0x156747c38   @\r\n```\r\n\r\nSame error message as #15189. Just in case similar root cause, OpenSSL version Python is built with is `OpenSSL 3.2.0 23 Nov 2023`.\r\n\n\n### Component(s)\n\nPython","comments":["Can you perhaps obtain a backtrace using lldb?","```\r\n0.\tProgram arguments: \/Library\/Developer\/CommandLineTools\/usr\/bin\/lldb \/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/bin\/python3\r\nStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\r\n0  lldb                     0x000000010414f7dc llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 56\r\n1  lldb                     0x000000010414ed38 llvm::sys::RunSignalHandlers() + 112\r\n2  lldb                     0x000000010414fe14 SignalHandler(int) + 304\r\n3  libsystem_platform.dylib 0x000000018180da24 _sigtramp + 56\r\n4  libsystem_pthread.dylib  0x00000001817ddcc0 pthread_kill + 288\r\n5  libsystem_c.dylib        0x00000001816e9a40 abort + 180\r\n6  libc++abi.dylib          0x0000000181795070 __cxxabiv1::__aligned_malloc_with_fallback(unsigned long) + 0\r\n7  libc++abi.dylib          0x0000000181785110 demangling_terminate_handler() + 320\r\n8  libobjc.A.dylib          0x000000018142b99c _objc_terminate() + 160\r\n9  libc++abi.dylib          0x0000000181794434 std::__terminate(void (*)()) + 16\r\n10 libc++abi.dylib          0x0000000181797520 __cxa_get_exception_ptr + 0\r\n11 libc++abi.dylib          0x0000000181797464 __cxxabiv1::exception_cleanup_func(_Unwind_Reason_Code, _Unwind_Exception*) + 0\r\n12 libc++.1.dylib           0x00000001817136b8 std::__1::__throw_system_error(int, char const*) + 100\r\n13 libc++.1.dylib           0x0000000181708318 std::__1::mutex::try_lock() + 0\r\n14 libarrow.1500.dylib      0x0000000110d6a330 google::protobuf::internal::OnShutdownRun(void (*)(void const*), void const*) + 72\r\n15 libarrow.1500.dylib      0x0000000110d61880 google::protobuf::internal::InitProtobufDefaultsSlow() + 84\r\n16 libarrow.1500.dylib      0x0000000110dfaca8 google::protobuf::(anonymous namespace)::AddDescriptors(google::protobuf::internal::DescriptorTable const*) + 128\r\n17 libarrow.1500.dylib      0x0000000110dfacec google::protobuf::internal::AddDescriptorsRunner::AddDescriptorsRunner(google::protobuf::internal::DescriptorTable const*) + 24\r\n18 dyld                     0x0000000181479a24 invocation function for block in dyld4::Loader::findAndRunAllInitializers(dyld4::RuntimeState&) const::$_0::operator()() const + 168\r\n19 dyld                     0x00000001814bf0f4 invocation function for block in dyld3::MachOAnalyzer::forEachInitializer(Diagnostics&, dyld3::MachOAnalyzer::VMAddrConverter const&, void (unsigned int) block_pointer, void const*) const + 172\r\n20 dyld                     0x00000001814b2668 invocation function for block in dyld3::MachOFile::forEachSection(void (dyld3::MachOFile::SectionInfo const&, bool, bool&) block_pointer) const + 496\r\n21 dyld                     0x00000001814592fc dyld3::MachOFile::forEachLoadCommand(Diagnostics&, void (load_command const*, bool&) block_pointer) const + 300\r\n22 dyld                     0x00000001814b16a0 dyld3::MachOFile::forEachSection(void (dyld3::MachOFile::SectionInfo const&, bool, bool&) block_pointer) const + 192\r\n23 dyld                     0x00000001814b4188 dyld3::MachOFile::forEachInitializerPointerSection(Diagnostics&, void (unsigned int, unsigned int, bool&) block_pointer) const + 160\r\n24 dyld                     0x00000001814bede8 dyld3::MachOAnalyzer::forEachInitializer(Diagnostics&, dyld3::MachOAnalyzer::VMAddrConverter const&, void (unsigned int) block_pointer, void const*) const + 432\r\n25 dyld                     0x0000000181475b38 dyld4::Loader::findAndRunAllInitializers(dyld4::RuntimeState&) const + 524\r\n26 dyld                     0x000000018147bf70 dyld4::JustInTimeLoader::runInitializers(dyld4::RuntimeState&) const + 36\r\n27 dyld                     0x0000000181475f24 dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array<dyld4::Loader const*>&) const + 220\r\n28 dyld                     0x0000000181475f00 dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array<dyld4::Loader const*>&) const + 184\r\n29 dyld                     0x0000000181475f00 dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array<dyld4::Loader const*>&) const + 184\r\n30 dyld                     0x0000000181475f00 dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array<dyld4::Loader const*>&) const + 184\r\n31 dyld                     0x0000000181475f00 dyld4::Loader::runInitializersBottomUp(dyld4::RuntimeState&, dyld3::Array<dyld4::Loader const*>&) const + 184\r\n32 dyld                     0x0000000181479ab0 dyld4::Loader::runInitializersBottomUpPlusUpwardLinks(dyld4::RuntimeState&) const::$_1::operator()() const + 112\r\n33 dyld                     0x00000001814760f0 dyld4::Loader::runInitializersBottomUpPlusUpwardLinks(dyld4::RuntimeState&) const + 380\r\n34 dyld                     0x00000001814947ac dyld4::APIs::dlopen_from(char const*, int, void*) + 1576\r\n35 Python3                  0x0000000104ccedf4 _Py_GetLocaleconvNumeric + 756\r\n36 Python3                  0x0000000104c9f66c PyImport_AppendInittab + 5100\r\n37 Python3                  0x0000000104c9ee08 PyImport_AppendInittab + 2952\r\n38 Python3                  0x0000000104beb188 PyCMethod_New + 764\r\n39 Python3                  0x0000000104c77e50 _PyEval_EvalFrameDefault + 23428\r\n40 Python3                  0x0000000104c7b7ec _PyEval_EvalFrameDefault + 38176\r\n41 Python3                  0x0000000104ba8f10 _PyFunction_Vectorcall + 236\r\n42 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n43 Python3                  0x0000000104c77b78 _PyEval_EvalFrameDefault + 22700\r\n44 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n45 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n46 Python3                  0x0000000104c77b54 _PyEval_EvalFrameDefault + 22664\r\n47 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n48 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n49 Python3                  0x0000000104c77bf4 _PyEval_EvalFrameDefault + 22824\r\n50 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n51 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n52 Python3                  0x0000000104c77bf4 _PyEval_EvalFrameDefault + 22824\r\n53 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n54 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n55 Python3                  0x0000000104c77bf4 _PyEval_EvalFrameDefault + 22824\r\n56 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n57 Python3                  0x0000000104baa0cc PyObject_CallMethodObjArgs + 492\r\n58 Python3                  0x0000000104baa258 _PyObject_CallMethodIdObjArgs + 112\r\n59 Python3                  0x0000000104c9daf4 PyImport_ImportModuleLevelObject + 1284\r\n60 Python3                  0x0000000104c76888 _PyEval_EvalFrameDefault + 17852\r\n61 Python3                  0x0000000104c7b7ec _PyEval_EvalFrameDefault + 38176\r\n62 Python3                  0x0000000104c72204 PyEval_EvalCode + 80\r\n63 Python3                  0x0000000104c6eb5c _PyAST_ExprAsUnicode + 19156\r\n64 Python3                  0x0000000104beb188 PyCMethod_New + 764\r\n65 Python3                  0x0000000104c77e50 _PyEval_EvalFrameDefault + 23428\r\n66 Python3                  0x0000000104c7b7ec _PyEval_EvalFrameDefault + 38176\r\n67 Python3                  0x0000000104ba8f10 _PyFunction_Vectorcall + 236\r\n68 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n69 Python3                  0x0000000104c77b78 _PyEval_EvalFrameDefault + 22700\r\n70 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n71 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n72 Python3                  0x0000000104c77b54 _PyEval_EvalFrameDefault + 22664\r\n73 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n74 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n75 Python3                  0x0000000104c77bf4 _PyEval_EvalFrameDefault + 22824\r\n76 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n77 Python3                  0x0000000104c7aa38 _PyEval_EvalFrameDefault + 34668\r\n78 Python3                  0x0000000104c77bf4 _PyEval_EvalFrameDefault + 22824\r\n79 Python3                  0x0000000104ba8fc8 _PyFunction_Vectorcall + 420\r\n80 Python3                  0x0000000104baa0cc PyObject_CallMethodObjArgs + 492\r\n81 Python3                  0x0000000104baa258 _PyObject_CallMethodIdObjArgs + 112\r\n82 Python3                  0x0000000104c9daf4 PyImport_ImportModuleLevelObject + 1284\r\n83 Python3                  0x0000000104c76888 _PyEval_EvalFrameDefault + 17852\r\n84 Python3                  0x0000000104c7b7ec _PyEval_EvalFrameDefault + 38176\r\n85 Python3                  0x0000000104c72204 PyEval_EvalCode + 80\r\n86 Python3                  0x0000000104cb62f0 PyParser_ASTFromStringObject + 500\r\n87 Python3                  0x0000000104cb4eb4 PyRun_StringFlags + 152\r\n88 LLDB                     0x0000000116b8f3f4 lldb_private::python::runStringMultiLine(llvm::Twine const&, lldb_private::python::PythonDictionary const&, lldb_private::python::PythonDictionary const&) + 148\r\n89 LLDB                     0x0000000116b9503c lldb_private::ScriptInterpreterPythonImpl::ExecuteMultipleLines(char const*, lldb_private::ExecuteScriptOptions const&) + 884\r\n90 LLDB                     0x0000000116b9a5c8 lldb_private::ScriptInterpreterPythonImpl::LoadScriptingModule(char const*, lldb_private::LoadScriptOptions const&, lldb_private::Status&, std::__1::shared_ptr<lldb_private::StructuredData::Object>*, lldb_private::FileSpec) + 1924\r\n91 LLDB                     0x0000000116d9342c CommandObjectCommandsScriptImport::DoExecute(lldb_private::Args&, lldb_private::CommandReturnObject&) + 328\r\n92 LLDB                     0x00000001167a5ad0 lldb_private::CommandObjectParsed::Execute(char const*, lldb_private::CommandReturnObject&) + 656\r\n93 LLDB                     0x000000011679c75c lldb_private::CommandInterpreter::HandleCommand(char const*, lldb_private::LazyBool, lldb_private::CommandReturnObject&, bool) + 2024\r\n94 LLDB                     0x000000011679ff9c lldb_private::CommandInterpreter::IOHandlerInputComplete(lldb_private::IOHandler&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>&) + 828\r\n95 LLDB                     0x00000001166d77c8 lldb_private::IOHandlerEditline::Run() + 304\r\n96 LLDB                     0x00000001166bc0c4 lldb_private::Debugger::RunIOHandlers() + 140\r\n97 LLDB                     0x00000001167a1178 lldb_private::CommandInterpreter::RunCommandInterpreter(lldb_private::CommandInterpreterRunOptions&) + 196\r\n98 LLDB                     0x000000011651d798 lldb::SBDebugger::RunCommandInterpreter(bool, bool) + 124\r\n99 lldb                     0x00000001041402b8 Driver::MainLoop() + 2712\r\n100 lldb                     0x0000000104140d14 main + 2036\r\n101 dyld                     0x000000018145d0e0 start + 2360\r\n```","It seems that this is happen at https:\/\/github.com\/protocolbuffers\/protobuf\/blob\/976a6eb6a4f462c6bb0b2a87fcd95f0aeb17eedb\/src\/google\/protobuf\/message_lite.cc#L726 .\r\n\r\nIs tink statically linked with its bundled Protobuf? (pyarrow does.)  If tink is also linked with its bundled Protobuf, they may be conflicted."],"labels":["Type: bug","Component: Python"]},{"title":"[Java][JPMS][IDE] Fix developer docs for building Arrow Java JPMS in IntelliJ IDE","body":"### Describe the enhancement requested\n\nThe process of executing `mvn clean install` did not encounter any problems when we were trying to build Java modules by command line (i.e. Github CI).\r\n\r\nThe following error appears if we are trying to run and debug tests using an IDE.\r\n\r\n```\r\n\/arrow\/java\/memory\/memory-core\/src\/main\/java\/module-info.java:18\r\njava: modules are not supported in -source 8\r\n  (use -source 9 or higher to enable modules)\r\n```\n\n### Component(s)\n\nJava","comments":["There may not be an option to continue using IntelliJ JPS build for our Java modules project.\r\n\r\nIntelliJ offers the following options for compiling Java projects:\r\n\r\n1. Compile using maven commands by \"View --> Tool Window --> Maven --> Run maven build\" : OK\r\n2. Compile using IntelliJ own JPS compiler by \"Build --> Build Project\" : NOK ( modules are not supported in -source 8)\r\n3. Compile using terminal \"View --> Tool Window --> Terminal --> Run maven build\" : OK\r\n\r\nOption (1) and (3) work without errors because these options **honors** maven\/pom.xml definitions.\r\n\r\nOption (2) uses their **own native compiler**. IntelliJ IDEA compiles code using a tool called JPS (JetBrains Project System). JPS is designed to speed up compiling by performing incremental and background compilations.\r\n\r\nTo continue using option (2) we need to \"**Delegate build and run actions to Maven**\" using [these steps](https:\/\/www.jetbrains.com\/help\/idea\/delegate-build-and-run-actions-to-maven.html#delegate_to_maven).\r\n\r\n![image](https:\/\/github.com\/apache\/arrow\/assets\/4554485\/7bc864a0-a08e-43ae-b880-319920515368)\r\n\r\n","Delegating build and run actions to Maven will result in the following features being lost:\r\n\r\n- **Incremental Compilation**: JPS tracks changes in your project as you make modifications. When you compile, instead of recompiling your entire project from scratch, JPS compiles only the files that have changed since the last compile and any dependent files. This saves time by skipping the recompilation of unchanged files.\r\n- **Background Compilation**: As you make changes to your project, JPS compiles those changes in the background. This means that when you run or debug your project, most of your recent changes have already been compiled and are ready to go.\r\n- **Distributed and Parallel Compilation**: JPS can leverage multiple CPU cores and distribute the compile workload among them for faster compiling. This can be greatly beneficial in large projects with many modules and dependencies.\r\n","HI @jduo, James. Do you have any hacks for building Arrow Java modules within IntelliJ?","Would you be able to suggest any next steps to resolve this issue, @danepitkin \/ @lidavidm:\r\n\r\n1. Would it be necessary to spend more time figuring out a workaround to continue using IntelliJ JPS features for the size and variety of our Java modules? , or\r\n2. This issue can be resolved by adding Delegate build and run actions to Maven to our build document. \r\n\r\n@vibhatha ","I configure my IntelliJ's build to use maven rather than use its own build system:\n\nhttps:\/\/www.jetbrains.com\/help\/idea\/delegate-build-and-run-actions-to-maven.html#delegate_to_maven","That option is far, far slower (at least for me) though. ","I was able to get part of Arrow building inside IntelliJ when I set the source\/target level to 21. But I ran into cases where the module-info declarations were incorrect. If we fix those we might be able to create a Maven profile for IntelliJ specifically that lets it use its builtin build system.","> I was able to get part of Arrow building inside IntelliJ when I set the source\/target level to 21. But I ran into cases where the module-info declarations were incorrect. If we fix those we might be able to create a Maven profile for IntelliJ specifically that lets it use its builtin build system.\r\n\r\nI observed the same. ","I went a few steps ahead and got stuck here\r\n\r\n```bash\r\n\/home\/arrowuser\/github\/fork\/arrow\/java\/memory\/memory-unsafe\/src\/main\/java\/module-info.java:21:35\r\njava: module not found: org.apache.arrow.memory.core\r\n```"],"labels":["Type: enhancement","Component: Java"]},{"title":"[Java] Can't initialize RootAllocator when Unsafe isn't present even when using the NettyAllocator","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nIn an Alpine docker container, the JVM doesn't have the Unsafe jars (for version 17 of openjdk).  Even though we set the properties to use the Netty allocator instead, Arrow tries to load the Unsafe jars anyway, doesn't catch the exception and throws it to the caller of new RootAllocator().  \r\n\r\nThe problem is this line of code:\r\n\tat org.apache.arrow.memory.NettyAllocationManager.<clinit>(NettyAllocationManager.java:51)\r\n`  private static final PooledByteBufAllocatorL INNER_ALLOCATOR = new PooledByteBufAllocatorL();`\r\n\r\nIt is only used a couple of times and only for default initialization.  Perhaps just hard-code those defaults instead or just pull them from the same properties Netty does.  You have to be really careful with static variables and if they throw exceptions, they probably aren't suitable for being in a static variable.\r\n\r\nHere is the exception and relevant output:\r\n\r\nException:\r\n```\r\njava.lang.UnsupportedOperationException: sun.misc.Unsafe unavailable\r\n\tat io.netty.util.internal.CleanerJava9.<clinit>(CleanerJava9.java:68)\r\n\tat io.netty.util.internal.PlatformDependent.<clinit>(PlatformDependent.java:191)\r\n\tat io.netty.buffer.PooledByteBufAllocator.<clinit>(PooledByteBufAllocator.java:116)\r\n\tat io.netty.buffer.PooledByteBufAllocatorL.<init>(PooledByteBufAllocatorL.java:49)\r\n\tat org.apache.arrow.memory.NettyAllocationManager.<clinit>(NettyAllocationManager.java:51)\r\n\tat java.base\/java.lang.Class.forName0(Native Method)\r\n\tat java.base\/java.lang.Class.forName(Class.java:375)\r\n\tat org.apache.arrow.memory.DefaultAllocationManagerOption.getFactory(DefaultAllocationManagerOption.java:108)\r\n\tat org.apache.arrow.memory.DefaultAllocationManagerOption.getNettyFactory(DefaultAllocationManagerOption.java:127)\r\n\tat org.apache.arrow.memory.DefaultAllocationManagerOption.getDefaultAllocationManagerFactory(DefaultAllocationManagerOption.java:91)\r\n\tat org.apache.arrow.memory.BaseAllocator$Config.getAllocationManagerFactory(BaseAllocator.java:773)\r\n\tat org.apache.arrow.memory.ImmutableConfig.access$801(ImmutableConfig.java:24)\r\n\tat org.apache.arrow.memory.ImmutableConfig$InitShim.getAllocationManagerFactory(ImmutableConfig.java:83)\r\n\tat org.apache.arrow.memory.ImmutableConfig.<init>(ImmutableConfig.java:47)\r\n\tat org.apache.arrow.memory.ImmutableConfig.<init>(ImmutableConfig.java:24)\r\n\tat org.apache.arrow.memory.ImmutableConfig$Builder.build(ImmutableConfig.java:485)\r\n\tat org.apache.arrow.memory.BaseAllocator.<clinit>(BaseAllocator.java:62)\r\n\tat schema:public\/\/org.seme.hadoop.ArrowFileWriterUDF.writeArrowFile(ArrowFileWriterUDF.java:131)\r\n\tat org.postgresql.pljava.internal@1.6.5\/org.postgresql.pljava.internal.EntryPoints.lambda$invocable$0(EntryPoints.java:130)\r\n\tat java.base\/java.security.AccessController.doPrivileged(AccessController.java:399)\r\n\tat org.postgresql.pljava.internal@1.6.5\/org.postgresql.pljava.internal.EntryPoints.doPrivilegedAndUnwrap(EntryPoints.java:312)\r\n\tat org.postgresql.pljava.internal@1.6.5\/org.postgresql.pljava.internal.EntryPoints.invoke(EntryPoints.java:158)\r\n```\r\n\r\nDebug output:\r\n```\r\n18:37:28.295 [main] DEBUG io.netty.util.ResourceLeakDetector -- -Dio.netty.leakDetection.level: simple\r\n18:37:28.295 [main] DEBUG io.netty.util.ResourceLeakDetector -- -Dio.netty.leakDetection.targetRecords: 4\r\n18:37:28.305 [main] DEBUG io.netty.util.internal.PlatformDependent0 -- -Dio.netty.noUnsafe: true\r\n18:37:28.305 [main] DEBUG io.netty.util.internal.PlatformDependent0 -- sun.misc.Unsafe: unavailable (io.netty.noUnsafe)\r\n18:37:28.305 [main] DEBUG io.netty.util.internal.PlatformDependent0 -- Java version: 17\r\n18:37:28.306 [main] DEBUG io.netty.util.internal.PlatformDependent0 -- java.nio.DirectByteBuffer.<init>(long, {int,long}): unavailable\r\n18:37:28.307 [main] DEBUG io.netty.util.internal.PlatformDependent -- maxDirectMemory: 16802381824 bytes (maybe)\r\n18:37:28.307 [main] DEBUG io.netty.util.internal.PlatformDependent -- -Dio.netty.tmpdir: \/tmp (java.io.tmpdir)\r\n18:37:28.307 [main] DEBUG io.netty.util.internal.PlatformDependent -- -Dio.netty.bitMode: 64 (sun.arch.data.model)\r\n18:37:28.307 [main] DEBUG io.netty.util.internal.PlatformDependent -- -Dio.netty.maxDirectMemory: -1 bytes\r\n18:37:28.307 [main] DEBUG io.netty.util.internal.PlatformDependent -- -Dio.netty.uninitializedArrayAllocationThreshold: -1\r\n18:37:28.309 [main] DEBUG io.netty.util.internal.PlatformDependent -- -Dio.netty.noPreferDirect: true\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.numHeapArenas: 16\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.numDirectArenas: 16\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.pageSize: 8192\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.maxOrder: 9\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.chunkSize: 4194304\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.smallCacheSize: 256\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.normalCacheSize: 64\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.maxCachedBufferCapacity: 32768\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.cacheTrimInterval: 8192\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.cacheTrimIntervalMillis: 0\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.useCacheForAllThreads: false\r\n18:37:28.310 [main] DEBUG io.netty.buffer.PooledByteBufAllocator -- -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023\r\n18:37:28.313 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap -- -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024\r\n18:37:28.313 [main] DEBUG io.netty.util.internal.InternalThreadLocalMap -- -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096\r\n18:37:28.320 [main] DEBUG io.netty.buffer.AbstractByteBuf -- -Dio.netty.buffer.checkAccessible: true\r\n18:37:28.320 [main] DEBUG io.netty.buffer.AbstractByteBuf -- -Dio.netty.buffer.checkBounds: true\r\n18:37:28.321 [main] DEBUG io.netty.util.ResourceLeakDetectorFactory -- Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@7cc5bdbd\r\n18:37:28.400 [shutdown-hook-0] DEBUG org.apache.hadoop.hdfs.DataStreamer -- block==null waiting for ack for: -1\r\n```\r\n\r\n\r\n\r\n### Component(s)\r\n\r\nJava","comments":["Arrow itself has a hard dependency on Unsafe anyways, even outside this.","Are there any plans to fix that dependency?  If not, what is the point of the Netty heap allocator?","Netty does a lot of things besides just calling malloc (e.g. pooling\/reusing allocations)\r\n\r\nThere is no way to fix the dependency until FFM is stabilized and support for Java <26 (presumably) is dropped","That sounds like a long time from now.  This is this correct?  Also from the POV of Java code I have to ask why this is the case.  You can get access to memcpy (the syscall you really want) without all of this complexity.  System.arrayCopy is just a bounds checked memcpy.  Seems to me that you are making this overly complex due to the libraries you are using.\r\n\r\nPS I worked on database codebases for about a decade.  Some of that time was in the DB kernel.  I do know of what I speak.\r\nPPS If your buffers aren't a multiple of the size of a virtual page in the OS (usually 4k), then accessing memcpy doesn't really help performance.  Other factors will overwhelm any minimal performance improvements you get by bypassing the Java heap.\r\n","You'd have to ask the original developers. Arrow Java always uses direct buffers and manipulates them directly via Unsafe, including for reads\/writes.","Ok, thank you for taking the time to answer my question.","You shouldn't be closing this issue.  Its a real bug.  At the very least, you should update the docs to warn people that this doesn't work.","We document that Unsafe must be opened to the application: https:\/\/arrow.apache.org\/docs\/java\/install.html#java-compatibility","@davisusanibar @vibhatha perhaps we could catch this exception and throw our usual warning that Unsafe must be enabled?","@lidavidm that would be very helpful.  The exception comes from initializing a static field.  I feel a bit of cleanup in this class could make it so the exception never gets thrown in the first place.  That might be an easier way to fix this issue.  Either way would be appreciated.","@hunterpayne I'm trying to set up my environment, I would appreciate it if you could let me know:\r\n\r\n- What Docker base image you're using `i.e. FROM jcxldn\/openjdk-alpine:17-jre`? \r\n- Do those docker images remove Unsafe or how do you remove support for sun.misc.Unsafe?\r\n\r\nI apologize if this is a basic setup\/answer."," postgres:15.3-alpine and they don't include Unsafe in their JDK\/JRE packages which seems to be a conscious decision.\r\n\r\nHunter\r\n    On Wednesday, February 21, 2024 at 01:32:43 PM PST, david dali susanibar arce ***@***.***> wrote:  \r\n \r\n \r\n\r\n\r\n@hunterpayne I'm trying to set up my environment, I would appreciate it if you could let me know:\r\n   \r\n   - What Docker base image you're using i.e. FROM jcxldn\/openjdk-alpine:17-jre?\r\n   - Do those docker images remove Unsafe or how do you remove support for sun.misc.Unsafe?\r\n\r\nI apologize if this is a basic setup\/answer.\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n  ","> postgres:15.3-alpine and they don't include Unsafe in their JDK\/JRE packages which seems to be a conscious decision. \r\n\r\nIn my test, I've used the `postgres:15.3-alpine` image, however it does not include Java JDK\/JRE packages since they are not required.\r\n\r\n\r\n","@hunterpayne, what Java could I use to test this UnsupportedOperationException: sun.misc.Unsafe unavailable scenario? I would appreciate if you could provide me with a basic setup to replicate this scenario and apply the changes suggested?"," Make a docker container with this dockerfile:\r\nFROM postgres:15.3-alpineRUN apk add openjdk17 --repository=http:\/\/dl-cdn.alpinelinux.org\/alpine\/edge\/communityRUN apk add git cmake maven gcc musl-dev gcompat libc6-compat make \r\nENTRYPOINT [\"tail\", \"-f\", \"\/dev\/null\"]\r\n\r\nThen run it, then exec into the container, then you can build arrow and reproduce this issue using a unit test\r\nHunter\r\n\r\n    On Thursday, February 22, 2024 at 06:04:42 AM PST, david dali susanibar arce ***@***.***> wrote:  \r\n \r\n \r\n\r\n\r\n@hunterpayne, what Java could I use to test this UnsupportedOperationException: sun.misc.Unsafe unavailable scenario? I would appreciate if you could provide me with a basic setup to replicate this scenario and apply the changes suggested?\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n  "],"labels":["Type: bug","Component: Java"]},{"title":"[Python] Segmentation fault when calling parquet.FileMetaData().row_group(0)","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nHello,\r\n\r\ni want to report a minor bug. Running the following (useless) code results in a Segmentation fault. \r\n\r\n```python\r\nimport pyarrow.parquet as pq\r\n\r\npq.FileMetaData().row_group(0)\r\n```\r\n\r\npyarrow Version: 15.0.0\r\nos. Linux x86_64\r\n\n\n### Component(s)\n\nParquet, Python","comments":["Aha this should throw an ex...\r\n\r\n@jorisvandenbossche I'm not familiar with Python, I got segment fault when:\r\n\r\n```\r\nimport pyarrow.parquet as pq\r\n\r\nfmdt = pq.FileMetaData()\r\nfmdt.num_row_groups()\r\n```\r\n\r\nguess maybe `FileMetadata` should not being created like that?"],"labels":["Type: bug","Component: Parquet","Component: Python"]},{"title":"[C++] Import\/Export ArrowDeviceArrayStream","body":"### Describe the enhancement requested\n\nhttps:\/\/github.com\/apache\/arrow\/pull\/34972 added the definition of `ArrowDeviceArrayStream` to the C Device Data interface, but the initial PR for the device interface in C++ only added support for `ArrowDeviceArray` (https:\/\/github.com\/apache\/arrow\/issues\/36488 \/ https:\/\/github.com\/apache\/arrow\/pull\/36489).\r\n\r\ncc @zeroshade \n\n### Component(s)\n\nC++","comments":["@zeroshade are there plans to add this to the C++ library? We have added `ArrowDeviceArrayStream` to the Arrow spec, but AFAIU there is no implementation of it anywhere?","There are definitely plans to do so, It's on my list of things. I'll try to bump up the priority"],"labels":["Type: enhancement","Component: C++"]},{"title":"[Python] Is there a programmatic way to drop \"special columns\"","body":"### Describe the usage question you have. Please include as many useful details as  possible.\n\n\nSometimes when reading or mutating data we end up with special columns:\r\n\r\n`__fragment_index, __last_in_fragment, etc.`\r\n\r\nMost of them are listed here: https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.dataset.Scanner.html and are easy enough to drop, but with hash joins you end up with `__fragment_index_{left,right}` \r\n\r\nIs there an idiomatic way way to drop these? I'm concerned that more of these will be a pyarrow change and more will be added, so a centralized way to remove these internally-used columns would be useful.\n\n### Component(s)\n\nPython","comments":[],"labels":["Component: Python","Type: usage"]},{"title":"[R][Python] Unable to load pyarrow package through reticulate in R on windows","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI'm trying to use arrow to share large data frames between python and R. If I load the R packages then try to import the python module this fails with an ImportError from python:\r\n\r\n```\r\n> library(arrow)\r\nThe tzdb package is not installed. Timezones will not be available to Arrow compute functions.\r\n\r\nAttaching package: \u2018arrow\u2019\r\n\r\nThe following object is masked from \u2018package:utils\u2019:\r\n\r\n    timestamp\r\n\r\n> library(reticulate)\r\n> py_discover_config()\r\n...\r\nversion:        3.9.18 | packaged by conda-forge | (main, Dec 23 2023, 16:29:04) [MSC v.1929 64 bit (AMD64)]\r\nArchitecture:   64bit\r\n...\r\n> pa <- import(\"pyarrow\")\r\nError in py_module_import(module, convert = convert) : \r\n  ImportError: DLL load failed while importing lib: The specified procedure could not be found.\r\nRun `reticulate::py_last_error()` for details.\r\n\r\n```\r\n\r\nNote that I can load the pyarrow in a vanilla python REPL just fine.\r\n\r\nIf I ensure to import pyarrow before loading the R package then it works, however this is difficult to guarantee (other packages may have already loaded the R arrow package), and causes failures during package development.\r\n\r\n```\r\n> library(reticulate)\r\n> import('pyarrow')\r\nModule(pyarrow)\r\n> library(arrow)\r\nThe tzdb package is not installed. Timezones will not be available to Arrow compute functions.\r\n\r\nAttaching package: \u2018arrow\u2019\r\n\r\nThe following object is masked from \u2018package:utils\u2019:\r\n\r\n    timestamp\r\n\r\n```\r\n\r\nBecause it seems to be driven by some sort of incompatibility between the R and python arrow libraries, I tried various different versions to see if I could get anything that works. Initially I was using R 4.2.1, but could also reproduce with a fresh install of R 4.3.2. I tried various python versions including fresh installs of the latest (3.11.5), and also the conda version as installed by `reticulate::install_miniconda` (3.9.18 above). I tried using the latest version of the pyarrow package (15.0.0), and also downgrading to 14.0.0 to match the latest available R package.\r\n\n\n### Component(s)\n\nPython, R","comments":["I've attempted to diagnose it by loading the dll directly as follows:\r\n\r\n```\r\n> library(arrow)\r\n...\r\n> reticulate::repl_python()\r\nPython 3.12.2 (C:\/Program Files\/Python312\/python.exe)\r\nReticulate 1.35.0 REPL -- A Python interpreter in R.\r\nEnter 'exit' or 'quit' to exit the REPL and return to R.\r\n>>> import ctypes as c\r\n>>> dll = c.WinDLL(r'C:\\Program Files\\Python312\\Lib\\site-packages\\pyarrow\\lib.cp312-win_amd64.pyd')\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\PROGRA~1\\PYTHON~2\\Lib\\ctypes\\__init__.py\", line 379, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\r\nOSError: [WinError 127] The specified procedure could not be found\r\n```\r\n\r\nThat produces a pop up with the mangled C++ symbol name that caused the issues, in this case eg: `\"The procedure entry point ?type@StringBuilder@arrow@@UEBA?AV?$shared_ptr@VDataType@arrow@@@std@@XZ could not be located in the dynamic link library C:\\Program Files\\Python312\\Lib\\site-packages\\pyarrow\\lib.cp312-win_amd64.pyd\"`.\r\n\r\nIf I repeat the above without first loading the R arrow package, it will load the python libs just fine. Is this problem caused by both the R and python packages having different \"arrow.dll\" files that they're attempting to load into the same address space? ","After some more digging, I think that the cause is indeed the two \"arrow.dll\" files. Loading the python package first works because the R \"arrow.dll\" is loaded directly, whereas the pyarrow version is loaded indirectly by being dependencies of the .pyd files.\r\n\r\nI tried a hacky solution of renaming the python one to \"xrrow.dll\", and running `sed -i.bak 's\/arrow.dll\/xrrow.dll\/g'` on the .pyd files in the directory, and it now \"works\" when loading the R package first:\r\n\r\n```\r\n> library(arrow)\r\n> library(reticulate)\r\n...\r\n> pa = import('pyarrow')\r\n> pa\r\nModule(pyarrow)\r\n```\r\n\r\nPerhaps the easiest solution then is for the R library to use a different name for its dll, though I'm not sure if it has to match the package name or if that is just a convention? Alternatively perhaps dll manifests can be used on the python side.","I was able to get a manifest solution to work, following https:\/\/devblogs.microsoft.com\/oldnewthing\/20171011-00\/?p=97195.\r\n\r\nFirst I created \"force_local_arrow.manifest\" with the following:\r\n\r\n```\r\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\r\n<assembly xmlns=\"urn:schemas-microsoft-com:asm.v1\" manifestVersion=\"1.0\">\r\n    <file name=\"arrow.dll\" \/>\r\n<\/assembly>\r\n```\r\n\r\nThen I patched each of \"arrow_flight.dll\", \"arrow_python.dll\", \"lib.cp312-win_amd64.pyd\", and \"_hdfsio.cp312-win_amd64.pyd\" (probably incomplete, just enough to load the package) as follows:\r\n\r\n```\r\n> mt.exe -manifest force_local_arrow.manifest -outputresource:lib.cp312-win_amd64.pyd;#2\r\n```\r\n\r\nPerhaps this approach is better than just renaming the R dll; by amending the build process for the windows python wheel to include the necessary flags to embed the manifest and avoid the problem altogether.","Hi @aquasync, thanks for the report and your work getting to a cause and fix. I can reproduce what you saw in your original report on my Windows ARM machine so I can offer that as a start here. I'll work on getting some more eyes on this.","If it's of any use\/related, this message just started showing up in some of our R CMD CHECK github actions of an R package that has arrow as a dependency. In tests of a function that uses arrow to read csv files (without loading the package) we now get a new message:\r\n```\r\n Message\r\n The tzdb package is not installed. Timezones will not be available to Arrow compute functions.\r\n```\r\n**only on windows**, causing comparison against our previous snapshots to fail because of the message: https:\/\/github.com\/Infectious-Disease-Modeling-Hubs\/hubValidations\/actions\/runs\/8083994354\/job\/22088275404?pr=74\r\n\r\nThe last time the action was run and passed was Feb 13.\r\n\r\nLet me know if you think this should be moved into a separate issue","Hi @annakrystalli, I don't think that's related but feel free to file a new issue and tag me. It doesn't look like anything related to that code has changed recently so my guess is that something in your environment may have changed. tzdb is a `Suggests` and getting this warning is expected on Windows.","Hey @aquasync, I did a little bit more research on this today and have a few notes:\r\n\r\nI found [this older Jira ticket](https:\/\/issues.apache.org\/jira\/browse\/ARROW-8281) that seemed related. That led me to try installing R arrow from conda instead of CRAN and I was able to get things to work:\r\n\r\n```sh\r\n$ conda create -n reticulate-test -c conda-forge python==3.12 r-reticulate r-arrow pyarrow\r\n$ conda activate reticulate-test\r\n$ R\r\n> library(arrow)\r\n> library(reticulate)\r\n> import(\"pyarrow\")\r\nModule(pyarrow)\r\n```\r\n\r\nSo that's possibly a workaround in case you need one right now.\r\n\r\nAs for next steps, it seems like we can modify either the Python build or the R build here and my preference would be to modify the R build as it would be relatively low impact and we know it already works because of our conda package. And we'd only have to modify it for Windows. @assignUser do you have any immediate reaction or insights into this idea?\r\n\r\nBefore I figured the above out, I filed [an issue over on the reticulate repo](https:\/\/github.com\/rstudio\/reticulate\/issues\/1541) to see if they have any ideas so I'll be curious to see if they have any thoughts.","Ah ok, interesting, thanks for investigating. I did try a conda install but that was just for python & python packages, not R.\r\n\r\nI'm just using a modified wheel in the interim with the manifest attached to all the dlls, but if the solution of renaming the R dll is already being used in the conda context it makes sense to just use that for default builds."],"labels":["Type: bug","Component: R","Component: Python"]},{"title":"GH-39791: [C++][Flight RPC] Flight C++ ServerMiddleware::SendingHeaders call ordering fix","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\nDocumentation for C++ Flight server middleware implies that SendingHeaders is called when headers are to be sent, however it is instead called when middleware is initialized, prior to the app's RPC handler.  This is also inconsistent with e.g. Java where the call happens post-handler.\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\n[In-progress] Fix for the above call ordering, so far affecting only non-streaming handlers, including DoAction() but not yet including Handshake().  This PR should also [pending] include a more involved fix for DoGet, DoPut, and DoExchange, for which helpers are already present.\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nExisting integration tests pass.\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nBarely:  There is a \"breaking\" change in that if a C++ RPC handler mutates the call's middleware instance in a way that affects its SendingHeaders() behaviour, this will now work as expected.  However, apps relying on this being broken (i.e. doing something currently moot and expecting it not to work) are themselves fundamentally broken so I am not concerned about this breaking aspect of this change.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n**This PR includes breaking changes to public APIs.**\r\n\r\nSee above regarding user-facing changes.  This SHOULD have zero affect on even remotely sane applications, as above.\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* Closes: #39791","comments":[":warning: GitHub issue #39791 **has been automatically assigned in GitHub** to PR creator.","I'm still not certain why we can't implement sessions as an in-RPC helper (that uses ServerCallContext::AddHeader) instead of trying to do more workarounds for what is ultimately gRPC's broken middleware implementation (since the original implementation here was trying to work around the fact that gRPC middleware aren't allowed to communicate with the RPC handler)","Either that, or revisit gRPC's middleware and see if the issue has been fixed so that we're not just fighting the framework","I've been working on the Go implementation of sessions (#40284) and also ran into this issue in a few ways. My general takeaway is that I think it likely makes sense to send metadata which may be dependent on RPC behavior (such as session cookies) in the response trailer rather than the header. In fact, given the [documentation](https:\/\/grpc.io\/docs\/guides\/metadata\/#headers) on gRPC metadata it's not clear that ignoring changes to headers after RPC handlers have started is in fact broken behavior. There is a separate but related issue, however, which is that not all clients currently check the trailer for metadata. Go and C++ do, but it appears Java only does when an error is caught.","gRPC does this too: implementations may send the result code in either the trailer or consolidate it with the headers","@lidavidm as you're more familiar with that area do you have any time to peek at the current state of things with gRPC support for proper (not duct-taped on like this) interceptors to handle this as-desired for this issue?  If not I can probably find time to dig around, just figured as you're more familiar with the interface it might be a quickie to check if there have been any changes (which I somewhat doubt).  TIA","> I've been working on the Go implementation of sessions (#40284) and also ran into this issue in a few ways. My general takeaway is that I think it likely makes sense to send metadata which may be dependent on RPC behavior (such as session cookies) in the response trailer rather than the header. In fact, given the [documentation](https:\/\/grpc.io\/docs\/guides\/metadata\/#headers) on gRPC metadata it's not clear that ignoring changes to headers after RPC handlers have started is in fact broken behavior. There is a separate but related issue, however, which is that not all clients currently check the trailer for metadata. Go and C++ do, but it appears Java only does when an error is caught.\r\n\r\nUnfortunately cookies absolutely have to be set as part of headers not trailers: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Headers\/Trailer#directives\r\n\r\nI'm definitely not saying that this is broken (not-as-documented) behaviour on gRPC's part, but it's inconsistent with Arrow Flight documentation and with other e.g. Java implementations thereof, and also extremely inconvenient (breaking) as the existing behaviour means that RPC handlers\u2014which otherwise have access to the middleware context\u2014can have no influence on Set-Cookie response headers.","IIRC, it's basically impossible, but you may have better luck getting answers from the gRPC team than I have. (I think the call filter API lets you do this, but because of various hardcoded things that I saw, basically only one call filter can be registered globally, so that's even more reason that Flight shouldn't touch it.)\r\n\r\nHence, why I've suggested multiple times that for C++, the best way to do this would be to just inline the middleware into the RPC handler, instead of having a formal middleware (then the RPC handler is in full control).\r\n\r\ngRPC doesn't necessarily follow HTTP conventions in the first place (example: the HTTP status code is always 200). So that's not necessarily binding. And as mentioned some gRPC implementations already switch between headers\/trailers for responses.","You could chime in here for instance: https:\/\/github.com\/grpc\/grpc\/issues\/32508#issuecomment-1967321290\r\n\r\nOpenTelemetry is another case where gRPC interceptors are artificially limited for similar reasons as here: you can't communicate a value between the interceptor and the RPC handler, so we have to resort to hacks like the one here. Now that we're taking the hack much further, I'm less willing to accommodate gRPC and would rather they fix interceptors to actually be useful.","> Unfortunately cookies absolutely have to be set as part of headers not trailers: https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Headers\/Trailer#directives\r\n\r\nThis makes me wonder if cookies are an appropriate mechanism for session passing. How would this generalize to streaming RPC when the session state may not be known until after the headers are set?","It appears upstream gRPC is considering fixing interceptors. https:\/\/github.com\/grpc\/grpc\/issues\/32508#issuecomment-1992885391"],"labels":["Component: C++","Component: FlightRPC","awaiting changes"]},{"title":"[C++] Possible data race in accessing scalars","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nI use an ```arrow::compute::literal``` in my parquet reading pipeline and get ThreadSanitizer reports about this code:\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/c23a097965b5c626cbc91b229c76a6c13d36b4e8\/cpp\/src\/arrow\/array\/data.cc#L288-L290\r\n\r\nIf I am reading it right, this results in issuing write instructions during what would seem to be read-only accesses, and this indeed looks racy.\n\n### Component(s)\n\nC++, Parquet","comments":["Can you show your stack trace?","Arrow is built from commit 3c655df6d459f50cc9734b4f4da31c84ece6c030\r\n[tsan-40069.txt](https:\/\/github.com\/apache\/arrow\/files\/14283806\/tsan-40069.txt)\r\n","Thank you. It looks like calling `ArraySpan::FillFromScalar` from several threads is legitimate when the same scalar is used as input for different computations. Given that this is probably always writing the same value, it would probably be enough to use relaxed atomic stores.\r\n\r\n@bkietz ","> Thank you. It looks like calling `ArraySpan::FillFromScalar` from several threads is legitimate when the same scalar is used as input for different computations. Given that this is probably always writing the same value, it would probably be enough to use relaxed atomic stores.\r\n> \r\n> @bkietz\r\n\r\n@pitrou Just a thought, seems like the information to fill into the scalar's scratch space is totally self-contained, should we do the filling by a (virtual) member function of the scalar class, e.g. `virtual BufferSpan Scalar::FillScrach(...)`? And we can probably use an acquire-release atomic flag to mark if the scratch has already been filled. This might be more semantically clear imho.\r\n\r\n(But this may bring ABI issues, which I'm not sure how is dealt with in arrow.)","@bkietz Would you be able to take a look at this?","@zanmato1984 I'm not sure about the answer to the rest of your question, but for the record Arrow C++ doesn't promise any ABI stability. We do try to maintain API compatibility (with the occasional deprecation or signature change), but people are expected to recompile when switching from one version of Arrow C++ to another.","> @zanmato1984 I'm not sure about the answer to the rest of your question, but for the record Arrow C++ doesn't promise any ABI stability. We do try to maintain API compatibility (with the occasional deprecation or signature change), but people are expected to recompile when switching from one version of Arrow C++ to another.\r\n\r\nThank you for the info.\r\n\r\nAfter some attempts to draft a fix, I found something worth discussion.\r\n\r\nFirst I was able to reproduce the TSAN error using a simple UT. Then I tried to do a plane fix proposed by @pitrou , that is using relaxed atomic stores. Given that I'm not aware of how to apply atomic operations on plane memory in C++, I'll have to change the definition of the scratch space into several atomic variables. However, the interpretation of the scratch space is type-dependent, i.e., some types treat it as two 32-byte space and others treat it as two 64-byte space. This makes \"defining the scratch space as a fixed number of atomic variables in the class\" not feasible.\r\n\r\nNow I'm drafting the alternative way, which is using extra atomic flags + spinning (keeping scratch space itself unchanged - as plane memory), which is basically what I talked about in my previous comment. It'll come later.","> However, the interpretation of the scratch space is type-dependent, i.e., some types treat it as two 32-byte space and others treat it as two 64-byte space.\r\n\r\nBut all threads accessing the scratch space should interpret it as the same type, right?\r\n\r\n>  extra atomic flags + spinning (keeping scratch space itself unchanged - as plane memory)\r\n\r\nPlease let's avoid home-grown synchronization like this. Either we can use atomic accesses without spinning, or we should use a lock.\r\n","> > However, the interpretation of the scratch space is type-dependent, i.e., some types treat it as two 32-byte space and others treat it as two 64-byte space.\r\n> \r\n> But all threads accessing the scratch space should interpret it as the same type, right?\r\n> \r\n\r\nRight. The question here is that the scratch space is defined in a generic place, i.e., `ArraySpanFillFromScalarScratchSpace` (base class of all \"scratched\" scalars) and we couldn't know how it is going to be interpreted at runtime, despite whether all threads interpret it as the same type or not.\r\n\r\n> > extra atomic flags + spinning (keeping scratch space itself unchanged - as plane memory)\r\n> \r\n> Please let's avoid home-grown synchronization like this. Either we can use atomic accesses without spinning, or we should use a lock.\r\n\r\nYeah, of course. I was thinking about using mutex too.","> Right. The question here is that the scratch space is defined in a generic place, i.e., `ArraySpanFillFromScalarScratchSpace` (base class of all \"scratched\" scalars) and we couldn't know how it is going to be interpreted at runtime, despite whether all threads interpret it as the same type or not.\r\n\r\nIt would be incorrect for different threads to interpret it differently, even with a lock.","> > Right. The question here is that the scratch space is defined in a generic place, i.e., `ArraySpanFillFromScalarScratchSpace` (base class of all \"scratched\" scalars) and we couldn't know how it is going to be interpreted at runtime, despite whether all threads interpret it as the same type or not.\n> \n> \n> \n> It would be incorrect for different threads to interpret it differently, even with a lock.\n\nYou are right about this too.\n\nI think here are two things:\n1. Is the scratch being interpreted identically by different threads?\n2. If 1 is yes, is it a race?\n\nFor 1, if the answer is no, then it should fall into some sort of undefined behavior area. That will be a user code problem that is using scalar the wrong way. It is not essential whether this is a race, or how the synchronization is implemented. And hopefully, and fortunately, I think current compute and Acero components are using scalar the \"right way\". So we don't really need to worry about it. \n\nFor 2, this is what we are really talking about. Aren't we :)","Right, so I think we can simply use atomic stores here.\r\nIn debug mode, we can perhaps be a bit more sophisticated and check that we don't write two different values, but that's optional IMHO.","> Right, so I think we can simply use atomic stores here. In debug mode, we can perhaps be a bit more sophisticated and check that we don't write two different values, but that's optional IMHO.\r\n\r\nAre you suggesting something like this?\r\nhttps:\/\/github.com\/apache\/arrow\/pull\/40237\/files#diff-1d848527e898cd9da6a41bdbd68cab5d5826986128851220d20d1fdac9752d8bR289-R292","Yes, roughly.","> Yes, roughly.\r\n\r\nOK. I'll move on to add some more tests and get the PR ready.\r\n\r\nPS: in my previous several comments I didn't recall that there is such a way to operate plane memory atomically so there might have been some misunderstanding. My apology and really appreciate your patient :)"],"labels":["Type: bug","Component: Parquet","Component: C++"]},{"title":"GH-20213: [C++] Implement cast to\/from halffloat","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\n### What changes are included in this PR?\r\n\r\nThis PR implements casting to and from float16 types using the vendored float16 library included in arrow at `cpp\/arrrow\/util\/float16.*`.\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\n### Are these changes tested?\r\n\r\nUnit tests are included in this PR.\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\n### Are there any user-facing changes?\r\n\r\nIn that casts to and from float16 will now work, yes.\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* Closes: #20213\r\n\r\n### TODO\r\n\r\n- [x] Add casts to\/from float64.\r\n- [x] String <-> float16 casts.\r\n- [x] Integer <-> float16 casts.\r\n- [x] Tests.\r\n- [x] Update https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/status.rst about half float.\r\n- [x] Rebase.\r\n- [x] Run clang format over this PR.\r\n* GitHub Issue: #20213","comments":[":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","This still needs work, but I would greatly appreciate someone who is more familiar with the arrow code-base to comment on whether my approach is sane or not. ",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","Thanks for posting this. The approach is sane, I posted some comments.\r\n\r\nYou will have to decide whether you want to tackle casting to string, or prefer leaving that for later\/another contributor.","So far this PR seems to work for the following casts:\r\n\r\n- float16 to float32\r\n- float16 to float64\r\n- float32 to float16\r\n- float64 to float16\r\n\r\n@pitrou you mentioned string cast(s).\r\n\r\nIs that:\r\n\r\n1. float16 to string\r\n2. string to float16\r\n\r\n?\r\n\r\nWhat other casts are required to consider this done?","> @pitrou you mentioned string cast(s).\r\n> Is that:\r\n>     1. float16 to string\r\n>     2. string to float16\r\n> ?\r\n\r\nYes.\r\n\r\n> What other casts are required to consider this done?\r\n\r\nIdeally, integer-to-half and half-to-integer would be implemented as well.\r\n\r\n",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","Please correct me if I'm wrong, but it seems like float32\/float64 cast to string is ultimately handled by this vendored library `cpp\/src\/arrow\/vendored\/double-conversion`, source repo here: https:\/\/github.com\/google\/double-conversion .\r\n\r\nThere doesn't seem to be a direct way to cast from a half float ultimately represented by a `uint16_t` to a string, but I wonder if it's acceptable to cast to float first and then use the existing float to string cast? or is that unacceptable overhead?","> There doesn't seem to be a direct way to cast from a half float ultimately represented by a `uint16_t` to a string, but I wonder if it's acceptable to cast to float first and then use the existing float to string cast? or is that unacceptable overhead?\r\n\r\nYes, it's acceptable. The only annoyance would be the superfluous precision, but that's not a blocking issue.","By the way, you'll have to implement unit tests at some point (in C++).","@ClifHouck thanks for working on this!\r\n\r\nSmall bit of housekeeping: \r\n\r\nThere is a note in [`docs\/source\/status.rst`](https:\/\/github.com\/apache\/arrow\/blob\/main\/docs\/source\/status.rst) that says:\r\n>(1) Casting to\/from Float16 in C++ is not supported.\r\n\r\nCould you please push a commit here that removes that note? Looks like you will need to rebase first because there have been some changes in `status.rst` since you began work on this PR.","So one question I currently have is, should I specialize `Datum` on `HalfFloatType` (or similar)? Right now tests are failing when trying to compare expected to actual output from doing casts from, say, int to float16. ","> So one question I currently have is, should I specialize `Datum` on `HalfFloatType` (or similar)?\r\n\r\nWhich method would you specialize?","> > So one question I currently have is, should I specialize `Datum` on `HalfFloatType` (or similar)?\r\n> \r\n> Which method would you specialize?\r\n\r\nAfter reading more code, more likely I need to specialize or modify `CompareFloating`, `VisitFloatingEquality`, etc. But I'm not certain. ","> After reading more code, more likely I need to specialize or modify `CompareFloating`, `VisitFloatingEquality`, etc. But I'm not certain.\r\n\r\nIf you're struggling on this, someone else can take a look and help. Perhaps @benibus or @felipecrv ?\r\n(or I can get to it later when I have time)","I'm currently stuck on getting a new `FloatingToFloating` test to pass. AFAICT the other existing tests now pass with the new float16 casts. Trying to make progress on getting this last test to pass today.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","I think I've fixed things with my latest commit. Turned out a specialization of `CastPrimitive` was not being called as I expected, and instead the generic template version was being used.","I added a few more TODOs but I think this is very close to being ready for review. ",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","I think this is close enough for a round of reviews. There's some iffy-ness around `ConvertNumber` for `HalfFloatType` and the test situation, but hopefully can be ironed out with reviews from people with more arrow experience.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.",":warning: GitHub issue #20213 **has been automatically assigned in GitHub** to PR creator.","It seems I have some build\/test failures to comb through. Any tips on running these builds locally?","For the Docker-based builds (i.e. all Linux builds), you can take a look at https:\/\/arrow.apache.org\/docs\/developers\/continuous_integration\/docker.html","I believe the last commit resolves the test failures.","It seems like the 2 R related checks are failing due to something about duckdb: https:\/\/github.com\/apache\/arrow\/actions\/runs\/8366814875\/job\/22912629785?pr=40067#step:7:146\r\n\r\nNot sure if this is problem caused by the changes in this PR or if this is something unrelated.","Yep, looks like I'm running into this: https:\/\/github.com\/apache\/arrow\/issues\/40702","Yes, those are unrelated, sorry.","Rebased on latest main with duckdb fix","Looks like CI issues again.","They are caused by #40744. This PR isn't related."],"labels":["Component: C++","Component: GLib","Component: Documentation","awaiting merge"]},{"title":"[Python] Add nanoarrow integration test","body":"### Describe the enhancement requested\n\nAs suggested in https:\/\/github.com\/apache\/arrow\/pull\/39985#issuecomment-1938994073 , now that nanoarrow has Python bindings, it would be beneficial for both pyarrow and nanoarrow to have a set of integration tests, perhaps as `test_nanoarrow.py` in `python\/pyarrow\/tests`. It might be a good idea for the in-progress Python interface to stabilize a little although the `c_array()`, `c_schema()`, and `c_array_stream()` constructors are all literal wrappers around the ABI-stable C structs and probably won't evolve much.\n\n### Component(s)\n\nPython","comments":[],"labels":["Type: enhancement","Component: Python"]},{"title":"[C++][Python] Row-major conversion of Table\/RecordBatch to Arrow Tensor","body":"### Describe the enhancement requested\n\nThis issue is a part of https:\/\/github.com\/apache\/arrow\/issues\/40058 and adds `Table`\/`RecordBatch` \u2192 `Tensor` conversion with **row-major** memory layout of the resulting `Tensor`.\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++][Python] Conversion of Table to Arrow Tensor","body":"### Describe the enhancement requested\n\nThis issue is a part of https:\/\/github.com\/apache\/arrow\/issues\/40058 and adds `Table` \u2192 `Tensor` conversion, using the implementation for `RecordBatch`.\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++][Python] Basic conversion of RecordBatch to Arrow Tensor - add option to cast NULL to NaN","body":"### Describe the enhancement requested\n\nThis issue is a part of https:\/\/github.com\/apache\/arrow\/issues\/40058 adds option to cast `NULL` to `NaN` mimicking the cast from [ConvertIntegerWithNulls](https:\/\/github.com\/apache\/arrow\/blob\/c67d0260d4e96472b5cbdff66ca67ead2b9abe4c\/python\/pyarrow\/src\/arrow\/python\/arrow_to_pandas.cc#L550).\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++][Python] Basic conversion of RecordBatch to Arrow Tensor - add support for different data types","body":"### Describe the enhancement requested\n\nThis issue is a part of https:\/\/github.com\/apache\/arrow\/issues\/40058 adds support for different data types and covers:\r\n\r\n- Finding a common data type or use [MergeTypes](https:\/\/github.com\/apache\/arrow\/blob\/2e8bd8d0b53560a561656337021abc3e4aa73f8c\/cpp\/src\/arrow\/type.cc#L741) to merge column data types one by one.\r\n- Casting columns mimicking [ConvertIntegerWithNulls](https:\/\/github.com\/apache\/arrow\/blob\/c67d0260d4e96472b5cbdff66ca67ead2b9abe4c\/python\/pyarrow\/src\/arrow\/python\/arrow_to_pandas.cc#L550)\n\n### Component(s)\n\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[C++][Python] Conversion of RecordBatch and Table to 2D array \/ Tensor","body":"### Describe the enhancement requested\r\n\r\n## Background\r\n\r\nThere is currently one way to convert tabular PyArrow data structure into an array with contiguous memory layout (`_Tabular` to numpy ndarray) and that is through the [array protocol](https:\/\/numpy.org\/devdocs\/user\/basics.interoperability.html) using the `__array__` method.\r\n\r\nIn the PyArrow implementation of the array protocol we use NumPy to convert each of the table columns into a numpy array and then to stack the columns together into a 2-dimensional array. The method produces a row-major layout as we [stack the columns by axis 1](https:\/\/github.com\/apache\/arrow\/blob\/3fe598ae4dfd7805ab05452dd5ed4b0d6c97d8d5\/python\/pyarrow\/table.pxi#L1480).\r\n\r\n**There is no method in Arrow C++ to convert Table\/RecordBatch to a Tensor.**\r\n\r\nThere exists another conversion producing a similar result. This conversion is used to convert PyArrow `_Tabluar` object to a pandas DataFrame. Pandas DataFrame has a 2D array layout structure defined for each data type present. This 2D data structure is called a block. Therefore a dataframe with 3 `int8` columns and 2 `float32` columns will have a structure of 2 blocks, first being an `int8` block with size `(3, n)` and second being a float32 block of size `(2, n)`.\r\n\r\nIf all columns in the PyArrow table were of the same data type we would get one 2D block that would have a contiguous memory layout.\r\n\r\n**The resulting tensor memory layout is in this case column-major. For row-major memory layout the design will need a bit more research and discussion.**\r\n\r\n<details>\r\n<summary>Code example<\/summary>\r\n<br>\r\n\r\n```Python\r\nIn [1]: import pyarrow as pa\r\n   ...: import numpy as np\r\n   ...: \r\n   ...: # Construct a pyarrow.Table\r\n   ...: arr_1 = pa.array([2, 4, 5, 100], type=pa.int8())\r\n   ...: arr_2 = pa.array([1, 2, 3, 4], type=pa.int16())\r\n   ...: names = [\"arr_1\", \"arr_2\"]\r\n   ...: t = pa.table([arr_1, arr_2], names=names)\r\n   ...: # Define common data type and cast all the columns\r\n   ...: common_dtype = pa.int16()\r\n   ...: column_arrays = [\r\n   ...:     t.column(i).cast(target_type=common_dtype) for i in range(t.num_columns)\r\n   ...: ]\r\n   ...: \r\n   ...: options = dict(\r\n   ...:     pool=None,\r\n   ...:     strings_to_categorical=False,\r\n   ...:     zero_copy_only=False,\r\n   ...:     integer_object_nulls=False,\r\n   ...:     date_as_object=True,\r\n   ...:     timestamp_as_object=False,\r\n   ...:     use_threads=True,\r\n   ...:     deduplicate_objects=True,\r\n   ...:     safe=True,\r\n   ...:     split_blocks=False,\r\n   ...:     self_destruct=False,\r\n   ...:     maps_as_pydicts=None,\r\n   ...:     coerce_temporal_nanoseconds=False\r\n   ...: )\r\n   ...: \r\n   ...: # CONVERT TABLE TO 2 DIMENSIONAL NDARRAY\r\n   ...:\r\n   ...: # Using pyarrow -> pandas conversion\r\n   ...: r1 = pa.lib.table_to_blocks(options, pa.table(column_arrays, names=names), None, None)[0][\"block\"]\r\n   ...: # Using array protocol\r\n   ...: r2 = t.__array__()\r\n\r\nIn [2]: t\r\nOut[2]: \r\npyarrow.Table\r\narr_1: int8\r\narr_2: int16\r\n----\r\narr_1: [[2,4,5,100]]\r\narr_2: [[1,2,3,4]]\r\n\r\nIn [3]: r1\r\nOut[3]: \r\narray([[  2,   4,   5, 100],\r\n       [  1,   2,   3,   4]], dtype=int16)\r\n\r\nIn [4]: r2\r\nOut[4]: \r\narray([[  2,   1],\r\n       [  4,   2],\r\n       [  5,   3],\r\n       [100,   4]], dtype=int16)\r\n\r\nIn [5]: r1.flags\r\nOut[5]: \r\n  C_CONTIGUOUS : True\r\n  F_CONTIGUOUS : False\r\n  OWNDATA : False\r\n  WRITEABLE : True\r\n  ALIGNED : True\r\n  WRITEBACKIFCOPY : False\r\n\r\nIn [6]: r2.flags\r\nOut[6]: \r\n  C_CONTIGUOUS : True\r\n  F_CONTIGUOUS : False\r\n  OWNDATA : True\r\n  WRITEABLE : True\r\n  ALIGNED : True\r\n  WRITEBACKIFCOPY : False\r\n```\r\n\r\n<\/details>\r\n\r\n## Feature proposal\r\n\r\nAdd a new feature in Arrow C++ to convert Arrow `Table` and `RecordBatch` to a `Tensor`. The conversion should:\r\n\r\n- return an Arrow `Tensor` (not `FixedShapeTensorArray`!),\r\n- support following data types: `uint` (8\/16\/32\/64), `int` (8\/16\/32\/64) and `float` (16\/32\/64)\r\n- support `NaNs`,\r\n- add an option to convert `NULL` to `NaN` (validity bitmaps not supported in the resulting `Tensor`),\r\n- support column and row-major layout.\r\n\r\nWe also plan to [implement the DLPack protocol](https:\/\/github.com\/apache\/arrow\/issues\/39294) for the Tensor class separately. With these features the ML libraries will have the option to consume matrices (2D tensors) directly or via the DLPack protocol without the need to do their own gymnastics.\r\n\r\n## Umbrella issue\r\n\r\nThis issue is an umbrella issue for smaller tasks created for easier reviews:\r\n\r\n- [x] https:\/\/github.com\/apache\/arrow\/issues\/40059\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40357\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40297\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40060\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40061\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40062\r\n- [ ] https:\/\/github.com\/apache\/arrow\/issues\/40063\r\n\r\n### Component(s)\r\n\r\nC++, Python","comments":[],"labels":["Type: enhancement","Component: C++","Component: Python"]},{"title":"[Python] `pa.array(<pd.Series of structs>)` changes field order to be sorted","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\nIf I give a column of structs, then pyarrow seems to sort the fields by name. I would expect the field order to be preserved.\r\n\r\n```python\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nprint(pa.__version__)\r\n# 15.0.0\r\n\r\ns = pd.Series([{\"b\": 2, \"a\": 1}, {\"b\": 4, \"a\": 3}])\r\na = pa.array(s)\r\nprint(a.type)\r\n# struct<a: int64, b: int64>\r\n```\r\n\r\nThis was discovered in https:\/\/github.com\/ibis-project\/ibis\/issues\/8166\n\n### Component(s)\n\nPython","comments":["Quick guess, but I assume this might stem from a time when Python dicts didn't have an order, and so the keys were sorted to guarantee stable results.","Although it seems we iterate over the dict using `PyDict_Next`. The Python docs (https:\/\/docs.python.org\/3\/c-api\/dict.html#c.PyDict_Next) are not very clear about the order of the iteration with this function.","Thanks for the help @jorisvandenbossche !\r\n\r\n(disclaimer, not a very good C++ programmer) Are you referring to [this function](https:\/\/github.com\/apache\/arrow\/blob\/bbe59b35de33a0534fc76c9617aa4746031ce16c\/python\/pyarrow\/src\/arrow\/python\/inference.cc#L628-L660)? Could the shuffling be due to the use of `struct_inferrers_.insert()`, where `struct_inferrers_` is a `std::map<std::string, TypeInferrer>`, so it doesn't preserve order? ","> Are you referring to [this function](https:\/\/github.com\/apache\/arrow\/blob\/bbe59b35de33a0534fc76c9617aa4746031ce16c\/python\/pyarrow\/src\/arrow\/python\/inference.cc#L628-L660)?\r\n\r\nYes, and then the `struct_inferrers_` gets converted to the struct type here: \r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/bbe59b35de33a0534fc76c9617aa4746031ce16c\/python\/pyarrow\/src\/arrow\/python\/inference.cc#L668-L676\r\n\r\n\r\n> Could the shuffling be due to the use of `struct_inferrers_.insert()`, where `struct_inferrers_` is a `std::map<std::string, TypeInferrer>`, so it doesn't preserve order?\r\n\r\nGood catch! Indeed, a map will sort the keys. If we want to preserve the order of the dict keys, we will have to use a different data structure (or keep track of the order separately)","If you point me in the direction of which data structure you think we should use, I can take a stab at this.\r\n\r\nEDIT: maybe just a `std::vector<std::pair<std::string, TypeInferrer>>`? There is [one place where we look up a field by name](https:\/\/github.com\/apache\/arrow\/blob\/bbe59b35de33a0534fc76c9617aa4746031ce16c\/python\/pyarrow\/src\/arrow\/python\/inference.cc#L644), which would be slow without a hash-based index. Would this be in the hot path, called over and over for every passed in python dictionary?\r\n\r\nI feel like somewhere in the arrow codebase there MUST be somewhere else an ordered map is used, but [I didn't find it](https:\/\/github.com\/search?q=repo%3Aapache%2Farrow%20ordered%20map&type=code).\r\n\r\nI found https:\/\/stackoverflow.com\/questions\/1098175\/a-stdmap-that-keep-track-of-the-order-of-insertion. I'm assuming we don't want to bring in any external deps. Could copy-paste https:\/\/github.com\/Tessil\/ordered-map\/blob\/master\/include\/tsl\/ordered_map.h? Or, its much smaller, but more \"one-off\", could [implement it with a list and a map.](https:\/\/stackoverflow.com\/a\/1098236\/5156887)\r\n\r\n"],"labels":["Type: bug","Component: Python"]},{"title":"[JS] `vectorFromArray()` can't create Int64 from Number","body":"### Describe the bug, including details regarding any error messages, version, and platform.\n\n\r\n```js\r\nconst { vectorFromArray, Int64 } = require('apache-arrow');\r\nvectorFromArray([1], new Int64())\r\n```\r\n\r\nFails with\r\n\r\n```\r\nUncaught TypeError: Cannot convert 1 to a BigInt\r\n    at DataBufferBuilder.set (\/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/builder\/buffer.js:79:42)\r\n    at Int64Builder.setValue (\/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/builder\/int.js:24:22)\r\n    at Int64Builder.set (\/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/builder.js:183:18)\r\n    at Int64Builder.append (\/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/builder.js:165:33)\r\n    at \/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/factories.js:183:25\r\n    at Generator.next (<anonymous>)\r\n    at vectorFromArray (\/Users\/willjones\/Downloads\/repro2\/node_modules\/apache-arrow\/factories.js:52:55)\r\n```\r\n\r\nIf we wrap in `BigInt`, it works:\r\n\r\n```\r\n> vectorFromArray([BigInt(1)], new Int64());\r\nVector [IntVector<Int>] {\r\n  isValid: [Function (anonymous)],\r\n  get: [Function (anonymous)],\r\n  ...\r\n```\r\n\r\nThis seems odd, as it seems like `1` is being converted internally to something that can't be converted into a BigInt. I would expect it to be preserved, unless I am misunderstanding how the conversion to BitInt is happening.\r\n\r\n\n\n### Component(s)\n\nJavaScript","comments":[],"labels":["Type: bug","Component: JavaScript","javascript"]},{"title":"[R] write_parquet() a large data.table with index will make the parquet file unreadable ","body":"### Describe the bug, including details regarding any error messages, version, and platform.\r\n\r\nSaving a data.frame with a big attribute (like an index commonly used in the `data.table` package) will make the parquet file unreadable and produce this error :\r\n\r\n```\r\nError in `open_dataset()`:\r\n! IOError: Error creating dataset. Could not read schema from 'path\/example.parquet'. \r\nIs this a 'parquet' file?: \r\nCould not open Parquet input source 'path\/example.parquet':\r\nCouldn't deserialize thrift: TProtocolException: Exceeded size limit\r\n```\r\n\r\nBug as understood by the [stackoverflow issue](https:\/\/stackoverflow.com\/questions\/77982801\/error-package-arrowr-read-parquet-open-dataset-couldnt-deserialize-thrift-t) : The normal efficiency of binary-data-storage in parquet files is not afforded to R attributes, so a big attribute (like `data.table` indexes) would break the format.\r\n\r\n**This bug has important reliability implications for the parquet format**\r\n\r\n### Repex : \r\n```r\r\nlibrary(arrow)\r\nlibrary(data.table)\r\n# Seed\r\nset.seed(1L)\r\n# Big enough data.table \r\ndt = data.table(x = sample(1e5L, 1e7L, TRUE), y = runif(100L)) \r\n# Save in parquet format\r\nwrite_parquet(dt, \"example_ok.parquet\")\r\n# Readable\r\ndt_ok <- open_dataset(\"example_ok.parquet\")\r\n# Simple filter \r\ndt[x == 989L]\r\n# Save in parquet format\r\nwrite_parquet(dt, \"example_error.parquet\")\r\n# Error\r\ndt_error <- open_dataset(\"example_error.parquet\")\r\n```\r\n\r\n\r\n### Component(s)\r\n\r\nParquet, R","comments":["Hi @Yannaubineau, thanks for the report. Both the R and Python packages have tests covering this behavior so it's a known issue. Though, as you found out, they will happily write a Parquet file that can't be read in cases like this.\r\n\r\nA work-around for now would be to pass an extra option to `open_dataset` that sets the limit to a high-enough value:\r\n\r\n```r\r\ndt_error <- open_dataset(\".\/example_error.parquet\", thrift_string_size_limit=1000000000)\r\n```\r\n\r\nI'm not sure we want to increase or remove the default limit, as that might cause other problems. @Yannaubineau do you think a more informative error would be enough of a fix here?","Hi @amoeba, thank you for your answer. Sorry if it is a non-issue.\r\n\r\nThe main problem I faced was the **lack of indication of the source of the error**, and **the absence of warning prior to \"creating\" the error**.\r\n\r\nThank you for the sample code, it works like a charm !\r\n\r\nI think there is two aspects to this : \r\n\r\n- The error output is confusing, `Is this a 'parquet' file?` doesn't feel right if the error is known related to a string size limit parameter. **So informing the user of this parameter inside the error message would definitely be an improvement.** \r\n- But I also think that it could be very informative to trigger a warning when a parquet file is created (through `write_parquet` or `write_dataset`) **from a data.frame containing attributes**, simply because of how massive the size of the parquet file can get compared to the same data.frame without any attribute. **Users should be aware in some way that attributes in their data are impeding on the efficiency of the binary-data-storage.**","> The error output is confusing, Is this a 'parquet' file? doesn't feel right if the error is known related to a string size limit parameter. So informing the user of this parameter inside the error message would definitely be an improvement.\r\n\r\nIt's actually more likely that user pointed the reader to a random file that starts with bytes encoding a huge length value for the metadata string. Note that after asking if it's a Parquet file, it says `Couldn't deserialize thrift: TProtocolException: Exceeded size limit` -- perhaps that message could be improved with a hint on how to increase the Thrift size limit.","Thanks @Yannaubineau. I do think this is an issue and I think (1) warning when writing such a file and (2) giving the user a better error when reading one are both good improvements here. Would you have any interest in contributing either or both?","I would sure be interested but I failed to even find where the text message originated from in the code base - so it feels more right for someone else to do it","I believe the message comes from here\r\n\r\nhttps:\/\/github.com\/apache\/arrow\/blob\/main\/cpp\/src\/parquet\/thrift_internal.h#L444-L463\r\n\r\n`ThriftException` is converted to an ~arrow::Status~ `ParquetException`, so more text could be added depending on what the thrift exception is about."],"labels":["Type: bug","Component: R","Component: Parquet"]},{"title":"GH-40038: [Java] Export non empty offset buffer for variable-size layout through C Data Interface","body":"<!--\r\nThanks for opening a pull request!\r\nIf this is your first pull request you can find detailed information on how \r\nto contribute here:\r\n  * [New Contributor's Guide](https:\/\/arrow.apache.org\/docs\/dev\/developers\/guide\/step_by_step\/pr_lifecycle.html#reviews-and-merge-of-the-pull-request)\r\n  * [Contributing Overview](https:\/\/arrow.apache.org\/docs\/dev\/developers\/overview.html)\r\n\r\n\r\nIf this is not a [minor PR](https:\/\/github.com\/apache\/arrow\/blob\/main\/CONTRIBUTING.md#Minor-Fixes). Could you open an issue for this pull request on GitHub? https:\/\/github.com\/apache\/arrow\/issues\/new\/choose\r\n\r\nOpening GitHub issues ahead of time contributes to the [Openness](http:\/\/theapacheway.com\/open\/#:~:text=Openness%20allows%20new%20users%20the,must%20happen%20in%20the%20open.) of the Apache Arrow project.\r\n\r\nThen could you also rename the pull request title in the following format?\r\n\r\n    GH-${GITHUB_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\nor\r\n\r\n    MINOR: [${COMPONENT}] ${SUMMARY}\r\n\r\nIn the case of PARQUET issues on JIRA the title also supports:\r\n\r\n    PARQUET-${JIRA_ISSUE_ID}: [${COMPONENT}] ${SUMMARY}\r\n\r\n-->\r\n\r\n### Rationale for this change\r\n\r\n<!--\r\n Why are you proposing this change? If this is already explained clearly in the issue then this section is not needed.\r\n Explaining clearly why changes are proposed helps reviewers understand your changes and offer better suggestions for fixes.  \r\n-->\r\n\r\nWe encountered an error when exchanging string array from Java to Rust through Arrow C data interface. At Rust side, it complains that the buffer at position 1 (offset buffer) is null. After tracing down and some debugging, it looks like the issue is Java Arrow `BaseVariableWidthVector` class assigns an empty offset buffer if the array is empty (value count 0).\r\n\r\nAccording to Arrow [spec](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#variable-size-binary-layout) for variable size binary layout:\r\n\r\n> The offsets buffer contains length + 1 signed integers ...\r\n\r\nSo for an empty string array, its offset buffer should be a buffer with one element (generally it is `0`).\r\n\r\n### What changes are included in this PR?\r\n\r\n<!--\r\nThere is no need to duplicate the description in the issue here but it is sometimes worth providing a summary of the individual changes in this PR.\r\n-->\r\n\r\nThis patch replaces current empty offset buffer in variable-size layout vector classes when exporting arrays through C Data Interface.\r\n\r\n### Are these changes tested?\r\n\r\n<!--\r\nWe typically require tests for all PRs in order to:\r\n1. Prevent the code from being accidentally broken by subsequent changes\r\n2. Serve as another way to document the expected behavior of the code\r\n\r\nIf tests are not included in your PR, please explain why (for example, are they covered by existing tests)?\r\n-->\r\n\r\nAdded test cases.\r\n\r\n### Are there any user-facing changes?\r\n\r\n<!--\r\nIf there are user-facing changes then we may require documentation to be updated before approving the PR.\r\n-->\r\n\r\nNo\r\n\r\n<!--\r\nIf there are any breaking changes to public APIs, please uncomment the line below and explain which changes are breaking.\r\n-->\r\n<!-- **This PR includes breaking changes to public APIs.** -->\r\n\r\n<!--\r\nPlease uncomment the line below (and provide explanation) if the changes fix either (a) a security vulnerability, (b) a bug that caused incorrect or invalid data to be produced, or (c) a bug that causes a crash (even when the API contract is upheld). We use this to highlight fixes to issues that may affect users without their knowledge. For this reason, fixing bugs that cause errors don't count, since those are usually obvious.\r\n-->\r\n<!-- **This PR contains a \"Critical Fix\".** -->\r\n* Closes: #40038","comments":[":warning: GitHub issue #40038 **has been automatically assigned in GitHub** to PR creator.","Ah, okay, I see. I've not found the ticket. Looks like both cases (empty or single zero element) are acceptable. I will close this. Thanks for the info.","Are you using arrow2? It's possible it was never updated to fix this, but arrow-rs appears to have been fixed","I'm using arrow-rs. Although its array data allows empty offset, the issue happens in ffi module which doesn't allow empty offsets for now. I proposed a fix there.","Hmm. While we allow that for IPC, I think I'm wrong about C Data: we explicitly specify this is not allowable https:\/\/arrow.apache.org\/docs\/format\/CDataInterface.html#c.ArrowArray.buffers\r\n\r\n@pitrou presumably Java should actually be fixed here, but what do other implementations do\/we appear to be missing tests for this case?","> The buffer pointers MAY be null only in two situations:\r\n> for the null bitmap buffer, if ArrowArray.null_count is 0;\r\n> for any buffer, if the size in bytes of the corresponding buffer would be 0.\r\n\r\nIsn't it allowed for second situation?","Ah, fair. It reads as a bit ambiguous to me: would the size of the buffer be 0, or is it that it should not be 0, but we allowed 0 as an exception before? I guess both interpretations lead to the buffer being 0-sized, though, and so it applies. Maybe we should spell out the case...\r\n\r\nRegardless, it does seem we're missing an integration test then.","> @pitrou presumably Java should actually be fixed here, but what do other implementations do\/we appear to be missing tests for this case?\r\n\r\nYes, I think it should be fixed. Zero-length columns _are_ tested in the integration test, but whether they actually miss an offsets buffer depends on how the JSON reader behaves.\r\n","Sorry, this is for the C Data integration tests","Ah ok, now I see what you mean","It seems this fix might be a bit heavy-handed if it adds a heap allocation for every empty var-width vector? AFAIU, Arrow Java uses empty vectors quite liberally.","> Ah, fair. It reads as a bit ambiguous to me: would the size of the buffer be 0, or is it that it should not be 0, but we allowed 0 as an exception before? I guess both interpretations lead to the buffer being 0-sized, though, and so it applies. Maybe we should spell out the case...\r\n\r\nI think it should mean that the buffer size is 0, the buffer pointers may be null, for the second situation.\r\n\r\n","> I think it should mean that the buffer size is 0, the buffer pointers may be null, for the second situation.\r\n\r\nYes, but it's the _buffer size_, not the array size. For an empty array, the buffer size should be 4 (or 8 for a large offsets type), so the buffer can't be null.\r\n","> It seems this fix might be a bit heavy-handed if it adds a heap allocation for every empty var-width vector? AFAIU, Arrow Java uses empty vectors quite liberally.\r\n\r\nYea, and I believe that @lidavidm meant that these empty offsets for empty var-width vector are valid. That's why I closed this and proposed a fix at ffi module at Rust instead.","> Yes, but it's the buffer size, not the array size. For an empty array, the buffer size should be 4 (or 8 for a large offsets type), so the buffer can't be null.\r\n\r\nThat's what I thought at the beginning and the reason I proposed this fix.\r\n\r\nBut somehow from the context @lidavidm provided:\r\nhttps:\/\/lists.apache.org\/thread\/w7g1zfqrjxx0bvrct0mt5zwxvdnc9nob\r\n\r\nI think he means that it is a valid case (empty offset).\r\n\r\n","I guess that this might be something we really need to fix (at least for the C data export part for var-size arrays). Reopened it.","> Yes, but it's the buffer size, not the array size. For an empty array, the buffer size should be 4 (or 8 for a large offsets type), so the buffer can't be null.\r\n\r\nExcept for empty array, I cannot image any other case that could result in empty offset buffer. So I guess that it means we should not never have null offset buffer in C data interface?\r\n\r\nBut from above context and the implementation in arrow-rs, it looks like we allow empty offset buffer for empty array?","> But somehow from the context @lidavidm provided: https:\/\/lists.apache.org\/thread\/w7g1zfqrjxx0bvrct0mt5zwxvdnc9nob\r\n> \r\n> I think he means that it is a valid case (empty offset).\r\n\r\nIt is allowed for historical reason in the Arrow IPC format, but it should be discouraged IMHO, and should not propagate to the C Data Interface.\r\n\r\nAs long as data stays internal to Arrow Java, the missing buffer is ok, but for purposes of exchanging data, a proper offsets buffers should be exported (this one can easily be statically allocated, by the way).","> As long as data stays internal to Arrow Java, the missing buffer is ok, but for purposes of exchanging data, a proper offsets buffers should be exported (this one can easily be statically allocated, by the way).\r\n\r\nI see. Then it makes sense. So seems I should not change these empty offset buffers as currently did in this PR but should change C data interface in Java to properly export it for empty offset cases.","Yes, I think it would be the most reasonable change here.","@vibhatha @davisusanibar @danepitkin I'm also concerned that none of the approaches here have failed tests consistently even when there should be an apparent memory leak, so the C Data Interface testing seems very suspect"],"labels":["Component: Java","awaiting changes"]}]
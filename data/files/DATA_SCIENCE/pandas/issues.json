[{"title":"BUG: Wrong kurtosis outcome due to inadequate fix to previous issues","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport polars as pl\r\nimport pandas as pd\r\nimport numpy as np\r\nimport scipy.stats as st\r\n\r\ndata = np.array([-2.05191341e-05,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00, -4.10391103e-05,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\r\n        0.00000000e+00])\r\n\r\nprint(pl.Series(data).kurtosis())\r\nprint(pd.Series(data).kurt())\r\nprint(st.kurtosis(data))\n```\n\n\n### Issue Description\n\nThe output of `pandas` kurtosis function is incorrect. \r\n\r\nAfter simple debugging I found a comment at `core\/nanops.py` line 1360, in function `nankurt`,\r\nsaying to fix #18044 it manually zeros out values less than 1e-14, which is in any way improper.\r\nThis affects whatever data comes with not much variance but lots of data.\r\n\r\n\n\n### Expected Behavior\n\nOutput of provided example:\r\n```text\r\n14.916104870028523\r\n0.0\r\n14.916104870028551\r\n```\r\n\r\nExpected output: roughly 14.9161 for unbiased (`pandas`'s default behaviour) is correct.\r\n\n\n### Installed Versions\n\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.19.0-1010-nvidia-lowlatency\r\nVersion               : #10-Ubuntu SMP PREEMPT_DYNAMIC Wed Apr 26 00:40:27 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.5.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Good point. Reproducing your example, this does happen in your example. Trying to scale it up to larger input distributions alleviates the issue though.\r\n\r\nYour example is a sweet spot for this error, rescaling your distribution to be larger, the zeroing out stops happening very quickly due to the O(count^2) and O(count^3) terms in the numerator and denominator equations counteracting lifting the very small m4 and m2^2 above the e-14 threshold.\r\n\r\nDoing a check of the form (pseudocode)\r\n`count < 100 and abs(frexp(denominator) - frexp(numerator)) < 24`\r\nbefore doing the zeroing out should alleviate this issue, but I would like to hear someone else's opinion before putting in a PR.","Another note: the kurtosis fomulation then still deviates from the scipy implementation by 3, up until a distribution size of about 10x your example, using the same shape of your example.\r\n\r\nI was not able to iron out that instability, though.","> Another note: the kurtosis fomulation then still deviates from the scipy implementation by 3, up until a distribution size of about 10x your example, using the same shape of your example.\r\n> \r\n> I was not able to iron out that instability, though.\r\n\r\nDo you mean that the difference of their output is roughly 3? If you have not set `bias=False` in `scipy` or `polars`, the difference here will be roughly 3.","> Do you mean that the difference of their output is roughly 3?\r\n\r\nExactly \r\n\r\n> If you have not set `bias=False` in `scipy` or `polars`, the difference here will be roughly 3.\r\n\r\nI did not, so then that's also explained. Then I see no issues with my solution anymore.\r\n\r\n"],"labels":["Bug","Needs Triage"]},{"title":"CLN: Enforce deprecation of argmin\/max and idxmin\/max with NA values","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nRef: #33941, #51276\r\n\r\nThis is complicated by #57745 - we still need a proper deprecation for groupby's idxmin\/idxmax. For DataFrame with EAs and axis=1, we use groupby's implementation. So I'm leaving that deprecation in place for now, and we can enforce it after groupby's is deprecated and ready to be enforced.","comments":[],"labels":["Missing-data","Clean","Reduction Operations"]},{"title":"WEB: Updating active\/inactive core devs","body":"I've checked with the devs who weren't active in pandas recently to see if they wished to become inactive, and few were happy to become inactive.\r\n\r\nBesides to manage expectations of other core devs and the community, this is relevant for the decision making proposed in [PDEP-1](https:\/\/github.com\/pandas-dev\/pandas\/pull\/53576\/files). With the current proposal, the number of maintainers to have a quorum is lowered from 12 to 11 after this PR, and it'll have an impact if more people become a maintainer.\r\n\r\n@bashtage I couldn't get an answer from you via email (I sent two emails). If you'd like to remain active that's totally fine, just let me know.","comments":["I'm updating the finance workgroup. Wes was the lead but he's becoming inactive, so I'm removing him from the workgroup. @mroeschke I'm adding you as leader of the workgroup, since I think you've been doing most of the work recently (at least approving expenses). If you want to have a discussion with the rest of the workgroup and there is a different outcome, please let me know:\r\n\r\nCC: @jreback @jorisvandenbossche @TomAugspurger ","SGTM. If anyone else wants a spot on the finance group I'm more than happy to step aside.","sgtm ","@TomAugspurger I personally don't think it's needed to have many people in the workgroups. I think having a couple of people to have a backup is good, or maybe 3 depending on the workgroup. So, seems reasonable that you step aside even if nobody new takes your spot, we already got Matt, Jeff and Joris.\r\n\r\nAlso, in the responsibilities we've got for the finance workgroup now we only have \"approve the expenses\". Not sure if it'd make sense to include requesting the grants and organizing the funds. Maybe in that case it could make sense if @phofl joins it?","> Maybe in that case it could make sense if @phofl joins it?\r\n\r\nsgtm"],"labels":["Admin","Web"]},{"title":"\"ValueError: Must have equal len keys and value when setting with an iterable\" when updating an object type cell using .loc with a nd.array","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({1: [1, 2, 3], 2: [np.zeros(3), np.ones(3), np.zeros(0)]})\r\n\r\nprint(df.dtypes)\r\n\r\ndf.loc[1, 2] = np.zeros(3)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe loc assign throws a \"ValueError: Must have equal len keys and value when setting with an iterable\" despite dtypes being object for column 2. This started to happen when I updated Pandas from  1.4.4 ->  2.2.1. In 1.4.4 this syntax worked. This issue also happens when assigning multiple columns at the same time as soon as one of the new values is an iterable.\r\n\r\n### Expected Behavior\r\n\r\nThe assign should set the second row to np.array of zeros. \r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.19.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.4.0\r\nVersion               : Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:06 PST 2024; root:xnu-10063.101.15~2\/RELEASE_ARM64_T8103\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0\r\nsetuptools            : 69.2.0\r\npip                   : 24.0\r\nCython                : 3.0.9\r\npytest                : 8.1.1\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.1\r\nnumba                 : 0.59.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.3\r\nsqlalchemy            : 2.0.28\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\nNone\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: IndexError with pandas.DataFrame.cumsum where dtype=timedelta64[ns]","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# Example DataFrame from pandas.DataFrame.cumsum documentation\r\ndf = pd.DataFrame([[2.0, 1.0],\r\n                   [3.0, np.nan],\r\n                   [1.0, 0.0]],\r\n                  columns=list('AB'))\r\n\r\n# IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\r\ndf.astype('timedelta64[ns]').cumsum()\r\n\r\n# These are fine\r\ndf.cumsum().astype('timedelta64[ns]')\r\ndf.apply(lambda s: s.astype('timedelta64[ns]').cumsum())\r\ndf.astype('timedelta64[ns]').apply(lambda s: s.cumsum())\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nA cumulative sum along the columns of a `DataFrame` with `timedelta64[ns]` types results in an `IndexError`. Cumulative min and max work correctly, and product correctly results in a `TypeError`, since it's not supported for timedelta.\r\n\r\nCumulative sums work correctly for `Series` with `timedelta64[ns]` type, so this seems to be a bug in `DataFrame`.\r\n\r\nThis is perhaps related to #41720?\r\n\r\n### Expected Behavior\r\n\r\n`pandas.DataFrame.cumsum` on a `DataFrame` with `timedelta64[ns]` columns should not erroneously raise `IndexError` and instead compute the cumulative sum, with the same effect as `df.apply(lambda s: s.cumsum())`.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n<summary>Installed Versions<\/summary>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.2.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.4.0\r\nVersion               : Darwin Kernel Version 23.4.0: Wed Feb 21 21:45:48 PST 2024; root:xnu-10063.101.15~2\/RELEASE_ARM64_T8122\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : None\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.22.2\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n```\r\n<\/details>","comments":["The code seems to be taking an single list instead of numpy array below\r\n<img width=\"484\" alt=\"Screenshot 2024-03-22 at 12 30 21\u202fAM\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/58086649\/9a1ac5d1-cc4e-4745-b9aa-17b60ea92ba0\">\r\n","  original_list = func(y)\r\n    result = [[x] for x in original_list]\r\n    result = np.array(result)\r\n    \r\n    This might help?"],"labels":["Bug","Needs Triage"]},{"title":"BUG: encoding is **silently** ignored for `read_csv` on FileLike objects","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pathlib\r\n\r\nimport pandas\r\nimport pytest\r\n\r\n\r\ndef test_encoding_is_ignored(\r\n    tmp_path: pathlib.Path,\r\n):\r\n    _test_csv = tmp_path \/ \"test.csv\"\r\n    with open(_test_csv, \"w\", encoding=\"latin1\") as f:\r\n        f.write(\"A,B\\n\u00dc,\u00c4\\n\")\r\n    # works\r\n    _direct_read = pandas.read_csv(_test_csv, encoding=\"latin1\")\r\n    with open(_test_csv, \"r\") as f:\r\n        # fails\r\n        _indirect_read = pandas.read_csv(f, encoding=\"latin1\")\n```\n\n\n### Issue Description\n\nIf `read_csv` is used with a FileLike object, the encoding is correctly delegated to the object, but the encoding parameter is silently ignored.\n\n### Expected Behavior\n\nA warning should be created to make the user aware of this behavior, which might be expected if the encoding is given explicitly to the `open` but not if it is using its default.\r\nAn easy way would be a comparison of the FileType's encoding to the one `read_csv` would use, if the latter is not the default.\r\nThis checking for deviation of the default could also be done by setting `encoding=None` in the definition and setting the internal encoding like `encoding = encoding or \"utf-8\"`\n\n### Installed Versions\n\n<details>\r\n\r\n>>> pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.2.0-26-generic\r\nVersion               : #26-Ubuntu SMP PREEMPT_DYNAMIC Mon Jul 10 23:39:54 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : 69.2.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 8.1.1\r\nhypothesis            : 6.99.11\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.22.2\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Hm, I guess we could raise if we're passed a file-like object and encoding != None, when the given encoding doesn't match the encoding for the file-like object.\r\n\r\nPRs for this would be welcome.","take"],"labels":["Bug","Error Reporting","IO CSV"]},{"title":"BUG: pandas df.to_markdown() with period at end of field - is not displayed.","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n>>> df = pd.DataFrame({'a': ['1234.']})\r\n>>> df\r\n       a\r\n0  1234.\r\n>>> df.to_markdown()\r\n'|    |    a |\\n|---:|-----:|\\n|  0 | 1234 |'\r\n\r\n^^ notice the 1234 above is lacking the '.' at the end of the string.  When I print the dataframe \r\ncell, the period appears:\r\n\r\n>>> print(df['a'][0])\r\n1234.\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen printing a string field that ends in a period, to_markdown() fails to print the period at the end of the string.\r\n\r\n\r\n### Expected Behavior\r\n```python\r\n>>> df.to_markdown()\r\n'|    |    a |\\n|---:|-----:|\\n|  0 | 1234. |'\r\n```\r\n### Installed Versions\r\n```\r\n>>> pd.show_versions()\r\n\/usr\/lib\/python3.9\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.16.final.0\r\npython-bits           : 64\r\nOS                    : CYGWIN_NT-10.0-19045\r\nOS-release            : 3.5.0-1.x86_64\r\nVersion               : 2024-02-01 11:02 UTC\r\nmachine               : x86_64\r\nprocessor             :\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.2\r\npip                   : 24.0\r\nCython                : 0.29.33\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 4.4.0\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n```\r\n### DEVELOPMENT VERSIONS\r\n\r\nALSO the bug appears on the latest development versions:\r\n```\r\n>>> pd.show_versions()\r\n\/usr\/lib\/python3.9\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\ncommit                : d17723c6d447d37c0cb753a517df74705806f4a2\r\npython                : 3.9.16.final.0\r\npython-bits           : 64\r\nOS                    : CYGWIN_NT-10.0-19045\r\nOS-release            : 3.5.0-1.x86_64\r\nVersion               : 2024-02-01 11:02 UTC\r\nmachine               : x86_64\r\nprocessor             :\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+578.gd17723c6d4\r\nnumpy                 : 1.26.4\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.2\r\npip                   : 24.0\r\nCython                : 0.29.33\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 4.4.0\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n```\r\n","comments":["Explanation from `python-tabulate\/README.md`:\r\n> For tabulate, anything which can be parsed as a number is a number. Even numbers represented as strings are aligned properly. This feature comes in handy when reading a mixed table of text and numbers from a file\r\n> To disable this feature use `disable_numparse=True`.\r\n\r\nSo to achieve your desired output:\r\n```\r\n>>> df.to_markdown(disable_numparse=True)\r\n'|    | a     |\\n|:---|:------|\\n| 0  | 1234. |'\r\n```\r\n"],"labels":["Bug","IO Data","Usage Question","Closing Candidate"]},{"title":"Potential regression induced by \"Refactored pandas_timedelta_to_timedeltastruct\"","body":"PR  #55999\r\n\r\nIf it is expected please ignore this issue. \r\n\r\n@WillAyd \r\n\r\n\"eb55bca5bf91541a5c4f6213b18824589415127b\": {\r\n        \"`tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field` (Python) with field='microseconds', size=1000000\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb93bf74c75d480006fe97e7b82d8...065fb94f7876798380009c8d16cc367e\",\r\n        \"`tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field` (Python) with field='seconds', size=1000000\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb93bf6bf7a7d8000621519222b60...065fb94f77ee7e3e800087d3ad953647\",\r\n        \"`tslibs.fields.TimeGetTimedeltaField.time_get_timedelta_field` (Python) with field='nanoseconds', size=1000000\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb93bf7e07ec98000d31ecf9914d4...065fb94f78fa779080005b333b82e55a\"\r\n    }\r\n<img width=\"720\" alt=\"Screenshot 2024-03-21 at 17 00 27\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/11835246\/92fa12ae-955b-4db5-a343-7756434f2c41\">\r\n","comments":["I think the issue here is that we combined a few case statements into one:\r\n\r\n```c\r\n  case NPY_FR_s:\r\n  case NPY_FR_ms:\r\n  case NPY_FR_us:\r\n  case NPY_FR_ns: {\r\n    npy_int64 per_sec;\r\n    if (base == NPY_FR_s) {\r\n      per_sec = 1;\r\n    } else if (base == NPY_FR_ms) {\r\n      per_sec = 1000;\r\n    } else if (base == NPY_FR_us) {\r\n      per_sec = 1000000;\r\n    } else {\r\n      per_sec = 1000000000;\r\n    }\r\n\r\n    ...\r\n```\r\n\r\nWhereas previously the code was copy\/pasted and slightly tweaked for every case statement.\r\n\r\nThere are probably some tricks we can try like using a static lookup table instead of branches:\r\n\r\n```c\r\n  case NPY_FR_s:\r\n  case NPY_FR_ms:\r\n  case NPY_FR_us:\r\n  case NPY_FR_ns: {\r\n    const npy_int64 sec_per_day = 86400;\r\n    static npy_int64 per_secs[] = {\r\n      1, \/\/ NPY_FR_s\r\n      1000, \/\/ NPY_FR_ms\r\n      1000000, \/\/ NPY_FR_us\r\n      1000000000 \/\/ NPY_FR_ns\r\n    };\r\n    const npy_int64 per_sec = per_secs[base - NPY_FR_s];\r\n```\r\n\r\nbut I'm not really sure the tricks are worth it. And I think the code de-duplication is probably the increased timing here"],"labels":["Performance","Regression","Needs Triage"]},{"title":"Potential regression induced by \"Cython guard against [c|m|re]alloc failures\"","body":"PR #57705 \r\n\r\nIf it is expected please ignore this issue.\r\nPlease be aware that the server is slow to respond some times. \r\n\r\n@WillAyd \r\n\r\ncommit \"114a84d8a0eee8fb93a4d2d701a2a6e62ebcf6d2\": {\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Float64', method='cummax', with_nans=True\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944a14b793780003322ee823635...065fb8e49664747b8000f70e4ec84ae6\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Int64', method='cummin', with_nans=True\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944ae8474e58000b3cec8275f57...065fb8e498ce7594800008071c2d8038\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Int64', method='cummin', with_nans=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944b2b47a138000cb49180477b4...065fb8e499487a9e8000b44250789920\",\r\n        \"`algos.isin.IsInLongSeriesLookUpDominates.time_isin` (Python) with MaxNumber=1000, dtype='Float64', series_type='random_misses'\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb9407ebe716e8000ed2fe62f5b83...065fb8e1b287741080003a30c54d529f\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='int64', method='cummin', with_nans=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb94491a376d180006f30866e9cb7...065fb8e4939b78d480008873c990c076\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Int64', method='cummax', with_nans=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944bc017c0780002137a5469d07...065fb8e49a7476d9800097b740074116\",\r\n        \"`algorithms.Factorize.time_factorize` (Python) with dtype='boolean', sort=True, unique=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb9400e25709d8000634602d20fa9...065fb8e14c1270388000c8005022c82f\",\r\n        \"`algos.isin.IsInLongSeriesLookUpDominates.time_isin` (Python) with MaxNumber=1000, dtype='float64', series_type='random_misses'\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb9406aa0734d8000d932d3e79d24...065fb8e1a1037615800024333e87bbf7\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Float64', method='cummin', with_nans=True\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb9449c0075d7800006671aaa142b...065fb8e4954a7e678000d0cf25e62d15\",\r\n        \"`groupby.GroupByCythonAggEaDtypes.time_frame_agg` (Python) with dtype='Int32', method='any'\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb945a1cb705f800027e9e412ca99...065fb8e4baf27ba58000e2dcb9c6c91c\",\r\n        \"`algos.isin.IsinWithArange.time_isin` (Python) with M=8000, dtype=<class 'numpy.float64'>, offset_factor=-2\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb940c55774118000016187a81f4d...065fb8e1f156773380009660ce668fd7\",\r\n        \"`algos.isin.IsInLongSeriesLookUpDominates.time_isin` (Python) with MaxNumber=1000, dtype='float64', series_type='monotone_misses'\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb9406baf7c048000e4b240c9b233...065fb8e1a200710f8000cf630f8dcec9\",\r\n        \"`groupby.GroupByCythonAggEaDtypes.time_frame_agg` (Python) with dtype='Float64', method='first'\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb94566c67d2f80005785dda0523a...065fb8e4acf773d98000663b6949b9a7\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='float64', method='cummax', with_nans=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944863a73328000100ab11e015f...065fb8e491f073228000be1a0dcb0813\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Int64', method='cummax', with_nans=True\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944b887736180002dd51b13dfc4...065fb8e499e779438000356086562483\",\r\n        \"`groupby.Cumulative.time_frame_transform` (Python) with dtype='Float64', method='cummax', with_nans=False\": \"http:\/\/57.128.112.95:5000\/compare\/benchmarks\/065fb944a4cc74dd80007276e8078ead...065fb8e496fc7c4d80002e67128bc5cf\"\r\n    }\r\n\r\n<img width=\"721\" alt=\"Screenshot 2024-03-21 at 16 53 24\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/11835246\/3f80f028-2f17-423b-a850-e5c026715229\">\r\n","comments":["Hmm that's interesting. OK thanks - definitely unexpected. Will take a look later","I checked the generated code and it generates things like:\r\n\r\n```c\r\nif (unlikely(ptr == NULL)) {\r\n    ...  \/\/ raise MemoryError\r\n}\r\n```\r\n\r\nSo I don't think this can be any more efficient. I noticed also some of the benchmarks were not really regressions when compared to values a few months back:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/609873\/81e169f6-7422-4504-abbe-b2452c531160)\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/609873\/a4c84388-a9f6-4de6-beaa-265bcd61b1e4)\r\n\r\nSo I don't think there is much to do here","@WillAyd - only question I have is how many times are these checks hit in the test? If it's an inner loop that's hit a few hundred times then a 2ms regression makes sense. But if they are just hit a few times per test run then perhaps more investigation is warranted. I'll come back and see if I can find out in a few days.","These all come after malloc calls. I don't believe any are done in a tight loop, but if they were the surprising part is that the malloc itself is where you would expect to see a bottleneck, not the subsequent call to check if malloc failed","Interesting - I'm not seeing any of the checks added in #57705 hit by \r\n\r\n> groupby.Cumulative.time_frame_transform (Python) with dtype='Float64', method='cummax', with_nans=True\r\n\r\nSame with\r\n\r\n> algos.isin.IsInLongSeriesLookUpDominates.time_isin (Python) with MaxNumber=1000, dtype='Float64', series_type='random_misses'"],"labels":["Performance","Closing Candidate"]},{"title":"BUG: Unable to use CustomBusinessDays in a MultiIndex","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nbus_day = pd.offsets.CustomBusinessDay(holidays=['2023-07-04'])  # Mon-Fri except July 4th\r\n\r\ndates = pd.date_range('2023-07-03', '2023-07-06', freq=bus_day)\r\ndf1 = pd.DataFrame({'col1': [300, 500, 600]}, index=dates)\r\nprint(df1.index.freq.holidays)  # Can get freq from dataframe index\r\n\r\nmi = pd.MultiIndex.from_product([dates, ('foo','bar')])\r\ndf2 = pd.DataFrame({'col1': [300, 301, 500, 501, 600, 601]}, index=mi)\r\nprint(df2.index.get_level_values(0).freq)  # Gives None. Freq lost from dataframe multi-index\r\n\r\nperiods = pd.period_range('2023-07-03', '2023-07-06', freq=bus_day)  # Try PeriodIndex?\r\n# Gives: TypeError: CustomBusinessDay cannot be used with Period or PeriodDtype\n```\n\n\n### Issue Description\n\nI can use a `CustomBusinessDay` frequency in a `DatetimeIndex` and retrieve the frequency from the `.index` of a dataframe.\r\n\r\nIf I use a `DatetimeIndex` in a `MultiIndex`, then I lose the `freq` from the level of the index.\r\n\r\nI note the `freq` _does_ persist in a `PeriodIndex` within a `MultiIndex`, but you can't use a `CustomBusinessDay` in a `PeriodIndex` as shown. \n\n### Expected Behavior\n\nI would like the Datetime level of the MultiIndex to show the orginial freq:\r\n`print(df2.index.get_level_values(0).freq)` would give `<CustomBusinessDay>`\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.13.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.145.2-1.cm2\r\nVersion             : #1 SMP Wed Jan 17 15:39:07 UTC 2024\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : en_US.UTF-8\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.4\r\npytz                : 2024.1\r\ndateutil            : 2.9.0.post0\r\nsetuptools          : 65.5.0\r\npip                 : 24.0\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.3\r\nIPython             : 8.22.2\r\npandas_datareader   : None\r\nbs4                 : 4.12.3\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2024.1\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["This loss of information does happen with the DateTimeIndex and not without the PeriodIndex. Reason is that the `freq`is an attribute of each value in the PeriodIndex. With the DateTimeIndex, the freq, etc. get lost when calling `get_level_values()`, due to the shallow copying logic.\r\n\r\nThe DateTimeIndex is the only index type with this edge case."],"labels":["Bug","Needs Triage"]},{"title":"BUG: itemsize wrong for date32[day][pyarrow] dtype?","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\npd.ArrowDtype(pa.date32())  # date32[day][pyarrow]\r\npd.ArrowDtype(pa.date32()).itemsize  # 8\n```\n\n\n### Issue Description\n\nI think it should show `4`? `pa.date32()` is 32 bits, so 4 bytes\n\n### Expected Behavior\n\npd.ArrowDtype(pa.date32()).itemsize  # 4\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : b033ca94e7ae6e1320c9d65a8163bd0a6049f40a\r\npython                : 3.10.12.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.146.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Jan 11 04:09:03 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.1.0.dev0+761.gb033ca94e7\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.1\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 8.0.2\r\nhypothesis            : 6.98.15\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.2.0\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.8\r\nfastparquet           : 2024.2.0\r\nfsspec                : 2024.2.0\r\ngcsfs                 : 2024.2.0\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npyarrow               : 15.0.2\r\npyreadstat            : 1.2.6\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2024.2.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.27\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.2.0\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Not sure what to do here, as there isn't a numpy dtype corresponding to date32\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/77f9d7abee14888447a1f9942f7f6f4cdbcd927b\/pandas\/core\/dtypes\/dtypes.py#L2215-L2218\r\n\r\n","I think it's OK to override `ArrowDtype.itemsize` to handle this case separately","PyArrow data types of fixed width have a `bit_width` attribute that could be used here. That does raise for nested types, though at the moment we just return 8 from the numpy object dtype, which also doesn't necessarily makes sense.\r\n\r\n```\r\nIn [13]: pd.ArrowDtype(pa.list_(pa.int32())).itemsize\r\nOut[13]: 8\r\n```"],"labels":["Bug","Arrow"]},{"title":"BUILD: cross-compiling issue using crossenv","body":"### Installation check\n\n- [X] I have read the [installation guide](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/getting_started\/install.html#installing-pandas).\n\n\n### Platform\n\nSynology toolchain for any (x64, armv7, aarch64, ppc)\n\n### Installation Method\n\nBuilt from source\n\n### pandas Version\n\n2.0.3 and 2.2.1\n\n### Python Version\n\n3.11.8\n\n### Installation Logs\n\n<details>\r\n\r\nRequirement already satisfied: pybind11==2.11.1 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages (2.11.1)\r\nRequirement already satisfied: expandvars==0.12.0 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages (0.12.0)\r\nRequirement already satisfied: python-dateutil==2.9.0.post0 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages (2.9.0.post0)\r\nRequirement already satisfied: pytz==2024.1 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages (2024.1)\r\nCollecting tzdata==2024.1\r\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nRequirement already satisfied: six>=1.5 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages (from python-dateutil==2.9.0.post0) (1.16.0)\r\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4\/345.4 kB 3.9 MB\/s eta 0:00:00\r\nInstalling collected packages: tzdata\r\nSuccessfully installed tzdata-2024.1\r\nRequirement already satisfied: pybind11==2.11.1 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/lib\/python3.11\/site-packages (2.11.1)\r\nRequirement already satisfied: expandvars==0.12.0 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/lib\/python3.11\/site-packages (0.12.0)\r\nRequirement already satisfied: python-dateutil==2.9.0.post0 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/lib\/python3.11\/site-packages (2.9.0.post0)\r\nRequirement already satisfied: pytz==2024.1 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/lib\/python3.11\/site-packages (2024.1)\r\nCollecting tzdata==2024.1\r\n  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nRequirement already satisfied: six>=1.5 in \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/lib\/python3.11\/site-packages (from python-dateutil==2.9.0.post0) (1.16.0)\r\nUsing cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\r\nInstalling collected packages: tzdata\r\nSuccessfully installed tzdata-2024.1\r\n===>  Processing wheels of homeassistant\r\n===>  Downloading wheels from src\/requirements-abi3.txt ...\r\n===>  pip download [pycryptodome], version [3.18.0]\r\nINFO: File already exists [pycryptodome-3.18.0.tar.gz]\r\n===>  pip download [pycryptodomex], version [3.18.0]\r\nINFO: File already exists [pycryptodomex-3.18.0.tar.gz]\r\n===>  Downloading wheels from src\/requirements-crossenv.txt ...\r\n===>  pip download [pandas], version [2.2.1]\r\nINFO: File already exists [pandas-2.2.1.tar.gz]\r\n===>  pip download [webrtcvad], version [2.0.10], URL: [git+https:\/\/github.com\/wiseman\/py-webrtcvad@3bd761332a9404f5c9276105070ee814c4428342#egg=webrtcvad==2.0.10]\r\nWARNING: Skipping download URL - Downloaded at build time\r\n===>  Adding existing src\/requirements-abi3.txt file as ABI-limited\r\n===>  Adding existing src\/requirements-crossenv.txt file as cross-compiled (discarding any pure-python)\r\n===>  Adding existing src\/requirements-pure.txt file as pure-python (discarding any cross-compiled)\r\n===>  Cross-compiling wheels\r\n===>  Python crossenv found: [\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/bin\/activate]\r\n===>  pip crossenv found: [\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/pip]\r\n===>  [pandas]           --config-settings=editable-verbose=true\r\nmake[3]: Entering directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\n===>  _PYTHON_HOST_PLATFORM=x86_64-pc-linux-gnu \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/pip wheel --disable-pip-version-check --no-binary :all: --find-links \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip --cache-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/pip --no-deps --wheel-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/wheelhouse --no-index --config-settings=editable-verbose=true --no-build-isolation pandas==2.2.1\r\nLooking in links: \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip\r\nProcessing \/home\/spksrc\/py311-update\/spksrc\/distrib\/pip\/pandas-2.2.1.tar.gz\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'done'\r\nERROR: Exception:\r\nTraceback (most recent call last):\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/cli\/base_command.py\", line 180, in exc_logging_wrapper\r\n    status = run_func(*args)\r\n             ^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/cli\/req_command.py\", line 245, in wrapper\r\n    return func(self, options, args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/commands\/wheel.py\", line 147, in run\r\n    requirement_set = resolver.resolve(reqs, check_supported_wheels=True)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/resolver.py\", line 95, in resolve\r\n    result = self._result = resolver.resolve(\r\n                            ^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/resolvelib\/resolvers.py\", line 546, in resolve\r\n    state = resolution.resolve(requirements, max_rounds=max_rounds)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/resolvelib\/resolvers.py\", line 397, in resolve\r\n    self._add_to_criteria(self.state.criteria, r, parent=None)\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/resolvelib\/resolvers.py\", line 173, in _add_to_criteria\r\n    if not criterion.candidates:\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/resolvelib\/structs.py\", line 156, in __bool__\r\n    return bool(self._sequence)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/found_candidates.py\", line 155, in __bool__\r\n    return any(self)\r\n           ^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/found_candidates.py\", line 143, in <genexpr>\r\n    return (c for c in iterator if id(c) not in self._incompatible_ids)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/found_candidates.py\", line 47, in _iter_built\r\n    candidate = func()\r\n                ^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/factory.py\", line 182, in _make_candidate_from_link\r\n    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/factory.py\", line 228, in _make_base_candidate_from_link\r\n    self._link_candidate_cache[link] = LinkCandidate(\r\n                                       ^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/candidates.py\", line 290, in __init__\r\n    super().__init__(\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/candidates.py\", line 156, in __init__\r\n    self.dist = self._prepare()\r\n                ^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/candidates.py\", line 222, in _prepare\r\n    dist = self._prepare_distribution()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/resolution\/resolvelib\/candidates.py\", line 301, in _prepare_distribution\r\n    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/operations\/prepare.py\", line 525, in prepare_linked_requirement\r\n    return self._prepare_linked_requirement(req, parallel_builds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/operations\/prepare.py\", line 640, in _prepare_linked_requirement\r\n    dist = _get_prepared_distribution(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/operations\/prepare.py\", line 71, in _get_prepared_distribution\r\n    abstract_dist.prepare_distribution_metadata(\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/distributions\/sdist.py\", line 67, in prepare_distribution_metadata\r\n    self.req.prepare_metadata()\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/req\/req_install.py\", line 579, in prepare_metadata\r\n    self.metadata_directory = generate_metadata(\r\n                              ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/operations\/build\/metadata.py\", line 35, in generate_metadata\r\n    distinfo_dir = backend.prepare_metadata_for_build_wheel(metadata_dir)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_internal\/utils\/misc.py\", line 766, in prepare_metadata_for_build_wheel\r\n    return super().prepare_metadata_for_build_wheel(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/pyproject_hooks\/_impl.py\", line 186, in prepare_metadata_for_build_wheel\r\n    return self._call_hook('prepare_metadata_for_build_wheel', {\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/pyproject_hooks\/_impl.py\", line 321, in _call_hook\r\n    raise BackendUnavailable(data.get('traceback', ''))\r\npip._vendor.pyproject_hooks._impl.BackendUnavailable: Traceback (most recent call last):\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/build\/lib\/python3.11\/site-packages\/pip\/_vendor\/pyproject_hooks\/_in_process\/_in_process.py\", line 77, in _build_backend\r\n    obj = import_module(mod_path)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/spksrc\/py311-update\/spksrc\/native\/python311\/work-native\/install\/usr\/local\/lib\/python3.11\/importlib\/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1140, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'mesonpy'\r\n\r\nmake[3]: *** [..\/..\/mk\/spksrc.wheel.mk:203: cross-compile-wheel-pandas] Error 2\r\nmake[3]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake[2]: *** [..\/..\/mk\/spksrc.wheel.mk:144: build_wheel_target] Error 1\r\nmake[2]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake[1]: *** [..\/..\/mk\/spksrc.spk.mk:719: build-arch-x64-7.1] Error 1\r\nmake[1]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake: *** [..\/..\/mk\/spksrc.spk.mk:713: arch-x64-7.1] Error 2\r\n\r\n<\/details>\r\n","comments":["Associated PR (although code is in my local tree) https:\/\/github.com\/SynoCommunity\/spksrc\/pull\/6040","Can you provide your build command?\r\n\r\nLooking at the error, you are missing the ``meson\/meson-python`` dependencies.\r\n\r\n","Indeed it was missing, thnx now added:\r\n```\r\nmake[3]: Entering directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\n===>  _PYTHON_HOST_PLATFORM=x86_64-pc-linux-gnu \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/pip wheel --disable-pip-version-check --no-binary :all: --find-links \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip --cache-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/pip --no-deps --wheel-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/wheelhouse --no-index --no-clean --config-settings=editable-verbose=true --no-build-isolation pandas==2.2.1\r\nLooking in links: \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip\r\nProcessing \/home\/spksrc\/py311-update\/spksrc\/distrib\/pip\/pandas-2.2.1.tar.gz\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'error'\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [10 lines of output]\r\n      + meson setup \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9 \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\/.mesonpy-y9035jpi -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=\/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\/.mesonpy-y9035jpi\/meson-python-native-file.ini\r\n      The Meson build system\r\n      Version: 1.4.0\r\n      Source dir: \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\r\n      Build dir: \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\/.mesonpy-y9035jpi\r\n      Build type: native build\r\n      \r\n      ..\/meson.build:5:13: ERROR: Command `\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/python \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\/generate_version.py --print` failed with status 1.\r\n      \r\n      A full log can be found at \/tmp\/pip-wheel-8eh27d9c\/pandas_598e5b7c95ba46e28cbfac20f70c52d9\/.mesonpy-y9035jpi\/meson-logs\/meson-log.txt\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\nSo from here I now added `--no-clean` to pip but still the `meson-log.txt` gets deleted.  Actually the whole `.meson-*` is deleted from the temporary folder.  Any arguments I can pass at pip build time so meson's files do not get deleted?","I was able to keep the log file by setting-up the build directory: `--config-settings=builddir=build`\r\n```\r\nBuild started at 2024-03-23T13:48:57.608601\r\nMain binary: \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/python\r\nBuild Options: -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md -Dvsenv=True --cross-file=\/home\/spksrc\/py311-update\/\r\nspksrc\/spk\/homeassistant\/work-x64-7.1\/tc_vars.meson --native-file=\/tmp\/pip-wheel-s1xttpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83\r\nedf1917\/build\/meson-python-native-file.ini\r\nPython system: Linux\r\nThe Meson build system\r\nVersion: 1.4.0\r\nSource dir: \/tmp\/pip-wheel-s1xttpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83edf1917\r\nBuild dir: \/tmp\/pip-wheel-s1xttpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83edf1917\/build\r\nBuild type: cross build\r\nRunning command: \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/python \/tmp\/pip-wheel-s1xt\r\ntpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83edf1917\/generate_version.py --print\r\n--- stdout ---\r\n\r\n--- stderr ---\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/pip-wheel-s1xttpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83edf1917\/generate_version.py\", line 8, in <module>\r\n    import versioneer\r\nModuleNotFoundError: No module named 'versioneer'\r\n\r\n\r\n\r\n..\/meson.build:5:13: ERROR: Command `\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/python\r\n \/tmp\/pip-wheel-s1xttpye\/pandas_2f0e7fd58d5b45ebaf8fb7d83edf1917\/generate_version.py --print` failed with status 1.\r\n```","Now added `versioneer` to my crossenv, with the following result (progress I guess):\r\n\r\nEDIT: Note that I found curious the `Visual Studio environment` message.\r\n\r\n```\r\n===>  _PYTHON_HOST_PLATFORM=x86_64-pc-linux-gnu \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/pip wheel --disable-pip-version-check --no-binary :all: --find-links \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip --cache-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/pip --no-deps --wheel-dir \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/wheelhouse --no-index --no-clean --config-settings=builddir=build --config-settings=editable-verbose=true --config-settings=setup-args=--cross-file=\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/tc_vars.meson --no-build-isolation pandas==2.2.1\r\nLooking in links: \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/..\/..\/distrib\/pip\r\nProcessing \/home\/spksrc\/py311-update\/spksrc\/distrib\/pip\/pandas-2.2.1.tar.gz\r\n  Preparing metadata (pyproject.toml): started\r\n  Preparing metadata (pyproject.toml): finished with status 'error'\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [117 lines of output]\r\n      + meson setup \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287 \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --cross-file=\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/tc_vars.meson --native-file=\/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/build\/meson-python-native-file.ini\r\n      The Meson build system\r\n      Version: 1.4.0\r\n      Source dir: \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\r\n      Build dir: \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/build\r\n      Build type: cross build\r\n      Project name: pandas\r\n      Project version: 2.2.1\r\n      C compiler for the host machine: \/home\/spksrc\/py311-update\/spksrc\/toolchain\/syno-x64-7.1\/work\/x86_64-pc-linux-gnu\/bin\/x86_64-pc-linux-gnu-gcc (gcc 8.5.0 \"x86_64-pc-linux-gnu-gcc (GCC) 8.5.0\")\r\n      C linker for the host machine: \/home\/spksrc\/py311-update\/spksrc\/toolchain\/syno-x64-7.1\/work\/x86_64-pc-linux-gnu\/bin\/x86_64-pc-linux-gnu-gcc ld.bfd 2.30\r\n      C++ compiler for the host machine: \/home\/spksrc\/py311-update\/spksrc\/toolchain\/syno-x64-7.1\/work\/x86_64-pc-linux-gnu\/bin\/x86_64-pc-linux-gnu-g++ (gcc 8.5.0 \"x86_64-pc-linux-gnu-g++ (GCC) 8.5.0\")\r\n      C++ linker for the host machine: \/home\/spksrc\/py311-update\/spksrc\/toolchain\/syno-x64-7.1\/work\/x86_64-pc-linux-gnu\/bin\/x86_64-pc-linux-gnu-g++ ld.bfd 2.30\r\n      Cython compiler for the host machine: cython (cython 0.29.37)\r\n      C compiler for the build machine: cc (gcc 10.2.1 \"cc (Debian 10.2.1-6) 10.2.1 20210110\")\r\n      C linker for the build machine: cc ld.bfd 2.35.2\r\n      C++ compiler for the build machine: c++ (gcc 10.2.1 \"c++ (Debian 10.2.1-6) 10.2.1 20210110\")\r\n      C++ linker for the build machine: c++ ld.bfd 2.35.2\r\n      Cython compiler for the build machine: cython (cython 0.29.37)\r\n      Build machine cpu family: x86_64\r\n      Build machine cpu: x86_64\r\n      Host machine cpu family: x86_64\r\n      Host machine cpu: x86_64\r\n      Target machine cpu family: x86_64\r\n      Target machine cpu: x86_64\r\n      Program python3 found: YES (\/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/python)\r\n      Found pkg-config: NO\r\n      Run-time dependency python found: YES 3.11\r\n      Build targets in project: 53\r\n      \r\n      pandas 2.2.1\r\n      \r\n        User defined options\r\n          Cross files : \/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant\/work-x64-7.1\/tc_vars.meson\r\n          Native files: \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/build\/meson-python-native-file.ini\r\n          buildtype   : release\r\n          vsenv       : true\r\n          b_ndebug    : if-release\r\n          b_vscrt     : md\r\n      \r\n      Found ninja-1.10.1 at \/usr\/bin\/ninja\r\n      \r\n      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\r\n      \/home\/spksrc\/py311-update\/spksrc\/spk\/python311\/work-x64-7.1\/crossenv\/cross\/bin\/meson compile -C .\r\n      + \/usr\/bin\/ninja\r\n      [1\/151] Generating pandas\/_libs\/khash_primitive_helper_pxi with a custom command\r\n      [2\/151] Generating pandas\/_libs\/algos_common_helper_pxi with a custom command\r\n      [3\/151] Generating pandas\/_libs\/intervaltree_helper_pxi with a custom command\r\n      [4\/151] Generating pandas\/_libs\/sparse_op_helper_pxi with a custom command\r\n      [5\/151] Generating pandas\/_libs\/hashtable_class_helper_pxi with a custom command\r\n      [6\/151] Generating pandas\/_libs\/algos_take_helper_pxi with a custom command\r\n      [7\/151] Copying file pandas\/__init__.py\r\n      [8\/151] Generating pandas\/_libs\/hashtable_func_helper_pxi with a custom command\r\n      [9\/151] Generating pandas\/_libs\/index_class_helper_pxi with a custom command\r\n      [10\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/base.pyx\r\n      [11\/151] Compiling C object pandas\/_libs\/tslibs\/parsing.cpython-311-x86_64-linux-gnu.so.p\/.._src_parser_tokenizer.c.o\r\n      [12\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/ccalendar.pyx\r\n      [13\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/dtypes.pyx\r\n      [14\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/np_datetime.pyx\r\n      [15\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/nattype.pyx\r\n      [16\/151] Compiling C object pandas\/_libs\/tslibs\/base.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_base.pyx.c.o\r\n      [17\/151] Linking target pandas\/_libs\/tslibs\/base.cpython-311-x86_64-linux-gnu.so\r\n      [18\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/conversion.pyx\r\n      [19\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/fields.pyx\r\n      [20\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/strptime.pyx\r\n      [21\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/parsing.pyx\r\n      [22\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/offsets.pyx\r\n      [23\/151] Compiling C object pandas\/_libs\/tslibs\/ccalendar.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.o\r\n      [24\/151] Linking target pandas\/_libs\/tslibs\/ccalendar.cpython-311-x86_64-linux-gnu.so\r\n      [25\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/timestamps.pyx\r\n      [26\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/timedeltas.pyx\r\n      [27\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/timezones.pyx\r\n      [28\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/period.pyx\r\n      [29\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/tzconversion.pyx\r\n      [30\/151] Compiling C object pandas\/_libs\/tslibs\/np_datetime.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_np_datetime.pyx.c.o\r\n      [31\/151] Linking target pandas\/_libs\/tslibs\/np_datetime.cpython-311-x86_64-linux-gnu.so\r\n      [32\/151] Compiling C object pandas\/_libs\/tslibs\/dtypes.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.o\r\n      [33\/151] Linking target pandas\/_libs\/tslibs\/dtypes.cpython-311-x86_64-linux-gnu.so\r\n      [34\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/algos.pyx\r\n      FAILED: pandas\/_libs\/algos.cpython-311-x86_64-linux-gnu.so.p\/pandas\/_libs\/algos.pyx.c\r\n      cython -M --fast-fail -3 --include-dir \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/build\/pandas\/_libs '-X always_allow_keywords=true' \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/algos.pyx -o pandas\/_libs\/algos.cpython-311-x86_64-linux-gnu.so.p\/pandas\/_libs\/algos.pyx.c\r\n      \r\n      Error compiling Cython file:\r\n      ------------------------------------------------------------\r\n      ...\r\n          # each incremental i value can be looked up in the lexsort_indexer\r\n          # array that we sorted previously, which gives us the location of\r\n          # that sorted value for retrieval back from the original\r\n          # values \/ masked_vals arrays\r\n          # TODO(cython3): de-duplicate once cython supports conditional nogil\r\n          with gil(numeric_object_t is object):\r\n                 ^\r\n      ------------------------------------------------------------\r\n      \r\n      \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/algos.pyx:1147:12: Expected ':', found '('\r\n      [35\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/tslibs\/vectorized.pyx\r\n      [36\/151] Compiling C object pandas\/_libs\/tslibs\/nattype.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_nattype.pyx.c.o\r\n      [37\/151] Compiling Cython source \/tmp\/pip-wheel-cpma2d0r\/pandas_1745047ec6b24424b3bcdf2679ac0287\/pandas\/_libs\/arrays.pyx\r\n      [38\/151] Compiling C object pandas\/_libs\/tslibs\/conversion.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_conversion.pyx.c.o\r\n      pandas\/_libs\/tslibs\/conversion.cpython-311-x86_64-linux-gnu.so.p\/pandas\/_libs\/tslibs\/conversion.pyx.c: In function \u2018__pyx_pf_6pandas_5_libs_6tslibs_10conversion_cast_from_unit_vectorized.isra.48\u2019:\r\n      pandas\/_libs\/tslibs\/conversion.cpython-311-x86_64-linux-gnu.so.p\/pandas\/_libs\/tslibs\/conversion.pyx.c:2398:79: warning: \u2018__pyx_v_i\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n           __Pyx_GetItemInt_Fast(o, (Py_ssize_t)i, is_list, wraparound, boundscheck) :\\\r\n                                                                                     ^~\r\n           (is_list ? (PyErr_SetString(PyExc_IndexError, \"list index out of range\"), (PyObject*)NULL) :\\\r\n      \r\n      pandas\/_libs\/tslibs\/conversion.cpython-311-x86_64-linux-gnu.so.p\/pandas\/_libs\/tslibs\/conversion.pyx.c:3853:14: note: \u2018__pyx_v_i\u2019 was declared here\r\n         Py_ssize_t __pyx_v_i;\r\n                    ^~~~~~~~~\r\n      [39\/151] Compiling C object pandas\/_libs\/tslibs\/timezones.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_timezones.pyx.c.o\r\n      [40\/151] Compiling C object pandas\/_libs\/tslibs\/fields.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_fields.pyx.c.o\r\n      [41\/151] Compiling C object pandas\/_libs\/tslibs\/parsing.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_parsing.pyx.c.o\r\n      [42\/151] Compiling C object pandas\/_libs\/tslibs\/tzconversion.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_tzconversion.pyx.c.o\r\n      [43\/151] Compiling C object pandas\/_libs\/tslibs\/strptime.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_strptime.pyx.c.o\r\n      [44\/151] Compiling C object pandas\/_libs\/tslibs\/period.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_period.pyx.c.o\r\n      [45\/151] Compiling C object pandas\/_libs\/tslibs\/timestamps.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_timestamps.pyx.c.o\r\n      [46\/151] Compiling C object pandas\/_libs\/tslibs\/timedeltas.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_timedeltas.pyx.c.o\r\n      [47\/151] Compiling C object pandas\/_libs\/tslibs\/offsets.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_pandas__libs_tslibs_offsets.pyx.c.o\r\n      ninja: build stopped: subcommand failed.\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\nmake[3]: *** [..\/..\/mk\/spksrc.wheel.mk:203: cross-compile-wheel-pandas] Error 1\r\nmake[3]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake[2]: *** [..\/..\/mk\/spksrc.wheel.mk:144: build_wheel_target] Error 1\r\nmake[2]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake[1]: *** [..\/..\/mk\/spksrc.spk.mk:719: build-arch-x64-7.1] Error 1\r\nmake[1]: Leaving directory '\/home\/spksrc\/py311-update\/spksrc\/spk\/homeassistant'\r\nmake: *** [..\/..\/mk\/spksrc.spk.mk:713: arch-x64-7.1] Error 2\r\n```","You need Cython 3 to be able to compile pandas. It looks like you have Cython 0.29.37."],"labels":["Build","Needs Info"]},{"title":"BUG: read_csv() raises IndexError when dict supplied as dtype uses integer keys of same sign","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport numpy as np\r\nfrom pathlib import Path\r\n\r\n# Create a simple df with a RangeIndex for the columns. N.B. The RangeIndex\r\n# start and stop have the same sign.\r\nrange_idx = pd.RangeIndex(start=-10, stop=-5, step=1)\r\ndf = pd.DataFrame(np.random.randn(3, len(range_idx)), columns=range_idx)\r\n\r\ncsv_file = Path(\".\/data\/test_range_index.csv\")\r\ncsv_file.parent.mkdir(exist_ok=True)\r\n\r\n# Write df to a CSV along with row+col indices\r\ndf.to_csv(\r\n    csv_file,\r\n    header=True,\r\n    index=True,\r\n)\r\n\r\n# Create a dictionary to specify the dtypes\r\ndtype_spec  = {}\r\nfor v in range_idx:\r\n    dtype_spec[v] = \"float64\"\r\n    # dtype_spec[f\"{v}\"] = \"float64\"   # * Alternatively, convert to str, which works\r\n\r\n# Reload the CSV\r\ndf_reload = pd.read_csv(\r\n    csv_file,\r\n    header=0,\r\n    index_col=0,\r\n    float_precision='round_trip',\r\n    dtype=dtype_spec,\r\n)\n```\n\n\n### Issue Description\n\nThis may in part be my misunderstanding of how dtypes should be specified and the implicit conversion of the column index to str.\r\n\r\nNotwithstanding that, the problem is when reading a CSV using `read_csv()` with a header row that consists of integers of the same sign and specifying the dtype with a dictionary that has integer keys: it fails with an `IndexError: list index out of range`.\r\n\r\n### Traceback\r\n\r\n<details>\r\n\r\n```python-traceback\r\nIndexError                                Traceback (most recent call last)\r\nCell In[106], [line 1](vscode-notebook-cell:?execution_count=106&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=106&line=1) df_reload = pd.read_csv(\r\n      [2](vscode-notebook-cell:?execution_count=106&line=2)     csv_file,\r\n      [3](vscode-notebook-cell:?execution_count=106&line=3)     header=0,\r\n      [4](vscode-notebook-cell:?execution_count=106&line=4)     index_col=0,\r\n      [5](vscode-notebook-cell:?execution_count=106&line=5)     float_precision='round_trip',\r\n      [6](vscode-notebook-cell:?execution_count=106&line=6)     dtype=dtype_spec,\r\n      [7](vscode-notebook-cell:?execution_count=106&line=7) )\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1026](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1026), in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\r\n   [1013](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1013) kwds_defaults = _refine_defaults_read(\r\n   [1014](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1014)     dialect,\r\n   [1015](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1015)     delimiter,\r\n   (...)\r\n   [1022](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1022)     dtype_backend=dtype_backend,\r\n   [1023](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1023) )\r\n   [1024](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1024) kwds.update(kwds_defaults)\r\n-> [1026](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1026) return _read(filepath_or_buffer, kwds)\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:626](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:626), in _read(filepath_or_buffer, kwds)\r\n    [623](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:623)     return parser\r\n    [625](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:625) with parser:\r\n--> [626](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:626)     return parser.read(nrows)\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1923](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1923), in TextFileReader.read(self, nrows)\r\n   [1916](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1916) nrows = validate_integer(\"nrows\", nrows)\r\n   [1917](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1917) try:\r\n   [1918](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1918)     # error: \"ParserBase\" has no attribute \"read\"\r\n   [1919](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1919)     (\r\n   [1920](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1920)         index,\r\n   [1921](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1921)         columns,\r\n   [1922](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1922)         col_dict,\r\n-> [1923](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1923)     ) = self._engine.read(  # type: ignore[attr-defined]\r\n   [1924](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1924)         nrows\r\n   [1925](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1925)     )\r\n   [1926](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1926) except Exception:\r\n   [1927](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py:1927)     self.close()\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:333](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:333), in CParserWrapper.read(self, nrows)\r\n    [330](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:330)     data = {k: v for k, (i, v) in zip(names, data_tups)}\r\n    [332](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:332)     names, date_data = self._do_date_conversions(names, data)\r\n--> [333](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:333)     index, column_names = self._make_index(date_data, alldata, names)\r\n    [335](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:335) return index, column_names, date_data\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:372](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:372), in ParserBase._make_index(self, data, alldata, columns, indexnamerow)\r\n    [370](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:370) elif not self._has_complex_date_col:\r\n    [371](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:371)     simple_index = self._get_simple_index(alldata, columns)\r\n--> [372](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:372)     index = self._agg_index(simple_index)\r\n    [373](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:373) elif self._has_complex_date_col:\r\n    [374](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:374)     if not self._name_processed:\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:489](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:489), in ParserBase._agg_index(self, index, try_parse_dates)\r\n    [484](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:484)     if col_name is not None:\r\n    [485](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:485)         col_na_values, col_na_fvalues = _get_na_values(\r\n    [486](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:486)             col_name, self.na_values, self.na_fvalues, self.keep_default_na\r\n    [487](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:487)         )\r\n--> [489](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:489) clean_dtypes = self._clean_mapping(self.dtype)\r\n    [491](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:491) cast_type = None\r\n    [492](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:492) index_converter = False\r\n\r\nFile [~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:455](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:455), in ParserBase._clean_mapping(self, mapping)\r\n    [453](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:453) for col, v in mapping.items():\r\n    [454](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:454)     if isinstance(col, int) and col not in self.orig_names:\r\n--> [455](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:455)         col = self.orig_names[col]\r\n    [456](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:456)     clean[col] = v\r\n    [457](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/peterma\/exio_dev\/repo\/df_file_interchange\/pandas_github_issues\/~\/miniconda3\/envs\/exio-datamanager\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/base_parser.py:457) if isinstance(mapping, defaultdict):\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n### Further discussion\r\n\r\nThe `IndexError` does not get raised if the original `RangeIndex` has a start and stop of different signs.\r\n\r\nNeither does it occur if the `dtype=` is not specified in `read_csv()`'s arguments or if the keys in the dictionary of the dtypes are converted to str.\r\n\r\nIn all successful cases, the columns are an `Index` that contain the str representation of the integers. Does this mean they should always be integers for the purposes of `read_csv()` and that any conversion back to integer has to happen later?  If so, the docs might benefit from being a little bit clearer.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Expected Behavior\n\nThat `read_csv()` would succeed when specifying the dtypes with a dict with integer keys (of the same sign).\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.8.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-101-generic\r\nVersion               : #111~20.04.1-Ubuntu SMP Mon Mar 11 15:44:43 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0\r\nsetuptools            : 69.2.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.1.1\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.22.2\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : 2024.2.0\r\nfsspec                : 2024.3.1\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"PERF: DataFrame(dict) returns RangeIndex columns when possible","body":"Discovered in https:\/\/github.com\/pandas-dev\/pandas\/pull\/57441\r\n\r\nAlso removed a seemingly unused `_sort_msg`","comments":[],"labels":["Performance","Index","Constructors"]},{"title":"BUG: index duplicates keys with non ascii chars","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\nWhen creating a dataframe with an index containing non-ascii chars, pandas is merging different keys into a single key.\r\n\r\n```python\r\nimport pandas as pd\r\ncar = \"\u00e9\".encode(\"latin1\").decode('utf8', 'surrogateescape')\r\ndata = [(1,\"a-\"+car, \"x-\"+car), (2, \"b-\"+car, \"y-\"+car)]\r\ndf = pd.DataFrame(data, columns=[\"c1\", \"c2\", \"c3\"]).set_index([\"c2\", \"c3\"]).reset_index()\r\nprint(list(df[\"c3\"]))\r\n```\r\n\r\nreturns the same two keys:\r\n\r\n```\r\n['x-\\udce9', 'x-\\udce9']\r\n```\r\n\r\nExpected behavior:\r\n\r\n```\r\n['x-\\udce9', 'y-\\udce9']\r\n```\r\n\r\nNote that when using ascii chars, the behavior is correct:\r\n\r\n```\r\nimport pandas as pd\r\ncar = \"0\"\r\ndata = [(1,\"a-\"+car, \"x-\"+car), (2, \"b-\"+car, \"y-\"+car)]\r\ndf = pd.DataFrame(data, columns=[\"c1\", \"c2\", \"c3\"]).set_index([\"c2\", \"c3\"]).reset_index()\r\nprint(list(df[\"c3\"]))\r\n```\r\n\r\nreturns two different keys:\r\n\r\n```\r\n['x-0', 'y-0']\r\n```\r\n\r\nNote that the behavior is correct with non-ascii char and using a single column in the index:\r\n\r\n```\r\ncar = \"\u00e9\".encode(\"latin1\").decode('utf8', 'surrogateescape')\r\ndata = [(1,\"a-\"+car, \"x-\"+car), (2, \"b-\"+car, \"y-\"+car)]\r\ndf = pd.DataFrame(data, columns=[\"c1\", \"c2\", \"c3\"]).set_index([\"c3\"]).reset_index()\r\nprint(list(df[\"c3\"]))\r\n```\r\n\r\nreturns two different keys:\r\n\r\n```\r\n['x-\\udce9', 'y-\\udce9']\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nCreating a multi-index with non-ascii characters will not keep unique indices. Instead, keys are merged.\r\n\r\n### Expected Behavior\r\n\r\nCreating a multi-index with non-ascii characters should keep unique keys.\r\n\r\n### Installed Versions\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 3.10.105\r\nVersion               : #25556 SMP Sat Aug 28 02:13:34 CST 2021\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : None\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.22.2\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n```","comments":["Interesting. This seems to happen due to the malformed unicode characters, somewhere in the MultIndex calls a search for duplicates (in e.g., `['x-\\udce9', 'y-\\udce9']`) in the index happens via a cython hash map, this duplicate check does not happen for a single level index.\r\n\r\nIn the end this filters out a false positive duplicate, leading to the `['x-\\udce9', 'x-\\udce9']` index values in your example.\r\n\r\nI don't think changing the hashmap implementation is a good idea, but one could at least throw an exception if the user wants to put malformed unicode into a MultiIndex."],"labels":["Bug","Needs Triage"]},{"title":"Remove versioneer \/ setuptools","body":"trying to follow the lead set by https:\/\/github.com\/numpy\/numpy\/pull\/24196\r\n\r\n@lithomas1","comments":[],"labels":["Build"]},{"title":"ENH:  Rename internal `DataFrame._append()` ","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [X] Removing existing functionality in pandas\n\n\n### Problem Description\n\nThis comment on the python discussion is interesting:\r\nhttps:\/\/discuss.python.org\/t\/name-suggestions-for-attributeerrors-and-possibly-nameerrors-should-not-include-names-with-single-leading-underscores\/48588\r\n\r\nExample code (from main):\r\n```python\r\n>>> pd.DataFrame.append()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: type object 'DataFrame' has no attribute 'append'. Did you mean: '_append'?\r\n```\r\n\r\nWhat is happening is that the python interpreter is suggesting people use `DataFrame._append()` since we removed `append()`\r\n\r\n\n\n### Feature Description\n\nI think if we renamed the internal `_append()` to something like `_internal_append_do_not_use()` then maybe the interpreter wouldn't do that suggestion.\r\n\n\n### Alternative Solutions\n\nBring back `DataFrame.append()` ???\r\n\n\n### Additional Context\n\n_No response_","comments":["Is the proposal to do this with all internal methods that are similar to removed external names? I'm pretty opposed here - I think we should be free to chose internal names however we see fit.","> Is the proposal to do this with all internal methods that are similar to removed external names? I'm pretty opposed here - I think we should be free to chose internal names however we see fit.\r\n\r\nNo, but in this case, since we deprecated `DataFrame.append()`, we don't want people using `DataFrame._append()` as a workaround so we need to hide it better.\r\n","~~I'd me somewhat +1 - see this SO answer  -https:\/\/stackoverflow.com\/a\/76449334\/2214597\r\nwhile it does have a disclaimer, it's still pretty heavily upvoted.~~\r\n\r\nNvm - this SO question is already mentioned in the Discourse thread","I would definitely wait until https:\/\/github.com\/python\/cpython\/issues\/116871 has a resolution upstream before preemptively acting here.  "],"labels":["Enhancement","API Design","Needs Discussion"]},{"title":"ENH: Add split method to DataFrame for flexible row-based partitioning","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nCurrently, pandas does not provide a direct, built-in method to split a DataFrame into multiple smaller DataFrames based on a number of specified rows. Users seeking to partition a DataFrame into chunks either have to relay on workarounds using loops and manual indexing. Adding such a feature can save users time in scenarios requiring a lot of processing of data into smaller segments, such as batch processing, cross-validation in machine learning, or dividing data for parallel processing.\n\n### Feature Description\n\nThe .split() method that I am proposing for pandas DataFrames would allow users to divide a DataFrame into smaller DataFrames based on a specified number of rows, with flexible handling of any remainder rows. The method can be described in detail as follows:\r\n\r\nDataFrame.split(n, remainder='first' or 'last' or None)\r\n\r\n**Parameters:**\r\n\r\n1. n (integer) Would represent number of rows each resulting dataframe should contain\r\n2. remainder (string, optional): Specifies how to handle remainder rows that do not fit evenly into the split. It accepts the following values:\r\n       2.1. 'first': Include the remainder rows in the first split DataFrame.\r\n       2.2. 'last': Include the remainder rows in the last split DataFrame.\r\n       2.3. 'None': If the DataFrame cannot be evenly split, raise an error. This is the default behavior.\r\n       \r\n**Pseudocode Description:**\r\n\r\ndef split_dataframe(df, n, remainder=None):\r\n    1.) Get total number of rows in the dataframe\r\n    2.) Check for divisibility of remainder is None:\r\n          2.1) If length is not perfectly divisible by n, raise an error.\r\n    3.) If the remainder is 'first':\r\n          3.1) Calculate the number of rows for the first split to include the remainder.\r\n          3.2) Split the DataFrame accordingly, adjusting subsequent splits to have n rows.\r\n     4.) If the remainder is 'last':\r\n           4.1) Split the DataFrame into partitions of n rows, except for the last partition\r\n           4.2) Calculate and append the last partition separately to include any remainder.\r\n     5.) Else, if the dataframe is perfectly divisible ( or if the remainder is None and the divisibility check has passed):\r\n           5.1.) Split the DataFrame into equal parts of n rows.\r\n     6) Return the list of split DataFrames.\r\n     \r\n     \r\nExample of usage:\r\n\r\nSay that we have a DataFrame consisting of 100 rows. Then we could split this DataFrame into a list of 10 dataframes:\r\n\r\nsplit_dfs = df.split(10)\r\n\r\nIn another case, we have a dataframe consisting of 99 rows. Then we could split the dataframe into 10 dataframes, where the first DataFrame consists of 9 rows and the rest of the DataFrames have 10 rows.\r\n\r\nsplit_dfs = df.split(9, remainder='first')\r\n\r\nIn the other case, we could split the DataFrame into a list of 10 DataFrames, where the last DataFrame consists of the remaining 9 rows.\n\n### Alternative Solutions\n\nOne approach could involve splitting a DataFrame into smaller chunks using `numpy.array_split`. This function can divide an array or DataFrame into a specified number of parts and handle any remainders by evenly distributing them across splits. However there are limitations to this approach:\r\n\r\n1.) The result is a list of numpy arrays, not DataFrames. So you lose the DataFrame context and metadata (like column names).\r\n2.) It requires an additional step to convert these arrays back into DataFrames, adding more complexity.\r\n\r\n\r\nWe could also do manual looping and slicing. The problem is that it requires boilerplate code, which is error prone and inefficient especially when working with larger groups of people. This approach also lacks the simplicity and ease of use that a built-in method would provide.\r\n\r\nThere are also third-party libraries and packages for splitting DataFrames, such as dask.DataFrame or more_itertools.chunked. Though dask.dataframe allows for processing data into chunks, you still would have too many modifications to implement the functionality that I've described for df.split. more_itertools.chunked can be used to split an iterable into smaller iterables of a specified size, which can be applied to DataFrames. The problem is you need to convert chunks back into DataFrames. So it would be much simpler to have a built in df.split method.\n\n### Additional Context\n\nPractical use Cases: I've encountered several scenarios in data preprocessing for machine learning where batch processing of DataFrame chunks was necessary. Implementing a custom solution each time has not been ideal. I've also run into several situations where I've needed to split DataFrame into individual DataFrames for subsequent calculations on the partitioned data.","comments":["I would like to contribute to the implementation of this feature, pending approval of the feature request.\r\n\r\nIn addition to the initial proposal, I believe enhancing the function to include an option to split along either axis would significantly increase its utility.\r\n\r\nDataFrame.split(n, axis=1, remainder='first', or 'last' or None) where axis=0 (split along rows) is the default.","Thanks for the request - it seems to me pandas provides a number of ways to split up existing DataFrames that can be utilized, I don't see this feature provided any more utility that isn't already possible. For example, if `chunker` is any iterable (e.g. `[0, 0, 0, 1, 1, 1]`), then one can do:\r\n\r\n    [chunk for _, chunk in df.groupby(chunker)]\r\n\r\nIn addition, since pandas does not support multiple processes or distributed computing, it seems to me most common operations would benefit from not splitting up a DataFrame.","Thank you for the feedback, especially regarding your suggestion on using df.groupby(chunker) as a solution.\r\n\r\nI want to clarify the intent behind proposing this method for splitting DataFrames into smaller chunks.\r\n\r\nThe primary motivation is to enhance pandas with additional flexibility for handling large datasets, which would include scenarios such as:\r\n\r\n1.) stepwise processing or algorithms requiring partitioned input, where splitting a DataFrame into smaller segments is necessary.\r\n2.) Chunk-wise data export\/import, simplifying the management of large datasets.\r\n3.) Memory constraints, where processing an entire dataset at once is not feasible.\r\n4.) External parallelization, where despite pandas not supporting distributed computing, processing smaller, independent chunks in parallel using external tools can significantly speed up operations.\r\n\r\nThe suggested method of using df.groupby(chunker) is a great solution for segmenting a DataFrame based on predefined criteria but differs from a bit from what I was suggesting:\r\n\r\n1.) Uniformity and simplicity:  The df.split() method aims to uniformly split a DataFrame into chunks based on a specified size, offering a straightforward solution when division based on size rather than data-driven groupings is important. This especially useful for tasks requiring equal-sized partions and for users seeking a simple, direct way to divide their dataframe.\r\n2.) Ease of use: For those aiming to divide DataFrame into smaller parts without complex grouping criteria, the 'df.split()' method simplifies the process by requiring only the desired chunk size as input and optionally how to handle remainders.\r\n\r\nI am thinking of something roughly along the lines of this:\r\n\r\n\r\n\r\n```Python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndef split_dataframe(df, n, remainder=None):\r\n    # Get number of rows in DataFrame\r\n    total_rows = len(df)\r\n    \r\n    # Initialize list to hold the split DataFrames\r\n    dfs = []\r\n    \r\n    # Calculate the number of DataFrames\r\n    num_dfs = total_rows \/\/ n\r\n    extra_rows = total_rows % n\r\n    \r\n    if remainder == 'first' and extra_rows > 0:\r\n        # Size of first chunk is equal to remainder. Remaining chunks are same size.\r\n        dfs.append(df.iloc[:n + extra_rows])\r\n        start_idx = n + extra_rows\r\n        for _ in range(1, num_dfs):\r\n            dfs.append(df.iloc[start_idx:start_idx + n])\r\n            start_idx += n\r\n    elif remainder == 'last' and extra_rows > 0:\r\n        # Last chunk consists of remaining rows. The other chunks are the same size.\r\n        for i in range(num_dfs):\r\n            dfs.append(df.iloc[i*n:(i+1)*n])\r\n        # Add extra rows to the last chunk\r\n        dfs.append(df.iloc[num_dfs*n:])\r\n    elif remainder == 'spread':\r\n        # Evenly spread extra rows across the first few chunks\r\n        for i in range(num_dfs + (1 if extra_rows > 0 else 0)):\r\n            size = n + (1 if i < extra_rows else 0)\r\n            dfs.append(df.iloc[i*size:min((i+1)*size, total_rows)])\r\n    else:\r\n        if extra_rows > 0:\r\n            raise ValueError(f\"DataFrame length is not perfectly divisible by {n}. Please specify the 'remainder' parameter.\")\r\n        # If remainder is None and the DataFrame is perfectly divisible\r\n        for i in range(num_dfs):\r\n            dfs.append(df.iloc[i*n:(i+1)*n])\r\n    \r\n    return dfs\r\n\r\n\r\n# Example usage\r\ndf1 = pd.DataFrame(np.arange(100), columns=['Column'])\r\ndf2 = pd.DataFrame(np.arange(99), columns=['Column'])\r\ndfs_equal = split_dataframe(df1, 10)  # Uses the default 'equal' distribution\r\ndfs_first = split_dataframe(df2, 10, remainder='first')\r\ndfs_last = split_dataframe(df2, 10, remainder='last')\r\ndfs_spread = split_dataframe(df2, 10, remainder='spread')\r\n```\r\n\r\n"],"labels":["Enhancement","Indexing","Needs Discussion"]},{"title":"RFE: move away from deprecated `html5lib`","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [X] Removing existing functionality in pandas\n\n\n### Problem Description\n\n**Is your feature request related to a problem? Please describe.**\r\nIt would be nice tu cut tail of some legacy modules decencies.\r\nOne of those modules is `html5lib`.\r\n\r\n`pandas` only optionally depends on  `html5lib` but I think that ATM even that optional dependency can be removed like it has been done ~2 years ago wit pip https:\/\/github.com\/pypa\/pip\/pull\/11259\n\n### Feature Description\n\nCut tail of some legacy modules dependenies.\n\n### Alternative Solutions\n\nN\/A\n\n### Additional Context\n\n_No response_","comments":["Can you elaborate more?\r\n\r\nHas Python's stdlib ``html.parser`` caught up to ``html5lib``?\r\n\r\nLooking at bs4's docs, it looks like html5lib is more lenient than the stdlib\/lxml (which might be one reason people want to use it)\r\n\r\n<img width=\"822\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/47963215\/e48bb106-ef40-42a8-80a5-ec141e06befc\">"],"labels":["IO HTML","Deprecate","Needs Info","Dependencies"]},{"title":"BUG: pd.read_csv error when parse_dates used on index_col for dtype_backend=\"pyarrow\"","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport datetime\r\n\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [1], 'b': [datetime.datetime.now()]})\r\ndf.to_csv('test.csv', index=None)\r\n\r\n\r\na = pd.read_csv(\r\n    'test.csv',\r\n    parse_dates=['b'],\r\n    index_col=\"b\",\r\n    dtype_backend=\"pyarrow\",\r\n    engine=\"pyarrow\",\r\n)\r\n\r\npd.read_csv(\r\n    'test.csv',\r\n    parse_dates=['b'],\r\n    index_col=\"b\",\r\n)\r\n\r\n```\r\n\r\nthe following fails : \r\n```python\r\npd.read_csv(\r\n    'test.csv',\r\n    parse_dates=['b'],\r\n    index_col=\"b\",\r\n    dtype_backend=\"pyarrow\",\r\n)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nreturns \r\n`ValueError: not all elements from date_cols are numpy arrays`\r\n\r\n<details>\r\nValueError                                Traceback (most recent call last)\r\nCell In[78], [line 1](vscode-notebook-cell:?execution_count=78&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=78&line=1) pd.read_csv(\r\n      [2](vscode-notebook-cell:?execution_count=78&line=2)     'test.csv',\r\n      [3](vscode-notebook-cell:?execution_count=78&line=3)     parse_dates=['b'],\r\n      [4](vscode-notebook-cell:?execution_count=78&line=4)     index_col=\"b\",\r\n      [5](vscode-notebook-cell:?execution_count=78&line=5)     dtype_backend=\"pyarrow\",\r\n      [6](vscode-notebook-cell:?execution_count=78&line=6) )\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:948), in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\r\n    [935](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:935) kwds_defaults = _refine_defaults_read(\r\n    [936](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:936)     dialect,\r\n    [937](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:937)     delimiter,\r\n   (...)\r\n    [944](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:944)     dtype_backend=dtype_backend,\r\n    [945](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:945) )\r\n    [946](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:946) kwds.update(kwds_defaults)\r\n--> [948](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:948) return _read(filepath_or_buffer, kwds)\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:617), in _read(filepath_or_buffer, kwds)\r\n    [614](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:614)     return parser\r\n    [616](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:616) with parser:\r\n--> [617](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:617)     return parser.read(nrows)\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1748), in TextFileReader.read(self, nrows)\r\n   [1741](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1741) nrows = validate_integer(\"nrows\", nrows)\r\n   [1742](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1742) try:\r\n   [1743](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1743)     # error: \"ParserBase\" has no attribute \"read\"\r\n   [1744](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1744)     (\r\n   [1745](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1745)         index,\r\n   [1746](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1746)         columns,\r\n   [1747](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1747)         col_dict,\r\n-> [1748](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1748)     ) = self._engine.read(  # type: ignore[attr-defined]\r\n   [1749](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1749)         nrows\r\n   [1750](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1750)     )\r\n   [1751](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1751) except Exception:\r\n   [1752](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/readers.py:1752)     self.close()\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:333](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:333), in CParserWrapper.read(self, nrows)\r\n    [330](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:330)     data = {k: v for k, (i, v) in zip(names, data_tups)}\r\n    [332](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:332)     names, date_data = self._do_date_conversions(names, data)\r\n--> [333](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:333)     index, column_names = self._make_index(date_data, alldata, names)\r\n    [335](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py:335) return index, column_names, date_data\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:371](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:371), in ParserBase._make_index(self, data, alldata, columns, indexnamerow)\r\n    [369](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:369) elif not self._has_complex_date_col:\r\n    [370](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:370)     simple_index = self._get_simple_index(alldata, columns)\r\n--> [371](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:371)     index = self._agg_index(simple_index)\r\n    [372](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:372) elif self._has_complex_date_col:\r\n    [373](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:373)     if not self._name_processed:\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:468](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:468), in ParserBase._agg_index(self, index, try_parse_dates)\r\n    [466](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:466) for i, arr in enumerate(index):\r\n    [467](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:467)     if try_parse_dates and self._should_parse_dates(i):\r\n--> [468](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:468)         arr = self._date_conv(\r\n    [469](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:469)             arr,\r\n    [470](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:470)             col=self.index_names[i] if self.index_names is not None else None,\r\n    [471](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:471)         )\r\n    [473](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:473)     if self.na_filter:\r\n    [474](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:474)         col_na_values = self.na_values\r\n\r\nFile [c:\\ProgramData\\miniconda3\\envs\\DLL_Forecast\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:1142](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1142), in _make_date_converter.<locals>.converter(col, *date_cols)\r\n   [1139](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1139)     return date_cols[0]\r\n   [1141](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1141) if date_parser is lib.no_default:\r\n-> [1142](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1142)     strs = parsing.concat_date_cols(date_cols)\r\n   [1143](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1143)     date_fmt = (\r\n   [1144](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1144)         date_format.get(col) if isinstance(date_format, dict) else date_format\r\n   [1145](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1145)     )\r\n   [1147](file:\/\/\/C:\/ProgramData\/miniconda3\/envs\/DLL_Forecast\/Lib\/site-packages\/pandas\/io\/parsers\/base_parser.py:1147)     with warnings.catch_warnings():\r\n\r\nFile parsing.pyx:1155, in pandas._libs.tslibs.parsing.concat_date_cols()\r\nValueError: not all elements from date_cols are numpy arrays\r\n<\/details>\r\n\r\n### Expected Behavior\r\n\r\nAs with dtype_backend=\"numpy_nullable\" the index column should be parsed correctly.\r\n\r\nIt works with the following set-up, so likely an issue with the C parser (?)\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.8.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19042\r\nmachine             : AMD64\r\nprocessor           : AMD64 Family 23 Model 24 Stepping 1, AuthenticAMD\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : Dutch_Belgium.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.4\r\npytz                : 2024.1\r\ndateutil            : 2.9.0\r\nsetuptools          : 69.2.0\r\npip                 : 24.0\r\nCython              : None\r\npytest              : 8.1.1\r\nhypothesis          : None\r\nsphinx              : 7.2.6\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : 3.1.9\r\nlxml.etree          : 5.1.0\r\nhtml5lib            : 1.1\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.3\r\nIPython             : 8.22.2\r\npandas_datareader   : None\r\nbs4                 : 4.12.3\r\nbottleneck          : 1.3.8\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2024.3.0\r\ngcsfs               : None\r\nmatplotlib          : 3.8.3\r\nnumba               : 0.59.0\r\nnumexpr             : 2.9.0\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 15.0.0\r\npyreadstat          : None\r\npyxlsb              : 1.0.10\r\ns3fs                : None\r\nscipy               : 1.12.0\r\nsqlalchemy          : 2.0.28\r\ntables              : None\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : 2.0.1\r\nzstandard           : 0.22.0\r\ntzdata              : 2024.1\r\nqtpy                : 2.4.1\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Hm, this works on main for me.\r\n\r\nCan you try a newer version of pandas (e.g. 2.2.1)?","@lithomas1 I could reproduce this error on pandas version 2.3.0\r\n","@lithomas1 Do you need any help in other triaging other defects\/ bugs? ","My bad, forgot to run the second command.\r\n\r\nNice catch.\r\n\r\nLooks like ``parsing.concat_date_cols`` (which concatenates date_cols) is getting passed an ArrowExtensionArray.\r\n\r\nThat behavior is deprecated (to be removed in 3.0), so it should be easy to make this work for 3.0.\r\n","> Can you try a newer version of pandas (e.g. 2.2.1)?\r\n\r\nMy bad. I should have checked and not have assumed the kernel I was using used the latest version. Thanks for the work."],"labels":["Bug","IO CSV","Arrow"]},{"title":"ENH: Improve error message when specifying dtype=\"float32[pyarrow]\" while PyArrow is not installed","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nIf PyArrow is not installed properly, running the following code snippet from the [User Guide](https:\/\/pandas.pydata.org\/docs\/user_guide\/pyarrow.html):\r\n`ser = pd.Series([-1.5, 0.2, None], dtype=\"float32[pyarrow]\")`\r\nwill result in:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"...\/python3.12\/site-packages\/pandas\/core\/series.py\", line 493, in __init__\r\n    dtype = self._validate_dtype(dtype)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"...\/python3.12\/site-packages\/pandas\/core\/generic.py\", line 515, in _validate_dtype\r\n    dtype = pandas_dtype(dtype)\r\n            ^^^^^^^^^^^^^^^^^^^\r\n  File \"...\/python3.12\/site-packages\/pandas\/core\/dtypes\/common.py\", line 1624, in pandas_dtype\r\n    result = registry.find(dtype)\r\n             ^^^^^^^^^^^^^^^^^^^^\r\n  File \"...\/python3.12\/site-packages\/pandas\/core\/dtypes\/base.py\", line 576, in find\r\n    return dtype_type.construct_from_string(dtype)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"...\/python3.12\/site-packages\/pandas\/core\/dtypes\/dtypes.py\", line 2251, in construct_from_string\r\n    pa_dtype = pa.type_for_alias(base_type)\r\n               ^^\r\nNameError: name 'pa' is not defined\r\n```\r\nwhich is not very informative.\r\n\r\n### Feature Description\r\n\r\nWe can improve the error message by letting the user know that there is something wrong regarding the installation of PyArrow, especially when the user believes that he\/she has installed PyArrow, but actually installed it in a wrong location or installed an outdated version.\r\n\r\n### Alternative Solutions\r\n\r\nCatch the `NameError` and raise another `ImportError` from it describing what happened.\r\n\r\n### Additional Context\r\n\r\nThis conforms to the description in [Installation Guide](https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#optional-dependencies): If the optional dependency is not installed, pandas will raise an `ImportError` when the method requiring that dependency is called.","comments":["PRs for this would be greatly appreciated!"],"labels":["Enhancement","Error Reporting","Dependencies","Arrow"]},{"title":"DateOffset add to ndarray","body":"This is WIP to implement changes needed to allow DateOffset subclasses e.g. MonthEnd to be directly added to numpy.ndarray, e.g. `numpy.array(['2022-01-01'], dtype='datetime64') + pandas.offsets.MonthEnd(1)`\r\n\r\nI need some guidance on:\r\n\r\n- cython mechanism to check that passed ndarray is datetime64 dtype i.e.cython equivalent of pandas.api.types.is_datetime64_any_dtype\r\n- how to approach adding unit tests? two ideas I have\r\n  - somehow duplicate existing offset tests but run them on ndarray instead of Series?  looks on the surface to be more difficult to implement\r\n  - apply a set of offsets to a predefined Series and ndarray equivalent, and then check results match\r\n\r\nMany thanks!\r\n\r\n- [x] closes #47014\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.","comments":[],"labels":["Numeric Operations","Frequency"]},{"title":"BUG: Groupby median on timedelta column with NaT returns odd value","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\"label\": [\"foo\", \"foo\"], \"timedelta\": [pd.NaT, pd.Timedelta(\"1d\")]})\r\n\r\nprint(df.groupby(\"label\")[\"timedelta\"].median())\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen calculating the median of a timedelta column in a grouped DataFrame, which contains a `NaT` value, Pandas returns a strange value:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\"label\": [\"foo\", \"foo\"], \"timedelta\": [pd.NaT, pd.Timedelta(\"1d\")]})\r\n\r\nprint(df.groupby(\"label\")[\"timedelta\"].median())\r\n```\r\n```\r\nlabel\r\nfoo   -53376 days +12:06:21.572612096\r\nName: timedelta, dtype: timedelta64[ns]\r\n```\r\n\r\nIt looks to me like the same issue as described in #10040, but with groupby.\r\n\r\n### Expected Behavior\r\n\r\nIf you calculate the median directly on the timedelta column, without the groupby, the output is as expected:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\"label\": [\"foo\", \"foo\"], \"timedelta\": [pd.NaT, pd.Timedelta(\"1d\")]})\r\n\r\nprint(df[\"timedelta\"].median())\r\n```\r\n```\r\n1 days 00:00:00\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.2.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-26-generic\r\nVersion               : #26-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar  5 21:19:28 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.0\r\nnumba                 : None\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, confirmed on main. Further investigations and PRs to fix are welcome!"],"labels":["Bug","Groupby","Missing-data","Reduction Operations","datetime.date"]},{"title":"BUG: `NotImplementedError |S8` when converting categories from object containing byte arrays to bytes","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n# Imagine we receive byte array like this but much larger (e.g. 10M rows of 128 bit uuids in bytes)\r\n# Unfortunately not already converted to bytes dtype for some reason\r\na = pd.Series([b\"fdhijklm\", b\"fdhijklm\", b\"fdaijklm\"], dtype=object)\r\n\r\n# We want to first convert to categorical for efficiency reasons (it is faster to first detect unique values and then convert them)\r\nb = a.astype(\"category\")\r\n\r\n# Then we would like to convert the categories to proper numpy byte array for memory reasons\r\nnew_cats = b.cat.categories.astype(bytes)  # <-- NotImplementedError: |S8\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nSee comments in the example\r\n\r\n### Expected Behavior\r\n\r\nNo error: new_cats would be created. Hopefully that would allow to convert the previous categories into the new ones, so that the categories take less space in memory.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.13.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : AMD64 Family 25 Model 68 Stepping 1, AuthenticAMD\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : fr_FR.cp1252\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.1.0\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 6.1.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : 2024.2.0\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: Inconsistent implicit conversion of 1-arrays inside data frames","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(\r\n    [None, None], dtype=object\r\n)\r\ndf.at[0, 0] = np.arange(2)\r\ndf.at[1, 0] = np.arange(1)\r\n\r\nprint(df)\r\n#         0\r\n# 0  [0, 1]\r\n# 1       0\n```\n\n\n### Issue Description\n\nThere's an inconsistent handling of 1D arrays in pandas: arrays of length 1 are converted to scalars while arrays of other shapes aren't. To make things worse, pandas can be tricked into holding 1-arrays using: \r\n\r\n```python\r\ndf.applymap(lambda x: x.reshape(-1))\r\n\r\nprint(df)\r\n#         0\r\n# 0  [0, 1]\r\n# 1     [0]\r\n```\r\n\r\nThe cause of this error in the code is in the file `core\/internals\/base.py` (which doesn't exist in the `main` branch):\r\n```\r\n    def setitem_inplace(self, indexer, value, warn: bool = True) -> None: \r\n       # ....\r\n        if isinstance(value, np.ndarray) and value.ndim == 1 and len(value) == 1:\r\n            # NumPy 1.25 deprecation: https:\/\/github.com\/numpy\/numpy\/pull\/10615\r\n            value = value[0, ...]\r\n\r\n```\r\n\r\nThis comment does appear in a few places in the `master` branch so it should happen there as well.\r\nA possible fix is to check the case when the `dtype` of the block is `object`. It is logical that in such cases, implicit conversions should be avoided.\n\n### Expected Behavior\n\nSetting the values should be consistent.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-75-generic\r\nVersion               : #82~20.04.1-Ubuntu SMP Wed Jun 7 19:37:37 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.1\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : 2023.12.2post1\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.25\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: NamedTuples do no match tuples in pandas.Index","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n$ ipython\r\nPython 3.11.8 (main, Mar 19 2024, 17:46:15) [GCC 11.4.0]\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import pandas\r\n\r\nIn [2]: import collections\r\n\r\nIn [3]: MyNamedTuple = collections.namedtuple(\"MyNamedTuple\", \"id sub_id\")\r\n\r\nIn [4]: first = MyNamedTuple('identity','1234')\r\n\r\nIn [5]: idx = pandas.Index([('identity','1234')])\r\n\r\nIn [6]: idx\r\nOut[6]: \r\nMultiIndex([('identity', '1234')],\r\n           )\r\n\r\nIn [7]: idx2 = idx.to_flat_index()\r\n\r\nIn [8]: idx2\r\nOut[8]: Index([('identity', '1234')], dtype='object')\r\n\r\nIn [9]: first in idx\r\nOut[9]: True\r\n\r\nIn [10]: first in idx2\r\nOut[10]: False\r\n\r\nIn [11]: first in idx2.to_list()\r\nOut[11]: True\r\n\r\nIn [12]: first == idx2[0]\r\nOut[12]: True\r\n\r\nIn [13]: pandas.__version__\r\nOut[13]: '2.2.1'\r\n\r\nIn [14]: idx.get_loc(first)\r\nOut[14]: 0\r\n\r\nIn [15]: idx2.get_loc(first)\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nFile ~\/.pyenv\/versions\/3.11.8\/envs\/test-venv\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/base.py:3805, in Index.get_loc(self, key)\r\n   3804 try:\r\n-> 3805     return self._engine.get_loc(casted_key)\r\n   3806 except KeyError as err:\r\n\r\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\r\n\r\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\r\n\r\nFile pandas\/_libs\/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\nFile pandas\/_libs\/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\nKeyError: MyNamedTuple(id='identity', sub_id='1234')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[15], line 1\r\n----> 1 idx2.get_loc(first)\r\n\r\nFile ~\/.pyenv\/versions\/3.11.8\/envs\/test-venv\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/base.py:3812, in Index.get_loc(self, key)\r\n   3807     if isinstance(casted_key, slice) or (\r\n   3808         isinstance(casted_key, abc.Iterable)\r\n   3809         and any(isinstance(x, slice) for x in casted_key)\r\n   3810     ):\r\n   3811         raise InvalidIndexError(key)\r\n-> 3812     raise KeyError(key) from err\r\n   3813 except TypeError:\r\n   3814     # If we have a listlike key, _check_indexing_error will raise\r\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\r\n   3816     #  the TypeError.\r\n   3817     self._check_indexing_error(key)\r\n\r\nKeyError: MyNamedTuple(id='identity', sub_id='1234')\r\n\r\nIn [16]:\n```\n\n\n### Issue Description\n\nUpgraded from pandas 1.2.5 to pandas 1.3.5 and noticed that I was unable to reference columns in a dataframe with column labels that were tuples via a NamedTuple, i.e. KeyError. Grabbed the latest pandas and reduced the issue down to pandas.Index.get_loc - though it works in the case where I leave the Index as a MultiIndex.\r\n\r\nNote: I have seen the code work in about 25% of cases, so if you see it succeed please try again\n\n### Expected Behavior\n\nNamedTuples should match regular tuples as they do elsewhere in python (as illustrated by the fact that they match when one does `idx.to_list()`)\n\n### Installed Versions\n\n<details>\r\n\r\nIn [16]: pandas.show_versions()\r\n\/home\/russellm\/.pyenv\/versions\/3.11.8\/envs\/test-venv\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.8.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-21-generic\r\nVersion               : #21~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Feb  9 13:32:52 UTC 2\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : 65.5.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.22.2\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\nIn [17]: \r\n\r\n<\/details>\r\n","comments":["FWIW\r\n\r\n```\r\nIn [20]: idx2._engine\r\nOut[20]: <pandas._libs.index.ObjectEngine at 0x7f071ddcab00>\r\n\r\nIn [21]: idx2._engine.values\r\nOut[21]: array([('identity', '1234')], dtype=object)\r\n\r\nIn [22]: first in idx2._engine.values\r\nOut[22]: False\r\n\r\nIn [23]: first == idx2._engine.values[0]\r\nOut[23]: True\r\n\r\nIn [24]: hash(first)\r\nOut[24]: 5766037510587733218\r\n\r\nIn [25]: hash(idx2._engine.values[0])\r\nOut[25]: 5766037510587733218\r\n\r\nIn [26]: \r\n\r\n```\r\n","Happens on main as well, including the non deterministic success in a minority of attempts.","Thanks for the report. In general you will find very little support for containers as elements of an index or columns. \r\n\r\nRelated: https:\/\/github.com\/pandas-dev\/pandas\/pull\/57004#issuecomment-1906984802\r\n\r\n"],"labels":["Bug","Indexing"]},{"title":"CI: Make pep8 validation work with multiple files simultaneously","body":"The validation is bottlenecked due to the repeated sys calls, subprocess spawning, and pep8 calls that only validate a single file each. Running the validation on a list of files is approximately as fast as running it on a single file. This PR makes the first step, enabling the pep8 validation of a list of files, but for now still running it only on a single file. It will be followed by a few more PRs. The end result will be a complete refactoring of `validate_docstrings.py`, that will then take about 10 seconds to validate all docstrings.\r\n\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n\r\n@datapythonista ","comments":["I'm travelling and I could only revew with the phone, but looks good.\r\n\r\nQuick question, would it make sense to use ruff instead of flake8 for this?","Thanks for the quick review!\r\n\r\n> Quick question, would it make sense to use ruff instead of flake8 for this?\r\n\r\nI think it would be slightly faster, but someone would have to configure ruff to behave the same way as we intend flake8 to behave here. On my machine, the whole `code_check.sh docstrings` validation finishes in about 10 seconds, 7s of those are separate calls to `numpydoc.validate` (I put in a PR there that brings this down to 5.5s), and the remaining 3s are split between our `validate.py`and pep8. \r\n\r\nSo I think even with ruff we would at best get to 5.5s instead of 8.5s if we keep `numpydoc.validate`. Might still be worth an issue to discuss the (future) role of ruff here and in other places.\r\n\r\n","Yep, trying to improve 10s doesn't make sense. I know we are using ruff in the CI nowz bit I didn't check the exact state. The question was more about consistency, I didn't check if we already replaced flake8 with ruff.","I see ruff being used as part of the pre-commit hooks. And there it let a docstring error that I introduced in a non-replaced docstring through. It's also replaced flake8 for the docstring formatting, but back when I introduced my error, only the ``code_check.sh`` showed up as failed."],"labels":["Docs","CI"]},{"title":"BUG: regression, is_unique is incorrect since pandas 2.1.0","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nprint(\"pandas version\", pd.__version__)\r\n\r\nvalues = [1, 1, 2, 3, 4]\r\nindex = pd.Index(values)\r\n\r\nprint(\"===========\")\r\nprint(index)\r\nprint(\"is_unique=\", index.is_unique)\r\n\r\nfiltered_index = index[2:].copy()\r\n\r\nprint(\"===========\")\r\nprint(filtered_index)\r\nprint(\"is_unique=\", filtered_index.is_unique)\r\n\r\n\r\nindex = pd.Index(values)\r\nfiltered_index = index[2:].copy()\r\n\r\nprint(\"===========\")\r\nprint(filtered_index)\r\nprint(\"is_unique=\", filtered_index.is_unique)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nHello, \r\n\r\nWe found a regression, `index.is_unique` is incorrect since pandas 2.1.0.\r\n\r\nI looked for open issues but did not find any fix or existing discussion.\r\nHaving a look at the changelog, there were lots of changes in 2.1.0 to introduce copy-on-write optimizations on the index. \r\nI think the issue could be related to that, my best guess, maybe `index[2:]` cached something from the original `index` that is no longer correct?\r\n\r\nAttaching a simple repro, it's very easy to reproduce. :)\r\n\r\nThank you.\r\n\r\n### Expected Behavior\r\n\r\n```\r\npandas version 1.5.3\r\n===========\r\nInt64Index([1, 1, 2, 3, 4], dtype='int64')\r\nis_unique= False\r\n===========\r\nInt64Index([2, 3, 4], dtype='int64')\r\nis_unique= True\r\n===========\r\nInt64Index([2, 3, 4], dtype='int64')\r\nis_unique= True\r\n```\r\n\r\n```\r\npandas version 2.2.1\r\n===========\r\nIndex([1, 1, 2, 3, 4], dtype='int64')\r\nis_unique= False\r\n===========\r\nIndex([2, 3, 4], dtype='int64')\r\nis_unique= False    # <---------------- INCORRECT\r\n===========\r\nIndex([2, 3, 4], dtype='int64')\r\nis_unique= True\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\ntested on:\r\n\r\n* pandas 1.5.3: PASS\r\n* pandas 2.0.0: PASS\r\n* pandas 2.0.3: PASS\r\n* pandas 2.1.0: INCORRECT\r\n* pandas 2.1.4: INCORRECT\r\n* pandas 2.2.1 (latest): INCORRECT\r\n\r\n","comments":["While this is a lazy fix, I've found that in `pandas\/_libs\/index.pyx` if we omit the functionality of `IndexEngine`'s  `_update_from_sliced` method (such that `self.need_unique_check` and `self.need_monotonic_check` = 1), it causes `is_unique` to work as expected and all related tests pass","Hi, I can reproduce this on main - tentatively marking as for 2.2.2.\r\n\r\nI haven't looked into this further myself - but further investigation\/PRs would be welcome.","take","thank you for working on a fix,\r\n\r\nwould it be possible to roll the fix to the 2.1.x branch as well? \r\nit would be really helpful since the regression was introduced with 2.1.0\r\n\r\n","We don't support the 2.1.x branch anymore, we will only release this on 2.2.x"],"labels":["Bug","Regression","Index"]},{"title":"pd.read_parquet ignore my partition (after repartition ) : ArrowInvalid: No match for FieldRef.Name","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n\r\nThe case is:\r\n\r\n1. I use joblib process 4000 items, and write to `partition_cols=['year', 'month', 'code']\r\n\r\n```python\r\ndef etl_book_feature(code):\r\n    ....\r\n    df.to_parquet(DATA_PATH, partition_cols=['year', 'month', 'code'])\r\n\r\nParallel(n_jobs=-1)(delayed(etl_book_feature)(i) for i in codes)\r\n```\r\n2. Because `read_parquet` is very slow on `DATA_PATH`, I merge data under each year&month combination folders, and use a new column '__time__' to write back : `df.to_parquet(target_dir, partition_cols=['year', 'month', '__time__'])`\r\n```python\r\ndef repartition_by_date(base_dir, date, target_dir=None):\r\n    # date = pd.to_datetime(date)\r\n    year = date.year\r\n    month = date.month\r\n    year_month_dirpath = os.path.join(base_dir, f'year={year}', f'month={month}')\r\n    if not os.path.exists(year_month_dirpath):\r\n        print(f'Partition not found:{year_month_dirpath}')\r\n        return\r\n\r\n    df = pd.read_parquet(year_month_dirpath)\r\n    df['year'] = year\r\n    df['month'] = month\r\n\r\n    # ensure\r\n    df['date'] = pd.to_datetime(df['date'])\r\n \r\n    if not target_dir:\r\n        target_dir = base_dir\r\n        \r\n    if target_dir == base_dir:\r\n        delete_path(year_month_dirpath)\r\n        os.mkdir(year_month_dirpath)\r\n    \r\n    df['__time__'] = df['time'].map(convert_time_to_short_str)\r\n    \r\n    df.to_parquet(target_dir, partition_cols=['year', 'month', '__time__'])\r\n\r\nParallel(n_jobs=5, verbose=51, batch_size=5)(\r\n        delayed(repartition_by_date)(DATA_PATH, item,) for item in pd.date_range('20180101', '20240301', freq='M')\r\n    )\r\n```\r\n\r\n3. test \r\n```\r\npd.read_parquet(DATA_PATH, filters=[[('__time__', '=', '1000')], [('year', '=', 2021)], [('month', '=', 1)]])\r\n```\r\ngot\r\n```\r\nArrowInvalid: No match for FieldRef.Name(__time__)\r\n```\r\n\r\nI am not sure is it a bug or document problem.\r\n\r\n### Issue Description\r\n\r\nCurrent, `read_parquet` perform very poor when there are too much partitions  .\r\n1. As the example I mention, I have to do ETL in parallel , so the initial number of partions must be very large. I have to do repartition to improve reading speed. \r\n2. I read each year\/month partiion ,and rewrite to orginal folder with  `df.to_parquet(target_dir, partition_cols=['year', 'month', '__time__'])`\r\n3.  read new data got  ArrowInvalid: No match for FieldRef.Name(__time__)  \r\n\r\n### Expected Behavior\r\n\r\nsuccess\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.13.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19041\r\nmachine             : AMD64\r\nprocessor           : AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : Chinese (Simplified)_China.936\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.4\r\npytz                : 2024.1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.1.1\r\npip                 : 24.0\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 5.1.0\r\nhtml5lib            : 1.1\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.3\r\nIPython             : 8.22.1\r\npandas_datareader   : None\r\nbs4                 : 4.12.3\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : 2024.2.0\r\nfsspec              : 2024.2.0\r\ngcsfs               : None\r\nmatplotlib          : 3.8.3\r\nnumba               : 0.57.1\r\nnumexpr             : 2.9.0\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 15.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : 2024.2.0\r\nscipy               : 1.12.0\r\nsqlalchemy          : 2.0.27\r\ntables              : 3.9.2\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : 2.0.1\r\nzstandard           : None\r\ntzdata              : 2024.1\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\n<\/details>\r\n","comments":["I write a reproducable code:\r\n\r\n```python\r\nimport os\r\nimport shutil\r\n\r\ndef delete_path(path):\r\n    '''\r\n    os.path.isfile \u6587\u4ef6\u4e0d\u5b58\u5728\u4e5f\u662f\u8fd4\u56deFalse ! \u4e0d\u5982\u76f4\u63a5\u7528try\r\n    '''\r\n    try:\r\n        os.remove(path)\r\n    except OSError:\r\n        # \u53ea\u63a5\u6536\u76ee\u5f55\u4e3a\u53c2\u6570\uff0cos.rmdir \u4e0d\u80fd\u5220\u9664\u975e\u7a7a\u6587\u4ef6\u5939\r\n        shutil.rmtree(path)\r\n\r\ndef repartition_by_date(base_dir, date, target_dir=None):\r\n    '''\r\n    \u8981\u6c42\u76ee\u6807\u8def\u5f84\u5fc5\u987b\u6709 year \u548c month, \u5c06\u4e0b\u5c42\u6570\u636e\u8fdb\u884c\u5408\u5e76\u8f93\u51fa\r\n    '''\r\n    # date = pd.to_datetime(date)\r\n    year = date.year\r\n    month = date.month\r\n    \r\n    year_month_dirpath = os.path.join(base_dir, f'year={year}', f'month={month}')\r\n    \r\n    if not os.path.exists(year_month_dirpath):\r\n        print(f'Partition not found:{year_month_dirpath}')\r\n        return\r\n    \r\n    ## \u5982\u679c\u6ca1\u6709\u8bfb\u5230\u5206\u533a\u4f1a\u62a5\u9519\r\n    # df = pd.read_parquet(base_dir, filters=[('year', '=', year), ('month', '=', month)])\r\n    \r\n    # \u76f4\u63a5\u6784\u9020path\u63d0\u9ad8\u8bfb\u53d6\u6548\u7387\uff0c\u5e76\u9632\u6b62\u62a5\u9519\r\n    df = pd.read_parquet(year_month_dirpath)\r\n    df['year'] = year\r\n    df['month'] = month\r\n\r\n    # ensure\r\n    df['date'] = pd.to_datetime(df['date'])\r\n    \r\n    # \u7528parquet\u5199\uff0c \u4e3a\u4e86\u9632\u6b62\u6309\u6708\u5199 \u76f8\u4e92\u8986\u76d6\uff0c\u5fc5\u987b\u5f97\u52a0\u4e0a code partition\r\n    \r\n    if not target_dir:\r\n        target_dir = base_dir\r\n        \r\n    if target_dir == base_dir:\r\n        delete_path(year_month_dirpath)\r\n        os.mkdir(year_month_dirpath)\r\n    \r\n    \r\n    df['__time__'] = df['day'].astype('str')\r\n    \r\n    df.to_parquet(target_dir, partition_cols=['year', 'month', '__time__'])\r\n    \r\ntry:\r\n    delete_path('.\/new_data2')\r\nexcept:\r\n    pass\r\n\r\n# create data\r\nsr = pd.date_range('2020','2023', freq='H').to_series()\r\ndf = pd.DataFrame(dict(date=sr, year=sr.dt.year, month=sr.dt.month, day=sr.dt.day, hour=sr.dt.hour))\r\ndf.to_parquet('.\/new_data2', partition_cols=['year','month', 'hour'])\r\n\r\n# repartion (remove hour=xx folder, add new folder __time__=xx )\r\n_ = [repartition_by_date('.\/new_data2', d) for d in pd.date_range('2020','2023', freq='MS')]\r\n\r\n# read new data\r\npd.read_parquet('.\/new_data2', filters=[('__time__', '=', 1), ('year', '=', 2021), ('month', '=', 1), ])\r\n```"],"labels":["Bug","Needs Triage"]},{"title":"BUG: groupby(group_keys=False).apply(func=transform_function) with duplicate index does not preserve original dataframe order","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\ndf = pd.DataFrame(                    [\r\n                        [\"k0\", 13, \"e\"],\r\n                        [\"k1\", 14, \"d\"],\r\n                        [\"k0\", 15, \"c\"],\r\n                        [\"k0\", 16, \"b\"],\r\n                        [None, 17, \"a\"],\r\n                    ],\r\n                    index=pd.Index([\"i1\", None, \"i0\", \"i2\", None], name=\"index\"),\r\n                    columns=pd.Index([\"string_col_1\", \"int_col\", \"string_col_2\"], name=\"x\"),)\r\nprint(df.index)\r\nresult = df.groupby(\"index\", sort=False, dropna=False, group_keys=False)['int_col'].apply(lambda v: v)\r\nprint(result.index)\r\nassert result.index.equals(df.index)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\ngroupby.apply transforms should restore the original dataframe order.\r\n\r\nThe current implementation loses the original order so when the axis has duplicates, there's no way to correctly reindex the result back to the original order [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/e14a9bd41d8cd8ac52c5c958b735623fe0eae064\/pandas\/core\/groupby\/groupby.py#L1234-L1246).\r\n\r\n### Expected Behavior\r\n\r\nwhen `func` acts as a transform, groupby.apply should produce a result that has the same index as the original. The result for the nth value with group key a in the input dataframe should be the nth value with group key a in the output dataframe.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.4.0\r\nVersion               : Darwin Kernel Version 23.4.0: Wed Feb 21 21:45:49 PST 2024; root:xnu-10063.101.15~2\/RELEASE_ARM64_T6020\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Hello I would like to work on this issue please. ","Thanks for the report; agreed apply should reorder the result here. For a workaround, `groupby.transform` appears to behave correctly."],"labels":["Bug","Groupby","Apply"]},{"title":"[WIP]: BLD, TST: Build and test Pyodide wheels for `pandas` in CI","body":"- [x] closes #57891 \r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\n## Description\r\n\r\nThis PR adds a workflow, `emscripten.yml`, to the GitHub Actions CI activities where WebAssembly wheels for `pandas` using Pyodide\/Emscripten are built and then tested. I have fixed up a few of the tests, but most of them are skipped for now and these differences _probably_ do not affect major functionality.","comments":["Here is one of the latest workflow runs on my fork: https:\/\/github.com\/agriyakhetarpal\/pandas\/actions\/runs\/8331354944\/job\/22798065116\r\n\r\nThe associated failures right now are mostly coming from a singular test that (which is associated with a hypothesis generator that brings additional test cases?)\r\n\r\nFor example, there is a floating point inaccuracy, here \u2013 which could be corrected by setting the relative and absolute tolerances differently:\r\n\r\n```\r\nE           AssertionError: DataFrame.iloc[:, 2] (column name=\"dt_as_dt\") are different\r\nE           \r\nE           DataFrame.iloc[:, 2] (column name=\"dt_as_dt\") values are different (100.0 %)\r\nE           [index]: [0, 1]\r\nE           [left]:  [253402127999999, 1564703999999]\r\nE           [right]: [253402127999998, 1564703999998]\r\n```\r\n\r\nand some `tzlocal` tests, which seem to bring overflow errors. Note: this output is truncated, please see the entirety of them in the workflow logs:\r\n\r\n```\r\n_______________ test_apply_out_of_range[tzlocal()-BusinessHour] ________________\r\n[XPASS(strict)] OverflowError inside tzlocal past 2038\r\n_____________ test_apply_out_of_range[tzlocal()-BusinessMonthEnd] ______________\r\n[XPASS(strict)] OverflowError inside tzlocal past 2038\r\n____________ test_apply_out_of_range[tzlocal()-BusinessMonthBegin] _____________\r\n[XPASS(strict)] OverflowError inside tzlocal past 2038\r\n```\r\n\r\nI am not completely sure how to fix these tests \u2013 I don't get why these aren't skipped completely? I would appreciate another pair of eyes on this; happy to receive pointers from someone experienced with the codebase :)","> I am not completely sure how to fix these tests \u2013 I don't get why these aren't skipped completely? I would appreciate another pair of eyes on this; happy to receive pointers from someone experienced with the codebase :)\r\n\r\nAt least for the second test it looks like it is imperatively skipping for platforms that are not 64 bit. I take it the platform these are running on is still 64 bit but we are cross compiling to a 32 bit WASM target?\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/495f80896852e450badc8866ee7ebe8c434fa228\/pandas\/tests\/indexes\/datetimes\/methods\/test_resolution.py#L25\r\n","> At least for the second test it looks like it is imperatively skipping for platforms that are not 64 bit. I take it the platform these are running on is still 64 bit but we are cross compiling to a 32 bit WASM target?\r\n\r\nHi, @WillAyd, yes \u2013 cross-compilation is indeed what is happening here and the platform is 64-bit, but the Python interpreter that runs the test suite is 32-bit (so it should, in theory, skip this, but it doesn't). I disabled the check in https:\/\/github.com\/pandas-dev\/pandas\/pull\/57896\/commits\/d54d198a3824a6c13150a494b7f4b2af247737aa which I hope should fix it."],"labels":["Testing"]},{"title":"Bump meson","body":"Meson 1.4.0 adds first class support for NumPy > 2.0 detection https:\/\/mesonbuild.com\/Release-notes-for-1-4-0.html#new-numpy-custom-dependency\r\n\r\nNot an urgent upgrade but figured worth starting","comments":["I believe the pypy failure is a Cython issue - see https:\/\/github.com\/cython\/cython\/issues\/6099\r\n\r\nGenerally I was surprised that the amount of warnings changed with this PR. But I fixed a few \/ ignored others (mostly for MSVC). Would be nice to get a lot of the MSVC warnings fixed up but I think that will take a dedicated initiative.\r\n"],"labels":["Build"]},{"title":"ENH: out-of-tree Pyodide builds in CI for `pandas`","body":"### Feature Type\r\n\r\n- [x] Adding new functionality to pandas\r\n- [ ] Changing existing functionality in pandas\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nThis feature proposes adding out-of-tree Pyodide builds for `pandas` on its own CI. As opposed to in-tree builds in the Pyodide packages repository (see the [recipe](https:\/\/github.com\/pyodide\/pyodide\/blob\/main\/packages\/pandas\/meta.yaml)), this will enable building WASM wheels (which are 32-bit)[^1] via [the Emscripten toolchain](https:\/\/emscripten.org\/) on GitHub Actions.\r\n\r\nIn my most recent work assignment, I am working on improving the interoperability for the Scientific Python ecosystem of packages with Pyodide and with each other (https:\/\/github.com\/Quansight-Labs\/czi-scientific-python-mgmt\/issues\/18), which shall culminate with efforts towards bringing interactive documentation for these packages where they can then be run in [JupyterLite notebooks](https:\/\/jupyter.org\/try-jupyter\/lab\/index.html), through nightly builds and wheels for these packages pushed to PyPI-like indices on Anaconda.org (https:\/\/github.com\/Quansight-Labs\/czi-scientific-python-mgmt\/issues\/19) at and during a later phase during the project.\r\n\r\n[^1]: It is to be noted that #15889 removed 32-bit support by removing the wheels, but if I understand correctly, `pandas` continues to be supported when building from source on 32-bit platforms.\r\n\r\n### Feature Description\r\n\r\n1. A CI pipeline via GitHub Actions that builds and tests `pandas` in an activated virtual environment created and handled by Pyodide.\r\n2. Fixing up the tests wherever applicable, conforming to [limitations](https:\/\/pyodide.org\/en\/stable\/usage\/wasm-constraints.html) that come from the Emscripten toolchain (current limitations include the lack of threading support and for running subprocesses, limited file system access, and others).\r\n\r\n### Alternative Solutions\r\n\r\nN\/A\r\n\r\n### Additional Context\r\n\r\nI notice that the build and the tests are going to be split into separate packages in #53007, which can be handled (I assume that the imminent `pandas-tests` package will be a pure Python one, which does not need compilation for Pyodide \u2013 they are supported by default).\r\n\r\ncc @lithomas1 for any additional comments here, as required. Thomas and I have been in conversation about this just about a fortnight ago, and we had agreed that reducing the size of the wheels would be helpful for the WebAssembly rendition of `pandas` as well \u2013 it is particularly useful where reducing bandwidth usage is in the question. I am happy to follow along on further developments on that regard.","comments":["I have experimented with this in a branch on my fork, where I have managed to bring the test suite down to just 31 failures by skipping certain tests and fixing a few other ones \u2013 I can open a draft PR for this for the purposes of visibility as of now, and work towards getting down to zero failures with the help and support of the maintainers and core developers.","xref some other discussions about Pyodide as follows (some of them coming from the PyArrow requirement that revolves around [PDEP-10](https:\/\/pandas.pydata.org\/pdeps\/0010-required-pyarrow-dependency.html)):\r\n\r\n1. https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466\r\n2. https:\/\/github.com\/pandas-dev\/pandas\/issues\/52509\r\n3. https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711\r\n4. https:\/\/github.com\/pandas-dev\/pandas\/pull\/47428\r\n\r\nThe issue at hand with PyArrow as a required dependency is the increased size of the download, which brings negative implications for using `pandas` in a JupyterLite-enabled kernel\/notebook \u2013 I haven't followed up on the NumPy string DType improvements that have come up recently, but I would happy if someone could fill me in on the latest developments :)\r\n\r\n","I am not super optimistic about having to debug pyodide stuff in my PRs, I am really hesitant on adding official support for the platform","Hi, @phofl \u2013 I totally understand that. It does come with its concerns related to maintenance; if patches might be required to disable certain functionality and debugging them is indeed painful. I would be happy to take on that sort of maintenance if needed, but realistically, yes, it won't be possible to do so for me or anyone else every time there is a failure or if any critical changes are required in a jiffy. I will keep my PR marked as draft for now, but I hope that the team changes their mind :) \r\n\r\nThat being said, if it is ever enabled it will still be regarded as an \"experimental\" platform though, and it will stay that way for a long while until things in the Pyodide ecosystem become stable and we have a v1 release. But I suppose it also depends upon the popularity of the platform continues to evolve in the coming years. Here's what the scikit-learn developers think about that if you wish to have a read: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23727","How far is Numpy's progress on this front? I would also be hesitant with officially supporting this too until pandas' required dependencies have WASM support (at least CI testing)","numpy has a workflow testing this already\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/emscripten.yml.","Thank you, @lithomas1. Yes, @mroeschke, NumPy is currently testing against Pyodide for all PRs with the link to the workflow as shared above, restarting with  this PR: https:\/\/github.com\/numpy\/numpy\/pull\/25894 \u2013 thanks to @rgommers for his help. It did require a bit of back and forth with the migration to the new build system, but that's not really required with `pandas` and with my limited experience I have seen that Emscripten handles Cython code well enough. A lot of the tests for NumPy were being skipped through a WASM platform check and a pytest decorator\/marker already before the PR came to fruition, which would ideally be the case here too (that's what has been done in my PR wherever applicable).","A bit of context:\r\n\r\n- Both NumPy and scikit-learn have had a CI job for running with Pyodide for quite a while now (maybe 1.5 years, sklearn maybe longer).\r\n  - NumPy only disabled it during the transition from setuptools to Meson, to not have to think about it for a while. Other than that it's been quite unproblematic.\r\n- Adding a CI job is not a commitment, so I wouldn't call it \"officially supported\". Putting wheels up on PyPI for a new platform is a commitment, because once they're there and users rely on them, you can't easily stop producing them. However, PyPI does not allow `wasm32` wheels, so that is not what is proposed here.\r\n  - For this CI job, the `numpy` (and `pyarrow` and other deps) to build\/test against would come from Pyodide itself.\r\n- Why is this a useful exercise? At the moment, to ensure the Pandas test suite passes on Pyodide. Later, to also enable interactive docstring examples with little extra work, through support in the `pydata-sphinx-theme` and `jupyterlite-sphinx`.\r\n- If you don't want to add the CI job before other projects have more experience or before the interactive docs are fully ready, then just merging the test suite improvements is also a useful thing to do.\r\n\r\nSince @agriyakhetarpal is ready to do the work, including changes\/improvements after initial merge, it seems quite low-risk to merge the job - and if it turns out to be a burden, then you can simply decide to disable the job again.","> I am not super optimistic about having to debug pyodide stuff in my PRs, I am really hesitant on adding official support for the platform\r\n\r\nThat's a fair point. \r\n\r\nIs there an official Docker image for pyodide? I personally would be comfortable debugging if this was available, not sure about others, though.","Since other pydata libraries have been testing it, I would be OK adding it to the CI but, if it comes to it, allow it to fail without blocking a merge","> Is there an official Docker image for pyodide? I personally would be comfortable debugging if this was available, not sure about others, though.\r\n\r\n@lithomas1 there is a Pyodide Docker image, yes: https:\/\/pyodide.org\/en\/stable\/development\/building-from-sources.html#using-docker. I don't use it personally and I prefer debugging on GitHub Actions because they don't have an aarch64 image yet (https:\/\/github.com\/pyodide\/pyodide\/issues\/1386), and emulation is slow up to multiple magnitudes for me."],"labels":["Enhancement","Build"]},{"title":"ENH: Allow args to be specified for the pivot_table aggfunc","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nCurrently the `pivot_table` has an `aggfunc` parameter which is used to do a groupby aggregation [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/core\/reshape\/pivot.py#L170). However, no additional arguments can be passed into that `agg` call. I'm specifically referring to the `*args` which can be specified in (df\/series) [groupby.agg](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.core.groupby.DataFrameGroupBy.agg.html) function. It would be very useful if `pivot_table` could accept additional arguments for the `aggfunc`.\n\n### Feature Description\n\nI'm not 100% sure, but I think it would be something like this:\r\n1. Add a `**aggfunc_args` or `aggfunc_args: dict` parameter to the `pivot_table` function.\r\n2. Do the same for the `__internal_pivot_table` function\r\n3. Change the line [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/core\/reshape\/pivot.py#L170), from `agged = grouped.agg(aggfunc)` to `agged = grouped.agg(aggfunc, **aggfunc_args)`.\r\n4. Update the docs\n\n### Alternative Solutions\n\nThe same functionality can currently be achieved by specifying a custom function as `aggfunc`, but using that is much slower. My use case is pretty much the same as [this](https:\/\/stackoverflow.com\/questions\/56890105\/how-to-keep-nan-in-pivot-table).\r\nInstead of using `pd.pivot_table(... , aggfunc=lambda x: x.sum(min_count=1))`, I would like to be able to do `pd.pivot_table(... , aggfunc=sum, min_count=1)` or similiar.\n\n### Additional Context\n\n_No response_","comments":[],"labels":["Enhancement","Needs Triage"]},{"title":"EHN: add ability to format index and col names to Styler","body":"- [ ] xref #48936  #47489 and #57362\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nStage one. Part of an multi-stages effort: https:\/\/github.com\/pandas-dev\/pandas\/pull\/57880#issuecomment-2003636401\r\n\r\nAdding a method to Styler and ensuring it works for the default HTML cases with tests in the appropriate pages.","comments":["Hi thanks for this PR.\r\n\r\nA `Styler.format_names` or `Styler.format_index_names` method has been on the requested agenda for a while now.\r\nThis PR mixes three concepts, however. \r\n\r\n- One is adding a method to `Styler` and ensuring it works for the default HTML cases with tests in the appropriate pages.\r\n- Two is extending it to the `Styler.to_latex` implementation and adding specific to_latex tests.\r\n- Three is ensuring that the legacy `DataFrame.to_latex` implementation is capable of using it (in its limited form).\r\n\r\n- (Four would be about checking if `Styler.to_string` and `Styler.to_excel` will function accordingly)\r\n- (Five is ensuring documentation and examples exist in `Styler.format_names` )\r\n\r\nSince a method addition is quite a significant undertaking. I would ask that you split this PR into multiple stages.\r\nThe first stage I would look to review is part One. Adding the method just to `Styler` and adding tests in generic places, including testing the default implementation for HTML output.\r\n\r\nFrom what I can see you have followed the template of previous code and that is a great start.\r\n\r\n\r\n\r\n ","Hi @attack68,\r\n\r\nI've updated my code. It's now my attempt at the first point in your list.\r\nPlease help me take a look.","I think we should probably call this method `format_index_names`. Even though its longer I think it is more obvious what it does.\r\n\r\nI think we also need greater test coverage to cover lines.\r\nTake a look at https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/tests\/io\/formats\/style\/test_format.py\r\n\r\nYou should probably look to replicate the tests for the specific case :\r\n\r\n- test_format_index_level\r\n- test_format_clear\r\n- test_format_callable\r\n- test_format_dict\r\n- test_display_format_index","Since your code also involves cases where things are hidden you should also include a test to show that, say if a multi-index has a level hidden, the format function still works correctly on the levels that are still shown. ","@attack68 added a bunch of new tests.\r\n\r\ndon't immediately see the point of `test_display_format_index` for this case so i didnt adapt it.","Are you familiar with `coverage` ? The idea was that that collection of tests should pretty much cover all your code branches. Some of the re-used material like `_maybe_wrap_formatter` is already well covered.","> Are you familiar with `coverage` ? The idea was that that collection of tests should pretty much cover all your code branches. Some of the re-used material like `_maybe_wrap_formatter` is already well covered.\r\n\r\nYeah. I ran coverage and see my new method is well covered. Mostly because i reuse a lot of things, as you have pointed out.\r\n\r\nSo i don't really understand what your suggestion is. Can you elaborate? Or maybe most of the material is reuse so you suggest we could achieve the same coverage without so much test? I hope not.","My point is you need to cover your code with your tests. Aim for 100%. It seems you have achieved that. Do you agree?","Thought so too.\n\nIf there is nothing major, I think i would fix up whats left and wrap this up. Then ill start to work on the next point in a new PR.\n\nIn case if things go stale, this PR can still be merged as an enhancement on its own. (ofc versionadded and whatsnew have to be filled before merge).\n\nDoes that sound good to you?","Indeed, is what I would suggest. Thanks for the good work here and following the structure. Hopefully the other reviewers will recognise this as quite a tidy addition and be fairly quick approve also. I'll look to add my approval when all the bits in and tests pass.\r\n\r\nThere is one additional file you have to add to: doc\/source\/reference\/style.rst"],"labels":["Enhancement","Styler"]},{"title":"BUG:  pandas pivot_table count over a dataframe with 2 columns results in empty dataset when using aggfunc=\"count\"","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\nRun:\r\n\r\n```python\r\n\r\nimport pandas as pd\r\n\r\ndata = {'Category': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\r\n        'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90]}\r\n\r\ndf = pd.DataFrame(data)\r\n\r\npivot_table = df.pivot_table(index='Category', columns='Value', values='Value', aggfunc=\"count\")\r\n\r\nprint(pivot_table)\r\n```\r\n\r\nand you will get:\r\n\r\n```\r\nEmpty DataFrame\r\nColumns: []\r\n```\r\n\r\nChange to \r\n\r\n```\r\npivot_table = df.pivot_table(index='Category', columns='Value', values='Value', aggfunc=len)\r\n```\r\n\r\nand you will get:\r\n\r\n```\r\nValue      10   20   30   40   50   60   70   80   90\r\nCategory                                             \r\nA         1.0  NaN  NaN  1.0  NaN  NaN  1.0  NaN  NaN\r\nB         NaN  1.0  NaN  NaN  1.0  NaN  NaN  1.0  NaN\r\nC         NaN  NaN  1.0  NaN  NaN  1.0  NaN  NaN  1.0\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nTry to use df.pivot_table over a dataframe with 2 columns, using same column name for the columns and values paramete, aggregate using \"count\" gets you an empty dataset. Switch to len or size and you get the right result.\r\n\r\nFurthermore, strangely, this workaround somehow fixes \"count\":\r\n\r\n```python\r\ndf[\"ValueCopy\"] = df[\"Value\"]\r\npivot_table = df.pivot_table(index='Category', columns='Value', values='ValueCopy', aggfunc=\"count\")\r\n```\r\n\r\nValueCopy and Value contain identical data, so they should be interchangeable, that is, using either should lead to the same result, but they do not. \r\n\r\nFor sanity checking I tried the same pivoting in **Polars**, and there, count works fine:\r\n\r\n```python\r\nimport polars as pl\r\n\r\n# Create a DataFrame\r\ndata = {\r\n    'Category': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\r\n    'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90]\r\n}\r\ndf = pl.DataFrame(data)\r\n\r\n# Pivot the DataFrame\r\npivot_table = df.pivot(index='Category', columns='Value', values='Value', aggregate_function=\"count\")\r\n\r\n# Print the pivot table\r\nprint(pivot_table)\r\n```\r\n\r\nresults in:\r\n\r\n```\r\nShape: (3, 10)\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Category \u2506 10   \u2506 20   \u2506 30   \u2506 \u2026 \u2506 60   \u2506 70   \u2506 80   \u2506 90   \u2502\r\n\u2502 ---      \u2506 ---  \u2506 ---  \u2506 ---  \u2506   \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\r\n\u2502 str      \u2506 u32  \u2506 u32  \u2506 u32  \u2506   \u2506 u32  \u2506 u32  \u2506 u32  \u2506 u32  \u2502\r\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\r\n\u2502 A        \u2506 1    \u2506 null \u2506 null \u2506 \u2026 \u2506 null \u2506 1    \u2506 null \u2506 null \u2502\r\n\u2502 B        \u2506 null \u2506 1    \u2506 null \u2506 \u2026 \u2506 null \u2506 null \u2506 1    \u2506 null \u2502\r\n\u2502 C        \u2506 null \u2506 null \u2506 1    \u2506 \u2026 \u2506 1    \u2506 null \u2506 null \u2506 1    \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nLikewise with **DuckDb**:\r\n\r\n```python\r\nimport pandas as pd\r\nimport duckdb\r\n\r\n# Your existing DataFrame\r\ndata = {'Category': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\r\n        'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90]}\r\ndf = pd.DataFrame(data)\r\n\r\n# Create a DuckDB connection\r\ncon = duckdb.connect()\r\n\r\n# Register the DataFrame with DuckDB\r\ncon.register('df_pivot', df)\r\n\r\n# Perform the pivot operation in DuckDB\r\nquery = \"\"\"\r\n\r\nPIVOT df_pivot\r\nON Value\r\nUSING Count(Value)\r\nGROUP BY Category\r\n\r\n\"\"\"\r\npivot_table = con.execute(query).fetchdf()\r\n\r\nprint(pivot_table)\r\n```\r\n\r\nit works out all-right:\r\n\r\n```\r\n  Category  10  20  30  40  50  60  70  80  90\r\n0        A   1   0   0   1   0   0   1   0   0\r\n1        B   0   1   0   0   1   0   0   1   0\r\n2        C   0   0   1   0   0   1   0   0   1\r\n```\r\n\r\n### Expected Behavior\r\n\r\n`pivot_table = df.pivot_table(index='Category', columns='Value', values='Value', aggfunc=\"count\")`\r\n\r\nand\r\n\r\n`pivot_table = df.pivot_table(index='Category', columns='Value', values='Value', aggfunc=\"size\")`\r\n\r\nand \r\n\r\n`pivot_table = df.pivot_table(index='Category', columns='Value', values='Value', aggfunc=len)`\r\n\r\nshould all produce an non empty dataset like the one below (but `count` somehow fails and produces an empty one):\r\n\r\n```\r\nValue      10   20   30   40   50   60   70   80   90\r\nCategory                                             \r\nA         1.0  NaN  NaN  1.0  NaN  NaN  1.0  NaN  NaN\r\nB         NaN  1.0  NaN  NaN  1.0  NaN  NaN  1.0  NaN\r\nC         NaN  NaN  1.0  NaN  NaN  1.0  NaN  NaN  1.0\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-1015-gcp\r\nVersion               : #15~22.04.1-Ubuntu SMP Wed Feb 14 21:22:00 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : 69.0.2.post0\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":["When I read the [documentation](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.pivot_table.html) of `pandas.pivot_table`, I think it assumes that the dataframe has a value column(s) to aggregate, but your `Value` column works like a category (`index` or `columns`) to aggregate values.\r\n\r\nI guess probably this is not what you are looking for, but here is one way to avoid empty dataframe.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndata = {'Category': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\r\n        'Value': [10, 20, 30, 40, 50, 60, 70, 80, 90]}\r\n\r\ndf = pd.DataFrame(data)\r\ndf[\"num\"] = 1\r\n\r\npivot_table = df.pivot_table(index='Category', columns='Value', values='num', aggfunc=\"count\")\r\n\r\nprint(pivot_table)\r\n```"],"labels":["Bug","Needs Triage"]},{"title":"DOC: add note to how pandas deduplicate header when read from file","body":"- [ ] closes #57792 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/30631476\/3d5ee26f-8b6c-4493-bcfd-5491cd68417d\">\r\n\r\n","comments":["\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57874\/"],"labels":["Docs","IO CSV"]},{"title":"ENH: rolling window, make the very first window have the first element of my series on its left","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nFor a data series with datetime index... I wish there is an easy way to make the very first element to be on the left side of the window when rolling starts.\r\n\r\nAs of pandas 2.1.4 I can align a rolling window so the very first element of my series is on the right side of the window when the rolling starts (i.e. center=False).\r\nI can also align the verify first element to be in the middle of the window when rolling starts (i.e. center=True)\r\n\r\nBut there is no easy way to make the very first element to be on the left side of the window when rolling starts.\r\n\r\nFor integer window I can see that I can use FixedForwardWindowIndexer as shown in the following page\r\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.api.indexers.FixedForwardWindowIndexer.html#pandas.api.indexers.FixedForwardWindowIndexer\r\n\r\nBut that doesn't seem to work for when the index is datetime.\r\n\r\n\n\n### Feature Description\n\n```\r\nimport pandas as pd\r\n\r\n# create series\r\ns = pd.Series(range(5), index=pd.date_range('2020-01-01', periods=5, freq='1D'))\r\n\r\n# create a function to call for each window\r\ndef roll_apply(window):\r\n    # return first element of current window\r\n    return window[:1].iloc[0]\r\n\r\n# if center=False, the very first window will have the first element of my series on its right most side\r\nright = s.rolling('4D', center=False).apply(roll_apply)\r\n\r\n# if center=False, the very first window will have the first element of my series on its center\r\ncenter = s.rolling('4D', center=True).apply(roll_apply)\r\n\r\n# there is no easy way to have the very first window have the first element of my series on its left\r\n# ???\r\n\r\n\r\nprint(\"********* original *********\")\r\nprint(s)\r\n\r\nprint(\"********* right aligned *********\")\r\nprint(right)\r\n\r\nprint(\"********* center aligned *********\")\r\nprint(center)\r\n\r\nprint(\"********* left aligned *********\")\r\nprint(\"Not Implemented Yet!\")\r\n```\r\n\r\n# output\r\n\r\n```\r\n********* original *********\r\n2020-01-01    0\r\n2020-01-02    1\r\n2020-01-03    2\r\n2020-01-04    3\r\n2020-01-05    4\r\nFreq: D, dtype: int64\r\n********* right aligned *********\r\n2020-01-01    0.0\r\n2020-01-02    0.0\r\n2020-01-03    0.0\r\n2020-01-04    0.0\r\n2020-01-05    1.0\r\nFreq: D, dtype: float64\r\n********* center aligned *********\r\n2020-01-01    0.0\r\n2020-01-02    0.0\r\n2020-01-03    1.0\r\n2020-01-04    2.0\r\n2020-01-05    3.0\r\nFreq: D, dtype: float64\r\n********* left aligned *********\r\nNot Implemented Yet!\r\n```\n\n### Alternative Solutions\n\nFor integer window I can see that I can use FixedForwardWindowIndexer as shown in the following page\r\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.api.indexers.FixedForwardWindowIndexer.html#pandas.api.indexers.FixedForwardWindowIndexer\r\n\r\nBut that doesn't seem to work for when the index is datetime.\n\n### Additional Context\n\n_No response_","comments":[],"labels":["Enhancement","Needs Triage"]},{"title":"DEP: Deprecate CoW and no_silent_downcasting options","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":[],"labels":["Deprecate","Copy \/ view semantics"]},{"title":"DEPR: Deprecate remaining copy usages","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":[],"labels":["Deprecate","Copy \/ view semantics"]},{"title":"DOC: Add examples for creating an index accessor","body":"Picked up issue #49202. This adds examples for creating index, dataframe, and series accessors.\r\n- [X] closes #49202 \r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["pre-commit.ci autofix","pre-commit.ci autofix","pre-commit.ci autofix","Can you have a look at the CI errors please?"],"labels":["Docs"]},{"title":"DOC: RT03 fix for various DataFrameGroupBy and SeriesGroupBy methods","body":"- [x] xref https:\/\/github.com\/pandas-dev\/pandas\/issues\/57416\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57862\/","@quangngd we changed how we specify the pending errors in `code_checks.sh` and you'll have to resolve the conflicts. Sorry for the inconvenience.","@datapythonista resolved"],"labels":["Docs"]},{"title":"BUG: v2.2.x Pandas DataFrame Raises a __deepcopy__ error when class placed in `attrs`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n\r\nimport json as _ujson\r\nfrom collections import OrderedDict\r\nfrom collections.abc import MutableMapping, Mapping\r\n\r\n\r\n###########################################################################\r\nclass InsensitiveDict(MutableMapping):\r\n    \"\"\"\r\n    A case-insensitive ``dict`` like object used to update and alter JSON\r\n\r\n    A varients of a case-less dictionary that allows for dot and bracket notation.\r\n    \"\"\"\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __init__(self, data=None):\r\n        self._store = OrderedDict()  #\r\n        if data is None:\r\n            data = {}\r\n\r\n        self.update(data)\r\n        self._to_isd(self)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __repr__(self):\r\n        return str(dict(self.items()))\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __str__(self):\r\n        return str(dict(self.items()))\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __setitem__(self, key, value):\r\n        if str(key).lower() in {\"_store\"}:\r\n            super(InsensitiveDict, self).__setattr__(key, value)\r\n        else:\r\n            if isinstance(value, dict):\r\n                self._store[str(key).lower()] = (key, InsensitiveDict(value))\r\n            else:\r\n                self._store[str(key).lower()] = (key, value)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __getitem__(self, key):\r\n        return self._store[str(key).lower()][1]\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __delitem__(self, key):\r\n        del self._store[str(key).lower()]\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __getattr__(self, key):\r\n        if str(key).lower() in {\"_store\"}:\r\n            return self._store\r\n        else:\r\n            return self._store[str(key).lower()][1]\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __setattr__(self, key, value):\r\n        if str(key).lower() in {\"_store\"}:\r\n            super(InsensitiveDict, self).__setattr__(key, value)\r\n        else:\r\n            if isinstance(value, dict):\r\n                self._store[str(key).lower()] = (key, InsensitiveDict(value))\r\n            else:\r\n                self._store[str(key).lower()] = (key, value)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __dir__(self):\r\n        return self.keys()\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __iter__(self):\r\n        return (casedkey for casedkey, mappedvalue in self._store.values())\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __len__(self):\r\n        return len(self._store)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def _lower_items(self):\r\n        \"\"\"Like iteritems(), but with all lowercase keys.\"\"\"\r\n        return (\r\n            (lowerkey, keyval[1])\r\n            for (lowerkey, keyval) in self._store.items()\r\n        )\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __setstate__(self, d):\r\n        \"\"\"unpickle support\"\"\"\r\n        self.__dict__.update(InsensitiveDict(d).__dict__)\r\n        self = InsensitiveDict(d)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __getstate__(self):\r\n        \"\"\"pickle support\"\"\"\r\n        return _ujson.loads(self.json)\r\n\r\n    # ----------------------------------------------------------------------\r\n    @classmethod\r\n    def from_dict(cls, o):\r\n        \"\"\"Converts dict to a InsensitiveDict\"\"\"\r\n        return cls(o)\r\n\r\n    # ----------------------------------------------------------------------\r\n    @classmethod\r\n    def from_json(cls, o):\r\n        \"\"\"Converts JSON string to a InsensitiveDict\"\"\"\r\n        if isinstance(o, str):\r\n            o = _ujson.loads(o)\r\n            return InsensitiveDict(o)\r\n        return InsensitiveDict.from_dict(o)\r\n\r\n    # ----------------------------------------------------------------------\r\n    def __eq__(self, other):\r\n        if isinstance(other, Mapping):\r\n            other = InsensitiveDict(other)\r\n        else:\r\n            return NotImplemented\r\n        return dict(self._lower_items()) == dict(other._lower_items())\r\n\r\n    # ----------------------------------------------------------------------\r\n    def copy(self):\r\n        return InsensitiveDict(self._store.values())\r\n\r\n    # ---------------------------------------------------------------------\r\n    def _to_isd(self, data):\r\n        \"\"\"converts a dictionary from InsensitiveDict to a dictionary\"\"\"\r\n        for k, v in data.items():\r\n            if isinstance(v, (dict, InsensitiveDict)):\r\n                data[k] = self._to_isd(v)\r\n            elif isinstance(v, (list, tuple)):\r\n                l = []\r\n                for i in v:\r\n                    if isinstance(i, dict):\r\n                        l.append(InsensitiveDict(i))\r\n                    else:\r\n                        l.append(i)\r\n                if isinstance(v, tuple):\r\n                    l = tuple(l)\r\n                data[k] = l\r\n        return data\r\n\r\n    # ---------------------------------------------------------------------\r\n    def _json(self):\r\n        \"\"\"converts an InsensitiveDict to a dictionary\"\"\"\r\n        d = {}\r\n        for k, v in self.items():\r\n            if isinstance(v, InsensitiveDict):\r\n                d[k] = v._json()\r\n            elif type(v) in (list, tuple):\r\n                l = []\r\n                for i in v:\r\n                    if isinstance(i, InsensitiveDict):\r\n                        l.append(i._json())\r\n                    else:\r\n                        l.append(i)\r\n                if type(v) is tuple:\r\n                    v = tuple(l)\r\n                else:\r\n                    v = l\r\n                d[k] = v\r\n            else:\r\n                d[k] = v  # not list, tuple, or InsensitiveDict\r\n        return d\r\n\r\n    # ----------------------------------------------------------------------\r\n    @property\r\n    def json(self):\r\n        \"\"\"returns the value as JSON String\"\"\"\r\n        o = self._json()  # dict(self.copy())\r\n        return _ujson.dumps(dict(o))\r\n\r\n\r\ndata = {\r\n    \"A\": [1, 2, 3],\r\n    \"B\": ['a', 'b', None],\r\n}\r\n\r\ndf = pd.DataFrame(data=data)\r\ndf.attrs[\"metadata\"] = InsensitiveDict(data)\r\n\r\nprint(df.head()) # error here\n```\n\n\n### Issue Description\n\nIn Pandas 2.1 and below, we could put whatever we wanted in the `attrs` and the data would not throw a `__deepcopy__` error.  We would like it if the `attrs` didn't require information to be deep copied like previous version. \r\n\r\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.attrs.html\n\n### Expected Behavior\n\nWhen working with data and you have any custom class that doesn't implement `__deepcopy__`, the pandas data is essentially useless on any operation.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.23.5\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.6.1\r\npip                   : 23.3.1\r\nCython                : 0.29.34\r\npytest                : None\r\nhypothesis            : 6.72.0\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.0\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : 1.1\r\npymysql               : 1.0.3\r\npsycopg2              : 2.9.6\r\njinja2                : 3.1.2\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : 2023.2.0\r\nfsspec                : 2023.4.0\r\ngcsfs                 : 2023.4.0\r\nmatplotlib            : 3.7.1\r\nnumba                 : 0.56.4\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : 0.19.1\r\npyarrow               : 15.0.1\r\npyreadstat            : 1.2.1\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : None\r\nscipy                 : 1.10.1\r\nsqlalchemy            : 2.0.9\r\ntables                : 3.8.0\r\ntabulate              : 0.9.0\r\nxarray                : 2023.4.1\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.21.0\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["To follow up, this PR https:\/\/github.com\/pandas-dev\/pandas\/pull\/55314 is the breaking change. ","As outlined in the PR, deepcopy was introduced to prevent unwanted side effects when attrs are used and propagated. \n\nThe failure mode of using a class without `__deepcopy__` is transparent, just like the fix required (implement deepcopy) to be able to use attrs with the custom class.\n\nOn the other hand, side effects from not deepcopy'ing attrs in the background are intransparent to the end user. So in the end using deepcopy is probably preferable.","@achapkowski Maybe try `df.attrs[\"metadata\"] = {**InsensitiveDict(data)}`.","@achapkowski Since you are making a custom implementation of a dictionary, you can implement a `__deepcopy__` like this:\r\n\r\n```python\r\n    def __deepcopy__(self, memo):\r\n        result = InsensitiveDict(deepcopy({**self}))\r\n        memo[id(self)] = result\r\n        return result\r\n```","I know I can implement deepcopy, but the issue is if I use any 3rd party library, like this class from requests, I have to monkey patch it.  \r\n","How is it problematic?  From what I read it's a problem with geopandas not pandas. This is a major regression because most classes and packages do not implement deepcopy. "],"labels":["Bug","Needs Triage"]},{"title":"BUG: upcasting when assigning to an enlarged series does not produce a FutureWarning ","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ns = pd.Series([1, 2, 3], index=list('abc'), dtype='UInt8')\r\nprint(s.dtype)\r\n# we upcast on enlargement\r\ns.loc[4] = 'x'\r\nprint(s.dtype)\n```\n\n\n### Issue Description\n\nAccording to https:\/\/pandas.pydata.org\/pdeps\/0006-ban-upcasting.html\r\nraising a `FutureWarning` on upcasting during enlargement if out of the scope of this pdep.\r\n\r\nHowever, it is inconsistent with the new behavior. I expected to see an issue report discussing this problem, but I did not find one (searching for `upcasting` in the issue reports). I created one to either start the discussion, or be redirected to where it is discussed. \r\n\n\n### Expected Behavior\n\nI would expect to have a FutureWarning in this case (and an error in the future). \r\n\r\n\n\n### Installed Versions\n\n<details>\r\nIn [412]: pd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.0.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 11\r\nVersion               : 10.0.22631\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : fr_FR.cp1252\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : 3.0.8\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.0\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 14.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.3\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"Deprecate accepting None in pd.concat","body":"`pd.concat` accepts iterables that may contain `Series`, `DataFrame` and `None`, where `None` are simply ignored.\r\n\r\n```py\r\npd.concat([None, series, None, series, None]) # works, same as pd.concat([series, series])\r\npd.concat([None]) # raises\r\n```\r\n\r\nIt would be great to deprecate\/remove accepting `None`s. This will help to better annotate `concat` https:\/\/github.com\/pandas-dev\/pandas-stubs\/issues\/888\r\n\r\nPandas-stubs currently has to choose between either false-positives (accept `concat([None])` ) or false-negatives (reject some `concat(Iterable[Any\/Unknown])` that could succeed).\r\n\r\n","comments":["I am -1 on simply deprecating this because of typing reasons. Let's think about use cases before we make a decision. We definitely can't do this without a deprecation","I agree, it would be good first to understand which use case supporting `None` enables!\r\n\r\n@Dr-Irv found a way to get it to type check without having to decide between false negatives\/positives :)","If the function should accept None's in `objs`, then it seems most natural to me for it to return an empty DataFrame if `objs` contains **only** None's.\r\n\r\nThis is backwards compatible and would also resolve the typing issue.\r\n\r\nA hypothetical use case for `Iterable[DataFrame | None]`: you have a list of files to load and parse and you want to put them into a single DataFrame, but not all of the filenames you've been given exist, or maybe parsing is allowed to fail. I could see someone implementing this as a function that returns `DataFrame | None`, and putting that into a list comprehension. So a situation where you just want to put everything you have into one table, and if everything you have is None, then your table is empty."],"labels":["Reshaping","Deprecate","Needs Discussion"]},{"title":"improve accuracy of to_pytimedelta","body":"```\r\n>>> a = pd.Timedelta(1152921504609987375)\r\n>>> a.to_pytimedelta()\r\ndatetime.timedelta(days=13343, seconds=86304, microseconds=609988)\r\n>>>\r\n```\r\n\r\n609988 for microseconds but expected is 609987\r\n\r\n```\r\n>>> a = pd.Timedelta(1152921504609987374)\r\n>>> a.to_pytimedelta()\r\ndatetime.timedelta(days=13343, seconds=86304, microseconds=609987)\r\n>>>\r\n```\r\n\r\nIn this example, a difference of 1 nanosecond, shouldn't result in a difference of 1 microsecond when converted from ns to us unit.","comments":["Ok, but the reason this behavior is occurring I don't believe is related to python's round function. It's because python's integer division goes through a floating point which introduces accuracy issues for large integers (and in this case, result is rounded anyway). The issue is with the division, not the rounding done by timedelta.","Sorry, you are correct. I didn't realize the problem with the floating point precision.\r\n\r\nI wonder what's the impact on performance of this change, did you execute a benchmark to compare the execution times of both implementations?\r\n\r\n@MarcoGorelli @jbrockmendel thoughts on this change?","Added whatsnew, are there existing benchmarks for comparing performance?"],"labels":["Timedelta"]},{"title":"BUG: pyarrow.Array.from_pandas converts empty timestamp[s][pyarrow, UTC] pandas Series to ChunkedArray, not TimestampArray","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> import pyarrow as pa\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))))  # correct\r\n<pyarrow.lib.TimestampArray object at 0x7fb665f77fa0>\r\n[]\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))).dt.tz_localize('UTC'))  # incorrect\r\n<pyarrow.lib.ChunkedArray object at 0x7fb665fd8680>\r\n[\r\n\r\n]\r\n```\r\n\r\nSame issue with `pa.array()` instead of `pa.Array.from_pandas()`. The Arrow folks [say](https:\/\/github.com\/apache\/arrow\/issues\/40538) it's a pandas issue.\r\n\r\n\r\n\r\n### Issue Description\r\n\r\n`pyarrow.Array.from_pandas` converts an empty `timestamp[s][pyarrow, UTC]` pandas Series to a `ChunkedArray`, not a `TimestampArray`. It correctly converts to a `TimestampArray` when there is no timezone.\r\n\r\n### Expected Behavior\r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> import pyarrow as pa\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))))\r\n<pyarrow.lib.TimestampArray object at 0x7fb665f77fa0>\r\n[]\r\n>>> pa.Array.from_pandas(pd.Series([], dtype=pd.ArrowDtype(pa.timestamp('s'))).dt.tz_localize('UTC'))\r\n<pyarrow.lib.TimestampArray object at 0x7fb665fd8680>\r\n[]\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.12.2.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.4.0-22621-Microsoft\r\nVersion               : #2506-Microsoft Fri Jan 01 08:00:00 PST 2016\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : C.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Added a comment upstream to pyarrow that _might_ actually be a pyarrow bug https:\/\/github.com\/apache\/arrow\/issues\/40538#issuecomment-1997736174"],"labels":["Bug","Timezones","Upstream issue","Arrow"]},{"title":"BUG: Can't change datetime precision in columns\/rows","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# ONLY WORKING CONVERSION:\r\ndf = pd.DataFrame({'time': pd.to_datetime(['2021-01-01 12:00:00', '2021-01-01 12:00:01', '2021-01-01 12:00:02']), 'value': [1, 2, 3]})\r\ndf['time'] = df['time'].astype('M8[us]') \r\nprint(df.dtypes)\r\n# time     datetime64[us]\r\n# value             int64\r\n# dtype: object\r\n\r\n# NON-WORKING CONVERSIONS\r\n\r\ndf = pd.DataFrame({'time': pd.to_datetime(['2021-01-01 12:00:00', '2021-01-01 12:00:01', '2021-01-01 12:00:02']),\r\n                           'value': [1, 2, 3]})\r\ndf.iloc[:, 0] = df.iloc[:, 0].astype('M8[us]') \r\nprint(df.dtypes)\r\n# time     datetime64[ns]\r\n# value             int64\r\n# dtype: object\r\n\r\n\r\ndf = pd.DataFrame({'time': pd.to_datetime(['2021-01-01 12:00:00', '2021-01-01 12:00:01', '2021-01-01 12:00:02']),\r\n                           'value': [1, 2, 3]})\r\ndf.loc[:, ['time']] = df.loc[:, ['time']].astype('M8[us]') \r\nprint(df.dtypes)\r\n# time     datetime64[ns]\r\n# value             int64\r\n# dtype: object\r\n\r\ndf = pd.DataFrame({'time': pd.to_datetime(['2021-01-01 12:00:00', '2021-01-01 12:00:01', '2021-01-01 12:00:02']),\r\n                           'value': [1, 2, 3]})\r\nidxs = [0]\r\naxis = 1\r\ndf.iloc(axis=axis)[idxs] = df.iloc(axis=axis)[idxs].astype('M8[us]')\r\nprint(df.dtypes)\r\n# time     datetime64[ns]\r\n# value             int64\r\n# dtype: object\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nConversion of columns (\/rows) between datetime dtypes with different precision does not change the datatype of the columns (except for in the simplest case).\r\n\r\nThe absurd is that if I were to change the dtype of the \"value\" column in the above example, all of these example would've worked.\r\n\r\n### Expected Behavior\r\n\r\nAll printouts should be the same as the first:\r\n```\r\ntime     datetime64[us]\r\nvalue             int64\r\ndtype: object\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-91-generic\r\nVersion               : #101~20.04.1-Ubuntu SMP Thu Nov 16 14:22:28 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_IL\r\nLOCALE                : en_IL.UTF-8\r\npandas                : 2.2.1\r\nnumpy                 : 1.24.4\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3\r\nCython                : 3.0.6\r\npytest                : 7.4.3\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : 2.8.6\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":["cc @MarcoGorelli is this pdep6 related? It looks like this is a case of upcasting\r\n\r\n@erezinman all cases that don't work set inplace instead of swapping out the underlying data, so different semantics can happen. ","thanks for the ping\r\n\r\nlooks like it's been like this since at least 2.0.2, so I don't think it's related to any pdep-6 work (which only started in 2.1):\r\n```python\r\n\r\nIn [2]: import pandas as pd\r\n\r\nIn [3]:\r\n   ...: df = pd.DataFrame({'time': pd.to_datetime(['2021-01-01 12:00:00', '2021-01-01 12:00:01', '2021-01-01 12:00:02'])\r\n   ...: ,\r\n   ...:                            'value': [1, 2, 3]})\r\n\r\nIn [4]: df.iloc[:, 0] = df.iloc[:, 0].astype('M8[us]')\r\n\r\nIn [5]: df.dtypes\r\nOut[5]:\r\ntime     datetime64[ns]\r\nvalue             int64\r\ndtype: object\r\n\r\nIn [6]: pd.__version__\r\nOut[6]: '2.0.2'\r\n```"],"labels":["Bug","Timeseries"]},{"title":"BUG: Using DateOffset with shift on a daylight savings transition produces error","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndt = pd.date_range(\"2024-03-31 00:00\", \"2024-03-31 07:00\", freq=\"1h\", tz=\"utc\")\r\ndf = pd.DataFrame(index=dt, data={\"A\":range(0, len(dt))})\r\n\r\ndf_nl = df.tz_convert(tz=\"Europe\/Amsterdam\")\r\ndf_nl[\"B\"] = df_nl[\"A\"].shift(freq=pd.DateOffset(hours=1))\n```\n\n\n### Issue Description\n\nThis last line gives an error:\r\n\r\n`pytz.exceptions.NonExistentTimeError: 2024-03-31 02:00:00`\r\n\r\nWith full traceback:\r\n\r\n```\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py\", line 11230, in shift\r\n    return self._shift_with_freq(periods, axis, freq)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py\", line 11263, in _shift_with_freq\r\n    new_ax = index.shift(periods, freq)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/indexes\/datetimelike.py\", line 503, in shift\r\n    return self + offset\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/ops\/common.py\", line 76, in new_method\r\n    return method(self, other)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/arraylike.py\", line 186, in __add__\r\n    return self._arith_method(other, operator.add)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/indexes\/base.py\", line 7238, in _arith_method\r\n    return super()._arith_method(other, op)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/base.py\", line 1382, in _arith_method\r\n    result = ops.arithmetic_op(lvalues, rvalues, op)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/ops\/array_ops.py\", line 273, in arithmetic_op\r\n    res_values = op(left, right)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/ops\/common.py\", line 76, in new_method\r\n    return method(self, other)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimelike.py\", line 1372, in __add__\r\n    result = self._add_offset(other)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimes.py\", line 828, in _add_offset\r\n    result = result.tz_localize(self.tz)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/_mixins.py\", line 81, in method\r\n    return meth(self, *args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py310_sdkv2\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimes.py\", line 1088, in tz_localize\r\n    new_dates = tzconversion.tz_localize_to_utc(\r\n  File \"tzconversion.pyx\", line 431, in pandas._libs.tslibs.tzconversion.tz_localize_to_utc\r\n```\n\n### Expected Behavior\n\nThis would be the desired ouput:\r\n\r\n```\r\n                           A B\r\n2024-03-31 01:00:00+01:00  0 NaN\r\n2024-03-31 03:00:00+02:00  1 NaN\r\n2024-03-31 04:00:00+02:00  2 1\r\n2024-03-31 05:00:00+02:00  3 2\r\n2024-03-31 06:00:00+02:00  4 3\r\n2024-03-31 07:00:00+02:00  5 4\r\n2024-03-31 08:00:00+02:00  6 5\r\n2024-03-31 09:00:00+02:00  7 6\r\n```\r\n\r\nThe point of converting a UTC timeseries to Europe\/Amsterdam time is that I want to look up behaviour of people, which stays consistent to their timezone. E.g. if someone goes to work every day at 08:00, that remains at 08:00 in their timezone, even after the daylight savings shift. In UTC, that person appears to leave one hour earlier (at 07:00). By converting to Europe\/Amsterdam time, then shifting, this should be handled correctly.\n\n### Installed Versions\n\n<details>\r\n\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.10.11.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-1040-azure\r\nVersion               : #47~20.04.1-Ubuntu SMP Fri Jun 2 21:38:08 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.25.0\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.8.0\r\npip                   : 23.1.2\r\nCython                : 0.29.35\r\npytest                : 8.1.1\r\nhypothesis            : 6.99.5\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.6.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.1\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.10.1\r\nsqlalchemy            : 2.0.16\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: concat with datetime index returns Series instead of scalar if microsecond=0","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nfrom datetime import UTC, datetime\r\n\r\nfrom pandas import DataFrame, concat\r\n\r\nt1 = datetime.now(tz=UTC)\r\nt2 = datetime.now(tz=UTC).replace(microsecond=0)\r\n\r\nt1_str = str(t1)\r\nt2_str = str(t2)\r\n\r\ndf1 = DataFrame({'a': [1]}, index=[t1])\r\nprint(type(df1.loc[t1].a))\r\nprint(type(df1.loc[t1_str].a))\r\n\r\ndf2 = DataFrame({'a': [2]}, index=[t2])\r\nprint(type(df2.loc[t2].a))\r\nprint(type(df2.loc[t2_str].a))\r\n\r\ndf = concat([df1, df2])\r\n\r\nprint(type(df.loc[t1].a))\r\nprint(type(df.loc[t1_str].a))\r\n\r\nprint(type(df.loc[t2].a))\r\nprint(type(df.loc[t2_str].a))\n```\n\n\n### Issue Description\n\n`.a` is correctly returned as `numpy.int64` in all cases, except for the last line when I use `t2_str` and suddenly it's a one element `Series` containing the `numpy.int64`.\r\n\r\nI have no idea what on earth is going on, it took a lot of \ud83d\udd0d to get a repro.\r\nI found it while writing a unit test where I was passing a timestamp from test data as a string.\r\n\r\nIf you remove the `.replace(microsecond=0)` you'll see it works as expected \ud83e\udd2f \n\n### Expected Behavior\n\n`.loc` should be consistent before and after a `concat`.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.2.0-1019-azure\r\nVersion               : #19~22.04.1-Ubuntu SMP Wed Jan 10 22:57:03 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.1\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : 2023.2.0\r\nfsspec                : 2023.10.0\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.10.1\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Maybe, @jbrockmendel knows more, he last changed the relevant code in `datetime._parse_with_reso`.\r\n\r\nThere is some additional logic for handling the lookup with `str` labels on a `DateTimeIndex` that infers a lookup-resolution from the string itself. In the end, the `str(t2)` has no zero'd microseconds in it, thus a sec resolution is inferred, matching both `t1`and `t2`.\r\n\r\nIf one wants to keep this feature of resolution dependent lookup, one would have to add the trailing zeros to times like `str(t2)` manually, since datetime always removes them from what I could see.  ","Correct, this is a feature called \u201cpartial string slicing\u201d on dateteindex"],"labels":["Bug","Needs Triage"]},{"title":"Potential regression induced by PR \"PERF: Categorical(range).categories returns RangeIndex instead of Index\"","body":"PR #57787 \r\n\r\nIf this is expected please ignore the issue.\r\n\r\nThe regressions seem to be here:\r\n\r\n`indexing.MultiIndexing.time_xs_full_key` (Python) with unique_levels=True\r\n`indexing.MultiIndexing.time_loc_all_scalars` (Python) with unique_levels=True\r\n\r\n@mroeschke\r\n\r\n<img width=\"797\" alt=\"Screenshot 2024-03-13 at 12 50 02\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/11835246\/54a6a689-7ffc-4957-8e5c-5c1848efc1f0\">\r\n<img width=\"811\" alt=\"Screenshot 2024-03-13 at 12 52 54\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/11835246\/bcc22b80-e213-49ce-8635-dac8c66163f9\">\r\n\r\n","comments":["Thanks for the report. After profiling for a bit, it looks like there's an slight increase when creating the `MulitiIndex._engine`, but given that this is cached I am not too concerned given the memory reduction of the Index level class."],"labels":["Performance","MultiIndex"]},{"title":"CI: Fail tests on all builds for FutureWarning\/DeprecationWarning from numpy or pyarrow","body":"I don't think we should necessarily limit this to just to nightly build of these libraries","comments":[],"labels":["Testing","CI"]},{"title":"BUG: improve pd.io.json_normalize","body":"- [x] closes #57810\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/v2.2.2.rst` file if fixing a bug or adding a new feature.\r\n","comments":["@slavanorm in case you haven't seen it, your changes are making the tests fail: https:\/\/github.com\/pandas-dev\/pandas\/actions\/runs\/8239141116\/job\/22531710877?pr=57811#step:8:53","I'm not sure about this change - the point of the `errors` argument is to ignore missing keys. Shouldn't the test case you added still create the column but with all empty data?","yes it should but it creates rows only for the dictionaries with record path.\r\ni will edit the fixture and assertion in order to get into this case of if\r\n","This docs check is failing, and its not fixable. Wonder what should we do now "],"labels":["Bug","IO JSON"]},{"title":"BUG: pd.json_normalize improvement","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\npd.json_normalize(\r\n    data=dict(x=[1, 2], y=[]),\r\n    record_path='x',\r\n    meta=[['y', 'yy']])\r\n```\r\n\r\n```python-traceback\r\nTypeError: list indices must be integers or slices, not str\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nHello. \r\njson_normalize is great function. it has feature that allows to coerce errors when provided json structure has some missing keys.\r\n\r\nI wish to improve the coercion algorithm.\r\n\r\n### Expected Behavior\r\n\r\nabove code should result with {x:[1,2],y:[pd.nan,pd.nan]}.\r\nnow it just throws typeError.\r\n\r\nI have already made a fix for that, but would be happy to discuss some other improvements before proposing a PR.\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.3.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 21.4.0\r\nVersion               : Darwin Kernel Version 21.4.0: Mon Feb 21 20:36:53 PST 2022; root:xnu-8020.101.4~2\/RELEASE_ARM64_T8101\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.0.1\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.2\r\nIPython               : 8.19.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : 0.22.0\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["I've edited the pd.io.json._normalize:394 \r\nfrom\r\n                    if result is None:\r\nto\r\n                    if result is None or result==[]:\r\n\r\nand it fixes. but maybe we could use something better"],"labels":["Bug","IO JSON"]},{"title":"DEPR: rename `startingMonth` to `starting_month` (argument in BQuarterBegin)","body":"Renamings should be done with care, but this one strikes me as especially odd\r\n\r\n```python\r\npandas.tseries.offsets.QuarterBegin(startingMonth=1)\r\n```\r\nIt looks very odd in Python to have a camelcase argument name...I thought this was probably a typo in the docs when I saw it, but no, it runs\r\n\r\nOK with deprecating in favour of `starting_month`?\r\n\r\nI think this is the only place in pandas where this happens:\r\n```console\r\n$ git grep -E ' [a-z]+[A-Z][a-z]+: ' pandas\r\npandas\/_libs\/tslibs\/offsets.pyi:        self, n: int = ..., normalize: bool = ..., startingMonth: int | None = ...\r\npandas\/_libs\/tslibs\/offsets.pyi:        startingMonth: int = ...,\r\npandas\/_libs\/tslibs\/offsets.pyi:        startingMonth: int = ...,\r\n$ git grep -E ' [a-z]+[A-Z][a-z]+ : ' pandas\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int, default 3\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int, default 3\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int, default 3\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int, default 3\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int {1, 2, ... 12}, default 1\r\npandas\/_libs\/tslibs\/offsets.pyx:    startingMonth : int {1, 2, ..., 12}, default 1\r\n```","comments":[],"labels":["Frequency","Deprecate"]},{"title":"BUG: read_csv inconsistent behavior","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom io import StringIO\r\n\r\ncsv_data = \"\"\"0,1.992422433360555e-06,0,1.992422433374335e-06,0,0,0,0.01867750880504259,0,0.8,0,0.002722413402762714,0,0.002722413278797551\r\n3e-12,1.992418292306889e-06,3e-12,1.992418292319521e-06,3e-12,0,3e-12,0.01867750887016915,3e-12,0.8,3e-12,0.002722413686257528,3e-12,0.002722413562292257\r\n9.000000000000001e-12,1.992416874989637e-06,9.000000000000001e-12,1.992416875007223e-06,9.000000000000001e-12,0,9.000000000000001e-12,0.01867750900743637,9.000000000000001e-12,0.8,9.000000000000001e-12,0.00272241426436429,9.000000000000001e-12,0.002722414140398993\"\"\"\r\n\r\n# Use StringIO to create a file-like object from the string\r\ncsv_data_io1 = StringIO(csv_data)\r\ncsv_data_io2 = StringIO(csv_data)\r\n\r\n# Use pandas.read_csv to read from the file-like object\r\ndf1 = pd.read_csv(csv_data_io1)\r\ndf2 = pd.read_csv(csv_data_io2, header=None)\r\n\r\n# assertions\r\nprint(df1.columns.tolist())\r\nprint(df2.iloc[0].tolist())\n```\n\n\n### Issue Description\n\nColumns are incorrectly read from CSV\n\n### Expected Behavior\n\ndf1.columns.tolist() being the same as df2.iloc[0].tolist()\r\ni.e.: [0.0, 1.992422433360555e-06, 0.0, 1.992422433374335e-06, 0.0, 0.0, 0.0, 0.0186775088050425, 0.0, 0.8, 0.0, 0.0027224134027627, 0.0, 0.0027224132787975]\r\n\r\nHowever, it is:\r\n['0', '1.992422433360555e-06', '0.1', '1.992422433374335e-06', '0.2', '0.3', '0.4', '0.01867750880504259', '0.5', '0.8', '0.6', '0.002722413402762714', '0.7', '0.002722413278797551']\r\nwhich is not any row in the file\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.7.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.133.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : 7.4.1\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.0\r\nnumba                 : None\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["I believe in your df1, pandas has used the first line as the header. However, headers should be distinct and hence pandas has added the \".1\", \".2\", \".3\", etc. to all the headers that are the same, i.e. 0, to differentiate them.","Correct @wleong1 - I think this behavior should be documented in the docstring of read_csv for the header argument.","Noted, thanks!","Is anybody working on that? If Not, I could help here.","Hi @rhshadrach \r\n\r\nI noticed that this issue is currently unassigned, and I'm interested in working on it. Could you please assign it to me? I believe I can contribute to document in the docstring of read_csv for the header argument (as per you mentioned).\r\n\r\nThanks!","is this still open for contribution , im new to open source please be kind ?","> is this still open for contribution , im new to open source please be kind ?\r\n\r\n@Dxuian if you noticed above your message, it says that a PR has already been created by @quangngd "],"labels":["Docs","IO CSV","good first issue"]},{"title":"API: Revert 57042 - MultiIndex.names|codes|levels returns tuples","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nManual revert of #57042\r\n\r\nCloses #57607\r\n\r\nThe behavior described in that issue seems quite undesirable, especially for a breaking change.\r\n\r\ncc @mroeschke ","comments":["I would be OK reverting this but would like:\r\n\r\n1. To expose `FrozenList` in `pandas.api.typing`\r\n2. Deprecate the `union` and `difference` methods","> I would be OK reverting this but would like:\r\n> \r\n> 1. To expose `FrozenList` in `pandas.api.typing`\r\n> 2. Deprecate the `union` and `difference` methods\r\n\r\nOkay with doing this in a follow up? (I likely won't be able to return to this for ~1 week in any case)","> Okay with doing this in a follow up? (I likely won't be able to return to this for ~1 week in any case)\r\n\r\nYup that's good","> Deprecate the union and difference methods\r\n\r\nwhat's the reasoning for those deprecations?","> what's the reasoning for those deprecations?\r\n\r\nMainly cleanup. IMO if we keep `FrozenList` and make it available as a public API, that API should be minimal"],"labels":["MultiIndex"]},{"title":"BUG: timedelta.round fails to round to nearest day","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nIt seems like timedelta rounding broke with the 2.2.0 release or the naming convention for rounding frequency is not properly documented.\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nUnder pandas>=2.2.0, I cannot get any rounding to occur whether I use 'D', 'd', 'day', or 'days':\r\n```python\r\n>>> import pandas as pd\r\n>>> print(pd.__version__)\r\n2.2.1\r\n>>> pd.Series([\r\n...     pd.Timedelta('1 days 23:18:00'),\r\n...     pd.Timedelta('2 days 07:00:00')]).round('d')\r\n0   1 days 23:18:00\r\n1   2 days 07:00:00\r\ndtype: timedelta64[ns]\r\n```\r\n\r\n### Expected Behavior\r\n\r\nBefore 2.2.0 timedeltas rounded to the nearest day appropriately:\r\n```python\r\n>>> import pandas as pd\r\n>>> print(pd.__version__)\r\n2.1.4\r\ndtype: timedelta64[ns]\r\n>>> pd.Series([\r\n...     pd.Timedelta('1 days 23:18:00'),\r\n...     pd.Timedelta('2 days 07:00:00')]).round('d')\r\n0   2 days\r\n1   2 days\r\ndtype: timedelta64[ns]\r\n```\r\n\r\n### Installed Versions\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.5.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 126 Stepping 5, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.24.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.0.0\r\npip                   : 23.3\r\nCython                : None\r\npytest                : 7.4.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.0\r\nnumba                 : 0.58.0\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.3\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None","comments":["Tried to reproduce. Your example for .round on a Series fails for me as well, however rounding single timedeltas (e.g. `pd.Timedelta('1 days 23:18:00').round('d')`) works fine.","Thanks for the report! Result of a git bisect:\r\n\r\n```\r\ncommit 6dbeeb4009bbfac5ea1ae2111346f5e9f05b81f4\r\nAuthor: Lumberbot (aka Jack)\r\nDate:   Mon Jan 8 23:24:22 2024 +0100\r\n\r\n    Backport PR #56767 on branch 2.2.x (BUG: Series.round raising for nullable bool dtype) (#56782)\r\n    \r\n    Backport PR #56767: BUG: Series.round raising for nullable bool dtype\r\n    \r\n    Co-authored-by: Patrick Hoefler\r\n```\r\n\r\ncc @phofl\r\n\r\nNote that `ser.dt.round('d')` still works. I don't believe that `Series.round` having special logic for different dtypes is documented anywhere.","Yeah this is a bit weird, DataFrame.round never worked for those, Series.round now follows this behavior. We can either make both work or keep as is","Making this work for DataFrame.round seems undesirable - numeric columns would take the number of digits as an argument whereas timedelta\/datetime would take a frequency. I'd lean toward keeping this as-is."],"labels":["Bug","Needs Triage"]},{"title":"DOC: Clarify doc for converting timestamps to epoch","body":"- [x] closes #56343 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Sorry, I was not meant to close this PR. I closed it by accident when cleaning up my fork"],"labels":["Docs"]},{"title":"BUG: to_parquet on column containing StringArray objects fails","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nrecords = [\r\n    {\"name\": \"A\", \"project\": \"foo\"}, \r\n    {\"name\": \"A\", \"project\": \"bar\"}, \r\n    {\"name\": \"A\", \"project\": pd.NA},\r\n]\r\ndf = pd.DataFrame.from_records(records).astype({\"project\": \"string\"})\r\nagg_df = df.groupby(\"name\").agg({\"project\": \"unique\"})\r\n\r\nagg_df.to_parquet(\"test.parquet\")\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nIt seems that when aggregating on a `\"string\"` type column (potentially nullable), the resulting dataframe uses `StringArray` to represent the aggregated value (here `project`). When attempting to write to parquet, this fails with the following error:\r\n\r\n```\r\npyarrow.lib.ArrowInvalid: (\"Could not convert <StringArray>\\n['foo', 'bar', <NA>]\\nLength: 3, dtype: string with type StringArray: did not recognize Python value type when inferring an Arrow data type\", 'Conversion failed for column project with type object')\r\n```\r\n\r\nI'm not sure why this is happening, because `StringArray` clearly implements the `__arrow_array__` [protocol](https:\/\/arrow.apache.org\/docs\/python\/extending_types.html) that should be allowing this conversion. This also happens if the string is of type `\"string[pyarrow]\"`.\r\n\r\nThe workaround I've been using so far is to re-type the column as `object` prior to the `groupby`. This results in values of type `np.ndarray`, which are parquet-able. But this is suboptimal because we use `read_parquet` upstream that types the column as `\"string\"`, which means that we have to remember to retype it every time we do this aggregation, else it will fail downstream when writing to parquet. It seems like this should work with the column typed as `\"string\"`.\r\n\r\n### Expected Behavior\r\n\r\nIt should successfully write the parquet file without failures.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.17.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.3.0\r\nVersion               : Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : 58.1.0\r\npip                   : 23.0.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["take\r\n","Hm, this looks like Arrow is not liking having a StringArray \"scalar\" inside an object dtype column.\r\n\r\nCalling ``pyarrow.array`` on ``agg_df['project']`` fails (and seems to be the root error)\r\n\r\n```\r\n>>> pa.array(agg_df['project'])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow\/array.pxi\", line 339, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 85, in pyarrow.lib._ndarray_to_array\r\n  File \"pyarrow\/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert <StringArray>\r\n['foo', 'bar', <NA>]\r\nLength: 3, dtype: string with type StringArray: did not recognize Python value type when inferring an Arrow data type\r\n```\r\n\r\nTentatively labelling upstream issue, since the affected code paths go through pyarrow."],"labels":["Bug","Needs Triage"]},{"title":"DOC: Fix PR01 and SA01 errors in pandas.ExcelFile","body":"- [x] xref #57438 \r\n- [x] xref #57417 \r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Thanks for the PR @iridiium - looks like docstring validation is failing here and needs fixed.","I am aware of and currently fixing the issues, sorry for requesting reviews when unneeded.","@iridiium we changed how we specify the pending errors in `code_checks.sh` and you'll have to resolve the conflicts now, sorry about it."],"labels":["Docs","IO Excel"]},{"title":"BUG: Setting a numpy array as a column in Pandas uses only the first column of the array. ","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas \r\nimport numpy as np \r\np = np.array([\r\n              [1,2,3], \r\n              [1,2,3]\r\n             ])\r\n\r\ndf = pandas.DataFrame(columns=['n', 'p'])\r\ndf['n'] = np.array([0,0])\r\ndf['p'] = p \r\n\r\nprint(df)\r\n```\r\n\r\n```\r\n   n  p\r\n0  0  1\r\n1  0  1\r\n```\r\n\r\n### Issue Description\r\n\r\nPassing the columns as separate arrays, but one of the arrays has the wrong dimensions Nx2 instead of Nx1. In that case, the dataframe column 'p' can be assigned that Nx2 array, but only the first column of the array is actually assigned to 'p'. \r\n\r\nWhile this is hard to happen by accident, it's not impossible. \r\n\r\n### Expected Behavior\r\n\r\nEither store each row of the array to the corresponding row of the dataframe or raise a warning\/error for trying to store a NxM array as a column. \r\n\r\nSee for example: \r\n\r\n```python\r\nimport pandas \r\nimport numpy as np \r\n\r\np =          [\r\n              [1,2], \r\n              [1,2]\r\n             ]\r\n\r\ndf = pandas.DataFrame(columns=['n', 'p'])\r\ndf['n'] = [0,0]\r\ndf['p'] = p \r\n\r\nprint(df)\r\n```\r\n\r\n```\r\n   n       p\r\n0  0  [1, 2]\r\n1  0  [1, 2]\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.10.12.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.1.58+\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.25.2\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 23.1.2\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 7.34.0\r\npandas_datareader     : 0.10.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.6.0\r\ngcsfs                 : 2023.6.0\r\nmatplotlib            : 3.7.1\r\nnumba                 : 0.58.1\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : 0.19.2\r\npyarrow               : 14.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.28\r\ntables                : 3.8.0\r\ntabulate              : 0.9.0\r\nxarray                : 2023.7.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Thanks for the report. The current behavior does look problematic to me, and the expected behavior seems reasonable, but I think this needs some more discussion.","Should raise","It is indeed an interesting issue, worth exploring the implicit handling within `pandas`, as well as adjusting strategies. I will continue to keep an eye on it. The following is just a method of avoidance.\r\n\r\nThe array `p` is a two-dimensional array, which will cause the issue. If you want to add each sub-array in the `p` array as independent rows to the 'p' column, you should initialize it directly when creating the `DataFrame`, instead of adding the column afterwards.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\np = np.array([\r\n    [1, 2, 3],\r\n    [1, 2, 3]\r\n])\r\n\r\n# Create a DataFrame containing the 'p' array and initialize the 'n' column\r\ndf = pd.DataFrame({\r\n    'n': [0, 0],\r\n    'p': list(map(list, p))  # Convert each sub-array to a list and assign as independent rows\r\n})\r\n\r\nprint(df)\r\n```\r\n\r\n```\r\n   n          p\r\n0  0  [1, 2, 3]\r\n1  0  [1, 2, 3]\r\n```\r\n\r\nIn this corrected code, `list(map(list, p))` is converting each sub-array in the two-dimensional `NumPy` array to a list. Then, we can assign these lists as independent rows to the 'p' column of the DataFrame.","Thank you, @TinusChen, that is reasonable. \r\n\r\nThe context for this issue is that I was inspecting a codebase that was accidentally passing an Nx2 array instead of an Nx1 to the frame. Now because no warning was raised and because only the first column of the array was assigned, nobody was the wiser to what was happening. \r\n","I think I agree with @jbrockmendel "],"labels":["Bug","Indexing","Needs Discussion"]},{"title":"BUG: DataFrame Interchange Protocol errors on Boolean columns","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\ncloses #55332\r\n\r\n~~needs rebasing onto #57764~~","comments":[],"labels":["Interchange"]},{"title":"DEPR: make_block","body":"- [x] closes #56815 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":[],"labels":["Internals","Deprecate"]},{"title":"BUG: `Timestamp.unit` should reflect changes in components after `Timestamp.replace`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport datetime\r\nimport pandas as pd\r\n\r\nts = pd.Timestamp(\"2023-07-15 23:08:12.134567123\")\r\nprint(ts)  # shows Timestamp('2023-07-16 23:08:12.134567123')\r\nts2 = pd.Timestamp(datetime.datetime.utcfromtimestamp(int(ts.timestamp())))\r\nts2 = ts2.replace(microsecond=ts.microsecond, nanosecond=ts.nanosecond)\r\nprint(ts2)  # shows Timestamp('2023-07-16 23:08:12.134567123')\r\nassert ts == ts2  # fails\n```\n\n\n### Issue Description\n\nWhile repr(ts) and repr(ts2) are equal, direct comparison fails.\n\n### Expected Behavior\n\nts and ts2 shall be equal.\n\n### Installed Versions\n\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.16.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 22.5.0\r\nVersion               : Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : zh_CN.UTF-8\r\nLOCALE                : zh_CN.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.25.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 61.2.0\r\npip                   : 22.3.1\r\nCython                : 3.0.2\r\npytest                : 8.0.2\r\nhypothesis            : 6.46.5\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.1.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : 1.3.8\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.2\r\nnumba                 : None\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 11.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.1\r\nsqlalchemy            : 2.0.19\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : 0.21.0\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None","comments":["Thanks for the report. This is due to the `unit` attributes not matching\r\n\r\n```python\r\nIn [4]: ts.unit\r\nOut[4]: 'ns'\r\n\r\nIn [5]: ts2.unit\r\nOut[5]: 'us'\r\n```\r\n\r\nHowever, I suppose the `unit` should probably reflect changes when `.replace` is used.","take"],"labels":["Bug","Non-Nano"]},{"title":"ENH: No longer always show hour, minute and second components for pd.Interval ","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\n`pd.Interval` behaviour change from #55035 means that hour, minute and second components is always shown regardless of if timezone is included as an argument. The default behaviour until now has been for time to be dropped if 00:00:00. In our use-case this broke some tests and does not provide useful information. \r\n\r\n```\r\n  -    anchor_year  i_interval                  interval   data  is_target\r\n    -0          2019          -1  [2019-07-04, 2019-12-31)   14.5      False\r\n    -1          2019           1  [2019-12-31, 2020-06-28)  119.5       True\r\n    -2          2020          -1  [2020-07-04, 2020-12-31)  305.5      False\r\n    -3          2020           1  [2020-12-31, 2021-06-29)  485.5       True\r\n    +   anchor_year  i_interval                                    interval   data  is_target\r\n    +0         2019          -1  [2019-07-04 00:00:00, 2019-12-31 00:00:00)   14.5      False\r\n    +1         2019           1  [2019-12-31 00:00:00, 2020-06-28 00:00:00)  119.5       True\r\n    +2         2020          -1  [2020-07-04 00:00:00, 2020-12-31 00:00:00)  305.5      False\r\n    +3         2020           1  [2020-12-31 00:00:00, 2021-06-29 00:00:00)  485.5       True\r\n```\n\n### Feature Description\n\nIt would be nice to revert to dropping hour, minute and second components when 00:00:00 and when no timezone is specified. \n\n### Alternative Solutions\n\nN\/A\n\n### Additional Context\n\n_No response_","comments":["To add to this: @ClaireDons is advocating for reverting to the previous behavior (pre-2.2.0).\r\n\r\nIn cases where only dates matter and all the data is timezone naive, adding the hours, minutes and seconds to the `__repr__` of the `pd.Timestamp` adds a lot of visual clutter.\r\n\r\nThis change was not warned for, nor discussed or reviewed in #55035 and #55015, but came as a byproduct of showing the timezone offset.","cc @mroeschke "],"labels":["Bug","Output-Formatting","Regression"]},{"title":"CI: only keep conda cache for one day","body":"Our conda envs are being cached, but that also means that for the dev builds (eg numpy nightly, pyarrow nightly), you don't actually get the latest version, depending on whether some other change happened in the CI setup that would invalidate the cache.\r\n\r\nMaybe ideally we do this _only_ for those builds that test for other dev packages? \r\n\r\nFrom https:\/\/github.com\/mamba-org\/setup-micromamba?tab=readme-ov-file#caching  \r\nRelated issue: https:\/\/github.com\/mamba-org\/setup-micromamba\/issues\/50","comments":["I would also support invalidating all caches daily (the pre-commit caches I think are the only ones that are not affected by changing the conda key). You could update this workflow's cron to daily (and rename the file) https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/.github\/workflows\/cache-cleanup-weekly.yml "],"labels":["CI"]},{"title":"DEPR: groupby.idxmin\/idxmax will all NA values","body":"#54234 deprecated the case of `skipna=False` and any NA value, but for consistency with Series\/DataFrame versions, we also want the case of `skipna=True` with all NA values in a group to also raise. E.g. this should raise a ValueError\r\n\r\n```\r\ndf = pd.DataFrame({'a': [1, 1, 2], 'b': np.nan})\r\ngb = df.groupby(\"a\")\r\nprint(gb.idxmin())\r\n```","comments":[],"labels":["Groupby","Missing-data","Deprecate","Reduction Operations"]},{"title":"COMPAT: Utilize `copy` keyword in `__array__`","body":"https:\/\/github.com\/pandas-dev\/pandas\/pull\/57172 add a copy keyword that will be passed in with `__array__` in Numpy 2.0. Currently it is unused but it would be useful to respect that option when passed","comments":["Linking the numpy issue I opened to discuss the expected semantics for this: https:\/\/github.com\/numpy\/numpy\/issues\/25941 (in several places we now also ignore the `dtype` keyword, but for `copy` that's something we shouldn't do)"],"labels":["Compat"]},{"title":"BUG:  Convertion fails for columns with datetime64[ms]","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nif __name__ == \"__main__\":\r\n    df = pd.DataFrame(\r\n        [\r\n            [\"2023-09-29 02:55:54\"],\r\n            [\"2023-09-29 02:56:03\"],\r\n        ],\r\n        columns=[\"timestamp\"],\r\n        dtype=\"datetime64[ms]\",\r\n    )\r\n\r\n    serialized = df.to_json()\r\n    print(serialized)\r\n    # Got: {\"timestamp\":{\"0\":1695956,\"1\":1695956}}\r\n    # Should be: {\"timestamp\":{\"0\":1695956154000,\"1\":1695956163000}}\r\n    deserialized = pd.read_json(serialized, convert_dates=[\"timestamp\"])\r\n    print(pd.to_datetime(deserialized[\"timestamp\"], unit=\"ms\"))\r\n    # Got:\r\n    # 0   1970-01-01 00:28:15.956\r\n    # 1   1970-01-01 00:28:15.956\r\n    # Instead of:\r\n    # 0   2023-09-29 02:55:54\r\n    # 1   2023-09-29 02:56:03\n```\n\n\n### Issue Description\n\nWhen a dataframe contains a column which dtype is `datetime64[ms]`  and one tries to convert it to json using `df.to_json()` the data does not correspond to the correct value. So trying to convert it back will give the default timestamp.\r\nSee example.\r\n\n\n### Expected Behavior\n\nThe json values for the timestamp should be the correspoinding one for the given date string so it we can restore the correct date\/time.\r\nIt works when using datetime64[ns].\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.4.0-150-generic\r\nVersion               : #167~18.04.1-Ubuntu SMP Wed May 24 00:51:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.1\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : 8.0.2\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.27\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\nNone\r\n\r\n<\/details>\r\n","comments":["xref #55827 ","I encountered this same problem with the to_datetime64() function when upgrading to version 2.2\r\n\r\nAccording to the documentation, until version 2.0.3 the pandas.Timestamp.to_datetime64 function returned \"a numpy.datetime64 object with 'ns' precision\". Since version 2.1.4 that function returns \"a numpy.datetime64 object with same precision\".\r\n\r\nThis change has broken critical parts of my code that assumed conversion with nanosecond precision, and now depending on the decimals of the argument passed the behavior of the function is unpredictable.\r\n\r\nI recommend downgrading to an older version of pandas to resolve it.","If you specify ``date_format=\"iso\"`` in the ``to_json`` call, the round-tripping happens successfully. \r\n\r\n(Note: the dtype will still be ``datetime64[ns]``, though for stuff in range for datetime64ns. I don't know if we should be reading stuff that's in-bounds for datetime64[ns] as non-nano)\r\n\r\nIt looks like we can do better with preserving a dtype for non-nano datetimes, though.\r\n\r\nRunning\r\n```\r\nimport pandas as pd\r\n\r\nif __name__ == \"__main__\":\r\n    df = pd.DataFrame(\r\n        [\r\n            [\"1000-09-29 02:55:54\"],\r\n            [\"1000-09-29 02:56:03\"],\r\n        ],\r\n        columns=[\"timestamp\"],\r\n        dtype=\"datetime64[ms]\",\r\n    )\r\n    import io\r\n    serialized = df.to_json(date_format=\"iso\")\r\n    print(serialized)\r\n    deserialized = pd.read_json(io.StringIO(serialized), convert_dates=[\"timestamp\"])\r\n    print(deserialized)\r\n    print(deserialized.dtypes)\r\n```\r\n\r\nI get object dtype for the deserialized json timestamp column\r\n\r\nOutput\r\n```\r\n{\"timestamp\":{\"0\":\"1000-09-29T02:55:54.000\",\"1\":\"1000-09-29T02:56:03.000\"}}\r\n                 timestamp\r\n0  1000-09-29T02:55:54.000\r\n1  1000-09-29T02:56:03.000\r\ntimestamp    object\r\ndtype: object\r\n```\r\n\r\n"],"labels":["Bug","IO JSON","Non-Nano"]},{"title":"CI\/TST: Use worksteal over loadfile for pytest-xdist","body":"In theory, this should help alleviate a worker that is running tests from a larger file. Let's see if it saves any time on the CI.","comments":[],"labels":["CI"]},{"title":"BUG: ValueError with loc[] = (Regression 2.1.0)","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ndf = pd.DataFrame(index=[1, 1, 2, 2], data=[\"1\", \"1\", \"2\", \"2\"])\r\ndf.loc[df[0].str.len() > 1, 0] = df[0]\r\ndf\n```\n\n\n### Issue Description\n\nGiven code fails on the third line with exception given below\r\nCode executes normally with panda versions <2.1.0\r\n\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 3, in <module>\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 885, in __setitem__\r\n    iloc._setitem_with_indexer(indexer, value, self.name)\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1888, in _setitem_with_indexer\r\n    indexer, value = self._maybe_mask_setitem_value(indexer, value)\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 789, in _maybe_mask_setitem_value\r\n    value = self.obj.iloc._align_series(indexer, value)\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 2340, in _align_series\r\n    return ser.reindex(new_ix)._values\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\series.py\", line 4982, in reindex\r\n    return super().reindex(\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5521, in reindex\r\n    return self._reindex_axes(\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 5544, in _reindex_axes\r\n    new_index, indexer = ax.reindex(\r\n  File \"C:\\Users\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 4433, in reindex\r\n    raise ValueError(\"cannot reindex on an axis with duplicate labels\")\r\nValueError: cannot reindex on an axis with duplicate labels\n\n### Expected Behavior\n\nCode should execute normally with result\r\n   0\r\n1  1\r\n1  1\r\n2  2\r\n2  2\r\n\r\n(No reindexing should be necessary since no rows are selected with code on line 3.)\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : ba1cccd19da778f0c3a7d6a885685da16a072870\r\npython              : 3.9.0.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19041\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 142 Stepping 12, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_Ireland.1252\r\npandas              : 2.1.0\r\nnumpy               : 1.24.2\r\npytz                : 2023.3\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.1\r\npip                 : 22.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report!\r\n\r\n> (No reindexing should be necessary since no rows are selected with code on line 3.)\r\n\r\nTo be sure, it's not the left hand side that is reindexing, it's the right. E.g.\r\n\r\n    df.loc[df[0].str.len() > 1, 0] = 5\r\n\r\nworks. I believe we raise anytime the RHS has a duplicate value because the result _can be_ ambiguous, even though it won't necessarily be ambiguous. In general we try to avoid values-dependent behavior. In this case, if it just so happens that in one case the mask on the left is all False you may think the code works, but will then fail as soon as it isn't all False. That can be a bad user experience.","> Code executes normally with panda versions <2.1.0\r\n\r\nAh, I missed this! Thanks for that detail. We should run a git blame and see where this ended up getting changed."],"labels":["Bug","Indexing"]},{"title":"PERF: improve StringArray.isna","body":"See https:\/\/github.com\/pandas-dev\/pandas\/issues\/57431#issuecomment-1978293416 for context.\r\n\r\n- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\n\r\nSpeed-up is around 2x for this case (extracted from one of our ASVs):\r\n\r\n```python\r\ndtype = \"string\"\r\nN = 10**6\r\ndata = np.array([str(i) * 5 for i in range(N)], dtype=object)\r\nna_value = pd.NA\r\nser = pd.Series(data, dtype=dtype)\r\n\r\n%timeit ser.isna()\r\n# 11.3 ms \u00b1 47.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)    <-- main\r\n# 5.01 ms \u00b1 55.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)    <-- PR\r\n```\r\n\r\nNot entirely sure it is worth the extra code, but so it definitely gives a decent speedup for a common operation.","comments":[],"labels":["Performance","Strings"]},{"title":"Replace khash + tempita with khashl","body":"supersedes #56432\r\n\r\nthis is just a research project at the moment, but the main thing this solves is using templated functions to achieve better code organization and runtime performance, compared to what's in main.\r\n\r\nHaven't spent a ton of time optimization but first benchmark run shows factorization of a high cardinality column can be over 2x as fast, though factorizationn of a low cardinality column is 2x as slow.\r\n\r\n```sh\r\n| Change   | Before [58e63ec1] <khashl-usage~7^2>   | After [23fc5842] <khashl-usage>   |   Ratio | Benchmark (Parameter)                                                                        |\r\n|----------|----------------------------------------|-----------------------------------|---------|----------------------------------------------------------------------------------------------|\r\n| +        | 1.39\u00b10.04ms                            | 3.05\u00b10.3ms                        |    2.19 | hash_functions.Unique.time_unique_with_duplicates('Int64')                                   |\r\n| +        | 21.4\u00b10.8\u03bcs                             | 40.1\u00b120\u03bcs                         |    1.88 | hash_functions.NumericSeriesIndexingShuffled.time_loc_slice(<class 'numpy.float64'>, 10000)  |\r\n| +        | 2.55\u00b10.09ms                            | 3.96\u00b10.2ms                        |    1.55 | hash_functions.Unique.time_unique('Int64')                                                   |\r\n| +        | 53.5\u00b13\u03bcs                               | 81.6\u00b120\u03bcs                         |    1.52 | hash_functions.NumericSeriesIndexingShuffled.time_loc_slice(<class 'numpy.float64'>, 100000) |\r\n| +        | 3.88\u00b10.3ms                             | 5.85\u00b10.8ms                        |    1.51 | hash_functions.Unique.time_unique_with_duplicates('Float64')                                 |\r\n| +        | 15.3\u00b10.2\u03bcs                             | 17.2\u00b10.6\u03bcs                        |    1.12 | hash_functions.NumericSeriesIndexing.time_loc_slice(<class 'numpy.float64'>, 10000)          |\r\n| -        | 8.46\u00b10.3ms                             | 7.28\u00b10.4ms                        |    0.86 | hash_functions.UniqueAndFactorizeArange.time_unique(5)                                       |\r\n| -        | 9.01\u00b10.2ms                             | 7.48\u00b10.2ms                        |    0.83 | hash_functions.UniqueAndFactorizeArange.time_unique(15)                                      |\r\n| -        | 8.68\u00b10.4ms                             | 7.14\u00b10.3ms                        |    0.82 | hash_functions.UniqueAndFactorizeArange.time_unique(4)                                       |\r\n| -        | 8.84\u00b10.4ms                             | 7.23\u00b10.1ms                        |    0.82 | hash_functions.UniqueAndFactorizeArange.time_unique(9)                                       |\r\n| -        | 8.86\u00b10.4ms                             | 7.14\u00b10.4ms                        |    0.81 | hash_functions.UniqueAndFactorizeArange.time_unique(11)                                      |\r\n| -        | 8.76\u00b10.3ms                             | 6.64\u00b10.1ms                        |    0.76 | hash_functions.UniqueAndFactorizeArange.time_unique(8)                                       |\r\n| -        | 14.4\u00b11ms                               | 7.10\u00b10.8ms                        |    0.49 | hash_functions.UniqueAndFactorizeArange.time_factorize(15)                                   |\r\n| -        | 14.7\u00b10.9ms                             | 6.66\u00b10.3ms                        |    0.45 | hash_functions.UniqueAndFactorizeArange.time_factorize(10)                                   |\r\n| -        | 14.7\u00b11ms                               | 6.58\u00b10.1ms                        |    0.45 | hash_functions.UniqueAndFactorizeArange.time_factorize(11)                                   |\r\n| -        | 15.1\u00b12ms                               | 6.76\u00b10.2ms                        |    0.45 | hash_functions.UniqueAndFactorizeArange.time_factorize(12)                                   |\r\n| -        | 15.4\u00b11ms                               | 6.95\u00b10.3ms                        |    0.45 | hash_functions.UniqueAndFactorizeArange.time_factorize(7)                                    |\r\n| -        | 15.2\u00b11ms                               | 6.92\u00b10.4ms                        |    0.45 | hash_functions.UniqueAndFactorizeArange.time_factorize(8)                                    |\r\n| -        | 14.9\u00b11ms                               | 6.60\u00b10.2ms                        |    0.44 | hash_functions.UniqueAndFactorizeArange.time_factorize(4)                                    |\r\n| -        | 15.4\u00b11ms                               | 6.69\u00b10.1ms                        |    0.44 | hash_functions.UniqueAndFactorizeArange.time_factorize(5)                                    |\r\n| -        | 15.3\u00b11ms                               | 6.77\u00b10.2ms                        |    0.44 | hash_functions.UniqueAndFactorizeArange.time_factorize(6)                                    |\r\n| -        | 15.7\u00b10.8ms                             | 6.27\u00b10.1ms                        |    0.4  | hash_functions.UniqueAndFactorizeArange.time_factorize(9)                                    |\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE DECREASED.\r\n\r\n```","comments":[],"labels":["Performance"]},{"title":"BUG: to_sql does not get the correct dialect","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport urllib.parse\r\n\r\nimport pandas as pd\r\nimport sqlalchemy\r\n\r\nusername = \"SOME_USERNAME\"\r\npassword = urllib.parse.quote(\"SOME_PASSWORD\")\r\nhost = \"SOME_HOST\"\r\nport = SOME_PORT\r\n\r\n# connection to denodo plattform\r\nconn_str = f\"denodo:\/\/{username}:{password}@{host}:{port}\/dv_tutorial\"\r\n\r\nprint(\"________________________\")\r\nprint(conn_str)\r\nprint(type(conn_str))\r\n\r\nengine = sqlalchemy.create_engine(conn_str, echo=True)\r\n\r\n# read is possible\r\nresult_set = engine.execute(\"SELECT * FROM dv_tutorial.test\")\r\nfor row in result_set:\r\n    print(row)\r\n\r\n# write is not possible\r\nwith engine.connect() as conn:\r\n    print(\"BBBBBBBBBBBBBB\")\r\n    print(type(conn))\r\n    df1 = pd.DataFrame({\"name\": [\"User 4\", \"User 5\"]})\r\n    # :arrowdown: this produces the error\r\n    df1.to_sql(\r\n        name=\"test_table\",\r\n        schema=\"dv_tutorial\",\r\n        con=conn.connection,\r\n        index=False,\r\n    )\n```\n\n\n### Issue Description\n\nWe want to connect to denodo Platform and create a table with pd.to_sql(). The output shows in our opinion that it can not get the correct dialect. It tries to check if the table already exists in a SQL Lite DB.\r\n\r\n```\r\n(dbaccess) []$ \/bin\/python3.11 .\/dbaccess\/test.py\r\n________________________\r\ndenodo:\/\/SOME_PARAMS\/dv_tutorial\r\n<class 'str'>\r\n .\/dbaccess\/test.py:20: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https:\/\/sqlalche.me\/e\/b8d9)\r\n  result_set = engine.execute(\"SELECT * FROM dv_tutorial.test\")\r\n2024-03-04 16:23:16,383 INFO sqlalchemy.engine.Engine select current_schema()\r\n2024-03-04 16:23:16,383 INFO sqlalchemy.engine.Engine [raw sql] {}\r\n2024-03-04 16:23:16,428 INFO sqlalchemy.engine.Engine SELECT * FROM dv_tutorial.test\r\n2024-03-04 16:23:16,429 INFO sqlalchemy.engine.Engine [raw sql] {}\r\n(1,)\r\nBBBBBBBBBBBBBB\r\n<class 'sqlalchemy.engine.base.Connection'>\r\n .\/dbaccess\/test.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine\/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\r\n  df1.to_sql(\r\nTraceback (most recent call last):\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 2672, in execute\r\n    cur.execute(sql, *args)\r\npsycopg2.OperationalError: Syntax error: Invalid parameterized expression\r\nDETAIL:  java.sql.SQLException: Syntax error: Invalid parameterized expression\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/s901193\/tkdbaccess\/silas.py\", line 29, in <module>\r\n    df1.to_sql(\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/util\/_decorators.py\", line 333, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py\", line 3084, in to_sql\r\n    return sql.to_sql(\r\n           ^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 842, in to_sql\r\n    return pandas_sql.to_sql(\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 2848, in to_sql\r\n    table.create()\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 984, in create\r\n    if self.exists():\r\n       ^^^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 970, in exists\r\n    return self.pd_sql.has_table(self.name, self.schema)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 2863, in has_table\r\n    return len(self.execute(query, [name]).fetchall()) > 0\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/s901193\/.local\/lib\/python3.11\/site-packages\/pandas\/io\/sql.py\", line 2684, in execute\r\n    raise ex from exc\r\npandas.errors.DatabaseError: Execution failed on sql '\r\n        SELECT\r\n            name\r\n        FROM\r\n            sqlite_master\r\n        WHERE\r\n            type IN ('table', 'view')\r\n            AND name=?;\r\n        ': Syntax error: Invalid parameterized expression\r\nDETAIL:  java.sql.SQLException: Syntax error: Invalid parameterized expression\r\n```\n\n### Expected Behavior\n\nWe want to use pd.to_sql() to write a table into denodo plattform and use the dialect from the string.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.5.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.18.0-513.11.1.el8_9.x86_64\r\nVersion               : #1 SMP Thu Dec 7 03:06:13 EST 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0.post0\r\nsetuptools            : 69.0.3\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.2\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n\r\n\r\nAND pip list \r\n\r\nPackage           Version\r\n----------------- -----------\r\ndenodo-sqlalchemy 20240229\r\ngreenlet          3.0.3\r\nhdbcli            2.19.21\r\nnumpy             1.26.4\r\npandas            2.2.1\r\npip               23.3.1\r\npolars            0.20.13\r\npsycopg2-binary   2.9.9\r\npyarrow           15.0.0\r\npython-dateutil   2.9.0.post0\r\npytz              2024.1\r\nsetuptools        69.0.2\r\nsix               1.16.0\r\nSQLAlchemy        1.4.51\r\nsqlalchemy-hana   1.3.0\r\ntyping_extensions 4.10.0\r\ntzdata            2024.1\r\nwheel             0.42.0","comments":["pandas 2.2 doesn't support sqlalchemy < 2 https:\/\/pandas.pydata.org\/docs\/getting_started\/install.html#sql-databases \r\n\r\nI think there are open issues under discussion to re-add support for 1.4.x","I came across the same issue today when I wanted to upload data to Snowflake using the .to_sql() method.\r\n\r\nLooks like neither snowflake-sqlalchemy nor the latest Apache Airflow release support SQLAlchemy 2 yet. Having support for 1.4.x re-added would be really helpful.","xref: #57049","So @asishm should I close this one?","If you can confirm that it works with pandas<2.2 and sqla 1.4.x, then imo it can be closed as a duplicate of the linked issue. But if you want to wait for a pandas core dev to respond, that's probably fine as well."],"labels":["Bug","Needs Triage"]},{"title":"BUG: fillna() doesn't work with `value=None` without method specified, but method is deprecated","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n>>> s = pd.Series([1,None,3])\r\n>>> s\r\n0    1.0\r\n1    NaN\r\n2    3.0\r\ndtype: float64\r\n>>> s.fillna()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Code\\pandas_dev\\pandas\\pandas\\core\\generic.py\", line 6887, in fillna\r\n    value, method = validate_fillna_kwargs(value, method)\r\n  File \"C:\\Code\\pandas_dev\\pandas\\pandas\\util\\_validators.py\", line 293, in validate_fillna_kwargs\r\n    raise ValueError(\"Must specify a fill 'value' or 'method'.\")\r\nValueError: Must specify a fill 'value' or 'method'.\r\n>>> s.fillna(value=None, method=\"ffill\")\r\n<stdin>:1: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\r\n0    1.0\r\n1    1.0\r\n2    3.0\r\ndtype: float64\n```\n\n\n### Issue Description\n\nThe `method` parameter is deprecated in version 2.1.  But the docs for `Series.fillna()` indicate that a value of `None` is valid.  So you can't do a `fillna()` with a value of `None`\r\n\n\n### Expected Behavior\n\nUnclear.  Do we want to allow `None` as the possible value in fillna()?  This could be a docs issue or a code issue.\r\n\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.4.0-19041-Microsoft\r\nVersion               : #3996-Microsoft Thu Jan 18 16:36:00 PST 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 8.0.1\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["I believe this issue existed prior to the deprecation: it was never possible to just `.fillna(value=None)`. And doing `.fillna(value=None, method=\"ffill\")` would ignore the `value` parameter (since forward filling does not utilize a value). When the deprecation is enforced, we can make the `value` parameter required (instead of having a default), and then enable users to use `.fillna(value=None)`.","Ref: #47639","> I believe this issue existed prior to the deprecation: it was never possible to just `.fillna(value=None)`. And doing `.fillna(value=None, method=\"ffill\")` would ignore the `value` parameter (since forward filling does not utilize a value). When the deprecation is enforced, we can make the `value` parameter required (instead of having a default), and then enable users to use `.fillna(value=None)`.\r\n\r\nSo should we update the docs now to reflect that `Series.fillna(value=None)` doesn't work?  Because the way it reads now, it appears that it should work.  I think that is what led to the confusion of the OP in  https:\/\/github.com\/pandas-dev\/pandas-stubs\/issues\/884\r\n","> So should we update the docs now to reflect that Series.fillna(value=None) doesn't work?\r\n\r\nI would vote no, but not opposed if someone wants to do that. This deficiency in the docs has existed for a long time, and I expect it to work come 3.0, so it doesn't seem worth the effort to change now.","agree with @rhshadrach ","I have a PR in the works for enforcing said deprecation, I'll add a test for this and use that PR to close this issue."],"labels":["Bug","Missing-data"]},{"title":"DOC: SQL-style join conditions","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\nhttps:\/\/pandas.pydata.org\/docs\/getting_started\/comparison\/comparison_with_sql.html\n\n### Documentation problem\n\nNot described how to do more complicated SQL-like joins on conditions, such as \r\n```sql\r\nSELECT *\r\nFROM A\r\nLEFT JOIN B\r\n  ON A.key = B.key AND A.a >= B.b \r\n```\n\n### Suggested fix for documentation\n\nSomething from https:\/\/stackoverflow.com\/questions\/23508351\/how-to-do-workaround-a-conditional-join-in-python-pandas\r\n\r\nThis is a common operation and a canonical answer is useful","comments":["@jxu you could contribute a PR to add this to the docs","I'm not familiar with SQL mechanics but does it actually only merge on rows that `A.a >= B.b` evaluates true or does it match on `A.key = B.key` first and then filter it further with `A.a >= B.b` (meaning there's some intermediate result)?\r\n\r\nIf it's the latter, then I suppose an example showing the merge followed by a query could be done but I believe that's inferable from the current examples.\r\n\r\nTo my knowledge there is no way to do the former with pandas","I'm not actually sure how to do this. My understanding of LEFT JOIN in SQL is logically \r\n1. Cartesian Product\r\n2. Filter rows based on condition\r\n3. Give rows not matched from left table nulls \r\n\r\nSo I think the pandas merge is join but only on keys... you could do what I wrote step by step, but I think the product step would be expensive in implementation ","Merging on the keys and then filtering is usually what SQL does as well (if there is an equality condition). ","That's the logical result, but the SQL engine can run it in a way that's more efficient. But if I write it as two steps in pandas, the whole cross product is calculated before any filtering.","> I'm not familiar with SQL mechanics but does it actually only merge on rows that `A.a >= B.b` evaluates true or does it match on `A.key = B.key` first and then filter it further with `A.a >= B.b` (meaning there's some intermediate result)?\r\n> \r\n> If it's the latter, then I suppose an example showing the merge followed by a query could be done but I believe that's inferable from the current examples.\r\n> \r\n> To my knowledge there is no way to do the former with pandas\r\n\r\nThe SQL join condition is applied once, so there's no logical distinction between the key condition and any other condition.","@jxu I'm not sure what you mean by cross product. A merge on the equal keys is not a cross product","Cartesian product or cross join. Cross product is something for vector spaces ","@jxu if you are running an equi join with a non equi join in pandas, then there is no Cartesian join. The join is executed first on the equi join, followed by the non equi join(which is a filter on the results of the equi join). SQL does the same but in a transparent way - an EXPLAIN plan will likely show an equi join followed by a non equi join filter","ok, I did not know that "],"labels":["Docs"]},{"title":"BUG: Line plots aligned from start #57594","body":"#57594 BUG: df.plot makes a shift to the right if frequency multiplies to n > 1\r\n\r\nThe cause of this bug is that by default all elements are generated to align to the end of\r\na period instead of at the start and the period technically does not end at the start of\r\nthe last period but rather just before the period subsequent to the last period. To fix this\r\nproblem we therefore passed a parameter stating that the elements should be aligned to\r\nthe start of the period instead of the end. Parameter \"how=S\" was added in pandas\/plotting\/_matplotlib\/timeseries.py::maybe_convert_index(), \r\non the function call data.index.asfreq(freq=freq_str, how = \"S\").  \r\n\r\nAdditional tests were added to test whether input data match data in plot and one test was merged into one the added test to reduce code duplication. ","comments":["Thank you for working on this @NoelMT. Do you mind adding plots to this PR (as a comment or in the description) that show how things are rendered before and after your changes please? That would give context to reviewers and it'll make reviewing much easier. Thanks! ","Thanks @NoelMT for working on this. Seems like it will fix the problem with visualisation of `DataFrame` if `PeriodIndex` as index provided.\r\ne.g. for the first scenario:\r\n```\r\nfrom pandas import *\r\nimport numpy as np\r\n\r\nidx = period_range(\"01\/01\/2000\", freq='7h', periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\ndf.plot()\r\nprint(df)\r\n```\r\n\r\nplot before:\r\n\r\n<img width=\"552\" alt=\"res_7h\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/47bccd86-a957-42e1-b021-de15e63cf85e\">\r\n\r\nplot after:\r\n\r\n<img width=\"620\" alt=\"dfplot_7h_correct\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/235ed96a-8b94-4d50-9cb4-181a387e9242\">\r\n","@NoelMT could you please add a note to `doc\/source\/whatsnew\/v3.0.0.rst` in the section\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/dc19148bf7197a928a129b1d1679b1445a7ea7c7\/doc\/source\/whatsnew\/v3.0.0.rst?plain=1#L356\r\n"],"labels":["Visualization"]},{"title":"BUG: Boxplot does not apply colors set by Matplotlib rcParams for certain plot elements","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\ndf = pd.DataFrame(np.random.default_rng(12).random((10, 2)))\r\nwith mpl.rc_context({'boxplot.boxprops.color': 'red',\r\n                     'boxplot.whiskerprops.color': 'green',\r\n                     'boxplot.capprops.color': 'orange',\r\n                     'boxplot.medianprops.color': 'cyan',\r\n                     'patch.facecolor': 'grey'}):\r\n    df.plot.box(patch_artist=True) # OR df.plot(kind='box', patch_artist=True)\r\n    plt.show()\n```\n\n\n### Issue Description\n\nIf the 'Reproducible Example' code is run it will result in the following:\r\n\r\n![pandas-example](https:\/\/github.com\/pandas-dev\/pandas\/assets\/40262729\/f37f4507-2715-4962-b4ec-d9446014093d)\r\n\r\nIf run directly through Matplotlib like so:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\ndf = pd.DataFrame(np.random.default_rng(12).random((10, 2)))\r\nwith mpl.rc_context({'boxplot.boxprops.color': 'red',\r\n                     'boxplot.whiskerprops.color': 'green',\r\n                     'boxplot.capprops.color': 'orange',\r\n                     'boxplot.medianprops.color': 'cyan',\r\n                     'patch.facecolor': 'grey'}):\r\n    plt.boxplot(df, patch_artist=True)\r\n    plt.show()\r\n```\r\nYou end up with this:\r\n\r\n![matplotlib-example](https:\/\/github.com\/pandas-dev\/pandas\/assets\/40262729\/6f456b59-b8f9-48b1-b008-46091291660d)\r\n\r\nAs you can see Pandas completely ignores the rcParams assignment, and sets it's own colours. \r\n\r\nI have only included in this example the exact elements (box, whiskers, caps, medians and box-face) that are ignored. It should also be noted that as the rcParams are ignored, Matplotlib stylesheets are also ignored if applied to these elements.\r\n\r\nAs Pandas does this **only** for these specific elements in the boxplot, it can result in some terrible looking plots if someone uses a comprehensive set of rcParams (or stylesheet) that have a significantly different set of colours.\r\n\r\n### A solution?\r\n\r\nI have looked into where this occurs, and all the relevant code resides in:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/plotting\/_matplotlib\/boxplot.py\r\n\r\nSpecifically, methods `_get_colors()`  and `_color_attrs(self)`. These two methods (among other bits of linked code) basically pick specific colours from the assigned colormap and apply them to the plot.\r\n\r\nI know what needs adjusting, and could put in a PR. However, due to the nature of rcParams being the \"default\" and hence having the lowest priority in terms of application, I see no way to adjust the code without changing the current default colours (i.e. blue, and a green median taken from the \"tab10\" colormap). \r\n\r\nThat is why I am filing this 'bug', as I can see this change might be objectionable, and as such will require further discussion on the appropriate solution. The solution I am proposing, of using matplotlib rcParam defaults, would result in the following \"default\" plot:\r\n\r\n![matplotlib-default](https:\/\/github.com\/pandas-dev\/pandas\/assets\/40262729\/6d2af3ab-a99d-459c-84d5-1e072d2b8d94)\r\n\r\nMy personal opinion is that this visual change is minor, and therefore should be implemented. I would also argue that accessibility is hindered by the current implementation (colour blindness being an example). \r\n\r\n### Items to note\r\n\r\nWhile reviewing the code I noticed the following:\r\n\r\n1.  #40769 is not completely solved as it was only fixed for the method `plot.box` and not `boxplot` (the two methods use different code within `boxplot.py`) - see line 376 of `boxplot.py` for the hardcoded black value for the caps using the method `boxplot` `result = np.append(result, \"k\")`\r\n2.  the section of code refactored by #30346 does not distinguish between `edgecolor` and `facecolor` when `patch_artist` is set to `True`. This may or may not have been intentional, but should probably be separated out as it is the only reason `patch.facecolor` features this current bug report. \r\n\r\n\r\n\n\n### Expected Behavior\n\nIf colours are set in matplotlib rcParams (or stylesheets) by the user, they should be applied to the plot, not ignored. \n\n### Installed Versions\n\n<details>\r\n\r\ncommit                : 69f03a39ecf654e23490cc6d2e103c5e06ccae18\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.7.4-2-MANJARO\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Sat Feb 10 09:41:20 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 3.0.0.dev0+448.g69f03a39ec\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.9.0\r\nsetuptools            : 69.1.1\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 8.0.2\r\nhypothesis            : 6.98.15\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.8\r\nfastparquet           : 2024.2.0\r\nfsspec                : 2024.2.0\r\ngcsfs                 : 2024.2.0\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npyarrow               : 15.0.0\r\npyreadstat            : 1.2.6\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2024.2.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.27\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.2.0\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["I'm interested in taking the bug once it is triaged, if possible as part of a university project.","> I'm interested in taking the bug once it is triaged, if possible as part of a university project.\r\n\r\nAs I mentioned in the bug report, I have already looked into the problem and have a potential solution ready and waiting in the form of a pull request (pending discussion and approval with the Pandas devs of course). \r\n\r\nIn my opinion, it might be better for the Pandas project to concentrate your efforts on a bug that is open, but has no fix offered? There are currently 3.6k issues open, so you should be spoilt for choice.\r\n\r\n\r\n\r\n\r\n"],"labels":["Bug","Needs Triage"]},{"title":"BUG: groupby.agg should always agg","body":"- [x] closes #52362\r\n- [x] closes #39920\r\n- [x] closes #39436\r\n- [x] closes #39169\r\n- [x] closes #28570\r\n- [x] closes #26611\r\n\r\nBuilt on top of #57671; the diff should get better once that's merged. Still plan on splitting part of this up as a precursor (and perhaps multiple).\r\n\r\nFor the closed issues above, tests here still likely need to be added.\r\n\r\nThe goal here is to make groupby.agg more consistently handle UDFs. Currently:\r\n\r\n - We sometimes raise if a UDF returns a NumPy ndarray\r\n - We sometimes treat non-scalars as transforms\r\n - We sometimes fail (non-purposefully) on non-scalars\r\n - We sometimes pass the entire group to the UDF rather than column-by-column\r\n\r\nMy opinion is that we should treat all UDFs as reducers, regardless of what they return. Some alternatives:\r\n\r\n 1. If we detect something as being a non-scalar, try treating it as a transform\r\n 2. Raise on anything detected as being a non-scalar\r\n\r\nFor 1, we will sometimes guess wrong, and transforming isn't something we should be doing in a method called `agg` anyways. For 2, we are restricting what I think are valid use cases for aggregation, e.g. `gb.agg(np.array)` or `gb.agg(list)`.\r\n\r\nIn implementing this, I ran into two issues:\r\n\r\n - `_aggregate_frame` fails if non-scalars are returned by the UDF, and also passes all of the selected columns as a DataFrame to the UDF. This is called when there is a single grouping and args or kwargs are provided, or when there is a single grouping and passing the UDF each column individually fails with a ValueError(\"No objects to concatenate\"). This does not seem possible to fix, would be hard to deprecate (could add a new argument or use a `future` option?), and is bad enough behavior that it seems to me we should just rip the band aid here for 3.0.\r\n - `Resampler.apply` is an alias for `Resample.agg`, and we do not want to impact `Resampler.apply` with these changes. For this, I kept the old paths through groupby specifically for resampler and plan to properly deprecate the current method and implement apply (by calling groupby's apply) as part of 3.x development. (Ref: #38463)\r\n","comments":["I think this is ready for review. Assuming the direction this is moving us is good, still need to decide if we are okay with this being a breaking change in 3.0 (my preference) or if it should be deprecated. If we do go the deprecation route, it will be noisy (many cases where results will be the same but we can't tell so need to warn). The only way I see a deprecation working is if we add an option, e.g. `future_groupby_agg` so that users can opt in to the new implementation.\r\n\r\ncc @jorisvandenbossche @MarcoGorelli @Dr-Irv @mroeschke for any thoughts."],"labels":["Enhancement","Groupby","API Design","Needs Discussion","Apply"]},{"title":"Nanosecond fixed precision of DatetimeIndex with time zone information, Fix #53473","body":"The expected behaviour is now correct from issue #53473 The out->picoseond value wasn't set which ended with a Json error regarding the ns calculations.\r\n\r\n- [x] closes #53473\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Hi @WillAyd could you please check the new fixes, we would appreciate that :)"],"labels":["IO JSON"]},{"title":"BUG: inconsisntent construction results between `pd.array` and `pd.Series` for dtype=str","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\npd.array([1, None], dtype=str)\r\n# <NumpyExtensionArray>\r\n# ['1', 'None']\r\n# Length: 2, dtype: str128\r\n\r\npd.Series([1, None], dtype=str).array\r\n# <NumpyExtensionArray>\r\n# ['1', None]\r\n# Length: 2, dtype: object\r\n\r\npd.DataFrame([1, None], dtype=str)[0].array\r\n# <NumpyExtensionArray>\r\n# ['1', None]\r\n# Length: 2, dtype: object\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhile pd.Series returns object dtype when including `None` value in data for `dtype=str`, pd.array convert it to `\"None\"` string value. I am not sure this is intended or already discussed, but seems curious.\r\n\r\nAs investigating in construction process, `pd.array` pass dtype as is (str) but `pd.Sereis` call (via `lib.ensure_string_array`) dtype as \"object\"\r\n  \r\n### using `pd.array` \r\n```python\r\n# https:\/\/github.com\/pandas-dev\/pandas\/blob\/v2.2.1\/pandas\/core\/arrays\/numpy_.py#L127\r\nresult = np.asarray(scalars, dtype=dtype)\r\n```\r\n\r\n### using `pd.Sereis`\r\n```python\r\n# https:\/\/github.com\/pandas-dev\/pandas\/blob\/v2.2.1\/pandas\/core\/construction.py#L800-L802\r\n# lib.ensure_string_array(arr, convert_na_value=False, copy=copy).reshape(shape)\r\n# https:\/\/github.com\/pandas-dev\/pandas\/blob\/v2.2.1\/pandas\/_libs\/lib.pyx#L766\r\nresult = np.asarray(arr, dtype=\"object\")\r\n```\r\n\r\n### Expected Behavior\r\n\r\nAlthough there might be historical reasons behind this divergent behavior, striving for consistency between pd.array and pd.Series for dtype=str would be more intuitive.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\/tmp\/pandas-report\/.venv\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.133.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.5.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : 2024.2.0\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.27\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["I also confirmed the issue on the main branch version '3.0.0.dev0+480.gc71244ad21' with NumPy version '1.26.4' (installing NumPy-2.0.0.dev0 with Pandas-3.0.0.dev0+480.gc71244ad21 resulted in pandas import error without pyarrow installation).\r\n","take","here's what i know, coming from a beginner in contributing to open-source, np.asarray is where the change\/cast happens in the array, so we can't change anything in there. The only thing i can think of now is trying to cast each element and then create the numpy array with different parameters.","and should a cast fail, the element should be kept as is","but when it comes to a None, it can be casted to some dtypes (like string in the example above), that's why its possible to obtain the result with a 'None' ","i have developed a temporary solution but possibly not what one might look for, i separate the None's from the rest of the data and call np.asarray with dtype=<given_dtype> for those values, then i also generate an array for the None's by using np.asarray with dtype=\"object\" only if there are None's in the given set. If there are None's the final output will state the dtype as object like in both pd.Series(...).array and pd.Dataframe(...)[0].array"],"labels":["Bug","Needs Triage"]},{"title":"DEPR: depreecate uppercase units 'MIN', 'MS', 'US', 'NS' in Timedelta and 'to_timedelta'","body":"Deprecated uppercase strings `\u201cMIN\u201d, \u201cMS\u201d, \u201cUS\u201d, \u201cNS\u201d` denoting units from the class `Timedelta` and the method `to_timedelta`.\r\n\r\nThe reason: for period\/offsets these strings are already deprecated. Instead of the uppercase strings we use lowercase: `\u201cmin\u201d, \u201cms\u201d, \u201cus\u201d, \u201cns\u201d`.","comments":[],"labels":["Timedelta","Deprecate"]},{"title":"DOC: RT03 fix for min,max,mean,meadian,kurt,skew","body":"This PR will fix RT03 error for min,max,mean,meadian,kurt,skew methods\r\n\r\n- [x] xref #57416 \r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57682\/","Thanks @YashpalAhlawat, great job.\r\n\r\nThere are couple of details that would be good to address. This docstring is also being reused by `Series` methods, you can see how it looks here:\r\n\r\nhttps:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57682\/docs\/reference\/api\/pandas.Series.kurt.html\r\n\r\nFew things:\r\n- Can  we also remove the `Series.kurt`... methods from the validation as we did with the `DataFrame` methods?\r\n- If you see in the description you added, it says `applying the kurt function to the DataFrame elements` when we are in the `Series` docstring. Can you use a parameter so it says `... Series elements` please?\r\n- Finally, this is not from your PR, but if you see the docstring in the link, it says `scalar or scalar` for the return type, which doesn't make sense. I think for DataFrame it should says `Series or scalar` and for Series it should say simply `scalar`. Can you have a look to see if this is really the case, and make the docstring show the correct types please?\r\n\r\nThanks for the help with this!","> Thanks @YashpalAhlawat, great job.\r\n> \r\n> There are couple of details that would be good to address. This docstring is also being reused by `Series` methods, you can see how it looks here:\r\n> \r\n> https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57682\/docs\/reference\/api\/pandas.Series.kurt.html\r\n> \r\n> Few things:\r\n> \r\n> * Can  we also remove the `Series.kurt`... methods from the validation as we did with the `DataFrame` methods?\r\n> * If you see in the description you added, it says `applying the kurt function to the DataFrame elements` when we are in the `Series` docstring. Can you use a parameter so it says `... Series elements` please?\r\n> * Finally, this is not from your PR, but if you see the docstring in the link, it says `scalar or scalar` for the return type, which doesn't make sense. I think for DataFrame it should says `Series or scalar` and for Series it should say simply `scalar`. Can you have a look to see if this is really the case, and make the docstring show the correct types please?\r\n> \r\n> Thanks for the help with this!\r\n\r\n@datapythonista , All points has been addressed. If you have any other approach in mind to address third point. I will be happy to make changes in my implementation for that.\r\n\r\nThanks\r\n","Thanks for the updates @YashpalAhlawat. It would be good to change the parameters of `name1`, `name2` as needed in a static way. The idea is that we have a template we use in different methods. And when reusing the template we specify the values for each function. So, you can have a value `Series or scalar` or `scalar` as appropriate so the type renders correctly without requiring extra complexity. Does this make sense?","> Thanks for the updates @YashpalAhlawat. It would be good to change the parameters of `name1`, `name2` as needed in a static way. The idea is that we have a template we use in different methods. And when reusing the template we specify the values for each function. So, you can have a value `Series or scalar` or `scalar` as appropriate so the type renders correctly without requiring extra complexity. Does this make sense?\r\n\r\n@datapythonista , I have implemented a solution in a similar manner. It is functioning as expected. I would appreciate your feedback. If you believe there is a better approach, I am open to making the necessary changes.","\/preview\r\n","Thanks a lot for working on this @YashpalAhlawat.\r\n\r\nI've been checking, and it's quite complex how we are generating these docstrings. I think I'm happy to merge your changes as they are if you want, but while it fixes the problems with the docstrings you are fixing here, it adds even a bit more complexity to the function. Also, there will still be docstrings with related problems, for example https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.sum.html will still have the `scalar or scalar` return.\r\n\r\nWhat do you think about replacing `name1` and `name2` and using `return_type` instead containing both values together for all cases (e.g. `DataFrame or Series`, `Series or scalar`, `scalar`)? In some cases, that would require changing the description of the return, to avoid having the type (same as you did in one of the iterations of this PR, focusing on what is being returned conceptually instead of in the types).\r\n\r\nFinally, I think it'd be good to move the code of condition `if ndim == 1:` at the end of the `make_doc` function, so you can have the `if` you added together with the settings of the other `return_type` (`name1` and `name2` now).\r\n\r\nSo, you'll have something like:\r\n\r\n```python\r\n      if ndim == 1:\r\n          return_type = \"Series or scalar\"\r\n          axis_descr = \"{index (0)}\"\r\n      else:\r\n          if base_doc in (_num_doc, _sum_prod_doc):\r\n              return_type = \"scalar\"\r\n          else:\r\n              return_type = \"Series or scalar\"\r\n          axis_descr = \"{index (0), columns (1)}\"\r\n```\r\n\r\nNot sure if what we should do is to remove this function and simplify how docstrings are being reused, but I think this way at least things don't get too much more complicated than now. What do you think?","@datapythonista ,\r\n\r\nI would love to change the code for all. \r\nBut #57683 focuses on removing such code and adding string directly to methods.\r\n\r\nShouldn't I remove all logic and put doc strings directly to methods.\r\n","> Shouldn't I remove all logic and put doc strings directly to methods.\r\n\r\nYes, if you want to do that, I think for this case everybody will surely agree, the complexity here is quite high as you already experienced. If you do it, it's better to do it step by step, we can probably have a PR for every base docstring. Or as you think it makes sense, but not replacing the whole `make_doc` function in a single PR, that'll be very hard to review.\r\n\r\nThanks a lot for the work on this @YashpalAhlawat ","@datapythonista ,\r\n\r\nI have updated the code as per suggestion.\r\n\r\nFor removing all the code and putting plain docstrings in methods. That can be picked separately once this PR is merged.","\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57682\/","\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57682\/","@YashpalAhlawat you'll have to resolve the conflicts, we changed how the ignored errors are specified in `code_checks.sh`."],"labels":["Docs"]},{"title":"BUG: `join` with `list` does not behave like singleton","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport numpy as np\r\n\r\ncat_df = pd.DataFrame({'cat': pd.Categorical(['a', 'v', 'd'])}, index=pd.Index(['a', 'b', 'c'], name='y'))\r\njoin_df = pd.DataFrame({'foo': np.arange(6)}, index = pd.MultiIndex.from_tuples([(0, 'a'), (0, 'b'), (0, 'c'), (1, 'a'), (1, 'b'), (1, 'c')], names=('x', 'y')) )\r\njoin_df.join([cat_df]) # NaNs in the `cat` column\r\njoin_df.join(cat_df) # correct\n```\n\n\n### Issue Description\n\nI would expect the result to be identical.  I am really interested in being able to left join multiple `cat_df`s that share (one of) an index with the `join_df`.\n\n### Expected Behavior\n\nI would expect identical behavior.  I did notice that with the `on` `kwarg`, I get an error that might indicate this is not allowed:\r\n\r\n```python\r\njoin_df.join([cat_df], on='y')\r\n```\r\ngives\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/ilangold\/Projects\/Theis\/pandas\/pandas\/core\/frame.py\", line 10463, in join\r\n    raise ValueError(\r\nValueError: Joining multiple DataFrames only supported for joining on index\r\n```\n\n### Installed Versions\n\nNot sure what's up with the installed version commit since my `git log` looks like\r\n\r\n```\r\ncommit e14a9bd41d8cd8ac52c5c958b735623fe0eae064 (HEAD -> main, origin\/main, origin\/HEAD)\r\nAuthor: Eric Larson <larson.eric.d@gmail.com>\r\nDate:   Wed Feb 28 17:21:32 2024 -0500\r\n\r\n    ENH: Report how far off the formatting is (#57667)\r\n\r\n```\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 52cb549f443f09727448251cfee53227c6bca9d2\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 22.6.0\r\nVersion               : Darwin Kernel Version 22.6.0: Wed Oct  4 21:26:23 PDT 2023; root:xnu-8796.141.3.701.17~4\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 3.0.0.dev0+87.g52cb549f44.dirty\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : 3.0.8\r\npytest                : 8.0.2\r\nhypothesis            : 6.98.13\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.2.0\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.8\r\nfastparquet           : 2024.2.0\r\nfsspec                : 2024.2.0\r\ngcsfs                 : 2024.2.0\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npyarrow               : 15.0.0\r\npyreadstat            : 1.2.6\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2024.2.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.27\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.2.0\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["take","I believe the problem is that whenever join receives a list, that list is evaluated and then it either is concatenated or merged. With the current evaluation method [cat_df] is getting concatenated with join_df while cat_df gets merged with join_df. In my pull request I naively changed this evaluation to fix this issue but now it fails several other join tests. I'll look into the theory about concatenation vs merging in further detail and update the pull request.","Figured out that the simpler way to deal with this is that whenever a list of a single element is passed, convert it into a join with another element. The operation that evaluates the boolean \"can_concat\" has been there for 12 years (doubt it's wrong), however there might have been an oversight for some specific cases of this uncommon practice (passing a list with a single element).","@Dacops I will say that it isn't my _intention_ to pass a single item list, but when you are creating the lists from other things, it can happen."],"labels":["Bug","Needs Triage"]},{"title":"BUG: pyarrow stripping leading zeros with dtype=str","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom io import StringIO\r\n\r\nx = \"\"\"\r\n    AB|000388907|abc|0150\r\n    AB|101044572|abc|0150\r\n    AB|000023607|abc|0205\r\n    AB|100102040|abc|0205\r\n\"\"\"\r\n\r\ndf_arrow = pd.read_csv(\r\n    StringIO(x),\r\n    delimiter=\"|\",\r\n    header=None,\r\n    dtype=str,\r\n    engine=\"pyarrow\",\r\n    keep_default_na=False,\r\n)\r\n\r\ndf_python = pd.read_csv(\r\n    StringIO(x),\r\n    delimiter=\"|\",\r\n    header=None,\r\n    dtype=str,\r\n    engine=\"python\",\r\n    keep_default_na=False,\r\n)\r\n\r\ndf_arrow\r\n        0          1    2    3\r\n0      AB     388907  abc  150\r\n1      AB  101044572  abc  150\r\n2      AB      23607  abc  205\r\n3      AB  100102040  abc  205\r\n\r\ndf_python\r\n        0          1    2     3\r\n0      AB  000388907  abc  0150\r\n1      AB  101044572  abc  0150\r\n2      AB  000023607  abc  0205\r\n3      AB  100102040  abc  0205\n```\n\n\n### Issue Description\n\nwhen I use engine=pyarrow and set dtype to str i am seeing the leading zeros in my numeric columns removed even though the resulting column type is 'O'. When I use the python engine I see that the leading zeros are still there as expected.\n\n### Expected Behavior\n\nI would expect when treating all columns as strings that the leading zeros are retained and the data is unmodified.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.8.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-17-generic\r\nVersion               : #17-Ubuntu SMP PREEMPT_DYNAMIC Thu Jan 11 14:20:13 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.1\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.27\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["can confirm this in 2.2.1","Do you get the same result when you use the `pyarrow.csv.read_csv` method directly? https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.csv.read_csv.html","seems to be a problem with the pyarrow engine in pandas (pandas 2.2.1, pyarrow 15.0.1)\r\n```\r\npandas engine pandas dtype: 01\r\npyarrow engine pandas dtype: 1\r\npandas engine pyarrow dtype: 01\r\npyarrow engine pyarrow dtype: 1\r\npyarrow native: 01\r\n```\r\n\r\n```python\r\nimport io\r\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nfrom pyarrow import csv\r\n\r\ncsv_file = io.BytesIO(\r\n    \"\"\"a\r\n01\"\"\".encode()\r\n)\r\n\r\n\r\nprint(f\"pandas engine pandas dtype: {pd.read_csv(csv_file, dtype=str).iloc[0,0]}\")\r\n\r\ncsv_file.seek(0)\r\nprint(\r\n    f\"pyarrow engine pandas dtype: {pd.read_csv(csv_file, dtype=str, engine='pyarrow').iloc[0,0]}\"\r\n)\r\n\r\ncsv_file.seek(0)\r\nprint(\r\n    f\"pandas engine pyarrow dtype: {pd.read_csv(csv_file, dtype='str[pyarrow]').iloc[0,0]}\"\r\n)\r\n\r\ncsv_file.seek(0)\r\nprint(\r\n    f\"pyarrow engine pyarrow dtype: {pd.read_csv(csv_file, dtype='str[pyarrow]', engine='pyarrow').iloc[0,0]}\"\r\n)\r\n\r\ncsv_file.seek(0)\r\nconvert_options = csv.ConvertOptions(column_types={\"a\": pa.string()})\r\nprint(\r\n    f\"pyarrow native: {csv.read_csv(csv_file, convert_options=convert_options).column(0).to_pylist()[0]}\"\r\n)\r\n\r\n```","take","For context, the reason this is happening is because currrently the `dtype` argument is only handled as post-processing after pyarrow has read the CSV file. So, currently, we let pyarrow read the csv file with inferring (in this case) numerical types, and then afterwards we cast the result to the specified dtype (in this case `str`). So that explains why the leading zeros are lost.\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/e51039afe3cbdedbf5ffd5cefb5dea98c2050b88\/pandas\/io\/parsers\/arrow_parser_wrapper.py#L216\r\n\r\nPyArrow does provide a `column_types` keyword to specify the dtype while reading: https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions\r\n\r\nSo what we need to do is translate the `dtype` keyword in `read_csv` to the `column_types` argument for pyarrow, somewhere here:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/e51039afe3cbdedbf5ffd5cefb5dea98c2050b88\/pandas\/io\/parsers\/arrow_parser_wrapper.py#L120-L134\r\n\r\nAs a first step, I would just try to enable specifying it for a specific column, like `dtype={\"a\": str}`, because for something like `dtype=str` which applies to all columns, with the current pyarrow API you would need to know the column names up front (pyarrow only accepts specifying it per column, not for all columns at once)"],"labels":["Bug","IO CSV","Needs Info","Arrow"]},{"title":"BUG: melt no longer supports id_vars and value_vars collapsing levels accross MultiIndex","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nperiod1 = pd.Period(\"2018Q1\")\r\ndf = pd.DataFrame(\r\n    [[1.0]],\r\n    index=pd.PeriodIndex([period1, period1], name=\"period\"),\r\n    columns=pd.MultiIndex.from_tuples(\r\n        [(\"a\", \"b\")],\r\n        names=[\"x\", \"y\"],\r\n    ),\r\n)\r\n\r\npd.melt(df, id_vars=[\"a\"])\n```\n\n\n### Issue Description\n\nStarting in 2.2.0, pd.melt claims that columns referenced in id_vars or value_vars are not present in a MultiIndex:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"venv\/lib\/python3.11\/site-packages\/pandas\/core\/reshape\/melt.py\", line 74, in melt\r\n    raise KeyError(\r\nKeyError: \"The following id_vars or value_vars are not present in the DataFrame: ['a']\"\r\n```\n\n### Expected Behavior\n\nPrior to 2.2.0, pd.melt melts the dataframe.\r\n\r\nIt appears the behavior was changed in this commit: https:\/\/github.com\/pandas-dev\/pandas\/pull\/55948\/files#diff-a851c9e0a30604de55d051b077b0d9d2ab92d2386863991a2a4cb4444f103bffL44-L50\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.3.0\r\nVersion               : Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T6000\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 8.0.1\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.9.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : 2024.2.0\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2024.2.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : 2024.2.0\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["cc @mroeschke "],"labels":["Bug","Reshaping","Regression"]},{"title":"BUG: Transform() function returns unexpected results with list","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ngame = pd.DataFrame({\r\n    'team' : ['A', 'A', 'B', 'B', 'C', 'C', 'C'],\r\n    'members' : [1, 2, 3, 4, 5, 6, 7]\r\n})\r\n\r\ngame['all_team_members'] = game.groupby('team').members.transform(lambda x : x.tolist())\r\ngame\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nI would like to concatenate the values from column 'members' by group 'team' to form a list, and attach them to a new variable all_team_members. I got the following results:\r\n```\r\n  team  members  all_team_members\r\n0    A        1                 1\r\n1    A        2                 2\r\n2    B        3                 3\r\n3    B        4                 4\r\n4    C        5                 5\r\n5    C        6                 6\r\n6    C        7                 7\r\n```\r\n\r\n\r\n\r\n### Expected Behavior\r\n\r\nThe expected results are:\r\n```\r\n  team  members  all_team_members\r\n0    A        1            [1, 2]\r\n1    A        2            [1, 2]\r\n2    B        3            [3, 4]\r\n3    B        4            [3, 4]\r\n4    C        5         [5, 6, 7]\r\n5    C        6         [5, 6, 7]\r\n6    C        7         [5, 6, 7]\r\n```\r\n\r\nNote, `game.groupby('team').members.apply(lambda x : x.tolist())` worked as expected, so this problem is isolated to just the `transform()` function\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.5.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.22631\r\nmachine               : AMD64\r\nprocessor             : AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.24.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.0.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 7.4.0\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.4.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.2\r\nnumba                 : 0.57.1\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 11.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : 2023.4.0\r\nscipy                 : 1.11.1\r\nsqlalchemy            : None\r\ntables                : 3.8.0\r\ntabulate              : None\r\nxarray                : 2023.6.0\r\nxlrd                  : None\r\nzstandard             : 0.19.0\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. I agree this would be useful to have, but the trouble is in the inference of what to do with the result. `groupby.transform` can take methods that are transforms (return the same shape as the input) or reducers (return a scalar). When a User Defined Function (UDF) returns a list of the same length as the input as is the case here, how do we know which of the two the user desires?\r\n\r\nThe only option I see is to add an argument to transform, e.g. `is_reducer=[[False]|True]` or `is_reducer=[[\"infer\"]|True|False]` which provides the user control over how the result is interpreted. \r\n\r\nAnother option I've considered is to be very strict and require the UDF return a Series or DataFrame with an index the same as an input to be considered a transform, otherwise we assume it's a reducer. However I do not think this is a viable path - e.g. in the past we've recommended users use `.to_numpy()` if they wish to avoid transform's automatic alignment with the input, and this would be incompatible with that.\r\n\r\nYet another option would be to infer scalars (via pandas' `is_scalar`) and assume transform otherwise. This would notably not help the use case of the OP.","Thank you for investigating this. I believe that, intuitively, the behavior of `groupby('x').y.transform()` should be consistent with that of `groupby('x').y.apply()`. When opting for `transform()` over `apply()`, users anticipate obtaining the same results from the groupby() aggregation, with the distinction that the original DataFrame's structure is preserved. It is expected that the newly created variable captures the **aggregate** outcomes for the elements within each group. Therefore, distributing these aggregated values across individual rows seems counterintuitive. Furthermore, the official documentation does not address this aspect of behavior within the context of groupby()\r\n\r\n","> When opting for transform() over apply(), users anticipate obtaining the same results from the groupby() aggregation, with the distinction that the original DataFrame's structure is preserved. It is expected that the newly created variable captures the aggregate outcomes for the elements within each group.\r\n\r\nI think you also need to consider the use case of something like:\r\n\r\n    df.groupby(...).transform(lambda x: x.cumsum())\r\n\r\nThat is, where `func` doesn't reduce but transforms. That is currently supported today, and it seems reasonable to me that users expect it to be supported."],"labels":["Bug","Groupby","Needs Discussion","Transformations"]},{"title":"BUG: to_sql fails for Oracle BLOB columns","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```sql\r\ncreate table attachment\r\n(\r\n    att_id       number not null\r\n,   att_contents blob   not null\r\n)\r\n```\r\n\r\n```python\r\nimport pandas as pd\r\nfrom sqlalchemy import create_engine, text as sql_text\r\n\r\n\r\ncon_src = create_engine(\"oracle+oracledb:\/\/<username>:<password>@<source_database>\")\r\ncon_dst = create_engine(\"oracle+oracledb:\/\/<username>:<password>@<destination_database>\")\r\n\r\nwith con_src.connect() as conn:\r\n    rd = pd.read_sql(sql=sql_text('select * from attachment'), con=conn)\r\n\r\nwith con_dst.connect() as conn:\r\n    rd.to_sql(name=\"attachment\", con=conn, if_exists='append', index=False)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\nCell In[7], line 3\r\n      1 con_dst = eng_test\r\n      2 with con_dst.connect() as conn:\r\n----> 3     rd.to_sql(name=\"attachment\", con=conn, if_exists='append', index=False)\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)\r\n    327 if len(args) > num_allow_args:\r\n    328     warnings.warn(\r\n    329         msg.format(arguments=_format_argument_list(allow_args)),\r\n    330         FutureWarning,\r\n    331         stacklevel=find_stack_level(),\r\n    332     )\r\n--> 333 return func(*args, **kwargs)\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:3084, in NDFrame.to_sql(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\r\n   2886 \"\"\"\r\n   2887 Write records stored in a DataFrame to a SQL database.\r\n   2888 \r\n   (...)\r\n   3080 [(1,), (None,), (2,)]\r\n   3081 \"\"\"  # noqa: E501\r\n   3082 from pandas.io import sql\r\n-> 3084 return sql.to_sql(\r\n   3085     self,\r\n   3086     name,\r\n   3087     con,\r\n   3088     schema=schema,\r\n   3089     if_exists=if_exists,\r\n   3090     index=index,\r\n   3091     index_label=index_label,\r\n   3092     chunksize=chunksize,\r\n   3093     dtype=dtype,\r\n   3094     method=method,\r\n   3095 )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:842, in to_sql(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\r\n    837     raise NotImplementedError(\r\n    838         \"'frame' argument should be either a Series or a DataFrame\"\r\n    839     )\r\n    841 with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:\r\n--> 842     return pandas_sql.to_sql(\r\n    843         frame,\r\n    844         name,\r\n    845         if_exists=if_exists,\r\n    846         index=index,\r\n    847         index_label=index_label,\r\n    848         schema=schema,\r\n    849         chunksize=chunksize,\r\n    850         dtype=dtype,\r\n    851         method=method,\r\n    852         engine=engine,\r\n    853         **engine_kwargs,\r\n    854     )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:2018, in SQLDatabase.to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\r\n   2006 sql_engine = get_engine(engine)\r\n   2008 table = self.prep_table(\r\n   2009     frame=frame,\r\n   2010     name=name,\r\n   (...)\r\n   2015     dtype=dtype,\r\n   2016 )\r\n-> 2018 total_inserted = sql_engine.insert_records(\r\n   2019     table=table,\r\n   2020     con=self.con,\r\n   2021     frame=frame,\r\n   2022     name=name,\r\n   2023     index=index,\r\n   2024     schema=schema,\r\n   2025     chunksize=chunksize,\r\n   2026     method=method,\r\n   2027     **engine_kwargs,\r\n   2028 )\r\n   2030 self.check_case_sensitive(name=name, schema=schema)\r\n   2031 return total_inserted\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:1558, in SQLAlchemyEngine.insert_records(self, table, con, frame, name, index, schema, chunksize, method, **engine_kwargs)\r\n   1555 from sqlalchemy import exc\r\n   1557 try:\r\n-> 1558     return table.insert(chunksize=chunksize, method=method)\r\n   1559 except exc.StatementError as err:\r\n   1560     # GH34431\r\n   1561     # https:\/\/stackoverflow.com\/a\/67358288\/6067848\r\n   1562     msg = r\"\"\"(\\(1054, \"Unknown column 'inf(e0)?' in 'field list'\"\\))(?#\r\n   1563     )|inf can not be used with MySQL\"\"\"\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:1119, in SQLTable.insert(self, chunksize, method)\r\n   1116     break\r\n   1118 chunk_iter = zip(*(arr[start_i:end_i] for arr in data_list))\r\n-> 1119 num_inserted = exec_insert(conn, keys, chunk_iter)\r\n   1120 # GH 46891\r\n   1121 if num_inserted is not None:\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\pandas\\io\\sql.py:1010, in SQLTable._execute_insert(self, conn, keys, data_iter)\r\n    998 \"\"\"\r\n    999 Execute SQL statement inserting data\r\n   1000 \r\n   (...)\r\n   1007    Each item contains a list of values to be inserted\r\n   1008 \"\"\"\r\n   1009 data = [dict(zip(keys, row)) for row in data_iter]\r\n-> 1010 result = conn.execute(self.table.insert(), data)\r\n   1011 return result.rowcount\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416, in Connection.execute(self, statement, parameters, execution_options)\r\n   1414     raise exc.ObjectNotExecutableError(statement) from err\r\n   1415 else:\r\n-> 1416     return meth(\r\n   1417         self,\r\n   1418         distilled_parameters,\r\n   1419         execution_options or NO_OPTIONS,\r\n   1420     )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\sql\\elements.py:516, in ClauseElement._execute_on_connection(self, connection, distilled_params, execution_options)\r\n    514     if TYPE_CHECKING:\r\n    515         assert isinstance(self, Executable)\r\n--> 516     return connection._execute_clauseelement(\r\n    517         self, distilled_params, execution_options\r\n    518     )\r\n    519 else:\r\n    520     raise exc.ObjectNotExecutableError(self)\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1639, in Connection._execute_clauseelement(self, elem, distilled_parameters, execution_options)\r\n   1627 compiled_cache: Optional[CompiledCacheType] = execution_options.get(\r\n   1628     \"compiled_cache\", self.engine._compiled_cache\r\n   1629 )\r\n   1631 compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\r\n   1632     dialect=dialect,\r\n   1633     compiled_cache=compiled_cache,\r\n   (...)\r\n   1637     linting=self.dialect.compiler_linting | compiler.WARN_LINTING,\r\n   1638 )\r\n-> 1639 ret = self._execute_context(\r\n   1640     dialect,\r\n   1641     dialect.execution_ctx_cls._init_compiled,\r\n   1642     compiled_sql,\r\n   1643     distilled_parameters,\r\n   1644     execution_options,\r\n   1645     compiled_sql,\r\n   1646     distilled_parameters,\r\n   1647     elem,\r\n   1648     extracted_params,\r\n   1649     cache_hit=cache_hit,\r\n   1650 )\r\n   1651 if has_events:\r\n   1652     self.dispatch.after_execute(\r\n   1653         self,\r\n   1654         elem,\r\n   (...)\r\n   1658         ret,\r\n   1659     )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1848, in Connection._execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\r\n   1843     return self._exec_insertmany_context(\r\n   1844         dialect,\r\n   1845         context,\r\n   1846     )\r\n   1847 else:\r\n-> 1848     return self._exec_single_context(\r\n   1849         dialect, context, statement, parameters\r\n   1850     )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1988, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r\n   1985     result = context._setup_result_proxy()\r\n   1987 except BaseException as e:\r\n-> 1988     self._handle_dbapi_exception(\r\n   1989         e, str_statement, effective_parameters, cursor, context\r\n   1990     )\r\n   1992 return result\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2346, in Connection._handle_dbapi_exception(self, e, statement, parameters, cursor, context, is_sub_exec)\r\n   2344     else:\r\n   2345         assert exc_info[1] is not None\r\n-> 2346         raise exc_info[1].with_traceback(exc_info[2])\r\n   2347 finally:\r\n   2348     del self._reentrant_error\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1969, in Connection._exec_single_context(self, dialect, context, statement, parameters)\r\n   1967                 break\r\n   1968     if not evt_handled:\r\n-> 1969         self.dialect.do_execute(\r\n   1970             cursor, str_statement, effective_parameters, context\r\n   1971         )\r\n   1973 if self._has_events or self.engine._has_events:\r\n   1974     self.dispatch.after_cursor_execute(\r\n   1975         self,\r\n   1976         cursor,\r\n   (...)\r\n   1980         context.executemany,\r\n   1981     )\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:922, in DefaultDialect.do_execute(self, cursor, statement, parameters, context)\r\n    921 def do_execute(self, cursor, statement, parameters, context=None):\r\n--> 922     cursor.execute(statement, parameters)\r\n\r\nFile C:\\dev\\jupyter\\venv\\Lib\\site-packages\\oracledb\\cursor.py:382, in Cursor.execute(self, statement, parameters, **keyword_parameters)\r\n    380 self._set_input_sizes = False\r\n    381 if parameters is not None:\r\n--> 382     impl.bind_one(self, parameters)\r\n    383 impl.execute(self)\r\n    384 if impl.fetch_vars is not None:\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/cursor.pyx:391, in oracledb.base_impl.BaseCursorImpl.bind_one()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/cursor.pyx:57, in oracledb.base_impl.BaseCursorImpl._bind_values()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/cursor.pyx:98, in oracledb.base_impl.BaseCursorImpl._bind_values_by_name()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/bind_var.pyx:130, in oracledb.base_impl.BindVar._set_by_value()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/var.pyx:86, in oracledb.base_impl.BaseVarImpl._check_and_set_value()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/var.pyx:59, in oracledb.base_impl.BaseVarImpl._check_and_set_scalar_value()\r\n\r\nFile src\\\\oracledb\\\\impl\/base\/connection.pyx:76, in oracledb.base_impl.BaseConnImpl._check_value()\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\r\n```\r\n\r\n### Expected Behavior\r\n\r\nRows are successfully inserted into the table.\r\n\r\n### Installed Versions\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.0.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 85 Stepping 4, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_Switzerland.1252\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.2\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.10.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.23\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n```","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: groupby.apply respects as_index=False if and only if group_keys=True","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'A': [7, -1, 4, 5], 'B': [10, 4, 2, 8]}, index= pd.Index(['i3', 'i2', 'i1', 'i0'], name='i0'))\r\n\r\n################################\r\n# For transforms, like lambda x: x\r\n################################\r\n\r\n# when group_keys=True, apply() respects as_index=False. same is true when grouping by 'i0' or by ['i0', 'A']\r\nprint(df.groupby('A', as_index=True, group_keys=True).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', as_index=False, group_keys=True).apply(lambda x: x, include_groups=False))\r\n\r\n# when group_keys=False, apply() does not respect as_index=False. same is true when grouping by 'i0' or by ['i0', 'A']\r\nprint(df.groupby('A', as_index=True, group_keys=False).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', as_index=False, group_keys=False).apply(lambda x: x, include_groups=False))\r\n\r\n################################\r\n# For non-transform lambda x: pd.DataFrame([x.iloc[0].sum()])\r\n################################\r\n\r\n# when group_keys=True, grouping by data column respects as_index=False.  same is true when grouping by 'i0' or by ['i0', 'A']\r\nprint(df.groupby('A', as_index=True, group_keys=True).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\nprint(df.groupby('A', as_index=False, group_keys=True).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\n\r\n# when group_keys=False, grouping by data column does not respect as_index=False.  same is true when grouping by 'i0' or by ['i0', 'A']\r\nprint(df.groupby('A', as_index=True, group_keys=False).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\nprint(df.groupby('A', as_index=False, group_keys=False).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\ngroupby.apply respects as_index=False if and only if `group_keys=True`, but the documentation suggests that it should only respect `as_index` if `group_keys=False`.\r\n\r\nMy apologies in advance if I'm duplicating an issue or misunderstanding the intended behavior here. I know there has been some relevant discussion in #49543.\r\n\r\n### Expected Behavior\r\n\r\nI don't know what the correct behavior is here.  A simple and easily explainable behavior would be to always respect `as_index=False`. However, to be consistent with the documentation [here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/groupby.html#transformation), transform-like applies should never respect `as_index=False`, and I suppose that non-transform-like applies should respect it:\r\n\r\n> Since transformations do not include the groupings that are used to split the result, the arguments as_index and sort in [DataFrame.groupby()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby) and [Series.groupby()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.groupby.html#pandas.Series.groupby) have no effect.\r\n\r\nWhen `group_keys=True`, the result **does** include the \"groupings that are used to split the result\", so for the same reason that this note gives, `as_index` should have no effect. The current behavior is the opposite, though: `as_index` has an effect only when `group_keys=True`. (despite the description of [group_keys](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.groupby.html), it appears that `apply` includes the group keys in the index if and only if `group_keys=False`, regardless of whether `func` is a transform.)\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.3.0\r\nVersion               : Darwin Kernel Version 23.3.0: Wed Dec 20 21:31:00 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T6020\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n```\r\n\r\n\r\n<\/details>\r\n","comments":["`sort` interaction with `group_keys` is also confusing, but different: ~for transforms, we only sort if `sort=True, group_keys=True`, and in particular we do **not** sort if `sort=True, group_keys=False`.~ For non-transforms, `apply()` sorts if `sort=True`, regardless of the value of `group_keys`.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'A': [7, -1, 4, 5], 'B': [10, 4, 2, 8]}, index= pd.Index(['i3', 'i2', 'i1', 'i0'], name='i0'))\r\n\r\n################################\r\n# For transforms, like lambda x: x\r\n################################\r\n\r\n# when group_keys=True, apply() sorts if and only if sort=True as well.\r\nprint(df.groupby('A', sort=True, group_keys=True).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=True).apply(lambda x: x, include_groups=False))\r\n\r\n# when group_keys=False, never sort.\r\nprint(df.groupby('A', sort=True, group_keys=False).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=False).apply(lambda x: x, include_groups=False))\r\n\r\n################################\r\n# For non-transform lambda x: pd.DataFrame([x.iloc[0].sum()])\r\n################################\r\n\r\n# when group_keys=True, apply() respects sort=True and sort=False.\r\nprint(df.groupby('A', sort=True, group_keys=True).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=True).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\n\r\n# when group_keys=False, apply() respects sort=True and sort=False.\r\nprint(df.groupby('A', sort=True, group_keys=False).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=False).apply(lambda x: pd.DataFrame([x.iloc[0].sum()]), include_groups=False))\r\n```\r\n\r\nedit: see below for comment about `sort` behavior for transforms under `group_keys=True` and `group_keys=False`.","correction for `sort` behavior of transforms:\r\n\r\nRather than following following the usual interpretation of either sort=True or sort=False, it seems that when `group_keys=False`, we reindex back to the original dataframe order. so we return a dataframe with the same exact index as the original. OTOH, when group_keys=True, sort=True really means sort=True, and sort=False really means sort=False. My above examples don't capture this because the keys are unique. but this does:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'A': [7, -1, 4, 7], 'B': [10, 4, 2, 8]}, index= pd.Index(['i3', 'i2', 'i1', 'i0'], name='i0'))\r\n\r\n################################\r\n# For transforms, like lambda x: x\r\n################################\r\n\r\n# when group_keys=True, sort means the usual thing: sort = True means sort by values of group keys. sort = False\r\n# means sort by order of appearance of group keys.\r\nprint(df.groupby('A', sort=True, group_keys=True).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=True).apply(lambda x: x, include_groups=False))\r\n\r\n# when group_keys=False, reindex result to the index of the original dataframe. sort param has no effect.\r\nprint(df.groupby('A', sort=True, group_keys=False).apply(lambda x: x, include_groups=False))\r\nprint(df.groupby('A', sort=False, group_keys=False).apply(lambda x: x, include_groups=False))\r\n```","@mvashishtha - would you be able to condense this back into the OP?"],"labels":["Bug","Needs Triage"]},{"title":"BUG: Cannot use numpy FLS as indicies since pandas 2.2.1","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# List of fixed-length strings\r\nfixed_strings = [\"apple\", \"banana\", \"orange\", \"grape\"]\r\n\r\n# Define the fixed length for the strings\r\nstring_length = 6  # Adjust as needed\r\n\r\n# Create NumPy array of fixed-length strings\r\narr = np.array(fixed_strings, dtype=f\"S{string_length}\")\r\n\r\ndf = pd.DataFrame(pd.Series(arr), columns=[\"fruit\"])\r\n\r\n# Raises NotImplementedError: |S6\r\ndf.set_index(\"fruit\", inplace=True)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe issue shown in the provided code is that when attempting to set the index of the Pandas DataFrame using `df.set_index(\"fruit\", inplace=True)`, a `NotImplementedError` is raised with the message `|S6`.\r\n\r\nThe reason for this error is that the dtype `|S6` is not supported as an index type in Pandas. When you create a NumPy array with a dtype of fixed-length strings using np.array() and attempt to set it as an index in a DataFrame, Pandas tries to convert it to a suitable index type, but `|S6` is not recognized as a valid option for the index.\r\n\r\nThis is applicable to all `S` dtypes. The offending logic was introduced in pandas 2.2.1 by these lines of this commit: https:\/\/github.com\/pandas-dev\/pandas\/commit\/b6fb90574631c84f19f2dbdc68c26d6ce97446b4#diff-fb8a9c322624b0777f3ff7e3ef8320d746b15a2a0b80b7cab3dfbe2e12e06daaR239-R240\r\n\r\nThe code works as expected in pandas 2.2.0.\r\n\r\n### Expected Behavior\r\n\r\nWhat happens when using 2.2.0:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# List of fixed-length strings\r\nfixed_strings = [\"apple\", \"banana\", \"orange\", \"grape\"]\r\n\r\n# Define the fixed length for the strings\r\nstring_length = 6  # Adjust as needed\r\n\r\n# Create NumPy array of fixed-length strings\r\narr = np.array(fixed_strings, dtype=f\"S{string_length}\")\r\n\r\ndf = pd.DataFrame(pd.Series(arr), columns=[\"fruit\"])\r\n\r\ndf.set_index(\"fruit\", inplace=True)\r\n\r\nprint(df)\r\n```\r\n\r\nPrints:\r\n\r\n```\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [b'apple', b'banana', b'orange', b'grape']\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.1.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.6.10-76060610-generic\r\nVersion               : #202401051437~1704728131~22.04~24d69e2 SMP PREEMPT_DYNAMIC Mon J\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : 0.4.1\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.10.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : 2023.10.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.20\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\nNone\r\n\r\n<\/details>\r\n","comments":["cc @phofl","take"],"labels":["Bug","Regression","Needs Triage"]},{"title":"QST: Grouping at fixed time intervals (hours and minutes) regardeless the date and the first row time","body":"### Research\n\n- [X] I have searched the [[pandas] tag](https:\/\/stackoverflow.com\/questions\/tagged\/pandas) on StackOverflow for similar questions.\n\n- [X] I have asked my usage related question on [StackOverflow](https:\/\/stackoverflow.com).\n\n\n### Link to question on StackOverflow\n\nhttps:\/\/stackoverflow.com\/questions\/78062460\/grouping-in-minutes-intervals-starting-at-fixed-time-regardless-the-first-row-ti\n\n### Question about pandas\n\nI have this code intended to count occurrences in 30-minute intervals; the requirement is to have these intervals at fixed starting points, minute 00 and minute 30 of each hour. Regretfully, despite every attempt of mine, the second group is aligned to minute 03 and minute 33.\r\n\r\nI suspect that both the groups are aligned with the first time row and that the first one is correct just by chance. How can I tell the grouper to force the alignment to minutes 00 and 30?\r\n\r\n    # load and prepare data\r\n    df_long_forecast = pd.read_csv('df_long_forecast - reduced.csv')\r\n    df_long_forecast['after_12_max_datetime'] = pd.to_datetime(df_long_forecast['after_12_max_datetime'])\r\n    df_long_forecast['after_12_max_time'] = (df_long_forecast['after_12_max_datetime'] - df_long_forecast['after_12_max_datetime'].dt.normalize())  # timedelta64[ns]\r\n    \r\n    # count the number of maxes happening after 12am (absolute and percentage)\r\n    hist_max = df_long_forecast.groupby(pd.Grouper(key='after_12_max_time', freq='30T',  offset='0T', origin=origin))['Date'].count()\r\n    percent_max = round(hist_max \/ hist_max.sum() * 100, 2)\r\n    display(hist_max)\r\n    \r\n    # count the number of maxes after 12 that result in a profit trade (cannot be MORE than the previous ones)\r\n    df_long_forecast_profit = df_long_forecast[ df_long_forecast['after_12_max_>_9_to_12_high'] > 0 ]\r\n    profit_long = df_long_forecast_profit.groupby(pd.Grouper(key='after_12_max_time', freq='30T',  offset='0T', origin=origin))['Date'].count()\r\n    percent_profit_long = round(profit_long \/ hist_max.sum() * 100, 2)\r\n    display(profit_long)\r\n\r\nHere is the print of the hist_max df\r\n\r\n```none\r\nafter_12_max_time\r\n0 days 12:00:00    24\r\n0 days 12:30:00     5\r\n0 days 13:00:00     7\r\n0 days 13:30:00     5\r\n0 days 14:00:00     5\r\n0 days 14:30:00     4\r\n0 days 15:00:00     4\r\n0 days 15:30:00     1\r\n0 days 16:00:00     5\r\n0 days 16:30:00     7\r\n0 days 17:00:00     1\r\n0 days 17:30:00     6\r\n0 days 18:00:00     1\r\n0 days 18:30:00     1\r\n0 days 19:00:00     1\r\n0 days 19:30:00     6\r\n0 days 20:00:00     3\r\n0 days 20:30:00     0\r\n0 days 21:00:00     6\r\n0 days 21:30:00    19\r\n0 days 22:00:00     8\r\nFreq: 30T, Name: Date, dtype: int64\r\n```\r\nand this is profit_long\r\n\r\n```\r\nafter_12_max_time\r\n0 days 12:03:00     8\r\n0 days 12:33:00     4\r\n0 days 13:03:00     5\r\n0 days 13:33:00     4\r\n0 days 14:03:00     5\r\n0 days 14:33:00     4\r\n0 days 15:03:00     3\r\n0 days 15:33:00     2\r\n0 days 16:03:00     5\r\n0 days 16:33:00     6\r\n0 days 17:03:00     2\r\n0 days 17:33:00     5\r\n0 days 18:03:00     1\r\n0 days 18:33:00     2\r\n0 days 19:03:00     0\r\n0 days 19:33:00     5\r\n0 days 20:03:00     3\r\n0 days 20:33:00     0\r\n0 days 21:03:00     6\r\n0 days 21:33:00    21\r\n0 days 22:03:00     3\r\nFreq: 30T, Name: Date, dtype: int64\r\n```\r\n\r\nThe CSV file with just the pertinent columns can be downloaded from this [link][1].\r\n\r\n\r\n  [1]: https:\/\/www.dropbox.com\/scl\/fi\/mytcuez9yyb841vpj50vo\/df_long_forecast-reduced.csv?rlkey=llnlvviird802ikxmkd3sjw15&dl=0","comments":[],"labels":["Usage Question","Needs Triage"]},{"title":"Potential regression induced by PR #56921","body":"PR #56921 \r\nBenchmarks:\r\n`join_merge.Join.time_join_dataframes_cross` (Python) with sort=True\r\n`timeseries.ResetIndex.time_reset_datetimeindex` (Python) with t=None\r\n`timeseries.ResetIndex.time_reset_datetimeindex` (Python) with t='US\/Eastern'\r\n`join_merge.Join.time_join_dataframes_cross` (Python) with sort=False\r\n\r\n@rhshadrach \r\n![Screenshot 2024-02-27 at 09 58 18](https:\/\/github.com\/pandas-dev\/pandas\/assets\/11835246\/d8600110-4318-4625-9e98-773cd8429539)\r\n\r\n","comments":["This is hitting the added check here:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/737d390381ffd8006f15d738c934acb659b5c444\/pandas\/core\/internals\/managers.py#L1531-L1534\r\n\r\nIt is hit twice for the entire benchmark; i.e. not in a tight loop. The regression is 0.2ms; this is expected. Closing.","Reopening - this popped up on my regression tracker as well. However some of the times exceed 2ms, and should be investigated further.\r\n\r\n - [ ] [eval.Eval.time_chained_cmp](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_chained_cmp)\r\n   - [ ] [engine='numexpr'; threads='all'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_chained_cmp?p-engine=%27numexpr%27&p-threads=%27all%27)\r\n - [ ] [frame_methods.Shift.time_shift](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Shift.time_shift)\r\n   - [ ] [axis=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Shift.time_shift?p-axis=1)\r\n - [ ] [indexing.InsertColumns.time_assign_list_like_with_setitem](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.InsertColumns.time_assign_list_like_with_setitem)\r\n - [ ] [indexing.InsertColumns.time_assign_with_setitem](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.InsertColumns.time_assign_with_setitem)\r\n - [ ] [indexing.InsertColumns.time_insert](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.InsertColumns.time_insert)\r\n - [ ] [indexing.InsertColumns.time_insert_middle](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.InsertColumns.time_insert_middle)\r\n - [ ] [reshape.SparseIndex.time_unstack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.SparseIndex.time_unstack)\r\n - [ ] [timeseries.ResetIndex.time_reset_datetimeindex](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#timeseries.ResetIndex.time_reset_datetimeindex)\r\n   - [ ] [t='US\/Eastern'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#timeseries.ResetIndex.time_reset_datetimeindex?p-t=%27US\/Eastern%27)\r\n   - [ ] [t=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#timeseries.ResetIndex.time_reset_datetimeindex?p-t=None)\r\n"],"labels":["Performance","Regression","Warnings"]},{"title":"BUG: .str.replace repl string incorrectly parsed with pyarrow string dtype","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# Enable pandas 3.0 options\r\npd.options.mode.copy_on_write = True\r\npd.options.future.infer_string = True\r\n\r\npat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\r\nser = pd.Series(['One Two Three', 'Foo Bar Baz'])\r\n\r\nrepl = r\"\\g<three> \\g<two> \\g<one>\"\r\nser.str.replace(pat, repl, regex=True)\r\n\r\nrepl = r\"\\g<2>0\"  # Note that this should be different from r\"\\20\" according to the `re.sub` docs.\r\nser.str.replace(pat, repl, regex=True)\r\n\r\nrepl = r\"\\20\"  # Should throw error since group 20 doesn't exist according to the `re.sub` docs.\r\nser.str.replace(pat, repl, regex=True)\r\n```\r\n\r\n### Issue Description\r\n\r\nThe docs for [`.str.replace`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.str.replace.html) imply that the repl string can be anything that [`re.sub`](https:\/\/docs.python.org\/3\/library\/re.html#re.sub) supports when `regex=True`. I think this is true for the current functionality, but isn't when using the pyarrow string dtype (`pd.options.future.infer_string = True`).\r\n\r\nSpecifically, the first 2 cases above throw the following error, but should work:\r\n\r\n```\r\nArrowInvalid: Invalid replacement string: Rewrite schema error: '\\' must be followed by a digit or '\\'.\r\n```\r\n\r\nThe last case above should throw, but instead is incorrectly treated as `\\g<2>0`.\r\n\r\nI guess that this is really an issue in the PyArrow backend rather than in pandas itself.\r\n\r\n### Expected Behavior\r\n\r\nMatches `pd.options.future.infer_string = False` and matches `re.sub`.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.22621\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : English_United Kingdom.1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n```\r\n\r\n<\/details>\r\n","comments":["Full trace for first case:\r\n\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\nCell In[47], line 6\r\n      3 ser = pd.Series(['One Two Three', 'Foo Bar Baz'])\r\n      5 repl = r\"\\g<three> \\g<two> \\g<one>\"\r\n----> 6 ser.str.replace(pat, repl, regex=True)\r\n      8 repl = r\"\\g<2>0\"  # Note that this is different from r\"\\20\".\r\n      9 ser.str.replace(pat, repl, regex=True)\r\n\r\nFile ...\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:137, in forbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper(self, *args, **kwargs)\r\n    132     msg = (\r\n    133         f\"Cannot use .str.{func_name} with values of \"\r\n    134         f\"inferred dtype '{self._inferred_dtype}'.\"\r\n    135     )\r\n    136     raise TypeError(msg)\r\n--> 137 return func(self, *args, **kwargs)\r\n\r\nFile ...\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:1567, in StringMethods.replace(self, pat, repl, n, case, flags, regex)\r\n   1564 if case is None:\r\n   1565     case = True\r\n-> 1567 result = self._data.array._str_replace(\r\n   1568     pat, repl, n=n, case=case, flags=flags, regex=regex\r\n   1569 )\r\n   1570 return self._wrap_result(result)\r\n\r\nFile ...\\Lib\\site-packages\\pandas\\core\\arrays\\string_arrow.py:417, in ArrowStringArray._str_replace(self, pat, repl, n, case, flags, regex)\r\n    414     return super()._str_replace(pat, repl, n, case, flags, regex)\r\n    416 func = pc.replace_substring_regex if regex else pc.replace_substring\r\n--> 417 result = func(self._pa_array, pattern=pat, replacement=repl, max_replacements=n)\r\n    418 return type(self)(result)\r\n\r\nFile ...\\Lib\\site-packages\\pyarrow\\compute.py:263, in _make_generic_wrapper.<locals>.wrapper(memory_pool, options, *args, **kwargs)\r\n    261 if args and isinstance(args[0], Expression):\r\n    262     return Expression._call(func_name, list(args), options)\r\n--> 263 return func.call(args, options, memory_pool)\r\n\r\nFile ...\\Lib\\site-packages\\pyarrow\\_compute.pyx:385, in pyarrow._compute.Function.call()\r\n\r\nFile ...\\Lib\\site-packages\\pyarrow\\error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ...\\Lib\\site-packages\\pyarrow\\error.pxi:91, in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Invalid replacement string: Rewrite schema error: '\\' must be followed by a digit or '\\'.\r\n```\r\n\r\n<\/details>","__Update__: I've updated the issue to reflect the fact that I think there are wider issues with the repl string parsing than just named groups."],"labels":["Bug","Needs Triage"]},{"title":"ENH: Make ExtensionArray a Protocol","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nThe current pandas ExtensionArray is a standard Python class, and throughout our code base we do things like `isinstance(obj, ExtensionArray)` to determine at runtime if an object is an instance of the ExtensionArray.\r\n\r\nWhile this works for classes implemented purely in Python that may inherit from a Python class, it does not work with extension classes that are implemented in either Cython, pybind11, nanobind, etc... See https:\/\/cython.readthedocs.io\/en\/latest\/src\/userguide\/extension_types.html#subclassing for documentation of this limitation in Cython\r\n\r\nAs such, unless you implement your extension purely in Python it will not work correctly as an ExtensionArray\r\n\r\n### Feature Description\r\n\r\nPEP 544 describes the [runtime_checkable](https:\/\/peps.python.org\/pep-0544\/#runtime-checkable-decorator-and-narrowing-types-by-isinstance) decorator that in theory can solve this issue without any major changes to our code base (ignoring any performance implications for now)\r\n\r\n### Alternative Solutions\r\n\r\nNot sure there are any - I may be wrong but I do not think extension types in Python can inherit from Python types\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["> Not sure there are any - I may be wrong but I do not think extension types in Python can inherit from Python types\r\n\r\nCan't you\r\n\r\n```\r\ncdef class ActualImplementation:\r\n    [most of the implementation]\r\n\r\nclass MyEA(ActualImplementation, ExtensionArray):\r\n    pass\r\n```\r\n\r\nThat's basically what we do with `NDArrayBacked`.","That works until you try to call an method of the extension class. `MyEA().copy()` will return an instance of `ActualImplementation` not of `MyEA`","Ah I take that back - OK cool I'll have to look more into what Cython is doing to make that maintain the MyEA type. Was not getting this with nanobind so must be a Cython feature:\r\n\r\n```\r\nimport numpy as np\r\n\r\nfrom pandas.api.extensions import ExtensionArray\r\nfrom pandas._libs.arrays import NDArrayBacked\r\n\r\n\r\nclass MyEA(NDArrayBacked, ExtensionArray):\r\n    ...\r\n\r\narr = MyEA(np.arange(3), np.int64)\r\nassert type(arr) == type(arr.copy())\r\n```","Yah I\u2019m implicitly assuming the implementation returns type(self) instead\r\nof hard-coding the class there. That seems pretty harmless to me.\r\n\r\nOn Mon, Feb 26, 2024 at 1:20 PM William Ayd ***@***.***>\r\nwrote:\r\n\r\n> That works until you try to call an method of the extension class.\r\n> MyEA().copy() will return an instance of ActualImplementation not of MyEA\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/57633#issuecomment-1965310556>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AB5UM6CFMVNUFOA6VFEDQF3YVT4BNAVCNFSM6AAAAABD2Z4EU2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNRVGMYTANJVGY>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n","> PEP 544 describes the [runtime_checkable](https:\/\/peps.python.org\/pep-0544\/#runtime-checkable-decorator-and-narrowing-types-by-isinstance) decorator that in theory can solve this issue without any major changes to our code base (ignoring any performance implications for now)\r\n\r\nI think `isinstance` checks on a protocol are more expensive than on concrete classes: comparing all symbols (protocol) vs just checking `__mro__` (concrete class)","Yea there is going to be some performance overhead, I think especially before Python 3.12. How much that matters I don't know - I am under the impression we aren't doing these checks in a tight loop but if you have ideas on what to benchmark happy to profile"],"labels":["Enhancement","ExtensionArray"]},{"title":"Potential regression induced by PR #57302","body":"PR #57302 may have induced a performance regression. If it was a necessary behavior change, this may have been expected and everything is okay.\n\nPlease check the links below. If any ASVs are parameterized, the combinations of parameters that a regression has been detected for appear as subbullets.\n\n - [reshape.ReshapeExtensionDtype.time_stack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack)\n   - [dtype='Period[s]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack?p-dtype=%27Period%5Bs%5D%27)\n   - [dtype='datetime64[ns, US\/Pacific]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack?p-dtype=%27datetime64%5Bns%2C%20US\/Pacific%5D%27)\n - [reshape.ReshapeMaskedArrayDtype.time_stack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack)\n   - [dtype='Float64'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack?p-dtype=%27Float64%27)\n   - [dtype='Int64'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack?p-dtype=%27Int64%27)\n - [reshape.SimpleReshape.time_stack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.SimpleReshape.time_stack)\n\nSubsequent benchmarks may have skipped some commits. The link below lists the commits that are between the two benchmark runs where the regression was identified.\n\n[Commit Range](https:\/\/github.com\/pandas-dev\/pandas\/compare\/99a30a6d0b2765d5769b18a9d0da11f149bb7366...44c50b20e8d08613144b3353d9cd0844a53bd077)\n\ncc @rhshadrach\n","comments":[],"labels":["Performance","Reshaping","Regression"]},{"title":"ENH: `(Styler|DataFrame).to_typst()`","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\ntypst is a new markup language for scientific documents https:\/\/github.com\/typst\/typst. i think of it as latex for the 21 century.\n\n### Feature Description\n\ni've used `(df|styler).to_latex` many times and would find an equivalent `(df|styler).to_typst` equally useful now that i've transitioned to Typst.\n\n### Alternative Solutions\n\nexport to JSON\/CSV and [use Typst native JSON\/CSV import feature](https:\/\/typst.app\/docs\/reference\/data-loading) and generate table in Typst rather than in pandas. drawback: looses styling and all customization options available with `Styler`\n\n### Additional Context\n\n_No response_","comments":["Thanks for the request. I think we'll have to balance the popularity of the format vs the maintenance burden of `to_typst`. If adding in `to_typst` is not at all involved, then I think I'd be open to it. However, if it rivals the implementation `to_latex` while being an order of magnitude less popular (not sure - but I'd guess that's currently the case), then I think we should hold off.\r\n\r\n[pandoc](https:\/\/pandoc.org\/) now support typst - can you try converting the output of `to_latex` to typst using that for your use cases and let us know if any difficulties are encountered?\r\n\r\ncc @attack68 ","Agreed, nothing more to add. I think we would also need multiple developers familiar with typst to ensure that any development was pursuing the right direction.\r\n\r\n`to_latex` was a huge implementation: #40422 with many follow ups and add ons.\r\n`to_string` was a very quick job: #44502 \r\n\r\nA compromise would be allow a PR to be submitted on the scale of `to_string` and see if there are any follow up issues from users."],"labels":["Enhancement","Needs Discussion","IO LaTeX"]},{"title":"BUG: queries on categorical string columns in HDFStore.select() return unexpected results","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nmodels = pd.api.types.CategoricalDtype(categories=['name','longname','verylongname'])\r\ndf = pd.DataFrame({'modelId':['name','longname','longname'],\r\n                   'value'  :[1,2,3]}\r\n                ).astype({'modelId':models,'value':int})\r\nwith pd.HDFStore('test.h5','w') as store:\r\n    store.append('df',df,data_columns=['modelId'])\r\n\r\nwith pd.HDFStore('test.h5','r') as store:\r\n    \r\n    model = 'name'\r\n    print(store.select('df', 'modelId == model'))\r\n\r\n    model = 'longname'\r\n    print(store.select('df', 'modelId == model'))\r\n\r\n    model = 'verylongname'\r\n    print(store.select('df', 'modelId == model'))\n```\n\n\n### Issue Description\n\nThis seems to be the same or very similar to bug #39189 (regression?)\r\n\r\nI encountered the same behavior under pandas 2.2.0 and 2.2.1\n\n### Expected Behavior\n\nThe DataFrame stored in the HDF store is:\r\n\r\n```\r\n>>> df\r\n0      name      1\r\n1  longname      2\r\n2  longname      3\r\n\r\n```\r\nThe 3 print statements give:\r\n\r\n```\r\n# Below should have matched row 0 instead of Empty\r\n>>> model = 'name'\r\n>>> print(store.select('df', 'modelId == model'))\r\nEmpty DataFrame\r\nColumns: [modelId, value]\r\nIndex: []\r\n\r\n# Below matched row 0 but should have matched rows 1 and 2\r\n>>> model = 'longname'\r\n>>> print(store.select('df', 'modelId == model'))\r\n  modelId  value\r\n0    name      1\r\n\r\n# This is correct\r\n>>> model = 'verylongname'\r\n>>> print(store.select('df', 'modelId == model'))\r\nEmpty DataFrame\r\nColumns: [modelId, value]\r\nIndex: []\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : bdc79c146c2e32f2cab629be240f01658cfb6cc2\r\npython                : 3.11.8.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.1.69-gentoo-dist-hardened\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Thu Jan 18 22:03:57 CET 2024\r\nmachine               : x86_64\r\nprocessor             : AMD Ryzen 5 3600X 6-Core Processor\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_BE.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.1\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : None\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.25\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.1.1\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : None\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"BUG: Possible bad api side effect of replacing FrozenList with tuple","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\ndf = pd.DataFrame({'name': ['dale', 'frank'], 'age': [10, 20]})\r\ndf = df.set_index('name')\r\n\r\nindex_names = df.index.names\r\n\r\ndf = df.reset_index()\r\ndf.set_index(index_names)\r\n\r\n# KeyError: \"None of [('name',)] are in the columns\"\n```\n\n\n### Issue Description\n\nThis might not be considered a bug, but I would expect there to be symmetry in `df.set_index` and `df.index.names`. Previously when `index.names` was a FrozenList, this would work. However a tuple has different semantics than a list throughout pandas.\n\n### Expected Behavior\n\nI would expect `Index.names` to work for `set_index`.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f01ae209da4ecca9c7b260511886d86e61856c21\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.7.4-arch1-1\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Mon, 05 Feb 2024 22:07:49 +0000\r\nmachine               : x86_64\r\nprocessor             :\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+431.gf01ae209da\r\nnumpy                 : 1.24.4\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.1\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 7.4.0\r\nhypothesis            : 6.98.11\r\nsphinx                : 6.2.1\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.0\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : 1.1\r\npymysql               : 1.0.3\r\npsycopg2              : 2.9.6\r\njinja2                : 3.1.2\r\nIPython               : 8.22.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\nfastparquet           : 2023.4.0\r\nfsspec                : 2023.5.0\r\ngcsfs                 : 2023.5.0\r\nmatplotlib            : 3.7.1\r\nnumba                 : 0.57.0\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npyarrow               : 16.0.0.dev175+g2d44818d0\r\npyreadstat            : 1.2.1\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2023.5.0\r\nscipy                 : 1.10.1\r\nsqlalchemy            : 2.0.14\r\ntables                : 3.8.0\r\ntabulate              : 0.9.0\r\nxarray                : 2023.5.0\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.21.0\r\ntzdata                : 2023.3\r\nqtpy                  : 2.4.0\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["cc @mroeschke @jorisvandenbossche","https:\/\/github.com\/search?q=%22index.names+%3D%3D+%5BNone%5D%22&type=code\r\n\r\nfwiw checking for `df.index.names == [None]` happens pretty frequently. Variants of this type of error is how I ran into the change originally in private repo. Just ran into it with hvplot. The fixes to these are obviously simple, you just wrap `index.names` in a `list`. But unsure what the upside of the change is.","Maybe we could return a `list` here instead of a tuple since `FrozenList` was originally a list subclass.\r\n\r\nSince `codes\/names\/levels` that returned this before are not writable, maybe we don't need the mutability anymore","> maybe we don't need the mutability anymore\r\n\r\nWhat do you mean exactly with this?","Consider something like this\r\n\r\n```\r\ndf.index.names[0] = \"a\"\r\n```\r\n\r\nCurrently such code fails, because the FrozenList is not mutable (and also with the tuple it fails). But if this would return a list instead, that would actually \"work\", in the sense of not raising an error anymore, but I suppose it would not actually update the names?\r\n","> What do you mean exactly with this?\r\n\r\nSorry I meant immutability.\r\n\r\n> Consider something like this\r\n\r\nIt appears for `MultiIndex` that codes\/levels\/names are backed by private _codes\/_level\/_names attributes. If we have the public attributes return lists and the private attributes use tuples we can still achieve \"immutability\" as before. Here's an example from a branch I have that implements this\r\n\r\n```python\r\nIn [5]: df = pd.DataFrame([1], index=pd.MultiIndex(codes=[[0]], levels=[[\"a\"]], names=[0]))\r\n\r\nIn [6]: df\r\nOut[6]: \r\n   0\r\n0   \r\na  1\r\n\r\nIn [9]: df.index.names[0] = \"a\"\r\n\r\nIn [10]: df.index\r\nOut[10]: \r\nMultiIndex([('a',)],\r\n           names=[0])\r\n```","Yes, but so that has the behaviour that I mentioned above of _appearing to work_ (because it no longer raises an error about being immutable), while not actually working.\r\n\r\nI personally think it is perfectly fine that this doesn't work (that's how it always has been, and there are other APIs to change the name), but IMO it would be a UX regression if it no longer fails loudly.","Maybe we also just restore the FrozenList? Not a strong preference but I think the pattern the OP established is pretty reasonable. I would really never expect `df.set_index(index_names)` to act like a tuple indexer"],"labels":["Bug","Blocker","Needs Discussion","Index"]},{"title":"BUG: Unexpected behaviour when inserting timestamps into Series","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndt = pd.to_datetime(['2024-02-24 10:00'])\r\ns = pd.Series([1],index=dt)\r\n\r\ns.loc['2024-02-24 9:57'] = None     # is inserted into s\r\nprint(s)\r\ns.loc['2024-02-24 10:2:30'] = None  # is inserted into s\r\nprint(s)\r\ns.loc['2024-02-24 10:08'] = None    # is _not_ inserted into s\r\nprint(s)\r\n\r\ns.loc['2024-02-24 10:08:00'] = None # is inserted into s\r\nprint(s)\n```\n\n\n### Issue Description\n\nWhen inserting timestamps into a pandas Series, beginning with the format 'yyyy-mm-dd hh:mm', they are inserted correctly, until first time switching to format 'yyyy-mm-dd hh:mm:ss'. From then on format 'yyyy-mm-dd hh:mm' is silently not inserted, ending with seconds required.\r\n\r\nNot sure if this is intended, unexpected behaviour in my opinion.\n\n### Expected Behavior\n\nInsert timestamps as long as their format is parsed unambiguously. \n\n### Installed Versions\n\n<details>\r\npandas                    2.1.4           py311hf62ec03_0\r\n<\/details>\r\n","comments":["cc @jbrockmendel ","take","This is a tough one.  The '2024-02-24 10:08' case goes through the `if self._can_partial_date_slice(reso):` path in DatetimeIndex.get_loc, which in this case returns an empty array.  One possible place to catch this would be to check for it in `Loc._get_setitem_indexer`.  Not sure if there is a less-ugly\/special-case-y place to do it."],"labels":["Bug","Needs Triage"]},{"title":"BUG: df.plot makes a shift to the right if frequency multiplies to n > 1","body":"This bug exists on the latest version of pandas and might be related to the bug reported in #51425.\r\n\r\n`df.plot` DataFrame with `PeriodIndex` works well for frequency with `n=1`, while  f\u043er frequency that multiplies on `n>1` we have a shift to the right.\r\n\r\n\r\n**Reproducible Examples:**\r\n\r\nif freq=\"7h\"\r\n```\r\nfrom pandas import *\r\nimport numpy as np\r\n\r\nidx = period_range(\"01\/01\/2000\", freq='7h', periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\ndf.plot()\r\nprint(df)\r\n```\r\nthe plotting starts from 06:00\r\n\r\n<img width=\"552\" alt=\"res_7h\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/d09c229b-d665-4602-aaec-18f8bc9a6fab\">\r\n\r\n\r\nif freq=\"2h\"\r\n```\r\nidx = period_range(\"01\/01\/2000\", freq='2h', periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\ndf.plot()\r\nprint(df)\r\n```\r\n\r\nwe have a shift on +1 hour\r\n\r\n<img width=\"573\" alt=\"res_2h\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/11dc9fe2-2c3c-42f4-989e-cc0e1ed053b3\">\r\n\r\n\r\nBut if freq=\"h\"\r\n```\r\nidx = period_range(\"01\/01\/2000\", freq='h', periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\ndf.plot()\r\nprint(df)\r\n```\r\n\r\nthe result looks okay:\r\n\r\n<img width=\"560\" alt=\"res_h\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/12b9b607-74c3-4e26-a79a-b965a22ff4b8\">\r\n\r\n\r\n**Expected Behavior:**\r\n\r\nI think for both `freq=\"7h\"` and `freq=\"2h\"` we should start plotting from the point `(00:00, 0.0)` as we do for `freq=\"h\"` \r\n\r\nIn case DataFrame with `DatetimeIndex` plotting works correct, e.g.\r\n```\r\nidx = date_range(\"01\/01\/2000\", freq='7h', periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\ndf.plot()\r\nprint(df)\r\n```\r\n\r\noutput:\r\n<img width=\"550\" alt=\"res_7h_dr\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/8206da68-5462-4584-b3b7-57ac8ad528dc\">\r\n\r\n","comments":["take","take","take","take"],"labels":["Bug","Visualization"]},{"title":"DOC: Documentation website says version 2.2 is unstable","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\nhttps:\/\/pandas.pydata.org\/docs\/index.html\n\n### Documentation problem\n\nA large red banner says 'This is documentation for an unstable development version.'\r\n\r\nBelow that, a small drop-down menu says '2.2 (stable)'.\r\n\r\nClicking the 'Switch to Stable Version' button simply returns to the same page.\r\n\r\n<img width=\"1422\" alt=\"Screenshot 2024-02-23 at 14 06 23\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/7958850\/c223ab99-ebea-47a3-99f3-b330c97c5dcf\">\r\n \n\n### Suggested fix for documentation\n\nModify the website settings so that version 2.2 is identified to be stable (or have the button redirect users to the 2.1.4 documentation).","comments":["Hm, I'm a little conused by why this is happening?\r\n\r\nhttps:\/\/github.com\/pydata\/pydata-sphinx-theme\/issues\/1552 seems to be related\r\n\r\ncc @jorisvandenbossche @mroeschke ","I can confirm. It's very visually disrupting. ","It's not directly clear to me why this is happening for 2.2.1 (https:\/\/pandas.pydata.org\/pandas-docs\/version\/2.2.1\/index.html), but not for 2.2.0 (https:\/\/pandas.pydata.org\/pandas-docs\/version\/2.2.0\/index.html), as both use the exact same version of the sphinx theme","Ah, I suppose this is because the javascript code that decides whether to show the warning or not is \"comparing\" the versions (not just checking exact equality of the strings, https:\/\/github.com\/pydata\/pydata-sphinx-theme\/blob\/fa456d0ea9fcfd4363f7af9cf43f7e84002d19eb\/src\/pydata_sphinx_theme\/assets\/scripts\/pydata-sphinx-theme.js#L470-L474), and so \"2.2.0\" was comparable to \"2.2\", but \"2.2.1\" no longer compares equal (just a hypothesis, didn't actually check)","As a temporary fix, I patched the sources on our web server to:\r\n\r\n- edit `pandas-docs\/version\/2.2.1\/_static\/documentation_options.js` to say `VERSION: '2.2'` instead of `'2.2.1'`\r\n- Run `find .\/ -type f -exec sed -i \"s\/documentation_options.js?v=16656018\/documentation_options.js?v=166560180\/g\" {} \\;` (just adding a 0 to the `?v=` number, to ensure the browser fetches the new one, otherwise even a hard refresh doesn't help)\r\n\r\nWe should still further investigate how to fix it properly for the next release","If we're not able to fix this for the next release, is it just fine to turn off the warning banner for now?\r\n\r\nI think that's relatively easy to do. I'm not sure I can help much with debugging this further - I can't really read javascript."],"labels":["Docs","Blocker"]},{"title":"BUG: DataFrame([Series with different CategoricalIndexes]) results in non-Categorical columns","body":"```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [4]: s1 = pd.Series([1, 2], index=pd.CategoricalIndex([\"a\", \"b\"]))\r\n\r\nIn [5]: s2 = pd.Series([3, 4], index=pd.CategoricalIndex([\"a\", \"c\"]))\r\n\r\nIn [6]: pd.DataFrame([s1, s2]).columns\r\nOut[6]: Index(['a', 'b', 'c'], dtype='object')\r\n```\r\n\r\nI would have expected `CategoricalIndex(['a', 'b', 'c'])`","comments":["Hi @mroeschke, can I take this issue? If the answer is yes, what is the expected behavior when you pass a list series with and without categorical indices? Shouldn't we check the whole list? something like this:\r\n\r\n`all_categorical = all(isinstance(s.index, pandas.CategoricalIndex) for s in data)\r\n`\r\n\r\nand later in the code, change the columns to the right type:\r\n\r\n```\r\nif all_categorical:\r\n    columns = pandas.CategoricalIndex(columns)\r\n```","Sure feel free to work on it (although I am not sure this will be an easy issue to fix). If the indexes are of mixed types the result should be `Index[object]`","I've just created the PR. Please let me know if the changes make sense. As it is, columns will return a ``Index[object]`` if the indices are mixed, and a ``CategoricalIndex[category]``, if all the series had categorical indices. \r\n\r\nI do think it also makes sense to add a test to check this scenario. "],"labels":["Categorical","DataFrame","Constructors"]},{"title":"PERF: groupby.first and groupby.last fallback to apply for EAs","body":"#57589 showed groupby.first and groupby.last are falling back to apply for certain EAs. This should not fallback.","comments":["Do you know the type of eas that fall back?","From the CI run in the linked PR:\r\n\r\n```\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[decimal128(7, 3)] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[string] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[binary] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[time32[s]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[time32[ms]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[time64[us]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[time64[ns]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[date32[day]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_arrow.py::TestArrowArray::test_groupby_agg_extension[date64[ms]] - NotImplementedError\r\nFAILED pandas\/tests\/extension\/test_numpy.py::TestNumpyExtensionArray::test_groupby_agg_extension[float] - NotImplementedError: function is not implemented for this dtype: float64\r\nFAILED pandas\/tests\/extension\/test_numpy.py::TestNumpyExtensionArray::test_groupby_agg_extension[object] - NotImplementedError: function is not implemented for this dtype: object\r\nFAILED pandas\/tests\/extension\/test_sparse.py::TestSparseArray::test_groupby_agg_extension[0] - NotImplementedError: function is not implemented for this dtype: Sparse[float64, 0]\r\nFAILED pandas\/tests\/extension\/test_sparse.py::TestSparseArray::test_groupby_agg_extension[nan] - NotImplementedError: function is not implemented for this dtype: Sparse[float64, nan]\r\nFAILED pandas\/tests\/extension\/decimal\/test_decimal.py::TestDecimalArray::test_groupby_agg_extension - NotImplementedError: function is not implemented for this dtype: decimal\r\nFAILED pandas\/tests\/extension\/test_interval.py::TestIntervalArray::test_groupby_agg_extension - NotImplementedError: function is not implemented for this dtype: interval[float64, right]\r\nFAILED pandas\/tests\/extension\/json\/test_json.py::TestJSONArray::test_groupby_agg_extension - NotImplementedError: function is not implemented for this dtype: json\r\n= 16 failed, 221228 passed, 6664 skipped, 1811 xfailed, 94 xpassed, 40 warnings in 1525.24s (0:25:25) =\r\n```","Thx, none of those is really surprising unfortunately "],"labels":["Groupby","Performance","ExtensionArray","Reduction Operations"]},{"title":"RLS: 2.2.2","body":"Estimated date: Wed Mar 27, 2024","comments":[],"labels":["Release"]},{"title":"BUG: `df.plot` works differently for `freq=\"h\"` and `freq=\"60min\"`","body":"This bug has not been reported and exists on the latest version of pandas.\r\n\r\n**Reproducible Examples:**\r\nif `freq=\"h\"`\r\n```\r\nfrom pandas import *\r\nimport numpy as np\r\n\r\nfrqncy = 'h'\r\nidx = period_range(\"01\/01\/2000\", freq=frqncy, periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\nres = df.plot()\r\nprint(res.freq)\r\nprint(df)\r\n```\r\noutput:\r\n<img width=\"647\" alt=\"output_h\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/3b268767-ae06-47f0-bab9-82d83c934f92\">\r\n\r\nBut if `freq=\"60min\"`\r\n```\r\nfrqncy = '60min'\r\nidx = period_range(\"01\/01\/2000\", freq=frqncy, periods=4)\r\ndf = DataFrame(\r\n    np.array([0, 1, 0, 1]),\r\n    index=idx,\r\n    columns=[\"A\"],\r\n)\r\nres = df.plot()\r\nprint(res.freq)\r\nprint(df)\r\n```\r\nthe result of plotting is different\r\n<img width=\"623\" alt=\"output_60min\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/91160475\/ff4d44c3-2920-4238-bc8c-431f8489b460\">\r\n\r\n\r\n**Expected Behavior:**\r\n\r\nI think for both `freq=\"h\"` and `freq=\"60min\"` plotting should give the same result. The correct behavior should be that what is shown for  `freq=\"h\"`.\r\n","comments":["take"],"labels":["Bug","Visualization"]},{"title":"ENH: pd.series.sample when n > len","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nOftentimes, I have some code that samples from a df \/ series, as in\r\n```\r\ndf.sample(sample_size, random_state=88)\r\n```\r\nand given new data, `sample_size > len(df)`, getting\r\n```\r\n...Cannot take a larger sample than population...\r\n```\r\n\r\nI'd like a way to specify that if `sample_size > len(df)`, I just want all elements back. This is already the case with `.head()`. So I don't really understand why the behaviour is not the same here.\n\n### Feature Description\n\nI can see two possible solutions\r\n\r\n1. As in [here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.head.html), \"If n is larger than the number of rows, this function returns all rows.\"\r\n\r\n2. If keping back-compatibility is a must, then adding a parameter `errors`, with possible values `ignore` or `raise` (default being `raise`, again to keep back-compatibility).\r\n\r\nI'd lean towards (1), but I'd be content with (2)\n\n### Alternative Solutions\n\nI proposed two solutions. Of course, one can always add a line\r\n```\r\nsample_size = min(len(df), sample_size)\r\n```\r\nbefore the call, but I honestly think pandas should provide support for this common case + being consistent with other methods e.g. `head`.\n\n### Additional Context\n\nHappy to contribute with this feature, but first checking here. Just making sure owners would consider it, and nobody is working on this atm.","comments":["IIRC the error comes from numpy.\r\n\r\nThis is what I get\r\n```\r\n  File \"\/Users\/thomasli\/pandas\/pandas\/core\/sample.py\", line 153, in sample\r\n    return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\r\n  File \"numpy\/random\/mtrand.pyx\", line 1001, in numpy.random.mtrand.RandomState.choice\r\nValueError: Cannot take a larger sample than population when 'replace=False\r\n```\r\n\r\nIn that case, in my opinion, I think it's probably best for you to add an if check in your code before the call to sample to restrict the sample size to the length of the dataframe."],"labels":["Enhancement","Error Reporting","Series","Closing Candidate"]},{"title":"DOC: fixing GL08 errors for pandas.IntervalIndex.left, pandas.IntervalIndex.mid and pandas.IntervalIndex.length","body":"All GL08 Errors resolved in the following cases:\r\n\r\nscripts\/validate_docstrings.py --format=actions --errors=GL08 pandas.IntervalIndex.left\r\nscripts\/validate_docstrings.py --format=actions --errors=GL08 pandas.IntervalIndex.mid\r\nscripts\/validate_docstrings.py --format=actions --errors=GL08 pandas.IntervalIndex.length\r\n\r\nxref #57443","comments":["I want to know that what should I do next ? I am not sure what should I fix the error, because some of it I'm not really understand. Or should I just skip it?","pre-commit.ci autofix","@datapythonista just FYI there's an effort to minimize\/remove docstring reuse (https:\/\/github.com\/pandas-dev\/pandas\/pull\/57683, https:\/\/github.com\/pandas-dev\/pandas\/issues\/57578) to minimize complexity of the docstring reuse mechanisms","@j36426052 do you want to continue working in this PR? Besides addressing the comments you'll have to resolve the conflicts.","Sorry that I'm not really sure what part I need to fix. The comment part, should I reuse some comment to let it's structure more complete. I'm not sure how to minimize\/remove docstring. \r\n\r\nAlso the comflict in Numpy Dev, it show \"test-data.xml not found\", but I don't see some document talk about this.\r\n\r\nOr should I just skip the above I mention. Maybe you mean other comflict?"],"labels":["Docs"]},{"title":"CI: Docstring validation is slow","body":"It takes around 22 minutes, for some reason.","comments":["yeah it needs to generate all these docstrings, write them to temporary files, run validation against them...\r\n\r\ntbh I think we should rip all the docstring sharing logic, just have plain old docstrings without substitutions, and let ruff do all the linting and validation in a matter of seconds. this shouldn't be done manually though, maybe I should see if it's automatable\r\n\r\ndoctests can be validated with pytest --doctest-modules","> tbh I think we should rip all the docstring sharing logic, just have plain old docstrings without substitutions,\r\n\r\nBig +1","cool - no idea how difficult this actually is but I'll give it a go next week and see","I started this here: https:\/\/github.com\/pandas-dev\/pandas\/pull\/57683\r\n\r\nI have a little script to do this, which seems to working well enough. Before I go too far, just checking people are OK with it\r\n\r\nPros of keeping docstring substitutions:\r\n- shorter docstrings within the code\r\n- helps dedupe some parameter definitions\r\n\r\nPros of removing docstring substitions:\r\n- docstrings become simple and easy-to-review\r\n- drastically faster docstring checking thanks to ruff\r\n- blazingly fast docstring formatting becomes possible\r\n- unblocks using `darglint` rules (which may make their way into ruff!) https:\/\/github.com\/astral-sh\/ruff\/issues\/458, and any other fast docstring rules in ruff (they already have pydocstyle ones)\r\n\r\nI don't think ruff replaces _all_ of the `validate_docstrings` script, but that script can be used with or without docstring substitutions. But removing docstring substitutions at least means we can start removing things from `validate_docstrings` which are covered by ruff and cut down on CI like this","> yeah it needs to generate all these docstrings, write them to temporary files, run validation against them...\r\n\r\nI get we have a lot of docstrings but I am surprised that this would take 22 minutes.\r\n\r\nNot a hill I'd want to die on but I do think the shared docstrings serve a good purpose in keeping things consistent","Wasn't aware of this issue and wrote #57826 after I introduced a docstring error in my first PR and got annoyed at the speed of the docstring checks. It reduces the runtime of the `check_code.sh docstrings` down to 2-3 minutes again.\r\n\r\nI also have another PR ready to go once #57826 is merged. That second PR brings the validation down to about 10 seconds. Main issue was the way the temp files were created and how pep8 was called via the shell.\r\n\r\nSo maybe with a 10 second runtime it's not necessary to do any functionality changes to the docstring validation and sharing in the end?","> So maybe with a 10 second runtime it's not necessary to do any functionality changes to the docstring validation and sharing in the end?\r\n\r\nI think the docstring sharing is still worth removing to:\r\n\r\n1. Make it easier for new contributors to work on docstrings due to less complexity\r\n2. Allow `ruff` to take over docstring formatting and validation","I'm happy to move towards removing or reducing the docstring sharing. But based on my experience getting fully rid of it may take years. If we want to proactively work on duplicating now reused docstrings, I'd start by the ones with higher complexity and see how that goes, before opening issues to remove all them."],"labels":["CI"]},{"title":"ENH: Add Rich formatting support to Pandas object displays with `__rich_console__` method","body":"### Feature Type\r\n\r\n- [x] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\n[Rich](https:\/\/github.com\/Textualize\/rich) is a very popular and, at this point, a standard tool in the Python ecosystem for beautiful rendering of text in terminals. Pandas's rich rendering via `_repr_html_` is very optimized for the Jupyter ecoystem, but lags behind in the terminal space. As someone who primarily codes in a terminal with a Rich patched REPL, I find this unfortunate and unnecessary. Hence I am adding this feature request in hopes that the situation can be improved. I can work on this if it is agreed that this feature would be handy and useful.\r\n\r\n### Feature Description\r\n\r\nThis is a very rough solution that does not cover all the appropriate challenges, requirements and complexities that would need to be met. It is just to give you an idea that it is possible:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nclass DataFrame(pd.DataFrame):\r\n\r\n    def __rich_console__(self, console: \"rich.console.Console\", console_options: \"rich.console.Console\"):\r\n        from rich.table import Table\r\n        table = Table()\r\n        table.add_column(self.index.name if self.index.name is not None else \"\")\r\n        for c in self.columns:\r\n            table.add_column(str(c))\r\n        for r in self.itertuples():\r\n            table.add_row(*map(str, r))\r\n        yield from table.__rich_console__(console, console_options)\r\n\r\nfrom rich.console import Console\r\n\r\nconsole = Console()\r\n\r\ndf = DataFrame([\r\n    {\"name\": \"John Doe\", \"phone\": \"555-555-5555\", \"last_purchase_amount\": 1.23},\r\n    {\"name\": \"Mary Sue\", \"phone\": \"800-123-4567\", \"last_purchase_amount\": 100.00}\r\n])\r\n\r\nconsole.print(df)\r\n```\r\n\r\nA few things are missing from this solution. Notably, it needs to hook into the Pandas options; it needs to be able to truncate the rows and columns based on the relevant display options. Series objects also need to have a `__rich_console__`. The Styler object should, as well, and this should hook into the Styler object configuration. There should probably be a way to set additional display options for this somewhere (e.g. `display.rich.[...]` and `styler.rich.[...]`). You'd need to hook into the try-except import stuff that Pandas does on the backend for optional dependencies. Lastly, of course, this would all need to be documented.\r\n\r\n### Alternative Solutions\r\n\r\n- Users can just do it themselves, I suppose.\r\n- You can use the `__rich__` method, but I think you'd want to have access to the console parameters when fleshing out the code.\r\n\r\n\r\n### Additional Context\r\n\r\nI could not find any related issues.","comments":["Can you show examples?\r\n\r\nSpecifically with and without a `pandas.MultiIndex` both axis 0 and axis 1? Without and without axis 0 names? What kind of support does `rich` provide for multi indexes?","Looks like there's this external library that supports rich for pandas objects: https:\/\/pypi.org\/project\/rich-dataframe\/","@Delengowski \r\n\r\n> Can you show examples?\r\n\r\nThe example code I provided in the issue outputs like this:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/4986f07a-d94b-4353-bd0d-03656e282d25)\r\n\r\nOne can certainly do additional work however to implement more styling options. Rich has a bunch.\r\n\r\n> Specifically with and without a `pandas.MultiIndex` both axis 0 and axis 1? Without and without axis 0 names? What kind of support does `rich` provide for multi indexes?\r\n\r\nMultiindexes are a good point. Rich does not \"provide support for multindexes\" per se, it's is just a visualization library. The way you render them depends on how you want to render them.\r\n\r\nWith my code example above the following dataframe:\r\n\r\n```python\r\ndf = DataFrame(\r\n    [\r\n        {\"name\": \"John Doe\", \"phone\": \"555-555-5555\", \"last_purchase_amount\": 1.23},\r\n        {\"name\": \"Mary Sue\", \"phone\": \"800-123-4567\", \"last_purchase_amount\": 100.00},\r\n    ],\r\n    index=pd.MultiIndex.from_tuples([(1, 2), (3, 4)], names=[None, None])\r\n)\r\n```\r\n\r\nYou get this:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/f6b057ba-a1ae-4b1e-b320-23a8507c0b7e)\r\n\r\nYou can modify the code to support more functionality, though:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# todo: replace with actual pandas settings\r\nsettings__display_rich_combine_multindex = False\r\n\r\nclass DataFrame(pd.DataFrame):\r\n\r\n    def __rich_console__(self, console: \"rich.console.Console\", console_options: \"rich.console.Console\"):\r\n        from rich.table import Table\r\n        table = Table()\r\n        split_multiindices = False\r\n        # Todo: check if there is a better way to check for multi indices\r\n        if self.index.names and not settings__display_rich_combine_multindex:\r\n            split_multiindices = True\r\n            for name in self.index.names:\r\n                table.add_column(name if name is not None else \"\")\r\n        else:\r\n            if self.index.names:\r\n                name = \",\".join(self.index.names)\r\n            else:\r\n                name = self.index.name\r\n            table.add_column(name if name is not None else \"\")\r\n\r\n        for c in self.columns:\r\n            table.add_column(str(c))\r\n        for r in self.itertuples():\r\n            if split_multiindices:\r\n                row_data = map(str, [*r[0], *r[1:]])\r\n            else:\r\n                row_data = map(str, r)\r\n            table.add_row(*row_data)\r\n\r\n        yield from table.__rich_console__(console, console_options)\r\n\r\nfrom rich.console import Console\r\n\r\nconsole = Console(color_system=\"truecolor\")\r\n\r\ndf = DataFrame(\r\n    [\r\n        {\"name\": \"John Doe\", \"phone\": \"555-555-5555\", \"last_purchase_amount\": 1.23},\r\n        {\"name\": \"Mary Sue\", \"phone\": \"800-123-4567\", \"last_purchase_amount\": 100.00},\r\n    ],\r\n    index=pd.MultiIndex.from_tuples([(1, 2), (3, 4)], names=[\"foo\", \"bar\"])\r\n)\r\n\r\nconsole.print(df)\r\n```\r\n\r\nOutput:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/f121dc74-08bf-4394-af03-afb296ebd300)\r\n\r\nYou can do the thing where you create a new row just for the index names, if you want, as well. Again, it's very composable.\r\n\r\n---\r\n\r\nNote that all of this code is very rough. Implementing this would take a lot, lot more effort than I put into this code right here. I'm just trying to see first whether the Pandas dev team agrees that this should be implemented.","@mroeschke\r\n\r\n> Looks like there's this external library that supports rich for pandas objects: https:\/\/pypi.org\/project\/rich-dataframe\/\r\n\r\nI'm aware, and the owners of that library should definitely be consulted and should be brought into this discussion if we agree that this should exist as a native feature, as they have relevant knowledge and I'm sure they'd love to see this become a native feature + work on it.\r\n\r\nBut, I do think there is a lot of value in making this native, and not just relying on third-party support, even if said third-party support exists.\r\n\r\nI would put it like this: if there was a library called `jupyter-dataframe`, it would still have been a good idea for Pandas to implement `_repr_html_` natively. It would not be a good user experience to tell people, \"well, just use this third party library and wrap everything with `jupyter_dataframe.prettify(df)`. Arguably, Pandas may not have become nearly as popular as it is today if `_repr_html_` wasn't natively implemented.\r\n\r\nSome of the benefits of adding this natively:\r\n\r\n- Just type `>>> df` and get your pretty repr, instead of needing to wrap in `prettify(df)`. That's a massive difference in quality of life for exploratory workflows.\r\n- User does not need to install another library.\r\n- Likely to receive more functionality, bugfixes, and community support. For example, the linked library does not seem to support MultiIndexes, which I was just asked about above.\r\n- People are way more likely to use this in general, I think, if it is officially supported.\r\n\r\nAgain, Rich is popular (46k Github stars), modern, and people just really enjoy it. I'm not proposing pandas-dev creates a new optional dependency on some random no-name library. I'm proposing that terminal-based Pandas workflows should get a modern, native, built-in facelift, in the same spirit that Pandas already supports a quality native experience for Jupyter-based workflows.\r\n\r\nAlso to clarify, I don't necessarily see pointing this out as a tacit disagreement with my argument that it should be native. I do just want to state my position here that for purposes of native support, I don't see it as relevant that a third-party library exists, other than that we could @ the creator of that library and get them involved in this, if we agree it should be native \ud83d\ude04 ","It looks like all the unit-tests for Pandas display related stuff already are:\r\n\r\n- For dataframes reprs: https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/tests\/frame\/test_repr.py\r\n- For series reprs + other reprs: https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/tests\/series\/test_formats.py\r\n- For HTML, console, etc. outputs: https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/tests\/io\/formats (HTML tests seem to be the most comprehensive.)\r\n\r\nObviously, reasonable acceptance criteria would be that a Rich hook can implement all these behaviors but as Rich-formatted tables. There are a few stylistic decisions that one would still have to make even within this constraint, however, but reimplementing the spirit of these tests but for Rich would really narrow down the scope of the problem.","> Note that all of this code is very rough. Implementing this would take a lot, lot more effort than I put into this code right here. I'm just trying to see first whether the Pandas dev team agrees that this should be implemented.\r\n\r\nThanks for this. The first thought that came to mind was merging of cells for multi index. I would think that unless Rich can do that, it's a non starter because displaying rich bases repr for a data frame with multi index that looks different from the regular repr would be incredibly confusing. \r\n\r\n\r\nThat's why I was asking, to see if it has out of box support or if it would be a bit of work.\r\n\r\n","It looks like spans are desired but not yet implemented nor a high priority.\r\n\r\nhttps:\/\/github.com\/Textualize\/rich\/issues\/164","You mean to support this test case?\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/e14a9bd41d8cd8ac52c5c958b735623fe0eae064\/pandas\/tests\/io\/formats\/data\/html\/index_5.html\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/54a89c13-9809-40a0-934a-9acb7caeb541)\r\n\r\nIf you do not include horizontal lines to split out the rows (which is the default behavior of Rich, i.e. `show_lines=False`, and also is likely a reasonable default for Pandas too), then the reasonable 'work around' is just to display a blank string. Because there are no horizontal dividers, this will visually look correct.\r\n\r\nThis only gets finicky when `show_lines=True`, and then you have empty cells. This would not look very good.\r\n\r\nAn option here is (pseudocode):\r\n\r\n```python\r\nif not config.show_lines and index_is_same_as_above:\r\n   # don't repeat the row as above\r\n   multi_index_row_label = \"\"\r\nelse:\r\n   # if `show_lines` is on, stylistically the best option is to repeat the value over again.\r\n  multi_index_row_label = row.index\r\n```\r\n\r\nThis does not 100% meet the exact same style for MultiIndex support that HTML + base repr do, but only specifically in the case of `show_lines=True`. When `show_lines=False` then it should look correct to users.\r\n\r\nAnother option: Disable support for `show_lines=True`. I don't love this, as users would probably expect a bit of customizability for these Rich Table objects, but it is an option to enforce this style decision for multi-indexes.\r\n\r\n---\r\n\r\nIf the main blocker to adoption of this feature is a concern over support for this and a few other things, rather than e.g. this feature being out of scope in general for Pandas, we can potentially make changes to Rich itself.\r\n\r\nMy question is: If Rich had every feature you'd reasonably want to render the reprs in a way that is consistent with the current repr behavior (which I think it does), would Rich support still be considered out of scope for Pandas? Or would it be potentially in-scope? Because if the latter, i.e. Rich support is in-scope, it's possible we can try to prioritize any missing functionality on Textualize\/Rich's end; I'm sure they'd be interested in unblocking a major library like Pandas adopting Rich.","Here is an example of the snippet code modified to emulate the rowspan-like visualization that the base repr provides:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# todo: replace with actual pandas settings\r\nsettings__display_rich_combine_multindex = False\r\n\r\nclass DataFrame(pd.DataFrame):\r\n\r\n    def __rich_console__(self, console: \"rich.console.Console\", console_options: \"rich.console.Console\"):\r\n        from rich.table import Table\r\n        table = Table()\r\n        split_multiindices = False\r\n        # Todo: check if there is a better way to check for multi indices\r\n        if self.index.names and not settings__display_rich_combine_multindex:\r\n            split_multiindices = True\r\n            for name in self.index.names:\r\n                table.add_column(name if name is not None else \"\")\r\n        else:\r\n            if self.index.names:\r\n                name = \",\".join(self.index.names)\r\n            else:\r\n                name = self.index.name\r\n            table.add_column(name if name is not None else \"\")\r\n\r\n        _prev_index = None\r\n\r\n        for c in self.columns:\r\n            table.add_column(str(c))\r\n        for r in self.itertuples():\r\n            if split_multiindices:\r\n                if _prev_index is not None:\r\n                    new_index = [\r\n                        new if prev != new else \"\"\r\n                        for prev, new in zip(_prev_index, r[0])\r\n                    ]\r\n                else:\r\n                    new_index = r[0]\r\n                _prev_index = r[0]\r\n                row_data = map(str, [*new_index, *r[1:]])\r\n            else:\r\n                row_data = map(str, r)\r\n            table.add_row(*row_data)\r\n\r\n        yield from table.__rich_console__(console, console_options)\r\n\r\nfrom rich.console import Console\r\n\r\nconsole = Console(color_system=\"truecolor\")\r\n\r\ndf = DataFrame(\r\n    [\r\n        {\"name\": \"John Doe\", \"phone\": \"555-555-5555\", \"last_purchase_amount\": 1.23},\r\n        {\"name\": \"Mary Sue\", \"phone\": \"800-123-4567\", \"last_purchase_amount\": 100.00},\r\n        {\"name\": \"Bob James\", \"phone\": \"800-234-5678\", \"last_purchase_amount\": 50.00},\r\n    ],\r\n    index=pd.MultiIndex.from_tuples([(1, 1), (1, 2), (2, 1)], names=[\"foo\", \"bar\"])\r\n)\r\n\r\nconsole.print(df)\r\n```\r\n\r\nThis is the output:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/e7fc0ac2-832b-4970-a337-403d82fc2c3e)\r\n\r\nLooks decent if you ask me.\r\n\r\nHowever, you can see that it looks bad when I replace `table = Table()` with `table = Table(show_lines=True)`:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/ab659c3e-8260-491e-b3bc-da1a69fd2459)\r\n\r\nThe 'work around' I propose is, when `show_lines=True`, we display everything:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/31971762\/9a2095bc-ae11-4081-b4f1-137bb31946f5)\r\n\r\nIn the sloppy draft code above, you can achieve this by replacing this line: `if _prev_index is not None:` with: `if _prev_index is not None and not table.show_lines:`.\r\n\r\nThis is not ideal, but it's the best idea I can come up with that doesn't involve significant rewrites of Rich's internals and\/or getting Rich maintainers involved (again, not that this would be an impossible ask if a library as big as Pandas were to ask for a feature)."],"labels":["Enhancement","Output-Formatting","Needs Discussion"]},{"title":"BUG: still getting UnicodeDecodeError with encoding_errors=\"ignore\"","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport gzip\r\nfrom io import BytesIO\r\n\r\nfile_path = ...\r\n\r\nwith gzip.open(file_path, \"rb\") as gz:\r\n    data = gz.read()\r\nencoding = \"ascii\"\r\nbytes_io = BytesIO(data)\r\ndf = pd.read_csv(\r\n    bytes_io,\r\n    encoding=encoding,\r\n    encoding_errors=\"ignore\",\r\n    delimiter=\"|\",\r\n    dtype=str,\r\n    on_bad_lines=lambda x: \"skip\",\r\n    engine=\"pyarrow\",\r\n    keep_default_na=False,\r\n)\n```\n\n\n### Issue Description\n\nI am trying to use pandas internals to decode my file rather than calling data.decode(encoding...) and passing in a stringIO object, to save on ram.\r\n\r\nHowever when I do this I am getting this error\r\n\r\n```\r\n  File \"\/root\/conda\/envs\/main\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py\", line 1024, in read_csv\r\n    return _read(filepath_or_buffer, kwds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/conda\/envs\/main\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py\", line 624, in _read\r\n    return parser.read(nrows)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/conda\/envs\/main\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/readers.py\", line 1909, in read\r\n    df = self._engine.read()  # type: ignore[attr-defined]\r\n         ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/conda\/envs\/main\/lib\/python3.11\/site-packages\/pandas\/io\/parsers\/arrow_parser_wrapper.py\", line 266, in read\r\n    table = pyarrow_csv.read_csv(\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\/_csv.pyx\", line 1261, in pyarrow._csv.read_csv\r\n  File \"pyarrow\/_csv.pyx\", line 1270, in pyarrow._csv.read_csv\r\n  File \"pyarrow\/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118\r\n  File \"pyarrow\/io.pxi\", line 1843, in pyarrow.lib._cb_transform\r\n  File \"pyarrow\/io.pxi\", line 1884, in pyarrow.lib.Transcoder.__call__\r\n  File \"\/root\/conda\/envs\/main\/lib\/python3.11\/encodings\/ascii.py\", line 26, in decode\r\n    return codecs.ascii_decode(input, self.errors)[0]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 10356: ordinal not in range(128)\r\n```\r\n\r\nthis happens when I use `encoding_errors='ignore'` or `encoding_errors='replace'`\n\n### Expected Behavior\n\nThis should behave the same way as if I were doing this\r\n\r\n```\r\nstring_io = StringIO(data.decode(encoding, errors=\"replace\"))\r\ndf = pd.read_csv(string_io, ...)\r\n```\r\n\r\nand ignore the unicode errors in my file\r\n\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.8.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-17-generic\r\nVersion               : #17-Ubuntu SMP PREEMPT_DYNAMIC Thu Jan 11 14:20:13 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.27\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"DOC: Series.diff with boolean dtype does not return a series of dtype float","body":"### Pandas version checks\r\n\r\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\r\n\r\n\r\n### Location of the documentation\r\n\r\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.diff.html#pandas.Series.diff\r\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.diff.html#pandas.DataFrame.diff\r\n\r\n### Documentation problem\r\n\r\nThe documentation for `pandas.Series.diff` and `pandas.DataFrame.diff` states that no matter the dtype of the original series\/column, the output will be of dtype `float64`. This is not true for series\/columns of dtypes `bool` -- the output here is of dtype `object`.\r\n\r\nFor example:\r\n\r\n```py\r\nimport pandas as pd\r\n# pd.__version__ == '2.2.0'\r\n\r\ns = pd.Series([True, True, False, False, True])\r\nd = s.diff()\r\n\r\n# d.dtype is now 'object'\r\n```\r\nIndeed, the underlying function [algorithms.diff](https:\/\/github.com\/pandas-dev\/pandas\/blob\/v2.2.0\/pandas\/core\/algorithms.py#L1363) explicitly differentiates between boolean and integer dtypes.\r\n\r\n### Suggested fix for documentation\r\n\r\nThe `Notes` section should read something like this:\r\n\r\n```\r\nNotes\r\n-----\r\nFor boolean dtypes, this uses :meth:`operator.xor` rather than\r\n:meth:`operator.sub` and the result's dtype is ``object``.\r\nOtherwise, the result is calculated according to the current dtype in {klass},\r\nhowever the dtype of the result is always float64.\r\n```","comments":["I am also open to change the function behaviour to return a float64 dtype, as advertised. This would also solve another problem: in 2.2.0, downcasting object dtypes by fillna etc. is deprecated: `s.diff().fillna(False)` produces a warning and the best replacement I could come up with is `s.diff().astype(float).fillna(0.0).astype(bool)`","Thanks for the report. Returning object is the behavior on 1.0.0; haven't tried prior versions. I'm +1 on agreeing with the docs here and returning float64 instead of object dtype.\r\n\r\nIt might also be worth adding a `fill_value` argument, similar to `shift`.\r\n\r\nCould use some other eyes - cc @MarcoGorelli."],"labels":["Docs","Dtype Conversions","Needs Discussion","Transformations"]},{"title":"PERF: Improve merge performance","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nthis avoids a bunch of unnecessary checks, might be worth it\r\n\r\n\r\n```\r\n| Change   | Before [b2b1aae3] <merge~1>   | After [aebecfe9] <merge>   |   Ratio | Benchmark (Parameter)                                                                       |\r\n|----------|-------------------------------|----------------------------|---------|---------------------------------------------------------------------------------------------|\r\n| -        | 46.9\u00b13\u03bcs                      | 41.9\u00b10.5\u03bcs                 |    0.89 | join_merge.ConcatIndexDtype.time_concat_series('string[pyarrow]', 'monotonic', 0, True)     |\r\n| -        | 210\u00b17ms                       | 187\u00b15ms                    |    0.89 | join_merge.I8Merge.time_i8merge('left')                                                     |\r\n| -        | 2.11\u00b10.2ms                    | 1.88\u00b10.01ms                |    0.89 | join_merge.MergeEA.time_merge('Float32', False)                                             |\r\n| -        | 17.1\u00b13ms                      | 15.2\u00b10.2ms                 |    0.89 | join_merge.MergeMultiIndex.time_merge_sorted_multiindex(('int64', 'int64'), 'inner')        |\r\n| -        | 1.06\u00b10.04ms                   | 907\u00b120\u03bcs                   |    0.86 | join_merge.Merge.time_merge_dataframe_integer_2key(False)                                   |\r\n| -        | 7.99\u00b10.9ms                    | 6.82\u00b10.06ms                |    0.85 | join_merge.ConcatIndexDtype.time_concat_series('string[pyarrow]', 'non_monotonic', 1, True) |\r\n| -        | 224\u00b110ms                      | 191\u00b16ms                    |    0.85 | join_merge.I8Merge.time_i8merge('inner')                                                    |\r\n| -        | 513\u00b190\u03bcs                      | 425\u00b15\u03bcs                    |    0.83 | join_merge.ConcatIndexDtype.time_concat_series('string[python]', 'monotonic', 0, False)     |\r\n| -        | 519\u00b1100\u03bcs                     | 421\u00b12\u03bcs                    |    0.81 | join_merge.ConcatIndexDtype.time_concat_series('string[python]', 'non_monotonic', 0, False) |\r\n| -        | 1.18\u00b10.6ms                    | 751\u00b17\u03bcs                    |    0.63 | join_merge.ConcatIndexDtype.time_concat_series('int64[pyarrow]', 'has_na', 1, False)        |\r\n\r\n```","comments":["@WillAyd good to merge?"],"labels":["Performance","Reshaping"]},{"title":"API: avoid passing Manager to subclass __init__","body":"- [x] closes #57032 (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nMany subclasses have `__init__` methods that call `super().__init__`, which in the relevant cases with a Manager object is deprecated.  The solution in this PR is to construct the pd.DataFrame\/Series and pass that to the subclass's `_constructor`.\r\n\r\nThis removes _sliced_from_mgr and _expanddim_from_mgr, as they are not really necessary, simplifying things a bit.  Also makes _from_mgr `@final`.  Can separate these simplifications into their own PR on request.  xref #56681.\r\n\r\nFor subclasses that currently have special handling for Manager objects in their `__init__`, they will need to move that special handling to the appropriate `_constructor(_foo)?_from_mgr` method.  The only package I'm aware of that does this is geopandas, so this includes a shim special-casing GeoDataFrame intended to maintain the current behavior until they have time to move that logic.  cc @jorisvandenbossche ATM the shim is only in _constructor_from_mgr, pls advise on whether something analogous is needed in constructor_sliced_from_mgr and _constructor_expanddim_from_mgr.","comments":[],"labels":["Internals","Constructors"]},{"title":"BUG: .rolling-method is not working properly when called with a timedelta","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\ndf = pd.DataFrame(index=pd.date_range(start=\"2024-02-01\", end=\"2024-02-21\", freq=\"1d\"))\r\ndf[\"A\"] = df.index.day.astype(float)\r\ndf.iloc[-2, 0] = np.inf\r\ndf.iloc[-9, 0] = np.nan\r\nres1 = df.rolling(window=pd.Timedelta(\"4d\")).mean()\r\nres2 = df.rolling(window=4).mean()\n```\n\n\n### Issue Description\n\nWhen passing a timedelta to the rolling method, entries like nan and inf are being ignored. Also missing values at the beginning of the column are not taken into account for the calculation. In the Reproducible Example both results res1 and res2 should be equal, but they are not.\n\n### Expected Behavior\n\nMissing and invalid values should be taken into account properly when passing a TimeDelta.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.10.2.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : de_DE.cp1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 58.1.0\r\npip                   : 21.2.4\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report!\r\n\r\n> In the Reproducible Example both results res1 and res2 should be equal, but they are not.\r\n\r\nI don't think this is correct. From [the docs](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/reference\/api\/pandas.DataFrame.rolling.html):\r\n\r\n> For a window that is specified by an offset, min_periods will default to 1.\r\n> For a window that is specified by an integer, min_periods will default to the size of the window.\r\n\r\nSpecifying `min_periods` to be the same for each example, the output then matches.\r\n\r\n> entries like nan and inf are being ignored.\r\n\r\nCan you clarify what you mean here? What is the expected output?","Thanks for the fast response.\r\n\r\nIndeed you are right and i was wrong. Still i think that for a window specified by an offset the result should be np.nan if the observed window is shorter than the specified offset. (in my example the first 3 days, since the specified offset is 4 days).\r\n\r\nFurther i would expect the result for the windows inclusing np.inf or np.nan to be np.nan. Expecialle for np.inf since it is a workaround for the related problem (groupby.mean decribed here: https:\/\/github.com\/pandas-dev\/pandas\/issues\/32696)\r\n\r\nThanks for your thoughts on this in advance.","> Still i think that for a window specified by an offset the result should be np.nan if the observed window is shorter than the specified offset. (in my example the first 3 days, since the specified offset is 4 days).\r\n\r\nCan it be the case that the offset is not an integer multiple of the data?\r\n\r\n> Further i would expect the result for the windows inclusing np.inf or np.nan to be np.nan.\r\n\r\nFor np.nan, we would first need to implement `skipna` - I'm +1 here. We should probably set up a tracking issue for this for rolling. The one for groupby is here: https:\/\/github.com\/pandas-dev\/pandas\/issues\/15675.\r\n\r\nFor np.inf, the corresponding result for DataFrame \/ groupby is in fact inf, so I would think we should get the same here. Further investigations and PRs to fix are welcome!","> Can it be the case that the offset is not an integer multiple of the data?\r\n\r\nI am not sure if i understand you right here. What i wanted to say is: The first entries of the observed dataframe should be np.nan until the entries specify an offset >= the offset specified when calling the function. Like in my example the datetimeindex has a frequence of 1 day. The specified window size is 4 days. So the first 3 entries (window size only 3 days but offset is 4 days) is too small to evaluate the specified window.","And for e.g. `df.rolling(window=pd.Timedelta(\"3.5d\")).mean()`? Does that result in np.nan when there are only 3 and not 4 days in the window? Or since the 4th data point will never be used (even if it existed, should the result be non-nan?\r\n\r\nIn any case, I would recommend having this issue focus on the `np.inf` bug. The behavior with `window` being non-integral is documented and functioning correctly in accordance with the docs. If you think this behavior should be changed, can you open up a new issue?"],"labels":["Bug","Window"]},{"title":"Fix accidental loss-of-precision for to_datetime(str, unit=...)","body":"In Pandas 1.5.3, the `float(val)` cast was inline to the `cast_from_unit` call in `array_with_unit_to_datetime`. This caused the intermediate (unnamed) value to be a Python float.\r\n\r\nSince #50301, a temporary variable was added to avoid multiple casts, but with explicit type `cdef float`, which defines a _Cython_ float. This type is 32-bit, and causes a loss of precision, and a regression in parsing from 1.5.3.\r\n\r\nSince `cast_from_unit` takes an `object`, not a more specific Cython type, remove the explicit type from the temporary `fval` variable entirely. This will cause it to be a (64-bit) Python float, and thus not lose precision.\r\n\r\nFixes #57051\r\n\r\n- [x] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [n\/a] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Can this be declared as double instead? Using a PyObject is unnecessary overhead","As noted, `cast_from_unit` takes an `object`, so that doesn't seem to save anything. Here's the diff of the generated code when you add the type (vs having it commented out since that reduces line number changes in the diff):\r\n```diff\r\n--- untyped\/pandas\/_libs\/tslib.pyx.c\t2024-02-21 18:27:51.686387390 -0500\r\n+++ typed\/pandas\/_libs\/tslib.pyx.c\t2024-02-21 18:24:14.824227898 -0500\r\n@@ -25828,9 +25828,9 @@\r\n   int __pyx_v_is_raise;\r\n   PyArrayObject *__pyx_v_iresult = 0;\r\n   PyDateTime_TZInfo *__pyx_v_tz = 0;\r\n+  double __pyx_v_fval;\r\n   PyObject *__pyx_v_result = NULL;\r\n   PyObject *__pyx_v_val = NULL;\r\n-  double __pyx_v_fval;\r\n   PyObject *__pyx_v_err = NULL;\r\n   __Pyx_LocalBuf_ND __pyx_pybuffernd_iresult;\r\n   __Pyx_Buffer __pyx_pybuffer_iresult;\r\n@@ -25919,14 +25919,14 @@\r\n  *         bint is_raise = errors == \"raise\"\r\n  *         ndarray[int64_t] iresult\r\n  *         tzinfo tz = None             # <<<<<<<<<<<<<<\r\n- *         # double fval\r\n+ *         double fval\r\n  * \r\n  *\/\r\n   __Pyx_INCREF(Py_None);\r\n   __pyx_v_tz = ((PyDateTime_TZInfo *)Py_None);\r\n \r\n   \/* \"pandas\/_libs\/tslib.pyx\":280\r\n- *         # double fval\r\n+ *         double fval\r\n  * \r\n  *     assert is_coerce or is_raise             # <<<<<<<<<<<<<<\r\n  * \r\n@@ -32771,7 +32771,7 @@\r\n  *     ndarray[object] values,\r\n  *     str unit,\r\n  *\/\r\n-  __pyx_tuple__37 = PyTuple_Pack(13, __pyx_n_s_values, __pyx_n_s_unit, __pyx_n_s_errors, __pyx_n_s_i, __pyx_n_s_n, __pyx_n_s_is_coerce, __pyx_n_s_is_raise, __pyx_n_s_iresult, __pyx_n_s_tz, __pyx_n_s_result, __pyx_n_s_val, __pyx_n_s_fval, __pyx_n_s_err); if (unlikely(!__pyx_tuple__37)) __PYX_ERR(0, 239, __pyx_L1_error)\r\n+  __pyx_tuple__37 = PyTuple_Pack(13, __pyx_n_s_values, __pyx_n_s_unit, __pyx_n_s_errors, __pyx_n_s_i, __pyx_n_s_n, __pyx_n_s_is_coerce, __pyx_n_s_is_raise, __pyx_n_s_iresult, __pyx_n_s_tz, __pyx_n_s_fval, __pyx_n_s_result, __pyx_n_s_val, __pyx_n_s_err); if (unlikely(!__pyx_tuple__37)) __PYX_ERR(0, 239, __pyx_L1_error)\r\n   __Pyx_GOTREF(__pyx_tuple__37);\r\n   __Pyx_GIVEREF(__pyx_tuple__37);\r\n   __pyx_codeobj__38 = (PyObject*)__Pyx_PyCode_New(3, 0, 0, 13, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__37, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_home_elliott_code_pandas_pandas, __pyx_n_s_array_with_unit_to_datetime, 239, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__38)) __PYX_ERR(0, 239, __pyx_L1_error)\r\n```","Fair point on the existing structure of the code. But if it works should still add it - generally the more typing in Cython the better. And if this gets refactored in the future the next developer won't have to think about it","If you see us using float anywhere else probably worth changing to double everywhere in Cython (in a follow up PR)","Thanks a lot for your investigation and fix for this @QuLogic!"],"labels":["Timeseries"]},{"title":"BUG: na_values dict form not working on index column ","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nfrom io import StringIO\r\n\r\nfrom pandas._libs.parsers import STR_NA_VALUES\r\nimport pandas as pd\r\n\r\nfile_contents = \"\"\",x,y\r\nMA,1,2\r\nNA,2,1\r\nOA,,3\r\n\"\"\"\r\n\r\ndefault_nan_values = STR_NA_VALUES | {\"squid\"}\r\nnames = [None, \"x\", \"y\"]\r\nnan_mapping = {name: default_nan_values for name in names}\r\ndtype = {0: \"object\", \"x\": \"float32\", \"y\": \"float32\"}\r\n\r\npd.read_csv(\r\n    StringIO(file_contents),\r\n    index_col=0,\r\n    header=0,\r\n    engine=\"c\",\r\n    dtype=dtype,\r\n    names=names,\r\n    na_values=nan_mapping,\r\n    keep_default_na=False,\r\n)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nI'm trying to find a way to read in an index column as exact strings, but read in the rest of the columns as NaN-able numbers or strings. The dict form of na_values seems to be the only way implied in the documentation to allow this to happen, however, when I try this, it errors with the message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\/test.py\", line 17, in <module>\r\n    pd.read_csv(\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 1024, in read_csv\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 624, in _read\r\n    return parser.read(nrows)\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 1921, in read\r\n    ) = self._engine.read(  # type: ignore[attr-defined]\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/c_parser_wrapper.py\", line 333, in read\r\n    index, column_names = self._make_index(date_data, alldata, names)\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/base_parser.py\", line 372, in _make_index\r\n    index = self._agg_index(simple_index)\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/base_parser.py\", line 504, in _agg_index\r\n    arr, _ = self._infer_types(\r\n  File \"...\/venv\/lib\/python3.10\/site-packages\/pandas\/io\/parsers\/base_parser.py\", line 744, in _infer_types\r\n    na_count = parsers.sanitize_objects(values, na_values)\r\nTypeError: Argument 'na_values' has incorrect type (expected set, got dict)\r\n```\r\nThis is unhelpful, as the docs imply this should work, and I can't find any other way to turn off nan detection in the index column without disabling it in the rest of the table (which is a hard requirement)\r\n\r\n### Expected Behavior\r\n\r\nThe pandas table should be read without error, leading to a pandas table a bit like the following:\r\n```\r\n       x    y\r\nMA   1.0  2.0\r\nNA   2.0  1.0\r\nOA   NaN  3.0\r\n```\r\n\r\n### Installed Versions\r\nThis has been tested on three versions of pandas v1.5.2, v2.0.2, and v2.2.0, all with similar results. \r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.11.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-18-generic\r\nVersion               : #18~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb  7 11:40:03 UTC 2\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["import io\r\nimport pandas as pd\r\n\r\nfile_contents = \"\"\"\r\n,x,y\r\nMA,1,2\r\nNA,2,1\r\nOA,,3\r\n\"\"\"\r\n\r\ndefault_nan_values = set([\"NA\", \"squid\"])\r\nnames = [None, \"x\", \"y\"]\r\nnan_mapping = {name: default_nan_values for name in names}\r\ndtype = {0: \"object\", \"x\": \"float32\", \"y\": \"float32\"}\r\n\r\ntry:\r\n  df = pd.read_csv(\r\n      io.StringIO(file_contents),\r\n      index_col=0,\r\n      header=0,\r\n      engine=\"c\",\r\n      dtype=dtype,\r\n      names=names,\r\n      na_values=nan_mapping,\r\n      keep_default_na=True,\r\n  )\r\n  print(df)\r\nexcept Exception as e:\r\n  print(f\"Error occurred: {e}\")","Thanks for the report, confirmed on main. Further investigations and PRs to fix are welcome!","take","replacing the `None` in `names` with anything else (string) works fine.","@thomas-intellegens Sorry to bother, but in the issue post you mention that \r\n\r\n> The dict form of na_values seems to be the only way implied in the documentation to allow having no na values on a specific column\r\n\r\nIn case you might remember, was the documentation [this one?](https:\/\/pandas.pydata.org\/docs\/user_guide\/io.html#na-and-missing-data-handling)\r\n\r\nBecause otherwise, I cannot find, in the docs, where such property is mentioned.\r\n\r\nThank you","> In case you might remember, was the documentation [this one?](https:\/\/pandas.pydata.org\/docs\/user_guide\/io.html#na-and-missing-data-handling)\r\n\r\nYeah, this was the section I was reading. Many thanks for taking a look at this"],"labels":["Bug","IO CSV"]},{"title":"DOC: Add missing docstrings for Index.empty, Index.view and Index.names","body":"- [ All docstrings tests passed]\r\n","comments":["pre-commit.ci autofix","\/preview"],"labels":["Docs","Index"]},{"title":"DOC: Improve to_string() docstring to clarify that missing values are not passed through formatters","body":"## Summary\r\nThe current docstring for `to_string()` does not provide detail on how missing values are handled when using `formatters`.\r\n\r\nThis change adds an additional block to the `formatters` docstring explaining that `formatters` are **not** invoked for missing values. It also provides some guidance on how per-column formatting of missing values can be achieved.\r\n\r\n### Background\r\nThis issue came to my attention while fixing a bug likely caused by a misunderstanding of how `to_string()` handles missing values. Specifically, formatters were assumed to be invoked on all values, including missing ones (which is not the case).\r\n\r\n### Further issues\r\n- It is slightly unclear to me how broadly this behaviour applies (I've seen it with `DataFrame.to_string()`, and the implementation appears to be generic). I could be wrong though.\r\n- **Possible bug:** `NDFrame.to_latex()` takes a parameter called `formatters`, but following that through leads to the function `_to_latex_via_styler()` which **does not use the formatters parameter**!?\r\nThis looks like a potentially incomplete implementation without annotation (at `pandas\/core\/generic.py` , line ~3519). I can raise an issue for that separately if desired.\r\n\r\n### Checks\r\n~~- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature~~\r\nNot applicable: no new code.\r\n\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n  - Most tests passed. Exhaustive pre-commit suite did not complete due to failure of a check _completely unrelated to this change_ (incorrect type passed to `BaseBlockManager.copy()` in `pandas\/core\/generic.py`).\r\n\r\n~~- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.~~\r\nNot applicable: no new code.\r\n\r\n~~- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.~~\r\nNot applicable: this is just an improvement to documentation of existing functionality.\r\n","comments":["What do you think about moving much of the current content in this PR to the User Guide:\r\n\r\nhttps:\/\/pandas.pydata.org\/docs\/dev\/user_guide\/io.html#writing-a-formatted-string\r\n\r\nand then linking to that from the docstring? ","Thanks for the review @datapythonista and suggestion @rhshadrach \ud83d\ude03\r\n\r\nI agree what I've got feels overlong for a docstring, and probably makes more sense in the main docs.\r\nI've shorted the docstring and added notes to the IO tools doc page.\r\n\r\nIncluding a link in the docstring feels a bit clunky, though I can't see a good alternative. The other option would be saying `see 'Writing a formatted string' in the 'IO tools' section of the user guide`, and that is both long and less helpful. \ud83e\udd37","\/preview","Website preview of this PR available at: https:\/\/pandas.pydata.org\/preview\/pandas-dev\/pandas\/57545\/","\/preview","> \/preview\r\n\r\nThe preview needs that the documentation is built in the CI. Seems like the documentation job failed because of warnings:\r\n\r\n```\r\n\/home\/runner\/work\/pandas\/pandas\/pandas\/core\/frame.py:docstring of pandas.core.frame.DataFrame.to_html:31: ERROR: Unknown target name: \"io.formatting\".\r\n\/home\/runner\/work\/pandas\/pandas\/pandas\/core\/frame.py:docstring of pandas.core.frame.DataFrame.to_html:35: ERROR: Unknown target name: \"io.formatting\".\r\n\/home\/runner\/work\/pandas\/pandas\/pandas\/core\/frame.py:docstring of pandas.core.frame.DataFrame.to_string:31: ERROR: Unknown target name: \"io.formatting\".\r\n\/home\/runner\/work\/pandas\/pandas\/pandas\/core\/frame.py:docstring of pandas.core.frame.DataFrame.to_string:35: ERROR: Unknown target name: \"io.formatting\".\r\n```\r\n\r\nIf you get the CI green, at least for the docs job, the previewer will work.","I think I've fixed all my issues.\r\nThe doc build is still breaking in the GH action, but due to something I don't think I caused:\r\n\r\n`Doc Build and Upload` > `Test website`\r\n```\r\n  File \"\/home\/runner\/micromamba\/envs\/test\/lib\/python3.10\/site-packages\/pluggy\/_manager.py\", line 342, in _verify_hook\r\n    raise PluginValidationError(\r\npluggy._manager.PluginValidationError: Plugin 'pytest_cython' for hook 'pytest_collect_file'\r\nhookimpl definition: pytest_collect_file(path, parent)\r\nArgument(s) {'path'} are declared in the hookimpl but can not be found in the hookspec\r\n```","> The doc build is still breaking in the GH action, but due to something I don't think I caused:\r\n\r\npytest released today; guessing it's related to that.\r\n\r\nEdit: pytest yanked the release, rerunning the builds.","Looks like many of the other tests failed due to CircleCI deprecating a load of Ubuntu images yesterday:\r\nhttps:\/\/status.circleci.com\/incidents\/m04y1p0l3f3n\r\n\r\nEdit: one of the other test failures was due to a test mismatch with `dateutil` 2.9 ([fixed upstream](https:\/\/github.com\/pandas-dev\/pandas\/commit\/c3ae17d04d3993e1c3e4c0f8824fde03b8c9beac), merged in).\r\n\r\nEdit: merging in upstream again, looks like the test error about numpy arrays with copy=false has been addressed [here](https:\/\/github.com\/pandas-dev\/pandas\/commit\/b89f1d0d05f4c9f360985abc6bda421d73bae85f). Hopefully this'll fix the last test failure.","_bump_\r\n@rhshadrach I think everything is now addressed. If you're happy with it can you update your review to remove the requested changes flag?"],"labels":["Docs","Styler"]},{"title":"BUG: ADBC Postgres writer incorrectly names the table","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport adbc_driver_postgresql.dbapi as pg_dbapi\r\nimport pandas as pd\r\n\r\ncon = pg_dbapi.connect(uri=\"<PG_URI>\")\r\ntable_name = \"my_table\"\r\n\r\ndf = pd.DataFrame({\"c0\": [1, 2, 3], \"c1\": [\"foo\", \"boo\", \"bar\"]})\r\ndf.to_sql(name=table_name, con=con, schema=\"public\", if_exists=\"replace\")\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe table which ends up being created in the `public` schema ends up being named `public.my_table`, instead of just `my_table`. The table is then inaccessible, as the following code throws this error `ValueError: Table my_table not found`:\r\n\r\n```python\r\ndf_out = pd.read_sql_table(table, con, schema=\"public\")\r\n```\r\n\r\nThe created table can be found using the following code:\r\n```python\r\nobjects = con.adbc_get_objects(depth=\"tables\", db_schema_filter=\"public\", catalog_filter=\"postgres\").read_all()\r\ncatalogs = objects.to_pylist()\r\n\r\nall_tables = catalogs[0][\"catalog_db_schemas\"][0][\"db_schema_tables\"]\r\ntable = [x for x in all_tables if table in x[\"table_name\"]][0]\r\n```\r\n\r\n### Expected Behavior\r\n\r\nTable gets created under the name specified, without including the schema name in the table name.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.13.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.3.0\r\nVersion               : Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:27 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T8103\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.1\r\nhypothesis            : None\r\nsphinx                : 7.1.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : 1.4.6\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.12.3\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["take"],"labels":["Bug","IO SQL","Needs Triage"]},{"title":"BUG: `sort_values` is sorting the index too when `ignore_index=False`","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: pdf = pd.DataFrame({\"a\": [1, 3, 5, 2, 4], \"b\": [1, 1, 2, 2, 3], \"c\": [9, 7, 7, 7, 1]})\r\n\r\nIn [3]: pdf = pdf.set_index(['a', 'b'])\r\n\r\nIn [4]: pdf\r\nOut[4]: \r\n     c\r\na b   \r\n1 1  9\r\n3 1  7\r\n5 2  7\r\n2 2  7\r\n4 3  1\r\n\r\nIn [5]: pdf.sort_values(['c'], ignore_index=False)\r\nOut[5]: \r\n     c\r\na b   \r\n4 3  1\r\n3 1  7\r\n2 2  7\r\n5 2  7\r\n1 1  9\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen there seems to be a tie in column `c`, the `MultiIndex` levels seem to be sorted - which is kind of unexpected. The expected behavior would be to preserve the order of occurrence right?\r\n\r\n### Expected Behavior\r\n\r\n```python\r\nIn [5]: pdf.sort_values(['c'], ignore_index=False)\r\nOut[5]: \r\n     c\r\na b   \r\n4 3  1\r\n3 1  7\r\n5 2  7\r\n2 2  7\r\n1 1  9\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-94-generic\r\nVersion               : #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : 6.98.9\r\nsphinx                : 6.2.1\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : 0.59.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : 2024.2.0\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.27\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["I cannot seemingly get the sorted result on main\r\n\r\n```python\r\nIn [1]: In [1]: import pandas as pd\r\n   ...: \r\n   ...: In [2]: pdf = pd.DataFrame({\"a\": [1, 3, 5, 2, 4], \"b\": [1, 1, 2, 2, 3], \"c\": [9, 7, 7, 7, 1]})\r\n   ...: \r\n   ...: In [3]: pdf = pdf.set_index(['a', 'b'])\r\n\r\nIn [2]: pdf.sort_values(['c'], ignore_index=False)\r\nOut[2]: \r\n     c\r\na b   \r\n4 3  1\r\n3 1  7\r\n5 2  7\r\n2 2  7\r\n1 1  9\r\n```\r\n\r\nBut since the numpy sorts internally are unstable (xref https:\/\/github.com\/pandas-dev\/pandas\/issues\/53558) maybe you're hitting a case where the result gets reordered?"],"labels":["Bug","Sorting"]},{"title":"ENH: Better documentation or default behavior for GroupBy for columns with non-sortable values","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nCurrently (2.2.0), there is a relatively cryptic error message for groupby on columns that contain object values which are not sortable.\r\n\r\nThe default argument value for `sort` in groupby is `True`, which understandably can't be honored for columns with values that just can't be sorted (e.g. of different types).\n\n### Feature Description\n\ncurrently:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [False, 'string', True], 'b': [1, 2, 3]})\r\ndf.groupby('a').describe()  # works\r\n\r\ndf = pd.DataFrame({'a': [False, (1, 2,), True], 'b': [1, 2, 3]})\r\ndf.groupby('a').describe()  # fails!\r\n\r\n# TypeError: '<' not supported between instances of 'tuple' and 'bool'\r\n```\r\n\r\nIt would be nice if Pandas would just fall back to not sorting the output. As illustrated by the case with the `'string'` value, Pandas is lenient and sorts booleans and strings without complaining, which is not technically correct, and it would be convenient if this behavior is extended to other types.\n\n### Alternative Solutions\n\nI realize that the suggested change may be a conflicting one as the argument `sort=True` clearly requests a sorted output. For applications that would rely on this output being indeed sorted, it might be better to be more strict and keep having an error. In this case, however, it would be good to get a hint that `sort=False` would solve the issue. But then again, there's the issue that it currently is supported between non-sortable values (string vs bool), and for consistency, I would suggest the `sort` argument be ignored.\n\n### Additional Context\n\n_No response_","comments":["Thanks for the report.\r\n\r\n> It would be nice if Pandas would just fall back to not sorting the output.\r\n\r\nI'm opposed here. This makes the output harder to predict for users and can hide bugs.\r\n\r\n> As illustrated by the case with the 'string' value, Pandas is lenient and sorts booleans and strings without complaining, which is not technically correct, and it would be convenient if this behavior is extended to other types.\r\n\r\nWhat is meant by \"is extended to other types\"? In the first half of the line above you seem to suggest pandas shouldn't be sorting Booleans and strings, and the second half you seem to suggest pandas should be sorting more combinations of dtypes.\r\n\r\n> In this case, however, it would be good to get a hint that sort=False would solve the issue.\r\n\r\nI think this is a good idea, but the difficulty will be in the implementation. The failure happens within factorize, which is used in a variety of places in pandas. Since we can be attempting to sort any Python object, we can't predict the type of error a sorting failure will raise. So it seems to me we can't reliably catch the exception in the groupby code, nor can we raise a message like \"use sort=False instead\" in factorize. \r\n\r\nRelated: https:\/\/github.com\/pandas-dev\/pandas\/issues\/54847#issuecomment-1698257419","I suppose we could wrap the sorting in factorize:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/9a6c8f0ad02f7faa23a06a69cdd003bd4a47d6be\/pandas\/core\/algorithms.py#L807-L814\r\n\r\nwith something like:\r\n\r\n```\r\ntry:\r\n    ... = safe_sort(...)\r\nexcept Exception as err:\r\n    raise type(err)(\"factorize failed sorting\") from err\r\n```\r\n\r\nand then we could reliably catch and rethrow this in groupby.\r\n\r\ncc @mroeschke - any thoughts?","Is the exception from `safe_sort` be captured in groupby and rethrown instead?","Thanks for picking up this issue!\r\n\r\n> > It would be nice if Pandas would just fall back to not sorting the output.\r\n> \r\n> I'm opposed here. This makes the output harder to predict for users and can hide bugs.\r\n> \r\n> > As illustrated by the case with the 'string' value, Pandas is lenient and sorts booleans and strings without complaining, which is not technically correct, and it would be convenient if this behavior is extended to other types.\r\n> \r\n> What is meant by \"is extended to other types\"? In the first half of the line above you seem to suggest pandas shouldn't be sorting Booleans and strings, and the second half you seem to suggest pandas should be sorting more combinations of dtypes.\r\n> \r\n\r\nSorry if that was a bit confusing.\r\n\r\nWhat I meant here is that I found the current behavior inconsistent in the sense that some values (strings) get special treatment and work with `sort=True` despite fudamentally not being sortable with booleans, while others don't (my tuples in the example above).\r\nEssentially, I see two options to get more consistency:\r\n1. `sort=True` fails with any values for which `a < b` would raise a basic python exception, like e.g. `'string' < False` does. E.g. my 'alternative solution' above\r\n2. Be more lenient as currently is done for strings.\r\n\r\nWhat I proposed as the desired feature was to go with option 2 to resolve this inconsistency. My main reason was that it would not change the current behavior of not raising an exception with string values. With 'not technically correct', I meant that there fundamentally just is no order between booleans and strings, but pandas doesn't complain despite `sort=True`, and I can only guess what it actually does. With \"is extended to other types\" I meant that pandas should not complain in other scenarios that are not sortable either.\r\n\r\nThis brings up the strong argument of output predictability. I think, though that a focus on predictability would support option 1. I didn't dig deeper in the code or docs, so I don't know how pandas actually sorts bools and strings for groupby, so predictability in the current state might already not be as good as it could be. For comparison, I just did a quick check and in 2.1.4 Pandas refuses to sort a pd.Series with bool and string values. Maybe it would be better to also be more strict on groupby group values (option 1).\r\n","@mroeschke \r\n\r\n> Is the exception from `safe_sort` be captured in groupby and rethrown instead?\r\n\r\nI think you mean to try to catch the exception thrown from `safe_sort` as-is (that is, without changing `safe_sort`). Because it can be sorting an arbitrary Python object, I don't think there is a reliable way to catch error thrown from `safe_sort` within the groupby code while letting others propagate.\r\n\r\n@gabuzi \r\n\r\n> Essentially, I see two options to get more consistency:\r\n\r\nI think strings and numeric data occur common enough in index labels (e.g. pandas will default to creating column labels as integers, whereas strings are very common) that this may be special enough to warrant an exception, but that does not mean we have to \"sort the world\" :laughing: In any case, if we are going to allow strings, numerics, and NA values and nothing else, it should at least be documented better.\r\n\r\n> Maybe it would be better to also be more strict on groupby group values (option 1).\r\n\r\nIf we were designing the API from scratch, this would be my recommendation. However I think we have to consider the potential disruption to users. The benefit of having a more predictable\/consistent \"what is sortable\" seems minor to the disruption it might cause, but of course I'm only guessing here. Unfortunately I see no way to be sure. I wouldn't be opposed if this gains support.","> I don't think there is a reliable way to catch error thrown from safe_sort within the groupby code while letting others propagate.\r\n\r\nOK then doing what you suggested in https:\/\/github.com\/pandas-dev\/pandas\/issues\/57525#issuecomment-1962330777 seems reasonable"],"labels":["Enhancement","Groupby","Needs Discussion","Sorting"]},{"title":"BUG: Subtraction fails with matching index of different name and type","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns1 = pd.Series(\r\n    [23, 22, 21], index=pd.Index([\"a\", \"b\", \"c\"], name=\"index a\"), dtype=pd.Int64Dtype()\r\n)\r\ns2 = pd.Series(\r\n    [21, 22, 23],\r\n    index=pd.Index([\"a\", \"b\", \"c\"], name=\"index b\", dtype=pd.StringDtype()),\r\n    dtype=pd.Int64Dtype(),\r\n)\r\n\r\ns1 - s2\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThis example with two series with a nullable integer data type and a index with different names and of differing data types, on with a nullable string datatype and object datatype fails with and error from `pandas._libs.algos.ensure_platform_int()`\r\n\r\n<details>\r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[170], line 10\r\n      1 s1 = pd.Series(\r\n      2     [23, 22, 21], index=pd.Index([\"a\", \"b\", \"c\"], name=\"index a\"), dtype=pd.Int64Dtype()\r\n      3 )\r\n      4 s2 = pd.Series(\r\n      5     [21, 22, 23],\r\n      6     index=pd.Index([\"a\", \"b\", \"c\"], name=\"index b\", dtype=pd.StringDtype()),\r\n      7     dtype=pd.Int64Dtype(),\r\n      8 )\r\n---> 10 s1 - s2\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/ops\/common.py:76, in _unpack_zerodim_and_defer.<locals>.new_method(self, other)\r\n     72             return NotImplemented\r\n     74 other = item_from_zerodim(other)\r\n---> 76 return method(self, other)\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/arraylike.py:194, in OpsMixin.__sub__(self, other)\r\n    192 @unpack_zerodim_and_defer(\"__sub__\")\r\n    193 def __sub__(self, other):\r\n--> 194     return self._arith_method(other, operator.sub)\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/series.py:5814, in Series._arith_method(self, other, op)\r\n   5813 def _arith_method(self, other, op):\r\n-> 5814     self, other = self._align_for_op(other)\r\n   5815     return base.IndexOpsMixin._arith_method(self, other, op)\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/series.py:5844, in Series._align_for_op(self, right, align_asobject)\r\n   5841             left = left.astype(object)\r\n   5842             right = right.astype(object)\r\n-> 5844         left, right = left.align(right, copy=False)\r\n   5846 return left, right\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:10095, in NDFrame.align(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\r\n  10082     left, _right, join_index = self._align_frame(\r\n  10083         other,\r\n  10084         join=join,\r\n   (...)\r\n  10091         fill_axis=fill_axis,\r\n  10092     )\r\n  10094 elif isinstance(other, ABCSeries):\r\n> 10095     left, _right, join_index = self._align_series(\r\n  10096         other,\r\n  10097         join=join,\r\n  10098         axis=axis,\r\n  10099         level=level,\r\n  10100         copy=copy,\r\n  10101         fill_value=fill_value,\r\n  10102         method=method,\r\n  10103         limit=limit,\r\n  10104         fill_axis=fill_axis,\r\n  10105     )\r\n  10106 else:  # pragma: no cover\r\n  10107     raise TypeError(f\"unsupported type: {type(other)}\")\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:10224, in NDFrame._align_series(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\r\n  10221         new_mgr = self._mgr.reindex_indexer(join_index, lidx, axis=1, copy=copy)\r\n  10222         left = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\r\n> 10224     right = other._reindex_indexer(join_index, ridx, copy)\r\n  10226 else:\r\n  10227     # one has > 1 ndim\r\n  10228     fdata = self._mgr\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/series.py:4779, in Series._reindex_indexer(self, new_index, indexer, copy)\r\n   4776         return self.copy(deep=copy)\r\n   4777     return self\r\n-> 4779 new_values = algorithms.take_nd(\r\n   4780     self._values, indexer, allow_fill=True, fill_value=None\r\n   4781 )\r\n   4782 return self._constructor(new_values, index=new_index, copy=False)\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/array_algos\/take.py:115, in take_nd(arr, indexer, axis, fill_value, allow_fill)\r\n    110         arr = cast(\"NDArrayBackedExtensionArray\", arr)\r\n    111         return arr.take(\r\n    112             indexer, fill_value=fill_value, allow_fill=allow_fill, axis=axis\r\n    113         )\r\n--> 115     return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\r\n    117 arr = np.asarray(arr)\r\n    118 return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/masked.py:905, in BaseMaskedArray.take(self, indexer, allow_fill, fill_value, axis)\r\n    894 def take(\r\n    895     self,\r\n    896     indexer,\r\n   (...)\r\n    902     # we always fill with 1 internally\r\n    903     # to avoid upcasting\r\n    904     data_fill_value = self._internal_fill_value if isna(fill_value) else fill_value\r\n--> 905     result = take(\r\n    906         self._data,\r\n    907         indexer,\r\n    908         fill_value=data_fill_value,\r\n    909         allow_fill=allow_fill,\r\n    910         axis=axis,\r\n    911     )\r\n    913     mask = take(\r\n    914         self._mask, indexer, fill_value=True, allow_fill=allow_fill, axis=axis\r\n    915     )\r\n    917     # if we are filling\r\n    918     # we only fill where the indexer is null\r\n    919     # not existing missing values\r\n    920     # TODO(jreback) what if we have a non-na float as a fill value?\r\n\r\nFile ~\/beftett\/.venv\/lib\/python3.10\/site-packages\/pandas\/core\/algorithms.py:1309, in take(arr, indices, axis, allow_fill, fill_value)\r\n   1306 if not is_array_like(arr):\r\n   1307     arr = np.asarray(arr)\r\n-> 1309 indices = ensure_platform_int(indices)\r\n   1311 if allow_fill:\r\n   1312     # Pandas style, -1 means NA\r\n   1313     validate_indices(indices, arr.shape[axis])\r\n\r\nFile pandas\/_libs\/algos_common_helper.pxi:22, in pandas._libs.algos.ensure_platform_int()\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\r\n\r\n```\r\n\r\n<\/details>\r\n\r\nIf any of these three conditions is removed, the subtraction succeeds.\r\n\r\n### Expected Behavior\r\n\r\nThe subtraction should succeed.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\ncommit: 2a953cf80b77e4348bf50ed724f8abc0d814d9dd   \r\npython: 3.10.9.final.0   \r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.133+\r\nVersion             : #1 SMP Sat Dec 30 11:18:04 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : en_US.UTF-8\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\npandas              : 2.1.3\r\nnumpy               : 1.24.4\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.3\r\npip                 : 23.2.1\r\nCython              : None\r\npytest              : 7.4.4\r\nhypothesis          : 6.91.0\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 5.0.0\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.19.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.12.2\r\ngcsfs               : 2023.12.2post1\r\nmatplotlib          : 3.8.2\r\nnumba               : 0.57.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 13.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : 2.0.24\r\ntables              : None\r\ntabulate            : None\r\nxarray              : 2023.8.0\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n<\/details>\r\n","comments":["Thanks for the report. Might be related to #45564 - cc @phofl.\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/95308514e1221200e4526dfaf248283f3d7ade06\/pandas\/core\/series.py#L4637-L4646\r\n\r\nPrior to this PR, we would not call `take_nd` when `indexer is None`. Now we might be passing `indexer=None` to `take_nd`. For NumPy arrays, we end up in `_take_nd_array` which has the lines:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/95308514e1221200e4526dfaf248283f3d7ade06\/pandas\/core\/array_algos\/take.py#L127-L129\r\n\r\nbut no such logic exists for EAs. Perhaps it should?","take","take","take","take","take"],"labels":["Bug","Numeric Operations","ExtensionArray"]},{"title":"BUG: Test failures on 32-bit x86 with pyarrow installed","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\npython -m pytest # ;-)\n```\n\n\n### Issue Description\n\nWhen running the test suite on 32-bit x86 with `pyarrow` installed, I'm getting the following test failures (compared to a run without `pyarrow`):\r\n\r\n```\r\nFAILED pandas\/tests\/indexes\/numeric\/test_indexing.py::TestGetIndexer::test_get_indexer_arrow_dictionary_target - AssertionError: numpy array are different\r\nFAILED pandas\/tests\/interchange\/test_impl.py::test_large_string_pyarrow - OverflowError: Python int too large to convert to C ssize_t\r\nFAILED pandas\/tests\/frame\/methods\/test_join.py::test_join_on_single_col_dup_on_right[string[pyarrow]] - ValueError: putmask: output array is read-only\r\nFAILED pandas\/tests\/reshape\/merge\/test_multi.py::TestMergeMulti::test_left_join_multi_index[False-True] - ValueError: putmask: output array is read-only\r\n```\r\n\r\n<details>\r\n<summary>Tracebacks<\/summary>\r\n\r\n```pytb\r\n_______________________________________ TestGetIndexer.test_get_indexer_arrow_dictionary_target _______________________________________\r\n[gw8] linux -- Python 3.11.7 \/var\/tmp\/portage\/dev-python\/pandas-2.2.0-r1\/work\/pandas-2.2.0-python3_11\/install\/usr\/bin\/python3.11\r\n\r\nself = <pandas.tests.indexes.numeric.test_indexing.TestGetIndexer object at 0xd9180110>\r\n\r\n    def test_get_indexer_arrow_dictionary_target(self):\r\n        pa = pytest.importorskip(\"pyarrow\")\r\n        target = Index(\r\n            ArrowExtensionArray(\r\n                pa.array([1, 2], type=pa.dictionary(pa.int8(), pa.int8()))\r\n            )\r\n        )\r\n        idx = Index([1])\r\n    \r\n        result = idx.get_indexer(target)\r\n        expected = np.array([0, -1], dtype=np.int64)\r\n>       tm.assert_numpy_array_equal(result, expected)\r\nE       AssertionError: numpy array are different\r\nE       \r\nE       Attribute \"dtype\" are different\r\nE       [left]:  int32\r\nE       [right]: int64\r\n\r\nexpected   = array([ 0, -1], dtype=int64)\r\nidx        = Index([1], dtype='int64')\r\npa         = <module 'pyarrow' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/__init__.py'>\r\nresult     = array([ 0, -1], dtype=int32)\r\nself       = <pandas.tests.indexes.numeric.test_indexing.TestGetIndexer object at 0xd9180110>\r\ntarget     = Index([1, 2], dtype='dictionary<values=int8, indices=int8, ordered=0>[pyarrow]')\r\n\r\npandas\/tests\/indexes\/numeric\/test_indexing.py:406: AssertionError\r\n______________________________________________________ test_large_string_pyarrow ______________________________________________________\r\n[gw8] linux -- Python 3.11.7 \/var\/tmp\/portage\/dev-python\/pandas-2.2.0-r1\/work\/pandas-2.2.0-python3_11\/install\/usr\/bin\/python3.11\r\n\r\n    def test_large_string_pyarrow():\r\n        # GH 52795\r\n        pa = pytest.importorskip(\"pyarrow\", \"11.0.0\")\r\n    \r\n        arr = [\"Mon\", \"Tue\"]\r\n        table = pa.table({\"weekday\": pa.array(arr, \"large_string\")})\r\n        exchange_df = table.__dataframe__()\r\n        result = from_dataframe(exchange_df)\r\n        expected = pd.DataFrame({\"weekday\": [\"Mon\", \"Tue\"]})\r\n        tm.assert_frame_equal(result, expected)\r\n    \r\n        # check round-trip\r\n>       assert pa.Table.equals(pa.interchange.from_dataframe(result), table)\r\n\r\narr        = ['Mon', 'Tue']\r\nexchange_df = <pyarrow.interchange.dataframe._PyArrowDataFrame object at 0xd877c7d0>\r\nexpected   =   weekday\r\n0     Mon\r\n1     Tue\r\npa         = <module 'pyarrow' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/__init__.py'>\r\nresult     =   weekday\r\n0     Mon\r\n1     Tue\r\ntable      = pyarrow.Table\r\nweekday: large_string\r\n----\r\nweekday: [[\"Mon\",\"Tue\"]]\r\n\r\npandas\/tests\/interchange\/test_impl.py:104: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\/usr\/lib\/python3.11\/site-packages\/pyarrow\/interchange\/from_dataframe.py:113: in from_dataframe\r\n    return _from_dataframe(df.__dataframe__(allow_copy=allow_copy),\r\n        allow_copy = True\r\n        df         =   weekday\r\n0     Mon\r\n1     Tue\r\n\/usr\/lib\/python3.11\/site-packages\/pyarrow\/interchange\/from_dataframe.py:136: in _from_dataframe\r\n    batch = protocol_df_chunk_to_pyarrow(chunk, allow_copy)\r\n        allow_copy = True\r\n        batches    = []\r\n        chunk      = <pandas.core.interchange.dataframe.PandasDataFrameXchg object at 0xccc70410>\r\n        df         = <pandas.core.interchange.dataframe.PandasDataFrameXchg object at 0xccc70410>\r\n\/usr\/lib\/python3.11\/site-packages\/pyarrow\/interchange\/from_dataframe.py:182: in protocol_df_chunk_to_pyarrow\r\n    columns[name] = column_to_array(col, allow_copy)\r\n        allow_copy = True\r\n        col        = <pandas.core.interchange.column.PandasColumn object at 0xccc703d0>\r\n        columns    = {}\r\n        df         = <pandas.core.interchange.dataframe.PandasDataFrameXchg object at 0xccc70410>\r\n        dtype      = <DtypeKind.STRING: 21>\r\n        name       = 'weekday'\r\n\/usr\/lib\/python3.11\/site-packages\/pyarrow\/interchange\/from_dataframe.py:214: in column_to_array\r\n    data = buffers_to_array(buffers, data_type,\r\n        allow_copy = True\r\n        buffers    = {'data': (PandasBuffer({'bufsize': 6, 'ptr': 3445199088, 'device': 'CPU'}),\r\n          (<DtypeKind.STRING: 21>, 8, 'u', '=')),\r\n 'offsets': (PandasBuffer({'bufsize': 24, 'ptr': 1546049072, 'device': 'CPU'}),\r\n             (<DtypeKind.INT: 0>, 64, 'l', '=')),\r\n 'validity': (PandasBuffer({'bufsize': 2, 'ptr': 1544334624, 'device': 'CPU'}),\r\n              (<DtypeKind.BOOL: 20>, 8, 'b', '='))}\r\n        col        = <pandas.core.interchange.column.PandasColumn object at 0xccc703d0>\r\n        data_type  = (<DtypeKind.STRING: 21>, 8, 'u', '=')\r\n\/usr\/lib\/python3.11\/site-packages\/pyarrow\/interchange\/from_dataframe.py:396: in buffers_to_array\r\n    data_pa_buffer = pa.foreign_buffer(data_buff.ptr, data_buff.bufsize,\r\n        _          = (<DtypeKind.STRING: 21>, 8, 'u', '=')\r\n        allow_copy = True\r\n        buffers    = {'data': (PandasBuffer({'bufsize': 6, 'ptr': 3445199088, 'device': 'CPU'}),\r\n          (<DtypeKind.STRING: 21>, 8, 'u', '=')),\r\n 'offsets': (PandasBuffer({'bufsize': 24, 'ptr': 1546049072, 'device': 'CPU'}),\r\n             (<DtypeKind.INT: 0>, 64, 'l', '=')),\r\n 'validity': (PandasBuffer({'bufsize': 2, 'ptr': 1544334624, 'device': 'CPU'}),\r\n              (<DtypeKind.BOOL: 20>, 8, 'b', '='))}\r\n        data_buff  = PandasBuffer({'bufsize': 6, 'ptr': 3445199088, 'device': 'CPU'})\r\n        data_type  = (<DtypeKind.STRING: 21>, 8, 'u', '=')\r\n        describe_null = (<ColumnNullType.USE_BYTEMASK: 4>, 0)\r\n        length     = 2\r\n        offset     = 0\r\n        offset_buff = PandasBuffer({'bufsize': 24, 'ptr': 1546049072, 'device': 'CPU'})\r\n        offset_dtype = (<DtypeKind.INT: 0>, 64, 'l', '=')\r\n        validity_buff = PandasBuffer({'bufsize': 2, 'ptr': 1544334624, 'device': 'CPU'})\r\n        validity_dtype = (<DtypeKind.BOOL: 20>, 8, 'b', '=')\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n>   ???\r\nE   OverflowError: Python int too large to convert to C ssize_t\r\n\r\n\r\npyarrow\/io.pxi:1990: OverflowError\r\n________________________________________ test_join_on_single_col_dup_on_right[string[pyarrow]] ________________________________________\r\n[gw5] linux -- Python 3.11.7 \/var\/tmp\/portage\/dev-python\/pandas-2.2.0-r1\/work\/pandas-2.2.0-python3_11\/install\/usr\/bin\/python3.11\r\n\r\nleft_no_dup =    a       b\r\n0  a     cat\r\n1  b     dog\r\n2  c  weasel\r\n3  d   horse\r\nright_w_dups =                         c\r\na                        \r\n<NA>                 meow\r\n<NA>                 bark\r\n<NA>  um... weasel noise?\r\n<NA>                  nay\r\n<NA>                chirp\r\ne                     moo\r\ndtype = 'string[pyarrow]'\r\n\r\n    @pytest.mark.parametrize(\"dtype\", [\"object\", \"string[pyarrow]\"])\r\n    def test_join_on_single_col_dup_on_right(left_no_dup, right_w_dups, dtype):\r\n        # GH 46622\r\n        # Dups on right allowed by one_to_many constraint\r\n        if dtype == \"string[pyarrow]\":\r\n            pytest.importorskip(\"pyarrow\")\r\n        left_no_dup = left_no_dup.astype(dtype)\r\n        right_w_dups.index = right_w_dups.index.astype(dtype)\r\n>       left_no_dup.join(\r\n            right_w_dups,\r\n            on=\"a\",\r\n            validate=\"one_to_many\",\r\n        )\r\n\r\ndtype      = 'string[pyarrow]'\r\nleft_no_dup =    a       b\r\n0  a     cat\r\n1  b     dog\r\n2  c  weasel\r\n3  d   horse\r\nright_w_dups =                         c\r\na                        \r\n<NA>                 meow\r\n<NA>                 bark\r\n<NA>  um... weasel noise?\r\n<NA>                  nay\r\n<NA>                chirp\r\ne                     moo\r\n\r\npandas\/tests\/frame\/methods\/test_join.py:169: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npandas\/core\/frame.py:10730: in join\r\n    return merge(\r\n        concat     = <function concat at 0xec6cdc58>\r\n        how        = 'left'\r\n        lsuffix    = ''\r\n        merge      = <function merge at 0xec226e88>\r\n        on         = 'a'\r\n        other      =                         c\r\na                        \r\n<NA>                 meow\r\n<NA>                 bark\r\n<NA>  um... weasel noise?\r\n<NA>                  nay\r\n<NA>                chirp\r\ne                     moo\r\n        rsuffix    = ''\r\n        self       =    a       b\r\n0  a     cat\r\n1  b     dog\r\n2  c  weasel\r\n3  d   horse\r\n        sort       = False\r\n        validate   = 'one_to_many'\r\npandas\/core\/reshape\/merge.py:184: in merge\r\n    return op.get_result(copy=copy)\r\n        copy       = None\r\n        how        = 'left'\r\n        indicator  = False\r\n        left       =    a       b\r\n0  a     cat\r\n1  b     dog\r\n2  c  weasel\r\n3  d   horse\r\n        left_df    =    a       b\r\n0  a     cat\r\n1  b     dog\r\n2  c  weasel\r\n3  d   horse\r\n        left_index = False\r\n        left_on    = 'a'\r\n        on         = None\r\n        op         = <pandas.core.reshape.merge._MergeOperation object at 0xcbf60b30>\r\n        right      =                         c\r\na                        \r\n<NA>                 meow\r\n<NA>                 bark\r\n<NA>  um... weasel noise?\r\n<NA>                  nay\r\n<NA>                chirp\r\ne                     moo\r\n        right_df   =                         c\r\na                        \r\n<NA>                 meow\r\n<NA>                 bark\r\n<NA>  um... weasel noise?\r\n<NA>                  nay\r\n<NA>                chirp\r\ne                     moo\r\n        right_index = True\r\n        right_on   = None\r\n        sort       = False\r\n        suffixes   = ('', '')\r\n        validate   = 'one_to_many'\r\npandas\/core\/reshape\/merge.py:886: in get_result\r\n    join_index, left_indexer, right_indexer = self._get_join_info()\r\n        copy       = None\r\n        self       = <pandas.core.reshape.merge._MergeOperation object at 0xcbf60b30>\r\npandas\/core\/reshape\/merge.py:1142: in _get_join_info\r\n    join_index, left_indexer, right_indexer = _left_join_on_index(\r\n        left_ax    = RangeIndex(start=0, stop=4, step=1)\r\n        right_ax   = Index([<NA>, <NA>, <NA>, <NA>, <NA>, 'e'], dtype='string', name='a')\r\n        self       = <pandas.core.reshape.merge._MergeOperation object at 0xcbf60b30>\r\npandas\/core\/reshape\/merge.py:2385: in _left_join_on_index\r\n    left_key, right_key, count = _factorize_keys(lkey, rkey, sort=sort)\r\n        join_keys  = [<ArrowStringArray>\r\n['a', 'b', 'c', 'd']\r\nLength: 4, dtype: string]\r\n        left_ax    = RangeIndex(start=0, stop=4, step=1)\r\n        lkey       = <ArrowStringArray>\r\n['a', 'b', 'c', 'd']\r\nLength: 4, dtype: string\r\n        right_ax   = Index([<NA>, <NA>, <NA>, <NA>, <NA>, 'e'], dtype='string', name='a')\r\n        rkey       = <ArrowStringArray>\r\n[<NA>, <NA>, <NA>, <NA>, <NA>, 'e']\r\nLength: 6, dtype: string\r\n        sort       = False\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nlk = <pyarrow.lib.ChunkedArray object at 0xcba48fa0>\r\n[\r\n  [\r\n    \"a\",\r\n    \"b\",\r\n    \"c\",\r\n    \"d\"\r\n  ]\r\n]\r\nrk = <pyarrow.lib.ChunkedArray object at 0xcbf280f0>\r\n[\r\n  [\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    \"e\"\r\n  ]\r\n]\r\nsort = False\r\n\r\n    def _factorize_keys(\r\n        lk: ArrayLike, rk: ArrayLike, sort: bool = True\r\n    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:\r\n        \"\"\"\r\n        Encode left and right keys as enumerated types.\r\n    \r\n        This is used to get the join indexers to be used when merging DataFrames.\r\n    \r\n        Parameters\r\n        ----------\r\n        lk : ndarray, ExtensionArray\r\n            Left key.\r\n        rk : ndarray, ExtensionArray\r\n            Right key.\r\n        sort : bool, defaults to True\r\n            If True, the encoding is done such that the unique elements in the\r\n            keys are sorted.\r\n    \r\n        Returns\r\n        -------\r\n        np.ndarray[np.intp]\r\n            Left (resp. right if called with `key='right'`) labels, as enumerated type.\r\n        np.ndarray[np.intp]\r\n            Right (resp. left if called with `key='right'`) labels, as enumerated type.\r\n        int\r\n            Number of unique elements in union of left and right labels.\r\n    \r\n        See Also\r\n        --------\r\n        merge : Merge DataFrame or named Series objects\r\n            with a database-style join.\r\n        algorithms.factorize : Encode the object as an enumerated type\r\n            or categorical variable.\r\n    \r\n        Examples\r\n        --------\r\n        >>> lk = np.array([\"a\", \"c\", \"b\"])\r\n        >>> rk = np.array([\"a\", \"c\"])\r\n    \r\n        Here, the unique values are `'a', 'b', 'c'`. With the default\r\n        `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:\r\n    \r\n        >>> pd.core.reshape.merge._factorize_keys(lk, rk)\r\n        (array([0, 2, 1]), array([0, 2]), 3)\r\n    \r\n        With the `sort=False`, the encoding will correspond to the order\r\n        in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:\r\n    \r\n        >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)\r\n        (array([0, 1, 2]), array([0, 1]), 3)\r\n        \"\"\"\r\n        # TODO: if either is a RangeIndex, we can likely factorize more efficiently?\r\n    \r\n        if (\r\n            isinstance(lk.dtype, DatetimeTZDtype) and isinstance(rk.dtype, DatetimeTZDtype)\r\n        ) or (lib.is_np_dtype(lk.dtype, \"M\") and lib.is_np_dtype(rk.dtype, \"M\")):\r\n            # Extract the ndarray (UTC-localized) values\r\n            # Note: we dont need the dtypes to match, as these can still be compared\r\n            lk, rk = cast(\"DatetimeArray\", lk)._ensure_matching_resos(rk)\r\n            lk = cast(\"DatetimeArray\", lk)._ndarray\r\n            rk = cast(\"DatetimeArray\", rk)._ndarray\r\n    \r\n        elif (\r\n            isinstance(lk.dtype, CategoricalDtype)\r\n            and isinstance(rk.dtype, CategoricalDtype)\r\n            and lk.dtype == rk.dtype\r\n        ):\r\n            assert isinstance(lk, Categorical)\r\n            assert isinstance(rk, Categorical)\r\n            # Cast rk to encoding so we can compare codes with lk\r\n    \r\n            rk = lk._encode_with_my_categories(rk)\r\n    \r\n            lk = ensure_int64(lk.codes)\r\n            rk = ensure_int64(rk.codes)\r\n    \r\n        elif isinstance(lk, ExtensionArray) and lk.dtype == rk.dtype:\r\n            if (isinstance(lk.dtype, ArrowDtype) and is_string_dtype(lk.dtype)) or (\r\n                isinstance(lk.dtype, StringDtype)\r\n                and lk.dtype.storage in [\"pyarrow\", \"pyarrow_numpy\"]\r\n            ):\r\n                import pyarrow as pa\r\n                import pyarrow.compute as pc\r\n    \r\n                len_lk = len(lk)\r\n                lk = lk._pa_array  # type: ignore[attr-defined]\r\n                rk = rk._pa_array  # type: ignore[union-attr]\r\n                dc = (\r\n                    pa.chunked_array(lk.chunks + rk.chunks)  # type: ignore[union-attr]\r\n                    .combine_chunks()\r\n                    .dictionary_encode()\r\n                )\r\n    \r\n                llab, rlab, count = (\r\n                    pc.fill_null(dc.indices[slice(len_lk)], -1)\r\n                    .to_numpy()\r\n                    .astype(np.intp, copy=False),\r\n                    pc.fill_null(dc.indices[slice(len_lk, None)], -1)\r\n                    .to_numpy()\r\n                    .astype(np.intp, copy=False),\r\n                    len(dc.dictionary),\r\n                )\r\n    \r\n                if sort:\r\n                    uniques = dc.dictionary.to_numpy(zero_copy_only=False)\r\n                    llab, rlab = _sort_labels(uniques, llab, rlab)\r\n    \r\n                if dc.null_count > 0:\r\n                    lmask = llab == -1\r\n                    lany = lmask.any()\r\n                    rmask = rlab == -1\r\n                    rany = rmask.any()\r\n                    if lany:\r\n                        np.putmask(llab, lmask, count)\r\n                    if rany:\r\n>                       np.putmask(rlab, rmask, count)\r\nE                       ValueError: putmask: output array is read-only\r\n\r\ncount      = 5\r\ndc         = <pyarrow.lib.DictionaryArray object at 0xcb83ffb0>\r\n\r\n-- dictionary:\r\n  [\r\n    \"a\",\r\n    \"b\",\r\n    \"c\",\r\n    \"d\",\r\n    \"e\"\r\n  ]\r\n-- indices:\r\n  [\r\n    0,\r\n    1,\r\n    2,\r\n    3,\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    4\r\n  ]\r\nlany       = False\r\nlen_lk     = 4\r\nlk         = <pyarrow.lib.ChunkedArray object at 0xcba48fa0>\r\n[\r\n  [\r\n    \"a\",\r\n    \"b\",\r\n    \"c\",\r\n    \"d\"\r\n  ]\r\n]\r\nllab       = array([0, 1, 2, 3])\r\nlmask      = array([False, False, False, False])\r\npa         = <module 'pyarrow' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/__init__.py'>\r\npc         = <module 'pyarrow.compute' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/compute.py'>\r\nrany       = True\r\nrk         = <pyarrow.lib.ChunkedArray object at 0xcbf280f0>\r\n[\r\n  [\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    null,\r\n    \"e\"\r\n  ]\r\n]\r\nrlab       = array([-1, -1, -1, -1, -1,  4])\r\nrmask      = array([ True,  True,  True,  True,  True, False])\r\nsort       = False\r\n\r\npandas\/core\/reshape\/merge.py:2514: ValueError\r\n________________________________________ TestMergeMulti.test_left_join_multi_index[False-True] ________________________________________\r\n[gw3] linux -- Python 3.11.7 \/var\/tmp\/portage\/dev-python\/pandas-2.2.0-r1\/work\/pandas-2.2.0-python3_11\/install\/usr\/bin\/python3.11\r\n\r\nself = <pandas.tests.reshape.merge.test_multi.TestMergeMulti object at 0xd20d8ff0>, sort = False, infer_string = True\r\n\r\n    @pytest.mark.parametrize(\r\n        \"infer_string\", [False, pytest.param(True, marks=td.skip_if_no(\"pyarrow\"))]\r\n    )\r\n    @pytest.mark.parametrize(\"sort\", [True, False])\r\n    def test_left_join_multi_index(self, sort, infer_string):\r\n        with option_context(\"future.infer_string\", infer_string):\r\n            icols = [\"1st\", \"2nd\", \"3rd\"]\r\n    \r\n            def bind_cols(df):\r\n                iord = lambda a: 0 if a != a else ord(a)\r\n                f = lambda ts: ts.map(iord) - ord(\"a\")\r\n                return f(df[\"1st\"]) + f(df[\"3rd\"]) * 1e2 + df[\"2nd\"].fillna(0) * 10\r\n    \r\n            def run_asserts(left, right, sort):\r\n                res = left.join(right, on=icols, how=\"left\", sort=sort)\r\n    \r\n                assert len(left) < len(res) + 1\r\n                assert not res[\"4th\"].isna().any()\r\n                assert not res[\"5th\"].isna().any()\r\n    \r\n                tm.assert_series_equal(res[\"4th\"], -res[\"5th\"], check_names=False)\r\n                result = bind_cols(res.iloc[:, :-2])\r\n                tm.assert_series_equal(res[\"4th\"], result, check_names=False)\r\n                assert result.name is None\r\n    \r\n                if sort:\r\n                    tm.assert_frame_equal(res, res.sort_values(icols, kind=\"mergesort\"))\r\n    \r\n                out = merge(left, right.reset_index(), on=icols, sort=sort, how=\"left\")\r\n    \r\n                res.index = RangeIndex(len(res))\r\n                tm.assert_frame_equal(out, res)\r\n    \r\n            lc = list(map(chr, np.arange(ord(\"a\"), ord(\"z\") + 1)))\r\n            left = DataFrame(\r\n                np.random.default_rng(2).choice(lc, (50, 2)), columns=[\"1st\", \"3rd\"]\r\n            )\r\n            # Explicit cast to float to avoid implicit cast when setting nan\r\n            left.insert(\r\n                1,\r\n                \"2nd\",\r\n                np.random.default_rng(2).integers(0, 10, len(left)).astype(\"float\"),\r\n            )\r\n    \r\n            i = np.random.default_rng(2).permutation(len(left))\r\n            right = left.iloc[i].copy()\r\n    \r\n            left[\"4th\"] = bind_cols(left)\r\n            right[\"5th\"] = -bind_cols(right)\r\n            right.set_index(icols, inplace=True)\r\n    \r\n            run_asserts(left, right, sort)\r\n    \r\n            # inject some nulls\r\n            left.loc[1::4, \"1st\"] = np.nan\r\n            left.loc[2::5, \"2nd\"] = np.nan\r\n            left.loc[3::6, \"3rd\"] = np.nan\r\n            left[\"4th\"] = bind_cols(left)\r\n    \r\n            i = np.random.default_rng(2).permutation(len(left))\r\n            right = left.iloc[i, :-1]\r\n            right[\"5th\"] = -bind_cols(right)\r\n            right.set_index(icols, inplace=True)\r\n    \r\n>           run_asserts(left, right, sort)\r\n\r\nbind_cols  = <function TestMergeMulti.test_left_join_multi_index.<locals>.bind_cols at 0xc9928b18>\r\ni          = array([ 6, 40, 33, 38,  7, 46, 28, 45,  5, 34, 12, 18, 27,  3,  9, 39, 42,\r\n       23,  0, 26,  4, 10, 14, 41, 16, 43, 15, 48, 13, 24, 20, 25, 22, 49,\r\n        2, 11, 32, 44, 47, 17, 19, 37, 21, 29, 31, 30, 35, 36,  8,  1])\r\nicols      = ['1st', '2nd', '3rd']\r\ninfer_string = True\r\nlc         = ['a',\r\n 'b',\r\n 'c',\r\n 'd',\r\n 'e',\r\n 'f',\r\n 'g',\r\n 'h',\r\n 'i',\r\n 'j',\r\n 'k',\r\n 'l',\r\n 'm',\r\n 'n',\r\n 'o',\r\n 'p',\r\n 'q',\r\n 'r',\r\n 's',\r\n 't',\r\n 'u',\r\n 'v',\r\n 'w',\r\n 'x',\r\n 'y',\r\n 'z']\r\nleft       =     1st  2nd  3rd     4th\r\n0     v  8.0    g   701.0\r\n1   NaN  2.0    h   623.0\r\n2     k  NaN    v  2110.0\r\n3     l  2.0  NaN -9669.0\r\n4     i  4.0    p  1548.0\r\n5   NaN  8.0    s  1783.0\r\n6     z  4.0    e   465.0\r\n7     w  NaN    b   122.0\r\n8     o  3.0    h   744.0\r\n9   NaN  6.0  NaN -9737.0\r\n10    h  8.0    o  1487.0\r\n11    g  7.0    d   376.0\r\n12    t  NaN    l  1119.0\r\n13  NaN  1.0    r  1613.0\r\n14    y  8.0    k  1104.0\r\n15    f  0.0  NaN -9695.0\r\n16    y  5.0    z  2574.0\r\n17  NaN  NaN    r  1603.0\r\n18    j  2.0    k  1029.0\r\n19    b  6.0    e   461.0\r\n20    i  3.0    i   838.0\r\n21  NaN  5.0  NaN -9747.0\r\n22    s  NaN    x  2318.0\r\n23    w  1.0    u  2032.0\r\n24    z  7.0    i   895.0\r\n25  NaN  4.0    y  2343.0\r\n26    f  6.0    m  1265.0\r\n27    o  NaN  NaN -9686.0\r\n28    s  9.0    c   308.0\r\n29  NaN  4.0    c   143.0\r\n30    y  2.0    f   544.0\r\n31    l  6.0    w  2271.0\r\n32    n  NaN    r  1713.0\r\n33  NaN  9.0  NaN -9707.0\r\n34    p  8.0    q  1695.0\r\n35    l  6.0    k  1071.0\r\n36    p  3.0    n  1345.0\r\n37  NaN  NaN    p  1403.0\r\n38    m  0.0    w  2212.0\r\n39    f  1.0  NaN -9685.0\r\n40    m  3.0    x  2342.0\r\n41  NaN  3.0    p  1433.0\r\n42    b  NaN    v  2101.0\r\n43    l  5.0    m  1261.0\r\n44    c  6.0    s  1862.0\r\n45  NaN  8.0  NaN -9717.0\r\n46    t  8.0    n  1399.0\r\n47    b  NaN    f   501.0\r\n48    g  9.0    c   296.0\r\n49  NaN  3.0    b    33.0\r\nright      =                 5th\r\n1st 2nd 3rd        \r\nz   4.0 e    -465.0\r\nm   3.0 x   -2342.0\r\nnan 9.0 nan  9707.0\r\nm   0.0 w   -2212.0\r\nw   NaN b    -122.0\r\nt   8.0 n   -1399.0\r\ns   9.0 c    -308.0\r\nnan 8.0 nan  9717.0\r\n        s   -1783.0\r\np   8.0 q   -1695.0\r\nt   NaN l   -1119.0\r\nj   2.0 k   -1029.0\r\no   NaN nan  9686.0\r\nl   2.0 nan  9669.0\r\nnan 6.0 nan  9737.0\r\nf   1.0 nan  9685.0\r\nb   NaN v   -2101.0\r\nw   1.0 u   -2032.0\r\nv   8.0 g    -701.0\r\nf   6.0 m   -1265.0\r\ni   4.0 p   -1548.0\r\nh   8.0 o   -1487.0\r\ny   8.0 k   -1104.0\r\nnan 3.0 p   -1433.0\r\ny   5.0 z   -2574.0\r\nl   5.0 m   -1261.0\r\nf   0.0 nan  9695.0\r\ng   9.0 c    -296.0\r\nnan 1.0 r   -1613.0\r\nz   7.0 i    -895.0\r\ni   3.0 i    -838.0\r\nnan 4.0 y   -2343.0\r\ns   NaN x   -2318.0\r\nnan 3.0 b     -33.0\r\nk   NaN v   -2110.0\r\ng   7.0 d    -376.0\r\nn   NaN r   -1713.0\r\nc   6.0 s   -1862.0\r\nb   NaN f    -501.0\r\nnan NaN r   -1603.0\r\nb   6.0 e    -461.0\r\nnan NaN p   -1403.0\r\n    5.0 nan  9747.0\r\n    4.0 c    -143.0\r\nl   6.0 w   -2271.0\r\ny   2.0 f    -544.0\r\nl   6.0 k   -1071.0\r\np   3.0 n   -1345.0\r\no   3.0 h    -744.0\r\nnan 2.0 h    -623.0\r\nrun_asserts = <function TestMergeMulti.test_left_join_multi_index.<locals>.run_asserts at 0xc9928bb8>\r\nself       = <pandas.tests.reshape.merge.test_multi.TestMergeMulti object at 0xd20d8ff0>\r\nsort       = False\r\n\r\npandas\/tests\/reshape\/merge\/test_multi.py:158: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npandas\/tests\/reshape\/merge\/test_multi.py:108: in run_asserts\r\n    res = left.join(right, on=icols, how=\"left\", sort=sort)\r\n        bind_cols  = <function TestMergeMulti.test_left_join_multi_index.<locals>.bind_cols at 0xc9928b18>\r\n        icols      = ['1st', '2nd', '3rd']\r\n        left       =     1st  2nd  3rd     4th\r\n0     v  8.0    g   701.0\r\n1   NaN  2.0    h   623.0\r\n2     k  NaN    v  2110.0\r\n3     l  2.0  NaN -9669.0\r\n4     i  4.0    p  1548.0\r\n5   NaN  8.0    s  1783.0\r\n6     z  4.0    e   465.0\r\n7     w  NaN    b   122.0\r\n8     o  3.0    h   744.0\r\n9   NaN  6.0  NaN -9737.0\r\n10    h  8.0    o  1487.0\r\n11    g  7.0    d   376.0\r\n12    t  NaN    l  1119.0\r\n13  NaN  1.0    r  1613.0\r\n14    y  8.0    k  1104.0\r\n15    f  0.0  NaN -9695.0\r\n16    y  5.0    z  2574.0\r\n17  NaN  NaN    r  1603.0\r\n18    j  2.0    k  1029.0\r\n19    b  6.0    e   461.0\r\n20    i  3.0    i   838.0\r\n21  NaN  5.0  NaN -9747.0\r\n22    s  NaN    x  2318.0\r\n23    w  1.0    u  2032.0\r\n24    z  7.0    i   895.0\r\n25  NaN  4.0    y  2343.0\r\n26    f  6.0    m  1265.0\r\n27    o  NaN  NaN -9686.0\r\n28    s  9.0    c   308.0\r\n29  NaN  4.0    c   143.0\r\n30    y  2.0    f   544.0\r\n31    l  6.0    w  2271.0\r\n32    n  NaN    r  1713.0\r\n33  NaN  9.0  NaN -9707.0\r\n34    p  8.0    q  1695.0\r\n35    l  6.0    k  1071.0\r\n36    p  3.0    n  1345.0\r\n37  NaN  NaN    p  1403.0\r\n38    m  0.0    w  2212.0\r\n39    f  1.0  NaN -9685.0\r\n40    m  3.0    x  2342.0\r\n41  NaN  3.0    p  1433.0\r\n42    b  NaN    v  2101.0\r\n43    l  5.0    m  1261.0\r\n44    c  6.0    s  1862.0\r\n45  NaN  8.0  NaN -9717.0\r\n46    t  8.0    n  1399.0\r\n47    b  NaN    f   501.0\r\n48    g  9.0    c   296.0\r\n49  NaN  3.0    b    33.0\r\n        right      =                 5th\r\n1st 2nd 3rd        \r\nz   4.0 e    -465.0\r\nm   3.0 x   -2342.0\r\nnan 9.0 nan  9707.0\r\nm   0.0 w   -2212.0\r\nw   NaN b    -122.0\r\nt   8.0 n   -1399.0\r\ns   9.0 c    -308.0\r\nnan 8.0 nan  9717.0\r\n        s   -1783.0\r\np   8.0 q   -1695.0\r\nt   NaN l   -1119.0\r\nj   2.0 k   -1029.0\r\no   NaN nan  9686.0\r\nl   2.0 nan  9669.0\r\nnan 6.0 nan  9737.0\r\nf   1.0 nan  9685.0\r\nb   NaN v   -2101.0\r\nw   1.0 u   -2032.0\r\nv   8.0 g    -701.0\r\nf   6.0 m   -1265.0\r\ni   4.0 p   -1548.0\r\nh   8.0 o   -1487.0\r\ny   8.0 k   -1104.0\r\nnan 3.0 p   -1433.0\r\ny   5.0 z   -2574.0\r\nl   5.0 m   -1261.0\r\nf   0.0 nan  9695.0\r\ng   9.0 c    -296.0\r\nnan 1.0 r   -1613.0\r\nz   7.0 i    -895.0\r\ni   3.0 i    -838.0\r\nnan 4.0 y   -2343.0\r\ns   NaN x   -2318.0\r\nnan 3.0 b     -33.0\r\nk   NaN v   -2110.0\r\ng   7.0 d    -376.0\r\nn   NaN r   -1713.0\r\nc   6.0 s   -1862.0\r\nb   NaN f    -501.0\r\nnan NaN r   -1603.0\r\nb   6.0 e    -461.0\r\nnan NaN p   -1403.0\r\n    5.0 nan  9747.0\r\n    4.0 c    -143.0\r\nl   6.0 w   -2271.0\r\ny   2.0 f    -544.0\r\nl   6.0 k   -1071.0\r\np   3.0 n   -1345.0\r\no   3.0 h    -744.0\r\nnan 2.0 h    -623.0\r\n        sort       = False\r\npandas\/core\/frame.py:10730: in join\r\n    return merge(\r\n        concat     = <function concat at 0xec6bec58>\r\n        how        = 'left'\r\n        lsuffix    = ''\r\n        merge      = <function merge at 0xec217e88>\r\n        on         = ['1st', '2nd', '3rd']\r\n        other      =                 5th\r\n1st 2nd 3rd        \r\nz   4.0 e    -465.0\r\nm   3.0 x   -2342.0\r\nnan 9.0 nan  9707.0\r\nm   0.0 w   -2212.0\r\nw   NaN b    -122.0\r\nt   8.0 n   -1399.0\r\ns   9.0 c    -308.0\r\nnan 8.0 nan  9717.0\r\n        s   -1783.0\r\np   8.0 q   -1695.0\r\nt   NaN l   -1119.0\r\nj   2.0 k   -1029.0\r\no   NaN nan  9686.0\r\nl   2.0 nan  9669.0\r\nnan 6.0 nan  9737.0\r\nf   1.0 nan  9685.0\r\nb   NaN v   -2101.0\r\nw   1.0 u   -2032.0\r\nv   8.0 g    -701.0\r\nf   6.0 m   -1265.0\r\ni   4.0 p   -1548.0\r\nh   8.0 o   -1487.0\r\ny   8.0 k   -1104.0\r\nnan 3.0 p   -1433.0\r\ny   5.0 z   -2574.0\r\nl   5.0 m   -1261.0\r\nf   0.0 nan  9695.0\r\ng   9.0 c    -296.0\r\nnan 1.0 r   -1613.0\r\nz   7.0 i    -895.0\r\ni   3.0 i    -838.0\r\nnan 4.0 y   -2343.0\r\ns   NaN x   -2318.0\r\nnan 3.0 b     -33.0\r\nk   NaN v   -2110.0\r\ng   7.0 d    -376.0\r\nn   NaN r   -1713.0\r\nc   6.0 s   -1862.0\r\nb   NaN f    -501.0\r\nnan NaN r   -1603.0\r\nb   6.0 e    -461.0\r\nnan NaN p   -1403.0\r\n    5.0 nan  9747.0\r\n    4.0 c    -143.0\r\nl   6.0 w   -2271.0\r\ny   2.0 f    -544.0\r\nl   6.0 k   -1071.0\r\np   3.0 n   -1345.0\r\no   3.0 h    -744.0\r\nnan 2.0 h    -623.0\r\n        rsuffix    = ''\r\n        self       =     1st  2nd  3rd     4th\r\n0     v  8.0    g   701.0\r\n1   NaN  2.0    h   623.0\r\n2     k  NaN    v  2110.0\r\n3     l  2.0  NaN -9669.0\r\n4     i  4.0    p  1548.0\r\n5   NaN  8.0    s  1783.0\r\n6     z  4.0    e   465.0\r\n7     w  NaN    b   122.0\r\n8     o  3.0    h   744.0\r\n9   NaN  6.0  NaN -9737.0\r\n10    h  8.0    o  1487.0\r\n11    g  7.0    d   376.0\r\n12    t  NaN    l  1119.0\r\n13  NaN  1.0    r  1613.0\r\n14    y  8.0    k  1104.0\r\n15    f  0.0  NaN -9695.0\r\n16    y  5.0    z  2574.0\r\n17  NaN  NaN    r  1603.0\r\n18    j  2.0    k  1029.0\r\n19    b  6.0    e   461.0\r\n20    i  3.0    i   838.0\r\n21  NaN  5.0  NaN -9747.0\r\n22    s  NaN    x  2318.0\r\n23    w  1.0    u  2032.0\r\n24    z  7.0    i   895.0\r\n25  NaN  4.0    y  2343.0\r\n26    f  6.0    m  1265.0\r\n27    o  NaN  NaN -9686.0\r\n28    s  9.0    c   308.0\r\n29  NaN  4.0    c   143.0\r\n30    y  2.0    f   544.0\r\n31    l  6.0    w  2271.0\r\n32    n  NaN    r  1713.0\r\n33  NaN  9.0  NaN -9707.0\r\n34    p  8.0    q  1695.0\r\n35    l  6.0    k  1071.0\r\n36    p  3.0    n  1345.0\r\n37  NaN  NaN    p  1403.0\r\n38    m  0.0    w  2212.0\r\n39    f  1.0  NaN -9685.0\r\n40    m  3.0    x  2342.0\r\n41  NaN  3.0    p  1433.0\r\n42    b  NaN    v  2101.0\r\n43    l  5.0    m  1261.0\r\n44    c  6.0    s  1862.0\r\n45  NaN  8.0  NaN -9717.0\r\n46    t  8.0    n  1399.0\r\n47    b  NaN    f   501.0\r\n48    g  9.0    c   296.0\r\n49  NaN  3.0    b    33.0\r\n        sort       = False\r\n        validate   = None\r\npandas\/core\/reshape\/merge.py:184: in merge\r\n    return op.get_result(copy=copy)\r\n        copy       = None\r\n        how        = 'left'\r\n        indicator  = False\r\n        left       =     1st  2nd  3rd     4th\r\n0     v  8.0    g   701.0\r\n1   NaN  2.0    h   623.0\r\n2     k  NaN    v  2110.0\r\n3     l  2.0  NaN -9669.0\r\n4     i  4.0    p  1548.0\r\n5   NaN  8.0    s  1783.0\r\n6     z  4.0    e   465.0\r\n7     w  NaN    b   122.0\r\n8     o  3.0    h   744.0\r\n9   NaN  6.0  NaN -9737.0\r\n10    h  8.0    o  1487.0\r\n11    g  7.0    d   376.0\r\n12    t  NaN    l  1119.0\r\n13  NaN  1.0    r  1613.0\r\n14    y  8.0    k  1104.0\r\n15    f  0.0  NaN -9695.0\r\n16    y  5.0    z  2574.0\r\n17  NaN  NaN    r  1603.0\r\n18    j  2.0    k  1029.0\r\n19    b  6.0    e   461.0\r\n20    i  3.0    i   838.0\r\n21  NaN  5.0  NaN -9747.0\r\n22    s  NaN    x  2318.0\r\n23    w  1.0    u  2032.0\r\n24    z  7.0    i   895.0\r\n25  NaN  4.0    y  2343.0\r\n26    f  6.0    m  1265.0\r\n27    o  NaN  NaN -9686.0\r\n28    s  9.0    c   308.0\r\n29  NaN  4.0    c   143.0\r\n30    y  2.0    f   544.0\r\n31    l  6.0    w  2271.0\r\n32    n  NaN    r  1713.0\r\n33  NaN  9.0  NaN -9707.0\r\n34    p  8.0    q  1695.0\r\n35    l  6.0    k  1071.0\r\n36    p  3.0    n  1345.0\r\n37  NaN  NaN    p  1403.0\r\n38    m  0.0    w  2212.0\r\n39    f  1.0  NaN -9685.0\r\n40    m  3.0    x  2342.0\r\n41  NaN  3.0    p  1433.0\r\n42    b  NaN    v  2101.0\r\n43    l  5.0    m  1261.0\r\n44    c  6.0    s  1862.0\r\n45  NaN  8.0  NaN -9717.0\r\n46    t  8.0    n  1399.0\r\n47    b  NaN    f   501.0\r\n48    g  9.0    c   296.0\r\n49  NaN  3.0    b    33.0\r\n        left_df    =     1st  2nd  3rd     4th\r\n0     v  8.0    g   701.0\r\n1   NaN  2.0    h   623.0\r\n2     k  NaN    v  2110.0\r\n3     l  2.0  NaN -9669.0\r\n4     i  4.0    p  1548.0\r\n5   NaN  8.0    s  1783.0\r\n6     z  4.0    e   465.0\r\n7     w  NaN    b   122.0\r\n8     o  3.0    h   744.0\r\n9   NaN  6.0  NaN -9737.0\r\n10    h  8.0    o  1487.0\r\n11    g  7.0    d   376.0\r\n12    t  NaN    l  1119.0\r\n13  NaN  1.0    r  1613.0\r\n14    y  8.0    k  1104.0\r\n15    f  0.0  NaN -9695.0\r\n16    y  5.0    z  2574.0\r\n17  NaN  NaN    r  1603.0\r\n18    j  2.0    k  1029.0\r\n19    b  6.0    e   461.0\r\n20    i  3.0    i   838.0\r\n21  NaN  5.0  NaN -9747.0\r\n22    s  NaN    x  2318.0\r\n23    w  1.0    u  2032.0\r\n24    z  7.0    i   895.0\r\n25  NaN  4.0    y  2343.0\r\n26    f  6.0    m  1265.0\r\n27    o  NaN  NaN -9686.0\r\n28    s  9.0    c   308.0\r\n29  NaN  4.0    c   143.0\r\n30    y  2.0    f   544.0\r\n31    l  6.0    w  2271.0\r\n32    n  NaN    r  1713.0\r\n33  NaN  9.0  NaN -9707.0\r\n34    p  8.0    q  1695.0\r\n35    l  6.0    k  1071.0\r\n36    p  3.0    n  1345.0\r\n37  NaN  NaN    p  1403.0\r\n38    m  0.0    w  2212.0\r\n39    f  1.0  NaN -9685.0\r\n40    m  3.0    x  2342.0\r\n41  NaN  3.0    p  1433.0\r\n42    b  NaN    v  2101.0\r\n43    l  5.0    m  1261.0\r\n44    c  6.0    s  1862.0\r\n45  NaN  8.0  NaN -9717.0\r\n46    t  8.0    n  1399.0\r\n47    b  NaN    f   501.0\r\n48    g  9.0    c   296.0\r\n49  NaN  3.0    b    33.0\r\n        left_index = False\r\n        left_on    = ['1st', '2nd', '3rd']\r\n        on         = None\r\n        op         = <pandas.core.reshape.merge._MergeOperation object at 0xc9857b30>\r\n        right      =                 5th\r\n1st 2nd 3rd        \r\nz   4.0 e    -465.0\r\nm   3.0 x   -2342.0\r\nnan 9.0 nan  9707.0\r\nm   0.0 w   -2212.0\r\nw   NaN b    -122.0\r\nt   8.0 n   -1399.0\r\ns   9.0 c    -308.0\r\nnan 8.0 nan  9717.0\r\n        s   -1783.0\r\np   8.0 q   -1695.0\r\nt   NaN l   -1119.0\r\nj   2.0 k   -1029.0\r\no   NaN nan  9686.0\r\nl   2.0 nan  9669.0\r\nnan 6.0 nan  9737.0\r\nf   1.0 nan  9685.0\r\nb   NaN v   -2101.0\r\nw   1.0 u   -2032.0\r\nv   8.0 g    -701.0\r\nf   6.0 m   -1265.0\r\ni   4.0 p   -1548.0\r\nh   8.0 o   -1487.0\r\ny   8.0 k   -1104.0\r\nnan 3.0 p   -1433.0\r\ny   5.0 z   -2574.0\r\nl   5.0 m   -1261.0\r\nf   0.0 nan  9695.0\r\ng   9.0 c    -296.0\r\nnan 1.0 r   -1613.0\r\nz   7.0 i    -895.0\r\ni   3.0 i    -838.0\r\nnan 4.0 y   -2343.0\r\ns   NaN x   -2318.0\r\nnan 3.0 b     -33.0\r\nk   NaN v   -2110.0\r\ng   7.0 d    -376.0\r\nn   NaN r   -1713.0\r\nc   6.0 s   -1862.0\r\nb   NaN f    -501.0\r\nnan NaN r   -1603.0\r\nb   6.0 e    -461.0\r\nnan NaN p   -1403.0\r\n    5.0 nan  9747.0\r\n    4.0 c    -143.0\r\nl   6.0 w   -2271.0\r\ny   2.0 f    -544.0\r\nl   6.0 k   -1071.0\r\np   3.0 n   -1345.0\r\no   3.0 h    -744.0\r\nnan 2.0 h    -623.0\r\n        right_df   =                 5th\r\n1st 2nd 3rd        \r\nz   4.0 e    -465.0\r\nm   3.0 x   -2342.0\r\nnan 9.0 nan  9707.0\r\nm   0.0 w   -2212.0\r\nw   NaN b    -122.0\r\nt   8.0 n   -1399.0\r\ns   9.0 c    -308.0\r\nnan 8.0 nan  9717.0\r\n        s   -1783.0\r\np   8.0 q   -1695.0\r\nt   NaN l   -1119.0\r\nj   2.0 k   -1029.0\r\no   NaN nan  9686.0\r\nl   2.0 nan  9669.0\r\nnan 6.0 nan  9737.0\r\nf   1.0 nan  9685.0\r\nb   NaN v   -2101.0\r\nw   1.0 u   -2032.0\r\nv   8.0 g    -701.0\r\nf   6.0 m   -1265.0\r\ni   4.0 p   -1548.0\r\nh   8.0 o   -1487.0\r\ny   8.0 k   -1104.0\r\nnan 3.0 p   -1433.0\r\ny   5.0 z   -2574.0\r\nl   5.0 m   -1261.0\r\nf   0.0 nan  9695.0\r\ng   9.0 c    -296.0\r\nnan 1.0 r   -1613.0\r\nz   7.0 i    -895.0\r\ni   3.0 i    -838.0\r\nnan 4.0 y   -2343.0\r\ns   NaN x   -2318.0\r\nnan 3.0 b     -33.0\r\nk   NaN v   -2110.0\r\ng   7.0 d    -376.0\r\nn   NaN r   -1713.0\r\nc   6.0 s   -1862.0\r\nb   NaN f    -501.0\r\nnan NaN r   -1603.0\r\nb   6.0 e    -461.0\r\nnan NaN p   -1403.0\r\n    5.0 nan  9747.0\r\n    4.0 c    -143.0\r\nl   6.0 w   -2271.0\r\ny   2.0 f    -544.0\r\nl   6.0 k   -1071.0\r\np   3.0 n   -1345.0\r\no   3.0 h    -744.0\r\nnan 2.0 h    -623.0\r\n        right_index = True\r\n        right_on   = None\r\n        sort       = False\r\n        suffixes   = ('', '')\r\n        validate   = None\r\npandas\/core\/reshape\/merge.py:886: in get_result\r\n    join_index, left_indexer, right_indexer = self._get_join_info()\r\n        copy       = None\r\n        self       = <pandas.core.reshape.merge._MergeOperation object at 0xc9857b30>\r\npandas\/core\/reshape\/merge.py:1142: in _get_join_info\r\n    join_index, left_indexer, right_indexer = _left_join_on_index(\r\n        left_ax    = RangeIndex(start=0, stop=50, step=1)\r\n        right_ax   = MultiIndex([('z', 4.0, 'e'),\r\n            ('m', 3.0, 'x'),\r\n            (nan, 9.0, nan),\r\n            ('m', 0.0, 'w'),\r\n            ('w', nan, 'b'),\r\n            ('t', 8.0, 'n'),\r\n            ('s', 9.0, 'c'),\r\n            (nan, 8.0, nan),\r\n            (nan, 8.0, 's'),\r\n            ('p', 8.0, 'q'),\r\n            ('t', nan, 'l'),\r\n            ('j', 2.0, 'k'),\r\n            ('o', nan, nan),\r\n            ('l', 2.0, nan),\r\n            (nan, 6.0, nan),\r\n            ('f', 1.0, nan),\r\n            ('b', nan, 'v'),\r\n            ('w', 1.0, 'u'),\r\n            ('v', 8.0, 'g'),\r\n            ('f', 6.0, 'm'),\r\n            ('i', 4.0, 'p'),\r\n            ('h', 8.0, 'o'),\r\n            ('y', 8.0, 'k'),\r\n            (nan, 3.0, 'p'),\r\n            ('y', 5.0, 'z'),\r\n            ('l', 5.0, 'm'),\r\n            ('f', 0.0, nan),\r\n            ('g', 9.0, 'c'),\r\n            (nan, 1.0, 'r'),\r\n            ('z', 7.0, 'i'),\r\n            ('i', 3.0, 'i'),\r\n            (nan, 4.0, 'y'),\r\n            ('s', nan, 'x'),\r\n            (nan, 3.0, 'b'),\r\n            ('k', nan, 'v'),\r\n            ('g', 7.0, 'd'),\r\n            ('n', nan, 'r'),\r\n            ('c', 6.0, 's'),\r\n            ('b', nan, 'f'),\r\n            (nan, nan, 'r'),\r\n            ('b', 6.0, 'e'),\r\n            (nan, nan, 'p'),\r\n            (nan, 5.0, nan),\r\n            (nan, 4.0, 'c'),\r\n            ('l', 6.0, 'w'),\r\n            ('y', 2.0, 'f'),\r\n            ('l', 6.0, 'k'),\r\n            ('p', 3.0, 'n'),\r\n            ('o', 3.0, 'h'),\r\n            (nan, 2.0, 'h')],\r\n           names=['1st', '2nd', '3rd'])\r\n        self       = <pandas.core.reshape.merge._MergeOperation object at 0xc9857b30>\r\npandas\/core\/reshape\/merge.py:2375: in _left_join_on_index\r\n    lkey, rkey = _get_multiindex_indexer(join_keys, right_ax, sort=sort)\r\n        join_keys  = [<ArrowStringArrayNumpySemantics>\r\n['v', nan, 'k', 'l', 'i', nan, 'z', 'w', 'o', nan, 'h', 'g', 't', nan, 'y',\r\n 'f', 'y', nan, 'j', 'b', 'i', nan, 's', 'w', 'z', nan, 'f', 'o', 's', nan,\r\n 'y', 'l', 'n', nan, 'p', 'l', 'p', nan, 'm', 'f', 'm', nan, 'b', 'l', 'c',\r\n nan, 't', 'b', 'g', nan]\r\nLength: 50, dtype: string,\r\n array([ 8.,  2., nan,  2.,  4.,  8.,  4., nan,  3.,  6.,  8.,  7., nan,\r\n        1.,  8.,  0.,  5., nan,  2.,  6.,  3.,  5., nan,  1.,  7.,  4.,\r\n        6., nan,  9.,  4.,  2.,  6., nan,  9.,  8.,  6.,  3., nan,  0.,\r\n        1.,  3.,  3., nan,  5.,  6.,  8.,  8., nan,  9.,  3.]),\r\n <ArrowStringArrayNumpySemantics>\r\n['g', 'h', 'v', nan, 'p', 's', 'e', 'b', 'h', nan, 'o', 'd', 'l', 'r', 'k',\r\n nan, 'z', 'r', 'k', 'e', 'i', nan, 'x', 'u', 'i', 'y', 'm', nan, 'c', 'c',\r\n 'f', 'w', 'r', nan, 'q', 'k', 'n', 'p', 'w', nan, 'x', 'p', 'v', 'm', 's',\r\n nan, 'n', 'f', 'c', 'b']\r\nLength: 50, dtype: string]\r\n        left_ax    = RangeIndex(start=0, stop=50, step=1)\r\n        right_ax   = MultiIndex([('z', 4.0, 'e'),\r\n            ('m', 3.0, 'x'),\r\n            (nan, 9.0, nan),\r\n            ('m', 0.0, 'w'),\r\n            ('w', nan, 'b'),\r\n            ('t', 8.0, 'n'),\r\n            ('s', 9.0, 'c'),\r\n            (nan, 8.0, nan),\r\n            (nan, 8.0, 's'),\r\n            ('p', 8.0, 'q'),\r\n            ('t', nan, 'l'),\r\n            ('j', 2.0, 'k'),\r\n            ('o', nan, nan),\r\n            ('l', 2.0, nan),\r\n            (nan, 6.0, nan),\r\n            ('f', 1.0, nan),\r\n            ('b', nan, 'v'),\r\n            ('w', 1.0, 'u'),\r\n            ('v', 8.0, 'g'),\r\n            ('f', 6.0, 'm'),\r\n            ('i', 4.0, 'p'),\r\n            ('h', 8.0, 'o'),\r\n            ('y', 8.0, 'k'),\r\n            (nan, 3.0, 'p'),\r\n            ('y', 5.0, 'z'),\r\n            ('l', 5.0, 'm'),\r\n            ('f', 0.0, nan),\r\n            ('g', 9.0, 'c'),\r\n            (nan, 1.0, 'r'),\r\n            ('z', 7.0, 'i'),\r\n            ('i', 3.0, 'i'),\r\n            (nan, 4.0, 'y'),\r\n            ('s', nan, 'x'),\r\n            (nan, 3.0, 'b'),\r\n            ('k', nan, 'v'),\r\n            ('g', 7.0, 'd'),\r\n            ('n', nan, 'r'),\r\n            ('c', 6.0, 's'),\r\n            ('b', nan, 'f'),\r\n            (nan, nan, 'r'),\r\n            ('b', 6.0, 'e'),\r\n            (nan, nan, 'p'),\r\n            (nan, 5.0, nan),\r\n            (nan, 4.0, 'c'),\r\n            ('l', 6.0, 'w'),\r\n            ('y', 2.0, 'f'),\r\n            ('l', 6.0, 'k'),\r\n            ('p', 3.0, 'n'),\r\n            ('o', 3.0, 'h'),\r\n            (nan, 2.0, 'h')],\r\n           names=['1st', '2nd', '3rd'])\r\n        sort       = False\r\npandas\/core\/reshape\/merge.py:2309: in _get_multiindex_indexer\r\n    zipped = zip(*mapped)\r\n        index      = MultiIndex([('z', 4.0, 'e'),\r\n            ('m', 3.0, 'x'),\r\n            (nan, 9.0, nan),\r\n            ('m', 0.0, 'w'),\r\n            ('w', nan, 'b'),\r\n            ('t', 8.0, 'n'),\r\n            ('s', 9.0, 'c'),\r\n            (nan, 8.0, nan),\r\n            (nan, 8.0, 's'),\r\n            ('p', 8.0, 'q'),\r\n            ('t', nan, 'l'),\r\n            ('j', 2.0, 'k'),\r\n            ('o', nan, nan),\r\n            ('l', 2.0, nan),\r\n            (nan, 6.0, nan),\r\n            ('f', 1.0, nan),\r\n            ('b', nan, 'v'),\r\n            ('w', 1.0, 'u'),\r\n            ('v', 8.0, 'g'),\r\n            ('f', 6.0, 'm'),\r\n            ('i', 4.0, 'p'),\r\n            ('h', 8.0, 'o'),\r\n            ('y', 8.0, 'k'),\r\n            (nan, 3.0, 'p'),\r\n            ('y', 5.0, 'z'),\r\n            ('l', 5.0, 'm'),\r\n            ('f', 0.0, nan),\r\n            ('g', 9.0, 'c'),\r\n            (nan, 1.0, 'r'),\r\n            ('z', 7.0, 'i'),\r\n            ('i', 3.0, 'i'),\r\n            (nan, 4.0, 'y'),\r\n            ('s', nan, 'x'),\r\n            (nan, 3.0, 'b'),\r\n            ('k', nan, 'v'),\r\n            ('g', 7.0, 'd'),\r\n            ('n', nan, 'r'),\r\n            ('c', 6.0, 's'),\r\n            ('b', nan, 'f'),\r\n            (nan, nan, 'r'),\r\n            ('b', 6.0, 'e'),\r\n            (nan, nan, 'p'),\r\n            (nan, 5.0, nan),\r\n            (nan, 4.0, 'c'),\r\n            ('l', 6.0, 'w'),\r\n            ('y', 2.0, 'f'),\r\n            ('l', 6.0, 'k'),\r\n            ('p', 3.0, 'n'),\r\n            ('o', 3.0, 'h'),\r\n            (nan, 2.0, 'h')],\r\n           names=['1st', '2nd', '3rd'])\r\n        join_keys  = [<ArrowStringArrayNumpySemantics>\r\n['v', nan, 'k', 'l', 'i', nan, 'z', 'w', 'o', nan, 'h', 'g', 't', nan, 'y',\r\n 'f', 'y', nan, 'j', 'b', 'i', nan, 's', 'w', 'z', nan, 'f', 'o', 's', nan,\r\n 'y', 'l', 'n', nan, 'p', 'l', 'p', nan, 'm', 'f', 'm', nan, 'b', 'l', 'c',\r\n nan, 't', 'b', 'g', nan]\r\nLength: 50, dtype: string,\r\n array([ 8.,  2., nan,  2.,  4.,  8.,  4., nan,  3.,  6.,  8.,  7., nan,\r\n        1.,  8.,  0.,  5., nan,  2.,  6.,  3.,  5., nan,  1.,  7.,  4.,\r\n        6., nan,  9.,  4.,  2.,  6., nan,  9.,  8.,  6.,  3., nan,  0.,\r\n        1.,  3.,  3., nan,  5.,  6.,  8.,  8., nan,  9.,  3.]),\r\n <ArrowStringArrayNumpySemantics>\r\n['g', 'h', 'v', nan, 'p', 's', 'e', 'b', 'h', nan, 'o', 'd', 'l', 'r', 'k',\r\n nan, 'z', 'r', 'k', 'e', 'i', nan, 'x', 'u', 'i', 'y', 'm', nan, 'c', 'c',\r\n 'f', 'w', 'r', nan, 'q', 'k', 'n', 'p', 'w', nan, 'x', 'p', 'v', 'm', 's',\r\n nan, 'n', 'f', 'c', 'b']\r\nLength: 50, dtype: string]\r\n        mapped     = <generator object _get_multiindex_indexer.<locals>.<genexpr> at 0xc9b7a710>\r\n        sort       = False\r\npandas\/core\/reshape\/merge.py:2306: in <genexpr>\r\n    _factorize_keys(index.levels[n]._values, join_keys[n], sort=sort)\r\n        .0         = <range_iterator object at 0xc9856710>\r\n        index      = MultiIndex([('z', 4.0, 'e'),\r\n            ('m', 3.0, 'x'),\r\n            (nan, 9.0, nan),\r\n            ('m', 0.0, 'w'),\r\n            ('w', nan, 'b'),\r\n            ('t', 8.0, 'n'),\r\n            ('s', 9.0, 'c'),\r\n            (nan, 8.0, nan),\r\n            (nan, 8.0, 's'),\r\n            ('p', 8.0, 'q'),\r\n            ('t', nan, 'l'),\r\n            ('j', 2.0, 'k'),\r\n            ('o', nan, nan),\r\n            ('l', 2.0, nan),\r\n            (nan, 6.0, nan),\r\n            ('f', 1.0, nan),\r\n            ('b', nan, 'v'),\r\n            ('w', 1.0, 'u'),\r\n            ('v', 8.0, 'g'),\r\n            ('f', 6.0, 'm'),\r\n            ('i', 4.0, 'p'),\r\n            ('h', 8.0, 'o'),\r\n            ('y', 8.0, 'k'),\r\n            (nan, 3.0, 'p'),\r\n            ('y', 5.0, 'z'),\r\n            ('l', 5.0, 'm'),\r\n            ('f', 0.0, nan),\r\n            ('g', 9.0, 'c'),\r\n            (nan, 1.0, 'r'),\r\n            ('z', 7.0, 'i'),\r\n            ('i', 3.0, 'i'),\r\n            (nan, 4.0, 'y'),\r\n            ('s', nan, 'x'),\r\n            (nan, 3.0, 'b'),\r\n            ('k', nan, 'v'),\r\n            ('g', 7.0, 'd'),\r\n            ('n', nan, 'r'),\r\n            ('c', 6.0, 's'),\r\n            ('b', nan, 'f'),\r\n            (nan, nan, 'r'),\r\n            ('b', 6.0, 'e'),\r\n            (nan, nan, 'p'),\r\n            (nan, 5.0, nan),\r\n            (nan, 4.0, 'c'),\r\n            ('l', 6.0, 'w'),\r\n            ('y', 2.0, 'f'),\r\n            ('l', 6.0, 'k'),\r\n            ('p', 3.0, 'n'),\r\n            ('o', 3.0, 'h'),\r\n            (nan, 2.0, 'h')],\r\n           names=['1st', '2nd', '3rd'])\r\n        join_keys  = [<ArrowStringArrayNumpySemantics>\r\n['v', nan, 'k', 'l', 'i', nan, 'z', 'w', 'o', nan, 'h', 'g', 't', nan, 'y',\r\n 'f', 'y', nan, 'j', 'b', 'i', nan, 's', 'w', 'z', nan, 'f', 'o', 's', nan,\r\n 'y', 'l', 'n', nan, 'p', 'l', 'p', nan, 'm', 'f', 'm', nan, 'b', 'l', 'c',\r\n nan, 't', 'b', 'g', nan]\r\nLength: 50, dtype: string,\r\n array([ 8.,  2., nan,  2.,  4.,  8.,  4., nan,  3.,  6.,  8.,  7., nan,\r\n        1.,  8.,  0.,  5., nan,  2.,  6.,  3.,  5., nan,  1.,  7.,  4.,\r\n        6., nan,  9.,  4.,  2.,  6., nan,  9.,  8.,  6.,  3., nan,  0.,\r\n        1.,  3.,  3., nan,  5.,  6.,  8.,  8., nan,  9.,  3.]),\r\n <ArrowStringArrayNumpySemantics>\r\n['g', 'h', 'v', nan, 'p', 's', 'e', 'b', 'h', nan, 'o', 'd', 'l', 'r', 'k',\r\n nan, 'z', 'r', 'k', 'e', 'i', nan, 'x', 'u', 'i', 'y', 'm', nan, 'c', 'c',\r\n 'f', 'w', 'r', nan, 'q', 'k', 'n', 'p', 'w', nan, 'x', 'p', 'v', 'm', 's',\r\n nan, 'n', 'f', 'c', 'b']\r\nLength: 50, dtype: string]\r\n        n          = 0\r\n        sort       = False\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nlk = <pyarrow.lib.ChunkedArray object at 0xc983bb18>\r\n[\r\n  [\r\n    \"b\",\r\n    \"c\",\r\n    \"f\",\r\n    \"g\",\r\n    \"h\",\r\n    ...\r\n    \"t\",\r\n    \"v\",\r\n    \"w\",\r\n    \"y\",\r\n    \"z\"\r\n  ]\r\n]\r\nrk = <pyarrow.lib.ChunkedArray object at 0xc984c7f8>\r\n[\r\n  [\r\n    \"v\",\r\n    null,\r\n    \"k\",\r\n    \"l\",\r\n    \"i\",\r\n    ...\r\n    null,\r\n    \"t\",\r\n    \"b\",\r\n    \"g\",\r\n    null\r\n  ]\r\n]\r\nsort = False\r\n\r\n    def _factorize_keys(\r\n        lk: ArrayLike, rk: ArrayLike, sort: bool = True\r\n    ) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:\r\n        \"\"\"\r\n        Encode left and right keys as enumerated types.\r\n    \r\n        This is used to get the join indexers to be used when merging DataFrames.\r\n    \r\n        Parameters\r\n        ----------\r\n        lk : ndarray, ExtensionArray\r\n            Left key.\r\n        rk : ndarray, ExtensionArray\r\n            Right key.\r\n        sort : bool, defaults to True\r\n            If True, the encoding is done such that the unique elements in the\r\n            keys are sorted.\r\n    \r\n        Returns\r\n        -------\r\n        np.ndarray[np.intp]\r\n            Left (resp. right if called with `key='right'`) labels, as enumerated type.\r\n        np.ndarray[np.intp]\r\n            Right (resp. left if called with `key='right'`) labels, as enumerated type.\r\n        int\r\n            Number of unique elements in union of left and right labels.\r\n    \r\n        See Also\r\n        --------\r\n        merge : Merge DataFrame or named Series objects\r\n            with a database-style join.\r\n        algorithms.factorize : Encode the object as an enumerated type\r\n            or categorical variable.\r\n    \r\n        Examples\r\n        --------\r\n        >>> lk = np.array([\"a\", \"c\", \"b\"])\r\n        >>> rk = np.array([\"a\", \"c\"])\r\n    \r\n        Here, the unique values are `'a', 'b', 'c'`. With the default\r\n        `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:\r\n    \r\n        >>> pd.core.reshape.merge._factorize_keys(lk, rk)\r\n        (array([0, 2, 1]), array([0, 2]), 3)\r\n    \r\n        With the `sort=False`, the encoding will correspond to the order\r\n        in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:\r\n    \r\n        >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)\r\n        (array([0, 1, 2]), array([0, 1]), 3)\r\n        \"\"\"\r\n        # TODO: if either is a RangeIndex, we can likely factorize more efficiently?\r\n    \r\n        if (\r\n            isinstance(lk.dtype, DatetimeTZDtype) and isinstance(rk.dtype, DatetimeTZDtype)\r\n        ) or (lib.is_np_dtype(lk.dtype, \"M\") and lib.is_np_dtype(rk.dtype, \"M\")):\r\n            # Extract the ndarray (UTC-localized) values\r\n            # Note: we dont need the dtypes to match, as these can still be compared\r\n            lk, rk = cast(\"DatetimeArray\", lk)._ensure_matching_resos(rk)\r\n            lk = cast(\"DatetimeArray\", lk)._ndarray\r\n            rk = cast(\"DatetimeArray\", rk)._ndarray\r\n    \r\n        elif (\r\n            isinstance(lk.dtype, CategoricalDtype)\r\n            and isinstance(rk.dtype, CategoricalDtype)\r\n            and lk.dtype == rk.dtype\r\n        ):\r\n            assert isinstance(lk, Categorical)\r\n            assert isinstance(rk, Categorical)\r\n            # Cast rk to encoding so we can compare codes with lk\r\n    \r\n            rk = lk._encode_with_my_categories(rk)\r\n    \r\n            lk = ensure_int64(lk.codes)\r\n            rk = ensure_int64(rk.codes)\r\n    \r\n        elif isinstance(lk, ExtensionArray) and lk.dtype == rk.dtype:\r\n            if (isinstance(lk.dtype, ArrowDtype) and is_string_dtype(lk.dtype)) or (\r\n                isinstance(lk.dtype, StringDtype)\r\n                and lk.dtype.storage in [\"pyarrow\", \"pyarrow_numpy\"]\r\n            ):\r\n                import pyarrow as pa\r\n                import pyarrow.compute as pc\r\n    \r\n                len_lk = len(lk)\r\n                lk = lk._pa_array  # type: ignore[attr-defined]\r\n                rk = rk._pa_array  # type: ignore[union-attr]\r\n                dc = (\r\n                    pa.chunked_array(lk.chunks + rk.chunks)  # type: ignore[union-attr]\r\n                    .combine_chunks()\r\n                    .dictionary_encode()\r\n                )\r\n    \r\n                llab, rlab, count = (\r\n                    pc.fill_null(dc.indices[slice(len_lk)], -1)\r\n                    .to_numpy()\r\n                    .astype(np.intp, copy=False),\r\n                    pc.fill_null(dc.indices[slice(len_lk, None)], -1)\r\n                    .to_numpy()\r\n                    .astype(np.intp, copy=False),\r\n                    len(dc.dictionary),\r\n                )\r\n    \r\n                if sort:\r\n                    uniques = dc.dictionary.to_numpy(zero_copy_only=False)\r\n                    llab, rlab = _sort_labels(uniques, llab, rlab)\r\n    \r\n                if dc.null_count > 0:\r\n                    lmask = llab == -1\r\n                    lany = lmask.any()\r\n                    rmask = rlab == -1\r\n                    rany = rmask.any()\r\n                    if lany:\r\n                        np.putmask(llab, lmask, count)\r\n                    if rany:\r\n>                       np.putmask(rlab, rmask, count)\r\nE                       ValueError: putmask: output array is read-only\r\n\r\ncount      = 19\r\ndc         = <pyarrow.lib.DictionaryArray object at 0xc9083ed0>\r\n\r\n-- dictionary:\r\n  [\r\n    \"b\",\r\n    \"c\",\r\n    \"f\",\r\n    \"g\",\r\n    \"h\",\r\n    \"i\",\r\n    \"j\",\r\n    \"k\",\r\n    \"l\",\r\n    \"m\",\r\n    \"n\",\r\n    \"o\",\r\n    \"p\",\r\n    \"s\",\r\n    \"t\",\r\n    \"v\",\r\n    \"w\",\r\n    \"y\",\r\n    \"z\"\r\n  ]\r\n-- indices:\r\n  [\r\n    0,\r\n    1,\r\n    2,\r\n    3,\r\n    4,\r\n    5,\r\n    6,\r\n    7,\r\n    8,\r\n    9,\r\n    ...\r\n    9,\r\n    null,\r\n    0,\r\n    8,\r\n    1,\r\n    null,\r\n    14,\r\n    0,\r\n    3,\r\n    null\r\n  ]\r\nlany       = False\r\nlen_lk     = 19\r\nlk         = <pyarrow.lib.ChunkedArray object at 0xc983bb18>\r\n[\r\n  [\r\n    \"b\",\r\n    \"c\",\r\n    \"f\",\r\n    \"g\",\r\n    \"h\",\r\n    ...\r\n    \"t\",\r\n    \"v\",\r\n    \"w\",\r\n    \"y\",\r\n    \"z\"\r\n  ]\r\n]\r\nllab       = array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18])\r\nlmask      = array([False, False, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False, False, False, False,\r\n       False])\r\npa         = <module 'pyarrow' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/__init__.py'>\r\npc         = <module 'pyarrow.compute' from '\/usr\/lib\/python3.11\/site-packages\/pyarrow\/compute.py'>\r\nrany       = True\r\nrk         = <pyarrow.lib.ChunkedArray object at 0xc984c7f8>\r\n[\r\n  [\r\n    \"v\",\r\n    null,\r\n    \"k\",\r\n    \"l\",\r\n    \"i\",\r\n    ...\r\n    null,\r\n    \"t\",\r\n    \"b\",\r\n    \"g\",\r\n    null\r\n  ]\r\n]\r\nrlab       = array([15, -1,  7,  8,  5, -1, 18, 16, 11, -1,  4,  3, 14, -1, 17,  2, 17,\r\n       -1,  6,  0,  5, -1, 13, 16, 18, -1,  2, 11, 13, -1, 17,  8, 10, -1,\r\n       12,  8, 12, -1,  9,  2,  9, -1,  0,  8,  1, -1, 14,  0,  3, -1])\r\nrmask      = array([False,  True, False, False, False,  True, False, False, False,\r\n        True, False, False, False,  True, False, False, False,  True,\r\n       False, False, False,  True, False, False, False,  True, False,\r\n       False, False,  True, False, False, False,  True, False, False,\r\n       False,  True, False, False, False,  True, False, False, False,\r\n        True, False, False, False,  True])\r\nsort       = False\r\n\r\npandas\/core\/reshape\/merge.py:2514: ValueError\r\n```\r\n<\/details>\r\n\r\nFull build & test log (2.5M .gz, 52M uncompressed): [pandas.txt.gz](https:\/\/github.com\/pandas-dev\/pandas\/files\/14343832\/pandas.txt.gz)\r\n\r\nThis is on Gentoo\/x86 systemd-nspawn container. I'm using `-O2 -march=pentium-m -mfpmath=sse -pipe` flags to rule out i387-specific precision issues.\r\n\r\nI've also filed https:\/\/github.com\/apache\/arrow\/issues\/40153 for test failures in pyarrow itself. Some of them could be possibly be bugs in pandas instead.\n\n### Expected Behavior\n\nTests passing ;-).\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.7.final.0\r\npython-bits           : 32\r\nOS                    : Linux\r\nOS-release            : 6.7.5-gentoo-dist\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Sat Feb 17 07:30:27 -00 2024\r\nmachine               : x86_64\r\nprocessor             : AMD Ryzen 5 3600 6-Core Processor\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : None\r\nCython                : 3.0.5\r\npytest                : 7.4.4\r\nhypothesis            : 6.98.3\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.2.0\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : None\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.27\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.2.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : None\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["The first error looks like we need to use the platform specific int type instead of int64 (haven't looked too closely, though).\r\n\r\nThe rest seem like they could potentially be related to bugs in Arrow.","I already have a fix for one bug in Arrow, so I'm going to retry with it applied, later today.","Yeah, my patch fixed `pandas\/tests\/interchange\/test_impl.py::test_large_string_pyarrow`. I'll check if I can figure out a fix for the other three when I find some time."],"labels":["Bug","32bit","Arrow"]},{"title":"BUG: MultiIndex.factorize fails if index is 0-length","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nempty_ix = pd.MultiIndex.from_product([\r\n    pd.Index([], name='a', dtype=object),\r\n    pd.Index([], name='i', dtype='f4')\r\n])\r\nempty_ix.factorize() # Fails\r\n\r\n# Names are lost\r\npd.MultiIndex.from_product([\r\n    pd.Index(['a', 'b'], name='a'),\r\n    pd.Index([0, 1, 0], name='i', dtype='f4')\r\n]).factorize()[1].names\n```\n\n\n### Issue Description\n\nThe first example should succeed, but actually it fails:\r\n\r\n```\r\nFile ~\/opt\/conda\/envs\/mamba\/envs\/py3_2\/lib\/python3.11\/site-packages\/pandas\/core\/base.py:1203, in IndexOpsMixin.factorize(self, sort, use_na_sentinel)\r\n   1199     uniques = uniques.astype(np.float32)\r\n   1201 if isinstance(self, ABCIndex):\r\n   1202     # preserve e.g. MultiIndex\r\n-> 1203     uniques = self._constructor(uniques)\r\n   1204 else:\r\n   1205     from pandas import Index\r\nFile ~\/opt\/conda\/envs\/mamba\/envs\/py3_2\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/multi.py:222, in names_compat.<locals>.new_meth(self_or_cls, *args, **kwargs)\r\n    219 if \"name\" in kwargs:\r\n    220     kwargs[\"names\"] = kwargs.pop(\"name\")\r\n--> 222 return meth(self_or_cls, *args, **kwargs)\r\nFile ~\/opt\/conda\/envs\/mamba\/envs\/py3_2\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/multi.py:609, in MultiIndex.from_tuples(cls, tuples, sortorder, names)\r\n    607 if len(tuples) == 0:\r\n    608     if names is None:\r\n--> 609         raise TypeError(\"Cannot infer number of levels from empty list\")\r\n    610     # error: Argument 1 to \"len\" has incompatible type \"Hashable\";\r\n    611     # expected \"Sized\"\r\n    612     arrays = [[]] * len(names)  # type: ignore[arg-type]\r\nTypeError: Cannot infer number of levels from empty list\r\n```\r\n\r\nProbably because of the same underlying issue, `MultiIndex.factorize` always loses the `names` of the original index. It should preserve the original names instead.\n\n### Expected Behavior\n\nFirst `factorize()` should return `(np.array([], dtype=np.intp), empty_ix)`\r\n\r\nSecond example should return `['a', 'i']` instead of `[None, None]`\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.18.0-348.20.1.el8_5.x86_64\r\nVersion               : #1 SMP Thu Mar 10 20:59:28 UTC 2022\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : 2024.1.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Thanks for the report. Agreed this shouldn't raise, but I'm not certain about preserving names. `pd.Index([2, 4, 3], name=\"a\").factorize()` also does not preserve the name, and doing so might have some wide ranging complications (haven't checked). Further  investigations welcome - in particular, if we do preserve names, what is the impact on the test suite?","I also think names should be preserved in the regular Index case. It does break backwards compatibility to do this because existing code may be relying on them not being preserved, but leaving this aside it does seem very clear to me that dropping the names is surprising behaviour.","> but leaving this aside it does seem very clear to me that dropping the names is surprising behaviour.\r\n\r\nNo disagreement here offhand, but this could have wide ranging implications and the impact needs to be investigated. "],"labels":["Bug","Algos","Index"]},{"title":"BUG: Unexpected read_csv parse_dates behavior","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n#!\/opt\/apps\/anaconda3\/bin\/python\r\n\r\nimport pandas\r\nfrom io import StringIO\r\n\r\nif __name__ == \"__main__\":\r\n    csv = StringIO('''\r\nTicker,Last Update Timestamp\r\nAAA,01\/29\/2024 17:04:19\r\nAAA,01\/30\/2024 04:19:57\r\nABEQ,02\/08\/2024 14:33:51\r\nABEQ,02\/06\/2024 15:04:57\r\nABEQ,02\/13\/2024 07:53:11\r\n    ''')\r\n\r\n    columns={'Ticker': str, 'Last Update Timestamp': str}\r\n\r\n    df = pandas.read_csv(csv, usecols=columns.keys(), dtype=columns, parse_dates=['Last Update Timestamp'])\r\n\r\n    print(pandas.__version__)\r\n    print(df)\n```\n\n\n### Issue Description\n\nparse_dates in combination with dtype does not correctly identify date column as a DateTime object and, in addition, converts the column into int64 (that are not even valid epochs).\r\n\r\nThis used to work correctly with pandas 1.4.0\r\n\r\nThe output of the above example is:\r\n```\r\n2.2.0\r\n  Ticker Last Update Timestamp\r\n0    AAA   1706547859000000000\r\n1    AAA   1706588397000000000\r\n2   ABEQ   1707402831000000000\r\n3   ABEQ   1707231897000000000\r\n4   ABEQ   1707810791000000000\r\n```\n\n### Expected Behavior\n\n```\r\n#!\/opt\/apps\/anaconda3\/bin\/python\r\n\r\nimport pandas\r\nfrom io import StringIO\r\n\r\nif __name__ == \"__main__\":\r\n    csv = StringIO('''\r\nTicker,Last Update Timestamp\r\nAAA,01\/29\/2024 17:04:19\r\nAAA,01\/30\/2024 04:19:57\r\nABEQ,02\/08\/2024 14:33:51\r\nABEQ,02\/06\/2024 15:04:57\r\nABEQ,02\/13\/2024 07:53:11\r\n    ''')\r\n\r\n    columns={'Ticker': str, 'Last Update Timestamp': str}\r\n\r\n    df = pandas.read_csv(csv, parse_dates=['Last Update Timestamp'])\r\n\r\n    print(pandas.__version__)\r\n    print(df)\r\n\r\n```\r\n\r\nOutput:\r\n```\r\n> .\/date.py\r\n2.2.0\r\n  Ticker Last Update Timestamp\r\n0    AAA   2024-01-29 17:04:19\r\n1    AAA   2024-01-30 04:19:57\r\n2   ABEQ   2024-02-08 14:33:51\r\n3   ABEQ   2024-02-06 15:04:57\r\n4   ABEQ   2024-02-13 07:53:11\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\n\/opt\/apps\/anaconda3\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.10.0-9-amd64\r\nVersion               : #1 SMP Debian 5.10.70-1 (2021-09-30)\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.10.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.0\r\nnumba                 : 0.59.0\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 12.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.24\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.1.1\r\nxlrd                  : None\r\nzstandard             : 0.22.0\r\ntzdata                : 2023.4\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\nNone\r\n\r\n<\/details>\r\n","comments":["Same problem here. Though I don't think manual  using of `dtype` and `parse_dates` together can be considered as something that has \"expected behaviour\".\r\n\r\nBtw, if you use \r\n```python\r\ncolumns = {\"Ticker\": str, \"Last Update Timestamp\": pandas.StringDtype()}\r\n```\r\n\r\nThere is no problem at all.\r\n\r\nAnd one more thing: try to specify incorrect date_format like that:\r\n```python\r\ndf = pandas.read_csv(\r\n        csv,\r\n        usecols=list(columns.keys()),\r\n        dtype=columns,\r\n        parse_dates=[\"Last Update Timestamp\"],\r\n        date_format=\"%Y\/%m\/%d %H:%M:%S\",\r\n    )\r\n```\r\nKind of magic xD. Definitely a bug. Seems like date format inferring or something like that... because if you set\r\n```python\r\ndf = pandas.read_csv(\r\n        csv,\r\n        usecols=list(columns.keys()),\r\n        dtype=columns,\r\n        parse_dates=[\"Last Update Timestamp\"],\r\n        #date_format=\"%Y\/%m\/%d %H:%M:%S\",\r\n        infer_datetime_format=True,\r\n    )\r\n```\r\nStill has a bug.\r\n\r\n<details>\r\n<summary>INSTALLED VERSIONS<\/summary>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.4.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : Russian_Ukraine.1251\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.5.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.1\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.3\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>","Thanks for the report and investigation, confirmed on main. Further investigations and PRs to fix are welcome!\r\n\r\nAlso, while they appear as integers, I'm seeing that the output in the OP are in fact strings.","take","Is anyone working on this issue? I would like to help with it.","> Is anyone working on this issue? I would like to help with it.\r\n\r\nI'm working on it. Should have a PR ready very soon. Apologies for the delay.","Okay. No problem.Thank you for the update."],"labels":["Bug","Dtype Conversions","IO CSV","datetime.date"]},{"title":"BUG: Tick division is incorrect","body":"- [ ] closes #57264 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["did you look at Tick._next_higher_resolution?  it is what we use in `Tick.__mul__` when multiplying by a float, and i suspect that logic can be adapted pretty cleanly"],"labels":["Frequency"]},{"title":"BUG: Columns resulting from joining multiIndex dataFrame are incorrect in python 3.12.1","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas\r\n\r\ndf1=pandas.DataFrame({(\"A\",\"\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\ndf2=pandas.DataFrame({(\"B\",\"\"):[11,12,13]},index=[\"x\",\"y\",\"z\"])\r\n\r\ndf3 = df1.join(df2)\r\n\r\nprint(f\"{df3.columns=}\")\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nIn Python 3.12.1, the `print(f\"{df3=}\")` statement output the following string.\r\n\r\n```\r\ndf3.columns=Index([slice(None, None, None), slice(None, None, None)], dtype='object')\r\n```\r\n\r\nAnd `df3[(\"A\",\"\")]` occurs `KeyError`.\r\n\r\n\r\nBut in python 3.11.7,  `df3[(\"A\",\"\")]` does not occur `KeyError`.\r\n\r\n\r\n### Expected Behavior\r\n\r\nThe `print(f\"{df3=}\")` statement output the following string.\r\n\r\n```\r\ndf3.columns=MultiIndex([('A', ''),('B', '')])\r\n```\r\n\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\npd.show_versions()\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.12.1.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.133.1-microsoft-standard-WSL2\r\nVersion             : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : C.UTF-8\r\nLOCALE              : C.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.4\r\npytz                : 2024.1\r\ndateutil            : 2.8.2\r\nsetuptools          : None\r\npip                 : 24.0\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 5.1.0\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.3\r\nIPython             : 8.21.0\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n<\/details>\r\n","comments":["@yuji38kwmt - can you pls provide folder path for this issue.","@rajnee28 \r\n\r\n>can you pls provide folder path for this issue.\r\n\r\nThe bug does not depend on folder path.\r\n\r\nFor example, you can reproduce the bug in IPython.\r\n\r\n```\r\nIn [59]: import pandas\r\n    ...:\r\n    ...: df1=pandas.DataFrame({(\"A\",\"\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n    ...: df2=pandas.DataFrame({(\"B\",\"\"):[11,12,13]},index=[\"x\",\"y\",\"z\"])\r\n    ...:\r\n    ...: df3 = df1.join(df2)\r\n    ...:\r\n    ...: print(f\"{df3.columns=}\")\r\ndf3.columns=Index([slice(None, None, None), slice(None, None, None)], dtype='object')\r\n```","I don't know what causes the problem, but concat is a little workaround:\r\n```python\r\ndf3 = pd.concat([df1, df2], axis=1) #there is join options too\r\n```","The following code behaved expectedly.\r\n\r\n```\r\nIn [89]: import pandas\r\n    ...:\r\n    ...: df1=pandas.DataFrame({(\"A\",\"X\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n    ...: df2=pandas.DataFrame({(\"B\",\"Y\"):[11,12,13]},index=[\"x\",\"y\",\"z\"])\r\n    ...:\r\n    ...: df3 = df1.join(df2)\r\n\r\nIn [90]: print(f\"{df3.columns=}\")\r\ndf3.columns=MultiIndex([('A', 'X'),\r\n            ('B', 'Y')],\r\n           )\r\n\r\n```\r\n\r\n\r\nApparently the problem occurs when the key with label 1 is an empty string.","Confirmed that this still exists on 2.2.1, but not yet on main. Further investigations and PRs to fix are welcome!","I did a litter investigated the bug.\r\n\r\nThe following code creates a `pd.Series` containing index which of name is `slice(None, None, None)`.\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/v2.2.1\/pandas\/core\/reshape\/merge.py#L837-L838\r\n\r\n# Trial1\r\n\r\n* Python 3.12.1\r\n* pandas 2.2.1\r\n\r\n```\r\nIn [21]: df=pandas.DataFrame({\"A\":[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n\r\nIn [22]: df[:]\r\nOut[22]:\r\n   A\r\nx  1\r\ny  2\r\nz  3\r\n\r\nIn [24]: df[:].columns\r\nOut[24]: Index(['A'], dtype='object')\r\n\r\n\r\n\r\nIn [25]: df11=pandas.DataFrame({(\"A\",\"x\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n\r\nIn [26]: df11[:]\r\nOut[26]:\r\n   x\r\nx  1\r\ny  2\r\nz  3\r\n\r\nIn [27]: df11[:].columns\r\nOut[27]: Index(['x'], dtype='object')\r\n\r\nIn [28]: df12=pandas.DataFrame({(\"A\",\"\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n\r\n# Look at the code !\r\nIn [29]: df12[:]\r\nOut[29]:\r\nx    1\r\ny    2\r\nz    3\r\nName: slice(None, None, None), dtype: int64\r\n\r\nIn [30]: type(df12[:])\r\nOut[30]: pandas.core.series.Series\r\n```\r\n\r\n\r\n# Trial2\r\n* Python 3.11.7\r\n* pandas 2.2.1\r\n\r\n\r\n```\r\nIn [3]: df11=pandas.DataFrame({(\"A\",\"x\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n\r\nIn [4]: df11[:]\r\nOut[4]:\r\n   A\r\n   x\r\nx  1\r\ny  2\r\nz  3\r\n\r\nIn [5]: df11[:].columns\r\nOut[5]:\r\nMultiIndex([('A', 'x')],\r\n           )\r\n\r\nIn [6]: df12=pandas.DataFrame({(\"A\",\"\"):[1,2,3]},index=[\"x\",\"y\",\"z\"])\r\n\r\nIn [7]: df12[:]\r\nOut[7]:\r\n   A\r\n\r\nx  1\r\ny  2\r\nz  3\r\n\r\nIn [8]: df12[:].columns\r\nOut[8]:\r\nMultiIndex([('A', '')],\r\n           )\r\n```\r\n"],"labels":["Bug","Reshaping"]},{"title":"CLN: Remove is_foo_dtype checks","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["This looks like a very nice clean up. I'm missing context, but seems it's just missing fixing the conflict and updating or removing the test were those are still tested, right? Happy to see this finished and merged if that's the case."],"labels":["Clean"]},{"title":"CoW: Track references in unstack if there is no copy","body":"- [ ] xref #56633\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\ncc @jorisvandenbossche @rhshadrach \r\n\r\n@rhshadrach was correct, we can get there without making a copy. it's a special case, but nevertheless\r\n","comments":["might be worth back porting, but not necessary","this one should be good to merge"],"labels":["Reshaping","Copy \/ view semantics"]},{"title":"REGR: Index.map adding back tz to tz-agnostic result","body":"- [x] closes #57192 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nIt seems to me we should skip casting back to the caller's dtype when the caller dtype is datetime, since these are reliably represented as objects (unlike ints, which loses the size). But datetimes\/tzs are definitely not in my wheelhouse.\r\n\r\ncc @jbrockmendel ","comments":["I'm pretty sure the problem is in DatetimeArray._from_scalars, where we need to check that dont have tzawareness-mismatch.  instead of \r\n\r\n```\r\nif lib.infer_dtype(scalars, skipna=True) not in [\"datetime\", \"datetime64\"]:\r\n    raise ValueError\r\n```\r\n\r\nsomething like\r\n\r\n```\r\nif isinstance(dtype, DatetimeTZDtype) and not lib.is_datetime_with_singletz_array(scalars);\r\n     raise ValueError\r\nelif isinstance(dtype, np.dtype) and not lib.is_naive_datetime_array(scalars):\r\n    raise ValueError\r\n```\r\n\r\nis_naive_datetime_array doesn't actually exist but seems like it should.","Thanks @jbrockmendel - this is much better. One thing I'm running into is that `is_datetime_with_singletz_array` doesn't check if any of the values are actually datetimes (will always return True when there are no datetimes), so I think this still needs to have the line with `lib.infer_dtype(scalars, skipna=True)` or maybe use `lib.is_datetime_or_datetime64_array`? Any recommendations?","I'm trying to get rid of uses of lib.infer_dtype as it is very rarely the right tool to reach for.\r\n\r\nSeems like `is_datetime_with_singletz_array(values) and not isna(values).all()` is the simplest way to get what you've described.  Could try to combine those into a single function i guess but i wouldn't bother at this point."],"labels":["Regression","Apply","Index"]},{"title":"DEPR: to_pytimedelta to return Index[object] instead","body":"https:\/\/github.com\/pandas-dev\/pandas\/pull\/52459 did this for `to_pydatetime`, so we should do the same for `to_pytimedelta`","comments":["I'm working on this issue"],"labels":["Timedelta","Deprecate"]},{"title":"REGR: Behavior differs when adding Series to frame between matching and non matching index","body":"follow up to #56426\r\n\r\nthis was inconsistent now for matching vs non-matching\r\n\r\ncc @lukemanley ","comments":[],"labels":["Numeric Operations"]},{"title":"BUG: Inconsistent date_range output when using CustomBusinessDay as freq","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\n\r\noffset = CustomBusinessDay(weekmask='Sun Mon Tue Wed Thu')\r\nstart = datetime(2024, 2, 8, 23)\r\nend = datetime(2024, 2, 16, 14)\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-08 23:00:00', '2024-02-11 23:00:00',\r\n#               '2024-02-12 23:00:00', '2024-02-13 23:00:00',\r\n#               '2024-02-14 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n\r\nstart = datetime(2024, 2, 9, 23)\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00', '2024-02-14 23:00:00',\r\n#               '2024-02-15 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe last date generated from `date_range` with `start = datetime(2024, 2, 8, 23)` does not equal the last date generated from `date_range` when `start = datetime(2024, 2, 9, 23)`, other arguments unchanged. \r\n\r\nWhen the start date falls on a day within the weekmask in `CustomBusinessDay`, `date_range` output does not include 2024-02-15 (Thursday). However, when the start date is on a day not included in the weekmask (Friday or Saturday), `date_range` does include 2024-02-15.\r\n\r\n### Expected Behavior\r\n\r\nWith a freq of `CustomBusinessDay(weekmask='Sun Mon Tue Wed Thu')` and an end of 2024-02-16, I'd expect `date_range` to always include 2024-02-15, regardless of start.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 2e218d10984e9919f0296931d92ea851c6a6faf5\r\npython           : 3.10.4.final.0\r\npython-bits      : 64\r\nOS               : Windows\r\nOS-release       : 10\r\nVersion          : 10.0.22621\r\nmachine          : AMD64\r\nprocessor        : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : None\r\nLOCALE           : English_United States.1252\r\n\r\npandas           : 1.5.3\r\nnumpy            : 1.24.3\r\npytz             : 2022.7.1\r\ndateutil         : 2.8.2\r\nsetuptools       : 58.1.0\r\npip              : 24.0\r\nCython           : 0.29.32\r\npytest           : 7.2.1\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 3.0.3\r\nlxml.etree       : 4.9.1\r\nhtml5lib         : 1.1\r\npymysql          : 1.0.2\r\npsycopg2         : None\r\njinja2           : 3.1.2\r\nIPython          : 8.4.0\r\npandas_datareader: None\r\nbs4              : 4.11.1\r\nbottleneck       : None\r\nbrotli           : None\r\nfastparquet      : None\r\nfsspec           : None\r\ngcsfs            : None\r\nmatplotlib       : 3.5.3\r\nnumba            : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 3.1.1\r\npandas_gbq       : None\r\npyarrow          : 15.0.0\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.9.0\r\nsnappy           : None\r\nsqlalchemy       : 2.0.12\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nxlwt             : None\r\nzstandard        : None\r\ntzdata           : 2023.3\r\n\r\n<\/details>\r\n","comments":["I am guessing the Python range won't return the last value. Does it relate to this issue?","I can replicate the issue (installed versions given at the bottom). I tried messing around with the inputs a bit, and found some potentially interesting things.\r\n\r\nFor one, this does not appear to be an issue when there is no weekday mask. The following code works as you expect:\r\n```python\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\n\r\nstart = datetime(2024, 2, 8, 23)\r\nend = datetime(2024, 2, 16, 14)\r\nd_range = pd.date_range(start, end)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-08 23:00:00', '2024-02-09 23:00:00',\r\n#               '2024-02-10 23:00:00', '2024-02-11 23:00:00',\r\n#               '2024-02-12 23:00:00', '2024-02-13 23:00:00',\r\n#               '2024-02-14 23:00:00', '2024-02-15 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='D')\r\n\r\nstart = datetime(2024, 2, 9, 23)\r\nd_range = pd.date_range(start, end)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-09 23:00:00', '2024-02-10 23:00:00',\r\n#               '2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00', '2024-02-14 23:00:00',\r\n#               '2024-02-15 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='D')\r\n```\r\n\r\nI thought it might have something to do with the fact that the `end` day given falls on a masked weekday, so I tried the following code (just shifting the end date back by one, but also masking Thursdays), but it also worked as expected:\r\n```python\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\n\r\nstart = datetime(2024, 2, 8, 23)\r\nend = datetime(2024, 2, 15, 14)\r\noffset = CustomBusinessDay(weekmask='Sun Mon Tue Wed')\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00', '2024-02-14 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n\r\nstart = datetime(2024, 2, 9, 23)\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00', '2024-02-14 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n```\r\n\r\nAfter that, I thought maybe it had to do with the fact that both the beginning and ending day fell on masked days, so I tried shifting the starting days back by one:\r\n```python\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\n\r\nstart = datetime(2024, 2, 7, 23)\r\nend = datetime(2024, 2, 15, 14)\r\noffset = CustomBusinessDay(weekmask='Sun Mon Tue Wed')\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-07 23:00:00', '2024-02-11 23:00:00',\r\n#               '2024-02-12 23:00:00', '2024-02-13 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n\r\nstart = datetime(2024, 2, 8, 23)\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00', '2024-02-14 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n```\r\n\r\nSo it seems like the problem does exist in this case, which was also the case of the original example given. After that, I tried to change the `end` date to not be on a day where there was a mask, so I kept everything the same, but shifted the end back by a day. It seems to work as expected in that case:\r\n```python\r\nimport pandas as pd\r\nfrom pandas.tseries.offsets import CustomBusinessDay\r\nfrom datetime import datetime\r\n\r\nstart = datetime(2024, 2, 7, 23)\r\nend = datetime(2024, 2, 14, 14)\r\noffset = CustomBusinessDay(weekmask='Sun Mon Tue Wed')\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-07 23:00:00', '2024-02-11 23:00:00',\r\n#               '2024-02-12 23:00:00', '2024-02-13 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n\r\nstart = datetime(2024, 2, 8, 23)\r\nd_range = pd.date_range(start, end, freq=offset)\r\nprint(d_range)\r\n#DatetimeIndex(['2024-02-11 23:00:00', '2024-02-12 23:00:00',\r\n#               '2024-02-13 23:00:00'],\r\n#              dtype='datetime64[ns]', freq='C')\r\n```\r\n\r\nSo, overall, seems like it takes effect when the start and end days fall on masked days of the week.\r\n\r\n<details>\r\n  <summary>Version details<\/summary>\r\n  INSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.9.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.22000\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 142 Stepping 10, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_United States.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.4\r\npytz                : 2022.1\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.0\r\npip                 : 24.0\r\nCython              : 0.29.28\r\npytest              : 7.4.4\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.1\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.4.0\r\npandas_datareader   : None\r\nbs4                 : 4.11.1\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2022.7.1\r\ngcsfs               : None\r\nmatplotlib          : 3.7.1\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 9.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.1\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : 2023.2.0\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : 2.3.0\r\npyqt5               : None\r\n\r\n<\/details>"],"labels":["Bug","Frequency"]},{"title":"BUG: Inconsistent handling of named column indices in to_csv() and read_csv().","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Issue Description\r\n\r\nApparently, it is not possible to read in a .csv table with named column index AND named row index.\r\n\r\nHow to reproduce:\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnp.random.seed(0)\r\ndf = pd.DataFrame(np.random.randn(5,3), \r\n                  columns=[\"A\", \"B\", \"C\"]).round(1)\r\ndf.index.name = \"Count\"\r\ndf.columns.name = \"Name\"\r\n\r\n# INCONSISTENCY 1: \r\n# .to_csv() only saves the row index name, it ignores the column index name.\r\ndf.to_csv(\"out.csv\")\r\n\r\n# The work-around: convert to column index to MultiIndex\r\ndf.columns = pd.MultiIndex.from_arrays([df.columns])\r\n\r\n# Now, .to_csv() stores both column and index names.\r\ndf.to_csv(\"out.csv\")\r\n```\r\n\r\nThis is how the file looks like:\r\n\r\n```lang-none\r\nName,A,B,C\r\nCount,,,\r\n0,1.8,0.4,1.0\r\n1,2.2,1.9,-1.0\r\n2,1.0,-0.2,-0.1\r\n3,0.4,0.1,1.5\r\n4,0.8,0.1,0.4\r\n```\r\n\r\nHowever, when reading in, I found no way to preserve both the index name of row and column indices.\r\n\r\n```python\r\ndf_new = pd.read_csv(\"out.csv\")\r\nprint(df_new)\r\n# ==> Line 1 is a row instead of column header\r\n\r\ndf_new = pd.read_csv(\"out.csv\", header=[0])\r\nprint(df_new)\r\n# ==> Line 1 is a row instead of column header\r\n\r\ndf_new = pd.read_csv(\"out.csv\", header=[0, 1])\r\nprint(df_new)\r\n# ==> Creates a two-level multi-index column header\r\n\r\ndf_new = pd.read_csv(\"out.csv\", header=[0, 1], index_col=[0])\r\nprint(df_new)\r\n# ==> Creates a two-level multi-index column header\r\n\r\ndf_new = pd.read_csv(\"out.csv\", header=[1], index_col=[0])\r\nprint(df_new)\r\n# ==> Correctly create the row index, but loses the column info\r\n\r\ndf_new = pd.read_csv(\"out.csv\", header=[1], index_col=[0])\r\nprint(df_new)\r\n# ==> Correctly create the row index, but loses the column info\r\n```\r\n\r\nI think this behavior is inconsistent, as switching to a multi-index with two or more levels works as expected:\r\n\r\n```python\r\n# INCONSISTENCY 2:\r\n# A multi-index column header with >1 levels is handled correctly\r\ndf_mh = df.copy()\r\nl1 = list(\"ABC\")\r\nl2 = list(\"abc\")\r\ndf_mh.columns = pd.MultiIndex.from_arrays([l1, l2])\r\ndf_mh.columns.names = [\"Name1\", \"Name2\"]\r\ndf_mh.to_csv(\"out_mh.csv\")\r\ndf_new = pd.read_csv(\"out_mh.csv\", header=[0,1], index_col=[0])\r\n```\r\nThis reads in the CSV correctly.\r\n\r\n\r\n\r\nThe following SO posts are related:\r\n* https:\/\/stackoverflow.com\/questions\/35047842\r\n* https:\/\/stackoverflow.com\/questions\/67499614\r\n\r\n### Expected Behavior\r\n\r\nIn summary, I identified the following inconsistencies:\r\n1. For a simple pd.Index, the row index name is preserved using `to_csv()`. However, the column index name is not preserved. This is unexpected, and at least could be [documented](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.to_csv.html)\r\n2. For the corner case with a pd.MultiIndex using one level, pd.read_csv() fails to read the multi-header back in.\r\n\r\nI know, it's a corner case. However, the fact that the column index can have a name, and the fact that column and header are both represented using the same object, it is somewhat unexpected that those inconsistencies occur.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.5.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 21.6.0\r\nVersion               : Darwin Kernel Version 21.6.0: Thu Sep 29 20:13:56 PDT 2022; root:xnu-8020.240.7~1\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.23.1\r\npytz                  : 2022.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 58.1.0\r\npip                   : 24.0\r\nCython                : 0.29.30\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.1\r\nnumba                 : None\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Is there a format we could use that would be backwards compatible and allow for storing the column index name? If not, I do not think we should make a backwards incompatible change to support it. But we can document that it will get ignored.\r\n\r\nAs an aside, the CSV format is a good way to store simple tabular data in readable plaintext, but it is not a good format for round-tripping. If you are looking for round-tripping, I would suggest parquet.","Thank you for the answer. Yes, I totally agree with your point that CSV is a limited format and that Parquet and perhaps HDF5 are more suitable. It still occurs to me that when working with technically less skilled persons, CSV is hard to avoid.\r\n\r\n\r\nI don't exactly get the rational for the following behavior:\r\n\r\n```python\r\n# Two-level header\r\ndf1 = pd.read_csv(\"data.csv\", header=[0,1], index_col=[0])\r\n# One-level header\r\ndf2 = pd.read_csv(\"data.csv\", header=[0], index_col=[0])\r\n```\r\n\r\ndf1 is read as expected, with and without named column-levels, and also if the row index is not named. Note that depending on the presence of a row index name, read_csv() consumes either two or three lines before reading in the row data. So read_csv() can choose correctly based on some inner logic which case applies. For this special logic to be triggered, one has to provide both the arguments `header=[0,1]` and `index_col=[0]`. In this sense, the special reading logic is explicitly requested.\r\n\r\nHowever, in the case of a single-level column index, pandas cannot correctly distinguish between the case with and without  index name, that is, a CSV with or without an extra second line reserved for the row index name. Why is this ability lost?\r\n\r\nThe backwards compatibility requirement could be circumvented by using an extra flag. However, read_csv() already has a lot of parameters, so that introducing another argument affects the usability. In that sense, I'm also not so fond of adding an extra parameter.","> However, in the case of a single-level column index, pandas cannot correctly distinguish between the case with and without index name, that is, a CSV with or without an extra second line reserved for the row index name. Why is this ability lost?\r\n\r\nI do not know the reason, I think investigation here is necessary. If there is a way to make this more consistent and it's backwards compatible, then I'd likely be positive on the change. \r\n\r\n> In that sense, I'm also not so fond of adding an extra parameter.\r\n\r\nAgreed.","I looked into this for a bit and the internal logic behind\r\n`df_new = pd.read_csv(\"out_mh.csv\", header=[0,1], index_col=[0])`\r\nthat infers a third header row has at least four more minor issues related to input sanitizing in addition to being inconsistent with the non-MultiIndex case: \r\n1. Adding a trailing comma to csv line 2 while using `engine=python` does not lead to a `ParserError(\"Header rows must have an equal number of columns.\")` and the csv gets parsed as before. Technically this is also a header, but one can argue the specialness of this row...\r\n2. Doing the above for the c engine leads to a mangled index, with the MultiIndex being interpreted as tuples `Index([0, ('A', 'a'), ('B', 'b'), ('C', 'c')], dtype='object')`\r\n3. Doing the above for the pyarrow engine leads to a `TypeError: an integer is required` \r\n4. Removing all values from line 2 leads to only the MultiIndex with two levels being created when using the c and python engine, the empty inferred  header row being ignored. Under pyarrow, this leads to exception 3.\r\n\r\n\r\n\r\nOne of the potentially backwards compatible options for providing the index name would be to have an additional header row that has one more column than the other header rows. It would start with the index name and otherwise only contain delimiters. One could then explicitly include this additional row in the `header` list (in our MultiIndex example `header=[0,1,2]`). The extraneous column would distinguish this type of header row from one that is supposed to contain an additional MultiIndex level with generated default column names.\r\n \r\nPotentially backwards compatible since this kind of index name row is currently either \r\n\r\n- illegal when explicitly providing this index name row (exception from issue 1.), or\r\n- leads to a bug (2.) or an error (3.) when depending on the inferring logic when using a MultiIndex \r\n\r\nThough this solution seems hackish to me. \r\n\r\nI would at least look into fixing the inferring logic inconsistencies 1.-4."],"labels":["Bug","IO CSV","Needs Discussion"]},{"title":"BUG: `read_parquet` times out resolving S3 region","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport os\r\nimport pandas as pd\r\nimport s3fs\r\n\r\nos.environ[\"AWS_PROFILE\"] = \"my-bucket-rw\"\r\n\r\n# works perfectly\r\ndf = pd.read_csv(\"s3:\/\/my-bucket\/example.csv\")\r\n\r\n# throws error seemingly about the region\r\ndf = pd.read_parquet(\"s3:\/\/my-bucket\/example_parquet\")\r\n# ---------------------------------------------------------------------------\r\n# OSError                                   Traceback (most recent call last)\r\n# Cell In[7], line 1\r\n# ----> 1 df = pd.read_parquet(\"s3:\/\/my-bucket\/example_parquet\")\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pandas\/io\/parquet.py:667, in read_parquet(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\r\n#     664     use_nullable_dtypes = False\r\n#     665 check_dtype_backend(dtype_backend)\r\n# --> 667 return impl.read(\r\n#     668     path,\r\n#     669     columns=columns,\r\n#     670     filters=filters,\r\n#     671     storage_options=storage_options,\r\n#     672     use_nullable_dtypes=use_nullable_dtypes,\r\n#     673     dtype_backend=dtype_backend,\r\n#     674     filesystem=filesystem,\r\n#     675     **kwargs,\r\n#     676 )\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pandas\/io\/parquet.py:267, in PyArrowImpl.read(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\r\n#     264 if manager == \"array\":\r\n#     265     to_pandas_kwargs[\"split_blocks\"] = True  # type: ignore[assignment]\r\n# --> 267 path_or_handle, handles, filesystem = _get_path_or_handle(\r\n#     268     path,\r\n#     269     filesystem,\r\n#     270     storage_options=storage_options,\r\n#     271     mode=\"rb\",\r\n#     272 )\r\n#     273 try:\r\n#     274     pa_table = self.api.parquet.read_table(\r\n#     275         path_or_handle,\r\n#     276         columns=columns,\r\n#    (...)\r\n#     279         **kwargs,\r\n#     280     )\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pandas\/io\/parquet.py:117, in _get_path_or_handle(path, fs, storage_options, mode, is_dir)\r\n#     114 pa_fs = import_optional_dependency(\"pyarrow.fs\")\r\n#     116 try:\r\n# --> 117     fs, path_or_handle = pa_fs.FileSystem.from_uri(path)\r\n#     118 except (TypeError, pa.ArrowInvalid):\r\n#     119     pass\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pyarrow\/_fs.pyx:471, in pyarrow._fs.FileSystem.from_uri()\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pyarrow\/error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n# File \/opt\/conda\/lib\/python3.9\/site-packages\/pyarrow\/error.pxi:91, in pyarrow.lib.check_status()\r\n\r\n# OSError: When resolving region for bucket 'my-bucket': AWS Error NETWORK_CONNECTION during HeadBucket operation: curlCode: 28, Timeout was reached\r\n\r\n# the following two approaches work\r\ndf = pd.read_parquet(\"s3:\/\/my-bucket\/example_parquet\",\r\n                     storage_options = {\"client_kwargs\": {\r\n                         \"verify\": os.getenv(\"AWS_CA_BUNDLE\")\r\n                     }})\r\n\r\ns3 = s3fs.S3FileSystem()\r\ndf = pd.read_parquet(file, filesystem = s3)\n```\n\n\n### Issue Description\n\nI was not sure where to report this issue because it probably involves `pandas`, `pyarrow`, `s3fs` and `fsspec`.\r\n\r\nFWIW I'm using an S3 Compatible Storage called [Scality](https:\/\/www.scality.com\/ring\/).\r\n\r\nAs the example code shows, I'm able to use `read_csv` with the only configuration being the `AWS_PROFILE` environmental variable. Trying the analogous `read_parquet` it times out and seems as if it can't resolve the region. My S3-compatible instance is on-premises so there isn't a \"region\", however setting `AWS_DEFAULT_REGION` to `\"\"` also did not solve this.\r\n\r\nInterestingly, supplying either of `AWS_CA_BUNDLE` to `storage_options` or supplying the `filesystem` argument fixes this. Worth noting that in my `~\/.aws\/config` file the `region` and `ca_bundle` values are provided (as is `endpoint_url`).\n\n### Expected Behavior\n\nI think `read_parquet` should behave like `read_csv` and be able to access and read the data without passing in additional configuration.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.16.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.18.0-513.9.1.el8_9.x86_64\r\nVersion               : #1 SMP Thu Nov 16 10:29:04 EST 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : 2024.2.0\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : 0.19.0\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Grepping the pandas code, I find no instances of `AWS_PROFILE`. I don't think pandas is doing anything special in `read_csv`, so it seems likely this is not a pandas issue.","```Python\r\n# works perfectly\r\ndf = pd.read_csv(\"s3:\/\/my-bucket\/example.csv\")\r\n\r\n# throws error seemingly about the region\r\ndf = pd.read_parquet(\"s3:\/\/my-bucket\/example_parquet\")\r\n```\r\n\r\nJust to make sure; your `read_csv` includes a file extension, the `read_parquet` doesn't.\r\nIs that intentional?","Appreciate you both taking a look at this.\r\n\r\nI agree, it's odd I didn't include an extension. I did that because generally I'm working with multipart parquets from Spark which are stored in a folder. \r\n\r\nThat said, I was doing this testing with a single parquet file, and I just repeated it now with a proper extension and the same result occurs. Thanks for the suggestion though, it was worth trying."],"labels":["Bug","IO Network","IO Parquet","Closing Candidate"]},{"title":"BUG(?): Non nano + DatetOffset(component=0) re-infers resolution","body":"```python\r\nIn [1]: import pandas as pd; from io import StringIO; import numpy as np\r\n\r\nIn [2]: ser = pd.Series(pd.DatetimeIndex([\"2000-01-01 00:00:00.012\"], dtype=\"datetime64[ms]\"))\r\n\r\nIn [3]: ser\r\nOut[3]: \r\n0   2000-01-01 00:00:00.012\r\ndtype: datetime64[ms]\r\n\r\nIn [4]: ser + pd.DateOffset(microseconds=0)  # 2.1.4\r\nOut[4]: \r\n0   2000-01-01 00:00:00.012\r\ndtype: datetime64[ms]\r\n\r\nIn [3]: ser + pd.DateOffset(microseconds=0). # main\r\nOut[3]: \r\n0   2000-01-01 00:00:00.012\r\ndtype: datetime64[us]\r\n```\r\n\r\nPossibly from https:\/\/github.com\/pandas-dev\/pandas\/pull\/55595. Should the resolution be re-inferred here? cc @jbrockmendel ","comments":["i think bc you passed microseconds=0, offset._pd_timedelta comes back with unit=\"us\", which is why this gets upcast","Would it be worth no-oping these cases though? ","i dont think so, no.  This is analogous to `pd.Timedelta(0).as_unit(\"us\")` in that the \"us\" unit is informative despite the value being zero."],"labels":["Numeric Operations","Frequency","Non-Nano"]},{"title":"BUG: binop methods with `fill_value` not being respected in pandas 2.2","body":"```python\r\nIn [1]: import pandas as pd; from io import StringIO; import numpy as np\r\n\r\nIn [2]: pd.__version__\r\nOut[2]: '2.1.4'\r\n\r\nIn [3]: ser = pd.Series(pd.array([1, pd.NA], dtype=\"Float64\"))\r\n\r\nIn [4]: ser.eq(np.nan, fill_value=0.0).  # 2.1.4\r\nOut[4]: \r\n0    False\r\n1    False\r\ndtype: boolean\r\n\r\nIn [4]: ser.eq(np.nan, fill_value=0.0). # 2.2\r\nOut[4]: \r\n0    False\r\n1     <NA>\r\ndtype: boolean\r\n```\r\n\r\nI would expect `fill_value` to replace NA with 0.0 before comparing with `np.nan` as described in the docs","comments":["I believe behavior was changed in https:\/\/github.com\/pandas-dev\/pandas\/pull\/55568 but is the 2.2.0 result not the expected result? Since the documentation states that the fill value should only be used when one side is missing, in this case, since the left series is a nullable dtype, should np.nan not be treated as a missing value? I would have thought that even for nullable types, since nan is converted to NA in construction:\r\n\r\n```\r\n>>> pd.Series([np.nan], dtype=\"Float64\")\r\n0    <NA>\r\ndtype: Float64\r\n>>>\r\n```\r\n\r\nIn this example, since both left and right are missing values, fill value should not do a replacement.\r\n\r\n"],"labels":["Missing-data","Numeric Operations","NA - MaskedArrays"]},{"title":"API: Check index and column classess exactly by default","body":"- [ ] closes #57436 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":[],"labels":["Testing","Index"]},{"title":"API: Change `assert_index_equal(exact=)` default from `\"equiv\"` to `True`","body":"Recently, https:\/\/github.com\/pandas-dev\/pandas\/issues\/57429 slipped through the cracks because `Index[int64]` is considered equal to `RangeIndex`. While convenient, this allows code changes to potentially introduce a change that leads to increase in memory usage (or decrease in the hopeful case). I don't think this lack of clarity is worth the convenience of making `Index[int64]` equivalent to `RangeIndex` by default and therefore `exact` should default to `True`","comments":["+1"],"labels":["Testing","API Design","Index"]},{"title":"Potential perf regressions introduced by Copy-on-Write","body":"PR #56633 may have induced a performance regression. If it was a necessary behavior change, this may have been expected and everything is okay.\r\n\r\nPlease check the links below. If any ASVs are parameterized, the combinations of parameters that a regression has been detected for appear as subbullets.\r\n\r\n - [x] [arithmetic.FrameWithFrameWide.time_op_different_blocks](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks) -> https:\/\/github.com\/pandas-dev\/pandas\/pull\/57459\r\n   - [op=<built-in function floordiv>; shape=(1000000, 10)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20floordiv%3E&p-shape=%281000000%2C%2010%29)\r\n   - [op=<built-in function gt>; shape=(1000, 10000)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20gt%3E&p-shape=%281000%2C%2010000%29)\r\n   - [op=<built-in function gt>; shape=(10000, 1000)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20gt%3E&p-shape=%2810000%2C%201000%29)\r\n   - [op=<built-in function gt>; shape=(100000, 100)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20gt%3E&p-shape=%28100000%2C%20100%29)\r\n   - [op=<built-in function gt>; shape=(1000000, 10)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20gt%3E&p-shape=%281000000%2C%2010%29)\r\n - [ ] [arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0)\r\n   - [opname='add'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27add%27)\r\n   - [opname='eq'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27eq%27)\r\n   - [opname='ge'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27ge%27)\r\n   - [opname='gt'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27gt%27)\r\n   - [opname='le'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27le%27)\r\n   - [opname='lt'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27lt%27)\r\n   - [opname='mul'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27mul%27)\r\n   - [opname='ne'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27ne%27)\r\n   - [opname='sub'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27sub%27)\r\n   - [opname='truediv'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.MixedFrameWithSeriesAxis.time_frame_op_with_series_axis0?p-opname=%27truediv%27)\r\n - [x] [arithmetic.NumericInferOps.time_add](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add) (no item_cache overhead)\r\n   - [dtype=<class 'numpy.int16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add?p-dtype=%3Cclass%20%27numpy.int16%27%3E)\r\n   - [dtype=<class 'numpy.int8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add?p-dtype=%3Cclass%20%27numpy.int8%27%3E)\r\n   - [dtype=<class 'numpy.uint16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add?p-dtype=%3Cclass%20%27numpy.uint16%27%3E)\r\n   - [dtype=<class 'numpy.uint8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add?p-dtype=%3Cclass%20%27numpy.uint8%27%3E)\r\n - [x] [arithmetic.NumericInferOps.time_multiply](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply) (no item_cache overhead)\r\n   - [dtype=<class 'numpy.int16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply?p-dtype=%3Cclass%20%27numpy.int16%27%3E)\r\n   - [dtype=<class 'numpy.int8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply?p-dtype=%3Cclass%20%27numpy.int8%27%3E)\r\n   - [dtype=<class 'numpy.uint16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply?p-dtype=%3Cclass%20%27numpy.uint16%27%3E)\r\n   - [dtype=<class 'numpy.uint32'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply?p-dtype=%3Cclass%20%27numpy.uint32%27%3E)\r\n   - [dtype=<class 'numpy.uint8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_multiply?p-dtype=%3Cclass%20%27numpy.uint8%27%3E)\r\n - [x] [arithmetic.NumericInferOps.time_subtract](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_subtract) (no item_cache overhead)\r\n   - [dtype=<class 'numpy.int16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_subtract?p-dtype=%3Cclass%20%27numpy.int16%27%3E)\r\n   - [dtype=<class 'numpy.int8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_subtract?p-dtype=%3Cclass%20%27numpy.int8%27%3E)\r\n   - [dtype=<class 'numpy.uint16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_subtract?p-dtype=%3Cclass%20%27numpy.uint16%27%3E)\r\n   - [dtype=<class 'numpy.uint8'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_subtract?p-dtype=%3Cclass%20%27numpy.uint8%27%3E)\r\n - [ ] [eval.Eval.time_add](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_add)\r\n   - [engine='numexpr'; threads='all'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_add?p-engine=%27numexpr%27&p-threads=%27all%27)\r\n   - [engine='numexpr'; threads=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_add?p-engine=%27numexpr%27&p-threads=1)\r\n - [ ] [eval.Eval.time_mult](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_mult)\r\n   - [engine='numexpr'; threads='all'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_mult?p-engine=%27numexpr%27&p-threads=%27all%27)\r\n   - [engine='numexpr'; threads=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#eval.Eval.time_mult?p-engine=%27numexpr%27&p-threads=1)\r\n - [ ] [frame_ctor.FromNDArray.time_frame_from_ndarray](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_ctor.FromNDArray.time_frame_from_ndarray)\r\n - [ ] [frame_methods.Apply.time_apply_axis_1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Apply.time_apply_axis_1)\r\n - [ ] [frame_methods.Apply.time_apply_pass_thru](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Apply.time_apply_pass_thru)\r\n - [ ] [frame_methods.AsType.time_astype](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype)\r\n   - [from_to_dtypes=('Float64', 'float64'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27Float64%27%2C%20%27float64%27%29&p-copy=True)\r\n   - [from_to_dtypes=('Int64', 'Float64'); copy=False](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27Int64%27%2C%20%27Float64%27%29&p-copy=False)\r\n   - [from_to_dtypes=('Int64', 'Float64'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27Int64%27%2C%20%27Float64%27%29&p-copy=True)\r\n   - [from_to_dtypes=('float64', 'Float64'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27float64%27%2C%20%27Float64%27%29&p-copy=True)\r\n   - [from_to_dtypes=('float64', 'float64[pyarrow]'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27float64%27%2C%20%27float64%5Bpyarrow%5D%27%29&p-copy=True)\r\n   - [from_to_dtypes=('float64[pyarrow]', 'float64'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27float64%5Bpyarrow%5D%27%2C%20%27float64%27%29&p-copy=True)\r\n   - [from_to_dtypes=('int64[pyarrow]', 'float64[pyarrow]'); copy=True](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.AsType.time_astype?p-from_to_dtypes=%28%27int64%5Bpyarrow%5D%27%2C%20%27float64%5Bpyarrow%5D%27%29&p-copy=True)\r\n - [ ] [frame_methods.Dtypes.time_frame_dtypes](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Dtypes.time_frame_dtypes)\r\n - [ ] [frame_methods.Fillna.time_fillna](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Fillna.time_fillna)\r\n   - [inplace=True; dtype='datetime64[ns, tz]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Fillna.time_fillna?p-inplace=True&p-dtype=%27datetime64%5Bns%2C%20tz%5D%27)\r\n   - [inplace=True; dtype='datetime64[ns]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Fillna.time_fillna?p-inplace=True&p-dtype=%27datetime64%5Bns%5D%27)\r\n   - [inplace=True; dtype='timedelta64[ns]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Fillna.time_fillna?p-inplace=True&p-dtype=%27timedelta64%5Bns%5D%27)\r\n - [ ] [frame_methods.Iteration.peakmem_itertuples](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.peakmem_itertuples)\r\n - [ ] [frame_methods.Iteration.peakmem_itertuples_raw](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.peakmem_itertuples_raw)\r\n - [ ] [frame_methods.Iteration.peakmem_itertuples_raw_read_first](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.peakmem_itertuples_raw_read_first)\r\n - [ ] [frame_methods.Iteration.peakmem_itertuples_raw_start](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.peakmem_itertuples_raw_start)\r\n - [ ] [frame_methods.Iteration.peakmem_itertuples_start](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.peakmem_itertuples_start)\r\n - [x] [frame_methods.Iteration.time_iteritems_indexing](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.Iteration.time_iteritems_indexing)  (no item_cache overhead)\r\n - [x] [frame_methods.ToDict.time_to_dict_datetimelike](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToDict.time_to_dict_datetimelike) (no item_cache overhead)\r\n   - [orient='series'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToDict.time_to_dict_datetimelike?p-orient=%27series%27)\r\n - [x] [frame_methods.ToDict.time_to_dict_ints](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToDict.time_to_dict_ints)\r\n   - [orient='series'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToDict.time_to_dict_ints?p-orient=%27series%27)\r\n - [x] [frame_methods.ToNumpy.time_to_numpy_tall](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToNumpy.time_to_numpy_tall)\r\n - [x] [frame_methods.ToNumpy.time_to_numpy_wide](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToNumpy.time_to_numpy_wide)\r\n - [x] [frame_methods.ToNumpy.time_values_tall](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToNumpy.time_values_tall)\r\n - [x] [frame_methods.ToNumpy.time_values_wide](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToNumpy.time_values_wide)\r\n - [ ] [frame_methods.XS.time_frame_xs](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.XS.time_frame_xs)\r\n   - [axis=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.XS.time_frame_xs?p-axis=1)\r\n - [x] [groupby.Fillna.time_srs_bfill](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#groupby.Fillna.time_srs_bfill)\r\n - [x] [groupby.Fillna.time_srs_ffill](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#groupby.Fillna.time_srs_ffill)\r\n - [ ] [groupby.MultipleCategories.time_groupby_transform](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#groupby.MultipleCategories.time_groupby_transform)\r\n - [x] [indexing.Block.time_test](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.Block.time_test)\r\n   - [param1=True; param2='np.array(True)'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.Block.time_test?p-param1=True&p-param2=%27np.array%28True%29%27)\r\n   - [param1=True; param2=array(True)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.Block.time_test?p-param1=True&p-param2=array%28True%29)\r\n - [x] [indexing.DataFrameNumericIndexing.time_loc](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc)\r\n   - [dtype=<class 'numpy.float64'>; index_structure='nonunique_monotonic_inc'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc?p-dtype=%3Cclass%20%27numpy.float64%27%3E&p-index_structure=%27nonunique_monotonic_inc%27)\r\n   - [dtype=<class 'numpy.float64'>; index_structure='unique_monotonic_inc'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc?p-dtype=%3Cclass%20%27numpy.float64%27%3E&p-index_structure=%27unique_monotonic_inc%27)\r\n   - [dtype=<class 'numpy.int64'>; index_structure='nonunique_monotonic_inc'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc?p-dtype=%3Cclass%20%27numpy.int64%27%3E&p-index_structure=%27nonunique_monotonic_inc%27)\r\n   - [dtype=<class 'numpy.int64'>; index_structure='unique_monotonic_inc'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc?p-dtype=%3Cclass%20%27numpy.int64%27%3E&p-index_structure=%27unique_monotonic_inc%27)\r\n   - [dtype=<class 'numpy.uint64'>; index_structure='unique_monotonic_inc'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameNumericIndexing.time_loc?p-dtype=%3Cclass%20%27numpy.uint64%27%3E&p-index_structure=%27unique_monotonic_inc%27)\r\n - [ ] [indexing.DataFrameStringIndexing.time_at](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameStringIndexing.time_at)\r\n - [ ] [indexing.DataFrameStringIndexing.time_getitem_scalar](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameStringIndexing.time_getitem_scalar)\r\n - [ ] [indexing.DataFrameStringIndexing.time_loc](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.DataFrameStringIndexing.time_loc)\r\n - [ ] [indexing.GetItemSingleColumn.time_frame_getitem_single_column_int](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.GetItemSingleColumn.time_frame_getitem_single_column_int)\r\n - [ ] [indexing.GetItemSingleColumn.time_frame_getitem_single_column_label](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.GetItemSingleColumn.time_frame_getitem_single_column_label)\r\n - [ ] [indexing.SetitemObjectDtype.time_setitem_object_dtype](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#indexing.SetitemObjectDtype.time_setitem_object_dtype)\r\n - [x] [reindex.DropDuplicates.time_frame_drop_dups_bool](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reindex.DropDuplicates.time_frame_drop_dups_bool)\r\n   - [inplace=False](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reindex.DropDuplicates.time_frame_drop_dups_bool?p-inplace=False)\r\n - [x] [reshape.ReshapeExtensionDtype.time_stack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack)\r\n   - [dtype='Period[s]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack?p-dtype=%27Period%5Bs%5D%27)\r\n   - [dtype='datetime64[ns, US\/Pacific]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeExtensionDtype.time_stack?p-dtype=%27datetime64%5Bns%2C%20US\/Pacific%5D%27)\r\n - [x] [reshape.ReshapeMaskedArrayDtype.time_stack](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack)\r\n   - [dtype='Float64'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack?p-dtype=%27Float64%27)\r\n   - [dtype='Int64'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack?p-dtype=%27Int64%27)\r\n - [x] [reshape.Unstack.time_full_product](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.Unstack.time_full_product) -> https:\/\/github.com\/pandas-dev\/pandas\/pull\/57487\r\n   - [param1='int'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.Unstack.time_full_product?p-param1=%27int%27)\r\n - [x] [reshape.Unstack.time_without_last_row](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.Unstack.time_without_last_row)\r\n   - [param1='int'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.Unstack.time_without_last_row?p-param1=%27int%27)\r\n - [ ] [rolling.EWMMethods.time_ewm](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#rolling.EWMMethods.time_ewm)\r\n   - [constructor='Series'; kwargs_method=({'halflife': 1000}, 'mean'); dtype='float'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#rolling.EWMMethods.time_ewm?p-constructor=%27Series%27&p-kwargs_method=%28%7B%27halflife%27:%201000%7D%2C%20%27mean%27%29&p-dtype=%27float%27)\r\n   - [constructor='Series'; kwargs_method=({'halflife': 1000}, 'mean'); dtype='int'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#rolling.EWMMethods.time_ewm?p-constructor=%27Series%27&p-kwargs_method=%28%7B%27halflife%27:%201000%7D%2C%20%27mean%27%29&p-dtype=%27int%27)\r\n   - [constructor='Series'; kwargs_method=({'halflife': 10}, 'mean'); dtype='float'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#rolling.EWMMethods.time_ewm?p-constructor=%27Series%27&p-kwargs_method=%28%7B%27halflife%27:%2010%7D%2C%20%27mean%27%29&p-dtype=%27float%27)\r\n   - [constructor='Series'; kwargs_method=({'halflife': 10}, 'mean'); dtype='int'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#rolling.EWMMethods.time_ewm?p-constructor=%27Series%27&p-kwargs_method=%28%7B%27halflife%27:%2010%7D%2C%20%27mean%27%29&p-dtype=%27int%27)\r\n - [ ] [series_methods.Fillna.time_fillna](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.Fillna.time_fillna)\r\n   - [dtype='string'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.Fillna.time_fillna?p-dtype=%27string%27)\r\n - [ ] [series_methods.ToNumpy.time_to_numpy](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.ToNumpy.time_to_numpy)\r\n - [ ] [series_methods.ToNumpy.time_to_numpy_float_with_nan](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.ToNumpy.time_to_numpy_float_with_nan)\r\n - [ ] [sparse.ToCooFrame.time_to_coo](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#sparse.ToCooFrame.time_to_coo)\r\n - [ ] [stat_ops.Correlation.time_corr_series](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.Correlation.time_corr_series)\r\n   - [method='pearson'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.Correlation.time_corr_series?p-method=%27pearson%27)\r\n - [x] [stat_ops.FrameOps.time_op](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op) -> https:\/\/github.com\/pandas-dev\/pandas\/pull\/57459\r\n   - [op='kurt'; dtype='Int64'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27kurt%27&p-dtype=%27Int64%27&p-axis=0)\r\n   - [op='kurt'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27kurt%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='mean'; dtype='Int64'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27mean%27&p-dtype=%27Int64%27&p-axis=None)\r\n   - [op='mean'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27mean%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='mean'; dtype='float'; axis=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27mean%27&p-dtype=%27float%27&p-axis=1)\r\n   - [op='prod'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27prod%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='prod'; dtype='float'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27prod%27&p-dtype=%27float%27&p-axis=None)\r\n   - [op='sem'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sem%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='sem'; dtype='float'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sem%27&p-dtype=%27float%27&p-axis=None)\r\n   - [op='skew'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27skew%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='std'; dtype='Int64'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27std%27&p-dtype=%27Int64%27&p-axis=0)\r\n   - [op='std'; dtype='Int64'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27std%27&p-dtype=%27Int64%27&p-axis=None)\r\n   - [op='std'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27std%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='std'; dtype='float'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27std%27&p-dtype=%27float%27&p-axis=None)\r\n   - [op='sum'; dtype='Int64'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sum%27&p-dtype=%27Int64%27&p-axis=0)\r\n   - [op='sum'; dtype='Int64'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sum%27&p-dtype=%27Int64%27&p-axis=None)\r\n   - [op='sum'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sum%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='sum'; dtype='float'; axis=1](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sum%27&p-dtype=%27float%27&p-axis=1)\r\n   - [op='sum'; dtype='float'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27sum%27&p-dtype=%27float%27&p-axis=None)\r\n   - [op='var'; dtype='Int64'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27var%27&p-dtype=%27Int64%27&p-axis=0)\r\n   - [op='var'; dtype='Int64'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27var%27&p-dtype=%27Int64%27&p-axis=None)\r\n   - [op='var'; dtype='float'; axis=0](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27var%27&p-dtype=%27float%27&p-axis=0)\r\n   - [op='var'; dtype='float'; axis=None](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op?p-op=%27var%27&p-dtype=%27float%27&p-axis=None)\r\n - [x] [strings.Construction.time_construction](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#strings.Construction.time_construction) (Series constructor now has copy=True by default)\r\n   - [pd_type='series'; dtype='string[python]'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#strings.Construction.time_construction?p-pd_type=%27series%27&p-dtype=%27string%5Bpython%5D%27)\r\n\r\nSubsequent benchmarks may have skipped some commits. The link below lists the commits that are between the two benchmark runs where the regression was identified.\r\n\r\n[Commit Range](https:\/\/github.com\/pandas-dev\/pandas\/compare\/c3014abb3bf2e15fa66d194ebd2867161527c497...3b57972ceb49c11198eb0bec6be0006260d48085)\r\n\r\ncc @phofl\r\n","comments":["Thanks for the list! \r\nI was just planning to look at ASV for CoW today. Investigation of a few of the list:\r\n\r\n[reshape.Unstack.time_full_product](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.Unstack.time_full_product)\r\n\r\n```\r\nm = 100\r\nn = 1000\r\n\r\nlevels = np.arange(m)\r\nindex = pd.MultiIndex.from_product([levels] * 2)\r\ncolumns = np.arange(n)\r\nvalues = np.arange(m * m * n).reshape(m * m, n)\r\ndf = pd.DataFrame(values, index, columns)\r\n\r\n%timeit df.unstack()\r\n```\r\n\r\nI am using pandas 2.2.0 to easily be able to switch between then default and CoW mode. With that, I can reproduce a slowdown in `unstack`. Profiling this with CoW enabled, this seems to be caused by a slower DataFrame construction of the final result in `Unstacker.get_result`. I _assume_ this is because of the DataFrame constructor now copying an input numpy array, and in this case we can pass `copy=False` as we know we created the new `values` ourselves:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/core\/reshape\/reshape.py#L242-L244\r\n\r\n[arithmetic.FrameWithFrameWide.time_op_different_blocks op=; shape=(1000000, 10)](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.FrameWithFrameWide.time_op_different_blocks?p-op=%3Cbuilt-in%20function%20floordiv%3E&p-shape=%281000000%2C%2010%29)\r\n\r\n```\r\nn_rows, n_cols = 1_000_000, 10\r\n\r\n# construct dataframe with 2 blocks\r\narr1 = np.random.randn(n_rows, n_cols \/\/ 2).astype(\"f8\")\r\narr2 = np.random.randn(n_rows, n_cols \/\/ 2).astype(\"f4\")\r\ndf = pd.concat([DataFrame(arr1), DataFrame(arr2)], axis=1, ignore_index=True)\r\ndf._consolidate_inplace()\r\n\r\narr1 = np.random.randn(n_rows, max(n_cols \/\/ 4, 3)).astype(\"f8\")\r\narr2 = np.random.randn(n_rows, n_cols \/\/ 2).astype(\"i8\")\r\narr3 = np.random.randn(n_rows, n_cols \/\/ 4).astype(\"f8\")\r\ndf2 = pd.concat(\r\n    [DataFrame(arr1), DataFrame(arr2), DataFrame(arr3)],\r\n    axis=1,\r\n    ignore_index=True,\r\n)\r\ndf2._consolidate_inplace()\r\n\r\n%timeit df > df2\r\n```\r\n\r\nThis I can also reproduce, and a profile indicates the slowdown comes entirely from the actual `>` operation on the underlying data. So the one difference seems to be that with CoW, the underlying block values seem to be row major (column major in the transposed Block.values), while without CoW it's the expected column major layout.\r\nSimpler example:\r\n\r\n```\r\n>>> pd.options.mode.copy_on_write = False\r\n>>> arr = np.random.randn(n_rows, n_cols)\r\n>>> df = pd.concat([pd.DataFrame(arr), pd.DataFrame(arr)], axis=1, ignore_index=True)\r\n>>> df._mgr.blocks[0].values.shape\r\n(10, 1000000)\r\n>>> df._mgr.blocks[0].values.flags\r\n  C_CONTIGUOUS : True\r\n  F_CONTIGUOUS : False\r\n  OWNDATA : False\r\n  WRITEABLE : True\r\n  ALIGNED : True\r\n  WRITEBACKIFCOPY : False\r\n\r\n>>> pd.options.mode.copy_on_write = True\r\n>>> df = pd.concat([pd.DataFrame(arr), pd.DataFrame(arr)], axis=1, ignore_index=True)\r\n>>> df._mgr.blocks[0].values.flags\r\n  C_CONTIGUOUS : False\r\n  F_CONTIGUOUS : True\r\n  OWNDATA : False\r\n  WRITEABLE : True\r\n  ALIGNED : True\r\n  WRITEBACKIFCOPY : False\r\n```\r\n\r\nSo something seems to go wrong in the DataFrame constructor. With CoW enabled, this should make a copy by default, but when making this copy, it seems to not use the correct C vs F order.\r\n\r\n[series_methods.ToNumpy.time_to_numpy](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.ToNumpy.time_to_numpy)\r\n\r\n```\r\nN = 1_000_000\r\nser = pd.Series(np.random.randn(N,))\r\n\r\n%timeit ser.to_numpy()\r\n```\r\n\r\ngives\r\n```\r\nIn [80]: %timeit ser.to_numpy()\r\n846 ns \u00b1 57.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)   # witout CoW\r\n2.38 \u00b5s \u00b1 10.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)     # with CoW\r\n```\r\n\r\nIn this case there _is_ some extra work being done to return a read-only view:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/core\/base.py#L667-L669\r\n\r\nSo it might be this is just that (but haven't checked in detail, from a quick profile nothing stood really out apart from just the body of `to_numpy` taking a bit longer)","Starting a list here what is a false-positive\/expected:\r\n\r\n- [time_to_dict_datetimelike](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#frame_methods.ToDict.time_to_dict_datetimelike)\r\n\r\nboth to_dict cases are only as fast because of the item_cache which is something we removed, so nothing we can do\r\n\r\n- time_iteritems_indexing\r\n\r\nalso item_cache, iterating over the columns once is actually twice as fast now, but doing it repeatedly scales linearly now, but this is expected\r\n\r\n\r\n- [groupby.Fillna.time_srs_bfill](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#groupby.Fillna.time_srs_bfill) and fill actually are faster now if you increase the objects, the actual object is tiny, so it's probably the constant cost caused the increase (we are 20% faster if we create an object that runs around 100ms)\r\n\r\n- time_loc scales well, it's only on small dfs that we see the slowdown","For the several `time_stack` ones (eg [reshape.ReshapeMaskedArrayDtype.time_stack - dtype='Float64'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#reshape.ReshapeMaskedArrayDtype.time_stack?p-dtype=%27Float64%27)), it seems that the CoW PR only gave a small perf regression (which I think is again due to the item cache being gone), but those benchmarks show a much bigger slowdown some commits later (I see @rhshadrach and @DeaMariaLeon were discussing those yesterday on slack). \r\nGiven that those are not related to CoW, marking them as \"done\" in the top post here.\r\n","For the various [arithmetic.NumericInferOps.time_add - dtype=<class 'numpy.int16'>](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#arithmetic.NumericInferOps.time_add?p-dtype=%3Cclass%20%27numpy.int16%27%3E) cases (also for `time_multiply` and `time_subtract`, I assume): the small slowdown is also due to the item_cache removal.   \r\nAnd the fact that we only see it for the lower bitwidth dtypes, is because it seems the actual addition operation for those is much faster, and so the overhead of getting the column becomes relatively bigger, making this noticeable in the benchmark. \r\nMarked those as \"done\".","For [strings.Construction.time_construction](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#strings.Construction.time_construction), the reason of the slowdown is that `Series(nparr)` now by default makes a copy, so that is expected (without the benchmark using `copy=False`)","[stat_ops.FrameOps.time_op](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#stat_ops.FrameOps.time_op)\r\n\r\n```python\r\ndtype = \"float\"\r\naxis = 0\r\nvalues = np.random.randn(100000, 4)\r\ndf = pd.DataFrame(values).astype(dtype)\r\n\r\n%timeit df.sum(axis=axis)\r\n```\r\n\r\nThis was also because of the constructor copying the array but not creating the correct columnar layout, and so this should be fixed on main with https:\/\/github.com\/pandas-dev\/pandas\/pull\/57459 (confirmed locally, in a few days we should also see this on the ASV results)","[series_methods.Fillna.time_fillna - dtype='string'](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#series_methods.Fillna.time_fillna?p-dtype=%27string%27)\r\n\r\n```python\r\ndtype = \"string\"\r\nN = 10**6\r\ndata = np.array([str(i) * 5 for i in range(N)], dtype=object)\r\nna_value = pd.NA\r\nfill_value = data[0]\r\nser = pd.Series(data, dtype=dtype)\r\nser[::2] = na_value\r\n\r\n%timeit ser.fillna(fill_value)\r\n```\r\n\r\nThis is a slowdown that I can reproduce, and is due to the additional `hasna` check that we added (linking to the 2.2 code, so its clearer what is run in case of CoW vs before):\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/3cc5afa53e123a89e594df87630f9ac61e718a09\/pandas\/core\/internals\/blocks.py#L2321-L2329\r\n\r\nSo for CoW, we added the additional check to see if there are actually NAs to fill, and if not just return the original values + the correct refs. \r\nBut for the case where we are actually filling NAs, it seems this `_hasna` check gives a significant overhead (around 30% in this example, and at least for string dtype, since that's the only dtype that shows a regression in the ASV results)\r\n\r\nNow, that is also only for our own object-dtype string dtype, so given we have pyarrow-backed strings as a faster alternative if you care about performance, this is probably an OK trade-off (since the `hasna` check gives other benefits, like avoiding a copy if no NAs)","Although after taking a quick look at `isna` for strings, it's also easy to speed it up, to offset the additional overhead a bit -> https:\/\/github.com\/pandas-dev\/pandas\/pull\/57733","I've looked through the most recent benchmarks, confirmed https:\/\/github.com\/pandas-dev\/pandas\/issues\/57431#issuecomment-1978212744, and checked off the box."],"labels":["Performance","Regression","Copy \/ view semantics"]},{"title":"BUG: Resampling with non nano returns an error","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport numpy as np\r\nimport operator\r\n\r\n#Load the data\r\n\r\nurl = 'https:\/\/raw.githubusercontent.com\/LinkedEarth\/Pyleoclim_util\/master\/pyleoclim\/data\/LR04.csv'\r\ndf = pd.read_csv(url,skiprows=4,header=0)\r\n\r\nmultiplier = 1\r\nrule = f'{1_000*multiplier}YS'\r\n\r\nop=operator.sub\r\n\r\nSECONDS_PER_YEAR = 31556925.974592 \r\nindex = pd.DatetimeIndex(op(\r\n    np.datetime64(str(1950), 's'),\r\n    (df.iloc[:,0]*SECONDS_PER_YEAR*10**3).astype('timedelta64[s]')\r\n),name='datetime')\r\n\r\nvalue = df.iloc[:,1].to_numpy()\r\n\r\nser = pd.Series(value, index=index)\r\nser2 = ser.resample(rule)\n```\n\n\n### Issue Description\n\nProduces the following error: \r\n```\r\nTraceback (most recent call last):\r\n\r\n  Cell In[13], line 1\r\n    ser2 = ser.resample(rule)\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:9765 in resample\r\n    return get_resampler(\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:2044 in get_resampler\r\n    return tg._get_resampler(obj, kind=kind)\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:2225 in _get_resampler\r\n    return DatetimeIndexResampler(\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:187 in __init__\r\n    self.binner, self._grouper = self._get_binner()\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:252 in _get_binner\r\n    binner, bins, binlabels = self._get_binner_for_time()\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:1735 in _get_binner_for_time\r\n    return self._timegrouper._get_time_bins(self.ax)\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/resample.py:2323 in _get_time_bins\r\n    bins = lib.generate_bins_dt64(\r\n\r\n  File lib.pyx:891 in pandas._libs.lib.generate_bins_dt64\r\n\r\nValueError: Values falls before first bin\r\n```\r\n\r\nNote that this issue did not exist in 2.1.4\n\n### Expected Behavior\n\nA resampled timeseries as of 2.1.4\n\n### Installed Versions\n\n<details>\r\n\r\npd.show_versions()\r\n\/Users\/deborahkhider\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.8.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 22.6.0\r\nVersion               : Darwin Kernel Version 22.6.0: Wed Jul  5 22:21:56 PDT 2023; root:xnu-8796.141.3~6\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.24.3\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 23.1.2\r\nCython                : 0.29.33\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.6\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.6.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.6.3\r\nnumba                 : 0.57.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.23\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : 2023.1.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : 2.3.1\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, I am seeing this also fail on 2.1.4 - can you check that again and post the `pd.show_versions()` of the 2.1.4 environment you're seeing it succeed in?\r\n\r\nAlso - I'm seeing this:\r\n\r\n```\r\nprint(ser.index)\r\n# DatetimeIndex(['-5298046-05-16 11:11:03', '-5303046-05-17 11:19:50',\r\n#                '-5308046-05-19 11:28:37', '-5313046-05-20 11:37:24',\r\n#                '-5318046-05-22 11:46:11'],\r\n#               dtype='datetime64[s]', name='datetime', freq=None)\r\n```\r\n\r\nare those the desired values?","Confirming that it works for me with 2.1.4 on two different Mac machines. \r\n\r\nThis is from the second machine:\r\n```\r\npd.show_versions()\r\n\/Users\/deborahkhider\/anaconda3\/envs\/paleodev\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.3.0\r\nVersion             : Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:59 PST 2023; root:xnu-10002.81.5~7\/RELEASE_ARM64_T6030\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : en_US.UTF-8\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : 8.0.0\r\nhypothesis          : None\r\nsphinx              : 5.0.2\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.20.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : 0.58.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 15.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.12.0\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : 2.0.1\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : 2.4.1\r\npyqt5               : None\r\n```\r\n\r\nAlso, I recreated this function from the documentation of another code base that is now compiling fine on readthedocs. \r\n\r\nYes. these are the expected values. "],"labels":["Bug","Regression","Resample","Non-Nano"]},{"title":"BUG: read_parquet gives TypeError if dtype is a pyarrow list","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\nlist_int = pa.list_(pa.int64())\r\ns = pd.Series([[1, 1], [2, 2]], dtype=pd.ArrowDtype(list_int))\r\n\r\ndf = pd.DataFrame(s, columns=['col'])\r\ndf.to_parquet('ex.parquet')\r\npd.read_parquet('ex.parquet')\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe example code produces `TypeError: data type 'list<item: int64>[pyarrow]' not understood`\r\n\r\n\r\n\r\n### Expected Behavior\r\n\r\nNo error should occur and `pd.read_parquet('ex.parquet')` should gives a data frame identical to `df`\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.12.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.1.58+\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.25.2\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 23.1.2\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 7.34.0\r\npandas_datareader     : 0.10.0\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.6.0\r\ngcsfs                 : 2023.6.0\r\nmatplotlib            : 3.7.1\r\nnumba                 : 0.58.1\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : 0.19.2\r\npyarrow               : 10.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.25\r\ntables                : 3.8.0\r\ntabulate              : 0.9.0\r\nxarray                : 2023.7.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Can you give this issue a meaningful title? Currently it is `BUG:`.","> Thanks for the report. Can you give this issue a meaningful title? Currently it is `BUG:`.\r\n\r\nThanks! Title updated.","Thanks for the report - further investigations and PRs to fix are welcome!","take","I have done some digging into the problem. By inspecting the stack trace:\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"\/Users\/kinianlo\/github\/pandas\/scripts\/debug.py\", line 19, in <module>\r\n    pd.read_parquet('ex.parquet', engine='auto')\r\n  File \"\/Users\/kinianlo\/.pyenv\/versions\/3.10.13\/lib\/python3.10\/site-packages\/pandas\/io\/parquet.py\", line 670, in read_parquet\r\n    return impl.read(\r\n  File \"\/Users\/kinianlo\/.pyenv\/versions\/3.10.13\/lib\/python3.10\/site-packages\/pandas\/io\/parquet.py\", line 279, in read\r\n    result = pa_table.to_pandas(**to_pandas_kwargs)\r\n  File \"pyarrow\/array.pxi\", line 867, in pyarrow.lib._PandasConvertible.to_pandas\r\n  File \"pyarrow\/table.pxi\", line 4085, in pyarrow.lib.Table._to_pandas\r\n  File \"\/Users\/kinianlo\/.pyenv\/versions\/3.10.13\/lib\/python3.10\/site-packages\/pyarrow\/pandas_compat.py\", line 764, in table_to_blockmanager\r\n    ext_columns_dtypes = _get_extension_dtypes(\r\n  File \"\/Users\/kinianlo\/.pyenv\/versions\/3.10.13\/lib\/python3.10\/site-packages\/pyarrow\/pandas_compat.py\", line 817, in _get_extension_dtypes\r\n    pandas_dtype = _pandas_api.pandas_dtype(dtype)\r\n  File \"pyarrow\/pandas-shim.pxi\", line 140, in pyarrow.lib._PandasAPIShim.pandas_dtype\r\n  File \"pyarrow\/pandas-shim.pxi\", line 143, in pyarrow.lib._PandasAPIShim.pandas_dtype\r\n  File \"\/Users\/kinianlo\/.pyenv\/versions\/3.10.13\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/common.py\", line 1636, in pandas_dtype\r\n    npdtype = np.dtype(dtype)\r\nTypeError: data type 'list<item: int64>[pyarrow]' not understood\r\n``` \r\nit appears that the problem comes from the `table_to_blockmanager` function in `pyarrow.pandas_compat.py`. If `pandas_metadata` exists in the pyarrow table (which comes ultimately from the metadata in the parquet file written by pandas) AND the argument `ignore_metadata=False` then `table_to_blockmanager` would try to construct pandas extension data types using the information from the metadata. For some reason the `_get_extension_dtypes` function which is called by `table_to_blockmanager` would try to reconstruct a numpy datatype even if a `type_mapper` is supplied, using the `numpy_type` in the metadata. This would not cause a problem if `numpy_type` is set to `object` but if instead it is set to e.g. `list<item: int64>[pyarrow]` then numpy would struggle to understand this datatype.\r\n\r\n### Example of metadata:\r\nThe metadata that I was referring to can be obtained through a pyarrow table, e.g. `pq.read_table('ex.parquet').schema.metadata`\r\n- A __bad__ metadata that would causes the error reported:\r\n\r\n{b'pandas': b'{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"stop\": 2, \"step\": 1}], \"column_indexes\": [{\"name\": null, \"field_name\": null, \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": {\"encoding\": \"UTF-8\"}}], \"columns\": [{\"name\": \"col\", \"field_name\": \"col\", \"pandas_type\": \"list[int64]\", __\"numpy_type\": \"list<item: int64>[pyarrow]\"__, \"metadata\": null}], \"creator\": {\"library\": \"pyarrow\", \"version\": \"11.0.0\"}, \"pandas_version\": \"2.1.4\"}'}\r\n\r\n- A __good__ metadata:\r\n\r\n{b'pandas': b'{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"stop\": 2, \"step\": 1}], \"column_indexes\": [{\"name\": null, \"field_name\": null, \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": {\"encoding\": \"UTF-8\"}}], \"columns\": [{\"name\": \"col\", \"field_name\": \"col\", \"pandas_type\": \"list[int64]\", __\"numpy_type\": \"object\"__, \"metadata\": null}], \"creator\": {\"library\": \"pyarrow\", \"version\": \"11.0.0\"}, \"pandas_version\": \"2.1.4\"}'}\r\n\r\n\r\n### temporary fix\r\nUse `pyarrow.parquet.read_table` to read the parquet from disk, and then use `.to_pandas(ignore_metadata=True)` to convert the pyarrow table to a panda dataframe while ignoring the metadata.\r\n\r\n### potential long term fix\r\nEnsure that `numpy_type` is set to `object` when appropriate, e.g. when it is a list of somethings. "],"labels":["Bug","IO Parquet","Arrow"]},{"title":"Deprecate ``future.no_silent_downcasting``","body":"Deprecation is enforced, so should deprecate the option\r\n\r\n#57328","comments":[],"labels":["Deprecate"]},{"title":"TST: Add missing skips for unavailable pyarrow","body":"The `all_parsers` fixture has this check, but some of the other fixtures were missing it.\r\n\r\n- [n\/a] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [n\/a] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Given #54466, this is primarily targeted at getting backported to 2.2.x.","This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.","This isn't waiting for me, AFAICT."],"labels":["Testing","Stale","Arrow"]},{"title":"BUG: `KeyError: DictionaryType` when converting to\/from PyArrow backend","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport seaborn as sns\r\n\r\ndf = sns.load_dataset('diamonds')\r\ntable = pa.Table.from_pandas(df)\r\ndf2 = table.to_pandas(types_mapper=pd.ArrowDtype)\r\ndf2.convert_dtypes(dtype_backend='numpy_nullable')\n```\n\n\n### Issue Description\n\nI get an error `KeyError: DictionaryType(dictionary<values=string, indices=int8, ordered=0>)` with a traceback like the following:\r\n<details>\r\n  <summary>Click me<\/summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[27], line 9\r\n      7 table = pa.Table.from_pandas(df)\r\n      8 df2 = table.to_pandas(types_mapper=pd.ArrowDtype)\r\n----> 9 df2.convert_dtypes(dtype_backend='numpy_nullable')\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py:7025](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py#line=7024), in NDFrame.convert_dtypes(self, infer_objects, convert_string, convert_integer, convert_boolean, convert_floating, dtype_backend)\r\n   6896 \"\"\"\r\n   6897 Convert columns to the best possible dtypes using dtypes supporting ``pd.NA``.\r\n   6898 \r\n   (...)\r\n   7022 dtype: string\r\n   7023 \"\"\"\r\n   7024 check_dtype_backend(dtype_backend)\r\n-> 7025 new_mgr = self._mgr.convert_dtypes(  # type: ignore[union-attr]\r\n   7026     infer_objects=infer_objects,\r\n   7027     convert_string=convert_string,\r\n   7028     convert_integer=convert_integer,\r\n   7029     convert_boolean=convert_boolean,\r\n   7030     convert_floating=convert_floating,\r\n   7031     dtype_backend=dtype_backend,\r\n   7032 )\r\n   7033 res = self._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\r\n   7034 return res.__finalize__(self, method=\"convert_dtypes\")\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py:456](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py#line=455), in BaseBlockManager.convert_dtypes(self, **kwargs)\r\n    453 else:\r\n    454     copy = True\r\n--> 456 return self.apply(\r\n    457     \"convert_dtypes\", copy=copy, using_cow=using_copy_on_write(), **kwargs\r\n    458 )\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py:364](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py#line=363), in BaseBlockManager.apply(self, f, align_keys, **kwargs)\r\n    362         applied = b.apply(f, **kwargs)\r\n    363     else:\r\n--> 364         applied = getattr(b, f)(**kwargs)\r\n    365     result_blocks = extend_blocks(applied, result_blocks)\r\n    367 out = type(self).from_blocks(result_blocks, self.axes)\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/blocks.py:694](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/blocks.py#line=693), in Block.convert_dtypes(self, copy, using_cow, infer_objects, convert_string, convert_integer, convert_boolean, convert_floating, dtype_backend)\r\n    691 for blk in blks:\r\n    692     # Determine dtype column by column\r\n    693     sub_blks = [blk] if blk.ndim == 1 or self.shape[0] == 1 else blk._split()\r\n--> 694     dtypes = [\r\n    695         convert_dtypes(\r\n    696             b.values,\r\n    697             convert_string,\r\n    698             convert_integer,\r\n    699             convert_boolean,\r\n    700             convert_floating,\r\n    701             infer_objects,\r\n    702             dtype_backend,\r\n    703         )\r\n    704         for b in sub_blks\r\n    705     ]\r\n    706     if all(dtype == self.dtype for dtype in dtypes):\r\n    707         # Avoid block splitting if no dtype changes\r\n    708         rbs.append(blk.copy(deep=copy))\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/blocks.py:695](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/blocks.py#line=694), in <listcomp>(.0)\r\n    691 for blk in blks:\r\n    692     # Determine dtype column by column\r\n    693     sub_blks = [blk] if blk.ndim == 1 or self.shape[0] == 1 else blk._split()\r\n    694     dtypes = [\r\n--> 695         convert_dtypes(\r\n    696             b.values,\r\n    697             convert_string,\r\n    698             convert_integer,\r\n    699             convert_boolean,\r\n    700             convert_floating,\r\n    701             infer_objects,\r\n    702             dtype_backend,\r\n    703         )\r\n    704         for b in sub_blks\r\n    705     ]\r\n    706     if all(dtype == self.dtype for dtype in dtypes):\r\n    707         # Avoid block splitting if no dtype changes\r\n    708         rbs.append(blk.copy(deep=copy))\r\n\r\nFile [~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/dtypes\/cast.py:1150](http:\/\/localhost:8888\/lab\/tree\/~\/work\/keepdb\/env\/lib\/python3.11\/site-packages\/pandas\/core\/dtypes\/cast.py#line=1149), in convert_dtypes(input_array, convert_string, convert_integer, convert_boolean, convert_floating, infer_objects, dtype_backend)\r\n   1147             inferred_dtype = ArrowDtype(pa_type)\r\n   1148 elif dtype_backend == \"numpy_nullable\" and isinstance(inferred_dtype, ArrowDtype):\r\n   1149     # GH 53648\r\n-> 1150     inferred_dtype = _arrow_dtype_mapping()[inferred_dtype.pyarrow_dtype]\r\n   1152 # error: Incompatible return value type (got \"Union[str, Union[dtype[Any],\r\n   1153 # ExtensionDtype]]\", expected \"Union[dtype[Any], ExtensionDtype]\")\r\n   1154 return inferred_dtype\r\n\r\nKeyError: DictionaryType(dictionary<values=string, indices=int8, ordered=0>)\r\n```\r\n<\/details>\n\n### Expected Behavior\n\nI would expect `df2.convert_dtypes()` to run without error and return a DataFrame.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.2.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:55:06 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6020\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.1.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2024.1\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. I'm not certain this shouldn't raise. Besides raising, it seems to me the only reasonable results are to either not convert or convert to NumPy object dtype. But neither of these are NumPy-nullable dtypes. That suggests to me the operation cannot be performed, i.e. should raise.\r\n\r\ncc @jorisvandenbossche ","I see. So this might just be user error on my part due to lack of understanding.\r\n\r\nFor context, my broader goal is to define a \"canonical\" backend and some conversion functions to\/from PyArrow\/Parquet such that I can do a round trip from DataFrame --> Pyarrow\/Parquet --> DataFrame and get the same types back that I started with, and then test the dataframes for equality.\r\n\r\nIt wasn't immediately clear to me which collection of `types_mapper=pd.ArrowDtype`, `dtype_backend='numpy_nullable'`, and `dtype_backend='numpy_nullable'` I should be using. ","Generally when using Arrow types, it's important to realize there are Arrow types (like dictionary) that don't map to Numpy types. See the table in https:\/\/pandas.pydata.org\/docs\/reference\/arrays.html#pyarrow\r\n\r\nIf your goal to validate a round-trip, it's best to round trip within Arrow types or within Numpy types. As seen in the table in the link above, the only strong parallel between the types are numeric types.","> That suggests to me the operation cannot be performed, i.e. should raise.\r\n\r\nOr, ignore the column? (and leave it as is) \r\n\r\nI agree there is no operation that can be done (in the \"nullable dtypes\" realm, we don't yet have a categorical \/ dictionary type), but there are other cases where this also occurs, eg for our default categorical, or datetime64 or interval dtypes. Those dtypes are also not considered as \"numpy_nullable\", but such columns are left intact. \r\n","> such that I can do a round trip from DataFrame --> Pyarrow\/Parquet --> DataFrame and get the same types back that I started with, and then test the dataframes for equality.\r\n\r\n@ajfriend In general, not specifying any specific keyword (like `types_mapper` or `dtype_backend`) should work. When converting pandas to pyarrow, some metadata about the original dtypes is stored, which is used to try to restore the original dataframe as best as possible when converting back to pandas.","Thanks. This background helps. \r\n\r\nFor my current project, I am able pick what format I want the \"original\" dataframe to be in (the baseline that I would use for comparison), so I'm trying to research what the most stable types\/backend would be, knowing that I'll be storing them as parquet files (but open to alternatives) and with an eye towards what the future of Pandas\/Arrow will look like.\r\n\r\nAt a high level, I think I'm just trying to wrap my head around when I should be using Arrow types vs \"classic pandas\" types, and how they all fit together. Is it fair to say that these details are still being worked out as of Pandas 2.0, and there should be more obvious mappings between pandas types and arrow\/parquet types in Pandas 3.0? Perhaps the answer is that I should just be looking forward to 3.0.\r\n\r\nRight now, I don't have a great mental model to help me expect what the output of a conversion will be, or what the correct default type should be, if such a thing even exists.\r\n\r\nFor example, if I have data like\r\n\r\n```python\r\ndf = pd.DataFrame({\r\n    'colA': [1, 2, 3, None],\r\n    'colB': ['A', 'B', 'C', None],\r\n})\r\n```\r\n\r\nand I use various combinations of transforms like `.astype('category')`, `pa.Table.from_pandas(df).to_pandas()`, `convert_dtypes()`, with combinations of options `dtype_backend` and `types_mapper`, I'll can end up with types like\r\n\r\n`object`\r\n`string`\r\n`string[pyarrow]`\r\n`category`\r\n`dictionary<values=string, indices=int8, ordered=0>[pyarrow]`\r\n\r\nand\r\n\r\n`float64`\r\n`Float64`\r\n`Int64`\r\n`int64[pyarrow]`\r\n`double[pyarrow]`\r\n\r\nThat's a lot of options. I understand the differences between these types and their use cases, but I'm still not sure if I should be using, for example, `category`, `dictionary<values=string, indices=int8, ordered=0>[pyarrow]`, `string`, or `string[pyarrow]`. Or when I should expect a float vs a nullable int. Or when to expect an error like what I posed above (I also like the idea to ignore the column to avoid the error.)\r\n\r\nMaybe the answer for now is that I just need to pick one and be careful about the conversions, but I'm hoping for a future where a lot of that is taken care of for me with reasonable defaults.\r\n\r\nBut I'm sure that its a very tough problem for you all to figure out defaults that work well for most use cases, so I appreciate all the work you're putting in!\r\n\r\n\r\n","To phrase my problem another way, is there a standard way to \"canonicalize\" a Pandas dataframe to use PyArrow types? I'm looking for something idempotent and would be robust to roundtrips.\r\n\r\nHere are two attempts:\r\n\r\n```python\r\ndef canon1(df): # works, but doesn't use pyarrow categorical\r\n    df = df.convert_dtypes(dtype_backend='pyarrow')\r\n    table = pa.Table.from_pandas(df)\r\n    df = table.to_pandas()\r\n    df = df.convert_dtypes(dtype_backend='pyarrow')\r\n    return df\r\n\r\ndef canon2(df): # not idempotent; gives error\r\n    df = df.convert_dtypes(dtype_backend='pyarrow')\r\n    table = pa.Table.from_pandas(df)\r\n    df = table.to_pandas(types_mapper=pd.ArrowDtype)\r\n    return df\r\n```\r\n\r\nWith data like\r\n\r\n```python\r\ndf = pd.DataFrame({\r\n    'colA': [1, 2, 3, None],\r\n    'colB': ['A', 'B', 'C', None],\r\n    'colC': ['aa', 'bb', 'cc', None],\r\n})\r\ndf['colC'] = df['colC'].astype('category')\r\n```\r\n\r\nI end up with types\r\n\r\n```python\r\n>>> canon1(df).dtypes\r\ncolA     int64[pyarrow]\r\ncolB    string[pyarrow]\r\ncolC           category\r\n\r\n>>> canon2(df).dtypes\r\ncolA                                       int64[pyarrow]\r\ncolB                                      string[pyarrow]\r\ncolC    dictionary<values=string, indices=int8, ordere...\r\n```\r\n\r\nA few notes\/questions:\r\n\r\n- If I comment out the last transform of `canon1()` (`df = df.convert_dtypes(dtype_backend='pyarrow')`) I get slightly different outputs. Either `pandas.core.dtypes.dtypes.ArrowDtype` or `pandas.core.arrays.string_.StringDtype`, and it isn't immediately clear if there's a significant difference here. The string representation of the `StringDtype` type is different under `df.info()` and `df.dtypes`: `string` vs. `string[pyarrow]`\r\n- `canon1()` seems to work ok and seems to be idempotent, but doesn't use pyarrow for the categorical type :(\r\n- `canon2()` isn't idempotent; I get a `ValueError: format number 1 of \"dictionary<values=string, indices=int8, ordered=0>[pyarrow]\" is not recognized`\r\n- I also can't do a roundtrip to arrow like `pa.Table.from_pandas(canon2(df)).to_pandas()`. I get the same `ValueError: format number 1 of \"dictionary<values=string, indices=int8, ordered=0>[pyarrow]\" is not recognized`\r\n\r\nIs there a better way to do this? Does this idea of \"PyArrow canonicalization\" even make sense?\r\n\r\n(Also, I may have strayed from the original question, but I'm happy to split these out into separate issues if that seems worthwhile.)\r\n"],"labels":["Bug","Dtype Conversions","Needs Discussion","Arrow","pyarrow dtype retention"]},{"title":"Fix evaluations on Python 3.12","body":"List comprehensions no longer get their own scope [1], so adding a level to the (eventual) call to `sys._getframe` goes outside the actual caller. If running in `pytest` (so that there is a scope outside the caller), you end up looking in some unrelated scope. If you are running a script, then `sys._getframe` raises an error that the level is out of bounds.\r\n\r\nThe `Bitwise operations` warning in `test_scalar_unary` appears to always be raised, so remove the condition.\r\n\r\n[1] https:\/\/docs.python.org\/3.12\/whatsnew\/3.12.html#whatsnew312-pep709\r\n\r\nI'm not entirely sure why neither issue is seen on CI, but there are so many tests\/workflows, I'm also not sure which one to check. I guess I'll see how this goes in this PR.\r\n\r\n- [n\/a] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [n\/a] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["OK, so tests on CI are failing with the change to the warning condition. _However_, if you check [the logs for `main`](https:\/\/github.com\/pandas-dev\/pandas\/actions\/runs\/7880360042\/job\/21502267281#step:8:51), it seems like the warning is raised, but from a weird spot that isn't caught:\r\n```\r\n\/home\/runner\/micromamba\/envs\/test\/lib\/python3.12\/site-packages\/coverage\/parser.py:402: DeprecationWarning: Bitwise inversion '~' on bool is deprecated. This returns the bitwise inversion of the underlying int object and is usually not what you expect from negating a bool. Use the 'not' operator for boolean negation or ~int(x) if you really want the bitwise inversion of the underlying int.\r\n  self.code = compile(text, filename, \"exec\", dont_inherit=True)\r\n```","This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Testing","Stale","Python 3.12"]},{"title":"BUG: Identity checking NA in `map` is incorrect","body":"```python\r\nIn [2]: pd.Series([pd.NA], dtype=\"Int64\").map(lambda x: 1 if x is pd.NA else 2)\r\nOut[2]: \r\n0    2\r\ndtype: int64\r\n```\r\n\r\nIn pandas 2.1\r\n\r\n```python\r\nIn [2]: pd.Series([pd.NA], dtype=\"Int64\").map(lambda x: 1 if x is pd.NA  else 2)\r\nOut[2]: \r\n0    1\r\n```\r\n\r\nThis is probably because we call `to_numpy` before going through `map_array`","comments":["I hit the same issue in 2.2.0, based on https:\/\/github.com\/pandas-dev\/pandas\/issues\/56606#issuecomment-1871319732, it was mentioned this was the expected behavior going forward. Is this no longer the case?","Ah thanks @rohanjain101, I didn't realized you opened https:\/\/github.com\/pandas-dev\/pandas\/issues\/56606\r\n\r\nI would say in an ideal world `pd.NA` still shouldn't get coerced to `np.nan` when evaluating a UDF (and without going through object)"],"labels":["Regression","NA - MaskedArrays","Arrow"]},{"title":"BUG: assert_index_equal defaults to check_exact=False on integers","body":"Ref: https:\/\/github.com\/pandas-dev\/pandas\/pull\/57341#discussion_r1486812938","comments":[],"labels":["Bug","Testing","API - Consistency"]},{"title":"BUG: Inconsistent \"is_year_start\" from DatetimeIndex with freq \"MS\"","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ndr = pd.date_range(\"2017-12-01\", periods=2, freq=\"MS\")\r\ndr_comp = [dt.is_year_start for dt in dr]\r\ndr_attr = dr.is_year_start\r\nassert dr[1].is_year_start\r\nassert dr_comp[1]\r\nassert dr_attr[0]  # Should raise error but doesn't\r\nassert dr_attr[1]  # Raises error but shouldn't\n```\n\n\n### Issue Description\n\nWhen working with DatetimeIndex with frequency \"MS\" the attribute \"is_year_start\" does not obtain the same attribute if you obtain the attribute directly from the DatatimeIndex instead of obtaining directly from each individual value of the DatetimeIndex.\n\n### Expected Behavior\n\nI would expect for the same array of attributes independently of if it is obtained from a comprehensive list or as an attibute as shown in the documentation.\r\n\r\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DatetimeIndex.is_year_start.html\r\n\r\n```python\r\nimport pandas as pd\r\ndr = pd.date_range(\"2017-12-01\", periods=2, freq=\"MS\")\r\ndr_comp = [dt.is_year_start for dt in dr]\r\ndr_attr = dr.is_year_start\r\nassert not dr[1].is_year_start\r\nassert dr[1].is_year_start\r\nassert not dr_comp[1]\r\nassert dr_comp[1]\r\nassert not dr_attr[0] \r\nassert dr_attr[1] \r\n```\n\n### Installed Versions\n\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 3.10.0-1160.95.1.el7.x86_64\r\nVersion               : #1 SMP Fri Jun 23 08:44:55 EDT 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.16.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : 2.0.21\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Confirmed on main, further investigations and PRs to fix are welcome!","take","Upon further inspection, it looks like is_quarter_start with the freq 'MS' is having the same issue\r\n```python\r\nidx = pd.date_range('2017-01-01', periods=6, freq='MS')\r\nidx[idx.is_quarter_start]\r\n```\r\nThis gives the dates 2017-03-01 and 2017-06-01, which are of course not the actual start of quarters"],"labels":["Bug","datetime.date"]},{"title":"Potential regression induced by PR #56037","body":"PR #56037 may have induced a performance regression. If it was a necessary behavior change, this may have been expected and everything is okay.\r\n\r\nPlease check the links below. If any ASVs are parameterized, the combinations of parameters that a regression has been detected for appear as subbullets.\r\n\r\n - [inference.ToDatetimeFromIntsFloats.time_nanosec_float64](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#inference.ToDatetimeFromIntsFloats.time_nanosec_float64)\r\n - [inference.ToDatetimeFromIntsFloats.time_sec_float64](https:\/\/asv-runner.github.io\/asv-collection\/pandas\/#inference.ToDatetimeFromIntsFloats.time_sec_float64)\r\n\r\nSubsequent benchmarks may have skipped some commits. The link below lists the commits that are between the two benchmark runs where the regression was identified.\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/compare\/05c32ba18f88921b78dc5984c70956247497ab4c...d9f70b397a010754ae41e7d201bba05834294559\r\n\r\ncc @jbrockmendel","comments":["My local asv benchmarking shows the regression was introduced in d9f70b3 rather than the previous commits\r\n\r\n```\r\n(venv) \u279c  asv_bench git:(d9f70b397a) asv continuous -f 1.1 -E virtualenv HEAD~ HEAD -b inference.ToDatetimeFromIntsFloats.time_                                                                                                                                                                                                 Couldn't load asv.plugins._mamba_helpers because                                                                                                                                                                                                                                                                                No module named 'libmambapy'                                                                                                                                                                                                                                                                                                    \u00b7 Creating environments                                                                                                                                                                                                                                                                                                         \u00b7 Discovering benchmarks                                                                                                                                                                                                                                                                                                        \u00b7\u00b7 Uninstalling from virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter.                                                                                                                                              \u00b7\u00b7 Installing d9f70b39 <v2.3.0.dev0~271> into virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter.                                                                                                                     \u00b7 Running 12 total benchmarks (2 commits * 1 environments * 6 benchmarks)                                                                                                                                                                                                                                                       [ 0.00%] \u00b7 For pandas commit e37ff77b <v2.3.0.dev0~272> (round 1\/2):                                                                                                                                                                                                                                                            [ 0.00%] \u00b7\u00b7 Building for virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter..\r\n[ 0.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter                                                                                                                                           [ 4.17%] \u00b7\u00b7\u00b7 Running (inference.ToDatetimeFromIntsFloats.time_nanosec_float64--)......                                                                                                                                                                                                                                          [25.00%] \u00b7 For pandas commit d9f70b39 <v2.3.0.dev0~271> (round 1\/2):                                                                                                                                                                                                                                                            [25.00%] \u00b7\u00b7 Building for virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter..\r\n[25.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter\r\n[29.17%] \u00b7\u00b7\u00b7 Running (inference.ToDatetimeFromIntsFloats.time_nanosec_float64--)......                                                                                                                                                                                                                                          [50.00%] \u00b7 For pandas commit d9f70b39 <v2.3.0.dev0~271> (round 2\/2):                                                                                                                                                                                                                                                            [50.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter\r\n[54.17%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_float64                                                                                 260\u00b13ms\r\n[58.33%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_int64                                                                               3.16\u00b10.09ms\r\n[62.50%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_uint64                                                                               3.03\u00b10.2ms                                                                                                                                                                    [66.67%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_float64                                                                                     262\u00b12ms                                                                                                                                                                    [70.83%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_int64                                                                                    31.1\u00b10.3ms\r\n[75.00%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_uint64                                                                                   30.9\u00b10.2ms\r\n[75.00%] \u00b7 For pandas commit e37ff77b <v2.3.0.dev0~272> (round 2\/2):\r\n[75.00%] \u00b7\u00b7 Building for virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter..\r\n[75.00%] \u00b7\u00b7 Benchmarking virtualenv-py3.10-Cython3.0.5-jinja2-matplotlib-meson-meson-python-numba-numexpr-odfpy-openpyxl-pyarrow-python-build-scipy-sqlalchemy-tables-xlrd-xlsxwriter\r\n[79.17%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_float64                                                                              6.17\u00b10.3ms\r\n[83.33%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_int64                                                                               2.93\u00b10.06ms\r\n[87.50%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_nanosec_uint64                                                                              2.91\u00b10.06ms\r\n[91.67%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_float64                                                                                  5.51\u00b10.3ms\r\n[95.83%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_int64                                                                                    31.0\u00b10.3ms\r\n[100.00%] \u00b7\u00b7\u00b7 inference.ToDatetimeFromIntsFloats.time_sec_uint64                                                                                  30.9\u00b10.08ms\r\n| Change   | Before [e37ff77b] <v2.3.0.dev0~272>   | After [d9f70b39] <v2.3.0.dev0~271>   |   Ratio | Benchmark (Parameter)                                   |\r\n|----------|---------------------------------------|--------------------------------------|---------|---------------------------------------------------------|\r\n| +        | 5.51\u00b10.3ms                            | 262\u00b12ms                              |   47.56 | inference.ToDatetimeFromIntsFloats.time_sec_float64     |\r\n| +        | 6.17\u00b10.3ms                            | 260\u00b13ms                              |   42.2  | inference.ToDatetimeFromIntsFloats.time_nanosec_float64 |\r\n\r\nSOME BENCHMARKS HAVE CHANGED SIGNIFICANTLY.\r\nPERFORMANCE DECREASED.\r\n```","Thanks @rtlee9 - isn't that commit associated with the highlighted PR in the OP?","Yeah I was just confirming it was that commit in particular, since the asv benchmarks had skipped a few commits\r\n\r\n> Subsequent benchmarks may have skipped some commits. The link below lists the commits that are between the two benchmark runs where the regression was identified.\r\n","Ah - thanks for confirming."],"labels":["Performance","Regression","Non-Nano"]},{"title":"BUG: to_latex() does not escape index name","body":"### Pandas version checks\n\n- [ ] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\npd.DataFrame({'_A': [1], '_B': ['a']}).set_index(\"_A\").to_latex(escape=True)\r\n# produces:\r\n'\\\\begin{tabular}{ll}\\n\\\\toprule\\n & \\\\_B \\\\\\\\\\n_A &  \\\\\\\\\\n\\\\midrule\\n1 & a \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'\n```\n\n\n### Issue Description\n\nThe resulting table includes the raw index name, instead of escaping properly.\r\n\n\n### Expected Behavior\n\n\r\n```Python\r\nimport pandas as pd\r\n\r\npd.DataFrame({'_A': [1], '_B': ['a']}).set_index(\"_A\").to_latex(escape=True)\r\n>>'\\\\begin{tabular}{ll}\\n\\\\toprule\\n & \\\\_B \\\\\\\\\\n\\_A &  \\\\\\\\\\n\\\\midrule\\n1 & a \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n'\r\n```\n\n### Installed Versions\n\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.10.8.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.22631\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : de_DE.cp1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.1.2\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.0.1\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None","comments":["duplicated: #47489 ","closest fix: #48936 ","This can be fixed with \r\n```py\r\ndf1.to_latex(escape=True)\r\n```\r\n\r\nThe document says\r\n\r\n> By default, the value will be read from the pandas config module and set to True if the option styler.format.escape is \u201clatex\u201d. When set to False prevents from escaping latex special characters in column names.\r\n> Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to False.\r\n\r\nDon't know why this relies on `styler.format.escape`. But default to True would be more sensible imo.\r\n\r\n","> This can be fixed with\r\n> \r\n> ```python\r\n> df1.to_latex(escape=True)\r\n> ```\r\n\r\nDid you test this? The example includes \r\n\r\n```python \r\nescape=True\r\n```\r\n\r\n#47489 goes into more detail, this is definitely an issue. "],"labels":["Duplicate Report","IO LaTeX","Styler"]},{"title":"BUG: the `seconds` attribute of `.dt.components` for pyarrow timestamp datatype seem faulty","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nassert pd.__version__ == '2.2.0'\r\n\r\n# example from the book \"Effective Pandas 2 (by Matt Harrison)\"\r\nclasses = ['cs106', 'cs150', 'hist205', 'hist206', 'hist207']\r\n\r\nstart_dates = (\r\n    pd.Series([\r\n        '2015-03-08',\r\n        '2015-03-08',\r\n        '2015-03-09',\r\n        '2015-03-09',\r\n        '2015-03-11'\r\n    ], dtype='datetime64[ns]', index=classes\r\n).astype('timestamp[ns][pyarrow]'))\r\n\r\nend_dates = (\r\n    pd.Series([\r\n        '2015-05-28 23:59:59', \r\n        '2015-06-01 3:00:00', \r\n        '2015-06-03', \r\n        '2015-06-02 14:20', \r\n        '2015-06-01'\r\n    ], dtype='datetime64[ns]', index=classes\r\n).astype('timestamp[ns][pyarrow]'))\r\n\r\nduration = (end_dates - start_dates)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThese are the data:\r\n\r\n```python\r\nprint(duration)\r\n```\r\n\r\n```\r\ncs106      81 days 23:59:59\r\ncs150      85 days 03:00:00\r\nhist205    86 days 00:00:00\r\nhist206    85 days 14:20:00\r\nhist207    82 days 00:00:00\r\ndtype: duration[ns][pyarrow]\r\n```\r\n\r\nI noticed that calling `.dt.components` on the above duration object behaves differently for the `'timestamp[ns][pyarrow]'` vs. the `'timedelta64[ns]'` data types. For the former, the `seconds` attribute (column) give a very high value (and I assume it is total seconds for the 'HH:MM:SS' part.\r\n\r\n```python\r\nprint(duration.dt.components)\r\n```\r\n\r\nresults in:\r\n\r\n```\r\n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\r\n0    81     23       59    86399             0             0            0\r\n1    85      3        0    10800             0             0            0\r\n2    86      0        0        0             0             0            0\r\n3    85     14       20    51600             0             0            0\r\n4    82      0        0        0             0             0            0\r\n```\r\n\r\n### Expected Behavior\r\n\r\nConverting to pandas 1.x `'timedelta64[ns]'` give an expected result:\r\n\r\n```python\r\nprint(duration.astype('timedelta64[ns]').dt.components)\r\n```\r\n\r\nresults in:\r\n\r\n```\r\n         days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\r\ncs106      81     23       59       59             0             0            0\r\ncs150      85      3        0        0             0             0            0\r\nhist205    86      0        0        0             0             0            0\r\nhist206    85     14       20        0             0             0            0\r\nhist207    82      0        0        0             0             0            0\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 21.6.0\r\nVersion               : Darwin Kernel Version 21.6.0: Sun Nov  6 23:31:13 PST 2022; root:xnu-8020.240.14~1\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : de_DE.UTF-8\r\nLOCALE                : de_DE.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Thanks for the report. Confirmed on main, further investigations and PRs to fix are welcome!","Will try to work on this.","take"],"labels":["Bug","datetime.date","Arrow"]},{"title":"BUG: FutureWarning when splitting a dataframe using `np.split`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nTo reproduce it:\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a': [1, 2, 3]})\r\nlst = np.split(df, 3)\n```\n\n\n### Issue Description\n\nThe above code raises a FutureWarning:\r\n```none\r\nFutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\r\n```\r\nAs far as I understand, `np.split` uses `np.swapaxes` which is raising this warning.\n\n### Expected Behavior\n\nNot show a warning.\n\n### Installed Versions\n\n<details>\r\n\r\npython                : 3.11.5\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\n\r\n<\/details>\r\n","comments":["`swapaxes` is called from shape_base.py in numpy package","Thanks for the report - is there a reason you prefer to use `np.swapaxes(df)` over `df.T`?","@rhshadrach the use case here is not `np.swapaxes(df)` itself, it's `np.split(df)`, which apparently uses `np.swapaxes` under the hood.","Ah, thanks @amanlai. On main, `DataFrame.swapaxes` has been removed and the OP gives the output:\r\n\r\n```[array([[1]]), array([[2]]), array([[3]])]```\r\n\r\nOn 2.2.x, I am seeing\r\n\r\n```\r\n[   a\r\n0  1,    a\r\n1  2,    a\r\n2  3]\r\n```\r\n\r\ncc @jorisvandenbossche @phofl @mroeschke ","Marking this as a blocker for 3.0 so a decision is made."],"labels":["Regression","Compat","Blocker","Deprecate"]},{"title":"BUG: Out of bounds nanosecond timestamp when s was specified ","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport numpy as np\r\n\r\nser_index = pd.DatetimeIndex([\r\n            np.datetime64('0000-01-01', 's'),\r\n            np.datetime64('2000-01-01', 's'),\r\n        ])\n```\n\n\n### Issue Description\n\nTraceback (most recent call last):\r\n\r\n  Cell In[1], line 1\r\n    ser_index = pd.DatetimeIndex([\r\n\r\nNameError: name 'pd' is not defined\r\n\r\n\r\nimport pandas as pd\r\n\r\nimport numpy as np\r\n\r\nser_index = pd.DatetimeIndex([\r\n    np.datetime64('0000-01-01', 's'),\r\n    np.datetime64('2000-01-01', 's'),\r\n])\r\nTraceback (most recent call last):\r\n\r\n  File conversion.pyx:294 in pandas._libs.tslibs.conversion.get_datetime64_nanos\r\n\r\nOverflowError: Overflow occurred in npy_datetimestruct_to_datetime\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n\r\n  Cell In[4], line 1\r\n    ser_index = pd.DatetimeIndex([\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/indexes\/datetimes.py:370 in __new__\r\n    dtarr = DatetimeArray._from_sequence_not_strict(\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimes.py:369 in _from_sequence_not_strict\r\n    subarr, tz = _sequence_to_dt64(\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimes.py:2248 in _sequence_to_dt64\r\n    converted, inferred_tz = objects_to_datetime64(\r\n\r\n  File ~\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/datetimes.py:2398 in objects_to_datetime64\r\n    result, tz_parsed = tslib.array_to_datetime(\r\n\r\n  File tslib.pyx:414 in pandas._libs.tslib.array_to_datetime\r\n\r\n  File tslib.pyx:596 in pandas._libs.tslib.array_to_datetime\r\n\r\n  File tslib.pyx:520 in pandas._libs.tslib.array_to_datetime\r\n\r\n  File conversion.pyx:297 in pandas._libs.tslibs.conversion.get_datetime64_nanos\r\n\r\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 0000-01-01T00:00:00, at position 0\n\n### Expected Behavior\n\nUsed to work as of 2.0.4. \n\n### Installed Versions\n\n<details>\r\n\r\n\/Users\/deborahkhider\/opt\/anaconda3\/envs\/paleopandas\/lib\/python3.10\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.8.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 22.6.0\r\nVersion               : Darwin Kernel Version 22.6.0: Wed Jul  5 22:21:56 PDT 2023; root:xnu-8796.141.3~6\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.24.3\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 23.1.2\r\nCython                : 0.29.33\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.6\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.6.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.6.3\r\nnumba                 : 0.57.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.23\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : 2023.1.0\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : 2.3.1\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Should be fixed by #55901"],"labels":["Bug","Non-Nano"]},{"title":"BUG: Incompatible dype warning when assigning boolean series with logical indexer","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndata1 = pd.Series([True, True, True], dtype=bool)\r\ndata2 = pd.Series([False, False, False], dtype=bool)\r\ncondition = pd.Series([False, True, False], dtype=bool)\r\n\r\ndata1[condition] = data2[condition] # > FutureWarning: Setting an item of incompatible dtype...\n```\n\n\n### Issue Description\n\nThe assignment ```data1[condition] = data2[condition] ``` results in warning claiming incompatible types:\r\n\r\n> FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in \r\n> a future version of pandas. Value '[False]' has dtype incompatible with bool, please \r\n> explicitly cast to a compatible dtype first. data1[condition] = data2[condition]\r\n> \r\nWhich is clearly not true. \r\nThis bug is somewhat related to the one reported in #56600, although the use case is different.\r\nInterestingly, the problem disappears when using another dtype, such as int (code below works as expected, no warnings) :\r\n```\r\ndata1 = pd.Series([1 ,2, 3], dtype=int)\r\ndata2 = pd.Series([4, 5, 6], dtype=int)\r\ncondition = pd.Series([False, True, False], dtype=bool)\r\n\r\ndata1[condition] = data2[condition] \r\n``` \r\n\r\nNote: Reproduced on pandas==2.2.0 and 3.0.0.dev0+292.g70f367194a\r\n\n\n### Expected Behavior\n\nThere should be no warning.\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 70f367194aff043f4f9ca549003df8a2a97f286a\r\npython                : 3.10.12.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.133.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+292.g70f367194a\r\nnumpy                 : 1.26.4\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 59.6.0\r\npip                   : 24.0\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["This actually upcasts to object, which means that the warning is kind of correct, but this should definitely work and continue to work.\r\n\r\nI suspect that we do a align under the hood that causes this, cc @MarcoGorelli ","@phofl I'm not familiar with the implementation and have no idea about the reasons for the upcast, but, intuitively, I see no reason for that to happen. Both series already have the same dtype, so any conversion would be unnecessary. It is also very counter-intuitive to have different behaviors among primitive types (i.e., no upcast in case of ints or floats, but an upcast in case of bools...)."],"labels":["Bug","Indexing","PDEP6-related"]},{"title":"DEPR: pd.concat special cases DatetimeIndex to sort even when sort=False","body":"`pd.concat` has a specific behavior to always sort `DatetimeIndex` when `join=\"outer\"` and the non-concatentation axes are not aligned. This was undocumented (prior to 2.2.1) and is inconsistent with all other index types and data types including other temporal types such as pyarrow timestamps\/dates\/times.\r\n\r\nAn attempt to treat this as a bug fix highlighted that this has been long-standing behavior that users may be accustomed to. (xref #57006)\r\n\r\nHere are two options that come to mind:\r\n\r\n1. Deprecate the existing behavior and do not sort by default for `DatetimeIndex`. This would simplify things by removing the carve out and treating all index types \/ data types consistently. This would require users to explicitly pass `sort=True` when concatenating two frames with monotonic indexes if they wanted to ensure a monotonic result. \r\n\r\n2. Always sort the non-concatenation axis when `join=\"outer\"` (for all dtypes). This would be consistent with how `pd.merge` and `DataFrame.join` handle outer merges and in practice may be more useful behavior since concatenating two frames with monotonic indexes will return a frame with a monotonic index. \r\n\r\n","comments":["+1 on (1) - different sorting defaults for different dtypes is very surprising.\r\n\r\nFor (2), I do prefer concat's way of maintaining order by starting with the first frame and tacking on any new labels to the end as it proceeds through the list. When this order is desired, I think this behavior is harder for the user to replicate.","My vote very much goes to option 2. Setting `sort=False` then should behave the way @rhshadrach describes his preferred concat behaviour. Though I really must say, I don't quite see how often that is useful in real-word examples. Given how much time series-related functionality is built into Pandas I'm surprised option 2 isn't considered the more reasonable use case. In all the applications I see Pandas used it's very rare to have a DataFrame with both \"label-style\" index and columns; usually one of the two is rather a time dimension. Maybe @rhshadrach can provide concrete real-world examples where the non-sorting behaviour is desireable on a true outer join? I personally can't think of any good ones...\r\n\r\nI'm more than happy however to provide a slew of examples of `concat` usage where one of the axes represents a time dimension and, consequently, maintaining sorting on an outer join along the other axis is paramount. Maybe however I'm using the wrong tool? I.e. should I be using `merge` in those cases?","> In all the applications I see Pandas used it's very rare to have a DataFrame with both \"label-style\" index and columns; usually one of the two is rather a time dimension.\r\n\r\nI always use \"label-style\" index and columns.\r\n\r\n> Maybe @rhshadrach can provide concrete real-world examples where the non-sorting behaviour is desireable on a true outer join?\r\n\r\nAny DataFrame where the order of the rows\/columns contains information, such as a sequence of events.\r\n\r\n> I'm more than happy however to provide a slew of examples of `concat` usage where one of the axes represents a time dimension and, consequently, maintaining sorting on an outer join along the other axis is paramount.\r\n\r\nAnd that is still quite easy to attain: `df.sort_index(axis=\"index\")` and `df.sort_index(axis=\"columns\")`. However, if we sorted the non-concatenating index by default, how would users get the alternative order back? That isn't so easy.",">However, if we sorted the non-concatenating index by default, how would users get the alternative order back? That isn't so easy.\r\n\r\nBy using the `sort=False` argument in `pd.concat`, no? Indeed I'm not saying the sort kwarg should be ignored as it used to be, I'm just arguing that the default of sorting seems very reasonable to me in order to keep a vast majority of code short and sweet.\r\n\r\nThe label-only cases surely are out there, but given how much timeseries functionality has been built into Pandas I tend to be convinced that many usecases these days have a time dimension on one of the axes. For what it's worth, essentially all of mine do.\r\n\r\nAre you suggesting that otherwise anyone working almost exclusively with timeseries-like data will always have to invoke pd.concat with subsequent explicit sort calls? That seems rather verbose and error-prone to me. There surely then must be a better way to safely assemble DataFrames in such a setting; maybe concat is not the right thing to use?\r\n\r\n","> By using the `sort=False` argument in `pd.concat`, no?\r\n\r\nPerhaps I'm misunderstanding. @lukemanley opened this issue with the line:\r\n\r\n> `pd.concat` has a specific behavior to always sort `DatetimeIndex` when `join=\"outer\"` and the non-concatentation axes are not aligned.\r\n\r\nThen presented two options to rectify the inconsistency. The first is to treat `DatetimeIndex` as all other dtypes are treated today. The second was:\r\n\r\n> Always sort the non-concatenation axis when join=\"outer\" (for all dtypes). \r\n\r\nand this is the one you agreed with in https:\/\/github.com\/pandas-dev\/pandas\/issues\/57335#issuecomment-1946115289. Perhaps \"always\" doesn't mean what I think it means?\r\n\r\n> Are you suggesting that otherwise anyone working almost exclusively with timeseries-like data will always have to invoke pd.concat with subsequent explicit sort calls?\r\n\r\nNo - `sort=True` will still sort.\r\n","Rereading your previous comment, it seems to me you're advocating changing the default of `sort` in concat to `True` across the board for all dtypes. If that's the case, I think it should be discussed in a separate issue - this one is about rectifying a specific inconsistency with DatetimeIndex.\r\n\r\nIf, on the other hand, you're advocating for a dtype-dependent sorting default (`sort=True` for DatetimeIndex, `False` for other dtypes), then I think this is too complex and surprising of a default.",">  Perhaps \"always\" doesn't mean what I think it means?\r\n\r\nYeah was maybe not being clear here: by \"always\" I really meant when `sort=True` and to have that as the default, at least in the `DateTimeIndex` case.\r\n\r\n>If, on the other hand, you're advocating for a dtype-dependent sorting default (sort=True for DatetimeIndex, False for other dtypes), then I think this is too complex and surprising of a default.\r\n\r\nYeah I can see your point re. surprising behaviour and complexity here. I'd be quite content with `sort=True` by default across the board indeed. As I mentioned, `.groupby()` already has `sort=True` by default.\r\n\r\n> No - sort=True will still sort.\r\n\r\nSure, but unless one forgets to call that it will end up unsorted leading likely to failures that give erroneous results but without throwing an error, i.e. the worst kind of bug (again, coming form the timeseries perspective here). Putting what I'm saying differently: don't you think that sorting by default is safer than not sorting by default? Sure, the former might have unnecessary overheard in some cases, but those can be solved by then explicitly setting `sort=False`; the same goes for the cases where one wants to \"preserve\" order (i.e. the appending behaviour).\r\n\r\nCome to think of it, I would actually also argue that the behaviour of appending indices on the non-concatenation axis is not 100% transparent either: if `concat` is called on e.g. a dictionary, then one tends to think of that as an unsorted collection of elements to be concatenated, yet their implicit order will inlfuence the outcome of the non-concatenation axis when `sort=False`. You see what I mean?","> I'd be quite content with `sort=True` by default across the board indeed. As I mentioned, `.groupby()` already has `sort=True` by default.\r\n\r\nAs mentioned above, I recommend opening a new issue if you'd like to pursue changing the default value of sort."],"labels":["Reshaping","Deprecate","Needs Discussion","Sorting"]},{"title":"BUG: `sort_values` should have consistent behavior irrespective of the number of sort columns","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\ndf = pd.DataFrame([[\"\", 1], [datetime.date.today(), 2]], columns=['a', 'b']) # column a contains mixed data\r\ndf.sort_values(['a']) # raises error\r\ndf.sort_values(['a', 'b'])  # no error\r\ndf.sort_values(['a', 'a'])  # no error\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen using `sort_values` on a data frame where at least one column contains mixed data, it should handle it gracefully or raise appropriate errors. It raises an error if a mixed-type column is used for sorting and succeeds if multiple columns are used, where one of them is a mixed-type column.\r\n\r\n### Expected Behavior\r\n\r\nThe behvior should be consistent irrespective of the number of columns being used for sorting.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 0f437949513225922d851e9581723d82120684a6\r\npython           : 3.9.16.final.0\r\npython-bits      : 64\r\nOS               : Darwin\r\nOS-release       : 22.5.0\r\nVersion          : Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7\/RELEASE_X86_64\r\nmachine          : x86_64\r\nprocessor        : i386\r\nbyteorder        : little\r\nLC_ALL           : en_US.UTF-8\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\npandas           : 2.0.3\r\nnumpy            : 1.24.3\r\npytz             : 2022.1\r\ndateutil         : 2.8.2\r\nsetuptools       : 65.5.1\r\npip              : 22.0.4\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 3.0.3\r\nlxml.etree       : 4.9.1\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.9.3\r\njinja2           : 3.0.1\r\nIPython          : 8.10.0\r\npandas_datareader: None\r\nbs4              : 4.11.1\r\nbottleneck       : None\r\nbrotli           : None\r\nfastparquet      : None\r\nfsspec           : 2022.5.0\r\ngcsfs            : None\r\nmatplotlib       : 3.7.0\r\nnumba            : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 3.0.9\r\npandas_gbq       : None\r\npyarrow          : 14.0.1\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : None\r\nsnappy           : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : 0.8.9\r\nxarray           : None\r\nxlrd             : None\r\nzstandard        : None\r\ntzdata           : 2022.1\r\nqtpy             : None\r\npyqt5            : None\r\n\r\n<\/details>\r\n","comments":["take","Thanks for the report; looks like we take different paths depending on whether there is a single or multiple columns to sort by. I'd guess this was done for performance reasons. Further investigations and PRs to fix are welcome!","@rhshadrach Theres a few ways to patch this up but I'm concerned about some things. \r\n\r\nMerging the 2 conditionals within the core dataframe class' sort_values method\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/core\/frame.py#L7155-L7186\r\n\r\nto \r\n\r\n```\r\nif len(by):\r\n    keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\r\n\r\n    # need to rewrap columns in Series to apply key function\r\n    if key is not None:\r\n        # error: List comprehension has incompatible type List[Series];\r\n        # expected List[ndarray]\r\n        keys = [\r\n            Series(k, name=name)  # type: ignore[misc]\r\n            for (k, name) in zip(keys, by)\r\n        ]\r\n\r\n    indexer = lexsort_indexer(\r\n        keys, orders=ascending, na_position=na_position, key=key\r\n    )\r\n```\r\n\r\nensures consistency by sorting using np.lexsort for all cases. \r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/core\/frame.py#L7155-L7191\r\n\r\nThat being said, performance will suffer for single column case due to extra overhead of np.lexsort compared to current implementation using the more simple argsort.\r\n\r\nAnother approach is to modify the behaviour of the nargsort function in the sorting module to handle this specific case. In particular, changing \r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/core\/sorting.py#L369-L450\r\n\r\nto\r\n\r\n```\r\n    try:\r\n        indexer = non_nan_idx[non_nans.argsort(kind=kind)]\r\n    except (TypeError, AttributeError):\r\n        # Convert datetime objects to ordinal representations\r\n        non_nans_ordinals = []\r\n        for d in non_nans:\r\n            if isinstance(d, datetime.date):\r\n                non_nans_ordinals.append(d.toordinal())\r\n            else:\r\n                # Handle non-date values gracefully\r\n                non_nans_ordinals.append(d)\r\n\r\n        indexer = non_nan_idx[np.argsort(non_nans_ordinals, kind=kind)]\r\n```\r\n\r\nkeeps the n_columns == 1 and > 1 situation have consistent sorting for the case where a , but introduces inconsistency for other cases. For example, if you add an additional row of all with every column as value 3 to the dataframe, the n_columns > 1 case fails while the n_columns  == 1 case passes.\r\n\r\n```\r\nimport pandas as pd\r\nimport datetime\r\ndf = pd.DataFrame([[\"\", 1], [datetime.date.today(), 2], [3, 3]], columns=['a', 'b']) # column a contains mixed data\r\ndf.sort_values(['a']) # fixes this error\r\ndf.sort_values(['a', 'b'])  # new error\r\ndf.sort_values(['a', 'a'])  # new error\r\n```\r\n\r\nwhere the new error is \r\n\r\n> TypeError: 'values' is not ordered, please explicitly specify the categories order by passing in a categories argument.\r\n\r\nI guess I'm unsure whether there is any benefit to ensure consistency if performance will take a hit... second solution is beneficial but only for mixed type columns involving datetime objects. I'm also thinking it might actually be better to error out in case there are inconsistencies in the format of entries in a column so that its caught and relayed to the user.  anyone have any thoughts?","@remiBoudreau My 2 cents would be on ensuring consistency, whether that's raising errors or successful sorting. Of course, performance is important but not as important as accuracy and correctness. Let's try and side ourselves with whether we want to allow mixed-type columns or not and then the decision will follow to either raise errors or fix sorting.","@remiBoudreau - I think we should maintain the fastpath for a single `by` - this is a very common case. That leaves either improving this path to handle this case, or having it raise in both cases.  If we are to improve the fastpath to handle this case, it shouldn't be by using `try...except...`. That can hurt performance, hide bugs, and make things harder to debug.\r\n\r\nIf we want to make it so both paths raise here, we'd need to deprecate the current behavior for multiple `by` values. For this, a first step is in deciding what we want and don't want to support. For example, I know groupby has specific logic for handling sorting with integers, strings, and NA values mixed in."],"labels":["Bug","Sorting"]},{"title":"ENH: `bool` upcast to `Int64`","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nI wish I could upcast `bool` to `Int64`\r\n\r\n### Feature Description\r\n\r\nI have data which can be either `bool` or `NaN`.\r\n\r\nSo a simple `int(val)` wouldn't work (due to NaNs producing an error).\r\n\r\nI don't want a boolean output array, I want `Int64`. Is there a reason not to allow casting of `bools` to `Int64`?\r\n\r\n\r\n```py\r\n# Toy example\r\nimport pandas as pd\r\n\r\ndata = [True, False, pd.NA]\r\nout = pd.Series(dtype='Int64', index=np.arange(5))\r\n\r\n# Desired:\r\nfor val in data:\r\n    out[0] = val  # TypeError: Invalid value 'True' for dtype Int64\r\n```\r\n\r\n\r\n### Alternative Solutions\r\n\r\n```py\r\n# Current workaround:\r\nfor val in data:\r\n    out[0] = pd.Series(val, dtype='Int64')[0]\r\n```\r\n\r\n### Additional Context\r\n\r\n- Would the integration with Arrow be a good opportunity to implement these?\r\n- because [here](https:\/\/pandas.pydata.org\/docs\/dev\/user_guide\/basics.html#astype) it says:\r\n  > Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation.\r\n\r\n  .. maybe now it's not going to be \"always according to the NumPy rules\" but also to some Arrow rules?\r\n\r\n- Are there any other not-implemented upcasting rules which should be?\r\n\r\nHowever, this proves that such upcasting already works, so I'm puzzled why my example above doesn't:\r\n```py\r\ns = pd.Series(data)\r\ns.astype('Int64')\r\n```\r\nis the tag `BUG:` more appropriate here?\r\n","comments":["Thanks for the report. Perhaps a better alternative would be `out[0] = val if pd.isna(val) else int(val)`. This avoids (somewhat expensive) Series construction.\r\n\r\ncc @MarcoGorelli "],"labels":["Enhancement","Needs Discussion","NA - MaskedArrays","PDEP6-related"]},{"title":"BUG: pd.merge has unexpected behaviour when joining on index \/ column","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ndf1 = pd.DataFrame([ {'seq':i, 'val_1':10+i} for i in range(10) ])\r\ndf2 = pd.DataFrame([ {'seq':i+5, 'val_2':10+i+5} for i in range(10) ])\r\n\r\n# This produces NaN's in the index (main result in question)\r\npd.merge(df1, df2, left_index=True, right_on='seq', how='left')\n```\n\n\n### Issue Description\n\nWhen joining two data frames, in this case with a left join, using the left_index, I would expect the left index to be taken for this new data frame, not the values from the right column. As well as being unexpected behaviour, this also results in this case in NaNs in the index, which also has the side effect of changing the index type to float - all in all probably not what was intended.\r\n\r\nGenerate some data\r\n<img width=\"638\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/90188926\/02317b33-30e0-44d2-a51f-559c955319ac\">\r\n\r\ndf1:\r\n<img width=\"147\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/90188926\/6da97274-7c1a-4363-b2ac-1a8bb097119f\">\r\n\r\ndf2:\r\n<img width=\"136\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/90188926\/dcd9ceda-21bb-4341-8ef2-2e1c4925d6a5\">\r\n\r\njoin them:\r\n<img width=\"618\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/90188926\/b3f41136-a7f2-4062-beb6-97906bd31e47\">\r\n\r\n\n\n### Expected Behavior\n\nThe resultant dataframe of a left-join with left-index should have the index of the left data frame.\n\n### Installed Versions\n\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.4.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:54:10 PST 2023; root:xnu-10002.61.3~2\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : None\r\nIPython               : 8.21.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2024.2.0\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n","comments":["I am new to contributing to open source and I am not very experienced. But I believe these could be the possible solution.\r\n`pd.merge(df1,df2,how='left',left_index=True,right_index=True)`"],"labels":["Bug","Reshaping","Needs Triage"]},{"title":"DOC: Wrong generalization about slice indexing","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/version\/2.0.2\/user_guide\/indexing.html#slicing-ranges\n\n### Documentation problem\n\n> With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels\r\n\r\nAnd then shows examples of slice indexing `df[i:j]` where `i` and `j` are integers, and all of them behave as `df.iloc[i:j]` (that is, \"exactly as with an ndarray\").\r\n\r\nBut there is an exception (which I can't make sense of and fortunately is deprecated):\r\n\r\n```\r\n> pd.Series([1, 2, 3], [3., 2., 1.])[:3]\r\n\r\n3.0    1\r\ndtype: int64\r\n```\r\n\r\nSo, when the index is float, integer slices behave loc-like. Please document this exception in a callout because it's dangerous to assume that the iloc-like behavior is general.\r\n\r\nBTW, could someone give a rationale for this behavior? Is there a general rule that ends up in this weird situation, like \"if index and slice are int -> iloc, if not then if slice is of the type of the index -> loc else if slice is int -> iloc else fail\", and then an int slice is taken to be of the type of a float index?\r\n\r\n\n\n### Suggested fix for documentation\n\nDocument this exception in a callout because it's dangerous to assume that the iloc-like behavior is general.","comments":["> BTW, could someone give a rationale for this behavior?\r\n\r\nI guess the convoluted history that https:\/\/github.com\/pandas-dev\/pandas\/issues\/49612 tells explains it.\r\n\r\n1. Initially there was the intention to make all int-slice indexing label based but the deprecations weren't sufficient and at that point doing so would have broken many things.\r\n2. So it was decided that it was ok if int-slice indexing was consistent per se (always position based) despite being somewhat inconsistent wrt to other types of indexing (s[i:j] iloc-like vs s[i] or s[[i,j]] loc-like).\r\n3. But float indexes were\/are an exception for some historical reason (out of curiosity, which one?). So this case was deprecated.\r\n\r\nThat said, I believe that the documentation should be explicit about this exception until the deprecation is actually enforced."],"labels":["Docs","Indexing"]},{"title":"BUG: Series attrs disappear after modifcation to the DataFrame that contains them.","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# Example 1 - changing the data type.\r\ndf = pd.DataFrame(dict(column1=[1,2,3,4], column2=[1,2,3,4]))\r\ndf[\"column1\"].attrs['name'] = 'hello'\r\n\r\nprint(\"\\nExample 1 before:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\n\r\n# Changing the data type of the DataFrame drops the series attributes.\r\ndf1 = df.astype(float)\r\nprint(\"\\nAfter type change:\")\r\nprint(df1)\r\nprint(\"Column1 attributes:\", df1[\"column1\"].attrs)\r\n\r\n\r\n\r\n# Example 2 - assignment to a DataFrame.\r\ndf = pd.DataFrame(dict(column1=[1,2,3,4], column2=[1,2,3,4]))\r\ndf[\"column1\"].attrs['name'] = 'hello1'\r\ndf[\"column2\"].attrs['name'] = 'hello2'\r\n\r\nprint(\"\\n\\n\\nExample 2 before:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\nprint(\"Column2 attributes:\", df[\"column2\"].attrs)\r\n\r\n# Assigning a column to the DataFrame drops the attributes from the Series in DataFrame.\r\ndf[\"column1\"] = df[\"column1\"] + 5\r\nprint(\"\\nAfter assignment to DataFrame:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\nprint(\"Column2 attributes:\", df[\"column2\"].attrs)\n```\n\n\n### Issue Description\n\n# Issue\r\nThe attrs dictionary of Series are not carried through when operations are performed on a DataFrame.\r\n\r\n# Example 1\r\n```python\r\n# Example 1 - changing the data type.\r\ndf = pd.DataFrame(dict(column1=[1,2,3,4], column2=[1,2,3,4]))\r\ndf[\"column1\"].attrs['name'] = 'hello'\r\n\r\nprint(\"\\nExample 1 before:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\n\r\n\"\"\"\r\nOUTUPUT:\r\nExample 1 before:\r\n   column1  column2\r\n0        1        1\r\n1        2        2\r\n2        3        3\r\n3        4        4\r\nColumn1 attributes: {'name': 'hello'}\r\n\"\"\"\r\n```\r\n\r\nThe first series has an attr stored.  However, after changing the data types, the attribute is lost.\r\n\r\n```python\r\n# Changing the data type of the DataFrame drops the series attributes.\r\ndf1 = df.astype(float)\r\nprint(\"\\nAfter type change:\")\r\nprint(df1)\r\nprint(\"Column1 attributes:\", df1[\"column1\"].attrs)\r\n\r\n\"\"\"\r\nOUTUPUT:\r\nAfter type change:\r\n   column1  column2\r\n0      1.0      1.0\r\n1      2.0      2.0\r\n2      3.0      3.0\r\n3      4.0      4.0\r\nColumn1 attributes: {}         <-- NO ATTRS\r\n\"\"\"\r\n```\r\n\r\n# Example 2\r\n```python\r\n# Example 2 - assignment to a DataFrame.\r\ndf = pd.DataFrame(dict(column1=[1,2,3,4], column2=[1,2,3,4]))\r\ndf[\"column1\"].attrs['name'] = 'hello1'\r\ndf[\"column2\"].attrs['name'] = 'hello2'\r\n\r\nprint(\"\\n\\n\\nExample 2 before:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\nprint(\"Column2 attributes:\", df[\"column2\"].attrs)\r\n\r\n\"\"\"\r\nOUTPUT:\r\nExample 2 before:\r\n   column1  column2\r\n0        1        1\r\n1        2        2\r\n2        3        3\r\n3        4        4\r\nColumn1 attributes: {'name': 'hello1'}\r\nColumn2 attributes: {'name': 'hello2'}\r\n\"\"\"\r\n```\r\n\r\nThe first series has an attr stored.  However, reassigning a column drops the attributes.\r\n\r\n```python\r\n# Assigning a column to the DataFrame drops the attributes from the Series in DataFrame.\r\ndf[\"column1\"] = df[\"column1\"] + 5\r\nprint(\"\\nAfter assignment to DataFrame:\")\r\nprint(df)\r\nprint(\"Column1 attributes:\", df[\"column1\"].attrs)\r\nprint(\"Column2 attributes:\", df[\"column2\"].attrs)\r\n\r\n\"\"\"\r\nOUTPUT:\r\nAfter assignment to DataFrame:\r\n   column1  column2\r\n0        6        1\r\n1        7        2\r\n2        8        3\r\n3        9        4\r\nColumn1 attributes: {}        <-- NO ATTRS\r\nColumn2 attributes: {}        <-- NO ATTRS\r\n\"\"\"\r\n```\n\n### Expected Behavior\n\nThe attributes (attrs) should be maintained in the DataFrame when operations are performed on it.\r\n\r\nTested on 2.1.1 and 2.2.0\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.4.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19044\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 165 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en\r\nLOCALE                : English_United States.1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2024.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.0.0\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.1\r\nnumba                 : None\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.1\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report; attr propagation is only partially implemented currently. PRs to fix are welcome!","take","On main this type of Series.attr propagation is basically nonexistent. Practically, only DataFrame.attrs are accessible and get attached to the Series when the Series representation of a column is retrieved.\r\n\r\nI would try to implement a solution that introduces hidden column_attrs to the DataFrame that would support this type of Series.attrs propagation. Solutions that only make use of the DataFrame.attrs by design either lack propagation functionality or lead to confusing behaviour."],"labels":["Bug","metadata"]},{"title":"BUG: pandas.tseries.offsets.Second() division","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas.tseries.offsets as tso\r\na = tso.Second()\/10\r\nb = tso.Second()*.1\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nIn contrast to tso.Minute(), tso.Day(), division with tso.Second() with any integer always results in tso.Day(0) (a). This is not the case with the corresponding multiplication (b).  In previous pandas versions this did not happen. \r\n\r\n### Expected Behavior\r\n\r\ntso.Second()\/10 should return tso.Milli(100) and not tso.Day(0). \r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.10.11.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.22621\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : de_DE.UTF-8\r\nLOCALE                : de_DE.cp1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.25.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.5.0\r\npip                   : 23.0.1\r\nCython                : None\r\npytest                : 7.4.0\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.3\r\nnumba                 : 0.58.0\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.3\r\nsqlalchemy            : 2.0.19\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Result of a git bisect:\r\n\r\n```\r\ncommit 8b8f0d04795e80a9298688e55bb4164b597392bc\r\nAuthor: jbrockmendel\r\nDate:   Tue Dec 19 15:35:22 2023 -0800\r\n\r\n    DEPR: Tick.delta (#56558)\r\n```\r\n\r\ncc @jbrockmendel ","Looks like the difference is with the ``_value`` attr on Timedelta\r\n\r\n```\r\n>>> pd.Timedelta(a)._value\r\n1\r\n>>> a.delta._value\r\n<stdin>:1: FutureWarning: Second.delta is deprecated and will be removed in a future version. Use pd.Timedelta(obj) instead\r\n1000000000\r\n\r\n```\r\n\r\nTaking a look"],"labels":["Bug","Regression","Frequency"]},{"title":"QST: The concept behind how hashes are combined in pandas?","body":"### Research\n\n- [X] I have searched the [[pandas] tag](https:\/\/stackoverflow.com\/questions\/tagged\/pandas) on StackOverflow for similar questions.\n\n- [X] I have asked my usage related question on [StackOverflow](https:\/\/stackoverflow.com).\n\n\n### Link to question on StackOverflow\n\nhttps:\/\/stackoverflow.com\/questions\/77784124\/the-concept-behind-how-hashes-are-combined-in-pandas\n\n### Question about pandas\n\nI came across the `combine_hash_arrays` functions in pandas library [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/core\/util\/hashing.py). How does it keep good collision rate after combination ? Does anyone have the mathematical proof for this code ? \r\n\r\nThe below code is retrieved from pandas implementation.\r\n\r\n```\r\ndef combine_hash_arrays(\r\n    arrays: Iterator[np.ndarray], num_items: int\r\n) -> npt.NDArray[np.uint64]:\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    arrays : Iterator[np.ndarray]\r\n    num_items : int\r\n\r\n    Returns\r\n    -------\r\n    np.ndarray[uint64]\r\n\r\n    Should be the same as CPython's tupleobject.c\r\n    \"\"\"\r\n    try:\r\n        first = next(arrays)\r\n    except StopIteration:\r\n        return np.array([], dtype=np.uint64)\r\n\r\n    arrays = itertools.chain([first], arrays)\r\n\r\n    mult = np.uint64(1000003)\r\n    out = np.zeros_like(first) + np.uint64(0x345678)\r\n    last_i = 0\r\n    for i, a in enumerate(arrays):\r\n        inverse_i = num_items - i\r\n        out ^= a\r\n        out *= mult\r\n        mult += np.uint64(82520 + inverse_i + inverse_i)\r\n        last_i = i\r\n    assert last_i + 1 == num_items, \"Fed in wrong num_items\"\r\n    out += np.uint64(97531)\r\n    return out\r\n```\r\n\r\nI have already checked whether they work well by checking their combined hashes' collision rate.","comments":["#15227 indicates it was taken from https:\/\/github.com\/python\/cpython\/blob\/main\/Objects\/tupleobject.c; perhaps researching how CPython does this may be fruitful. I'd recommend searching the Python dev mailing list.","Here is a more specific link: https:\/\/github.com\/python\/cpython\/blob\/c32bae52904723d99e1f98e2ef54570268d86467\/Objects\/tupleobject.c#L291"],"labels":["Usage Question","hashing"]},{"title":"error_bad_lines not work, but how do I read data successfully?","body":"Since I use panda as version=2.2 I found \"error_bad_lines\" para was dropped, but I use pd.read_csv(\"unknown.csv\"), Got an Error:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\work\\email_reply\\data_process.py\", line 11, in <module>\r\n    df = pd.read_csv('.\/data\/data_0101.csv', on_bad_lines=\"warn\")\r\n  File \"D:\\miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1024, in read_csv\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"D:\\miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 624, in _read\r\n    return parser.read(nrows)\r\n  File \"D:\\miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1921, in read\r\n    ) = self._engine.read(  # type: ignore[attr-defined]\r\n  File \"D:\\miniconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\", line 234, in read\r\n    chunks = self._reader.read_low_memory(nrows)\r\n  File \"parsers.pyx\", line 838, in pandas._libs.parsers.TextReader.read_low_memory\r\n  File \"parsers.pyx\", line 905, in pandas._libs.parsers.TextReader._read_rows\r\n  File \"parsers.pyx\", line 874, in pandas._libs.parsers.TextReader._tokenize_rows\r\n  File \"parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._check_tokenize_status\r\n  File \"parsers.pyx\", line 2061, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\r\n\r\nSo how could I read this data sucessfully now ? If there is a better way to deal with this?","comments":["You can look into the ``on_bad_lines`` parameter.","I'm seeing the identical error [nearly to the same line numbers] on pip 2.2.0. I tried updating to 2.2.1 before reading this (obviously didn't help). Has anyone found a solution??","Can you provide a reproducible example?"],"labels":["Usage Question","IO CSV","Needs Info","Closing Candidate"]},{"title":"DOC: Pandas Case_When ","body":"### Pandas version checks\r\n\r\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\r\n\r\n\r\n### Location of the documentation\r\n\r\nhttps:\/\/pandas.pydata.org\/docs\/dev\/reference\/api\/pandas.Series.case_when.html\r\n\r\n### Documentation problem\r\n\r\nThe example provide is not intuitive.\r\n\r\n### Suggested fix for documentation\r\n\r\nI found the example of pd.Series.case_when very non intuitive hence here is the updated example\r\n\r\nExamples\r\n        --------\r\n        >>> # Real-world example: Updating product prices based on a dynamic pricing strategy\r\n        >>> current_prices = pd.Series([50, 75, 100, 120], name='current_prices')\r\n        >>> demand_forecast = pd.Series([100, 80, 50, 120], name='demand_forecast')\r\n\r\n        >>> # Conditions for price adjustments based on demand forecast\r\n        >>> conditions = [\r\n        ...     (demand_forecast > 90, current_prices * 1.2),  # Increase by 20% for high demand\r\n        ...     (demand_forecast < 60, current_prices * 0.8),  # Decrease by 20% for low demand\r\n        ...     (current_prices > 100, current_prices * 0.95)   # Apply a 5% discount for high-priced items\r\n        ... ]\r\n\r\n        >>> # Applying the dynamic pricing strategy\r\n        >>> updated_prices = current_prices.case_when(caselist=conditions)\r\n        >>> updated_prices\r\n        0     60.0\r\n        1     75.0\r\n        2     80.0\r\n        3    114.0\r\n        Name: current_prices, dtype: float64","comments":["Thanks for the suggestion.\r\n\r\n> The example provide is not intuitive.\r\n\r\nCan you expand on this?","I think the minimal example in the docs is good - because it is the simplest case, intended to demonstrate the mechanics as clearly as possible.\r\n\r\nThat said, for me another example - with the application of this method - would be also good to have. But perhaps even better to have - would be a reference to [`np.select`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.select.html#numpy-select) - because I didn't get it at first, but essentially it's like `np.select`, right? - just for a Series (and in-place ?). Can there be a reference to `np.select` in \"See also\", or is linking to methods outside of pandas prohibited?\r\n\r\nSome additional thoughts on improving that page:\r\n\r\n- good: It gives an example where both `a` and `b` compete for the `.gt(0)`, but bad: the order of dominance isn't described explicitly with words (as I understood, the higher replacement (closer to the start of the list) dominates in case of collisions). But since this isn't stated clearly, maybe there is some other logic of dominance..\r\n\r\n- purpose of `, name='c'` isn't clear.. Is this required? If not, why not remove it to have less confusion\r\n\r\n- returns this series edited in place, or a new series?"],"labels":["Docs","Needs Info","Conditionals"]},{"title":"ENH: Add all warnings check to the assert_produces_warnings, and separate messages for each warning.","body":"The function in question gets used in multiple places, that's why I felt it would be safer to preserve current behavior and API, and add an additional, optional parameter, which alters the behavior, enabling checking if all passed warning types get detected.\r\n\r\n- [x] closes #56555 (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["@rhshadrach Could you review this? Or direct me to someone who can? It's my first contribution here, so I don't know who to ask.","I should be able to get to this tomorrow.","@rhshadrach Could you take another look at this, when you have time? I addressed all change requests and resolved all merge conflicts. The docstring checks seem to still be failing, but it looks unrelated to the changes introduced in this PR.","@rhshadrach I just applied a patch that fixed the python 3.9 unit test not passing. With this, this should be ready for review."],"labels":["Testing","Warnings"]},{"title":"ENH: In sort methods, accept sequence or dict as key","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nPretty much every time I use the `key` kwarg of a pandas sort method, I use it to tell pandas the desired order of the unique values. For example:\r\n```python\r\nimport pandas as pd\r\n\r\nmy_index = pd.Index(['one', 'two', 'three', 'three', 'two', 'one', 'three'])\r\nmy_index.sort_values(\r\n    key=lambda idx: (idx\r\n        .to_series()\r\n        .rename(index={'one': 1, 'two': 2, 'three': 3})\r\n        .index\r\n    )\r\n)\r\n```\r\nThis is all a bit redundant, and I suspect it's a common use case.\r\n\r\nRelatedly, sometimes I want to use a different key function for different columns\/levels. In order to do that, I actually need to call the sort method multiple times.\r\n\r\n### Feature Description\r\n\r\nAccept sequences like `['one', 'two', 'three']` specifying the ascending order of the values.\r\n\r\nEDIT: Example equivalent to the `sort_values` call above:\r\n\r\n```python\r\nmy_index.sort_values(key=['one', 'two', 'three'])\r\n```\r\n\r\nAlso accept dictionaries like `{'col_1': ['one', 'two', 'three'], 'col_3': my_sort_fn}` to sort different columns or levels with different keys in any format.\r\n\r\n### Alternative Solutions\r\n\r\nMake sure all data that I want to lexsort is always an ordered Categorical.\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["Thanks for the request. You can accomplish this with map, argsort, and take:\r\n\r\n```\r\nmy_index = pd.Index(['one', 'two', 'three', 'three', 'two', 'one', 'three'])\r\nprint(my_index.take(my_index.map({'one': 1, 'two': 2, 'three': 3}).argsort()))\r\n\r\n# Result:\r\n# Index(['one', 'one', 'two', 'two', 'three', 'three', 'three'], dtype='object')\r\n```"],"labels":["Enhancement","Index","Needs Triage","Sorting"]},{"title":"BUG: Error when trying to use `pd.date_range`","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\nEdit[rhshadrach]: On pandas >= 3.0, you need to use `freq='1000000000YS-JAN'` below.\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\npd.date_range(start = np.datetime64('500000000-01-01', 's'), end = np.datetime64('1500000000-01-01', 's'), freq='1000000000AS-JAN', unit='s')\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nError:\r\n raise ValueError(f\"Offset {offset} did not increment date\")\r\n\r\nValueError: Offset <YearBegin: month=1> did not increment date\r\n\r\n\r\n\r\n### Expected Behavior\r\n\r\nCreate my date range\r\n\r\n### Installed Versions\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.1.0\r\nVersion               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:32:11 PDT 2023; root:xnu-10002.41.9~7\/RELEASE_ARM64_T6030\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n","comments":["Thanks for the report; it's not clear to me if `date_range` should be able to handle values at this scale. Further investigations are welcome.\r\n\r\ncc @MarcoGorelli ","This error came up as part of CI testing of a package. See here: https:\/\/github.com\/LinkedEarth\/Pyleoclim_util\/blob\/bf9716c22329b6bdefc2c9c2a39dc4982289ad5a\/pyleoclim\/tests\/test_core_Series.py#L1274\r\n\r\nThe tests used to work so it was supported in previous versions of Pandas.\r\n\r\nJust to confirm, it works with dates in the ~1500. ","Thanks for adding that, best to always indicate it's a regression! That said, I'm seeing this fail in pandas 2.1.0 and pandas 2.0.0. Prior to that, `pd.date_range` did not take the `unit` argument.\r\n\r\nCan you produce an environment where the example is successful, and report the output of `pd.show_versions()`?","Tried it on our JupyterHub and it worked (although note that this was before `AS` deprecation warning).\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/11758571\/29e55f7e-4e0c-46ea-8f59-99366fb6e09c)\r\n\r\nOutput of `pd.show_versions()`:\r\n\r\n\r\n```INSTALLED VERSIONS\r\n------------------\r\ncommit           : 965ceca9fd796940050d6fc817707bba1c4f9bff\r\npython           : 3.10.11.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.15.120+\r\nVersion          : #1 SMP Fri Jul 21 03:39:30 UTC 2023\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : en_US.UTF-8\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 2.0.2\r\nnumpy            : 1.23.5\r\npytz             : 2023.3\r\ndateutil         : 2.8.2\r\nsetuptools       : 68.0.0\r\npip              : 23.1.2\r\nCython           : 0.29.35\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.9.2\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 3.1.2\r\nIPython          : 8.14.0\r\npandas_datareader: None\r\nbs4              : 4.12.2\r\nbottleneck       : None\r\nbrotli           : \r\nfastparquet      : None\r\nfsspec           : 2023.6.0\r\ngcsfs            : None\r\nmatplotlib       : 3.7.1\r\nnumba            : 0.57.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 12.0.0\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : 2023.6.0\r\nscipy            : 1.10.1\r\nsnappy           : None\r\nsqlalchemy       : 2.0.15\r\ntables           : None\r\ntabulate         : 0.9.0\r\nxarray           : 2023.5.0\r\nxlrd             : 2.0.1\r\nzstandard        : None\r\ntzdata           : 2023.3\r\nqtpy             : 2.3.1\r\npyqt5            : None\r\nNone\r\n```\r\nThe container is available here: https:\/\/quay.io\/repository\/linkedearth\/linkedearthhub?tab=tags\r\n\r\nLatest tag should work. The problem with the GitHub tests is very new. It was passing as of a week ago. Unfortunately, we don't pin versions for CI so I have no idea what was actually installed then. ","Thanks - I've edited the OP to be `AS` for now. I was able to confirm on 2.0.2 but could not run a git-bisect due to the change in build systems. @phofl - I recall you had a solve for this?\r\n\r\nFurther investigations and PRs to fix are welcome!","Inside  `pandas\/core\/arrays\/datetimes.py:_generate_range`, when applying the offset to the current date\r\n\r\n```py\r\nnext_date = offset._apply(cur)\r\n```\r\n\r\nAs `Timestamp cur` is converted to cython object `_Timestamp`, its year is set to 1972. See `pandas\/_libs\/tslibs\/timestamps.pyx:create_timestamp_from_ts`\r\n\r\nThe change originates from https:\/\/github.com\/pandas-dev\/pandas\/pull\/47720. Don't really know where to go next tho. Maybe depends on how we want to handle out-of-bound pydatetime\r\n\r\n","cc @jbrockmendel","Looks like the issue is in shift_month.  Changing `year = stamp.year + dy` to `year = (<object>stamp).year + dy` seems to fix it.  That isn't a great solution though.  Might be preferable to change the declaration on shift_month from datetime to _Timestamp, though i expect that will take more work."],"labels":["Bug","Regression","datetime.date"]},{"title":"BUG: to_numeric loses precision when converting decimal type to integer","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nfrom decimal import *\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'column1': [Decimal('1'*19)]})\r\nprint(pd.to_numeric(df['column1'], downcast='integer'))\r\nprint(df['column1'].astype('int64'))\r\n\r\n\"\"\"\r\n0    1111111111111111168\r\nName: column1, dtype: int64\r\n0    1111111111111111111\r\nName: column1, dtype: int64\r\n\"\"\"\n```\n\n\n### Issue Description\n\nWhen converting data from decimal type to integer using `to_numeric`, we see loss of precision where as doing the same conversion using `astype('int64')` works as expected without any precision loss.\n\n### Expected Behavior\n\nPerform the conversion without a precision loss\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.16.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.24.2\r\npytz                  : 2022.7.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.6.3\r\npip                   : 23.0.1\r\nCython                : 0.29.34\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 5.0.2\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 10.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.2\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Thanks for the report. We likely go through float64. Further investigations and PRs to fix are welcome!","I'll be attempting this.","take"],"labels":["Bug","Dtype Conversions"]},{"title":"BUG: Dataframe's `to_sql` method fails while trying to persist Dataframe to Postgtres in Version 2.2.0","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom sqlalchemy import create_engine\r\n\r\nfrom .settings import get_settings  # pydantic way of reading the env variables\r\n\r\nconn_string = get_settings().db_conn_string\r\ndf = pd.DataFrame(range(5), columns=[\"id\"])\r\ndb = create_engine(conn_string)\r\nconn = db.connect()\r\n\r\nnr_rows = df.to_sql(\r\n    \"temp_pandas_test\", con=conn, if_exists=\"replace\", index=False, chunksize=1000\r\n)\n```\n\n\n### Issue Description\n\nExporting a pandas dataframe to Postgres doesn't work anymore in version 2.2.0. The code above works well with version 2.1.2, but in version 2.2.0 it fails with the following error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/robert-andreidamian\/Workspace\/panprices\/shelf_analytics_update_products\/demo.py\", line 14, in <module>\r\n    nr_rows = df.to_sql(\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/util\/_decorators.py\", line 333, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/core\/generic.py\", line 3059, in to_sql\r\n    return sql.to_sql(\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 858, in to_sql\r\n    return pandas_sql.to_sql(\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 2867, in to_sql\r\n    table.create()\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 1000, in create\r\n    if self.exists():\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 986, in exists\r\n    return self.pd_sql.has_table(self.name, self.schema)\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 2882, in has_table\r\n    return len(self.execute(query, [name]).fetchall()) > 0\r\n  File \"\/Users\/robert-andreidamian\/.local\/share\/virtualenvs\/shelf_analytics_update_products-BwnpeEoI\/lib\/python3.9\/site-packages\/pandas\/io\/sql.py\", line 2689, in execute\r\n    cur = self.con.cursor()\r\nAttributeError: 'Connection' object has no attribute 'cursor'\r\n```\r\n\r\nI noticed that [a new driver](https:\/\/pandas.pydata.org\/docs\/whatsnew\/v2.2.0.html#adbc-driver-support-in-to-sql-and-read-sql) was added in Version 2.2.0, which might be related to my issue. \n\n### Expected Behavior\n\nThe code above should execute the same way it used to do with the previous version, and export the dataframe to a table in my Postgres Database. \n\n### Installed Versions\n\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : c3014abb3bf2e15fa66d194ebd2867161527c497\r\npython                : 3.9.13.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.1.0\r\nVersion               : Darwin Kernel Version 23.1.0: Mon Oct  9 21:32:52 PDT 2023; root:xnu-10002.41.9~7\/RELEASE_ARM64_T8122\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_GB.UTF-8\r\npandas                : 3.0.0.dev0+210.gc3014abb3b\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None","comments":["cc @WillAyd related to your changes?","maybe related #57049 and #57178","Same error here with an **sqlite** database (I get the error with pandas 2.2.0 but not with 2.1.4)"],"labels":["Bug","IO SQL","Needs Triage"]},{"title":"BUG: fix bug in DataFrame.diff when dataframe has bool dtype and axis=1","body":"supersede #53248\r\n\r\n- [x] closes #52579 (Replace xxxx with the GitHub issue number)\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["@jbrockmendel @mroeschke gentle ping for review."],"labels":["Bug","DataFrame","Transformations"]},{"title":"BUG: pandas 2.2.0 `DataFrame.groupby` regression when group labels of type `pd.Timestamp`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nassert pd.__version__ == \"2.2.0\"\r\n\r\nindex = pd.date_range(\"2024-02-01\", \"2024-02-03\", freq=\"8h\", tz=\"UTC\")\r\n\r\n# index\r\n# DatetimeIndex(['2024-02-01 00:00:00+00:00', '2024-02-01 08:00:00+00:00',\r\n#                '2024-02-01 16:00:00+00:00', '2024-02-02 00:00:00+00:00',\r\n#                '2024-02-02 08:00:00+00:00', '2024-02-02 16:00:00+00:00',\r\n#                '2024-02-03 00:00:00+00:00'],\r\n#               dtype='datetime64[ns, UTC]', freq='8h')\r\n\r\ndf = pd.DataFrame([1] * len(index), index)\r\n\r\ndef f(ts: pd.Timestamp) -> pd.Timestamp:\r\n    \"\"\"Group by date, i.e. as timezone naive timestamp.\"\"\"\r\n    return ts.normalize().tz_convert(None)\r\n\r\ngrouped = df.groupby(by=f)\r\nkeys = list(grouped.groups.keys())\r\nprint(keys)\r\n\r\n# [Timestamp('2024-02-01 00:00:00+0000', tz='UTC'),\r\n#  Timestamp('2024-02-02 00:00:00+0000', tz='UTC'),\r\n#  Timestamp('2024-02-03 00:00:00+0000', tz='UTC')]\n```\n\n\n### Issue Description\n\nAlthough the return from the 'by' function is timezone naive, the group labels are timezone aware (UTC).\r\n\r\n(I've tried every combination of the `as_index` and `group_keys` options, always get the same group keys).\n\n### Expected Behavior\n\nWould have expected behaviour as pandas 2.1.4 and prior, i.e,. group keys as returned from the 'by' function...\r\n\r\n```python\r\nimport pandas as pd\r\nassert pd.__version__ == \"2.1.4\"\r\n\r\nindex = pd.date_range(\"2024-02-01\", \"2024-02-03\", freq=\"8h\", tz=\"UTC\")\r\ndf = pd.DataFrame([1] * len(index), index)\r\n\r\ndef f(ts: pd.Timestamp) -> pd.Timestamp:\r\n    return ts.normalize().tz_convert(None)\r\n\r\ngrouped = df.groupby(by=f)\r\nkeys = list(grouped.groups.keys())\r\nprint(keys)\r\n```\r\n```\r\n[Timestamp('2024-02-01 00:00:00'),\r\n Timestamp('2024-02-02 00:00:00'),\r\n Timestamp('2024-02-03 00:00:00')]\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.9.13.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.22631\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 141 Stepping 1, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United Kingdom.1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 58.1.0\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : 8.0.0\r\nhypothesis            : 6.97.3\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : 3.9.2\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Result of a git bisect:\r\n\r\n```\r\ncommit 746e5eee860b6e143c33c9b985e095dac2e42677\r\nAuthor: jbrockmendel\r\nDate:   Mon Oct 16 11:50:28 2023 -0700\r\n\r\n    ENH: EA._from_scalars (#53089)\r\n```\r\n\r\ncc @jbrockmendel ","yah i think thats a problem in DTA._from_scalars.  _from_scalars itself was a mistake xref #56430","This can be reproduced just using `index.map`, so taking off the groupby label. "],"labels":["Bug","Regression","Timezones","Index"]},{"title":"ENH: deprecate reading from clipboard only supports utf-8 encoding","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nCode following will raise `NotImplementedError: reading from clipboard only supports utf-8 encoding`\r\n\r\n```python\r\ndf = pd.read_clipboard(encoding=\"gbk\")\r\n```\r\n\r\nBUT, if user do it with:\r\n\r\n```python\r\npd.io.clipboard.ENCODING = \"gbk\"\r\ndf = pd.read_clipboard()\r\npd.io.clipboard.ENCODING = \"utf8\"\r\n```\r\n\r\nIt works well.\r\n\r\n### Feature Description\r\n\r\nI am not sure why `encoding` in pandas is fixed to 'utf8', but in wsl2 user's windows system default encoding is usually not Unicode and very hard to migrate this settings.\r\n\r\nAllow user change encoding of `read_clipboard` will be very helpful.\r\n\r\n### Alternative Solutions\r\n\r\n```python\r\npd.io.clipboard.ENCODING = \"gbk\"\r\ndf = pd.read_clipboard()\r\npd.io.clipboard.ENCODING = \"utf8\"\r\n```\r\n\r\n### Additional Context\r\n\r\ndata for testting: \r\n\r\n\u8fd9 | 1\r\n-- | --\r\n\u662f | 2\r\n\u4e00 | 3\r\n\u4e2a | 4\r\n\u8868 | 5\r\n\r\n\r\n```\r\n# stdout[:-2]\r\ndata = b'\\xd5\\xe2\\t1\\r\\n\\xca\\xc7\\t2\\r\\n\\xd2\\xbb\\t3\\r\\n\\xb8\\xf6\\t4\\r\\n\\xb1\\xed\\t5\\r\\n'\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":["Thanks for the request. pandas implements this by vendoring [pyperclip](https:\/\/github.com\/asweigart\/pyperclip). It does not appear to me that pyperclip (explicitly) supports anything but UTF-8, I would be hesistant to support other encodings unless pyperclip supports them or there is extensive testing."],"labels":["Enhancement","IO Data","Needs Discussion"]},{"title":"BUG: Series.map() coerces Int64Dtype and int64[pyarrow] series which contain missing values to float64","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nser1 = pd.Series([1,2,3,None,10], dtype=pd.Int64Dtype())\r\nser2 = pd.Series([1,2,3,pd.NA,10], dtype=pd.Int64Dtype())\r\nser3 = pd.Series([1,2,3,pd.NA,10], dtype=pd.ArrowDtype(pa.int64()))\r\n\r\nfor ser in ser1, ser2, ser3:\r\n    print(f\"initial dtype: {ser.dtype}\")\r\n    ser_mapped = ser.map(lambda x: x)\r\n    print(f\"post map dtype: {ser_mapped.dtype}\")\r\n    ser_filtered = ser.dropna().map(lambda x: x)\r\n    print(f\"dropna map dtype: {ser_mapped.dtype}\")\r\n\r\nser2.map(type)\r\nser2.dropna().map(type)\r\n\r\n\"\"\"\r\noutput:\r\n\r\nfor ser1:\r\ninitial dtype: Int64\r\npost map dtype: float64\r\ndropna map dtype: float64\r\n\r\nfor ser2:\r\ninitial dtype: Int64\r\npost map dtype: float64\r\ndropna map dtype: float64\r\n\r\nfor ser3:\r\ninitial dtype: int64[pyarrow]\r\npost map dtype: float64\r\ndropna map dtype: float64\r\n\r\n\r\n\"\"\"\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nUsing map on a Series with dtype Int64Dtype or int64[dtype] will coerce values to float if it contains any missing values.\r\n\r\n```\r\n>>> ser2.map(type)\r\n0    <class 'float'>\r\n1    <class 'float'>\r\n2    <class 'float'>\r\n3    <class 'float'>\r\n4    <class 'float'>\r\ndtype: object\r\n>>> ser2.dropna().map(type)\r\n0    <class 'int'>\r\n1    <class 'int'>\r\n2    <class 'int'>\r\n4    <class 'int'>\r\ndtype: object\r\n```\r\n\r\n### Expected Behavior\r\nSeries.map() should not coerce into float64 with these dtypes.\r\n\r\nAs stated in the documentation on [working with missing data](https:\/\/pandas.pydata.org\/docs\/user_guide\/missing_data.html#values-considered-missing):\r\n\r\n> [NA](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.NA.html#pandas.NA) for [StringDtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.StringDtype.html#pandas.StringDtype), [Int64Dtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Int64Dtype.html#pandas.Int64Dtype) (and other bit widths), Float64Dtype`(and other bit widths), :class:`BooleanDtype and [ArrowDtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.ArrowDtype.html#pandas.ArrowDtype). **These types will maintain the original data type of the data.**\r\n\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n>>> pd.show_versions()\r\n\/usr\/lib\/python3.12\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : db11e25d2b1175fdf85d963a88ff5a1d4bdb6fd8\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.6.13-200.fc39.x86_64\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Sat Jan 20 18:03:28 UTC 2024\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_IE.UTF-8\r\nLOCALE                : en_IE.UTF-8\r\n\r\npandas                : 3.0.0.dev0+197.gdb11e25d2b\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.7.2\r\npip                   : 23.2.1\r\nCython                : None\r\npytest                : 7.4.3\r\nhypothesis            : None\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 5.0.0\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.19.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":["take","https:\/\/github.com\/pandas-dev\/pandas\/issues\/56606\r\n\r\nI had the same issue, based on the issue above, it seemed to be intentional change in 2.2.X","@rohanjain101  Yeah you're right. I believe it changed from v2.1.x\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\/pandas\/core\/arrays\/base.py#L2188\r\n\r\nto \r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/c3014abb3bf2e15fa66d194ebd2867161527c497\/pandas\/core\/arrays\/masked.py#L1332-L1333\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/c3014abb3bf2e15fa66d194ebd2867161527c497\/pandas\/core\/arrays\/arrow\/array.py#L1426-L1430\r\n \r\nin v2.2.0. Guess you might have to cast it back to your wanted dtype @weltenseglr ","Thanks for your feedback, @rohanjain101 @remiBoudreau. I think the documentation is outdated then?\r\n[Series.map()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.map.html#pandas.Series.map) does not warn about this matter and  [missing data](https:\/\/pandas.pydata.org\/docs\/user_guide\/missing_data.html#values-considered-missing) states that\r\n> [NA](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.NA.html#pandas.NA) for [StringDtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.StringDtype.html#pandas.StringDtype), [Int64Dtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Int64Dtype.html#pandas.Int64Dtype) (and other bit widths), Float64Dtype`(and other bit widths), :class:`BooleanDtype and [ArrowDtype](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.ArrowDtype.html#pandas.ArrowDtype). **These types will maintain the original data type of the data.**\r\n\r\nCasting back won't help in my case, as my mapper must not receive coerced values...\r\n\r\nI'm trying to figure out how to maintain the original data type. Any ideas?\r\n\r\n\r\n\r\n","@weltenseglr You could use the workaround described in https:\/\/github.com\/pandas-dev\/pandas\/issues\/56606#issuecomment-1870453754\r\n\r\nUsing the python map operator instead of Series.map. Atleast for now, this preserves the original type.","thanks, @rohanjain101.\r\n\r\nI guess I will have to implement a workaround based on your suggestion or revert to a 2.1 release.\r\nHowever, it still feels wrong that pandas' map casts nullable numerical dtypes to float64.\r\n\r\nIs this going to change in pandas 3.0? I think this doesn't play well with the project's intentions regarding PDEP-10 and its benefits?"],"labels":["Bug","NA - MaskedArrays","Arrow"]},{"title":"ENH: align timedelta fractional seconds","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nIt's hard to read a column of timedeltas with fractional seconds of different precisions because it's not aligned, for example:\r\n```\r\n0             0 days 00:00:01\r\n1      0 days 00:00:00.500000\r\n2   0 days 00:00:00.333333333\r\ndtype: timedelta64[ns]\r\n```\r\n\r\nIt'd be easier to read like this:\r\n```\r\n0   0 days 00:00:01\r\n1   0 days 00:00:00.500000\r\n2   0 days 00:00:00.333333333\r\ndtype: timedelta64[ns]\r\n```\r\nor\r\n```\r\n0   0 days 00:00:01.000000000\r\n1   0 days 00:00:00.500000000\r\n2   0 days 00:00:00.333333333\r\ndtype: timedelta64[ns]\r\n```\r\n\r\nTo reproduce:\r\n```python\r\ns = 1 \/ pd.Series(np.arange(3)+1)\r\ntd = pd.to_timedelta(s, unit='s')\r\ntd\r\n```\r\n\r\n### Feature Description\r\n\r\nAdd padding on the right.\r\n\r\nIn this case, it can be done by left-justifying to the largest size:\r\n```python\r\nn = td.astype(str).apply(len).max()\r\ntd.astype(str).str.ljust(n)\r\n```\r\n```\r\n0    0 days 00:00:01          \r\n1    0 days 00:00:00.500000   \r\n2    0 days 00:00:00.333333333\r\ndtype: object\r\n```\r\nBut of course, this wouldn't work for different numbers of days. For example:\r\n```python\r\ntd1 = pd.concat([td, pd.Series(pd.Timedelta(days=4000))])\r\n```\r\n```\r\n0    0 days 00:00:01          \r\n1    0 days 00:00:00.500000   \r\n2    0 days 00:00:00.333333333\r\n0    4000 days 00:00:00       \r\ndtype: object\r\n```\r\nIf timedeltas had a `.replace()` method so you could zero out the days, then it would work, but AFAIK such a method doesn't exist.\r\n\r\nBTW, the same thing is already implemented for floats and datetimes.\r\n```python\r\n>>> s*10\r\n0    10.000000\r\n1     5.000000\r\n2     3.333333\r\ndtype: float64\r\n>>> pd.Timestamp(0) + td\r\n0   1970-01-01 00:00:01.000000000\r\n1   1970-01-01 00:00:00.500000000\r\n2   1970-01-01 00:00:00.333333333\r\ndtype: datetime64[ns]\r\n```\r\n\r\n### Alternative Solutions\r\n\r\nn\/a\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["Thanks for the report - agreed this would be an improvement. PRs to fix are welcome.","> PRs to fix are welcome.\r\n\r\nWorking on one :) but this functionality is buried in some Cython code (`_Timedelta._repr_base`), which I've never written before, so it's a bit of a slog."],"labels":["Enhancement","Output-Formatting","Timedelta"]},{"title":"Migrate pylint to ruff","body":"Hi @mroeschke, while looking into the `pylint` rule, I noticed that most of them are currently gated behind the `preview` flag. Do you think we should enable the `preview` flag or wait some rules to be promoted to stable in `ruff==0.2.0` (should be released in the next few weeks)\r\n\r\nOther than that, this PR is ready for review.\r\n\r\n- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Is it possible to opt in to preview mode with just pylint?","@mroeschke can you take a look at the latest commit and the 177 errors that it detects? I think we can achieve that with `pre-commit`, although we need to explicitly pass all necessary.\r\n\r\nThis new hook replaces the one below, which is disabled by default\r\n\r\n```\r\n-   repo: https:\/\/github.com\/pylint-dev\/pylint\r\n    rev: v3.0.1\r\n    hooks:\r\n    -   id: pylint\r\n        stages: [manual]\r\n        args: [--load-plugins=pylint.extensions.redefined_loop_name]\r\n```\r\n\r\nGiven that I can't fix all the errors within 1 PR, let me know how should I proceed","My main curiosity is how many of the 177 errors are ruff-pylint checks that are not enabled in the current pylint checks? https:\/\/github.com\/pandas-dev\/pandas\/blob\/cad0d87c63a914ebc6e33fb1b5e22c9068d75710\/pyproject.toml#L369\r\n\r\nIdeally this PR should just bump ruff and enable ruff-pylint, and all modifications are updates from other rules.\r\n\r\nThen in a follow up PR, the old pylint can be removed (and optional ruff-pylint rules can be enabled too)","hi @mroeschke, please take a look at the last commit, where I manage to make an automatic translation of `pylint` config to `ruff` config with the help of [`pylint-to-ruff`](https:\/\/github.com\/akx\/pylint-to-ruff).\r\n\r\nGiven the list of `pylint` rules that has not been implemented in `ruff` (the one that's commented out), let me know if it's ok to drop `pylint` in favor of `ruff`","@mroeschke thanks for your review. I rebased on `main`","hi @mroeschke, I wonder if we should split the version bump of `ruff` with the switch from `pylint` to `ruff` so that we can use the latest `ruff` version first. \r\n\r\nI can also take a look at the rather messy bump from `0.2.0` to `0.3.0` if it's good for you","Yes I think that's a good idea to bump ruff to 0.3.0 first before doing a pylint migration"],"labels":["Code Style"]},{"title":"ENH: Not silently ignore violation of minimal required version of SQLAlchemy","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\n#### Observation\r\nAfter upgrading to Pandas 2.2.0 calling `read_sql_query` with a SQLAlchemy query on an SQLAlchemy connection yielded `\"Query must be a string unless using sqlalchemy.\"` without any indication why or what may have broken with the upgrade.\r\n\r\n#### Analysis\r\nTracking the problem down it was found that `pandasSQL_builder` [(code)](https:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/io\/sql.py#L900) calls `sqlalchemy = import_optional_dependency(\"sqlalchemy\", errors=\"ignore\")` to resolve the optional SQLAlchemy package (which was installed in my case)\r\n\r\nDue to `errors=\"ignore\"` the violation of the minimal required version for SQLAlchemy does not lead to a warn or raise [(code)](https:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/compat\/_optional.py#L156) but just silently returns with `None`.\r\n\r\nThis in turn lets `pandasSQL_builder` silently default to `SQLiteDatabase(con)`.\r\n\r\n### Feature Description\r\n\r\n#### Proposed improvement\r\n\r\nDo not ignore the minimal version violation in `import_optional_dependency` in this context but make it obvious. For example by introducing an additional \"errors\"-mode like `import_optional_dependency(\"sqlalchemy\", errors=\"raise-on-version-violation\")`.\r\n\r\n### Alternative Solutions\r\n\r\n.\/.\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["Thanks for the report! It looks like this was changed in #45679 to allow for other dependencies. Perhaps the logic could be changed there so that when SQLAlchemy is used, we do check that the version is satisfied.\r\n\r\nFurther investigations and PRs to fix are welcome!","I have attempted to recreate the issue in a fresh environment  with `pandas==2.2.0; sqlalchemy==2.0.25`, and the output seems to be correct.  I may require assistance in reproducing the issue.\r\n\r\n```python\r\nfrom sqlalchemy import create_engine\r\nimport pandas as pd\r\n\r\nengine = create_engine(\"sqlite:\/\/\/:memory:\")\r\n\r\ndf = pd.DataFrame({\r\n    \"id\": [1, 2, 3],\r\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"]\r\n})\r\ndf.to_sql(\"Person\", engine, index=False)\r\n\r\nsql_query = \"SELECT * FROM Person\"\r\nresult = pd.read_sql_query(sql_query, engine)\r\nprint(result)\r\n```\r\n```\r\n   id     name\r\n0   1    Alice\r\n1   2      Bob\r\n2   3  Charlie\r\n```\r\n","The issue is not that it works with SQLAlchemy 2 but that it gives no clue why it does not work with SQLAlchemy < 2.x\r\n\r\nTo replicate install SQLAlchemy==1.4 and rerun the code-snippet from above. This results in..\r\n\r\n```\r\n$ python pandas_test.py\r\nD:\\temp\\pandas-issue-57178\\pandas_test.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine\/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\r\n  df.to_sql(\"Person\", engine, index=False)\r\nTraceback (most recent call last):\r\n  File \"D:\\temp\\pandas-issue-57178\\pandas_test.py\", line 10, in <module>\r\n    df.to_sql(\"Person\", engine, index=False)\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 333, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\core\\generic.py\", line 3081, in to_sql\r\n    return sql.to_sql(\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 842, in to_sql\r\n    return pandas_sql.to_sql(\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 2851, in to_sql\r\n    table.create()\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 984, in create\r\n    if self.exists():\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 970, in exists\r\n    return self.pd_sql.has_table(self.name, self.schema)\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 2866, in has_table\r\n    return len(self.execute(query, [name]).fetchall()) > 0\r\n  File \"D:\\temp\\pandas-issue-57178\\venv\\lib\\site-packages\\pandas\\io\\sql.py\", line 2673, in execute\r\n    cur = self.con.cursor()\r\nAttributeError: 'Engine' object has no attribute 'cursor'\r\n\r\n```\r\n\r\nwhich is surprising given that we just created the engine with SQLAlchemy. And it gives the user no clue that this is due to violation of version requirement.\r\n\r\nIt would be obvious what is the issue if the version check in `import_optional_dependency` raises the\r\n\r\n```\r\n            msg = (\r\n                f\"Pandas requires version '{minimum_version}' or newer of '{parent}' \"\r\n                f\"(version '{version}' currently installed).\"\r\n            )\r\n```\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/f538741432edf55c6b9fb5d0d496d2dd1d7c2457\/pandas\/compat\/_optional.py#L152\r\n\r\n... instead of just silently returning None (which only later leads to the miss-leading error from above.)\r\n","Seems related #57049, whether to support Sqlalchemy 1.4 is under discussion.","I think this is related. It should state that sqlalchemy <2.0 is not supported instead of \"not installed\".\r\n\r\n```python\r\nimport sqlalchemy\r\nprint(\"sqlalchemy Version:\", sqlalchemy.__version__)\r\npd.read_sql(\"select * from osm_point limit 1\", connection_string)\r\n```\r\n```sh\r\nsqlalchemy Version: 1.4.52\r\n...\r\nImportError: Using URI string without sqlalchemy installed.\r\n```"],"labels":["Enhancement","IO SQL","Dependencies"]},{"title":"ENH: Expose integer formatting also for default display, not only styler","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nI noticed that in Pandas version `2.2` the internal implementation of  `IntArrayFormatter` was changed (or at least renamed), marking the classes as private using one underscore. I used the implementation as described [here](https:\/\/stackoverflow.com\/a\/29663750\/9684872) to format integers in a DataFrame on each display.\r\n\r\nSee changes between `2.1` and `2.2` to make the linked patch work: [link](https:\/\/github.com\/RasmussenLab\/njab\/pull\/4\/commits\/10b0c84ddb28a1dd9c2bd58e9b6af148dbd6c9dc)\r\n\r\nI see that the styler supports formatting thousands:\r\n\r\n```python\r\nimport pandas as pd\r\npd.options.styler.format.thousands = ','\r\ns = pd.Series([1_000_000]).to_frame()\r\ns.style # does change display of s to 1,000,000\r\n```\r\nhowever\r\n```\r\ns # will display 1000000\r\n```\r\nWould it be possible to add the same to the display options?\r\n\r\n### Feature Description\r\n\r\npd.options.format.thousands = ','\r\n\r\nwould need to pass:\r\n\r\n```python\r\nif pd.options.format.thousands:\r\n  format_str = f':{pd.options.format.thousands}d'.format\r\nelse:\r\n  format_str = ':d'.format\r\n```\r\n\r\n\r\n\r\n### Alternative Solutions\r\n\r\nChange the default [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/io\/formats\/format.py#L1475)\r\n\r\n```python\r\nclass IntArrayFormatter(pf._GenericArrayFormatter):\r\n\r\n    def _format_strings(self):\r\n        formatter = self.formatter or '{:,d}'.format\r\n        fmt_values = [formatter(x) for x in self.values]\r\n        return fmt_values\r\n\r\npf._IntArrayFormatter = IntArrayFormatter\r\n```\r\n\r\nUsing the above patch I get the display I would prefer:\r\n\r\n```\r\n>>> pd.Series([1_000_000])\r\n0    1,000,000\r\ndtype: int64\r\n```\r\n\r\n### Additional Context\r\n\r\nWorking with large integers in tables is made easier if thousands are visually distinguished. Maybe there is already a way to specify that formatter function (`str.format`) for display, which would be perfect.","comments":["Thanks for the request. From your example:\r\n\r\n> s.style # does not change display of s\r\n\r\nI'm seeing that this does display the comma separator, are you not? Or perhaps you meant that `s` without `.style` still does not show the commas?","i dont think this feature is about Styler. It is about adding functionality to the default DataFrame display that is demonstrated by Styler, i.e. which Styler already possesses.","Yes exactly, currently I have to use the internals to make `s` already represent with the commas. \r\n\r\n```python\r\n>>> import pandas as pd\r\n>>> pd.options.styler.format.thousands = ','  # here it would be great to have the option pd.options.display.thousands \r\n>>> s = pd.Series([1_000_000])\r\n\r\n>>> s\r\n0    1000000\r\ndtype: int64\r\n>>>s.to_frame().style\r\n         0\r\n0\t1,000,000\r\n```\r\n\r\nI see that I put a \"not\" at the wrong place intially... Sorry about that. I'll clarify my example above by modifying it.","I would personally find this feature convenient, I'm positive on it. @attack68 do you have any thoughts?\r\n\r\nIf we do implement this, I think it should only be implemented for integer\/float dtypes, and documented as such. Users should not expect pandas to infer whether to insert commas in object dtype values.","Hello @enryH,\r\n\r\nMy project team is looking for Pandas enhancement features for our grad school semester long project. We saw this task and would like to contribute if possible!","i think its generally quite handy to have configurable options for precision, thousands and decimal separator. Styler avoided using the locale package, if I recall correctly, but it was a bit tricky in the end to make sure the european decimal-comma was compatible with the us comma-decimal style. \r\n\r\nTo make it easier this should definitely only be applied to float\/int dtypes.","i think its generally quite handy to have configurable options for precision, thousands and decimal separator. Styler avoided using the locale package, if I recall correctly, but it was a bit tricky in the end to make sure the european decimal-comma was compatible with the us comma-decimal style. \r\n\r\nTo make it easier this should definitely only be applied to float\/int dtypes.","take","I guess it will boil down to figuring out how the formatting function is set (see `IntArrayFormatter` [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/io\/formats\/format.py#L1475)):\r\n\r\n```python\r\nformatter = self.formatter or '{:d}'.format  # how to set self.formatter from pandas.options\r\n```","I ran into the same problem too (down to relying on the same StackOverflow code to modify display of integers). I also would like a cleaner way to do it.\r\n\r\nAdditionally, what is the recommended way to to handle the name change? I have some tools that need to support both `pd.io.formats.format.IntArrayFormatter` and `pd.io.formats.format._IntArrayFormatter`. Thank you.","> I ran into the same problem too (down to relying on the same StackOverflow code to modify display of integers). I also would like a cleaner way to do it.\r\n\r\nPRs welcome!\r\n\r\n> Additionally, what is the recommended way to to handle the name change? I have some tools that need to support both `pd.io.formats.format.IntArrayFormatter` and `pd.io.formats.format._IntArrayFormatter`. Thank you.\r\n\r\nIntArrayFormatter is not intended to be public. If you have a need to use it, can you open a new issue detailing it.\r\n\r\n","Hey @rhshadrach is the goal to create a new class or to improve the _IntArrayFormatter class and change it back to public? \r\n\r\nJust wanted to understand what was considered in scope before tackling this issue. \r\n\r\nIf this is not the right forum\/area to ask questions, please let know!","> IntArrayFormatter is not intended to be public. If you have a need to use it, can you open a new issue detailing it.\r\n\r\nMaybe it's easier to talk about what I am trying to do in order to avoid an XY problem. What I need is a way to automatically always show commas in integers. As `IntArrayFormatter` is not intended to be public, I'd be happy to use any different method to accomplish the same goal.\r\n","Thanks @saiwing-yeung - we're open for PRs to add an option (e.g. `pd.set_option(\"display.numeric_separator\")` - could probably use a better name!). It would default to `''`, but users could change to e.g. `'_'` or `','`.\r\n\r\n@Meadiocre \r\n\r\n> is the goal to create a new class or to improve the _IntArrayFormatter class and change it back to public?\r\n\r\nNo - just expose a new option for users."],"labels":["Enhancement","Output-Formatting"]},{"title":"BUG: does .any(axis=1, skipna=False) follow Kleene Logic?","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nIn [16]: pd.DataFrame({\r\n    ...:     'a': [True, False, None],\r\n    ...:     'b': [None, None, None],\r\n    ...: }, dtype='boolean[pyarrow]').any(axis=1, skipna=False)\r\nOut[16]:\r\n0     True  #\r\n1    False  # <=== should be NA?\r\n2    False  # <=== should be NA?\r\ndtype: bool[pyarrow]\r\n\r\n\r\nIn [17]: pd.Series([False, None], dtype='boolean[pyarrow]').any(skipna=False)\r\nOut[17]: <NA>\r\n\r\nIn [18]: pd.Series([None, None], dtype='boolean[pyarrow]').any(skipna=False)\r\nOut[18]: <NA>\r\n\r\nIn [19]: pd.Series([True, None], dtype='boolean[pyarrow]').any(skipna=False)\r\nOut[19]: True\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nI don't really understand what's happening, nor what's meant to be happening\r\n\r\n[The docs for `Series.any`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.any.html#pandas.Series.any) say\r\n\r\n> If skipna is False, then NA are treated as True, because these are not equal to zero.\r\n\r\nbut that doesn't seem to be the case. If that was the case, then\r\n```python\r\npd.Series([False, None], dtype='boolean[pyarrow]').any(skipna=False)\r\n```\r\nwould return `True`?\r\n\r\nInstead, it looks like `Series.any` follows Kleene Logic. Fair nuff - but then, `DataFrame.any(axis=1, skipna=False)` doesn't?\r\n\r\n### Expected Behavior\r\n\r\nI think I'd expect\r\n```python\r\n0     True\r\n1     <NA>\r\n2     <NA>\r\n```\r\nfrom the `DataFrame.any(axis=1, skipna=False)` example\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.12.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.133.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_GB.UTF-8\r\nLOCALE                : en_GB.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 59.6.0\r\npip                   : 22.0.2\r\nCython                : None\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.19.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : 2023.12.2post1\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.25\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["pretty sure that axis=1 casts to object, thus opting out of arrow dtypes and falling back to the default numpy behavior","~Not certain there is a cast to object~, we go through _reduce_axis1 here:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/c3014abb3bf2e15fa66d194ebd2867161527c497\/pandas\/core\/frame.py#L11471\r\n\r\nBut we're using NumPy and\/or ufuncs.\r\n\r\nEdit: Doh, of course that has to cast to NumPy object dtype to work."],"labels":["Bug","Reduction Operations"]},{"title":"BUG: DateOffset does not work with date32[pyarrow] datetypess","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport datetime as dt\r\n\r\ns = pd.Series([dt.date(2022, 12, 30)], dtype=\"date32[pyarrow]\")\r\n_ = s + pd.offsets.MonthEnd()\n```\n\n\n### Issue Description\n\nI receive the following error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/ops\/common.py\", line 76, in new_method\r\n    return method(self, other)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arraylike.py\", line 186, in __add__\r\n    return self._arith_method(other, operator.add)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/series.py\", line 6130, in _arith_method\r\n    return base.IndexOpsMixin._arith_method(self, other, op)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/base.py\", line 1380, in _arith_method\r\n    result = ops.arithmetic_op(lvalues, rvalues, op)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/ops\/array_ops.py\", line 273, in arithmetic_op\r\n    res_values = op(left, right)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/ops\/common.py\", line 76, in new_method\r\n    return method(self, other)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arraylike.py\", line 186, in __add__\r\n    return self._arith_method(other, operator.add)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arrays\/arrow\/array.py\", line 785, in _arith_method\r\n    return self._evaluate_op_method(other, op, ARROW_ARITHMETIC_FUNCS)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arrays\/arrow\/array.py\", line 727, in _evaluate_op_method\r\n    other = self._box_pa(other)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arrays\/arrow\/array.py\", line 408, in _box_pa\r\n    return cls._box_pa_scalar(value, pa_type)\r\n  File \"\/home\/myUser\/Experiments\/pandas\/pandas_venv\/lib64\/python3.9\/site-packages\/pandas\/core\/arrays\/arrow\/array.py\", line 444, in _box_pa_scalar\r\n    pa_scalar = pa.scalar(value, type=pa_type, from_pandas=True)\r\n  File \"pyarrow\/scalar.pxi\", line 1150, in pyarrow.lib.scalar\r\n  File \"pyarrow\/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowTypeError: No temporal attributes found on object.\r\n```\n\n### Expected Behavior\n\nSince it is a date-datatype, the DateOffset should be applied as for TimeStamps, i.e. the result should be\r\n`pd.Series( [dt.date(2022,12,31)], dtype=\"date32[pyarrow]\")`.\r\n\r\n\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : db11e25d2b1175fdf85d963a88ff5a1d4bdb6fd8\r\npython                : 3.9.18.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 4.18.0-513.9.1.el8_9.x86_64\r\nVersion               : #1 SMP Thu Nov 16 10:29:04 EST 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+197.gdb11e25d2b\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.4\r\ndateutil              : 2.8.2\r\nsetuptools            : 50.3.2\r\npip                   : 20.2.4\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["@jbrockmendel - is there planned support for arrow dtypes here?","I have no plans to implement it, but no objection if someone else wants to","Thanks for opening this Issue. The exact same problem has been bothering me for months and I never got around to report it. Yes --- it would be great if arrow types are not second-class citizens when it comes to a useful feature like offsets."],"labels":["Enhancement","Frequency","Arrow"]},{"title":"DEPR: groupby.corrwith","body":"As far as I can tell, [corrwith was added](https:\/\/github.com\/pandas-dev\/pandas\/pull\/5604) simply because it could be along with other methods on DataFrame. [Searching SO](https:\/\/stackoverflow.com\/search?q=%5Bpandas%5D+groupby+%22corrwith%22) gives two results, of which `corrwith` is not involved in the solution for both. I'm guessing this method really doesn't see much if any use.\r\n\r\nIt's signature is odd - it pairs each of the groups up with a single DataFrame and aligns them.\r\n\r\n```\r\ndf = pd.DataFrame({\"a\": [1, 1, 1, 2, 2, 2], \"b\": range(6)})\r\ndf2 = pd.DataFrame({\"a\": [1, 1, 1, 2, 2, 2, 3], \"b\": [10 - e for e in range(3)] + list(range(3)) + [10]})\r\ngb = df.groupby(\"a\")\r\nprint(gb.corrwith(df2))\r\n#      b   a\r\n# a         \r\n# 1 -1.0 NaN\r\n# 2  1.0 NaN\r\n```\r\n\r\nI'm thinking we should deprecate this method.","comments":["+1 for deprecation of the method."],"labels":["Groupby","Deprecate","Needs Discussion","Reduction Operations"]},{"title":"Update _get_dst_hours Cython annotation when numpy 2.0 is the minium version","body":"Essentially uncomment the line added in https:\/\/github.com\/pandas-dev\/pandas\/pull\/57144","comments":[],"labels":["Internals","Compat"]},{"title":"BUG: df.stack() returns wrong data when NaT is in index (regression since 2.1.0, ok in <= 2.0.3)","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    data=[[1, 2, 3]],\r\n    columns=pd.MultiIndex.from_tuples(\r\n        [\r\n            (\"MAT\", pd.Timestamp(\"2021-12-01\"), \"a\"),\r\n            (\"ignore\", pd.Timestamp(\"1970-12-01\"), \"a\"),\r\n            (\"ignore\", pd.NaT, \"a\"),\r\n        ],\r\n        names=(\"date_type\", \"date\", \"value_type\"),\r\n    ),\r\n)\r\n\r\nunique_dates_v1 = df.columns.get_level_values(\"date\")[\r\n    df.columns.get_level_values(\"date_type\") == \"MAT\"\r\n].unique()\r\n\r\nunique_dates_via_stack = (\r\n    df.stack(df.columns.names)\r\n    .xs(\"MAT\", level=\"date_type\")\r\n    .index.get_level_values(\"date\")\r\n    .unique()\r\n)\r\n\r\nprint(pd.__version__)\r\nprint(\"v1\", unique_dates_v1)\r\nprint(\"v2\", unique_dates_via_stack)\r\n\r\nassert all(unique_dates_v1 == pd.Timestamp(\"2021-12-01\"))\r\nassert all(unique_dates_via_stack == pd.Timestamp(\"2021-12-01\"))\r\nassert unique_dates_v1.equals(unique_dates_via_stack)\r\nprint(\"all ok\")\n```\n\n\n### Issue Description\n\nFirst of all, sorry for the rather complex dataframe. It was already quite challenging to reduce it from the one I was actually using...\r\n\r\nLet us consider a DataFrame with a column MultiIndex where a NaT happens to appear in one of the indexes.\r\n\r\nLet's try to find out the timestamps where `date_type == \"MAT\"`. This can be done in two ways:\r\na) `unique_dates_v1`: here it is a simple cut using `get_level_values` - works fine\r\nb) `unique_dates_via_stack`: by stacking all the columns, thus making a series where a cross section can then give us the result. This is the version failing from pandas >= 2.1.0\r\n\r\nI know there is `future_stack=True` in newer pandas versions - and the `future_stack` _seems_ to work fine (and is usually what I prefer). However, the error above was caused when migrating older code. The `stack` version simply returns _wrong_ data. There is no MAT entry at all with a 1970 date. Even if the old stack variant introduces additional NaNs, it should never return wrong data, not even in a deprecated stack implementation.\n\n### Expected Behavior\n\nbehavior as in pandas 2.0, i.e. not assigning wrong data to MAT\n\n### Installed Versions\n\nworks fine with pandas <= 2.0.3\r\nfails with pandas >= 2.1.0\r\n","comments":["The output in Pandas 2.2 is\r\n```\r\n(...some warnings: about future_stack and performance...)\r\n2.2.0\r\nv1 DatetimeIndex(['2021-12-01'], dtype='datetime64[ns]', name='date', freq=None)\r\nv2 DatetimeIndex(['1970-12-01'], dtype='datetime64[ns]', name='date', freq=None)\r\nTraceback (most recent call last):\r\n  File \"\/home\/behrenhoff\/stack-test.py\", line 31, in <module>\r\n    assert all(unique_dates_via_stack == pd.Timestamp(\"2021-12-01\"))\r\nAssertionError\r\n```\r\nv2 should never return the 1970 timestamp, but always the 2021 timestamp.\r\n\r\nIn Pandas 2.0.3 the output is\r\n```\r\n2.0.3\r\nv1 DatetimeIndex(['2021-12-01'], dtype='datetime64[ns]', name='date', freq=None)\r\nv2 DatetimeIndex(['2021-12-01'], dtype='datetime64[ns]', name='date', freq=None)\r\nall ok\r\n```","Thanks for the report. Indeed, not using `future_stack=True` is now deprecated and will be removed in pandas 3.0. I'm not sure I see much value in spending effort to fix that which will be removed. But if anyone does want to put in the effort and is able to fix, PRs are welcome!","Sure, I was just wondering why the behavior has changed at all when the old way is deprecated... This one is particular dangerous because it returns a data pair (\"MAT\" with \"1970-12-01\") that didn't exist in the original. This should never happen.\r\n\r\nIn case it is decided against a fix (which I could understand), I'd at least suggest a warning that stacking nan or nat in the index results in undefined behavior - I've happily ignored the existing performance and future warnings after the pandas upgrade. I did certainly not expect this value pair.\r\n\r\nAlso, the future_stack behavior works at least in this _specific_ case though I did NOT test the future_stack=True extensively. Maybe a few tests are missing here? Tests with Timestamps including NaT in index, and in MultiIndex, ..."],"labels":["Bug","Reshaping","Regression"]},{"title":"BUG: DataFrame.update() raises FutureWarning due to the use of 'where' method","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom datetime import datetime\r\ncolumns = ['Open', 'High', 'Low', 'Close', 'Volume']\r\ndf = pd.DataFrame([[451.5, 458.0, 449.0, 455.5, 1239498]], columns=columns, index=[datetime(2024, 1, 29)])\r\ndf2 = pd.DataFrame([[450.5, 457.5, 450.0, 453.5, 1385875], [451.5, 458.0, 449.0, 455.5, 1284000]], columns=columns, index=[datetime(2024, 1, 26), datetime(2024, 1, 29)])\r\ndf2.update(df)\n```\n\n\n### Issue Description\n\nDue to the use of 'where' method in DataFrame.update() (`self.loc[:, col] = self[col].where(mask, that)`), it would raise FutureWarning as follows:\r\n\r\n> FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\r\n\r\nTried calling result.infer_objects(copy=False), but it's still the same. And all dtypes should be the same between these two DataFrames.\n\n### Expected Behavior\n\nFutureWarning shouldn't be raised when using a non-deprecated method.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:34 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T8103\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.1\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.1\r\nnumba                 : None\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.3\r\nsqlalchemy            : None\r\ntables                : 3.9.2\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. I haven't checked, but I believe this would be fixed by #55634. I recall that PR being pretty close to being ready, and could be revived. However, I'm not certain that PR would be appropriate for a patch version (it entirely changes the computation), so perhaps this should be handled independently."],"labels":["Bug","Dtype Conversions","Regression","DataFrame","Conditionals"]},{"title":"pandas' recommendation on inplace deprecation and categorical column","body":"Working on making scikit-learn's code pandas=2.2.0 compatible, here's a minimal reproducer for where I started:\r\n\r\n```py\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col': [\"a\", \"b\", \"c\"]}, dtype=\"category\")\r\ndf[\"col\"].replace(to_replace=\"a\", value=\"b\", inplace=True)\r\n```\r\nwhich results in:\r\n\r\n```\r\n$ python -Werror::FutureWarning \/tmp\/4.py\r\n\/tmp\/4.py:1: DeprecationWarning: \r\nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\nbut was not found to be installed on your system.\r\nIf this would cause problems for you,\r\nplease provide us feedback at https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466\r\n        \r\n  import pandas as pd\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/4.py\", line 4, in <module>\r\n    df[\"col\"].replace(to_replace=\"a\", value=\"b\", inplace=True)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py\", line 7963, in replace\r\n    warnings.warn(\r\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\r\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\r\n\r\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\r\n```\r\n\r\nThe first pattern doesn't apply here, so from this message, I understand I should do:\r\n\r\n```py\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col': [\"a\", \"b\", \"c\"]}, dtype=\"category\")\r\ndf[\"col\"] = df[\"col\"].replace(to_replace=\"a\", value=\"b\")\r\n```\r\n\r\nBut this also fails with:\r\n\r\n```\r\n$ python -Werror::FutureWarning \/tmp\/4.py\r\n\/tmp\/4.py:1: DeprecationWarning: \r\nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\nbut was not found to be installed on your system.\r\nIf this would cause problems for you,\r\nplease provide us feedback at https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466\r\n        \r\n  import pandas as pd\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/4.py\", line 4, in <module>\r\n    df[\"col\"] = df[\"col\"].replace(to_replace=\"a\", value=\"b\")\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py\", line 8135, in replace\r\n    new_data = self._mgr.replace(\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/base.py\", line 249, in replace\r\n    return self.apply_with_block(\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py\", line 364, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py\", line 854, in replace\r\n    values._replace(to_replace=to_replace, value=value, inplace=True)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/categorical.py\", line 2665, in _replace\r\n    warnings.warn(\r\nFutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\r\n```\r\n\r\nWith a bit of reading docs, it seems I need to do:\r\n\r\n```py\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col': [\"a\", \"b\", \"c\"]}, dtype=\"category\")\r\ndf[\"col\"] = df[\"col\"].cat.rename_categories({\"a\": \"b\"})\r\n```\r\n\r\nwhich fails with\r\n\r\n```\r\n$ python -Werror::FutureWarning \/tmp\/4.py\r\n\/tmp\/4.py:1: DeprecationWarning: \r\nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\r\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\r\nbut was not found to be installed on your system.\r\nIf this would cause problems for you,\r\nplease provide us feedback at https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466\r\n        \r\n  import pandas as pd\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/4.py\", line 4, in <module>\r\n    df[\"col\"] = df[\"col\"].cat.rename_categories({\"a\": \"b\"})\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/accessor.py\", line 112, in f\r\n    return self._delegate_method(name, *args, **kwargs)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/categorical.py\", line 2939, in _delegate_method\r\n    res = method(*args, **kwargs)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/categorical.py\", line 1205, in rename_categories\r\n    cat._set_categories(new_categories)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/categorical.py\", line 924, in _set_categories\r\n    new_dtype = CategoricalDtype(categories, ordered=self.ordered)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/dtypes.py\", line 221, in __init__\r\n    self._finalize(categories, ordered, fastpath=False)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/dtypes.py\", line 378, in _finalize\r\n    categories = self.validate_categories(categories, fastpath=fastpath)\r\n  File \"\/home\/adrin\/miniforge3\/envs\/sklearn\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/dtypes.py\", line 579, in validate_categories\r\n    raise ValueError(\"Categorical categories must be unique\")\r\nValueError: Categorical categories must be unique\r\n```\r\n\r\nSo `rename_categories` is not the one I want apparently, but reading through the \"see also\":\r\n\r\n> [reorder_categories](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.cat.reorder_categories.html#pandas.Series.cat.reorder_categories)\r\n> \r\n>     Reorder categories.\r\n> [add_categories](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.cat.add_categories.html#pandas.Series.cat.add_categories)\r\n> \r\n>     Add new categories.\r\n> [remove_categories](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.cat.remove_categories.html#pandas.Series.cat.remove_categories)\r\n> \r\n>     Remove the specified categories.\r\n> [remove_unused_categories](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.cat.remove_unused_categories.html#pandas.Series.cat.remove_unused_categories)\r\n> \r\n>     Remove categories which are not used.\r\n> [set_categories](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.cat.set_categories.html#pandas.Series.cat.set_categories)\r\n> \r\n>     Set the categories to the specified ones.\r\n\r\nNone of them seem to do what I need to do.\r\n\r\nSo it seems the way to go would be:\r\n\r\n```py\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col': [\"a\", \"b\", \"c\"]}, dtype=\"category\")\r\ndf.loc[df[\"col\"] == \"a\", \"col\"] = \"b\"\r\ndf[\"col\"] = df[\"col\"].astype(\"category\").cat.remove_unused_categories()\r\n```\r\n\r\nWhich is far from what the warning message suggests. \r\n\r\nSo at the end:\r\n\r\n- did I arrive at the right conclusion as what the code should look like now.\r\n- I think the warning message might be a bit more concrete as where users should go.\r\n- should there be a method on `Series.cat` to do this easier?","comments":["I think making rename_categories accept this might make the most sense, the solution you arrived at is probably the best case at the moment but obviously not great\r\n\r\ncc @jbrockmendel","FWIW I ended up with this (not great either), that I find a bit more readable (but this may depend on the reader :wink:):\r\n```py\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col': [\"a\", \"b\", \"c\"]}, dtype=\"category\")\r\ndf['col'] = df['col'].astype(object).replace(to_replace=\"a\", value=\"b\").astype(\"category\")\r\n```","i think eventually we want users to do `obj.replace('a', 'b').cat.remove_unused_categories()`.  That works now, but the .replace issues a warning.  i guess we could update the warning message to suggest this pattern for that particular use case","@jbrockmendel your code gives this warning now:\r\n\r\n> FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\r\n\r\nI'm not sure if you want to remove the warning in this case, or to suggest a different solution?"],"labels":["Categorical","Needs Discussion","inplace"]},{"title":"BUG: unexpected behavior in read_csv with pyarrow engine and dtype string","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'var1': ['44794724', None]})\r\ndf.to_csv('sample.csv', index=False)\r\ndf.head\r\n\r\ndf1 = pd.read_csv('sample.csv', dtype='string[pyarrow]',dtype_backend='pyarrow')\r\ndf2 = pd.read_csv('sample.csv', dtype='string[pyarrow]',dtype_backend='pyarrow',engine=\"pyarrow\")\r\n\r\nprint(df1['var1'].astype(str).str.endswith('.0').sum())\r\nprint(df2['var1'].astype(str).str.endswith('.0').sum())\n```\n\n\n### Issue Description\n\nI've encountered an issue with read_csv when using the pyarrow engine. Specifically, when a numeric column with missing values is imported as a string type, each number in the column has \".0\" appended to it. This issue occurs when setting the dtype to 'string' or 'string[pyarrow]'. The issue is not observed if at least one of the data rows contains a string, or if the dtype is set to 'object'.\n\n### Expected Behavior\n\nExpected the same behavior turning on and off the pyarrow engine. The numeric values should be imported as strings without alteration.\n\n### Installed Versions\n\npandas '2.2.0' and '3.0.0.dev0+171.gd928a5cc22'\r\npyarrow=14.0.2\r\npython=3.11.7\r\nOS: win11-22H2","comments":["take","Able to reproduce the issue with both 2.2.0 and 1.5.3 Triaging the issue further","For cases where dtype_backend is not pyarrow, the fix is to modify the `table.to_pandas()` [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/266bd4cde81f917c9d30257102edc7b0b80fb404\/pandas\/io\/parsers\/arrow_parser_wrapper.py#L302) to take in a new parameter and change to `frame = table.to_pandas(integer_object_nulls=True)`.\r\n\r\nThis fix does not seems to be fixing the case where `dtype_backend='pyarrow'` . Need to explore further"],"labels":["Bug","IO CSV","Arrow"]},{"title":"BUG: Can't convert `int64[pyarrow]` series with missing values to legacy numpy `float` series","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nser = pd.Series([1,2,3,None,10], dtype='int64[pyarrow]')\r\nser.astype(float)\n```\n\n\n### Issue Description\n\nI'm trying to run some scikit learn models that impute missing values. The transformers aren't happy with `<NA>`, so I'm trying to convert the pyarrow int columns to numpy float columns and getting this error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[54], [line 2](vscode-notebook-cell:?execution_count=54&line=2)\r\n      [1](vscode-notebook-cell:?execution_count=54&line=1) ser = pd.Series([1,2,3,None,10], dtype='int64[pyarrow]')\r\n----> [2](vscode-notebook-cell:?execution_count=54&line=2) ser.astype(float)\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6637](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6637), in NDFrame.astype(self, dtype, copy, errors)\r\n   [6631](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6631)     results = [\r\n   [6632](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6632)         ser.astype(dtype, copy=copy, errors=errors) for _, ser in self.items()\r\n   [6633](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6633)     ]\r\n   [6635](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6635) else:\r\n   [6636](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6636)     # else, only a single dtype is given\r\n-> [6637](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6637)     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\r\n   [6638](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6638)     res = self._constructor_from_mgr(new_data, axes=new_data.axes)\r\n   [6639](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:6639)     return res.__finalize__(self, method=\"astype\")\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:431](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:431), in BaseBlockManager.astype(self, dtype, copy, errors)\r\n    [428](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:428) elif using_copy_on_write():\r\n    [429](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:429)     copy = False\r\n--> [431](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:431) return self.apply(\r\n    [432](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:432)     \"astype\",\r\n    [433](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:433)     dtype=dtype,\r\n    [434](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:434)     copy=copy,\r\n    [435](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:435)     errors=errors,\r\n    [436](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:436)     using_cow=using_copy_on_write(),\r\n    [437](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:437) )\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:364](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:364), in BaseBlockManager.apply(self, f, align_keys, **kwargs)\r\n    [362](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:362)         applied = b.apply(f, **kwargs)\r\n    [363](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:363)     else:\r\n--> [364](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:364)         applied = getattr(b, f)(**kwargs)\r\n    [365](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:365)     result_blocks = extend_blocks(applied, result_blocks)\r\n    [367](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/managers.py:367) out = type(self).from_blocks(result_blocks, self.axes)\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:758](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:758), in Block.astype(self, dtype, copy, errors, using_cow, squeeze)\r\n    [755](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:755)         raise ValueError(\"Can not squeeze with more than one column.\")\r\n    [756](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:756)     values = values[0, :]  # type: ignore[call-overload]\r\n--> [758](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:758) new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\r\n    [760](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:760) new_values = maybe_coerce_values(new_values)\r\n    [762](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/internals\/blocks.py:762) refs = None\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:237](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:237), in astype_array_safe(values, dtype, copy, errors)\r\n    [234](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:234)     dtype = dtype.numpy_dtype\r\n    [236](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:236) try:\r\n--> [237](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:237)     new_values = astype_array(values, dtype, copy=copy)\r\n    [238](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:238) except (ValueError, TypeError):\r\n    [239](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:239)     # e.g. _astype_nansafe can fail on object-dtype of strings\r\n    [240](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:240)     #  trying to convert to float\r\n    [241](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:241)     if errors == \"ignore\":\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:179](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:179), in astype_array(values, dtype, copy)\r\n    [175](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:175)     return values\r\n    [177](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:177) if not isinstance(values, np.ndarray):\r\n    [178](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:178)     # i.e. ExtensionArray\r\n--> [179](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:179)     values = values.astype(dtype, copy=copy)\r\n    [181](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:181) else:\r\n    [182](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/dtypes\/astype.py:182)     values = _astype_nansafe(values, dtype, copy=copy)\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/base.py:722](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/base.py:722), in ExtensionArray.astype(self, dtype, copy)\r\n    [718](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/base.py:718)     from pandas.core.arrays import TimedeltaArray\r\n    [720](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/base.py:720)     return TimedeltaArray._from_sequence(self, dtype=dtype, copy=copy)\r\n--> [722](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/base.py:722) return np.array(self, dtype=dtype, copy=copy)\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:661](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:661), in ArrowExtensionArray.__array__(self, dtype)\r\n    [659](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:659) def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:\r\n    [660](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:660)     \"\"\"Correctly construct numpy arrays when passed to `np.asarray()`.\"\"\"\r\n--> [661](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:661)     return self.to_numpy(dtype=dtype)\r\n\r\nFile [~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1406](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1406), in ArrowExtensionArray.to_numpy(self, dtype, copy, na_value)\r\n   [1404](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1404)     result = np.empty(len(data), dtype=dtype)\r\n   [1405](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1405)     mask = data.isna()\r\n-> [1406](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1406)     result[mask] = na_value\r\n   [1407](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1407)     result[~mask] = data[~mask]._pa_array.to_numpy()\r\n   [1408](~\/.python\/current\/lib\/python3.10\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:1408) return result\r\n\r\nTypeError: float() argument must be a string or a real number, not 'NAType'\r\n\r\n```\n\n### Expected Behavior\n\nConversion to work. (I'm pretty sure this worked in Pandas 2.1).\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.2.0-1018-azure\r\nVersion               : #18~22.04.1-Ubuntu SMP Tue Nov 21 19:25:02 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Thanks for the report! Result of a git bisect:\r\n\r\n```\r\ncommit bc2888d67d008b32c6f4e12af0b50201488e2c94\r\nAuthor: Luke Manley\r\nDate:   Fri Sep 1 12:55:49 2023 -0400\r\n\r\n    BUG: ArrowExtensionArray.to_numpy avoid object dtype when na_value provided (#54843)\r\n```\r\n\r\ncc @lukemanley #54843"],"labels":["Bug","Missing-data","Dtype Conversions","Arrow"]},{"title":"DOC: `DataFrame.any()` and `DataFrame.all()` documentation specifies the wrong return types","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\nhttps:\/\/pandas.pydata.org\/docs\/dev\/reference\/api\/pandas.DataFrame.any.html\r\nhttps:\/\/pandas.pydata.org\/docs\/dev\/reference\/api\/pandas.DataFrame.all.html\r\n\n\n### Documentation problem\n\nThe documentation specifies that the return type is either a DataFrame or Series, but the [code](https:\/\/github.com\/pandas-dev\/pandas\/blob\/441f65df5c6dea850e6334ab770528184dfb0d33\/pandas\/core\/frame.py#L11576) specifies a Series or bool.\r\n\r\n`Series` or `bool` is correct, and the `level` argument mentioned in the code no longer exists.  \n\n### Suggested fix for documentation\n\nReturns:\r\nDataFrame or scalar\r\nIf axis=None, then a scalar boolean is returned. Otherwise a `Series` is returned with index matching the `index` argument.","comments":["Thanks for the report, agreed - cleaning this up would be most welcome!","take","take","Hey, I'm new to open source but my interests lie in ML. Can I work on this issue?","Hi is Can I Contribute to this issue please\r\n"],"labels":["Docs","good first issue","Reduction Operations"]},{"title":"BUG:  `DataFrame.any()` inconsistent with other methods","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'x':[1,2,3],'y':[1,2,3]})\r\n\r\ndf.all(0) # Positioned arg for axis is allowed.\r\ndf.any(0) # Positioned arg for axis not allowed.\n```\n\n\n### Issue Description\n\nThe `any` method has a keyword only restriction for its arguments.  The `all` method doesn't.\r\n\r\n\n\n### Expected Behavior\n\n`any` and `all` should be consistent. To my POV `any` seems to be doing the right thing, while `all` (and other similar methods without a restriction and default arguments only, `min`, `max` etc) seems to be going against what Pandas has been moving toward where all arguments with defaults tend to fall on the keyword only side of the argument list.\r\n\r\nWhich is correct? \n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : b5a963c872748801faa7ff67b8d766f7043bb1c1\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.13-1rodete1-amd64\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Debian 6.5.13-1rodete1 (2023-12-06)\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+172.gb5a963c872\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 65.5.0\r\npip                   : 23.3.2\r\nCython                : 3.0.5\r\npytest                : 7.4.4\r\nhypothesis            : 6.97.0\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : 2023.10.1\r\nfsspec                : 2023.12.2\r\ngcsfs                 : 2023.12.2post1\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : 2.9.0\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : 1.2.6\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2023.12.2\r\nscipy                 : 1.12.0\r\nsqlalchemy            : 2.0.25\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2024.1.1\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. I'd favor keyword-argument only, but I think this needs a bit more discussion first.","I am also +1 to keyword-argument only","+1 for keyword-only as well. I already changed a couple of methods in this typing PR #56739 when writing overloads depended on `axis`.\r\n\r\nMany users would get the deprecation warning as even some tests\/docs call those methods with positional arguments (seems not uncommon).","Hi, can I work with this issue?","> Hi, can I work with this issue?\r\n\r\nIt might be good to wait until\/if this #56739 gets merged as it has already changed many of those methods to keyword-only (I'm sure there might be more that I did not cover)."],"labels":["good first issue","API - Consistency","Reduction Operations"]},{"title":"BUG: datetime column changed after storing to hdf5","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\nnp.random.seed(42)\r\ndf = pd.DataFrame(np.random.randn(5, 3), columns=list('abc')).assign(dt=pd.Timestamp('2024-01-01'))  # .astype({'dt': 'datetime64[ns]'})\r\nprint(f'{\"Before storing to HDF5\":-^80}')\r\nprint(df.dtypes)\r\nprint(df)\r\ndf.to_hdf('df.h5', key='df')\r\nprint(f'{\"After storing to HDF5\":-^80}')\r\ndf1 = pd.read_hdf('df.h5')\r\nprint(df1.dtypes)\r\nprint(df1)\r\n\r\n\r\n# -----------------------------Before storing to HDF5-----------------------------\r\n# a           float64\r\n# b           float64\r\n# c           float64\r\n# dt    datetime64[s]\r\n# dtype: object\r\n#           a         b         c         dt\r\n# 0  0.496714 -0.138264  0.647689 2024-01-01\r\n# 1  1.523030 -0.234153 -0.234137 2024-01-01\r\n# 2  1.579213  0.767435 -0.469474 2024-01-01\r\n# 3  0.542560 -0.463418 -0.465730 2024-01-01\r\n# 4  0.241962 -1.913280 -1.724918 2024-01-01\r\n# -----------------------------After storing to HDF5------------------------------\r\n# a            float64\r\n# b            float64\r\n# c            float64\r\n# dt    datetime64[ns]\r\n# dtype: object\r\n#           a         b         c                            dt\r\n# 0  0.496714 -0.138264  0.647689 1970-01-01 00:00:01.704067200\r\n# 1  1.523030 -0.234153 -0.234137 1970-01-01 00:00:01.704067200\r\n# 2  1.579213  0.767435 -0.469474 1970-01-01 00:00:01.704067200\r\n# 3  0.542560 -0.463418 -0.465730 1970-01-01 00:00:01.704067200\r\n# 4  0.241962 -1.913280 -1.724918 1970-01-01 00:00:01.704067200\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nCreating a dataframe with a datetime column by assigning a single `pd.Timestamp`, then storing it with `to_hdf`. the datetime column changed after reading from the hdf5 file.\r\nBut, if `astype` manually before storing, everthing will be OK.\r\n\r\n### Expected Behavior\r\n\r\n`df` should equal to `df1`\r\n\r\n### Installed Versions\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.4.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.0-78-generic\r\nVersion               : #85-Ubuntu SMP Fri Jul 7 15:25:09 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.24.3\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 66.0.0\r\npip                   : 23.0.1\r\nCython                : 3.0.0b2\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.0\r\nlxml.etree            : 4.9.2\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.16.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.9.0\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.1\r\nnumba                 : 0.57.0\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 12.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.10.1\r\nsqlalchemy            : 2.0.21\r\ntables                : 3.8.0\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n","comments":["Hey @xbanke, I've tried it in Jupyter and Colab but it is working correctly.\r\n\r\n### Colab :\r\n![Screenshot (302)](https:\/\/github.com\/pandas-dev\/pandas\/assets\/90236635\/155912e2-879b-4030-addf-2c1344536f93)\r\n\r\n### Jupyter :\r\n![Screenshot (303)](https:\/\/github.com\/pandas-dev\/pandas\/assets\/90236635\/986e36b4-b3dc-4ce6-af43-1bcdaf9f7b26)\r\n","@PritamSarbajna, So what is the possible problem of my environment?","There exists some difference:\r\n\r\n```py\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnp.random.seed(42)\r\n\r\ndf = pd.DataFrame(np.random.randn(5, 3), columns=list('abc')).assign(dt=pd.Timestamp('20240101'))\r\ndf1 = df.astype({'dt': 'datetime64[ns]'})\r\n\r\nprint(df['dt'].values)\r\nprint(df1['dt'].values)\r\n\r\n\r\n# array(['2024-01-01T00:00:00', '2024-01-01T00:00:00',\r\n#        '2024-01-01T00:00:00', '2024-01-01T00:00:00',\r\n#        '2024-01-01T00:00:00'], dtype='datetime64[s]')\r\n# array(['2024-01-01T00:00:00.000000000', '2024-01-01T00:00:00.000000000',\r\n#        '2024-01-01T00:00:00.000000000', '2024-01-01T00:00:00.000000000',\r\n#        '2024-01-01T00:00:00.000000000'], dtype='datetime64[ns]')\r\n```","Thanks for the report; given that non-nano resolutions are relatively new, I'm guessing that it's not yet built into hdf. Further investigations welcome!\r\n\r\ncc @jbrockmendel "],"labels":["Bug","IO HDF5","Non-Nano"]},{"title":"BUG: possible inconsistency between `inplace=True` and `inplace=False` in `DataFrame.where\/mask`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# examples from docstrings, inplace=False\r\ns = pd.Series(range(5))\r\nt = pd.Series([True, False])\r\nprint(s.where(t, 99))\r\n# 0     0\r\n# 1    99\r\n# 2    99\r\n# 3    99\r\n# 4    99\r\n# dtype: int64\r\nprint(s.mask(t, 99))\r\n# 0    99\r\n# 1     1\r\n# 2    99\r\n# 3    99\r\n# 4    99\r\n# dtype: int64\r\n\r\n# inplace=True\r\ns = pd.Series(range(5))\r\ns.where(t, 99, inplace=True)\r\nprint(s)\r\n# 0     0\r\n# 1    99\r\n# 2     2\r\n# 3     3\r\n# 4     4\r\n# dtype: int64\r\ns = pd.Series(range(5))\r\ns.mask(t, 99, inplace=True)\r\nprint(s)\r\n# 0    99\r\n# 1     1\r\n# 2     2\r\n# 3     3\r\n# 4     4\r\n# dtype: int64\n```\n\n\n### Issue Description\n\nThe first two examples are from the docstrings of `DataFrame.where` and `DataFrame.mask`. They agree with the documentations regarding how to fill the values of `cond` on misaligned index positions.\r\nHowever, when `inplace=True`, the results are different from `inplace=False` for both `where` and `mask`.\n\n### Expected Behavior\n\nI would expect `inplace` parameter does not affect the results. But I notice the first line of code below in the source code of `where`. So I wonder is this behaviour expected?\r\nThank you in advance.\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/d928a5cc222be5968b2f1f8a5f8d02977a8d6c2d\/pandas\/core\/generic.py#L10665-L10674\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 4c520e35f95429ceaf91ea67cd2ced5aabba9619\r\npython                : 3.10.13.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.10.16.3-microsoft-standard-WSL2\r\nVersion               : #1 SMP Fri Apr 2 22:23:49 UTC 2021\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0dev0+743.g4c520e35f9\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : 3.0.5\r\npytest                : 7.4.3\r\nhypothesis            : 6.91.0\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : 2.8.7\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Agreed this looks suspect, but the code seems quite deliberate. I haven't been able to track down where this behavior was introduced, I'm thinking the origin should be better understood.\r\n\r\nNote that these methods will retain `inplace` under [PDEP-8](#51466)."],"labels":["Bug","Needs Discussion","inplace","Conditionals"]},{"title":"BUG: Inconsistent resolution from constructing with and assigning datetime values","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nfrom datetime import datetime\r\nimport pandas as pd\r\n\r\nconstruct = pd.DataFrame([datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)])\r\nprint(construct.info())\r\n\r\nassign = pd.DataFrame([], index=[0])\r\nassign[\"time\"] = datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nprint(assign.info())\r\n\r\nassign_timestamp = pd.DataFrame([], index=[0])\r\nassign_timestamp[\"time\"] = pd.Timestamp(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nprint(assign_timestamp.info())\r\n\r\nassign_loc = pd.DataFrame([], index=[0])\r\nassign_loc.loc[0, \"time\"] = datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nprint(assign_loc.info())\n```\n\n\n### Issue Description\n\nHi! I'm faced with a problem about datetime columns. It was met in our prod env upgrade from pandas 1.5.3 to 2.1.4. Before that constructing and assigning both result in datetime64[ns]. But asof 2.1.4, the following results were get:\r\n```\r\nprint(construct.info())\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 1 entries, 0 to 0\r\nData columns (total 1 columns):\r\n #   Column  Non-Null Count  Dtype         \r\n---  ------  --------------  -----         \r\n 0   0       1 non-null      datetime64[ns]\r\ndtypes: datetime64[ns](1)\r\nmemory usage: 140.0 bytes\r\nNone\r\n\r\nprint(assign.info())\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 1 entries, 0 to 0\r\nData columns (total 1 columns):\r\n #   Column  Non-Null Count  Dtype         \r\n---  ------  --------------  -----         \r\n 0   time    1 non-null      datetime64[us]\r\ndtypes: datetime64[us](1)\r\nmemory usage: 16.0 bytes\r\nNone\r\n\r\nprint(assign_timestamp.info())\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 1 entries, 0 to 0\r\nData columns (total 1 columns):\r\n #   Column  Non-Null Count  Dtype         \r\n---  ------  --------------  -----         \r\n 0   time    1 non-null      datetime64[us]\r\ndtypes: datetime64[us](1)\r\nmemory usage: 16.0 bytes\r\nNone\r\n\r\nprint(assign_loc.info())\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 1 entries, 0 to 0\r\nData columns (total 1 columns):\r\n #   Column  Non-Null Count  Dtype         \r\n---  ------  --------------  -----         \r\n 0   time    1 non-null      datetime64[ns]\r\ndtypes: datetime64[ns](1)\r\nmemory usage: 124.0 bytes\r\nNone\r\n```\r\nWhile construction and loc assignment gives the dtype datetime64[ns], df[\"time\"] = datetime(...) results in datetime64[us].\r\n\r\nI tested in a different env using pandas 2.2.0. The result is different when using loc:\r\n```\r\nprint(assign_loc.info())\r\n<class 'pandas.core.frame.DataFrame'>\r\nIndex: 1 entries, 0 to 0\r\nData columns (total 1 columns):\r\n #   Column  Non-Null Count  Dtype         \r\n---  ------  --------------  -----         \r\n 0   time    1 non-null      datetime64[us]\r\ndtypes: datetime64[us](1)\r\nmemory usage: 124.0 bytes\r\nNone\r\n```\r\n\r\nThis time assign_loc gives datetime64[us], but construct still gives datetime64[ns].\r\n\r\nI've noticed in the release notes that changes were made to `astype`, making it parse inputs according to the original resolution, but it might not explain the inconsistency intuitively. This is causing massive concat errors in our code due to inconsistent dtypes, and we are truly concerned about it as we are not sure if this is a bug and what behavior will be stabilized. We feel like further confirming it.\r\n\r\nThank you so much and wish you all the best!\n\n### Expected Behavior\n\nConsistent datetime resolution\n\n### Installed Versions\n\n<details>\r\n2.1.4\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.22621\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : en_US.UTF-8\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : 7.4.0\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.3\r\nhtml5lib            : None\r\npymysql             : 1.0.2\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.20.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : 1.3.5\r\ndataframe-api-compat: None\r\nfastparquet         : 2023.8.0\r\nfsspec              : 2023.10.0\r\ngcsfs               : None\r\nmatplotlib          : 3.8.0\r\nnumba               : 0.58.1\r\nnumexpr             : 2.8.7\r\nodfpy               : None\r\nopenpyxl            : 3.0.10\r\npandas_gbq          : None\r\npyarrow             : 11.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : 2.0.25\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\nTEST 2.2.0\uff1a\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.12.0.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 11\r\nVersion               : 10.0.22621\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n\r\n<\/details>\r\n","comments":["Bisected this issue, regression happened on this commit:\r\n```\r\na2bb939ebd3aa9d78f96a72fa575f67b076f0bfa is the first bad commit\r\ncommit a2bb939ebd3aa9d78f96a72fa575f67b076f0bfa\r\nAuthor: jbrockmendel <jbrockmendel@gmail.com>\r\nDate:   Thu May 18 09:11:48 2023 -0700\r\n\r\n    API\/BUG: infer_dtype_from_scalar with non-nano (#52212)\r\n    \r\n    * API\/BUG: infer_dtype_from_scalar with non-nano\r\n    * update test\r\n    * xfail on 32bit\r\n    * fix xfail condition\r\n    * whatsnew\r\n    * xfail on windows\r\n```\r\n\r\nFile used for bisecting\r\n```python\r\n# t.py file\r\n\r\nfrom datetime import datetime\r\nimport pandas as pd\r\n\r\nconstruct = pd.DataFrame([datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)])\r\nassert str(construct.iloc[:,0].dtype) == 'datetime64[ns]'\r\n\r\nassign = pd.DataFrame([], index=[0])\r\nassign[\"time\"] = datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nassert str(assign.iloc[:,0].dtype) == 'datetime64[ns]'\r\n\r\nassign_timestamp = pd.DataFrame([], index=[0])\r\nassign_timestamp[\"time\"] = pd.Timestamp(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nassert str(assign_timestamp.iloc[:,0].dtype) == 'datetime64[ns]'\r\n\r\nassign_loc = pd.DataFrame([], index=[0])\r\nassign_loc.loc[0, \"time\"] = datetime(year=2024, month=1, day=26, hour=0, minute=0, second=0, microsecond=0)\r\nassert str(assign_loc.iloc[:,0].dtype) == 'datetime64[ns]'\r\n```\r\nBefore that all dtypes are `datetime64[ns]`\r\nAfter:\r\n```python\r\n>>> print(pd.__version__)\r\n2.1.0.dev0+797.ga2bb939ebd.dirty\r\n>>> str(construct.iloc[:,0].dtype)\r\n'datetime64[ns]'\r\n>>> str(assign.iloc[:,0].dtype)\r\n'datetime64[us]'\r\n>>> str(assign_timestamp.iloc[:,0].dtype)\r\n'datetime64[us]'\r\n>>> str(assign_loc.iloc[:,0].dtype)\r\n'datetime64[ns]'\r\n```","Also, I think the `ns` and `us` resolutions depend on whether we are assigning a scalar or a list. Refencing https:\/\/github.com\/pandas-dev\/pandas\/issues\/55014#issuecomment-1764341895","cc @jbrockmendel ","Should be addressed by #55901"],"labels":["Bug","Dtype Conversions","Regression","Non-Nano"]},{"title":"BUG: DataFrame.from_dict loses bool type with NumPy MaskedArray","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nprint('# array')\r\na = np.array([False, True])\r\nprint(a.dtype)\r\n\r\ndf = pd.DataFrame.from_dict({'a': a})\r\nprint(df['a'].dtype)\r\n\r\nprint('# masked array')\r\na_ = np.ma.array([False, True], mask=[1, 0])\r\nprint(a_.dtype)\r\n\r\ndf_ = pd.DataFrame.from_dict({'a': a_})\r\nprint(df_['a'].dtype)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nFor a regular numpy `array` the `bool` type is preserved, but for a `maskedarray` it becomes `object`:\r\n\r\n```\r\n# array\r\nbool\r\nbool\r\n# masked array\r\nbool\r\nobject\r\n```\r\n\r\n### Expected Behavior\r\n\r\nThe last line should be [`boolean`](https:\/\/pandas.pydata.org\/docs\/user_guide\/boolean.html).\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\/opt\/python\/3.11.6\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 736868671044605b0f2c2cc23ab5ea09fbc9a2ac\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.2.0-1018-azure\r\nVersion               : #18~22.04.1-Ubuntu SMP Tue Nov 21 19:25:02 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : C.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+170.g7368686710\r\nnumpy                 : 1.26.1\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : 2023.2.0\r\nfsspec                : 2023.10.0\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.10.1\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Uhh, so this actually has nothing to do with masked arrays as I discovered, it has to do with the fact that for some reason pandas parses a bool column with NaN values as an object dtype, which I posted about in #57119 .","@mroeschke isn't this also expected behaviour then?"],"labels":["Bug","Needs Triage"]},{"title":"ENH: Adding `pipe` operations to `pandas.api.typing.Rolling` instances","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nI wish I could use a `pipe` method on a `rolling` object for speeding up operations that require multiple vectorized (terminology?) operations on a `rolling` object.\r\n\r\n### Feature Description\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nn_samples = 60*60*2 # 2 hours \r\nstart_ts = pd.Timestamp(\"2022-01-01\")\r\nend_ts = start_ts + pd.Timedelta(f\"{n_samples}s\")\r\ns = pd.Series(\r\n    np.random.normal(0, 1, size=n_samples),\r\n    index=pd.date_range(start_ts, end_ts, freq=\"1s\", inclusive=\"left\")\r\n).rolling(\"1T\").pipe(lambda r: r.std()\/r.mean())\r\n```\r\n\r\n### Alternative Solutions\r\n\r\nI'm not aware of any.\r\n\r\n### Additional Context\r\n\r\nMaybe a bit silly, but I've seen recommendation to used chained operations when possible. I tend to like the style, but I haven't figured out an easy way to do this particular operation without a performance hit.\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/23382815\/8fad3077-fb4f-40b5-80ca-63ee1a562244)\r\n\r\nI could be misinterpreting how `pipe` actually works, but I think it might be equivalent to `calc(r)` performance wise?\r\n\r\nThank you for taking a look!","comments":["> I could be misinterpreting how pipe actually works, but I think it might be equivalent to calc(r) performance wise?\r\n\r\nYes - that's correct.\r\n\r\nI'm +1 here; cc @mroeschke ","Sure a PR to add pipe would be welcome. ","I'm happy to pick this one up, but would it rely on the caller to use only vectorised functions?","> would it rely on the caller to use only vectorised functions?\r\n\r\nIn using pipe, we pass the instance in question (in this case, Rolling) to the callable that the user passes. What they do with it from there on is their business :smile: ","take"],"labels":["Enhancement","Window"]},{"title":"DISC: Consider not requiring PyArrow in 3.0","body":"**TL;DR**: Don't make PyArrow required - instead, set minimum NumPy version to 2.0 and use NumPy's StringDType.\r\n\r\n## Background\r\n\r\nIn [PDEP-10](https:\/\/pandas.pydata.org\/pdeps\/0010-required-pyarrow-dependency.html), it was proposed that PyArrow become a required dependency. Several reasons were given, but the most significant reason was to adopt a proper string data type, as opposed to `object`.\r\nThis was voted on and agreed upon, but there have been some important developments since then, so I think it's warranted to reconsider.\r\n\r\n## StringDType in NumPy\r\n\r\nThere's a proposal in NumPy to add a StringDType to NumPy itself. This was brought up in the PDEP-10 discussion, but at the time [was not considered significant enough to delay the PyArrow requirement](https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#issuecomment-1620229173) because:\r\n1. NumPy itself might not accept its StringDType proposal.\r\n2. NumPy's StringDType might not come with the algorithms pandas needs.\r\n3. pyarrow's strings might still be significantly faster.\r\n4. because pandas typically supports older NumPy versions (in addition to the latest release), it would be 2+ years until pandas could use NumPy's strings.\r\n\r\nLet's tackle these in turn:\r\n1. I caught up with Nathan Goldbaum (author of the StringDType proposal) today, and he's said that [NEP55](https:\/\/numpy.org\/neps\/nep-0055-string_dtype.html) will be accepted (although technically still in draft status, it has several supporters and no objectors and so realistically is going to change to \"accepted\" very soon).\r\n2. The second concern was the algorithms. Here's an excerpt of the NEP I'd like to draw attention to:\r\n   > In addition, we will add implementations for the comparison operators as well as an add loop that accepts two string \r\n  arrays, multiply loops that accept string and integer arrays, an isnan loop, and implementations for the str_len, isalpha, \r\n   isdecimal, isdigit, isnumeric, isspace, find, rfind, count, strip, lstrip, rstrip, and replace string ufuncs [universal functions] that will be newly \r\n   available in NumPy 2.0.\r\n\r\n   So, NEP55 not only provides a NumPy StringDType, but also efficient string algorithms.\r\n\r\n   There's a [pandas fork](https:\/\/github.com\/ngoldbaum\/pandas\/tree\/stringdtype) implementing this in pandas, which Nathan has been keeping up-to-date. Once the NumPy StringDType is merged into NumPy main (likely next week) it'll be much easier for pandas devs to test it out. Note: some parts of the fork don't yet use the ufuncs, but they will do soon, it's just a matter of updating things.\r\n\r\n   For any ufunc that's missing, Nathan's said that now that the string ufuncs framework exists in NumPy, it's relatively straightforward to add new ones (e.g. for `.str.partition`). There is real funding behind this work, so it's likely to keep moving quite fast.\r\n\r\n3. Nathan's said he doesn't have timings to hand for this comparison, and is about to go on holiday \ud83c\udf34 He'll be able to provide timings in 1-2 weeks' time though.\r\n\r\n4. Personally, I'd be fine with requiring NumPy 2.0 as the minimum NumPy version for pandas, if it means efficient string handling by default without the need for PyArrow. Also, [Nathan Goldbaum's fork](https:\/\/github.com\/ngoldbaum\/pandas\/tree\/stringdtype) already implements this for pandas. So, no need to wait 2 years, it should just be a matter of months.\r\n\r\n## Feedback\r\n\r\nThe feedback issue makes for an interesting read: https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466.\r\nComplaints seem to come mostly (as far as I can tell) from other package maintainers who are considering moving away from pandas (e.g. [fairlearn](https:\/\/github.com\/fairlearn\/fairlearn\/pull\/1335)).\r\n\r\n[This one](https:\/\/github.com\/pandas-dev\/pandas\/issues\/54466#issuecomment-1913135898) surprised me, I don't think anyone had considered this one before? One could argue that it's VirusTotal's issue, but still, just wanted to bring visibility to it.\r\n\r\n## Tradeoffs\r\n\r\nIn [the PDEP-10 PR](https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#issuecomment-1531692136) it was mentioned that PyArrow could help reduce some maintenance work (which, despite some funding, still seems to be mostly volunteer-driven). Has this been investigated further? Is it still likely to be the case?\r\n\r\nFurthermore, not requiring PyArrow would mean not being able to infer `list` and `struct` dtypes by default (at least, not without significant further work).\r\n\r\n## \"No is temporary, yes is forever\"\r\n\r\nI'm not saying \"never require PyArrow\". I'm just saying, at this point in time, I don't think the requirement is justified. Of the proposed benefits, the most salient one is strings, and now there's a realistic alternative which doesn't require taking on an extra massive dependency.\r\n\r\nI acknowledge that lately I've been more focused on other projects, and so don't want to come across as \"I'm telling pandas what to do because I know best!\" (I certainly don't).\r\n\r\nCircumstances have changed since the PDEP-10 PR and vote, and personally I regret voting the way I did. Does anyone else feel the same?","comments":["**TLDR**: I am +1 not making pyarrow a required dependency in pandas 3.0. I am -1 on making NumPy 2.0 the min version and numpy StringDtypes the default in pandas 3.0. Keep the status quo in 3.0.\r\n\r\nA few thoughts:\r\n\r\n1. numpy StringDtype will still be net new in 2.0. While I expect the new type to be robust and more performant than `object`, I think with any new feature it should be opt-in first before being made the default as the scope of edge case incompatibility is unknown. pyarrow strings have been around since 1.3 and was not until recently decided to become the default (I understand it's a different type system too).\r\n\r\n2. I have a biased belief that pyarrow type system with it's nullability and support for more types would be a net benefit for users, but I understand that the current numpy type system is \"sufficient\". It would be cool to allow users to use pyarrow types everywhere in pandas by default, but making that opt-in I think is a likely end state for pyarrow + pandas.","I think we should still stick with PDEP-10 as is; even if user benefit 1 wasn't as drastic as envisioned, I still think benefits 2 and 3 help immensely. \r\n\r\nGenerally the story around pandas type system is very confusing; I am hopeful that moving towards the arrow type system solves that over time","Personally I am in favor of keeping pyarrow optional (although I voted for the PDEP, because I find it more important to have a proper string dtype). But I also agree with Matt that it seems to fast to require numpy >= 2 for pandas (not only because the string dtype is very new, but also just because this will be annoying for the ecosystem to require such a new version of numpy that many other packages will not yet be compatible with).\r\n\r\nIf we want a simple alternative to keep pyarrow optional, I don't think we need to use numpy's new string dtype, though. We already have a object-dtype based StringDtype that can be the fallback when pyarrow is not installed. User still get the benefit of a new default, proper `string` dtype in 3.0 in all cases, but if they also want the performance improvements of the new string dtype, they need to have pyarrow installed. Then it's up to users to make that trade-off (and we can find ways to strongly encourage users to use pyarrow).\r\n\r\nI would also like to suggest another potential idea to consider: we could adopt Arrow's _type_ (memory model) for strings, but without requiring _pyarrow_ the package. Building on @WillAyd's work in https:\/\/github.com\/pandas-dev\/pandas\/pull\/54506 using nanoarrow to use bitmaps in pandas, we could implement a basic StringArray in a similar way, and implement the basic required features in pandas itself (things like getitem, take, isna, unique\/factorize), and then for the string-specific methods either use pyarrow if installed, or fallback to Python's string methods otherwise (or if we could vendor some code for this, progressively implement some string methods ourselves).   \r\nThis of course requires a decent chunk of work in pandas itself. But with the advantages that this keeps compatibility with the Arrow type system (and zero-copy conversion to\/from Arrow), and also already gives some advantages for the case pyarrow is not installed (improved memory usage, performance improvements for a subset of methods).","> **TLDR**: I am +1 not making pyarrow a required dependency in pandas 3.0. I am -1 on making NumPy 2.0 the min version and numpy StringDtypes the default in pandas 3.0. Keep the status quo in 3.0.\r\n\r\n+1 on this as well. IMO, it's too early to require numpy 2.0 (since it's pretty hard to adapt to the changes).\r\n\r\ncc @pandas-dev\/pandas-core \r\n","+1 on not requiring numpy 2 for pandas 3.\r\n\r\nI'm fine to continue as planned with the PDEP. If we consider the option of another Arrow implementation replacing PyArrow, ot feels like using Arrow-rs is a better option than nanoarrow to me (at least an option also worth considering). Last time this was discussed it wasn't clear what would happen with the two Rust implementations, but now everybody (except Polars for now) is settled on Arrow-rs and Arrow2 is discontinued. So, things are stable.\r\n\r\nIf there is interest, I can research further and work on a prototype.","I think we should wait for more feedback in #54466 .  pandas 2.2 was released only 11 days ago.  I say we give it a month, or maybe until the end of February, and then make a decision.  The whole point of gaining feedback was to give us a chance to revisit the decision to make `pyarrow` a required dependency.  Seems like our options at this point with pandas 3.0 are:\r\n1. Require `pyarrow` as planned from PDEP-10\r\n2. Require `numpy 2.0` and use `numpy` implementation for strings.\r\n3. Postpone to a later date any requirement for `pyarrow` - make it optional but allow people to get better string performance by opting in.\r\n\r\nGiven the feedback so far and the arguments that @MarcoGorelli gives above and the other comments, I'm leaning towards (3), but I'd like to see more feedback from the community at large.\r\n","IMO, I think we should make a decision by the next dev call(Feb. 7th I think?).\r\n\r\nI'm probably going to release 2.2.1 at most 2 weeks after numpy releases the 2.0rc (so probably around Feb. 14, assuming the numpy 2.0 releases on schedule on Feb 1), and I think we should decide whether to roll back the warning for 2.2.1, to avoid confusion.","I did a quick test on how big it'd be a binary using Arrow-rs (Rust). In general in Rust only static linking is used, so just one `.so` and no dependencies would be needed. A sample library using Arrow-rs with the default components (arrow-json, arrow-ipc...) compiles to a file around 500kb. In that sense, the Arrow-rs approach would solve the installation and size issues. Of course this is not an option for pandas 3.0, and it requires a non-trivial amount of work.\r\n\r\nSomething that can make this happen quicker and with less effort is implementing the same PyArrow API for Arrow-rs for the parts we need. In theory, that would allow to simply replace PyArrow by the new package and update the imports.\r\n\r\nIf there is interest in giving this a try, I'd personally change my vote here from requiring PyArrow in pandas 3, to keep the status quo for now.","> IMO, I think we should make a decision by the next dev call(Feb. 7th I think?).\r\n\r\nI assume that the decision would be whether we plan to revise the PDEP and then go through the PDEP process again for the revised PDEP?\r\n\r\nThe PDEP process was created not only that decisions have sufficient discussion and visibility but also that once agreed people could then work towards the agreed changes\/improvements without being VETOd by individual maintainers.\r\n\r\nIn this case, however, it maybe that several maintainers would vote differently now.\r\n\r\nDoes our process allow us to re vote on the existing PDEP? (given that the PDEP did include the provision to collect feedback from the community)\r\n\r\nDoes the outcome of any discussions\/decisions on this affect whether the next pandas version is 3.0 or 2.3?","Agree with Simon, this concern was discussed as part of the original PDEP (https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#discussion_r1185126720) with some timelines discussed and the vote was still approved. I somewhat expected some of the pushback from developers of web-apps so am supportive of this new proposal and my original vote, but it needs to fit in with the governance established, and should possibly also be cautious of any development that has  taken place in H2 '23 that has been done in anticipation of the implementation of the PDEP. I would expect the approved PDEP to continue to steer the development until formally agreed otherwise. I don't see a reason why a new PDEP could not be proposed to alter\/amend the previous, particularly if there already seemed to be enough support to warrant one. ","> I would also like to suggest another potential idea to consider: we could adopt Arrow's _type_ (memory model) for strings, but without requiring _pyarrow_ the package. Building on @WillAyd's work in #54506 using nanoarrow to use bitmaps in pandas, we could implement a basic StringArray in a similar way, and implement the basic required features in pandas itself (things like getitem, take, isna, unique\/factorize), and then for the string-specific methods either use pyarrow if installed, or fallback to Python's string methods otherwise (or if we could vendor some code for this, progressively implement some string methods ourselves).\r\n\r\nWith @jorisvandenbossche idea I wanted to try and implement an Extension-Array compatable StringArray using nanoarrow. Some python idioms like negative indexing aren't yet implemented, and there was a limitation around classmethods I haven't worked around, but otherwise I did get implement this here:\r\n\r\nhttps:\/\/github.com\/WillAyd\/nanopandas\/tree\/7e333e25b1b4027e49b9d6ad2465591abf0c9b27\r\n\r\nI also implented some of the optional interface items like ``unique``, ``fillna`` and ``dropna`` alongside a few str accessor methods\r\n\r\nOf course benchmarking this would take some effort, but I think most of the algorithms we would need are pretty simple.","> Personally I am in favor of keeping pyarrow optional (although I voted for the PDEP, because I find it more important to have a proper string dtype). But I also agree with Matt that it seems to fast to require numpy >= 2 for pandas (not only because the string dtype is very new, but also just because this will be annoying for the ecosystem to require such a new version of numpy that many other packages will not yet be compatible with).\r\n> \r\n\r\nI too was keen to keep pyarrow optional but voted for the PDEP for the benefits for other dtypes. \r\n\r\nFrom the PDEP... \"Starting in pandas 3.0, the default type inferred for string data will be ArrowDtype with pyarrow.string instead of object. Additionally, we will infer all dtypes that are listed below as well instead of storing as object.\"\r\n\r\n> If we want a simple alternative to keep pyarrow optional, I don't think we need to use numpy's new string dtype, though. We already have a object-dtype based StringDtype that can be the fallback when pyarrow is not installed. User still get the benefit of a new default, proper `string` dtype in 3.0 in all cases, but if they also want the performance improvements of the new string dtype, they need to have pyarrow installed. Then it's up to users to make that trade-off (and we can find ways to strongly encourage users to use pyarrow).\r\n>\r\n\r\nIIRC I also made this point in the original discussion but there was pushback to having the object backed StringDType as the default if pyarrow is not installed that included not only concerns about performance but also regarding different behavior depending on if a dependency was installed.  (The timelines for NumPy's StringDtype precluded that as an option to address the performance concerns)\r\n\r\nHowever, I did not push this point once the proposal was expanded to dtypes other that strings.\r\n \r\n> I would also like to suggest another potential idea to consider: we could adopt Arrow's _type_ (memory model) for strings, but without requiring _pyarrow_ the package. Building on @WillAyd's work in #54506 using nanoarrow to use bitmaps in pandas, we could implement a basic StringArray in a similar way, and implement the basic required features in pandas itself (things like getitem, take, isna, unique\/factorize), and then for the string-specific methods either use pyarrow if installed, or fallback to Python's string methods otherwise (or if we could vendor some code for this, progressively implement some string methods ourselves).\r\n\r\nDidn't we also discuss using, say nanoarrow? (Or am I mixing up the discussion on requiring pyarrow for the I\/O interface.)\r\n\r\nIf this wasn't discussed then a new\/further discussion around this option would add value ( https:\/\/github.com\/pandas-dev\/pandas\/issues\/57073#issuecomment-1925417591) especially since @WillAyd is actively working on this. ","Another advantage of building on top of nanoarrow is we would have the ability to implement our own algorithms to fit the needs of pandas. Here is a quick benchmark of the nanopandas `isna()` implementation versus pandas:\r\n\r\n```python\r\nIn [3]: import nanopandas as nanopd\r\n\r\nIn [4]: import pandas as pd\r\n\r\nIn [5]: arr = nanopd.StringArray([None, \"foo\"] * 1_000_000)\r\n\r\nIn [6]: ser = pd.Series([None, \"foo\"] * 1_000_000, dtype=\"string[pyarrow]\")\r\n\r\nIn [7]: arr.isna().to_pylist() == list(ser.isna())\r\nOut[7]: True\r\n\r\nIn [8]: %timeit arr.isna()\r\n10.7 \u00b5s \u00b1 45 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\nIn [9]: %timeit ser.isna()\r\n2 ms \u00b1 43.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```\r\n\r\nThat's about a 200x speedup. Of course its not a fair comparison because the pandas arrow extension implementation calls `to_numpy()`, but we in theory would have more flexibility to avoid that copy to numpy if we take on more management of the lower level.\r\n\r\n","going down this path is a tremendous amount of work - replicating pyarrow effectively \n\nthis should not be taken lightly - the purpose of having pyarrow as a required dependency is to reduce overall project requirements","The added workload is a very valid concern, though its definitely not on the scale of replicating pyarrow. We should just be using the nanoarrow API and not even managing memory since the nanoarrow C++ helpers can do that for you.","While I'm surely +1 on replacing PyArrow by a better implementation, I guess the proposal is not to implement the string algorithms in nanoarrow and make this the default for pandas 3.0, right?\r\n\r\nSo,I think in some months we can have pandas strings based in:\r\n\r\n- NumPy 2\r\n- nanoarrow\r\n- Arrow-rs\r\n\r\nBesides the existing ones based in NumPy objects and PyArrow.\r\n\r\nTo narrow the discussion, I think we need to decide somehow independently:\r\n\r\n- What do we do for pandas 3? I think the only reasonable options are require PyArrow and have pyarrow strings by default, or keep the status quo\r\n- What do we do long term? Do we commit to any of those implementations (pyarrow, numpy2, nanoarrow...)? Would it make sense to implement those as extensions \/ separate packages and make the default depend on what's installed? Like having a `pandas` metapackage that depends on `pandas-core` and `pandas-pyarrow`, so we would have users by default use pyarrow string types and users on wasm and lambda and anyone else who cares can install `pandas-core` and cherrypick dependencies.","Yes, I too think that full implementations may be too ambitious and may not even be necessary (performance-wise). I would think that these implementations would only be needed as fallbacks if we were to u-turn on the pyarrow requirement so that we could move forward with defaulting to arrow memory backed arrays for the dtypes listed in the PDEP for pandas 3.0.\r\n\r\nThe feedback from users is not unexpected and was discussed (other than the noise regarding warnings)\r\n\r\nAs @attack68 said, \"I would expect the approved PDEP to continue to steer the development until formally agreed otherwise.\", i.e. through the PDEP revision procedure.","Good idea to distinguish the shorter and longer term. But on the short term options:\r\n\r\n> What do we do for pandas 3? I think the only reasonable options are require PyArrow and have pyarrow strings by default, or keep the status quo\r\n\r\nNo, as I mentioned above, IMO we already have an obvious fallback for pyarrow, which is the object-dtype backed StringDtype. So I think making that the default (if pyarrow is not installed) is another very reasonable option for pandas 3.0.\r\n\r\n(personally, for me the most important thing is that I want to see `string` in the `df.dtypes` for columns with strings, so that we can stop explaining to users \"if you see \"object\" that's _probably_ a column with strings\". How it's exactly implemented under the hood is more of a detail, although I certainly want a more performant implementation as well). \r\n\r\nSimon brought up the point of having two implementations with slight differences in behaviour:\r\n\r\n> > If we want a simple alternative to keep pyarrow optional, I don't think we need to use numpy's new string dtype, though. We already have a object-dtype based StringDtype that can be the fallback when pyarrow is not installed. ...\r\n> \r\n> IIRC I also made this point in the original discussion but there was pushback to having the object backed StringDType as the default if pyarrow is not installed that included not only concerns about performance but also regarding different behavior depending on if a dependency was installed. (The timelines for NumPy's StringDtype precluded that as an option to address the performance concerns)\r\n\r\nAnd that's certainly a valid downside of this (I seem to remember we have had issues with this in the past with different behaviour when numexpr or bottleneck was installed and \"silently\" used by default).  \r\nI do wonder however if we have an idea of whether there are many behaviour differences we are aware of from testing and user reports of the arrow-backed StringDtype over the last years (I know one reported to pyarrow about a different upper case for \u00df in https:\/\/github.com\/apache\/arrow\/issues\/34599). I don't know if we have some skips\/special cases in our tests because of behaviour differences. \r\n\r\nThis might also be unavoidable in general, for other data types as well. It seems likely that also for numeric data we will have a numpy-based and pyarrow-based implementation side by side for some time, and also there there will likely be slight differences in behaviour.","> * What do we do for pandas 3? I think the only reasonable options are require PyArrow and have pyarrow strings by default, or keep the status quo\r\n\r\nYes, it is all to easy for the discussion to go off at tangents and this issue was opened with the suggestion of requiring Numpy 2.0+ \r\n\r\nIt appears there is no support for this at all?\r\n\r\nThe other question raised was whether anyone would vote differently now. For this, however, it does appear that several maintainers would. For those that would, it would be interesting to know explicitly what changes to the PDEP would be expected.\r\n\r\nOr to keep the status quo, we would somehow need a re-vote on the current PDEP revision.\r\n\r\nto be clear, without any changes to the PDEP, I would not change my vote. I do not regret the decision since my decision was based on better data types other than just strings and discussions around a better string data type do not fully address this.","> this issue was opened with the suggestion of requiring Numpy 2.0+. It appears there is no support for this at all?\r\n\r\nUnless we want to change the timeline for 3.0 (or delay the introduction of the string dtype to a later pandas release), I think it's not very realistic. To start, this change hasn't even landed yet on numpy main. I think it will also be annoying for pandas to strictly requires numpy >= 2.0 that soon (given numpy 2.0 itself is also a breaking release). Further, numpy only implements a subset of string kernels (for now), so even when using numpy for the memory layout, we will still need a fallback to Python objects for quite some of our string methods. \r\nGiven the last item, we also would want to keep the option to use PyArrow for strings as well, resulting in this double implementation anyway (with the possible behaviour differences). At that point, I think the easier option is to use the object-dtype fallback instead of a new numpy-2 based fallback.","Sorry, I missed that option @jorisvandenbossche. I personally don't like using `string[object]` by default, it doesn't add value in functionality and performance, and makes users have to learn more cumbersome things. But it's an option, so for pandas 3 we have:\r\n\r\n1. Continue with the agreed PDEP and require PyArrow\r\n2.  \"Cancel\" the PDED and continue with the `object` type\r\n3. Use the `string` dtype backed by NumPy objects\r\n\r\nFor long term we have:\r\n\r\n- NumPy 2 (I don't know much about this, but I guess we don't need an extra dependency, but we lose the nice things about Arrow, like interoperability)\r\n- Nanoarrow\r\n- Arrow-rs\r\n\r\nWhile I don't think we need a full rewrite of PyArrow, I think we need next things in any Arrow implementation we use to be functional (only string operations don't seem useful to me, as it'd still require PyArrow anyway to have a dataframe with Arrow columns):\r\n\r\n- Convert NumPy columns to Arrow\r\n- Build Arrow arrays from Python lists\r\n- Arrow CSV, Parquet... loaders\r\n\r\nI think for the nanoarrow approach we need all this, which does feel like almost rewriting PyArrow from scratch. Also, do we have a reason to think the new implementation will be smaller than Arrow? What do you think @WillAyd? Maybe I'm missing something here. \r\n\r\nWhile Arrow-rs doesn't have Rust bindings, Datafusion does. It seems to provide all or most of what we need to fully replace PyArrow. The Python package doesn't have dependencies and it requires 43Mb. Quite big, but less than half of PyArrow. The build should be just standard building, I think that was another issue with PyArrow. I think it's an option worth exploring.","> Sorry, I missed that option @jorisvandenbossche. I personally don't like using `string[object]` by default, it doesn't add value in functionality and performance, and makes users have to learn more cumbersome things.\r\n\r\nDon't worry ;) But can you clarify which cumbersome things users would have to learn? For the average user, whether we use a pyarrow string array or a numpy object array under the hood, that's exactly the same and you shouldn't notice that (except for performance differences, of course).  \r\nWhile it indeed doesn't give any performance benefits, IMO it gives a big functionality advantage in simply _having_ a string dtype, compared the current catch-all object dtype (that's one of the main reasons we added this object-dtype based StringDtype already in pandas 1.0, https:\/\/pandas.pydata.org\/docs\/dev\/whatsnew\/v1.0.0.html#dedicated-string-data-type). Functionality-wise, there is actually basically no difference between the object or pyarrow based StringArray (with the exception of a few corner cases where pyarrow doesn't have an implementation and the pyarrow-backed array still falls back to Python).","I was rereading your original comment and I realize now that your initial proposal is to make the default the PyArrow string type, except when PyArrow is not installed, right? In the last comment sounded like you wanted to always default to the string object type, that's what I find complex to learn (considering what users already know about object...).\r\n\r\nString PyArrow type as default and string object as fallback seems like a reasonable trade off to me.","> String PyArrow type as default and string object as fallback seems like a reasonable trade off to me.\r\n\r\nyes. we had this discussion in the original PDEP starting around https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#issuecomment-1618007396 following on from a discussion around the NumPy string type and yet still voted (as a team) on requiring PyArrow as a required dependency.\r\n\r\nWhat I see as new to the discussion is considering using nanoarrow to instead provide some sort of fallback option, not the default.\r\n\r\nI see this could potentially address some of the concerns around data types other than strings eg. https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#issuecomment-1518717765","To be clear at no point was I suggesting we rewrite pyarrow; what I showed is simply an extension array that uses arrow native storage. That is a much smaller scope than what some of the discussions here have veered towards\r\n\r\nI don't think any of the arguments raised in this discussion are a surprise and I still vote to stick with the PDEP. I think if that got abandoned but we still wanted Arrow strings without pyarrow then the extension array I showcased above _may_ be a reasonable fallback and _may_ even be easier to integrate than a \"string[object]\" fallback because at the raw storage level it fits the same mold as a pyarrow string array","> I don't think any of the arguments raised in this discussion are a surprise and I still vote to stick with the PDEP. I think if that got abandoned but we still wanted Arrow strings without pyarrow then the extension array I showcased above _may_ be a reasonable fallback and _may_ even be easier to integrate than a \"string[object]\" fallback because at the raw storage level it fits the same mold as a pyarrow string array\r\n\r\nThanks @WillAyd for elaborating.\r\n\r\nI think if the PDEP was revised to include something like this (not requiring pyarrow but if not installed defaulting to pyarrow memory backed array on array construction but limited functionality and advising users to install pyarrow) I would perhaps vote differently now.\r\n\r\nSo agree that, at this point in time, these alternatives perhaps only need discussion if enough people are strongly against requiring pyarrow as planned.","I'd like to better-understand how the numpy string will differ from a pyarrow string.  In particular, will converting between them be zero-copy?\r\n\r\nShort term, could\/should we update the docs and warning messages to say we will infer _a_ performant string dtype without specifically saying \"pyarrow\"?  If a lighter-weight drop-in does become available, this might make it easier to swap them out.\r\n","@jbrockmendel the NEP is really well laid out and goes over the memory footprint as well as comparison to PyArrow:\n\nhttps:\/\/numpy.org\/neps\/nep-0055-string_dtype.html#memory-layout-examples\n\nUnfortunately looks to be a different layout so I think there would have to be a copy. @ngoldbaum may have more insights though","Indeed, the memory layout is different, and zero-copy conversion won't be possible (for the new Arrow string_view type, a partial zero-copy conversion might be possible for the numpy -> arrow direction, but also not for arrow -> numpy).\r\n\r\n> Short term, could\/should we update the docs and warning messages to say we will infer a performant string dtype without specifically saying \"pyarrow\"? \r\n\r\nAt that point, I don't think it would make sense to have a warning (i.e. then we should just remove the warning). Because AFAIK the warning wasn't really introduced to warn about the behaviour change of having a string dtype, but specifically for the change in required dependencies.","> Unfortunately looks to be a different layout so I think there would have to be a copy. @ngoldbaum may have more insights though\n\nThis is true. However:\n\n* The memory layout is an implementation detail. If in the future it turns out that the arrow layout would be better for the ecosystem, we could straightforwardly change it.\n\n* We need to support mutable arrays, which adds a number of wrinkles, so it was much more natural to embed the concept of string data living somewhere undefined on the heap in the API for the dtype rather than assuming a fixed memory layout. Also if we do use the arrow layout and support mutable arrays, then in the most straightforward implementation enlarging a single string would lead to reallocating the entire array, which isn't great.\n\n* We have no-gil python coming on the horizon which gives us a chance to rethink mutability in numpy in general as part of coming up with a real thread safety story. Interfacing with rust leads to similar problems. One could envision adding support for immutable arrays to numpy or support for freezing already-created arrays. In that case, a natural choice for a string frozen array is exactly the arrow memory layout.","It's true that feedback from users is not unexpected and was discussed, but I'm not sure that such negative feedback from maintainers of other projects (e.g. scikit-learn and related) was.\r\nFor example, [this comment](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25896#issuecomment-1641841477) came after most of the pandas devs had already voted on the PDEP, and [this one](https:\/\/github.com\/fairlearn\/fairlearn\/pull\/1335#issuecomment-1905855895) was made after 2.2. It's true that they'd already commented on the PDEP issue, but that was a much milder \"it would be a pity if pyarrow becomes a mandatory dependency\".\r\n\r\nI just wanted to make sure people are aware of them.\r\n\r\nRegardless of your thoughts on the library they're now recommending people switch to, is \"we've made scikit-learn devs mad\" a good look for pandas?","As a reminder, during the discussion on PDEP-10, I wrote the following at https:\/\/github.com\/pandas-dev\/pandas\/pull\/52711#issuecomment-1620923231  :\r\n> MHO, it's better that we get some feedback, rather than none. The wording as proposed doesn't commit us to saying we will not require pyarrow if we get negative feedback - it just says that we will get the feedback, which gives us the possibility of delaying the requirement based on that feedback.\r\n\r\nI'd split the current feedback into 2 broad areas:\r\n1. People who don't want to see pyarrow required.\r\n2. People who don't like the deprecation message (or specifics about it, like having it start with `\\n`)\r\n\r\nI also think that one of the original reasons for this requirement was to reduce the development burden on the pandas team by having only one supported string type.  It now seems that we may not be able to avoid that.\r\n\r\nGiven the nature of how this discussion has evolved, I personally would vote to not require `pyarrow` and we have to live with a strategy of \"use pyarrow if available for strings, and use 'something else' if pyarrow is not available.\"  It's not clear to me what the 'something else' should be (Numpy 2.0, current `StringDtype`, current object implementation, `nanoarrow`, etc.) but I think the decision to delay the pyarrow requirement can be made independent of whatever fallback mechanism we choose.\r\n\r\nAnd if we do decide to delay requiring `pyarrow`, we can create a 2.3 without the deprecation warning.  One advantage of reversing our decision is that it shows we do listen to the community.\r\n","> String PyArrow type as default and string object as fallback seems like a reasonable trade off to me.\r\n\r\nIf I'm understanding right, then behavior can change based on whether pyarrow is installed even though the code does nothing to invoke pyarrow. E.g.\r\n\r\n```\r\nser = pd.Series([\"1\", None, \"2\"])\r\nprint(ser.iloc[1] == ser.iloc[1])\r\n```\r\n\r\ngives False with pyarrow installed, True when not. That seems quite undesirable.","> gives False with pyarrow installed, True when not. That seems quite undesirable.\r\n\r\nYah, this came up in the PDEP-10 discussion.  Different default behavior depending on whether pyarrow is installed is a maintenance nightmare.","As commented here, it may be noted that if portability to Linux distributions is an issue, the alternative of using arrow-rs instead of C++ libarrow may not be viable.\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/issues\/54466#issuecomment-1936846576","> If we want a simple alternative to keep pyarrow optional, I don't think we need to use numpy's new string dtype, though. We already have a object-dtype based StringDtype that can be the fallback when pyarrow is not installed. User still get the benefit of a new default, proper string dtype in 3.0 in all cases, but if they also want the performance improvements of the new string dtype, they need to have pyarrow installed. Then it's up to users to make that trade-off (and we can find ways to strongly encourage users to use pyarrow).\r\n\r\nThis seems to be the best option out of a set of bad options if we don't require arrow"],"labels":["Needs Discussion"]},{"title":"BUG:  DataFrameGroupBy.transform with engine='numba' reorder output by index","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ndf = pd.DataFrame({'vals': [0, 1, 2, 3], 'group': [0, 1, 0, 1]})\r\ndf.index = df.index.values[::-1] #reverse index\r\n\r\ndef foo(values, index):\r\n    return values \r\n\r\ndf.groupby('group')['vals'].transform(foo, engine='numba')\r\n# wrong output:\r\n# 3    3.0\r\n# 2    2.0\r\n# 1    1.0\r\n# 0    0.0\r\n# Name: vals, dtype: float64\n```\n\n\n### Issue Description\n\n`DataFrameGroupBy.transform` with `engine='numba'` gives results that are ordered by the index values (and type has changed which is less important). This gives the wrong order unless the index is monotonically increasing. From what I can see we get the correct `values` and `index` in the `foo` function (same ordering as the dataframe), but this are ordered by the `index` before concatenating the results to the final series. This differs from the behaviour when called without `engine='numba'` (see below).\n\n### Expected Behavior\n\nThe example above should produce the same results as\r\n```python\r\ndf.groupby('group')['vals'].transform(lambda x: x)\r\n\r\n```\r\nwith output\r\n```\r\n3    0\r\n2    1\r\n1    2\r\n0    3\r\nName: vals, dtype: int64\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, confirmed on main. Further investigations and an PRs to fix are welcome!\r\n\r\ncc @lithomas1 ","take"],"labels":["Bug","Groupby","numba","Transformations"]},{"title":"BUG: Index.join preserving names incorrectly from pandas-2.x","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nIn [2]: import pandas as pd\r\nIn [3]: pd.__version__\r\nOut[3]: '2.2.0'\r\nIn [4]: idx1 = pd.Index([1, 1, 2, 3, 4, 4], name='a')\r\nIn [5]: idx2 = pd.Index([2, 5, 3, 4], name='b')\r\nIn [6]: idx1.join(idx2)\r\nOut[6]: Index([1, 1, 2, 3, 4, 4], dtype='int64', name='a')\r\nIn [7]: idx2.join(idx1)\r\nOut[7]: Index([2, 5, 3, 4, 4], dtype='int64', name='b')\n```\n\n\n### Issue Description\n\nWhen two index objects have different names, in pandas-1.x versions the end result index would have `None` as index name. Now in pandas-2.x the result index is always preserving the existing index name it is being called upon.\n\n### Expected Behavior\n\n```python\r\nIn [2]: import pandas as pd\r\nIn [3]: pd.__version__\r\nIn [4]: idx1 = pd.Index([1, 1, 2, 3, 4, 4], name='a')\r\nIn [5]: idx2 = pd.Index([2, 5, 3, 4], name='b')\r\nIn [6]: idx1.join(idx2)\r\nOut[6]: Index([1, 1, 2, 3, 4, 4], dtype='int64')\r\nIn [7]: idx2.join(idx1)\r\nOut[7]: Index([2, 5, 3, 4, 4], dtype='int64')\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.10.2.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\npandas                : 2.2.0\r\nnumpy                 : 1.23.2\r\npytz                  : 2022.2.1\r\ndateutil              : 2.8.2\r\nsetuptools            : 60.2.0\r\npip                   : 21.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.5.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : 2.8.4\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 11.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["cc: @mroeschke ","When left join is performed and the left index contains all the necessary values required for the new sample space they are just returning the copy of left variable. \r\n\r\nIn  first example:\r\n```python\r\nIn [6]: idx1.join(idx2)\r\nOut[6]: Index([1, 1, 2, 3, 4, 4], dtype='int64', name='a')\r\n```\r\nThe newly generated index is the copy of idx1. That is why you see the name \"a\"\r\n\r\nAnd for:\r\n\r\n```python\r\nIn [7]: idx2.join(idx1)\r\nOut[7]: Index([2, 5, 3, 4, 4], dtype='int64', name='b')\r\n```\r\n\r\nWhen index does not contain values of idx2 they have to construct a new index object and they are assigning the name of the variable on which join operation is performed. That is why you see the name \"b\"\r\n\r\nIn the join operation I simply assigned name as \"None\" for these two conditions in the pandas library and I can see your desired results:\r\n\r\n**Test 1:**\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/16797137\/4f63a4cd-30f1-491e-bcc2-ca24a736eb5d)\r\n\r\n**Test 2:**\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/16797137\/1b1f7e91-530e-480f-b7a8-c1dad2ef539a)\r\n\r\nBut I believe pandas team has had this to be an expected behavior. As I can see in the code they have intentionally added the name to index in many conditions which cannot be a bug. \r\n\r\nI believe the name comes from what index is joined and how the join is performed.\r\n\r\nHere it is named \"b\" because it is a **left** join\r\n```python\r\nIn [7]: idx2.join(idx1)\r\nOut[7]: Index([2, 5, 3, 4, 4], dtype='int64', name='b')\r\n```\r\n\r\nIf I change this to **right** join I get \"a\"\r\n\r\n```python\r\nidx2.join(idx1, how=\"right\")\r\n>>> Index([1, 1, 2, 3, 4, 4], dtype='int64', name='a')\r\n```\r\n\r\nCould be an expected behavior but an experienced team member can review it for you.\r\n\r\nThank you,\r\nVisweswaran N \r\n"],"labels":["Bug","Reshaping","Index"]},{"title":"RLS: 3.0","body":"Estimated release date: April 2024, but it will be updated closer to that time.","comments":[],"labels":["Release"]},{"title":"DEP: Deprecate date_format=\"epoch\" in to_json","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [X] Removing existing functionality in pandas\n\n\n### Problem Description\n\nWhile `date_format=\"epoch\"` made sense when we exclusively handled ns-precision timestamps since the epoch, it has started to lose its meaning as we support other precisions. \r\n\r\nIf you try to roundtrip JSON produced with this argument you just have to guess what the precision is\n\n### Feature Description\n\nSee above\n\n### Alternative Solutions\n\nSee above\n\n### Additional Context\n\n_No response_","comments":[],"labels":["IO JSON","Deprecate"]},{"title":"BUG: Selecting columns from a DataFrame affects display of warnings","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport warnings\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\"A\": [0, 1], \"B\": [2, 3], \"C\": [4, 5]})\r\n\r\nfor _ in range(3):\r\n    # Normal behaviour (warning will display once).\r\n    warnings.warn(\"First warning.\")\r\n\r\nfor _ in range(3):\r\n    warnings.warn(\"Second warning.\")\r\n\r\n    # Select two columns from the DataFrame.\r\n    # This line seems to affect the behaviour of the second\r\n    # warning - it will display on all 3 iterations of the loop.\r\n    df = df[[\"A\", \"B\"]]\n```\n\n\n### Issue Description\n\nSelecting columns from a DataFrame appears to affect the normal behaviour of Python warnings.\r\n\r\nWhere a warning is raised repeatedly by a given line within a loop, the [default behaviour](https:\/\/docs.python.org\/3\/library\/warnings.html#the-warnings-filter) is that the warning will only be displayed the first time it is raised. This is what happens in the first loop above.\r\n\r\nHowever, in the second loop, the line which filters the columns of the DataFrame (`df = df[[\"A\", \"B\"]]`) seems to affect this behaviour, as this time the warning is displayed on each iteration of the loop, rather than just once. Interestingly, this behaviour occurs regardless of whether this filtering line appears before or after the second `warn()` statement.\r\n\r\nThe following is the output produced:\r\n\r\n```python\r\n..\\repeated_warnings.py:8: UserWarning: First warning.\r\n  warnings.warn(\"First warning.\")\r\n..\\repeated_warnings.py:11: UserWarning: Second warning.\r\n  warnings.warn(\"Second warning.\")\r\n..\\repeated_warnings.py:11: UserWarning: Second warning.\r\n  warnings.warn(\"Second warning.\")\r\n..\\repeated_warnings.py:11: UserWarning: Second warning.\r\n  warnings.warn(\"Second warning.\")\r\n```\r\n\r\nI've experimented with putting `warnings.resetwarnings()` at the start of the script and then examining the contents of the `warnings.filters` list immediately prior to the two `warn()` statements - I thought perhaps Pandas might be adding filters to the filters list. However, in each case the filters list seems to be empty, so that doesn't seem to be the problem.\r\n\r\nThis may be in some way related to Issue #55801, although that issue relates to creation of a DataFrame, rather than selecting columns from it.\n\n### Expected Behavior\n\nI would expect each warning to display only once:\r\n\r\n```python\r\n..\\repeated_warnings.py:8: UserWarning: First warning.\r\n  warnings.warn(\"First warning.\")\r\n..\\repeated_warnings.py:11: UserWarning: Second warning.\r\n  warnings.warn(\"Second warning.\")\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 154 Stepping 4, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_Ireland.1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. I believe this is due to https:\/\/github.com\/python\/cpython\/issues\/73858. The example hits this line:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/622f31c9c455c64751b03b18e357b8f7bd1af0fd\/pandas\/core\/common.py#L244\r\n\r\nAs commented there, that catch_warnings can be removed once NumPy 1.24 is the minimum version. However this will continue to be a general issue until the Python bug is fixed.","If we're waiting for a fix to come from CPython, then this is a duplicate of #55801, but if we're wanting to do something in pandas, it's not. Leaving this open for the time being."],"labels":["Bug","Warnings","Upstream issue"]},{"title":"BUG: ht.PyObjectHashTable swallows exception","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nfrom pandas._libs import hashtable as ht\r\n\r\nclass testkey:\r\n    def __init__(self, val):\r\n        self._val = val;\r\n        self.throw = False\r\n        \r\n    def __hash__(self):\r\n        if self.throw:\r\n            import sys\r\n            print(\"exiting\")\r\n            sys.exit(0)\r\n        return self._val.__hash__()\r\n\r\n    def __eq__(self, other):\r\n        return self._val.__eq__(other._val)\r\n\r\n    def __repr__(self):\r\n        return self._val.__repr__()\r\n\r\ny1 = testkey(\"hello\")\r\ny2 = testkey(\"hello2\")\r\n\r\nx = ht.PyObjectHashTable()\r\n\r\nx.set_item(y1, 123)\r\nx.set_item(y2, 456)\r\n\r\nprint(f\"val1: {x.get_item(y2)}\")\r\ny2.throw = True\r\n\r\ntry:\r\n    print(f\"val2: {x.get_item(y2)}\")\r\nexcept KeyError:\r\n    print(\"missed key\")\r\n\r\ny2.throw = False\r\nprint(f\"val3: {x.get_item(y2)}\")\r\n```\r\n\r\nPrints:\r\n\r\n```\r\nval1: 456\r\nexiting\r\nmissed key\r\nval3: 456\r\n```\r\n\r\nor (I guess this depends on some random hash value used when the `__hash__` throws)\r\n\r\n```\r\nval1: 456\r\nexiting\r\nval2: 456\r\nval3: 456\r\n```\r\n\r\nWhile I would expect it to print\r\n\r\n```\r\nval1: 456\r\nexiting\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nI'm running to an issue in prod where I have a custom signal handler for SIGINT, which, when raised, will raise a `SystemExit` exception. Now, sometimes this `SystemExit` does not result in the application stopping, and the application keeps running. \r\n\r\nDebugging it so far, it seems to be this always happens when we're running some pandas code. It's a bit hard to debug, but I think the issue happens due to `PyObjectHashTable`'s handling of exceptions during `__hash__` and `__eq__` calls. It looks like these are swallowed (or replaced by another exception) under some conditions. I'm not too familiar with the Pandas code, so it was a bit hard to follow where it exactly goes wrong. The example attached, however, tries to show this behavior when we throw in the `__hash__` during `PyObjectHashTable.get_item()`. In this case the `SystemExit` exception gets replaced by a `KeyError`.\r\n\r\n### Expected Behavior\r\n\r\nI expect exceptions thrown during `__hash__` or `__eq__` to not be swallowed.\r\n\r\n### Installed Versions\r\n\r\n```\r\n>>> import pandas\r\npand>>> pandas.show_versions(\r\n... )\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.6.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 22.1.0\r\nVersion               : Darwin Kernel Version 22.1.0: Sun Oct  9 20:19:12 PDT 2022; root:xnu-8792.41.9~3\/RELEASE_ARM64_T6020\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 58.0.4\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n```","comments":["Thanks for the report, confirmed on main. Further investigations and PRs to fix are welcome.\r\n\r\nDid you also experience this before 2.2? I was able to reproduce on 2.1.x, but wanted to make sure.","Thanks for looking into it.\r\n\r\nOriginally saw it on 1.4.4. My hunch it\u2019s due to this branch but honestly\r\nnot familiar enought with any of this to be sure:\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/_libs\/include\/pandas\/vendored\/klib\/khash_python.h#L331\r\n\r\nOn Wed, 24 Jan 2024 at 23:30, Richard Shadrach ***@***.***>\r\nwrote:\r\n\r\n> Thanks for the report, confirmed on main. Further investigations and PRs\r\n> to fix are welcome.\r\n>\r\n> Did you also experience this before 2.2? I was able to reproduce on 2.1.x,\r\n> but wanted to make sure.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/pandas-dev\/pandas\/issues\/57052#issuecomment-1909019037>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AA35X2IU5MDLYNZ3QQF77MDYQGDQBAVCNFSM6AAAAABCIVSZLSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSMBZGAYTSMBTG4>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n","Good find - I do believe that is causing the issue. That said, the error raised from Python code needs to be propagated appropriately through the C layer. It looks like something like this is done in `pandas._libs.parsers.raise_parser_error`. I tried replicating here, but it seems like that logic does not handle system exit."],"labels":["Bug","Error Reporting","hashing"]},{"title":"DOC: Add notes to nullable types documentation about pd.NA column type","body":"- [x] closes #49201\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Thanks for the contribution @FilipRazek. Changes look reasonable given the discussion in the issue, but I'm not so sure what's proposed in the issue makes sense. \r\n\r\n@killerrex do you mind expanding on why is it useful to create a column with all missing values?\r\n\r\nI'm personally unsure about how useful this will be to users, seems to add more noise and confusion than value in my opinion.\r\n\r\nCC: @rhshadrach ","> do you mind expanding on why is it useful to create a column with all missing values?\r\n\r\nI've created columns by starting with all NA values, filling in certain indices from one or more sources, and then used `ffill` and\/or `bfill`.\r\n\r\nWhether or not it may be particularly useful, it can be done, and I think it can be surprising that the dtype results in object.\r\n\r\n> seems to add more noise and confusion\r\n\r\nNoise I can see, but I'm not sure what is meant by confusion here.\r\n\r\nBut no strong opinion here if there is any opposition.","By confusion I meant that users may wonder why they'd like to create a column with just `NA`. I guess the addition will be useful and clear to people who tried to do it before, but may leave more questions than answers for someone who didn't have the need.\r\n\r\nWhat I think it makes more sense is to show the full use case here. Instead of saying `when creating a column with NA values`, we could show the whole example `when creating a new column starting with NA values and the filling the present values, the type would be set to object in the df['new_col'] = pd.NA and performance on this column will be worse than with the appropriate type. It's better to use df['new_col'] = pd.Series(pd.NA, dtype=Int64)`. Surely it can be phrased better, but I think this shows what I'd do.\r\n\r\nMaybe I'm just overcomplicating things, feel free to go ahead with this if you think it's better. But I think in particular for beginners it'd be better to show a more real example.","> @killerrex do you mind expanding on why is it useful to create a column with all missing values?\r\n\r\nI used it when I fill a column progressively, like:\r\n```python\r\ndf: pd.DataFrame = load_the_big_data()\r\n\r\ndf['value'] = pd.Series(dtype='Int64')\r\n\r\nif calc_x:\r\n    mask = df['Col_A'] == 'x'   # <== Example row selection, may be really complex\r\n    df.loc[mask, 'value'] = complex_function(df.loc[mask, :])\r\n\r\nif calc_y:\r\n    mask = df['Col_B'] == 'y'\r\n    df.loc[mask, 'value'] = other_complex_function(df.loc[mask, :])\r\n\r\n# etc etc\r\n```\r\nIt is specially useful if later I need to plot that column (as the NA are not plotted) but also many times the fact that a column is not calculated is also meaningful.\r\n\r\nSometimes you can do the same filling the column with a canary value, but specially when the function is part of a library used in other projects it is not easy or possible to find an appropriate value.\r\n\r\nThe other main use I found is when I need to use a library that expect certain columns in the input dataframe and for the case I am using the concept of that column is not applicable (for example in satellite data analysis, for the same constellation not all the satellites have the same instruments)\r\nInitialising that sparse columns to NA guarantees that if the data is used unadvertily, the outputs are going to be NA and not some wrong value.\r\n\r\n","Thanks @datapythonista. My issue with your wording, albeit quite a minor one:\r\n\r\n> when creating a new column starting with NA values and the filling the present values\r\n\r\nis that this still holds even if you don't fill in the values. As a result, I find this a bit misleading. In more generality, we don't really know why users might create a column of all NA values - I gave a use case I've come across but there might be others.\r\n\r\nWhat do you think of wording like the following?\r\n\r\n> If you desire to create a column of all NA values...","How about this;\r\n`If you decide to create a column of NA values (for example to fill them later), the type would be set to object in the new column.\r\nThe performance on this column will be worse than with the appropriate type. It's better to use df['new_col'] = pd.Series(pd.NA, dtype=Int64) than df['new_col'] = pd.NA`\r\nWould this correspond to your needs?","I think that looks good @FilipRazek. Some minor tweaks\r\n\r\n> If you create a column of NA values (for example to fill them later) with `df['new_col'] = pd.NA`, the dtype would be set to object in the new column. The performance on this column will be worse than with the appropriate type. It's better to use `df['new_col'] = pd.Series(pd.NA, dtype=Int64)` (or another dtype as you desire) ."],"labels":["Docs"]},{"title":"BUG: Pandas 2.2 breaks SQLAlchemy 1.4 compatibility","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n# with SQL Alchemy 1.4 installed the following example fails:\r\nimport pandas as pd\r\nfrom sqlalchemy import create_engine\r\ncon = create_engine('sqlite:\/\/\/:memory:')\r\ndf = pd.DataFrame({'a': [0, 1, 2, 3]})\r\ndf.to_sql('test', con)\r\n=> AttributeError: 'Engine' object has no attribute 'cursor'\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhile Pandas 2.2 Release Notes declare sqlalchemy > 2.0.0 a minimum requirement, sqlalchemy 1.4 is still a widely used and maintained release. This change thus likely breaks a lot of existing code. Upgrading from sqlalchemy 1.4 to 2.0 is not trivial and therefore many projects using both pandas and sqlalchemy will not be able to upgrade to pandas > 2.2 easily. \r\n\r\n### Expected Behavior\r\n\r\nSQLAlchemy 1.4 compatibility should be continued, and 1.4 support should be deprecated before removal.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Further testing indicates the compatibility issue may only apply to SQLite. For example for MySQL + sqlalchemy 1.4 this still works:\r\n\r\n``` python\r\nimport pandas as pd\r\nfrom sqlalchemy import create_engine\r\ncon = create_engine('mysql+pymysql:\/\/root:password@localhost:3306\/test')\r\ndf = pd.DataFrame({'a': [0, 1, 2, 3]})\r\ndf.to_sql('test', con, if_exists='replace')\r\n```\r\n\r\nRunning mysql using docker `docker run --name=mysql -e MYSQL_ROOT_PASSWORD=password -p 3306:3306 mysql`","> While Pandas 2.2 Release Notes declare sqlalchemy > 2.0.0 a minimum requirement, sqlalchemy 1.4 is still a widely used and maintained release. This change thus likely breaks a lot of existing code. Upgrading from sqlalchemy 1.4 to 2.0 is not trivial and therefore many projects using both pandas and sqlalchemy will not be able to upgrade to pandas > 2.2 easily.\r\n\r\nThis seems reasonable to me, since it looks like sqlalchemy 1.4 is still getting releases, so it'd probably make sense to support it at least until sqlalchemy 2.1 comes out.\r\n\r\nUsually, we bump minimum versions of dependencies once they become around 2 years old (and sqlalchemy 1.4 is already 3 years old), but I think was it's probably fine to make an exception for sqlalchemy this time.\r\n\r\ncc @mroeschke ","Hi there. FWIW, we are hitting the same error on our CI with SQLAlchemy 1.4 on a 3rd-party SQLAlchemy dialect, see [run #7705314750](https:\/\/github.com\/crate\/crate-python\/actions\/runs\/7705314750\/job\/20999042942?pr=607#step:4:531).\r\n\r\n- https:\/\/github.com\/crate\/crate-python\/pull\/607\r\n","@mroeschke said at https:\/\/github.com\/pandas-dev\/pandas\/issues\/57137#issuecomment-1915754158:\r\n\r\n> The minimum sqlalchemy version for pandas 2.2 is 2.0.\r\n> -- https:\/\/pandas.pydata.org\/docs\/whatsnew\/v2.2.0.html#increased-minimum-versions-for-dependencies\r\n\r\nSo, I think it is clear that SQLAlchemy 1.4 will no longer be supported.\r\n","Since there have been several reports about Sqlalchemy 1.4, I would be open to supporting that again. Testing and prior compatibility code will needed to be added back in a PR.","Thanks Matthew. For us it will be totally fine to [have pandas >= 2.2 with sqlalchemy >= 2 only](https:\/\/github.com\/crate\/crate-python\/commit\/272bf18a79), just to clarify. When others from the community don't have an easy chance to update, for any reasons, I surely also see that the need for backward-compatibility is being expressed.\r\n","snowflake-sqlalchemy pins sqlalchemy to <2.0.0 (see https:\/\/github.com\/snowflakedb\/snowflake-sqlalchemy\/issues\/452) which leads to a version conflict with pandas 2.2.\r\n\r\nFor reference, https:\/\/github.com\/pandas-dev\/pandas\/pull\/55524 introduced the version bump. @mroeschke does any pandas code require sqlalchemy features only available in 2.0? According to https:\/\/www.sqlalchemy.org\/download.html, 1.4 still receives critical fixes, last 1.4 release is from Jan 3, 2024: https:\/\/pypi.org\/project\/SQLAlchemy\/1.4.51\/\r\n\r\nOf course, I'd prefer other projects move their requirements up rather than pandas staying on old versions, but maybe in this case it's the simplest way to postpone the version upgrade?\r\n\r\nBy the way, I found the error message misleading with regards to the issue. \r\n\r\nIt looks like in https:\/\/github.com\/pandas-dev\/pandas\/blob\/2.2.x\/pandas\/io\/sql.py#L900 the `errors=\"ignore\"` suppresses the actual error message that indicates the version conflict, so one ends up with a cryptic error message because the code fails later (pip did not raise the version conflict either, I guess because it's an optional dependency)","snowflake-sqlalchemy also seems to throw this error when you use the Snowflake recommended way to create a connection when using [private keys](https:\/\/docs.snowflake.com\/en\/developer-guide\/python-connector\/sqlalchemy#key-pair-authentication-support).\r\n\r\nI suspect it's the way URL works, as it's a snowflake.sqlalcheny import? `from snowflake.sqlalchemy import URL`\r\n\r\nNot sure if this a Snowflake SQLAlchemy issue or here though, apologies for the noise.","Just to add more context here, Airflow is pinning their newest 2.8 images to pandas < 2.2:\r\n\r\nhttps:\/\/github.com\/apache\/airflow\/pull\/37748\r\n\r\nIt does not seem that they are likely to drop their SQLAlchemy 1.4 support soon, since they have a few dependencies that are not SQLAlchemy 2.0 ready (as of December 2023):\r\n\r\nhttps:\/\/github.com\/apache\/airflow\/issues\/28723\r\n\r\nGoogle Composer released earlier this month their 2.6 version (Airflow 2.6-based), and I found out today when I upgraded that their choice to mix pandas 2.2.0 and SQLAlchemy 1.4.51 created issues running dags for me:\r\n\r\nhttps:\/\/issuetracker.google.com\/issues\/327158030","FYI: We just updated constraints for 2.8.2 and updated images for Airflow 2.8.2 to keep pandas 2.1.4. That will not prevent the users hitting the same problem if they \"just install\" airfow from scratch with the new pandas, but if they follow the recommended way of installing airflow with constraints, they will be good:  https:\/\/airflow.apache.org\/docs\/apache-airflow\/stable\/installation\/installing-from-pypi.html","Wanted to vote in favor of maintaining backward compatibility. We are also using a snowflake database behind sqlalchemy and are having this problem with the versioning. It would be nice if pandas could issue a deprecation error with potential to override first before fully abandoning support for SA 1.4"],"labels":["Regression","IO SQL","Compat","Dependencies"]},{"title":"DEPS: Make pytz an optional dependency","body":"- [ ] closes #34916 (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\n\r\nThis makes pandas return `zoneinfo.ZoneInfo` objects by default from string inputs","comments":["might be easier to break into steps e.g. one PR just changing which UTC, one just changing FixedOffset, then the hard one for the general case.","> one PR just changing which UTC, one just changing FixedOffset\r\n\r\nThese are already changed (I think you changed it for 2.0), so now this is tackling the general case."],"labels":["Timezones","Dependencies"]},{"title":"fix: Expose `to_pandas_kwargs` in `read_parquet` for pyarrow engine","body":"- [x] closes #49236\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["Hmm this design feels weird to me. What arguments would realistically be passed through this? ","In the underlying issue, the OP needs to send `timestamp_as_object=True` to `.to_pandas`, to control how the data is formatted as a dataframe.\r\nIf it is not a real concern, I can close this PR.","I am +1 on this, the most important is probably types_mapper, which is a real PITA that this is not possible at the moment","Something like `read_pandas(to_pandas_kwargs=...)`  feels strange to me from an API perspective, and has the downside of really intertwining the APIs of the libraries. If we later decide to change the allowable `to_pandas_kwargs` or pyarrow decides to change their API it would be quite messy\r\n\r\nWhy not just encourage users to call `pa.parquet.read_table` in such a case directly?","Would updating the documentation be sufficient in your opinion?"],"labels":["IO Parquet"]},{"title":"BUILD: Can't found module wrapt nor detect right meson version","body":"### Installation check\n\n- [X] I have read the [installation guide](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/getting_started\/install.html#installing-pandas).\n\n\n### Platform\n\nLinux-6.1.67-gentoo-x86_64-x86_64-AMD_Ryzen_7_5800H_with_Radeon_Graphics-with-glibc2.38\n\n### Installation Method\n\npip install\n\n### pandas Version\n\nGit\n\n### Python Version\n\n3.11.7\n\n### Installation Logs\n\nInstall Logs\r\n<details>\r\nCollecting git+https:\/\/github.com\/pandas-dev\/pandas\r\n  Cloning https:\/\/github.com\/pandas-dev\/pandas to \/tmp\/pip-req-build-1a98i_co\r\n  Running command git clone --filter=blob:none --quiet https:\/\/github.com\/pandas-dev\/pandas \/tmp\/pip-req-build-1a98i_co\r\n  Resolved https:\/\/github.com\/pandas-dev\/pandas to commit 72814750548b5ead5b08cd9b90d56a23c9a520ea\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [5 lines of output]\r\n      pip_system_certs: ERROR: could not register module: No module named 'wrapt'\r\n      \r\n      meson-python: error: Could not find meson version 0.63.3 or newer, found pip_system_certs: ERROR: could not register module: No module named 'wrapt'\r\n      meson-python: error: 1.2.1\r\n      meson-python: error: .\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n<\/details>\r\n\r\nPython Meson + wrapt (pip freeze in venv)\r\n<details>\r\nmeson==1.3.1\r\n\r\nwrapt==1.16.0\r\n<\/details>\r\n\r\nMeson + wrapt (on system)\r\n<details>\r\ndev-python\/wrapt-1.16.0\r\n\r\ndev-build\/meson-1.3.0\r\n<\/details>","comments":["Are you installing from source, or trying to install from a wheel?\r\n\r\nCan you also provide your install command?","Hi, here the commands:\r\n\r\n```bash\r\npip install git+https:\/\/github.com\/pandas-dev\/pandas\r\n```\r\n\r\nThe package ```wheel``` is installed on the python env.","Are you able to install ``meson-python`` in your venv with pip?\r\n\r\ne.g. does ``pip install meson-python`` work?","Hi yes:\r\n\r\n```bash\r\npip install --upgrade meson-python\r\nRequirement already satisfied: meson-python in \/home\/pipe\/pyenv\/lib\/python3.11\/site-packages (0.15.0)\r\nRequirement already satisfied: meson>=0.63.3 in \/home\/pipe\/pyenv\/lib\/python3.11\/site-packages (from meson-python) (1.3.1)\r\nRequirement already satisfied: pyproject-metadata>=0.7.1 in \/home\/pipe\/pyenv\/lib\/python3.11\/site-packages (from meson-python) (0.7.1)\r\nRequirement already satisfied: packaging>=19.0 in \/home\/pipe\/pyenv\/lib\/python3.11\/site-packages (from pyproject-metadata>=0.7.1->meson-python) (23.1)\r\n```","That's strange.\r\n\r\nIf you install all the build dependencies for pandas manually beforehand(listed [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/d928a5cc222be5968b2f1f8a5f8d02977a8d6c2d\/pyproject.toml#L5-L15)), are you able to install pandas with the ``--no-build-isolation`` flag passed to the pip install command?\r\n\r\nYour error message seems very strange to me btw - I haven't seen this before. The message makes me want to suspect that there's a broken meson\/meson-python installation somewhere.\r\n\r\nalso cc @eli-schwartz from meson","I'm finding it difficult to interpret the output which pip caught from meson-python... it superficially looks like meson-python tried to check what minimum version of meson was available and instead of detecting\n\n> great, you have meson version\n> ```\n> 1.3.1\n> ```\n\nIt found\n\n> Oh no, you have meson version\n> ```\n> pip_system_certs: ERROR: could not register module: No module named 'wrapt'\n> ```\n\nWhich is, erm, dunno what to say about that. What is pip_system_certs in this context anyway?","The name itself comes from https:\/\/pypi.org\/project\/pip-system-certs\/ which is a monkey-patching software that uses wrapt, but why is it interjecting itself here...","Hi! without isolation:\r\n\r\n```\r\npip install --no-build-isolation  git+https:\/\/github.com\/pandas-dev\/pandas\r\nCollecting git+https:\/\/github.com\/pandas-dev\/pandas\r\n  Cloning https:\/\/github.com\/pandas-dev\/pandas to \/tmp\/pip-req-build-k0onpkj_\r\n  Running command git clone --filter=blob:none --quiet https:\/\/github.com\/pandas-dev\/pandas \/tmp\/pip-req-build-k0onpkj_\r\n  Resolved https:\/\/github.com\/pandas-dev\/pandas to commit d928a5cc222be5968b2f1f8a5f8d02977a8d6c2d\r\n  Preparing metadata (pyproject.toml) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [10 lines of output]\r\n      + meson setup \/tmp\/pip-req-build-k0onpkj_ \/tmp\/pip-req-build-k0onpkj_\/.mesonpy-08ey_y7x -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=\/tmp\/pip-req-build-k0onpkj_\/.mesonpy-08ey_y7x\/meson-python-native-file.ini\r\n      The Meson build system\r\n      Version: 1.3.1\r\n      Source dir: \/tmp\/pip-req-build-k0onpkj_\r\n      Build dir: \/tmp\/pip-req-build-k0onpkj_\/.mesonpy-08ey_y7x\r\n      Build type: native build\r\n      \r\n      ..\/meson.build:5:13: ERROR: Command `\/home\/pipe\/pyenv\/bin\/python3.11 \/tmp\/pip-req-build-k0onpkj_\/generate_version.py --print` failed with status 1.\r\n      \r\n      A full log can be found at \/tmp\/pip-req-build-k0onpkj_\/.mesonpy-08ey_y7x\/meson-logs\/meson-log.txt\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\nFor some reason, the pip install path does not exists, so I run it with ```--log logs.txt``` to extract them, I'm appending the logs.\r\n[logs.txt](https:\/\/github.com\/pandas-dev\/pandas\/files\/14064986\/logs.txt)\r\n","You can get the log file by using the pip option `-Cbuilddir=$PWD\/build_dir\/` if I recall correctly. By default pip deleted everything it is aware of on a failed build, this tells meson-python to configure meson with a persistent build directory.","mmm...\r\n\r\n```\r\nBuild started at 2024-01-26T10:45:27.354015\r\nMain binary: \/home\/pipe\/pyenv\/bin\/python3.11\r\nBuild Options: -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md -Dvsenv=True --native-file=\/tmp\/build_dir\/meson-python-native-file.ini\r\nPython system: Linux\r\nThe Meson build system\r\nVersion: 1.3.1\r\nSource dir: \/tmp\/pip-req-build-9vzgvbf0\r\nBuild dir: \/tmp\/build_dir\r\nBuild type: native build\r\nRunning command: \/home\/pipe\/pyenv\/bin\/python3.11 \/tmp\/pip-req-build-9vzgvbf0\/generate_version.py --print\r\n--- stdout ---\r\n\r\n--- stderr ---\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/pip-req-build-9vzgvbf0\/generate_version.py\", line 8, in <module>\r\n    import versioneer\r\nModuleNotFoundError: No module named 'versioneer'\r\n\r\n\r\n\r\n..\/pip-req-build-9vzgvbf0\/meson.build:5:13: ERROR: Command `\/home\/pipe\/pyenv\/bin\/python3.11 \/tmp\/pip-req-build-9vzgvbf0\/generate_version.py --print` failed with status 1.\r\n```\r\n\r\nSo.. meson forgot to add a dep?","No, but if you disable build isolation you need to manually install dependencies.","No idea what to say....\r\n\r\nInstalling without isolation works, first need to install ```versioneer``` and ```cython``` from pip, then pandas is installed correctly.\r\n\r\nStill, with isolation fails with the weird ```wrapt``` error....","here the logs with debug mode\r\n\r\n[debug.txt](https:\/\/github.com\/pandas-dev\/pandas\/files\/14066011\/debug.txt)\r\n","Ok, marking this one as a closing candidate then since you seem to be able to install pandas from source (even if it forces the use of no-build-isolation).\r\n\r\nYou might want to take a look at what's going on with pip-system-certs on your system.","I was able to do a reprex, is little big, I'll reduce it and then show it here, I was able to confirm is a version on some packages issue.","Oks, finally steps to reproduce it:\r\n\r\n```bash\r\npython3 -m venv venv\r\nsource venv\/bin\/activate\r\npip install pip-system-certs==4.0\r\npip install pip install git+https:\/\/github.com\/pandas-dev\/pandas\r\n```\r\n\r\nWhat is the only difference with my system? My system does not have that package! seems only fails if is installed."],"labels":["Build","Needs Info","Closing Candidate"]},{"title":"BUG: Wrong column types when query has no rows in read_sql","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport sqlalchemy\r\n\r\nengine = sqlalchemy.create_engine(\"sqlite:\/\/\")\r\nconn.execute(sqlalchemy.text(\"CREATE TABLE sample (col REAL);\"))\r\nconn.commit()\r\n\r\nimport pandas\r\nret = pandas.read_sql(\"SELECT * FROM sample;\", conn)\r\nret[\"col\"].dtype\r\n\r\n#This should be float64!!\r\ndtype('O')\n```\n\n\n### Issue Description\n\nWhen we request a table, and has no rows all the columns types does not match with the original one.\n\n### Expected Behavior\n\nKeep the original column types in dtype.\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.1.67-gentoo-x86_64\r\nVersion               : #1 SMP PREEMPT_DYNAMIC Wed Dec 13 09:49:24 -03 2023\r\nmachine               : x86_64\r\nprocessor             : AMD Ryzen 7 5800H with Radeon Graphics\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : es_CL.utf8\r\nLOCALE                : es_CL.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3\r\ndateutil              : 2.8.2\r\nsetuptools            : 67.8.0\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.3\r\nhtml5lib              : 1.1\r\npymysql               : None\r\npsycopg2              : 2.9.7\r\njinja2                : 3.1.2\r\nIPython               : 8.14.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.7.1\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 15.0.0\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.1\r\nsqlalchemy            : 2.0.23\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : 2.4.0\r\npyqt5                 : None\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"PERF: Slow Series.plot() for DatetimeIndex for pandas 2.2.0 compared to 2.1.4","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this issue exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this issue exists on the main branch of pandas.\n\n\n### Reproducible Example\n\nUsing pandas 2.2.0 I'm experiencing very slow plotting of pd.Series objects with DatetimeIndex. I'm comparing below the time to plot using 2.2.0 and 2.1.4, as well as using matplotlib ax.plot().\r\n\r\nCould this be related to the date formatting that pandas applies to the matplotlib axis?\r\nUsing an integer index (pd.RangeIndex(start=0, stop=500_000)) does not result in slow Series.plot() with pandas 2.2.0\r\n\r\nRelated, but not to the possible 2.2.0 regression: https:\/\/github.com\/pandas-dev\/pandas\/issues\/50355\r\n\r\n```python\r\nimport time\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\nts_index = pd.date_range('2020-01-01 00:00', periods=500_000, freq='10min')\r\n\r\ns_a = pd.Series(data=1, index=ts_index)\r\ns_b = pd.Series(data=2, index=ts_index)\r\n\r\nt_s1a = time.time()\r\ns_a.plot()\r\nprint(f\"Time elapsed Series.plot() for s_a: {(time.time()-t_s1a):.2f} seconds\")\r\nt_s1b = time.time()\r\ns_b.plot()\r\nprint(f\"Time elapsed Series.plot() for s_b: {(time.time()-t_s1b):.2f} seconds\")\r\n\r\nfig, ax = plt.subplots()\r\nt_s2 = time.time()\r\nax.plot(s_a.index, s_a.to_numpy())\r\nax.plot(s_b.index, s_b.to_numpy())\r\nprint(f\"Time elapsed matplotlib, both series: {(time.time()-t_s2):.2f} seconds\")\r\n\r\nt_s3 = time.time()\r\ns_b.plot(ax=ax) # this is now fast\r\nprint(\"Time elapsed Series.plot() on existing plt axis with matplotlib line plot: \"\r\n      f\"{(time.time()-t_s3):.2f} seconds\")\r\n\r\n\r\nfig, ax = plt.subplots()\r\nt_s4 = time.time()\r\ns_a.plot(ax=ax)\r\nprint(\"Time elapsed Series.plot() on existing plt axis without matplotlib line plot: \"\r\n      f\"{(time.time()-t_s4):.2f} seconds\")\r\n```\r\n\r\nOutputs for pandas 2.2.0:\r\n```\r\nTime elapsed Series.plot() for s_a: 13.28 seconds\r\nTime elapsed Series.plot() for s_b: 17.23 seconds\r\nTime elapsed matplotlib, both series: 0.09 seconds\r\nTime elapsed Series.plot() on existing plt axis with matplotlib line plot: 0.63 seconds\r\nTime elapsed Series.plot() on existing plt axis without matplotlib line plot: 21.64 seconds\r\n```\r\n\n\n### Installed Versions\n\nEnvironment with pandas 2.2.0\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : f538741432edf55c6b9fb5d0d496d2dd1d7c2457\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19045\r\nmachine               : AMD64\r\nprocessor             : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_Europe.1252\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n\r\nEnvironment with pandas 2.1.4\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 186 Stepping 3, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_Europe.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.3\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n<\/details>\r\n\n\n### Prior Performance\n\nOutput for pandas 2.1.4:\r\n```\r\nTime elapsed Series.plot() for s_a: 1.77 seconds\r\nTime elapsed Series.plot() for s_b: 1.77 seconds\r\nTime elapsed matplotlib, both series: 0.05 seconds\r\nTime elapsed Series.plot() on existing plt axis with matplotlib line plot: 0.57 seconds\r\nTime elapsed Series.plot() on existing plt axis without matplotlib line plot: 1.69 seconds\r\n```","comments":["I'm seeing the same thing. Plotting a column with DatetimeIndex index is slow vs plotting with integer index. Can verify by dropping the DatetimeIndex before plot via .reset_index(drop=True)","Here is another example...about 33x slower on my machine (pandas 2.1.4 vs 2.2.1):\r\n\r\n```python\r\nimport time\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\nindex = pd.date_range('2020-01-01 00:00', periods=500_000, freq='10min')\r\ns1 = pd.Series(data=1, index=index)\r\n\r\nt1a = time.time()\r\ns1.plot()\r\nplt.show()\r\nt1z = time.time()\r\nprint(f\"Time elapsed Series.plot() for pandas {pd.__version__} datetime index: {(t1z-t1a):.2f} seconds\")\r\n\r\ns2 = s1.reset_index(drop=True)\r\nt2a = time.time()\r\ns2.plot()\r\nplt.show()\r\nt2z = time.time()\r\nprint(f\"Time elapsed Series.plot() for pandas {pd.__version__} integer index: {(t2z-t2a):.2f} seconds\")\r\n```\r\n\r\nOutput with pandas 2.1.4:\r\n```\r\nTime elapsed Series.plot() for pandas 2.1.4 datetime index: 5.38 seconds\r\nTime elapsed Series.plot() for pandas 2.1.4 integer index: 0.12 seconds\r\n```\r\n\r\nOutput with pandas 2.2.1:\r\n```\r\nTime elapsed Series.plot() for pandas 2.2.1 datetime index: 178.90 seconds\r\nTime elapsed Series.plot() for pandas 2.2.1 integer index: 0.12 seconds\r\n```"],"labels":["Visualization","Performance","Regression"]},{"title":"QST: FutureWarning: Resampling with a PeriodIndex is deprecated, how to resample now?","body":"### Research\n\n- [X] I have searched the [[pandas] tag](https:\/\/stackoverflow.com\/questions\/tagged\/pandas) on StackOverflow for similar questions.\n\n- [X] I have asked my usage related question on [StackOverflow](https:\/\/stackoverflow.com).\n\n\n### Link to question on StackOverflow\n\nhttps:\/\/stackoverflow.com\/questions\/77862775\/pandas-2-2-futurewarning-resampling-with-a-periodindex-is-deprecated\n\n### Question about pandas\n\nPandas version 2.2 raises a warning when using this code:\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame.from_dict({\"something\": {pd.Period(\"2022\", \"Y-DEC\"): 2.5}})\r\n# FutureWarning: Resampling with a PeriodIndex is deprecated.\r\n# Cast index to DatetimeIndex before resampling instead.\r\nprint(df.resample(\"M\").ffill())\r\n\r\n#          something\r\n# 2022-01        2.5\r\n# 2022-02        2.5\r\n# 2022-03        2.5\r\n# 2022-04        2.5\r\n# 2022-05        2.5\r\n# 2022-06        2.5\r\n# 2022-07        2.5\r\n# 2022-08        2.5\r\n# 2022-09        2.5\r\n# 2022-10        2.5\r\n# 2022-11        2.5\r\n# 2022-12        2.5\r\n```\r\nThis does not work:\r\n```\r\ndf.index = df.index.to_timestamp()\r\nprint(df.resample(\"M\").ffill())\r\n\r\n#             something\r\n# 2022-01-31        2.5\r\n```\r\nI have PeriodIndex all over the place and I need to resample them a lot, filling gaps with ffill. \r\nHow to do this with Pandas 2.2?\r\n","comments":["This comes from #55968 , and here's the relevant issue https:\/\/github.com\/pandas-dev\/pandas\/issues\/53481\r\n\r\nI'd suggest to create a datetimeindex and ffill","I just hit this too.\r\nFor upsampling the `PeriodIndex` had a different result than the `DatetimeIndex`, in particular `PeriodIndex` would create a row for each new period in the old period:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns = pd.Series(1, index=pd.period_range(pd.Timestamp(2024, 1, 1), pd.Timestamp(2024, 1, 2), freq=\"d\"))\r\ns.resample(\"1h\").ffill()\r\n```\r\n\r\nThis would create a series including __ALL__ hours of 2024-01-02.\r\n\r\nIf, instead, we first convert to `DatetimeIndex`:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns = pd.Series(1, index=pd.period_range(pd.Timestamp(2024, 1, 1), pd.Timestamp(2024, 1, 2), freq=\"d\"))\r\nd.index = d.index.to_timestamp()\r\ns.resample(\"1h\").ffill()\r\n```\r\n\r\nThe output will only contain 1 hour of 2024-01-02.\r\nEven if the series is constructed directly with a `DatetimeIndex` (even one containing the frequency information) the result is the same:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ns = pd.Series(1, index=pd.date_range(pd.Timestamp(2024, 1, 1), pd.Timestamp(2024, 1, 2), freq=\"d\"))\r\ns.resample(\"1h\").ffill()\r\n```\r\n\r\nWill there be no way to obtain the _old_ behaviour of `PeriodIndex` in future versions?\r\nIMO upsampling is quite common and the way `PeriodIndex` implemented it is more usefull. It would be a shame to loose it.","> IMO upsampling is quite common and the way PeriodIndex implemented it is more usefull. It would be a shame to loose it.\r\n\r\n\r\nI agree. Now you'll need to do reindexing manually, while with periodIndex this was a one-liner.\r\nFurthermore resampling with a datetime index seems to change the data type (a bug?). Here some sample code:\r\n\r\n```Python\r\nimport pandas as pd\r\n\r\n# some sample data\r\ndata = {2023: 1, 2024: 2}\r\ndf = pd.DataFrame(list(data.values()), index=pd.PeriodIndex(data.keys(), freq=\"Y\"))\r\n\r\n# Old style resampling, just a one-liner\r\nold_style_resampling = df.resample(\"M\").ffill()\r\nprint(old_style_resampling)\r\nprint(type(old_style_resampling.iloc[0][0]))\r\n\r\n# Convert index to DatetimeIndex\r\ndf.index = pd.to_datetime(df.index.start_time)\r\nlast_date = df.index[-1] + pd.offsets.YearEnd()\r\ndf_extended = df.reindex(\r\n    df.index.union(pd.date_range(start=df.index[-1], end=last_date, freq=\"D\"))\r\n).ffill()\r\nnew_style_resampling = df_extended.resample(\"ME\").ffill()\r\nprint(new_style_resampling)\r\nprint(type(new_style_resampling.iloc[0][0]))\r\n```\r\n\r\nI also opt for keeping the periodIndex resampling.","Related to my comments in #56588, I think that this is another example where Period is being deprecated too fast without a clear replacement in mind."],"labels":["Usage Question"]},{"title":"BUG: DeprecationWarning shown for internal operations on subclassed Series and DataFrame","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport warnings\r\n\r\nimport pandas as pd\r\n\r\nwarnings.simplefilter('always', DeprecationWarning)\r\n\r\n\r\nclass MySeries(pd.Series):\r\n\r\n    @property\r\n    def _constructor(self):\r\n        return MySeries\r\n\r\n\r\nseries = MySeries({'A': 1, 'B': 2, 'C': 3})\r\n\r\nseries.loc[['A', 'B']]\n```\n\n\n### Issue Description\n\nThe above code will print \r\n```\r\nDeprecationWarning: Passing a SingleBlockManager to MySeries is deprecated and will raise in a future version. Use public APIs instead.\r\n```\r\nbecause ``Series._constructor_from_mgr`` calls ``self._constructor(mgr)`` if ``self._constructor`` is not a Series without catching that warning. \r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/72814750548b5ead5b08cd9b90d56a23c9a520ea\/pandas\/core\/series.py#L665-L673\r\n\r\nThe warning is not very helpful in this case, because it recommends to use public APIs, but only public APIs are used. There's also no way to fix it except for just filtering it out.\n\n### Expected Behavior\n\nPandas should not raise this warning when it is actually caused by an internal operation that cannot be changed by a user.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 72814750548b5ead5b08cd9b90d56a23c9a520ea\r\npython                : 3.12.1.final.0\r\npython-bits           : 64\r\nOS                    : Windows\r\nOS-release            : 10\r\nVersion               : 10.0.19044\r\nmachine               : AMD64\r\nprocessor             : AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : English_United Kingdom.1252\r\npandas                : 3.0.0.dev0+163.g7281475054\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : 7.2.6\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.12.0\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["this comes from #52419, so gonna ping @jbrockmendel on this one","I suspect this was solved by #53871, which was reverted by #54922.  Discussion about these methods in #56681."],"labels":["Internals","Warnings","Subclassing"]},{"title":"BUG: FutureWarning about 'isin' where it is not used","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nfrom datetime import datetime\r\nimport pandas as pd\r\nfrom pandas import Timestamp\r\n\r\ndf = pd.DataFrame(\r\n\t\t{'Betrag': {0: '-30,60', 1: '-45,00', 2: '-582,00'},\r\n\t\t'Buchungsart': {0: '\u00dcberweisung', 1: '\u00dcberweisung', 2: '\u00dcberweisung'},\r\n\t\t'Dokumentdatum': {0: Timestamp('2024-01-25 00:00:00'),\r\n\t\t\t\t\t\t  1: Timestamp('2024-01-01 00:00:00'),\r\n\t\t\t\t\t\t  2: Timestamp('2024-01-08 00:00:00')},\r\n\t\t'F\u00e4lligkeitsdatum': {0: Timestamp('2024-01-25 00:00:00'),\r\n\t\t\t\t\t\t\t 1: Timestamp('2024-01-31 00:00:00'),\r\n\t\t\t\t\t\t\t 2: Timestamp('2024-02-01 00:00:00')},\r\n\t\t'Genauigkeit Betrag': {0: 'fix', 1: 'fix', 2: 'fix'},\r\n\t\t'Genauigkeit Datum': {0: 'fix', 1: 'fix', 2: 'fix'},\r\n\t\t'Kategorie': {0: 'Kategorie1',\r\n\t\t\t\t\t  1: 'Kategorie2',\r\n\t\t\t\t\t  2: 'Kategorie3'},\r\n\t\t'Kontrahent': {0: 'Kontrahent1', 1: 'Kontrahent2', 2: 'Kontrahent3'},\r\n\t\t'Kontrahent BIC': {0: '', 1: '', 2: ''},\r\n\t\t'Kontrahent IBAN': {0: 'XX000000000000000001',\r\n\t\t\t\t\t\t\t1: 'XX000000000000000002',\r\n\t\t\t\t\t\t\t2: 'XX000000000000000003'},\r\n\t\t'Kostentra\u0308ger': {0: 'Ort',\r\n\t\t\t\t\t\t 1: 'Person',\r\n\t\t\t\t\t\t 2: 'Gegenstand'},\r\n\t\t'Mandatsreferenz': {0: '', 1: '', 2: ''},\r\n\t\t'Produkt': {0: 'Produkt1',\r\n\t\t\t\t\t1: 'Produkt2',\r\n\t\t\t\t\t2: 'Produkt3'},\r\n\t\t'Zahlungsreferenz\/SEPA Creditor Id': {0: '3276842762347',\r\n\t\t\t\t\t\t\t\t\t\t\t  1: '327684276234712',\r\n\t\t\t\t\t\t\t\t\t\t\t  2: '32768427'},\r\n\t\t'erledigt': {0: '', 1: '', 2: ''},\r\n\t\t'gesendet am': {0: Timestamp('2024-01-14 00:00:00'), 1: None, 2: None},\r\n\t\t'laufende Summe': {0: '1.659,09', 1: '1.614,09', 2: '1.032,09'}}\r\n\t\t)\r\n\r\naktuelles_Jahr = datetime.today().year\r\naktueller_Monat = datetime.today().month\r\n\r\ndf_nicht_erfasst = df.query(\"F\u00e4lligkeitsdatum.dt.year == @aktuelles_Jahr and \"\r\n\t\t\t\t\t\"F\u00e4lligkeitsdatum.dt.month == @aktueller_Monat and \"\r\n\t\t\t\t\t\"Buchungsart == '\u00dcberweisung' and \"\r\n\t\t\t\t\t\"`gesendet am` == ''\")\r\n\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nAfter updating to pandas 2.2, when I run a program with the last line above, there is now the following warning displayed:\r\n\r\n```\r\nFutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\r\n```\r\nHowever, the line referenced in the warning does not contain any `isin` function.\r\n\r\nAlso, I am unclear what I am now supposed to do to remedy this warning.\r\n\r\n### Expected Behavior\r\n\r\nClarification about why this is about `isin`and pointer to explanation + example about how to change the code\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\/Users\/guy\/Library\/Python\/3.11\/lib\/python\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 17.7.0\r\nVersion               : Darwin Kernel Version 17.7.0: Fri Oct 30 13:34:27 PDT 2020; root:xnu-4570.71.82.8~1\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : de_AT.UTF-8\r\nLANG                  : de_AT.UTF-8\r\nLOCALE                : de_AT.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : 3.0.8\r\npytest                : 7.4.4\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 5.1.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : None\r\njinja2                : 3.1.3\r\nIPython               : 8.20.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.3\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 13.0.0.dev0+gb7d2f7ffc.d20231220\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.25\r\ntables                : None\r\ntabulate              : 0.9.0\r\nxarray                : None\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2023.4\r\nqtpy                  : 2.4.1\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Can you please provide a reproducer? The DataFrame is missing","@phofl,\r\nthank you. I have updated the code example.\r\n","Thanks for the reproducer - I can reproduce on my laptop.\r\n\r\nTentatively milestoning for 2.2.1.","The problem is `\"'gesendet am' == ''\"`. Where you want to filter on \"None\" values, which are stored as `NaT`.\r\n\r\nCould be fixed with\r\n```\r\ndf_nicht_erfasst = df.query(\"F\u00e4lligkeitsdatum.dt.year == @aktuelles_Jahr and \"\r\n\t\t\t\t\t\"F\u00e4lligkeitsdatum.dt.month == @aktueller_Monat and \"\r\n\t\t\t\t\t\"Buchungsart == '\u00dcberweisung' and \"\r\n\t\t\t\t\t\"`gesendet am` == 'NaT'\")\r\n```\r\nor \r\n```\r\ndf_nicht_erfasst = df.query(\"F\u00e4lligkeitsdatum.dt.year == @aktuelles_Jahr and \"\r\n\t\t\t\t\t\"F\u00e4lligkeitsdatum.dt.month == @aktueller_Monat and \"\r\n\t\t\t\t\t\"Buchungsart == '\u00dcberweisung' and \"\r\n\t\t\t\t\t\"`gesendet am` != `gesendet am`\")\r\n```","@lopof,\r\nThank you for looking into this.\r\n\r\nYour first solution still gives the warning, while the second one does not.\r\n\r\nAs I have gained a little bit better understanding of the workings of pandas, I have changed the last line of the query statement to\r\n```\r\n\"`gesendet am`.isnull()\")\r\n```\r\nas, I think, this most clearly reflects the intention, namely filtering for scheduled transactions with a missing \"sent on\" date (and this also does not trigger the `FutureWarning`).","@workflowsguy ah even better solution, nice. Didn't see that"],"labels":["Bug","Warnings","expressions"]},{"title":"BUG: `to_datetime` raises `OutOfBoundsDatetime` rather than falling back to `datetime.datetime`","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\npd.to_datetime(\"2998-01-01\")\r\n\r\n\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"conversion.pyx\", line 326, in pandas._libs.tslibs.conversion._TSObject.ensure_reso\r\n  File \"np_datetime.pyx\", line 683, in pandas._libs.tslibs.np_datetime.convert_reso\r\nOverflowError: result would overflow\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"~\/path\/to\/file.py\", line 2, in <module>\r\n    pd.to_datetime(\"2998-01-01\")\r\n  File \"~\/path\/to\/python\/lib\/python3.9\/site-packages\/pandas\/core\/tools\/datetimes.py\", line 1101, in to_datetime\r\n    result = convert_listlike(np.array([arg]), format)[0]\r\n  File \"~\/path\/to\/python\/lib\/python3.9\/site-packages\/pandas\/core\/tools\/datetimes.py\", line 435, in _convert_listlike_datetimes\r\n    result, tz_parsed = objects_to_datetime64(\r\n  File \"~\/path\/to\/python\/lib\/python3.9\/site-packages\/pandas\/core\/arrays\/datetimes.py\", line 2398, in objects_to_datetime64\r\n    result, tz_parsed = tslib.array_to_datetime(\r\n  File \"tslib.pyx\", line 414, in pandas._libs.tslib.array_to_datetime\r\n  File \"tslib.pyx\", line 596, in pandas._libs.tslib.array_to_datetime\r\n  File \"tslib.pyx\", line 571, in pandas._libs.tslib.array_to_datetime\r\n  File \"conversion.pyx\", line 332, in pandas._libs.tslibs.conversion._TSObject.ensure_reso\r\npandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2998-01-01, at position 0\r\n\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nAccording to the documentation for `to_datetime`, the return type falls back to `datetime.datetime` in case of out-of-range timestamp parsing. However, trying to parse any date greater than `pandas.Timestamp.max` raises `OutOfBoundsDatetime`.\r\n\r\n### Expected Behavior\r\n\r\nThe code should return a `datetime.datetime` object.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : fd3f57170aa1af588ba877e8e28c158a20a4886d\r\npython                : 3.9.6.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:54:05 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6031\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : None\r\nLOCALE                : None.UTF-8\r\n\r\npandas                : 2.2.0\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 58.0.4\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"REGR: DataFrame.from_records for subclasses no longer calls subclass constructor","body":"Example\r\n\r\n```\r\nIn [16]: from pandas.tests.frame.test_subclass import MySubclassWithMetadata\r\n\r\nIn [17]: df = MySubclassWithMetadata.from_records([{'a': 1, 'b': 2}])\r\n\r\nIn [18]: type(df)\r\nOut[18]: pandas.tests.frame.test_subclass.MySubclassWithMetadata\r\n\r\nIn [19]: df.my_metadata\r\n...\r\nAttributeError: 'MySubclassWithMetadata' object has no attribute 'my_metadata'\r\n```\r\n\r\nWith pandas 2.1.4, this works fine. \r\n\r\nThe reason for this is because ``from_records`` was updated (https:\/\/github.com\/pandas-dev\/pandas\/pull\/52419) to use\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/5740667a55aabffc660936079268cee2f2800225\/pandas\/core\/frame.py#L2532\r\n\r\ninstead of `cls(mgr)` for the final dataframe construction.\r\n\r\nI think this should probably at least use `_constructor_from_mgr` instead of `_from_mgr` (and maybe even still call the class? because in this case there is not finalize that can be called to finalize the initialization)","comments":[],"labels":["Regression","Subclassing"]},{"title":"BUG: rounding dates to 30mins does not work","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndates = pd.Series([pd.to_datetime(\"2022-01-11 12:10:00\")])\r\n\r\n# date should be rounded but is not\r\ndates.round(\"30min\")\r\n# >0   2022-01-11 12:10:00\r\n# >dtype: datetime64[ns]\r\n\r\n# Round just date works\r\ndates[0].round(\"30min\")\r\n# >Timestamp('2022-01-11 12:00:00')\n```\n\n\n### Issue Description\n\nIssue with the latest version of pandas (2.2.0). Rounding dates at 30 min increments does not round dates if they are in a series. This was working in pandas (2.1.4)\r\n\r\n<img width=\"234\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/55141407\/6a03f799-606f-4220-8b11-11a715b7ac8f\">\r\n\n\n### Expected Behavior\n\nDates should be rounded to 30 minute increments\r\n\r\nimport pandas as pd\r\n```python\r\ndates = pd.Series([pd.to_datetime(\"2022-01-11 12:10:00\")])\r\n\r\n# Round just date works\r\ndates[0].round(\"30min\")\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 53525ea1c333579ee612244ddea4958d900844fc\r\npython                : 3.10.0.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 5.15.133.1-microsoft-standard-WSL2\r\nVersion               : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : \r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+152.g53525ea1c3\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : 7.4.3\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.18.1\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : 2023.12.2\r\ngcsfs                 : 2023.12.2post1\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.1\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["thanks @Andre-Medina for the report, this needs fixing, running a git bisect now","From [git bisect](https:\/\/www.kaggle.com\/code\/marcogorelli\/pandas-regression-example?scriptVersionId=159988058), this was introduced in #56767, @phofl ","I am not sure that it is sensible to fix this, the new behavior aligns with DataFrame, which didn't support this before either.\r\n\r\n```\r\ndates.dt.round(\"30min\")\r\n```\r\n\r\nThat's what you want to do, we can fix this theoretically, but we should address the dataframe behavior as well then"],"labels":["Timeseries","Regression"]},{"title":"BUG: `to_timedelta` raises unexpected `OutOfBoundsTimedelta` error with development version of NumPy","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n>>> pd.to_timedelta(np.int32(0), \"D\")\r\nTraceback (most recent call last):\r\n  File \"conversion.pyx\", line 228, in pandas._libs.tslibs.conversion.cast_from_unit\r\nOverflowError: Python integer 86400000000000 out of bounds for int32\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"timedeltas.pyx\", line 377, in pandas._libs.tslibs.timedeltas._maybe_cast_from_unit\r\n  File \"conversion.pyx\", line 230, in pandas._libs.tslibs.conversion.cast_from_unit\r\npandas._libs.tslibs.np_datetime.OutOfBoundsDatetime: cannot convert input 0 with the unit 'D'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/spencer\/mambaforge\/envs\/2024-01-21-upstream-minimal\/lib\/python3.11\/site-packages\/pandas\/core\/tools\/timedeltas.py\", line 225, in to_timedelta\r\n    return _coerce_scalar_to_timedelta_type(arg, unit=unit, errors=errors)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/spencer\/mambaforge\/envs\/2024-01-21-upstream-minimal\/lib\/python3.11\/site-packages\/pandas\/core\/tools\/timedeltas.py\", line 235, in _coerce_scalar_to_timedelta_type\r\n    result = Timedelta(r, unit)\r\n             ^^^^^^^^^^^^^^^^^^\r\n  File \"timedeltas.pyx\", line 1896, in pandas._libs.tslibs.timedeltas.Timedelta.__new__\r\n  File \"timedeltas.pyx\", line 354, in pandas._libs.tslibs.timedeltas.convert_to_timedelta64\r\n  File \"timedeltas.pyx\", line 379, in pandas._libs.tslibs.timedeltas._maybe_cast_from_unit\r\npandas._libs.tslibs.np_datetime.OutOfBoundsTimedelta: Cannot cast 0 from D to 'ns' without overflow.\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nA test failure in xarray's build against the development versions of upstream packages (https:\/\/github.com\/pydata\/xarray\/issues\/8623) can be boiled down to the reproducible example above.  It seems like something goes wrong in casting an `int32` value to a `Timedelta` with the development version of NumPy.\r\n\r\n### Expected Behavior\r\n\r\nI would expect the example to run without an error:\r\n```\r\n>>> pd.to_timedelta(np.int32(0), \"D\")\r\nTimedelta('0 days 00:00:00')\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\/Users\/spencer\/mambaforge\/envs\/2024-01-21-upstream-minimal\/lib\/python3.11\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 21b3906a39eca5ba8220d5e34ca1f204a68a5adb\r\npython                : 3.11.7.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:54:10 PST 2023; root:xnu-10002.61.3~2\/RELEASE_X86_64\r\nmachine               : x86_64\r\nprocessor             : i386\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 3.0.0.dev0+149.g21b3906a39\r\nnumpy                 : 2.0.0.dev0+git20240120.6bd3abf\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 69.0.3\r\npip                   : 23.3.2\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : None\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["The issue appears to be in this section of the code:  https:\/\/github.com\/pandas-dev\/pandas\/blob\/e51039afe3cbdedbf5ffd5cefb5dea98c2050b88\/pandas\/_libs\/tslibs\/conversion.pyx#L214-L232\r\n\r\nIn line 223, `ts` is of type `np.int32` and `base` is a Python integer.  With NumPy < 2, `frac` is a Python integer:\r\n```\r\n>>> np.int32(0) - 0\r\n0\r\n```\r\nbut with NumPy >= 2, `frac` is of type `np.int32`:\r\n```\r\n>>> np.int32(0) - 0\r\nnp.int32(0)\r\n```\r\nFurther, NumPy >= 2 then raises when the `np.int32` `frac` is multiplied by `m`, which is a Python integer:\r\n```\r\n>>> np.int32(0) * 86400000000000\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nOverflowError: Python integer 86400000000000 out of bounds for int32\r\n```\r\nI believe this change in behavior is described more fully in [NEP 50](https:\/\/numpy.org\/neps\/nep-0050-scalar-promotion.html#nep-50-promotion-rules-for-python-scalars).  What do we think the cleanest way to address this might be?  It seems some sort of manual type promotion will be necessary."],"labels":["Bug","Needs Triage"]},{"title":"BUG: Assignment of pyarrow arrays yields unexpected dtypes","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport datetime\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\ndf = pd.DataFrame([[42]], columns=[\"col\"])\r\ndf[\"int16\"] = pa.array([16], type=pa.int16())\r\ndf[\"date\"] = pa.array([datetime.date(2024, 1, 1)], type=pa.date32())\r\ndf[\"string\"] = pa.array([\"foo\"], pa.string())\r\n```\n```\n\n\n### Issue Description\n\n```\r\n>>> df.dtypes\r\ncol               int64\r\nint16             int16\r\ndate      datetime64[s]\r\nstring           object\r\ndtype: object\r\n```\r\n\r\nI am surprised that the pyarrow type is not maintained during assignment\n\n### Expected Behavior\n\n```\r\n>>> df.dtypes\r\ncol               int64\r\nint16             int16[pyarrow]\r\ndate              date32[pyarrow]\r\nstring            string[pyarrow]\r\ndtype: object\r\n```\n\n### Installed Versions\n\non main","comments":[],"labels":["Bug","Indexing","Arrow"]},{"title":"BUG: groupby().groups and .indices always treat as_index as True","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'col0': [3], 'col1': [4]})\r\n\r\nprint(df.groupby('col0', as_index=False).groups)\r\nprint(df.groupby('col0', as_index=True).groups)\r\nprint(df.groupby('col0', as_index=False).indices)\r\nprint(df.groupby('col0', as_index=True).indices)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen `as_index=False`, the keys of `groups` should be a sequence of consecutive numbers starting from 0. Instead, the keys of the `group` dictionary are the group keys, as if `as_index` were `True`. Same with `.indices`.\r\n\r\n### Expected Behavior\r\n\r\nWhen `as_index=False`, the keys of `groups` and `indices` should be a sequence of consecutive numbers starting from 0. \r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.9.18.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.2.0\r\nVersion             : Darwin Kernel Version 23.2.0: Wed Nov 15 21:55:06 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6020\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : 8.18.1\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report!\r\n\r\n> When `as_index=False`, the keys of `groups` should be a sequence of consecutive numbers starting from 0. Instead, the keys of the `group` dictionary are the group keys, as if `as_index` were `True`. Same with `.indices`.\r\n\r\nI'm not sure I agree. I don't think `as_index` being True or False changes the groups, but rather just where the groups are placed (index vs column). For example,\r\n\r\n```\r\nprint(df.groupby('col0', as_index=False).sum())\r\n   col0  col1\r\n0     3     4\r\n```\r\n\r\nI would still say the group here is 3.","I can see where there is room for debate here. The [documented behavior](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.core.groupby.DataFrameGroupBy.groups.html) is `Dict {group name -> group labels}.` If someone does `groupby(as_index=False)`, are the \"group names\" the new row labels of the results, or the row labels of the dataframe that would result from an aggregation like `groupby(as_index=False).sum()`? I would say the latter. In the aggregation result, I think the \"group names\" are the group labels, not the values of the `by` columns. Note that  this principle would hold even in aggregations by expressions that are not part of the original dataframe's columns e.g.\r\n\r\n```python\r\ndf = pd.DataFrame({'col0': [3, 5], 'col1': [4, 6]})\r\nprint(df.groupby(df.col0 % 2, as_index=False).sum())\r\n   col0  col1\r\n0     8    10\r\n```\r\n\r\nIn this case with `as_index=False`, the group key is not part of the original columns, so it's not in the columns or the index.`as_index=False` effectively eliminates the group key from the result. But `df.groupby(df.col0 % 2, as_index=False).groups` would keep the group key `1` in the result.\r\n\r\nI think it's clearer if the groups that the user sees in `groupby().groups` always match the row labels of any `groupby` aggregation. In general, I think all parameters that can affect the sequence of the resulting row labels for aggregations like `groupby().sum()` should have the same effect on the sequence of keys in` groupby().groups`. So I stick to my original position here.","> are the \"group names\" the new row labels of the results, or the row labels of the dataframe that would result from an aggregation like `groupby(as_index=False).sum()`?\r\n\r\nI think neither. The DataFrame.groupby documents the `by` argument as\r\n\r\n> Used to determine the groups for the groupby. [snip] If a list or ndarray of length equal to the selected axis is passed (see the [groupby user guide](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/groupby.html#splitting-an-object-into-groups)), the values are used as-is to determine the groups.\r\n\r\nThese are the groups. \r\n\r\n> In this case with `as_index=False`, the group key is not part of the original columns, so it's not in the columns or the index.`as_index=False` effectively eliminates the group key from the result.\r\n\r\nThis behavior is deprecated and the groups will be in the columns in pandas 3.0. #49519","Thinking about this a bit more, `groups`, `indices`, `get_groups`, and `__iter__` all consistently treat the groups the same way. Changing any one of these I think would mean changing them all, and that's a lot of churn for what I think is little, if any, gain."],"labels":["Bug","Groupby","Needs Discussion"]},{"title":"BUG: Inconsistent stream processing between read_excel and read_csv","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    \"A\": [.1, .2, .3],\r\n    \"B\": [\"a\", \"b\", \"c\"]\r\n})\r\ndf.to_excel(\"test.xlsx\", index=False)\r\ndf.to_csv(\"test.csv\", index=False)\r\n\r\nwith open(\"test.xlsx\", \"rb\") as f:\r\n    df_excel_1 = pd.read_excel(f)\r\n    # the below line should fail or return an empty dataframe but doesn't\r\n    df_excel_2 = pd.read_excel(f)\r\n\r\n# instead, we have that pd.testing.assert_frame_equal(df_excel_1, df_excel_2)\r\n\r\nwith open(\"test.csv\", \"r\") as f:\r\n    df_csv_1 = pd.read_csv(f)\r\n    # the below line will fail with EmptyDataError, as expected\r\n    df_csv_2 = pd.read_csv(f)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nread_excel does not honor the starting position of the stream.\r\n\r\nThe problem is likely in `pandas.io.excel._base.py:580` which calls .seek(0)\r\n\r\n```python\r\n576:    if isinstance(self.handles.handle, self._workbook_class):\r\n577:        self.book = self.handles.handle\r\n578:    elif hasattr(self.handles.handle, \"read\"):\r\n579:        # N.B. xlrd.Book has a read attribute too\r\n580:        self.handles.handle.seek(0)\r\n581:        try:\r\n582:            self.book = self.load_workbook(self.handles.handle, engine_kwargs)\r\n583:        except Exception:\r\n584:            self.close()\r\n585:            raise\r\n586:    else:\r\n587:        raise ValueError(\r\n588:            \"Must explicitly set engine if not passing in buffer or path for io.\"\r\n589:        )\r\n```\r\n\r\n# Why this is incorrect behaviour\r\nI think calling seek(0) is anti-pattern in how streams are treated. For one, this ignores the current position of the stream for no apparent good reason. As second, a stream is not even required to implement a seek function to be considered a valid stream. \r\n\r\nHence, for example, you see other reports where people are passing a stream to read_excel that does not implement seek, such as, e.g. Requests response (https:\/\/github.com\/pandas-dev\/pandas\/issues\/28825). The latter issue appears to be resolved, but looks like it's been resolved by considering the request Response as a special case that is handled differently.\r\n\r\n# Path to solution\r\nFirst would be to remove seek(0) call altogether. \r\n\r\nIf load_workbook requires the stream to be seekable, however, the handle passed to load_workbook should be wrapped into a BytesIO wrapper. In either case, the stream should be read till the end and\/or until a terminating condition is met (I don't know enough about Excel internals) and then left there, and left to the subsequent consumer to either continue or to the caller to close the stream. \r\n\r\nAs an aside, the call to close the stream should only happen if the ExcelReader is the one that opened it. Otherwise, it should be the caller's responsibility to close it.\r\n\r\n\r\n### Expected Behavior\r\n\r\nread_excel should start reading bytes from the current position of the stream and should not reset it.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.3.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 165 Stepping 5, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : English_United Kingdom.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.0\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n```\r\n\r\n<\/details>\r\n","comments":["Sure - if this can be removed and still pass the test suite I would be OK with removing seek. There might be some historical cruft to that being in the there in the first place.\r\n\r\nDid you want to try that in a PR @jf2?","Hi @WillAyd, I've been playing around (I've even removed the `seek` line), and this doesn't seem to be a pandas bug. If you use openpyx1 to read the file, you get to read it as many times as you want without reopening the workbook:\r\n\r\n```\r\nimport openpyxl\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    \"A\": [.0, .1],\r\n    \"B\": [\"a\", \"b\"]\r\n})\r\ndf.to_excel(\"test.xlsx\", index=False)\r\n\r\nwb = openpyxl.load_workbook(\"test.xlsx\")\r\nsheet = wb.active\r\n\r\nrows_1 = list(sheet.values)\r\nrows_2 = list(sheet.values)\r\nrows_3 = list(sheet.values)\r\n```\r\n\r\nCould this be a feature on how excel files are handled? Wdyt?\r\n"],"labels":["Bug","IO Excel"]},{"title":"API: Consistently support `numeric_only` in groupby ops","body":" Inspecting the various groupby ops, I think the following are cases where we should have `numeric_only`.\r\n\r\n - [ ] agg (#50538)\r\n - [ ] aggregate (#50538)\r\n - [ ] apply\r\n - [ ] cumprod (#53072)\r\n - [ ] cumsum (#53072)\r\n - [ ] diff\r\n - [ ] pct_change\r\n - [ ] rank (#44438)\r\n - [ ] transform\r\n - [ ] ohlc\r\n\r\nThe following methods make sense to have `numeric_only`, but won't fail on any input (or hashable input for nunique and value_counts), and so I think it's okay if they don't. But is still nice to have.\r\n\r\n - [ ] all\r\n - [ ] any\r\n - [ ] bfill\r\n - [ ] count\r\n - [ ] ffill\r\n - [ ] nunique\r\n - [ ] value_counts\r\n\r\nThe following methods should not get a `numeric_only` argument. They fall into a few typical camps: filters, plotting, or they do not depend on the columns (e.g. cumcount and size)\r\n\r\n - boxplot\r\n - cumcount\r\n - describe  # Handled by `include=\"all\"`\r\n - filter\r\n - head\r\n - hist\r\n - nth\r\n - pipe\r\n - plot\r\n - shift\r\n - size\r\n - tail\r\n - take\r\n","comments":["Looking at the all possible methods that argument `numeric_only=` needs to support, I think...\r\n\r\nwouldn't it be better to have independent method `.numeric_only()` to select columns with numeric values or number dtypes, since `numeric_only=` argument can conflict with the function to be `.agg()` in the parameter name which is `numeric_only`\r\n\r\n","One can already do `DataFrame.select_dtypes('numeric')`. I think we should strive for consistency of arguments between DataFrame and groupby ops where it makes sense, and this is one of those cases. So unless we're going to deprecate `numeric_only` across the board, I'm still positive on including them in groupby.","But the purpose was to select numeric columns from DataFrameGroupby. I think `.select_dtypes('numer')` is applicable only on DataFrame object. Or we might add the method `select_dtypes()` also to DataFrameGroupyby.\r\n\r\nI think for the purpose of dendency problem I would suggest to maintain it(`numeric_only`) and also get `*arg`, and `**kwargs` for the functions to be applied... (I mean in the case of `.mean()` or `.std()`... etc, because `.mean(skipna=False)` does not work...) When you think about it, `numeric_only`  is not so popular parameter name and we can urge users to avoid it...(I wonder if there is any function that uses parameter name `numeric_only` or we can always make custom function with `lambda x: f(x, numeric_only=True)` or something similar."],"labels":["Groupby","API - Consistency"]},{"title":"BUG: error indexing non-nanosecond datetime outside of nanosecond datetime range","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nfrom datetime import datetime\r\nimport numpy as np\r\nimport pandas as pd\r\nyears = pd.date_range(start=datetime(1001, 12, 31),\r\n                      end=datetime(2000, 12, 31),\r\n                      freq=\"A\", unit=\"s\")\r\nts = pd.Series(np.arange(1000), index=years)\r\n\r\nIn [2]: ts.index\r\nOut[2]:\r\nDatetimeIndex(['1001-12-31', '1002-12-31', '1003-12-31', '1004-12-31',\r\n               '1005-12-31', '1006-12-31', '1007-12-31', '1008-12-31',\r\n               '1009-12-31', '1010-12-31',\r\n               ...\r\n               '1991-12-31', '1992-12-31', '1993-12-31', '1994-12-31',\r\n               '1995-12-31', '1996-12-31', '1997-12-31', '1998-12-31',\r\n               '1999-12-31', '2000-12-31'],\r\n              dtype='datetime64[s]', length=1000, freq='A-DEC')\r\n\r\nIn [4]: ts.loc[\"1600\"]\r\n---------------------------------------------------------------------------\r\nOutOfBoundsDatetime                       Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 ts.loc[\"1600\"]\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1153, in _LocationIndexer.__getitem__(self, key)\r\n   1150 axis = self.axis or 0\r\n   1152 maybe_callable = com.apply_if_callable(key, self.obj)\r\n-> 1153 return self._getitem_axis(maybe_callable, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1393, in _LocIndexer._getitem_axis(self, key, axis)\r\n   1391 # fall thru to straight lookup\r\n   1392 self._validate_key(key, axis)\r\n-> 1393 return self._get_label(key, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1343, in _LocIndexer._get_label(self, label, axis)\r\n   1341 def _get_label(self, label, axis: AxisInt):\r\n   1342     # GH#5567 this will fail if the label is not present in the axis.\r\n-> 1343     return self.obj.xs(label, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py:4236, in NDFrame.xs(self, key, axis, level, drop_level)\r\n   4234             new_index = index[loc]\r\n   4235 else:\r\n-> 4236     loc = index.get_loc(key)\r\n   4238     if isinstance(loc, np.ndarray):\r\n   4239         if loc.dtype == np.bool_:\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimes.py:611, in DatetimeIndex.get_loc(self, key)\r\n    609 if self._can_partial_date_slice(reso):\r\n    610     try:\r\n--> 611         return self._partial_date_slice(reso, parsed)\r\n    612     except KeyError as err:\r\n    613         raise KeyError(key) from err\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimelike.py:301, in DatetimeIndexOpsMixin._partial_date_slice(self, reso, parsed)\r\n    298 if not self._can_partial_date_slice(reso):\r\n    299     raise ValueError\r\n--> 301 t1, t2 = self._parsed_string_to_bounds(reso, parsed)\r\n    302 vals = self._data._ndarray\r\n    303 unbox = self._data._unbox\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimes.py:539, in DatetimeIndex._parsed_string_to_bounds(self, reso, parsed)\r\n    524 \"\"\"\r\n    525 Calculate datetime bounds for parsed time string and its resolution.\r\n    526\r\n   (...)\r\n    536 lower, upper: pd.Timestamp\r\n    537 \"\"\"\r\n    538 per = Period(parsed, freq=reso.attr_abbrev)\r\n--> 539 start, end = per.start_time, per.end_time\r\n    541 # GH 24076\r\n    542 # If an incoming date string contained a UTC offset, need to localize\r\n    543 # the parsed date to this offset first before aligning with the index's\r\n    544 # timezone\r\n    545 start = start.tz_localize(parsed.tzinfo)\r\n\r\nFile period.pyx:1651, in pandas._libs.tslibs.period.PeriodMixin.start_time.__get__()\r\n\r\nFile period.pyx:2000, in pandas._libs.tslibs.period._Period.to_timestamp()\r\n\r\nFile period.pyx:1158, in pandas._libs.tslibs.period.period_ordinal_to_dt64()\r\n\r\nFile np_datetime.pyx:231, in pandas._libs.tslibs.np_datetime.check_dts_bounds()\r\n\r\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1600-01-01 00:00:00\n```\n\n\n### Issue Description\n\nI receive an error trying to index a timeseries with second resolution outside the [standard nanosecond range](https:\/\/pandas.pydata.org\/docs\/user_guide\/timeseries.html#timeseries-timestamp-limits).\r\nIt appears that the time series is being converted to nanosecond resolution during the indexing operation.\r\n\r\nI believe this is issue would fall under https:\/\/github.com\/pandas-dev\/pandas\/issues\/46587\r\n\n\n### Expected Behavior\n\nThe expected behavior would function as it does within the standard nanosecond range:\r\n\r\nIn [3]: ts.loc[\"1700\"]\r\nOut[3]:\r\n1700-12-31    699\r\nFreq: A-DEC, dtype: int64\n\n### Installed Versions\n\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.0-91-generic\r\nVersion             : #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.3\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 5.1.0\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.3\r\nIPython             : 8.20.0\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.12.2\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : 0.58.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.2\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : 2023.12.0\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\n<\/details>\r\n","comments":["I just updated to Pandas 2.2.0 and received the same error:\r\n\r\n```\r\nIn [4]: ts.loc[\"1600\"]\r\n---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\nFile period.pyx:1169, in pandas._libs.tslibs.period.period_ordinal_to_dt64()\r\n\r\nOverflowError: Overflow occurred in npy_datetimestruct_to_datetime\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nOutOfBoundsDatetime                       Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 ts.loc[\"1600\"]\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1192, in _LocationIndexer.__getitem__(self, key)\r\n   1190 maybe_callable = com.apply_if_callable(key, self.obj)\r\n   1191 maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)\r\n-> 1192 return self._getitem_axis(maybe_callable, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1432, in _LocIndexer._getitem_axis(self, key, axis)\r\n   1430 # fall thru to straight lookup\r\n   1431 self._validate_key(key, axis)\r\n-> 1432 return self._get_label(key, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexing.py:1382, in _LocIndexer._get_label(self, label, axis)\r\n   1380 def _get_label(self, label, axis: AxisInt):\r\n   1381     # GH#5567 this will fail if the label is not present in the axis.\r\n-> 1382     return self.obj.xs(label, axis=axis)\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py:4295, in NDFrame.xs(self, key, axis, level, drop_level)\r\n   4293             new_index = index[loc]\r\n   4294 else:\r\n-> 4295     loc = index.get_loc(key)\r\n   4297     if isinstance(loc, np.ndarray):\r\n   4298         if loc.dtype == np.bool_:\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimes.py:610, in DatetimeIndex.get_loc(self, key)\r\n    608 if self._can_partial_date_slice(reso):\r\n    609     try:\r\n--> 610         return self._partial_date_slice(reso, parsed)\r\n    611     except KeyError as err:\r\n    612         raise KeyError(key) from err\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimelike.py:324, in DatetimeIndexOpsMixin._partial_date_slice(self, reso, parsed)\r\n    321 if not self._can_partial_date_slice(reso):\r\n    322     raise ValueError\r\n--> 324 t1, t2 = self._parsed_string_to_bounds(reso, parsed)\r\n    325 vals = self._data._ndarray\r\n    326 unbox = self._data._unbox\r\n\r\nFile ~\/miniconda3\/envs\/Nile-Laki\/lib\/python3.11\/site-packages\/pandas\/core\/indexes\/datetimes.py:538, in DatetimeIndex._parsed_string_to_bounds(self, reso, parsed)\r\n    536 freq = OFFSET_TO_PERIOD_FREQSTR.get(reso.attr_abbrev, reso.attr_abbrev)\r\n    537 per = Period(parsed, freq=freq)\r\n--> 538 start, end = per.start_time, per.end_time\r\n    540 # GH 24076\r\n    541 # If an incoming date string contained a UTC offset, need to localize\r\n    542 # the parsed date to this offset first before aligning with the index's\r\n    543 # timezone\r\n    544 start = start.tz_localize(parsed.tzinfo)\r\n\r\nFile period.pyx:1666, in pandas._libs.tslibs.period.PeriodMixin.start_time.__get__()\r\n\r\nFile period.pyx:1992, in pandas._libs.tslibs.period._Period.to_timestamp()\r\n\r\nFile period.pyx:1172, in pandas._libs.tslibs.period.period_ordinal_to_dt64()\r\n\r\nOutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1600-01-01 00:00:00\r\n\r\n```"],"labels":["Bug","Needs Triage"]},{"title":"DOC: Highlight the difference between DataFrame\/pd.Series\/numpy ops when there are NA values","body":"Edit[rhshadrach]: The original report about inconsistent skipna arguments in groupby is captured in #15675. For this issue, it suffices to add notes to the documentation about the default incompatibility between pandas and NumPy as described below.\r\n\r\n### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnp_series = [\r\n    np.array(['A','A','A', 'B', 'B']), \r\n    np.array([1, 2, 3, 4, 5]), \r\n    np.array([np.nan, 2, 3, 4, 5])\r\n]\r\n\r\n# NumPy behavior\r\ndisplay(np.std(np_series[1], ddof=1)) # 1.5811388300841898\r\ndisplay(np.std(np_series[2], ddof=1)) # nan\r\n\r\n# Pandas DataFrame behavior\r\ndf = pd.DataFrame(dict(zip(['Group', 'Val1', 'Val2'], np_series)))\r\ndisplay(df[['Val1', 'Val2']].std().to_frame())\r\ndisplay(df[['Val1', 'Val2']].std(skipna=False).to_frame())\r\n\r\n# Pandas Series behavior\r\ndisplay(pd.Series(np_series[1]).std())\r\ndisplay(pd.Series(np_series[2]).std())\r\ndisplay(pd.Series(np_series[2]).std(skipna=False)) # equivalent to the default numpy behavior when called with ddof=1\r\n\r\n# GroupBy behavior\r\ndisplay(df.groupby('Group').std()) # no nans reported, no option to pass skipna parameter\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nI've wasted quite some time trying to find out why in some edge cases (on a large dataset) a different value was reported by DataFrame.std vs numpy.std. The posts and documentation I've found focus on the DDOF parameter only. It turned out to be related to fewer NaN values being reported.\r\n\r\n### Expected Behavior\r\n\r\nChatGPT hinted to me that the [Pandas Series has a skipna=True argument](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.std.html#pandas-series-std). This is different from the [numpy.std](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.std.html), which does NOT skip na by default. And even has no such optional argument.\r\n\r\nI would suggest to:\r\n1. Add documentation to all DataFrame, Series and GroupBy classes regarding this different default behavior.\r\n2. Add the skipna=True argument to the [Series.std and GroupBy.std](https:\/\/github.com\/pandas-dev\/pandas\/blob\/f459437d7a7dc10db82437aa4438f058173212a7\/pandas\/core\/window\/expanding.py#L490C9-L490C12) methods\r\n\r\nI can contribute this patch if this is indeed the right way to go.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 0f437949513225922d851e9581723d82120684a6\r\npython           : 3.11.6.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 4.18.0-372.19.1.el8_6.x86_64\r\nVersion          : #1 SMP Tue Aug 2 16:19:42 UTC 2022\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : en_US.UTF-8\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 2.0.3\r\nnumpy            : 1.26.0\r\npytz             : 2023.3.post1\r\ndateutil         : 2.8.2\r\nsetuptools       : 68.2.2\r\npip              : 23.2.1\r\nCython           : None\r\npytest           : None\r\nhypothesis       : None\r\nsphinx           : None\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : 3.1.6\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : 2.9.9\r\njinja2           : 3.1.2\r\nIPython          : 8.16.1\r\npandas_datareader: None\r\nbs4              : None\r\nbottleneck       : None\r\nbrotli           : 1.1.0\r\nfastparquet      : None\r\nfsspec           : None\r\ngcsfs            : None\r\nmatplotlib       : 3.8.0\r\nnumba            : None\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : 3.1.2\r\npandas_gbq       : None\r\npyarrow          : 13.0.0\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.11.3\r\nsnappy           : None\r\nsqlalchemy       : 1.4.49\r\ntables           : None\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nzstandard        : None\r\ntzdata           : 2023.3\r\nqtpy             : None\r\npyqt5            : None\r\n<\/details>\r\n","comments":["Thanks for the report. Agreed on adding the `skipna=False` to Series\/DataFrame methods where we talk about being compatible with NumPy. The inconsistency of the presence of `skipna` in groupby is not desirable. I'd be in favor of adding it everywhere (as opposed to removing it everywhere), but I think this needs some more discussion.\r\n\r\ncc @jbrockmendel @mroeschke @jorisvandenbossche ","Related: #15675","The current docstring has a note:\r\n\r\n> To have the same behaviour as numpy.std, use ddof=0 (instead of the default ddof=1)\r\n\r\nThat could indeed definitely be updated to mention this is only the case if there are no missing values (or that you have to specify `skipna=False`), or in general more clearly warn about the differences with np.std","Is this task available, or is it in discussion as of now?\r\nI'd like to take the task once the decision is taken about adding the skipna=False option.\r\nThanks\r\n","```python\r\ndisplay(np.mean([np.nan,2,3,4,5])) # --> nan\r\ndisplay(pd.Series([np.nan,2,3,4,5]).mean()) # --> 3.5\r\n```\r\nThe mean function has the same `skipna=True` issue as `std`.\r\n\r\nAnd the `GroupBy` [also does not support the `skipna` argument](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.core.groupby.DataFrameGroupBy.mean.html)\r\n\r\nI think you may want to add these in a consistent manner over all `GroupBy` methods...?","What is the stance on being compatible to NumPy? Should Pandas follow that where possible?\r\n\r\nNB: I would argue that changing the default setting of the parameter should be considered a breaking change in the API. I presume that a lot of scripts already assume that skipna=True is indeed the default behavior now.","@Ahmedniz1 \r\n\r\n> Is this task available, or is it in discussion as of now?\r\n\r\nI think adding `skipna` to groupby operations needs more discussion. The addition to the documentation of `skipna` in Series\/DataFrame methods for compatibility with NumPy would be very much welcome!\r\n\r\n@JoostvanPinxten \r\n\r\n> What is the stance on being compatible to NumPy? Should Pandas follow that where possible?\r\n\r\nI think that is too strong. I view compatibility with NumPy a beneficial feature, but must be weighed against other impacts of changes. In this instance, I am against changing the default of `skipna` to agree with NumPy, at least just for the sake of compatibility.","> NB: I would argue that changing the default setting of the parameter should be considered a breaking change in the API. I presume that a lot of scripts already assume that skipna=True is indeed the default behavior now.\r\n\r\nI very much agree with this. Pandas is already a sort of convenience wrapper around NumPy, adding additional abstractions like coordinate names etc. Having NaNs ignored when computing standard deviation by default seems desireable in most cases, and even NumPy has a separate function indeed for handling NaNs in this case (`np.nanstd`). IMO it's safe to just assume that Pandas implements by default the more convenient of two options.\r\n\r\nPlease do not yet again change default behaviour just because someone was bored on a Sunday afternoon and had an epiphany; people write massive libraries based on Pandas and its default behaviours are often taken into account for the sake of code-readability and -brevity. `skipna=False` is documented and anyone who wants that level of strictness can easily enable it already that way.\r\n\r\nI do agree that the documentation giving instructions for replicating NumPy behaviour are incomplete; that's IMO all that should be fixed here.","+1 for adding the argument to groupby.std for consistency.","I've updated #15675 for tracking adding skipna; reworking this as purely a documentation issue.","Sounds good to me. I hope to have some time tonight, but I am also OK if someone else picks this up.","Can I get to know where to highlight the difference exactly in the documentation.\r\nI'd love to take this up.","I only know the resulting documentation, not where the sources are. There are [quite a few places where e.g. std is used](https:\/\/pandas.pydata.org\/docs\/search.html?q=std). A few samples:\r\n\r\n[GroupBy std](https:\/\/pandas.pydata.org\/pandas-docs\/version\/1.5\/reference\/api\/pandas.core.groupby.GroupBy.std.html)\r\n[DataFrameGroupBy std](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.core.groupby.DataFrameGroupBy.std.html)\r\n[SeriesGroupBy std](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.core.groupby.SeriesGroupBy.std.html#pandas.core.groupby.SeriesGroupBy.std)\r\n\r\nIt should contain something similar to:\r\n[DataFrame std](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.std.html#pandas.DataFrame.std)\r\n\r\nRegarding the skipna argument: \"Exclude NA\/null values. If an entire row\/column is NA, the result will be NA.\"\r\n\r\nRegarding the note about ddof and consistency with numpy, see an example here:\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/3332970\/44795447-d360-4341-bc3e-ae94f05ba685)\r\n\r\nBut there are other operations where skipna is also relevant and perhaps non-default in numpy. I do not know how to best approach this in the code-base though."],"labels":["Docs","Missing-data"]},{"title":"BUG: Improve MultiIndex label rename checks, docs and tests","body":"- [x] closes #55169 \r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.","comments":["Actually, I think I need to write an additional test for when `errors=\"raise\"` and a rename is successful against a `MultiIndex`. That fails without this patch because of the inconsistency between the check and transform logic ","Hi @phofl , @mroeschke,\r\nWould you be able to help review this PR?","This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Bug","MultiIndex","Stale"]},{"title":"ENH: Add decimal and thousand separator params to to_numeric()","body":"- [x] closes #4674 \r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nAdded the option to specify a different decimal point when using to_numeric() as described in #4674. Also added an option to specify the thousasnds separator, similar to functions such as read_csv(). This removes the need for users to change the string manually before calling to_numeric(), and is especially useful for processing numbers in international formats.\r\n","comments":["Setting ready for review as the only test failing is one that is also failing on the main branch - \"Unit Tests \/ Numpy Dev (push)\"","Added the whatsnew changes to the 3.0.0 file for now, will there be another minor version before then or is that the correct file to document the changes?","@WillAyd Thanks for the feedback, I'll have a look into the changes shortly. \r\n\r\nThe idea behind this is to mimic some of the functionality of functions such as `read_csv()` where you can define your own thousands and decimal seperators, mainly for ingestion of numerical data in international formats. If you are reading data directly into pandas through one of the `read_xxx()` functions then you usually have this custom separator functionality, but if for any reason the numeric data is ingested as a string into python first, and then try to \"ingest\" into pandas and convert with `to_numeric()` you don't have that functionality. The specific issue I had with this was when crawling data from websites that present numbers in the european format. For example data on websites that is not in a table readable by `read_html()`, and in some cases the data had to be obtained by parsing numbers from a pdf.\r\n\r\nRegarding python handling this, the `float()` function can only handle different seperators if you mess around with locales, as described and not recommended here: https:\/\/stackoverflow.com\/a\/6633912 . This is global and not really usable in production situations, and means you will then be locked into processing data in the format as defined by the locale (Also somewhat system dependent). Having these parameters in `to_numeric()` would allow for processing different columns in different formats, eg. one column of US style numbers and one of European. And doesn't risk unintended consequences like locale changes and different behaviour on different PCs."],"labels":["Enhancement","Numeric Operations"]},{"title":"BUG: to_dict inconsistency","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\ndf = ....\r\ndf.to_dict()\n```\n\n\n### Issue Description\n\nto dict manipulate float values which could not be restored\n\n### Expected Behavior\n\nlossless deserialization \n\n### Installed Versions\n\n2.1.4","comments":["Thanks for the report! Could you include a reproducible example? See https:\/\/matthewrocklin.com\/minimal-bug-reports for more details."],"labels":["Bug","IO Data","Needs Info"]},{"title":"BUG: KeyError when loading csv with NaNs","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n# Define parameters for data\r\nnan_rows = 3\r\nrandom_rows = 7\r\ntotal_rows = nan_rows + random_rows\r\n\r\n# With two levels\r\nlevel1 = ['Level 1'] * 1  \r\nlevel2 = ['Level 2'] * 1 \r\n\r\n# Combine these lists to create a MultiIndex for columns\r\ncolumns = pd.MultiIndex.from_arrays([level1, level2], names=['level1', 'level2'])\r\n\r\n# Create dummy data\r\ndata = np.random.rand(total_rows, len(columns))\r\ndata[:nan_rows, :] = np.nan \r\ndf_complex = pd.DataFrame(data, columns=columns)\r\n\r\n# Save the DataFrame as a CSV\r\ncsv_filename_complex = 'example_data_complex.csv'\r\ndf_complex.to_csv(csv_filename_complex)\r\n\r\n# Load the data back with header=[0, 1] and index_col=0\r\nloaded_df_complex = pd.read_csv(csv_filename_complex, header=[0, 1], index_col=0)\r\n\r\n# NaN in row 0 is ommited:\r\nprint(loaded_df_complex.head(total_rows))\r\n\r\n# Also throws a KeyError when indexing into it\r\ntry:\r\n    loaded_df_complex.loc[0]\r\n    print('Works')\r\nexcept KeyError:\r\n    print('Does not work')\n```\n\n\n### Issue Description\n\nWhen loading back a CSV file previously saved using pandas, the first row is omitted if it contains NaN values. The problem seems to be the default behaviour for replacing NaN with ''. If we force replacement to be 'NaN' the same example works, see below:\r\n\r\nMaybe related Issue: https:\/\/github.com\/pandas-dev\/pandas\/issues\/55803\n\n### Expected Behavior\n\n```python\r\n# Define parameters for data\r\nnan_rows = 3\r\nrandom_rows = 7\r\ntotal_rows = nan_rows + random_rows\r\n\r\n# With two levels\r\nlevel1 = ['Level 1'] * 1  \r\nlevel2 = ['Level 2'] * 1 \r\n\r\n# Combine these lists to create a MultiIndex for columns\r\ncolumns = pd.MultiIndex.from_arrays([level1, level2], names=['level1', 'level2'])\r\n\r\n# Create dummy data\r\ndata = np.random.rand(total_rows, len(columns))\r\ndata[:nan_rows, :] = np.nan \r\ndf_complex = pd.DataFrame(data, columns=columns)\r\n\r\n# Save the DataFrame as a CSV\r\ncsv_filename_complex = 'example_data_complex.csv'\r\ndf_complex.to_csv(csv_filename_complex, na_rep='NaN')\r\n\r\n# Load the data back with header=[0, 1] and index_col=0\r\nloaded_df_complex = pd.read_csv(csv_filename_complex, header=[0, 1], index_col=0)\r\n\r\n# NaN in row 0 is not ommited:\r\nprint(loaded_df_complex.head(total_rows))\r\n\r\n# Also throws a KeyError when indexing into it\r\ntry:\r\n    loaded_df_complex.loc[0]\r\n    print('Works')\r\nexcept KeyError:\r\n    print('Does not work')\r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 0f437949513225922d851e9581723d82120684a6\r\npython           : 3.8.16.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 5.15.0-91-generic\r\nVersion          : #101~20.04.1-Ubuntu SMP Thu Nov 16 14:22:28 UTC 2023\r\nmachine          : x86_64\r\nprocessor        : x86_64\r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\n\r\npandas           : 2.0.3\r\nnumpy            : 1.24.3\r\npytz             : 2023.3\r\ndateutil         : 2.8.2\r\nsetuptools       : 67.8.0\r\npip              : 23.1.2\r\nCython           : 3.0.0\r\npytest           : 7.4.0\r\nhypothesis       : None\r\nsphinx           : 7.1.0\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : None\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 3.1.2\r\nIPython          : 8.12.2\r\npandas_datareader: None\r\nbs4              : 4.12.2\r\nbottleneck       : None\r\nbrotli           : None\r\nfastparquet      : None\r\nfsspec           : None\r\ngcsfs            : None\r\nmatplotlib       : 3.7.1\r\nnumba            : None\r\nnumexpr          : 2.8.4\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : None\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.10.1\r\nsnappy           : None\r\nsqlalchemy       : None\r\ntables           : 3.8.0\r\ntabulate         : None\r\nxarray           : None\r\nxlrd             : None\r\nzstandard        : None\r\ntzdata           : 2023.3\r\nqtpy             : 2.4.1\r\npyqt5            : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report! Confirmed on main, further investigations and PRs to fix are welcome!","I narrowed it down as far as possible within the python part of the codebase. The faulty example gives the following when running within a debug session:\r\n![2024-01-18_13-55](https:\/\/github.com\/pandas-dev\/pandas\/assets\/5563464\/0f5543ca-aab4-4900-876b-5051515fc51a)\r\nThe expected behavior (script from above) shows this: \r\n![2024-01-18_13-56](https:\/\/github.com\/pandas-dev\/pandas\/assets\/5563464\/85a19b4a-2c9c-483f-83c4-b1cf36fe3d38)\r\n\r\nYou can see the difference in the Index, in the faulty example it takes 0 as name instead of the value. From [here](https:\/\/github.com\/CYHSM\/pandas\/blob\/f459437d7a7dc10db82437aa4438f058173212a7\/pandas\/io\/parsers\/c_parser_wrapper.py#L234) onwards it calls cython code and I'm not sure how to debug it properly, are there any guidelines for this?\r\n\r\nJust from looking at the parsers I assume the error occurs somewhere here:\r\n```python\r\n        self.allow_leading_cols = allow_leading_cols\r\n        self.leading_cols = 0  # updated in _get_header\r\n\r\n\r\n        # TODO: no header vs. header is not the first row\r\n        self.has_mi_columns = 0\r\n        self.orig_header = header\r\n        if header is None:\r\n            # sentinel value\r\n            self.parser.header_start = -1\r\n            self.parser.header_end = -1\r\n            self.parser.header = -1\r\n            self.parser_start = 0\r\n            prelim_header = []\r\n        else:\r\n            if isinstance(header, list):\r\n                if len(header) > 1:\r\n                    # need to artificially skip the final line\r\n                    # which is still a header line\r\n                    header = list(header)\r\n                    header.append(header[-1] + 1)\r\n                    self.parser.header_end = header[-1]\r\n                    self.has_mi_columns = 1\r\n                else:\r\n                    self.parser.header_end = header[0]\r\n\r\n\r\n                self.parser_start = header[-1] + 1\r\n                self.parser.header_start = header[0]\r\n                self.parser.header = header[0]\r\n                prelim_header = header\r\n            else:\r\n                self.parser.header_start = header\r\n                self.parser.header_end = header\r\n                self.parser_start = header + 1\r\n                self.parser.header = header\r\n                prelim_header = [header]\r\n```\r\nhttps:\/\/github.com\/CYHSM\/pandas\/blob\/f459437d7a7dc10db82437aa4438f058173212a7\/pandas\/_libs\/parsers.pyx#L537-L571\r\n\r\nEdit: More precisely, the TextReader class seems to think the first empty line is part of the header: \r\n![2024-01-18_15-11](https:\/\/github.com\/pandas-dev\/pandas\/assets\/5563464\/53162f18-a6b5-4e07-9771-ab59a5fa3f1d)\r\nThat Level 2 should not be in there. \r\n\r\nEdit 2:\r\nThe problem seems to be the following lines: https:\/\/github.com\/pandas-dev\/pandas\/blob\/f459437d7a7dc10db82437aa4438f058173212a7\/pandas\/_libs\/parsers.pyx#L694-L698\r\nwhere an empty header can not be differentiated from an empty first value. I am still wondering why we even look at the third row, given header is specified as [0,1]. This seems to be enforced here: https:\/\/github.com\/pandas-dev\/pandas\/blob\/f459437d7a7dc10db82437aa4438f058173212a7\/pandas\/_libs\/parsers.pyx#L553-L556\r\n\r\nWithout this line, it is a trivial fix as we would not try to read the third line as a header, but I assume this is there for a reason @rhshadrach? \r\n","Thanks for investigating here!\r\n\r\n> Without this line, it is a trivial fix as we would not try to read the third line as a header, but I assume this is there for a reason @rhshadrach?\r\n\r\nAs a next step I would try removing it and see if any tests in `pandas\/tests\/io\/csv` break. If they do, the way in which they break might indicate a proper approach."],"labels":["Bug","Missing-data","IO CSV"]},{"title":"ENH: allow list-like level in MultiIndex.get_level_values","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n\r\n### Problem Description\r\n\r\nI wish MultiIndex.get_level_values will take list-like levels in addition to int\/str.\r\n\r\n### Feature Description\r\n\r\nIt looks like: `MultiIndex.get_level_values(level: Union[str, int, list[int], list[str]`\r\n\r\n```\r\nif type(level) == list_of_str:\r\n    level = list_of_int # convert level to list of int by index name\r\nnew_index = insert_level_by_idx(self, level) # create new multiindex by insertion\r\nreturn new_index\r\n```\r\n\r\n### Alternative Solutions\r\n\r\nUse the `drop_level_number` function from pandas, with `level_numbers` as those columns you want to `all_idx - level_you_want_to_keep`\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["take"],"labels":["Enhancement","Needs Triage"]},{"title":"PERF: pd.BooleanDtype in row operations is still very slow","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this issue exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this issue exists on the main branch of pandas.\n\n\n### Reproducible Example\n\nThe performance issue is the same as #52016. I assume it may be caused by some dependency missing but i installed some accelerating dependency like numba and pyarrow and the time is still larger than 10 seconds. @rhshadrach\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nshape = 250_000, 100\r\nmask = pd.DataFrame(np.random.randint(0, 1, size=shape))\r\n\r\n\r\nnp_mask = mask.astype(bool)\r\npd_mask = mask.astype(pd.BooleanDtype())\r\n\r\nassert all(isinstance(dtype, pd.BooleanDtype) for dtype in pd_mask.dtypes)\r\nassert all(isinstance(dtype, np.dtype) for dtype in np_mask.dtypes)\r\n# column operations are not that much slower\r\n%timeit pd_mask.any(axis=0) \r\n%timeit np_mask.any(axis=0)\r\n# using pandas.BooleanDtype back end for ROW operations is MUCH SLOWER\r\n%timeit pd_mask.any(axis=1)\r\n%timeit np_mask.any(axis=1)\r\n\r\n13.1 ms \u00b1 522 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n5.7 ms \u00b1 21.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n10.2 s \u00b1 1.41 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n7.74 ms \u00b1 40.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.9.18.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.0-88-generic\r\nVersion             : #98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : 3.0.8\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : 8.18.1\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.12.2\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 14.0.2\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n\n\n### Prior Performance\n\n_No response_","comments":["Thanks for the report. I cannot reproduce on 2.1.4:\r\n\r\n```\r\n15.8 ms \u00b1 16.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n4.28 ms \u00b1 9.97 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n517 ms \u00b1 1.12 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n3.88 ms \u00b1 3.21 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nhowever I get similar times on 2.0.3:\r\n\r\n```\r\n14.6 ms \u00b1 40.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n4.33 ms \u00b1 5.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n11.6 s \u00b1 23.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n3.92 ms \u00b1 7.44 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nCan you try running this on the 2.2.0 release candidate? `pip install pandas==2.2.0rc0` to install via pip.","Thank you for your response.\r\n\r\nI run it on the Linux server and my mac with pandas==2.2.0rc0 and it turns out it only run slower on the Linux, which is kinda strange. I guess it's the problem with the OS rather than the pandas library if no one else has such perf issue.\r\n\r\n```\r\n# on linux\r\nOS                    : Linux\r\nOS-release            : 5.15.0-88-generic\r\nVersion               : #98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\n\r\n11.9 ms \u00b1 139 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n5.66 ms \u00b1 32.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n12.9 s \u00b1 573 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n7.73 ms \u00b1 17.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n```\r\nDarwin\r\nOS-release            : 23.0.0\r\nVersion               : Darwin Kernel Version 23.0.0: Thu Aug 17 21:23:05 PDT 2023; root:xnu-10002.1.11~3\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\n\r\n9.85 ms \u00b1 9.67 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n2.71 ms \u00b1 26.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n263 ms \u00b1 4.98 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n2.56 ms \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```","On Linux where you have 2.1 or later installed, can you try running `pd.options.future.infer_string = True`. I have a suspicion you are (accidentally) running 2.0.x and this is an option that was only added in 2.1.","Yeah, thank you, it does work...making it 4 times faster, but still slower than 263ms. I found the `pd.options.future.infer_string = True` in the pandas whatsnew doc. Is this the default setting? Could my performance issue be due to a misuse on my part?\r\n```\r\n11.7 ms \u00b1 61.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n6.2 ms \u00b1 41.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n3.74 s \u00b1 831 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n8.21 ms \u00b1 518 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```","`pd.options.future.infer_string = True` should have no impact on the performance here, it was just an option that was added in 2.1 to tell if you were running on 2.0.x vs 2.1.x (or later). I believe you're just seeing a large (random) variance on timings - this could be the case if you have other processes on your machine using resources.","Thank you. I understand it now. Btw, could anyone else encounter the `pd.options.future.infer_string` setting issue causing the runtime larger than expected if they do not set it as `True` in advance? ","This looks like a \u201clack of 2D EAs\u201d thing","Sorry I did not get the point. Did you mean the performance downgrade comes from lacking data structure(2D EA) support?\r\n\r\n> This looks like a \u201clack of 2D EAs\u201d thing\r\n\r\n"],"labels":["Performance","Needs Info","NA - MaskedArrays","Reduction Operations"]},{"title":"TST: Fix setitem parametrizations","body":"I'm not sure if i am on the right track with this PR but it's a try. If not i will close it and probably someone more into the code has to dig into that issue.\r\n\r\n- [x] closes #56727\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":["I'd like to understand how this relates to #31948 (for which the behavior seems to have changed since it was opened).\r\n\r\nIdeally we'd do checks on the indexer once at the DataFrame\/Series level instead of at the block level which can end up doing them repeatedly.","@jbrockmendel i updated #31948 with the current behavior. What would be the desired behavior? Maybe the test has wrong expectations...","With `@pytest.mark.filterwarnings(\"ignore::FutureWarning\")`, as the only change, instead of\r\n```\r\nif type(arr) is pd.Series:\r\n    arr.iloc[idx] = arr.iloc[0]\r\nelse:\r\n    arr[idx] = arr[0]\r\n```\r\nthe main reason the test are failing is `KeyError: '[0 1 2 <NA>] not in index'` is raised. \r\nBut this could also be the desired Error?","i opened a new PR, that only changes the `setitem-py`, but does does pass all tests. See #57629","@jbrockmendel @mroeschke maybe you can have a look at  #57629"],"labels":["Testing","ExtensionArray"]},{"title":"ENH: scatter_matrix new parameter nondiagonal to allow hexbin plots i\u2026","body":"\u2026nstead of scatter\r\n\r\n- [ ] closes #56887  (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n  - running the tests gives an error unrelated to my development: ModuleNotFoundError: No module named 'pandas._libs.pandas_parser'\r\n```\r\nImportError while loading conftest '\/workspaces\/pandas\/pandas\/conftest.py'.\r\npandas\/__init__.py:49: in <module>\r\n    from pandas.core.api import (\r\npandas\/core\/api.py:1: in <module>\r\n    from pandas._libs import (\r\npandas\/_libs\/__init__.py:16: in <module>\r\n    import pandas._libs.pandas_parser  # isort: skip # type: ignore[reportUnusedImport]\r\nE   ModuleNotFoundError: No module named 'pandas._libs.pandas_parser'\r\n```\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n  - same issue as tests\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n  - didn't add yet\r\n","comments":["I have the same issue. I can import pandas normally if i switch to the parent directory(the one just outside the root pandas directory) but when I try to import pandas from the directory containing the requirements file, it fails with the same error","Thanks for the pull request, but it appears to have gone stale. If interested in continuing, please merge in the main branch, address any review comments and\/or failing tests, and we can reopen.","@mroeschke  \r\nMerged. Failed tests were unrelated to the edits in this PR"],"labels":["Enhancement","Visualization"]},{"title":"ENH: pandas.plotting.scatter_matrix to support plotting function hexbin in addition to scatter","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nI wish I could use the [scatter_matrix](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.plotting.scatter_matrix.html) function to make [hexbin](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.hexbin.html) plots\n\n### Feature Description\n\nAdd a new parameter `nondiagonal` to `scatter_matrix` to support `hexbin` instead of `scatter`\r\n\r\n```\r\ndef scatter_matrix(...):\r\n    ...\r\n    if nondiagonal==\"scatter\":\r\n        ax.scatter(...)\r\n    if nondiagonal==\"hexbin\":\r\n        ax.hexbin(...)\r\n    ...\r\n```\n\n### Alternative Solutions\n\nSeaborn pairplot using `kind = hist`. Link: https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html\n\n### Additional Context\n\nThe general consensus on `pandas.plotting.scatter_matrix` is that \"seaborn does this better\" ([ref 1](https:\/\/github.com\/pandas-dev\/pandas\/issues\/11978#issuecomment-169533788), [ref 2](https:\/\/github.com\/pandas-dev\/pandas\/issues\/13400#issuecomment-224766311)). Nevertheless, the functionality was kept, and the changes to support this feature are a few lines of code.\r\n\r\nThe quality of the plots can improve significantly. Here's an example with scatter:\r\n\r\n```\r\n# Copy of example from scatter_matrix docs page, but increase data frame rows from 1k to 10k:\r\nimport pandas as pd\r\nimport numpy as np\r\ndf = pd.DataFrame(np.random.randn(1_0_000, 4), columns=['A','B','C','D'])\r\n```\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/8392324\/eabd5cc6-56d0-47fe-957d-da1015b38d79)\r\n\r\nSame example with hexbin:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/8392324\/c9d84343-53b6-464d-a701-43b2152343b6)\r\n","comments":[],"labels":["Enhancement","Visualization","Needs Triage"]},{"title":"DOC: Private-looking symbols in the public API?","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\ndoc\/source\/reference\/extensions.rst\n\n### Documentation problem\n\ndoc\/source\/reference\/extensions.rst (and probably some of the other references) documents methods to be public even though they start with a leading _ (which typically is used to indicate private symbols).  I know pandas declares that the documentation dictates what is public (as opposed to typical Python conventions) but it seems very confusing to document private-looking symbols as being part of the public API.\r\n\r\nxref https:\/\/github.com\/pandas-dev\/pandas-stubs\/issues\/850\n\n### Suggested fix for documentation\n\nIt would be good to either\r\n\r\n- remove private-looking symbols from the documentation (if they are actually private), or\r\n- rename them and deprecate the private-looking name\r\n \r\nAfter that, it would be great to have a script in pre-commit to check for private-looking symbols in the public documentation.","comments":["I definitely think these should not be renamed since these methods should not be used directly by users because they are called internally instead. \r\n\r\nSo I guess I would support removing this from the API reference, but IMO it would be nice that these methods have exposure somewhere in the docs to signal that these are important for EA authors to implement.","> it would be nice that these methods have exposure somewhere in the docs to signal that these are important for EA authors to implement.\r\n\r\nI think in other programming languages, this would correspond to a \"protected\" symbol. Not sure whether there is a PEP for public\/protected\/private in Python, but I understand now why it would be good to keep them documented.\r\n\r\n> these methods should not be used directly by users\r\n\r\nShould they then be exposed in pandas-stubs? I guess it depends on whether we expect\/want EA authors to use pandas or pandas-stubs @Dr-Irv \r\n","> Should they then be exposed in pandas-stubs? I guess it depends on whether we expect\/want EA authors to use pandas or pandas-stubs @Dr-Irv\r\n\r\nI would expect that an EA author would use pandas-stubs.  You'd want to make sure  your EA code and tests of your EA were properly typed.  This issue came from a report in pandas-stubs where someone is trying to write an EA using the stubs to support typing.\r\n\r\nHaving said that, one of the issues with the EA interface design is that EA authors have to implement a bunch of methods and some of them have \"private\" signatures, and others do not.  See https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.api.extensions.ExtensionArray.html#pandas.api.extensions.ExtensionArray\r\n\r\nWhy do we tell EA authors to implement `_from_sequence()` and `isna()`, for example?\r\n\r\n","> Having said that, one of the issues with the EA interface design is that EA authors have to implement a bunch of methods and some of them have \"private\" signatures, and others do not.\r\n\r\nBecause the public vs private is for users of the EAs, not the authors of the EA. Unfortunately, Python only provides public vs internal indications, but we need three: public to EA user, internal to EA user but \"public\" to EA author, internal to EA author.\r\n\r\n> Why do we tell EA authors to implement `_from_sequence()` and `isna()`, for example?\r\n\r\nWe don't want EA users to call `_from_sequence` so it is marked as internal. On the other hand, EA users need to be able to call `isna`. We could have EA authors implement `_isna` and just have `isna` call `_isna`, but that seems unnecessary.","> Because the public vs private is for users of the EAs, not the authors of the EA. Unfortunately, Python only provides public vs internal indications, but we need three: public to EA user, internal to EA user but \"public\" to EA author, internal to EA author.\r\n\r\nMaybe I'm just missing something here, but let's say we have the following:\r\n- EA Author \"Mary\" authors her own EA called `MaryEA`.  Mary makes this available to others.\r\n- A pandas user \"Bob\" writes code that uses `MaryEA`.  He does this by creating `Series`, via `pd.Series(MaryEA(arguments, ...))`. \r\n\r\nMy question is then would there be any reason for Bob to call the methods on the `MaryEA` object.  Wouldn't Bob just use the methods on `Series` ?\r\n\r\nAnd, if that's the case, is there really an \"EA user\", especially since EA is documented as an abstract class, which requires you to be an EA Author (like Mary) to use it?\r\n","See also https:\/\/mail.python.org\/pipermail\/pandas-dev\/2023-January\/001559.html","> My question is then would there be any reason for Bob to call the methods on the MaryEA object. Wouldn't Bob just use the methods on Series ?\r\n\r\nThe common case would probably be that the user would just use the Series methods, yes. But there is the public `Series.array` that allows the user to access the underlying array which would be the `MaryEA`. A user may want to work with a similar Series object but without the index.","And the underlying EA could also expose additional functionality that isn't directly used by Series\/DataFrame.","> And the underlying EA could also expose additional functionality that isn't directly used by Series\/DataFrame.\r\n\r\nYes, but in my example, wouldn't that be part of the documentation of `MaryEA`, and not something we worry about?\r\n","> See also https:\/\/mail.python.org\/pipermail\/pandas-dev\/2023-January\/001559.html\r\n\r\nCopying that text here, because it is quite relevant (thanks @jbrockmendel ):\r\n\r\nFor historical reasons we've built up an EA namespace without much internal\r\nlogic in terms of what is public\/private.  While this isn't _that_ big of a\r\ndeal, it'd be nice to make this more coherent.  I see two useful options:\r\n\r\n1) Use the traditional \"an underscore means this should only be called from\r\nwithin self\".  Very few methods on the base class satisfy that\r\ncharacteristic, including the constructor _from_sequence.  One benefit of\r\nmoving to this is it would make \"official\" that we shouldn't be using\r\n_values_for_foo from outside EA methods.\r\n\r\n2) Use underscores to signal to 3rd party authors whether or not there\r\nexists a working (not necessarily performant) implementation on the base\r\nclass.  In this scenario authors would _have_ to implement private methods,\r\nwhile implementing public methods would be optional.\r\n\r\nThoughts?","> > And the underlying EA could also expose additional functionality that isn't directly used by Series\/DataFrame.\r\n> \r\n> Yes, but in my example, wouldn't that be part of the documentation of `MaryEA`, and not something we worry about?\r\n\r\nI'm giving a reason why a user might need access to the underlying EA via `.array` as @mroeschke commented, in response to https:\/\/github.com\/pandas-dev\/pandas\/issues\/56874#issuecomment-1893809202:\r\n\r\n> My question is then would there be any reason for Bob to call the methods on the `MaryEA` object. Wouldn't Bob just use the methods on `Series` ?\r\n\r\n---\r\n\r\n> Copying that text here, because it is quite relevant (thanks @jbrockmendel ):\r\n> ...\r\n> Thoughts?\r\n\r\nIn both of the proposed conventions, EA authors need to implement `_foo` methods, and so we'd want to keep these documented.\r\n","> In both of the proposed conventions, EA authors need to implement `_foo` methods, and so we'd want to keep these documented.\r\n\r\n...and typed."],"labels":["Docs","ExtensionArray"]},{"title":"PERF: timezoned series created 6x faster than non-timezoned series","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this issue exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this issue exists on the main branch of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\nIn a jupyter notebook\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nn = 1_000_000\r\n%time a = pd.Series(np.repeat(pd.Timestamp('2019-01-01 00:00:00', tz=None), n))\r\n%time b = pd.Series(np.repeat(pd.Timestamp('2019-01-01 00:00:00', tz='UTC'), n))\r\n```\r\n```\r\nPU times: user 1.88 s, sys: 11.9 ms, total: 1.89 s\r\nWall time: 1.88 s\r\nCPU times: user 315 ms, sys: 3.99 ms, total: 319 ms\r\nWall time: 319 ms\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.6.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.0-76-generic\r\nVersion             : #83-Ubuntu SMP Thu Jun 15 19:16:32 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : en_GB.utf8\r\nLANG                : C.UTF-8\r\nLOCALE              : en_GB.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.4\r\npytz                : 2023.3\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.0.0\r\npip                 : 23.2.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.14.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.7.2\r\nnumba               : 0.57.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.2\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.1\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : 0.21.0\r\ntzdata              : 2023.3\r\nqtpy                : 2.3.1\r\npyqt5               : None\r\n\r\n<\/details>\r\n\r\n\r\n### Prior Performance\r\n\r\n_No response_","comments":["Thanks for the report, confirmed on main. Almost all the time is being spent in Series construction. Further investigations are welcome.","I've started poking into this, posting a summary note before I pick this up later:\r\n* Using `cProfile` the big difference comes up in the call to `objects_to_datetime64` here https:\/\/github.com\/pandas-dev\/pandas\/blob\/e3796929283c91f0f96fed3d37b2e85bf4007ef9\/pandas\/core\/arrays\/datetimes.py#L2368\r\n* This calls out to Cython code next so it doesn't get picked up by cProfile https:\/\/github.com\/pandas-dev\/pandas\/blob\/e3796929283c91f0f96fed3d37b2e85bf4007ef9\/pandas\/_libs\/tslib.pyx#L414\r\n* The key difference in the two cases appears to happen in the next call, which is `conversion.pyx`. \r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/e3796929283c91f0f96fed3d37b2e85bf4007ef9\/pandas\/_libs\/tslibs\/conversion.pyx#L773\r\n* Specifically, because case b's values contain the `tzinfo` attribute, it calls out to `convert_datetime_to_tsobject` while case a calls out to `(<_Timestamp>val)._as_creso(creso, round_ok=True)._value`\r\n\r\nAt this stage I probably still need to find a way to confirm if there is a significant performance difference between these branches, and whether there is a reasonable fix.","If I had to guess - it would seem that the `_TSObject` that is created in case b is lighter\/faster than the `_Timestamp` created in case a?","The difference is in the resolution conversion to `ns`\r\n\r\n```\r\nIn [13]: %time a = pd.Series(np.repeat(pd.Timestamp('2019-01-01 00:00:00', tz=None), n))\r\nCPU times: user 809 ms, sys: 6.04 ms, total: 815 ms\r\nWall time: 813 ms\r\n\r\nIn [14]: %time a = pd.Series(np.repeat(pd.Timestamp('2019-01-01 00:00:00', tz=None).as_unit(\"ns\"), n))\r\nCPU times: user 344 ms, sys: 3.31 ms, total: 347 ms\r\nWall time: 347 ms\r\n```\r\n\r\nAFAICT, the non-tz case actually does a resolution conversion from \"s\" to \"ns\" while the tz case just creates a new object \"replacing\" the \"s\" resolution with \"ns\".\r\n\r\nTechnically, both should return \"s\" resolution here."],"labels":["Performance","Timezones","Constructors","Timestamp"]},{"title":"BUG: Insert then delete column into MultiIndex with timestamps leads to RecursionError","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# multiindex with the second level being a Timestamp\r\ndf = pd.DataFrame({('A', pd.Timestamp('2024-01-01')): [0]})\r\n\r\n# insert using only the top level\r\ndf.insert(1, 'B', [1])\r\n\r\nprint(df.to_string())\r\n#            A   B\r\n#   2023-01-01 NaT\r\n# 0          0   1\r\n\r\n# raises RecursionError\r\ndel df['B']\n```\n\n\n### Issue Description\n\nCreating and deleting a column is leading to an unexpected error.\r\n\r\nThis is a contrived example, but was observed in wild when joining two dataframes which had `MultiIndex` columns with str and timestamp levels with a named index, say `'Index'`. The join ends up adding a column `('Index', pd.NaT)` then deleting it to set it as the index.\n\n### Expected Behavior\n\nIt should just delete the column. \r\n\r\n```python\r\ndel df['B']\r\nprint(df.to_string())\r\n#            A \r\n#   2023-01-01\r\n# 0          0 \r\n```\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 22.5.0\r\nVersion             : Darwin Kernel Version 22.5.0: Mon Apr 24 20:51:50 PDT 2023; root:xnu-8796.121.2~5\/RELEASE_X86_64\r\nmachine             : x86_64\r\nprocessor           : i386\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : 8.20.0\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : 1.3.5\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : 2.8.7\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report - confirmed on main. Further investigations and PRs to fix are welcome!"],"labels":["Bug","Indexing","MultiIndex"]},{"title":"BUG: Inconsistent handling of `converters=...` in `read_csv` C parser and other IO functions","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport io\r\n\r\nimport pandas as pd\r\n\r\ncsv_text = 'field\\n\"1.23,4.56\"\\n'\r\n\r\n\r\ndef convert(val: str) -> list[float]:\r\n    return [float(x) for x in val.split(\",\")]\r\n\r\n\r\nconverters = {\"field\": convert}\r\n\r\n# Works\r\npd.read_csv(io.StringIO(csv_text), converters=converters, engine=\"c\")\r\n\r\n# Doesn't work\r\npd.read_csv(io.StringIO(csv_text), converters=converters, engine=\"python\")\r\n\r\n# Doesn't work (openpyxl)\r\nwith io.BytesIO() as xlsx_buffer:\r\n    pd.read_csv(io.StringIO(csv_text)).to_excel(xlsx_buffer, index=False)\r\n    xlsx_buffer.seek(0)\r\n\r\n    pd.read_excel(xlsx_buffer, converters=converters)\r\n\r\n\r\n# Doesn't work (lxml)\r\nwith io.BytesIO() as xml_buffer:\r\n    pd.read_csv(io.StringIO(csv_text)).to_xml(xml_buffer, index=False)\r\n    xml_buffer.seek(0)\r\n\r\n    pd.read_xml(xml_buffer, converters=converters)\n```\n\n\n### Issue Description\n\nUnsure if bug or just a lack of documentation, but depending on the method converters is handled differently for different methods, and for csvs it is dependent on the engine.\r\n\r\nIt appears the python parser will not handle unhashable types for the converted values, but the `C` parser for `read_csv` will.\n\n### Expected Behavior\n\nEach IO method with converter handles the argument the same.\r\n\r\nI can see merits for either way, ie converters should only convert to a specific scalar value and transformation is otherwise done after read, or given pandas supports complex types as values you can use converters for any arbitrary conversion.\r\n\r\nIn either case I would expect the use of the argument to be the same for any IO method for which it is present, or for it to be at least documented more clearly.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.12.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.133.1-microsoft-standard-WSL2\r\nVersion             : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : C.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.3\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 5.1.0\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 14.0.2\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thank you for your interesting post! I see your example attempts to convert a string value to a `list` type within a column. The intended use of `converters` is more for scalar, atomic values and not embedded iterables. Hence, why the failed attempts raise:\r\n\r\n> TypeError: unhashable type: 'list'\r\n\r\nArguably, you can use a different data structure to store such embedded lists than the DataFrame which is intended to store byte-size data of primitive types (`boolean`, `str`, `int`, `float`) included their extensions in a two-dimensional array.\r\n\r\nAlso, unlike `read_excel` and `read_xml` that interface to external packages, `read_csv` uses pandas specific Cython and Python libraries simply given that the delimited text file is the most popular, defacto industry data format. Hence, more custom features are available to read CSV text files and handling of this special need to convert string to list differ between the IO methods. \r\n\r\nUsers can always run [`Series.str.split`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.str.split.html) after parsing the data: `df[\"field\"] = df[\"field\"].str.split(',')`. Really, `converters` may serve as a convenience feature not meant to handle complex cases, requiring ad-hoc data wrangling.","The documentation is not clear on the intended of converters if that is the case I don't think.\r\n\r\nThe docs say:\r\n> Dict of functions for converting values in certain columns. Keys can either be integers or column labels.\r\n\r\nIt is a little more descriptive for read_excel but still does not specifiy it is intended to return scalars, and nor does the user guide on I\/O.","Understood. The term _values_ is likely intended to generalize for the base case for data analytics that interact with scalars in cells and not complex types like embedded iterables and objects. Docs would need to expound on all unhashable types and class objects in Python of which is open-ended in number: lists, dicts, sets, numpy arrays, pandas Series\/DataFrames, ...\r\n\r\nBut `converters` mainly depends on the method used. In fact, had you used a hashable type like `tuple` in method, `converters` works across the IO methods in your example! And all scalar types (`bool`, `integer`, `float`, `str`, `None`) are hashable.\r\n\r\n```python\r\ndef convert(val: str) -> dict[float]:\r\n    return tuple(float(x) for x in val.split(\",\"))\r\n```\r\n\r\nMaybe docs can indicate that methods of `converters` should return hashable types if not using the c engine in `read_csv`? But then this may get deep under the hood since data analysts and scientists, principal users of pandas, would need to know what are hashable types which arguably fall in the domain of computer programming.","Understood on hashable types, but the current error message when converters returns a hashable type when using the python parser is just `TypeError: unhashable type: 'list'` (full traceback below), so a user is forced to know about hashable types to understand the error message. Probably an argument there that the error needs to be reemitted with a bit more detail too.\r\n\r\nCould the docs just link to the page on data types ([here](https:\/\/pandas.pydata.org\/docs\/reference\/arrays.html) or [here](https:\/\/pandas.pydata.org\/docs\/user_guide\/basics.html#basics-dtypes)) and highlight that a converter function is intended to be used to return one of the concrete types there, with a note that something that would be classified as an object dtype isn't guaranteed to have support? (reality may be that it needs to be a numpy type based on the source code for the function that errors out expects an ndarray)\r\n\r\n\r\n<details>\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 948, in read_csv\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 617, in _read\r\n    return parser.read(nrows)\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/readers.py\", line 1748, in read\r\n    ) = self._engine.read(  # type: ignore[attr-defined]\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/python_parser.py\", line 289, in read\r\n    conv_data = self._convert_data(data)\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/python_parser.py\", line 360, in _convert_data\r\n    return self._convert_to_ndarrays(\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/base_parser.py\", line 565, in _convert_to_ndarrays\r\n    cvals, na_count = self._infer_types(\r\n  File \"<snip>\/python3.10\/site-packages\/pandas\/io\/parsers\/base_parser.py\", line 743, in _infer_types\r\n    na_count = parsers.sanitize_objects(values, na_values)\r\n  File \"parsers.pyx\", line 2148, in pandas._libs.parsers.sanitize_objects\r\nTypeError: unhashable type: 'list'\r\n```\r\n\r\n","What docs do you recommend the change and what specific wording change? Usually, we want to keep pandas docs brief and not expound to cover every possible use case.  Also, we would need to test each of those concrete types to find what `converters` can support. \r\n\r\nSince the `converters` argument is not central for the IO read functions but serves more as a convenience function to handle `dtypes`, I wonder if extended doc notes would cause more confusion than not. But if you insist it is worthwhile, I can defer to others on the team."],"labels":["Bug","Needs Triage"]},{"title":"ENH: Processing of .mask() for pd.NA","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\n```python\r\ndf = pd.DataFrame(\r\n    {\r\n        'A': [0, 1, 2]\r\n    },\r\n)\r\n```\r\nFor example, suppose you have a DataFrame like the one above.\r\n\r\n```python\r\n>>> df[\r\n...     pd.Series([-1, 1, pd.NA]) < 0\r\n... ]\r\n\tA\r\n0\t0\r\n```\r\nThis works as expected.\r\n\r\n```python\r\n>>>df[\r\n...     pd.Series([-1, 1, pd.NA]).convert_dtypes() < 0\r\n... ]\r\n\tA\r\n0\t0\r\n```\r\nThis also works as expected.\r\n\r\n```python\r\n>>> df['A'].mask(\r\n...     pd.Series([-1, 1, pd.NA]) < 0,\r\n...     100\r\n... )\r\n\r\n0    100\r\n1      1\r\n2      2\r\nName: A, dtype: int64\r\n```\r\nThis also works as expected.\r\n\r\n```python\r\n>>> df['A'].mask(\r\n...     pd.Series([-1, 1, pd.NA]).convert_dtypes() < 0,\r\n...     100\r\n... )\r\n\r\n0    100\r\n1      1\r\n2    100  # !?\r\nName: A, dtype: int64\r\n```\r\nThis behavior confuses a lot of people.\r\n\n\n### Feature Description\n\nI think most people would expect this result.\r\n```python\r\n>>> df['A'].mask(\r\n...     pd.Series([-1, 1, pd.NA]).convert_dtypes() < 0,\r\n...     100\r\n... )\r\n0    100\r\n1      1\r\n2      2\r\nName: A, dtype: int64\r\n```\r\n\r\n\r\nI think you should either set the result of the logical operation on pd.NA to False \r\n```python\r\n>>> pd.Series([-1, 1, pd.NA]).convert_dtypes() < 0\r\n0     True\r\n1    False\r\n2    False\r\ndtype: boolean\r\n```\r\nor change the .mask() method to make pd.NA behave the same as False.\r\n\r\n\n\n### Alternative Solutions\n\nUsing .fillna(False) will do what you expect, but I think it would be a depressing task for many people.\r\n```python\r\n>>> df['A'].mask(\r\n...     (pd.Series([-1, 1, pd.NA]).convert_dtypes() < 0).fillna(False),\r\n...     100\r\n... )\r\n0    100\r\n1      1\r\n2      2\r\nName: A, dtype: int64\r\n```\n\n### Additional Context\n\n_No response_","comments":["Thanks for the bug report. Yea seems like mask is not properly handing pd.NA from your OP - pull requests to fix are always welcome!"],"labels":["Bug","Missing-data"]},{"title":"BUG: parquet serialization\/deserialization adds all dict keys into column","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\npd.DataFrame({'dictcol': [{'a': 1}, {'b': 2}, {'c': None}]}).to_parquet('\/tmp\/data.pqt')\r\npd.read_parquet('\/tmp\/data.pqt')\r\n# loaded dataframe contains all keys in every row\n```\n\n\n### Issue Description\n\nI have a column of type `dict[str, int]`, If I save and load the dataframe to parquet, every entry in column is filled with all keys.\r\n\r\nSo there are two problems: 1. it does not faithfully represents what was saved 2. it blows up because there are many keys that are resent in one-two rows.\r\n\r\nMaybe relevant (not sure):  https:\/\/github.com\/pandas-dev\/pandas\/issues\/55776\n\n### Expected Behavior\n\nSaved and loaded dataframes are identical.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.12.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 22.4.0\r\nVersion             : Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3\/RELEASE_ARM64_T6000\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.3\r\npytz                : 2022.7\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.6.3\r\npip                 : 23.0.1\r\nCython              : 0.29.33\r\npytest              : 7.2.0\r\nhypothesis          : None\r\nsphinx              : 5.3.0\r\nblosc               : None\r\nfeather             : 0.4.1\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.2\r\nhtml5lib            : None\r\npymysql             : 1.0.2\r\npsycopg2            : 2.9.5\r\njinja2              : 3.1.2\r\nIPython             : 8.3.0\r\npandas_datareader   : None\r\nbs4                 : 4.11.1\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : 2023.8.0\r\nfsspec              : 2023.9.0\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : 0.58.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.0.10\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.9.3\r\nsqlalchemy          : 2.0.4\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2022.7\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Update: this seems to be a behavior of pyarrow, because in fastparquet example above seems to work correctly.","cc @jorisvandenbossche - any insights here?","The reason for this behaviour is that in the conversion from pandas to Arrow, the default for dictionaries is to convert this to a `struct` type, while you probably want a `map` type. \r\n(a struct has a fixed set of keys present on each row (and so the conversion unifies the keys for all rows), while a map has a fixed type for the key and value (like `dict[str, int]`), but then can have variable keys)\r\n\r\nSo the default behaviour:\r\n\r\n```\r\nIn [17]: df = pd.DataFrame({'dictcol': [{'a': 1}, {'b': 2}, {'c': None}]})\r\n\r\nIn [18]: df\r\nOut[18]: \r\n       dictcol\r\n0     {'a': 1}\r\n1     {'b': 2}\r\n2  {'c': None}\r\n\r\nIn [20]: df.to_parquet('\/tmp\/data.pqt')\r\n\r\nIn [21]: pd.read_parquet('\/tmp\/data.pqt')\r\nOut[21]: \r\n                             dictcol\r\n0   {'a': 1.0, 'b': None, 'c': None}\r\n1   {'a': None, 'b': 2.0, 'c': None}\r\n2  {'a': None, 'b': None, 'c': None}\r\n```\r\n\r\nIntroducing all None (null) values for the keys that weren't initially present.\r\n\r\nYou can override the default conversion by providing the Arrow schema that the pandas.DataFrame should be converted to:\r\n\r\n\r\n```\r\nIn [22]: df.to_parquet('\/tmp\/data2.pqt', schema=pa.schema([(\"dictcol\", pa.map_(pa.string(), pa.int64()))]))\r\n\r\nIn [23]: pd.read_parquet('\/tmp\/data2.pqt')\r\nOut[23]: \r\n       dictcol\r\n0   [(a, 1.0)]\r\n1   [(b, 2.0)]\r\n2  [(c, None)]\r\n```\r\n\r\nThe default conversion of a map type from arrow to pandas, however, then gives you tuples and not dicts (that's because the spec allows duplicate keys, which wouldn't be representable by dicts). This can be overridden with a keyword, however in the `pandas.read_parquet`, it seems we currently don't allow to pass kwargs to the `pyarrow.Table.to_pandas` call, only to the parquet reading (we should somehow allow this, I think). So therefore illustrating this by reading in two steps (reading Parquet + converting to pandas):\r\n\r\n```\r\nIn [25]: import pyarrow.parquet as pq\r\n\r\nIn [26]: table = pq.read_table(\"\/tmp\/data.pqt\")\r\n\r\nIn [27]: table.to_pandas(maps_as_pydicts=\"strict\")\r\nIn [27]: \r\n       dictcol\r\n0   {'a': 1.0}\r\n1   {'b': 2.0}\r\n2  {'c': None}\r\n```\r\n\r\nSee https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.Table.html#pyarrow.Table.to_pandas for the keywords that you can specify to control the conversion from pandas to Arrow.\r\n\r\n> Update: this seems to be a behavior of pyarrow, because in fastparquet example above seems to work correctly.\r\n\r\nIt seems that the reason this roundtrips better with fastparquet, is because they store the python dictionaries serialized as JSON text, and then load that again as dicts. ","> it seems we currently don't allow to pass kwargs to the `pyarrow.Table.to_pandas` call, only to the parquet reading (we should somehow allow this, I think).\r\n\r\nExisting issue for this: https:\/\/github.com\/pandas-dev\/pandas\/issues\/49236","I don't see a rationale for structs to be default. \r\nBut well, ok, if that *should* be default for some reason - then some kind of warning should appear. \r\n\r\nSee, this serialization is not reversible - I can't say if key way not present or just value was None.\r\nIt is also subtle: I've noticed this behavior after been using parquets for serialization for a year. ","> I don't see a rationale for structs to be default.\r\n\r\nMaps are more restrictive than struct in certain ways: all values should have the same type (Arrow could do a first pass through the data to check that, and then base a decision on that, but that would also make it less predictable and data dependent). My feeling is also that in general structs are more common. \r\nAnyway, there are two options and pyarrow has to choose some default, so there is always going to be a group that wants the other default ..\r\n\r\nIn the end, the problem is that pandas does not (yet) have a proper struct and map type of its own, and so you have to use an object dtype with actual python dictionaries, and so the conversion to pyarrow always has to guess. \r\n\r\n(you can actually use the experimental ArrowDtype (https:\/\/pandas.pydata.org\/docs\/user_guide\/pyarrow.html) to have a map dtype in pandas, but I don't know if there are much operations supported already specifically for maps)\r\n\r\n> then some kind of warning should appear.\r\n\r\nI certainly understand that it is annoying to have to figure that you have data loss and why, but it is not necessarily that easy to know when to warn for this (to know the intention of the user). Maybe pyarrow could warn specifically in the case if the keys from the first row are not the same in all subsequent rows (I don't know how common this would be in cases you want it to be the default struct, because then that warning would be annoying). But in any case, this is a discussion for pyarrow -> https:\/\/github.com\/apache\/arrow\/issues"],"labels":["Usage Question","IO Parquet","Arrow"]},{"title":"DEPR: rename offset alias \u2018MS\u2019 to \u2018MB\u2019","body":"As a follow up of renaming `\u2018M\u2019` to `\u2018ME\u2019` for MonthEnd (#52064) I suggest to rename strings denoting `MonthBegin` frequency from `\u2018MS\u2019` to `\u2018MB\u2019`.\r\n\r\nThe reason: we have alias `\u2018ms\u2019` for the class Milli and using `\u2018MB\u2019` for MonthBegin will be more clear. It will help to create consistent naming for aliases.","comments":["Might be too late but it would make the life of downstream libraries easier if this was also included in 2.2 (if you do it). FYI @spencerkclark ","Would that only be for month? Then it would be inconsistent with the other freq strings, which could be confusing (`\"MB\"` and `\"YS\"`).","> Would that only be for month? Then it would be inconsistent with the other freq strings, which could be confusing (`\"MB\"` and `\"YS\"`).\r\n\r\nrenaming the alias for MonthBegin to \"MS\" is just the beginning. The whole list of aliases to change:\r\n`\"YE\", \"BYE\", \"QE\", \"BQE\", \"ME\", \"BME\", \"SME\", \"CBME\"`","\r\n\r\n\r\n> Might be too late but it would make the life of downstream libraries easier if this was also included in 2.2 (if you do it). FYI @spencerkclark\r\n\r\nunfortunately there is not enough time to resolve this issue. The reason: after renaming  `\"MS\"` to `\"MB\" `we need to rename other aliases denoting end of some time period.","> unfortunately there is not enough time to resolve this issue. The reason: after renaming \"MS\" to \"MB\" we need to rename other aliases denoting end of some time period.\r\n\r\nInconvenient but understandable.\r\n\r\n> renaming the alias for MonthBegin to \"MS\" is just the beginning. The whole list of aliases to change: `\"YE\", \"BYE\", \"QE\", \"BQE\", \"ME\", \"BME\", \"SME\", \"CBME\"`\r\n\r\nThese are already implemented for pandas 2.2, right? And these are all consistent: all use `\"E\"` for end. But changing _only_ `\"MS\"` to `\"MB\"`, will leave us with `\"YS\"`, `\"QS\"`, `\"MB\"`, ...., which I would find confusing.\r\n\r\nBut maybe I missed\/ misunderstood something.\r\n\r\n","> > unfortunately there is not enough time to resolve this issue. The reason: after renaming \"MS\" to \"MB\" we need to rename other aliases denoting end of some time period.\r\n> \r\n> Inconvenient but understandable.\r\n> \r\n> > renaming the alias for MonthBegin to \"MS\" is just the beginning. The whole list of aliases to change: `\"YE\", \"BYE\", \"QE\", \"BQE\", \"ME\", \"BME\", \"SME\", \"CBME\"`\r\n> \r\n> These are already implemented for pandas 2.2, right? And these are all consistent: all use `\"E\"` for end. But changing _only_ `\"MS\"` to `\"MB\"`, will leave us with `\"YS\"`, `\"QS\"`, `\"MB\"`, ...., which I would find confusing.\r\n> \r\n\r\nsorry, it's my mistake. You are right, the renaming \"Y\", \"Q\", etc. to \"YE\", \"QE\", etc. is already implemented for pandas 2.2. \r\n\r\nThe list of aliases to change: \u201cYS\u201d, \"BYS\u201d, \"QS\u201d, \"BQS\u201d, \"MS\u201d, \"BMS\u201d, \"SMS\u201d, \"CBMS\u201d.\r\n\r\n","Hi all,\r\nOn one hand, I agree that it makes more sense to have \"MB\" as the class is named \"MonthBegin\". On the other, theses changes are very breaking for us (developpers and users of [xclim](https:\/\/github.com\/Ouranosinc\/xclim\/)). By the nature of the library, we have those strings \"hardcoded\" everywhere in the code, but even more so in user code.\r\n\r\nI personally think that MS is fine as it is and that the change isn't worth the hassle. At the least, if all the breaking changes were in the same version (2.2), it would be much easier to work around!\r\n\r\nI know our usage isn't representative of the vast majority of pandas' users, but I thought it was pertinent to raise that flag here. Thanks!","-0, there were some of these that were a real problem, but AFAICT those have all been handled","thanks all for your comments!\r\n\r\nthere kind of still is a problem though. currently:\r\n- `pd.date_range('2000', periods=3, freq='YS')` is valid (YearBegin)\r\n- but `pd.date_range('2000', periods=3, freq='ys')` is also valid, and does the same thing (YearBegin)\r\n\r\nhowever:\r\n- `pd.date_range('2000', periods=3, freq='MS')` is valid (MonthBegin)\r\n- but `pd.date_range('2000', periods=3, freq='ms')` is also valid, and means something completely different (millisecond)\r\n\r\nOne solution, as Natalia's suggesting, is to rename the *S ones to *B (all in one go - it's too late for 2.2, this would have to be for the 3.x cycle if agreed)\r\nBut another solution would be to disallow mixed-casing. So, 'YS' would mean YearBegin, and 'ys' would raise. Else, if people get used to being able to pass both uppercase and lowercase, then the `'MS'` \/ `'ms'` difference is going to bite them","If I may, from my perspective disallowing mixed-casing would be a preferable option. Offsets are described in the doc with a specific case, and same case is always used in examples (AFAIK). Disallowing mixed-case is still a \"regression\" as it removes a \"feature\", but at least it does not break the documented usage of the library.","thanks for your input, this is useful! that may indeed be better"],"labels":["Frequency","Deprecate","Needs Discussion"]},{"title":"BUG: pickle.load() does not work with pandas1.4.2 files if pandas 2.1.4 is installed","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n# conda activate pandas2.1.4-env\r\nimport pickle\r\nd = pickle.load(open('orig.pkl', 'rb'))\r\n  File \"miniconda3\/envs\/terratest\/lib\/python3.9\/site-packages\/pandas\/core\/internals\/blocks.py\", line 2400, in new_block\r\n    return klass(values, ndim=ndim, placement=placement, refs=refs)\r\nTypeError: Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)\r\n\r\n# conda activate pandas1.4.2-env\r\nd = pickle.load(open('orig.pkl', 'rb'))\r\npickle.dump(d, open('mod.pkl', 'wb'))\r\n\r\n# conda activate pandas2.1.4-env\r\nimport pickle\r\nd = pickle.load(open('mod.pkl', 'rb'))\r\n# no error\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nHi, I have a pickle file (uncertain which version it was created with, it's protocol version 4). When I try to open it with pandas 2.1.4, I get this error:\r\nFile \"[...]\/miniconda3\/envs\/terratest\/lib\/python3.9\/site-packages\/pandas\/core\/internals\/blocks.py\", line 2400, in new_block\r\nreturn klass(values, ndim=ndim, placement=placement, refs=refs)\r\nTypeError: Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)\r\n\r\nWhen I load this with pandas 1.4.2 and dump it again, without changing anything, I can load it fine with pandas 2.1.4.\r\nThis is all with python 3.9.18.\r\nHow can I get to the bottom of this? I don't want to attach the file here but happy to pm it\r\nHappy to provide my pickle file to the person who is investigating, just not the entire internet.\r\n\r\n### Expected Behavior\r\n\r\norig.pkl should load in pandas2.1.4 environment\r\n\r\n### Installed Versions\r\n\r\npandas2.1.4-env:\r\n\r\nminiconda3\/envs\/terratest\/lib\/python3.9\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.9.18.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.133.1-microsoft-standard-WSL2\r\nVersion             : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : C.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.0.0\r\npip                 : 23.2.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.7.3\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.10.1\r\nsqlalchemy          : 2.0.6\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\npandas1.4.2-env:\r\npd.show_versions()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/pandas\/util\/_print_versions.py\", line 109, in show_versions\r\n    deps = _get_dependency_info()\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/pandas\/util\/_print_versions.py\", line 88, in _get_dependency_info\r\n    mod = import_optional_dependency(modname, errors=\"ignore\")\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/pandas\/compat\/_optional.py\", line 138, in import_optional_dependency\r\n    module = importlib.import_module(name)\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/importlib\/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/setuptools\/__init__.py\", line 7, in <module>\r\n    import _distutils_hack.override  # noqa: F401\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/_distutils_hack\/override.py\", line 1, in <module>\r\n    __import__('_distutils_hack').do_override()\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/_distutils_hack\/__init__.py\", line 77, in do_override\r\n    ensure_local_distutils()\r\n  File \"miniconda3\/envs\/joa\/lib\/python3.9\/site-packages\/_distutils_hack\/__init__.py\", line 64, in ensure_local_distutils\r\n    assert '_distutils' in core.__file__, core.__file__\r\nAssertionError: miniconda3\/envs\/joa\/lib\/python3.9\/distutils\/core.py\r\n\r\nbut alternatively:\r\n$ pip freeze\r\nbrotlipy==0.7.0\r\ncertifi==2020.6.20\r\ncffi==1.16.0\r\nclick==8.1.3\r\ncolorama==0.4.4\r\ncycler==0.11.0\r\nfonttools==4.33.3\r\nkiwisolver==1.4.2\r\nmatplotlib==3.5.1\r\nnumpy==1.22.3\r\npackaging==21.3\r\npandas==1.4.2\r\nPillow==9.1.0\r\npycparser==2.21\r\npyparsing==3.0.8\r\nPySide2==5.15.2\r\npython-dateutil==2.8.2\r\npytz==2022.1\r\nruamel.yaml==0.15.87\r\nscipy==1.8.0\r\nseaborn==0.11.2\r\nshiboken2==5.15.2\r\nsix==1.16.0\r\nwincertstore==0.2","comments":["same problem, so I use pd.read_pickle() instead and it works well.","Thanks for the tips.  pd.read_pickle() works very well :) "],"labels":["Bug","IO Pickle","Needs Triage"]},{"title":"DEPR: make_block","body":"`make_block` was deprecated and then reverted before the 2.2 release: https:\/\/github.com\/pandas-dev\/pandas\/pull\/56481\r\n\r\nOn the dev call today (2024-01-10), it was agreed that a deprecation would be issued during 3.0","comments":["Thinking about this again, I wonder if we need to explicitly deprecate this function, or if this would be auto-deprecated when we deprecate the entirety of core for 3.0."],"labels":["Internals","Deprecate","Blocker for rc"]},{"title":"BUG:  read_excel fuction return \"zipfile.BadZipFile: File is not a zip file\"","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\ndf = pd.read_excel(excel_file_path, sheet_name=sheet_name, engine='openpyxl')\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nProblem is a little bit odd due to a problem related only to an excel. I try to read some data from an excel, the code works perfectly fine with an excel created by me. I have an excel from PTC tool and with that excel when i try to read from it i get this error: \r\n\r\nraise BadZipFile(\"File is not a zip file\")\r\nzipfile.BadZipFile: File is not a zip file\r\n\r\nDid somebody know what problem can have an excel to raise this error? I can't put that excel here due to privacy policies, but it's not a complex one, actually have only some pivoted rows and tables, no functions behind or something else. Maybe one more thing to be mentioned, this excel was set to confidential as security manager , but i tried to remove that attribute and I've got the same error.\r\n\r\n### Expected Behavior\r\n\r\nRead from that excel\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nPython - 3.6.4\r\nOffice package - 2021\r\n<\/details>\r\n","comments":["I have come across this issue before with Microsoft and it's due to the sensitivity tag on the document.  The tag has the file encrypted which prevents it from being read correctly. The Microsoft's sensitivity tag can only be removed by the owner who placed it, or your orgs administrator.  Some orgs. will have that label automatically applied.\r\n\r\n\r\n\r\n","@fletcherjacob can you tell me where should I find those tags please? I think it is a place where are all displayed or something like that","If the confidential label\/tag is enabled, the document owner or the person to set it will have to remove it. Some organizations have this enabled by default and only a permission admin can remove it from the file. \r\nAre you able to remove the Sensitivity Label via the Excel application? If yes, are you still experiencing the error?  If you create an Excel file and apply the sensitivity label, Do you still experience the issue?\r\n\r\n\r\n**Regarding Microsoft Sensitivity Labels:**\r\nhttps:\/\/learn.microsoft.com\/en-us\/purview\/sensitivity-labels\r\n"],"labels":["Bug","IO Excel","Needs Info"]},{"title":"BUG: Interchange with PyArrow types does not match PyArrow behavior","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n>>> import datetime\r\n>>> import pandas as pd\r\n>>> df = pd.DataFrame([[datetime.date(2023, 1, 1)]], dtype=pd.ArrowDtype(pa.date32()))\r\n>>> df.dtypes\r\n0    date32[day][pyarrow]\r\ndtype: object\r\n\r\n>>> df.__dataframe__().get_column(0).get_buffers()\r\n{'data': (PandasBuffer({'bufsize': 8, 'ptr': 103818415071840, 'device': 'CPU'}), (<DtypeKind.DATETIME: 22>, 64, 'tdD', '=')), 'validity': None, 'offsets': None}\r\n\r\n>>> pa.Table.from_pandas(df).__dataframe__().get_column(0).get_buffers()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/willayd\/mambaforge\/envs\/pantab-dev\/lib\/python3.12\/site-packages\/pyarrow\/interchange\/dataframe.py\", line 139, in get_column\r\n    return _PyArrowColumn(self._df.column(i),\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/willayd\/mambaforge\/envs\/pantab-dev\/lib\/python3.12\/site-packages\/pyarrow\/interchange\/column.py\", line 239, in __init__\r\n    self._dtype = self._dtype_from_arrowdtype(dtype, bit_width)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/willayd\/mambaforge\/envs\/pantab-dev\/lib\/python3.12\/site-packages\/pyarrow\/interchange\/column.py\", line 322, in _dtype_from_arrowdtype\r\n    raise ValueError(\r\nValueError: Data type date32[day] not supported by interchange protocol\n```\n\n\n### Issue Description\n\nI find it surprising that pandas provides a different value to the interchange protocol for an Arrow dtype than Arrow itself would (side note, I am also surprised Arrow fails here)\r\n\r\ncc @MarcoGorelli \n\n### Expected Behavior\n\nShould match arrow\n\n### Installed Versions\n\nmain","comments":["> I find it surprising that pandas provides a different value to the interchange protocol for an Arrow dtype than Arrow itself\r\n\r\nEssentially the current implementation of the interchange protocol in pandas is based on numpy, so it converts (or tries to convert) whatever other type (our own nullable, or Arrow dtypes) to numpy. Which is the root cause of the various issues you opened, I think (https:\/\/github.com\/pandas-dev\/pandas\/issues\/56805, https:\/\/github.com\/pandas-dev\/pandas\/issues\/56777)","I assume there is historical reasoning to that, but is there any reason you think it should remain that way going forward?","No, there is definitely no good reason to not support it going forward, it was just to limit the scope of the initial PR I assume. "],"labels":["Bug","Interchange"]},{"title":"BUG: \"str.contains\" match groups UserWarning discourages best practices & slows performance","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\ntest_regex_groups_warning.py\r\n\r\nimport pandas as pd\r\n\r\n\r\nfruits = pd.Series([\"apple\", \"banana\", \"apple and banana\"])\r\n\r\n\r\ndef test_user_warning_in_series():\r\n    print(\"fruits:\\n\", fruits)\r\n    print(\"UNNAMED REGEX\")  # less accessible\r\n    nand_query = r\"^(?!(?=.*apple)(?=.*banana)).*$\"\r\n    print(f\"'{nand_query}'\")\r\n    filtered = fruits[fruits.str.contains(nand_query)]\r\n    print(\"filtered:\\n\", filtered)\r\n    nand_condition = ~(\r\n        filtered.str.contains(\"head\", case=False)\r\n        & filtered.str.contains(\"neck\", case=False)\r\n    )\r\n    print(\"nand_condition:\\n\", nand_condition)\r\n    if not nand_condition.all():\r\n        failures = filtered[~nand_condition]\r\n        print(\"failures:\\n\", failures)\r\n    assert nand_condition.all()\r\n    assert 0\r\n\r\n\r\ndef test_user_warning_with_names():\r\n    print(\"fruits:\\n\", fruits)\r\n    print(\"NAMED REGEX\")  # more clear about what is matched and why:\r\n    named_nand_query = r\"(?P<nand>^(?P<not_>(?!(?P<and_>(?=.*apple)(?=.*banana)))).*$)\"\r\n    print(f\"'{named_nand_query}'\")\r\n    filtered_named = fruits[fruits.str.contains(named_nand_query)]\r\n    print(\"filtered_named:\\n\", filtered_named)\r\n    named_nand_condition = ~(\r\n        filtered_named.str.contains(\"head\", case=False)\r\n        & filtered_named.str.contains(\"neck\", case=False)\r\n    )\r\n    print(\"named_nand_condition:\\n\", named_nand_condition)\r\n    if not named_nand_condition.all():\r\n        named_failures = filtered_named[~named_nand_condition]\r\n        print(\"named_failures:\\n\", named_failures)\r\n    assert named_nand_condition.all()\r\n    assert 0\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/24532336\/80ddc993-141a-4866-8c1c-761a68706afa)\r\n\r\n@phofl wrote re: [PR 56763 to remove this warning](https:\/\/github.com\/pandas-dev\/pandas\/pull\/56763)\r\n\r\n> Please open issues to facilitate a discussion before you open a PR\r\n\r\nSorry, my bad, here is an issue to discuss the warning I proposed to remove.\r\n\r\nPRE-TL;DR: \r\n- Named groups aid regex understanding and maintenance\r\n- The warning logic also slows down pandas regex perf\r\n\r\nresult of running reproducible example:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/24532336\/6853f1ad-2218-40b7-bb19-16a87d046a2c)\r\n\r\nThis warning is annoying and serves to discourage use of named capturing groups (which are more maintainable) because users must either switch to extracting the groups (not always necessary) or replace their named groups with \"(?:\" (noncapturing groups are harder to maintain because it's less clear what is their objective within a greater regex pattern).\r\n\r\nIf users need to specialize their regex patterns to each command, then they need to maintain multiple copies, some with non-captured groups, some without, just to silence some warning, also, if they remove the groups, then later on when they want to use them, they might have to figure out how to replace groups they removed just to silence a warning, and be frustrated.\r\n\r\nThe logical condition for the warning also compiles the pattern but then the compiled pattern is discarded, so this warning slows down every \"contains\" in pandas just to check if we should annoy people who probably know what they're doing.\r\n\r\nIf we remove this unnecessary warning, then we no longer discourage users who use named capturing groups, thus facilitating readability of the patterns, and portability to other contexts, such as debuggers or the \"extract\" method mentioned in the removed warning.\r\n\r\n**TL;DR: This warning doesn't need to exist, discourages best practices, and slows performance of every string contains query in pandas (a super \"hot path!\"), so I suggest we remove it.**\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/24532336\/092ff20a-1175-4b07-8e23-8215eb25753d)\r\n\r\n[here is a permalink to the line of code to check for the warning condition](https:\/\/github.com\/pandas-dev\/pandas\/blob\/f0cdc7c2c60c91c35f08766be88791a59b24ee01\/pandas\/core\/strings\/accessor.py#L1323)\r\n```diff\r\n        dtype: bool\r\n        \"\"\"\r\n-        if regex and re.compile(pat).groups: # compiled and discarded the pattern\r\n-            warnings.warn(\r\n-                \"This pattern is interpreted as a regular expression, and has \"\r\n-                \"match groups. To actually get the groups, use str.extract.\",\r\n-                UserWarning,\r\n-                stacklevel=find_stack_level(),\r\n-            )\r\n\r\n        result = self._data.array._str_contains(pat, case, flags, na, regex)\r\n        return self._wrap_result(result, fill_value=na, returns_string=False)\r\n```\r\njust compile the pattern and use the compiled pattern:\r\n```diff\r\n        dtype: bool\r\n        \"\"\"\r\n        result = self._data.array._str_contains(pat, case, flags, na, regex)\r\n        return self._wrap_result(result, fill_value=na, returns_string=False)\r\n``\r\noptionally (may cause issues depending on _str_contains logic:\r\n```diff\r\n- result = self._data.array._str_contains(pat, case, flags, na, regex)\r\n+ compiled_pattern = re.compile(pat, flags)\r\n+ result = self._data.array._str_contains(compiled_pattern, case, flags, na, regex)\r\n```\r\n\r\n### Expected Behavior\r\n\r\nno warning for containment queries using groups\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n> python\r\nPython 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pandas as pd\r\n>>> pd.show_versions()\r\n\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/pandas\/util\/_print_versions.py\", line 141, in show_versions\r\n    deps = _get_dependency_info()\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/pandas\/util\/_print_versions.py\", line 98, in _get_dependency_info\r\n    mod = import_optional_dependency(modname, errors=\"ignore\")\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/pandas\/compat\/_optional.py\", line 132, in import_optional_dependency\r\n    module = importlib.import_module(name)\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/importlib\/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/numba\/__init__.py\", line 42, in <module>\r\n    from numba.np.ufunc import (vectorize, guvectorize, threading_layer,\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/numba\/np\/ufunc\/__init__.py\", line 3, in <module>\r\n    from numba.np.ufunc.decorators import Vectorize, GUVectorize, vectorize, guvectorize\r\n  File \"\/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\/numba\/np\/ufunc\/decorators.py\", line 3, in <module>\r\n    from numba.np.ufunc import _internal\r\nSystemError: initialization of _internal failed without raising an exception\r\n\r\n<\/details>\r\n```\r\n> pip show pandas\r\nName: pandas\r\nVersion: 2.1.4\r\nSummary: Powerful data structures for data analysis, time series, and statistics\r\nHome-page: https:\/\/pandas.pydata.org\r\nAuthor:\r\nAuthor-email: The Pandas Development Team <pandas-dev@python.org>\r\nLicense: BSD 3-Clause License\r\n\r\n        Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\r\n        All rights reserved.\r\n\r\n        Copyright (c) 2011-2023, Open source contributors.\r\n\r\n        Redistribution and use in source and binary forms, with or without\r\n        modification, are permitted provided that the following conditions are met:\r\n\r\n        * Redistributions of source code must retain the above copyright notice, this\r\n          list of conditions and the following disclaimer.\r\n\r\n        * Redistributions in binary form must reproduce the above copyright notice,\r\n          this list of conditions and the following disclaimer in the documentation\r\n          and\/or other materials provided with the distribution.\r\n\r\n        * Neither the name of the copyright holder nor the names of its\r\n          contributors may be used to endorse or promote products derived from\r\n          this software without specific prior written permission.\r\n\r\n        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\r\n        AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\r\n        IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\r\n        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\r\n        FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\r\n        DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\r\n        SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\r\n        CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\r\n        OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n        OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\nLocation: \/home\/bion\/miniconda3\/envs\/py310\/lib\/python3.10\/site-packages\r\nRequires: numpy, python-dateutil, pytz, tzdata\r\n```","comments":["follow up: for easy wins in pandas performance, a review of all the logical conditions for warning in pandas might be warranted, if we're compiling stuff just to check if we ought to warn people, then we could remove these warnings where possible to speed everything up and simplify the codebase","Thanks for the report. There can be a performance hit on the actual operation when using capturing vs non-capturing groups however. I think this might be another good case for #55385."],"labels":["Bug","Needs Triage"]},{"title":"BUG: df.iloc[:, x] = df.iloc[:, x].apply(int) will failed just and only when `int` in apply()","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\npd.__version__\r\n# >>> '2.1.4'\r\ndf = pd.DataFrame(\r\n    [[1.0],[2.0]]\r\n    , columns=['A']\r\n)\r\ndf.iloc[:, 0] = df.iloc[:, 0].apply(int)\r\ndf\r\n# output below\r\n#     A\r\n# 0  1.0\r\n# 1  2.0\r\n\r\n# expected\r\n#     A\r\n# 0   1\r\n# 1   2\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\n<img width=\"361\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/94439026\/bb030ec2-f910-43cf-ac57-bf811f265133\">\r\n<img width=\"525\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/94439026\/cbd20fb6-e693-4e61-bd20-a720eb07eb07\">\r\n\r\nFYI: `df.iloc[:, 0] = df.iloc[:, 0].astype(int) ` failed too.\r\n\r\nI tried pandas below pandas2.0, such as pandas 1.4.1 it worked expectly. \r\n\r\nand during my tests, I found only the first time i put `int` or `lambda x: int(x)`  in `.apply()` not working. other function will work properly.\r\n\r\n\r\n### Expected Behavior\r\n\r\n```\r\n# expected\r\n#     A\r\n# 0   1\r\n# 1   2\r\n```\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.5.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 21.6.0\r\nVersion             : Darwin Kernel Version 21.6.0: Mon Aug 22 20:20:05 PDT 2022; root:xnu-8020.140.49~2\/RELEASE_ARM64_T8101\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : zh_CN.UTF-8\r\npandas              : 2.1.4\r\nnumpy               : 1.24.4\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.1.2\r\npip                 : 23.3.1\r\nCython              : 3.0.0a10\r\npytest              : 7.4.3\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.4.3\r\nnumba               : 0.57.0\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\n<\/details>\r\n","comments":["If you're using `.apply(int)` as just a simple example, that's perfectly fine, but for that exact operation I think one should prefer ` .astype(int)`. But both result in the same issue.\r\n\r\nOn main, the corresponding example does upcast\r\n\r\n```\r\ndf = pd.DataFrame([[1], [2]], columns=['A'])\r\ndf.iloc[:, 0] = df.iloc[:, 0].astype(float)\r\nprint(df)\r\n#      A\r\n# 0  1.0\r\n# 1  2.0\r\n```\r\n\r\ncc @MarcoGorelli ","thanks for the report - this looks even odder\r\n```python\r\nIn [1]: import pandas as pd\r\n   ...: pd.__version__\r\n   ...: df = pd.DataFrame(\r\n   ...:     [[1.0],[2.5]]\r\n   ...:     , columns=['A']\r\n   ...: )\r\n   ...: df.iloc[:, 0] = df.iloc[:, 0].apply(int)\r\n\r\nIn [2]: df\r\nOut[2]:\r\n     A\r\n0  1.0\r\n1  2.0\r\n```\r\nlooks like `int` is applied, but the return dtype is preserved?\r\n\r\nEDIT\r\n----\r\nAs commented below, I think this is expected - the right-hand-side is integer dtype, and `.iloc` preserves the left-hand-side's dtype","@MarcoGorelli  I think your example goes back to NumPy behavior\r\n\r\n```\r\nna = np.array([[1.5, 2.5, 3.5], [1.5, 2.5, 3.5]])\r\nna[:, 0] = np.array([1, 2])\r\n```\r\n\r\nNumPy casts here for us, so not warning seems fine","And when the warning being introduced in #56146 goes into effect, pandas will have consistent behavior with respect to upcasting\/downcasting.","I think this is correct?\r\n\r\nSo, the R.H.S. evaluates to:\r\n```python\r\nIn [3]: df.iloc[:, 0].apply(int)\r\nOut[3]:\r\n0    1\r\n1    2\r\nName: A, dtype: int64\r\n```\r\n\r\nIt's then set into the left-hand-side, preserving the left-hand-side's dtype, so we get\r\n```python\r\n\r\nIn [5]: df\r\nOut[5]:\r\n     A\r\n0  1.0\r\n1  2.0\r\n```\r\n\r\n@spsspro I think you might want to do it the setitem-way, which isn't in-place:\r\n```python\r\nIn [8]: df = pd.DataFrame(\r\n   ...:     [[1.0],[2.0]]\r\n   ...:     , columns=['A']\r\n   ...: )\r\n   ...: df['A'] = df['A'].apply(int)\r\n\r\nIn [9]: df\r\nOut[9]:\r\n   A\r\n0  1\r\n1  2\r\n```"],"labels":["Bug","Indexing","Dtype Conversions","Needs Triage","Closing Candidate","PDEP6-related"]},{"title":"BUG: NDFrame.__setattr__ calls object.__setattribute__ on the same name, which can cause unexpected behavior","body":"- [x] closes #56793\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [x] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [x] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n","comments":[],"labels":["Bug"]},{"title":"BUG: NDFrame.__setattr__ calls object.__setattribute__ on the same name, which can cause unexpected behavior (rewrote #56773)","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```\r\npython\r\nfrom pandas import DataFrame\r\nfrom functools import cached_property\r\nclass Frame(DataFrame):\r\n    @cached_property\r\n    def spam(self):\r\n        print('<time wasting task>')\r\n\r\nFrame().spam = 5\r\n```\r\n\r\n```\r\n<time wasting task>\r\n```\r\n\r\n\r\n\r\n### Issue Description\r\n\r\nIn the example, the user is trying to assign a value to a property, but `NDFrame.__setattr__` first calls `object.__getattribute__` on the same name. It wastes runtime by unnecessarily calling properties or raising `AttributeError`. Here is the original code:\r\n```\r\n    @final\r\n    def __setattr__(self, name: str, value) -> None:\r\n        \"\"\"\r\n        After regular attribute access, try setting the name\r\n        This allows simpler access to columns for interactive use.\r\n        \"\"\"\r\n        # first try regular attribute access via __getattribute__, so that\r\n        # e.g. ``obj.x`` and ``obj.x = 4`` will always reference\/modify\r\n        # the same attribute.\r\n\r\n        try:\r\n            object.__getattribute__(self, name)\r\n            return object.__setattr__(self, name, value)\r\n        except AttributeError:\r\n            pass\r\n\r\n        # if this fails, go on to more involved attribute setting\r\n        # (note that this matches __getattr__, above).\r\n        if name in self._internal_names_set:\r\n            object.__setattr__(self, name, value)\r\n        elif name in self._metadata:\r\n            object.__setattr__(self, name, value)\r\n        else:\r\n            try:\r\n                existing = getattr(self, name)\r\n                if isinstance(existing, Index):\r\n                    object.__setattr__(self, name, value)\r\n                elif name in self._info_axis:\r\n                    self[name] = value\r\n                else:\r\n                    object.__setattr__(self, name, value)\r\n            except (AttributeError, TypeError):\r\n                if isinstance(self, ABCDataFrame) and (is_list_like(value)):\r\n                    warnings.warn(\r\n                        \"Pandas doesn't allow columns to be \"\r\n                        \"created via a new attribute name - see \"\r\n                        \"https:\/\/pandas.pydata.org\/pandas-docs\/\"\r\n                        \"stable\/indexing.html#attribute-access\",\r\n                        stacklevel=find_stack_level(),\r\n                    )\r\n                object.__setattr__(self, name, value)\r\n\r\n\r\n```\r\n\r\n### Expected Behavior\r\n\r\nThe attribute should not be accessed, avoiding otherwise wasted time or unexpected behavior. \r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit           : 0f437949513225922d851e9581723d82120684a6\r\npython           : 3.11.6.final.0\r\npython-bits      : 64\r\nOS               : Linux\r\nOS-release       : 6.1.69-1-MANJARO\r\nVersion          : #1 SMP PREEMPT_DYNAMIC Thu Dec 21 12:29:38 UTC 2023\r\nmachine          : x86_64\r\nprocessor        : \r\nbyteorder        : little\r\nLC_ALL           : None\r\nLANG             : en_US.UTF-8\r\nLOCALE           : en_US.UTF-8\r\npandas           : 2.0.3\r\nnumpy            : 1.24.4\r\npytz             : 2023.3\r\ndateutil         : 2.8.2\r\nsetuptools       : 65.5.1\r\npip              : 23.2.1\r\nCython           : 0.29.36\r\npytest           : 7.4.0\r\nhypothesis       : None\r\nsphinx           : 7.1.2\r\nblosc            : None\r\nfeather          : None\r\nxlsxwriter       : None\r\nlxml.etree       : 4.9.3\r\nhtml5lib         : None\r\npymysql          : None\r\npsycopg2         : None\r\njinja2           : 3.1.2\r\nIPython          : 8.14.0\r\npandas_datareader: None\r\nbs4              : 4.12.2\r\nbottleneck       : None\r\nbrotli           : None\r\nfastparquet      : None\r\nfsspec           : None\r\ngcsfs            : None\r\nmatplotlib       : 3.7.2\r\nnumba            : 0.57.1\r\nnumexpr          : None\r\nodfpy            : None\r\nopenpyxl         : None\r\npandas_gbq       : None\r\npyarrow          : 12.0.1\r\npyreadstat       : None\r\npyxlsb           : None\r\ns3fs             : None\r\nscipy            : 1.11.1\r\nsnappy           : None\r\nsqlalchemy       : None\r\ntables           : None\r\ntabulate         : 0.9.0\r\nxarray           : None\r\nxlrd             : None\r\nzstandard        : None\r\ntzdata           : 2023.3\r\nqtpy             : None\r\npyqt5            : None\r\n\r\n<\/details>\r\n","comments":["@dhodcz2 - is this the same as #56773?","Yes, that was my first issue posted and kind of bad--I rewrote it as it's more of a \"bug\" than a \"feature request,\" thanks"],"labels":["Bug","Needs Triage"]},{"title":"BUG: Concat for two dataframes row-wise fails if one of columns is datetime and iterrows was used","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nimport datetime\r\n\r\ndata = {'Column1': [1, 2, 3, 33],\r\n        'Column2': [4, 5, 6, 66],\r\n        'Column3': [7, 8, 9, 99]}\r\ndf = pd.DataFrame(data)\r\n\r\n\r\n# once the below is uncommented, the pd.concat would fail...\r\n# df[\"Column2\"] = datetime.datetime(2018, 1, 1)\r\n\r\n# ... if this is uncommented, it would work though\r\n# df[\"Column2\"] = datetime.date(2018, 1, 1)\r\n\r\nrandom_rows = pd.DataFrame([row for _, row in df.sample(n=3).iterrows()])\r\ndf = pd.concat([df, random_rows])\r\nprint(df)\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nThe `pd.concat` of two dataframes fails when two conditions are met:\r\n\r\n- one of the columns is `datetime` (if it is `date` everything works though)\r\n- the second dataframe in `concat` was constructed using `.iterrows()`\r\n\r\nChanging `iterrows()` to `itertuples()` (in case something broke down in types) doesn't solve the problem\r\n\r\n### Expected Behavior\r\n\r\nIt works with the `date` object, so would expect it to be a bug.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.1.0\r\nVersion             : Darwin Kernel Version 23.1.0: Mon Oct  9 21:28:12 PDT 2023; root:xnu-10002.41.9~6\/RELEASE_ARM64_T8103\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : 1.3.5\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : 2.8.7\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Can you post the traceback?","I am not able to replicate on main.","> Can you post the traceback?\r\n\r\n\r\n```\r\ncd \/Users\/ilko\/Code\/clay\/backend ; \/usr\/bin\/env \/Users\/ilko\/Code\/clay\/backend\/.conda\/bin\/python \/Users\/ilko\/.vscode\r\n\/extensions\/ms-python.python-2023.22.1\/pythonFiles\/lib\/python\/debugpy\/adapter\/..\/..\/debugpy\/launcher 63255 -- main.py --init \r\nTraceback (most recent call last):\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/main.py\", line 17, in <module>\r\n    df = pd.concat([df, random_rows])\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/reshape\/concat.py\", line 393, in concat\r\n    return op.get_result()\r\n           ^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/reshape\/concat.py\", line 680, in get_result\r\n    new_data = concatenate_managers(\r\n               ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/concat.py\", line 189, in concatenate_managers\r\n    values = _concatenate_join_units(join_units, copy=copy)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/concat.py\", line 486, in _concatenate_join_units\r\n    concat_values = concat_compat(to_concat, axis=1)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/dtypes\/concat.py\", line 132, in concat_compat\r\n    return cls._concat_same_type(to_concat_eas)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/arrays\/datetimelike.py\", line 2251, in _concat_same_type\r\n    new_obj = super()._concat_same_type(to_concat, axis)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/ilko\/Code\/clay\/backend\/.conda\/lib\/python3.11\/site-packages\/pandas\/core\/arrays\/_mixins.py\", line 230, in _concat_same_type\r\n    return super()._concat_same_type(to_concat, axis=axis)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"arrays.pyx\", line 190, in pandas._libs.arrays.NDArrayBacked._concat_same_type\r\nValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3\r\n```","I ran into a similar issue today when concatenating two dataframes that had a mix of `datetime[ns]` and `datetime[us]` types in a datetime column.","That appears to be the issue here @cameronbronstein - while concat on 2.2.x and main now succeed, the DataFrame originally has `Colum2` as `datetime64[us]` and upon resample becomes `datetime64[ns]`. I think this is unexpected. Further investigations and PRs to fix are welcome!"],"labels":["Bug","Dtype Conversions","datetime.date"]},{"title":"BUG: to_csv with mode 'a' and zip compression if write by chunk creates multiple files insead of appending content","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\n# enough data to cause chunking into multiple files\r\nn_data = 1300\r\ndf = pd.DataFrame(\r\n    {'name': [\"Raphael\"] * n_data,\r\n     'mask': [\"red\"] * n_data,\r\n     'weapon': [\"sai\"] * n_data,\r\n     }\r\n)\r\ndf.to_csv('in.csv', index=False)\r\ncompression_opts = dict(method='zip')\r\nfor chunk in pd.read_csv(filepath_or_buffer='in.csv', chunksize=1000):\r\n    chunk.to_csv('out.csv.gz', mode= 'a', index=False,  compression=compression_opts)\n```\n\n\n### Issue Description\n\nIf read data by chunk from any source (tested with csv, sql) and then export them with zip compression and mode 'a' (append data) the data is not appended, but new files added to the zip file with the same name.\r\nThus if you read 10 chunks and export them to zip, you'll have 10 files in a zip file instead of 1. \r\nThe problem relates to zip files only. The 'gz' and without compression modes are ok\n\n### Expected Behavior\n\nsame behaviour as without compression - 1 file in a zip archive\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.8.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.17763\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.3\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["We had similar issues in the past ([seems for tar and zip](https:\/\/github.com\/pandas-dev\/pandas\/blob\/04b45b10b1e46780a903e03f885163bc06d4aa8a\/pandas\/io\/common.py#L949)) where pandas internally called `write` multiple times (which causes issues for zip files: https:\/\/bugs.python.org\/issue2824). In your case, `to_csv` is called multiple times by the user, so I think we cannot find a workaround for that. It would be good to show a warning when using `\"a\" in mode and compression in (\"zip\", \"tar\")`!\r\n","Its a pitty that zip has such limitations that it is not possible to modify file in archive. Thus it would be great if there will be notice that  mod \"a\" is not compatible with (\"zip\", \"tar\")\r\nThe workaround is simple actually - archive with gzip and repack to zip but this could be handled by user, not pandas.\r\n"],"labels":["Bug","IO Data","Warnings"]},{"title":"BUG: `pd.cut` with `IntervalIndex` produces unexpected results when `freq` of data and bins differ","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nbins = {\r\n    \"0D\": (0, 1),\r\n    \"1D-2W\": (1, 14),\r\n    \"2W-6M\": (14, 365 \/ 2),\r\n    \"6M+\": (365 \/ 2, None),\r\n}\r\n\r\n# convert bins to pd.Timedelta, inserting an arbitrary maximum value\r\nbins = [\r\n    (pd.Timedelta(days=left), pd.Timedelta(days=right or pd.Timedelta.max.days))\r\n    for left, right in bins.values()\r\n]\r\n\r\n# convert bins to pd.IntervalIndex\r\nidx = pd.IntervalIndex.from_tuples(bins, closed=\"left\")\r\n\r\n# inspect dtype - note ns\r\nidx.dtype\r\n\r\n# interval[timedelta64[ns], left]\r\n\r\n# construct pd.Series\r\nser = pd.Series([\r\n    pd.Timedelta(0, unit=\"days\"),\r\n    pd.Timedelta(10, unit=\"days\"),\r\n    pd.Timedelta(100, unit=\"days\")\r\n])\r\n\r\n# all results are put into the (0, 1) day bin\r\npd.cut(ser.astype(\"timedelta64[ms]\"), bins=idx)\r\n\r\n# 0    [0 days 00:00:00, 1 days 00:00:00)\r\n# 1    [0 days 00:00:00, 1 days 00:00:00)\r\n# 2    [0 days 00:00:00, 1 days 00:00:00)\r\n# dtype: category\r\n# Categories (4, interval[timedelta64[ns], left]): [[0 days 00:00:00, 1 days 00:00:00) < [1 days 00:00:00, 14 days 00:00:00) < [14 days 00:00:00, 182 days 12:00:00) < [182 days 12:00:00, 106751 days 00:00:00)]\r\n\r\n# data are correctly binned\r\npd.cut(ser.astype(\"timedelta64[ns]\"), bins=idx)\r\n\r\n# 0       [0 days 00:00:00, 1 days 00:00:00)\r\n# 1      [1 days 00:00:00, 14 days 00:00:00)\r\n# 2    [14 days 00:00:00, 182 days 12:00:00)\r\n# dtype: category\r\n# Categories (4, interval[timedelta64[ns], left]): [[0 days 00:00:00, 1 days 00:00:00) < [1 days 00:00:00, 14 days 00:00:00) < [14 days 00:00:00, 182 days 12:00:00) < [182 days 12:00:00, 106751 days 00:00:00)]\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nHi all,\r\n\r\nI've noticed that when the resolution of the `pd.IntervalIndex` used to construct the bins, and the `pd.Series` of values to be binned differ, `pd.cut` produces unexpected outputs.\r\n\r\nI can understand comparing different resolutions can maybe lead us into undefined behaviour territory, but I would at the very least expect a warning and maybe even an error. To give back the incorrect result with no notification to the user is prime for producing bugs.\r\n\r\nThanks\r\n\r\n### Expected Behavior\r\n\r\nEither `pd.cut` to produce the \"right\" answer (i.e. as per `pd.cut(ser.astype(\"timedelta64[ns]\"), bins=idx)`), or a warning or error to be raised\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n>>> pd.show_versions()\r\nC:\\Miniconda3-py37_4.10.3\\envs\\py311pandas\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 165 Stepping 3, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_United Kingdom.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : 1.3.5\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : 2.8.7\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Interval","Needs Triage"]},{"title":"PERF: Potential room for optimizaing the performance of .loc with big non-unique masked index dataframe","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this issue exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this issue exists on the main branch of pandas.\n\n\n### Reproducible Example\n\nHello, I am writing to suggest a potential improvement in the time efficiency of Pandas. This pertains to a performance issue similar to the one highlighted in xref. #54550.  However, its PR, #54746, overlooks the aspect of adjusting the threshold for `get_indexer_non_unique` in the `class MaskedIndexEngine`. My suggestion is to revise this limit, aligning with the strategy adopted in #54746, specifically setting it to len(targets) < (n \/ (2 * n.bit_length())).  I believe this adjustment could positively impact the performance.\r\n\r\nI am willing to create a pull request for this if you believe it would be beneficial.\r\n\r\n```\r\nimport random\r\nimport time\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nif __name__ == \"__main__\":\r\n    # Create a large pandas dataframe with non-unique indexes and some NaN values\r\n    table_size = 10_000_000\r\n    num_index = 1_000_000\r\n    data = [1] * table_size\r\n    # Introduce NaNs into the data\r\n    for _ in range(table_size \/\/ 10):  # Introduce NaNs in 10% of the data\r\n        data[random.randint(0, table_size - 1)] = np.nan\r\n    df = pd.DataFrame(data)\r\n    index = random.choices(range(num_index), k=table_size)\r\n    df.index = index\r\n    df = df.sort_index()\r\n\r\n    # Pre-query the index to force optimizations.\r\n    df.loc[[5, 6, 7, 456, 65743]]\r\n    df.loc[[1000]]\r\n\r\n    # Testing 'df.loc' with all at once using a list of indexes, on masked data.\r\n    for i in range(10):\r\n        indexes = random.sample(list(df.index), k=i+1)\r\n        start = time.monotonic()\r\n        df.loc[indexes]\r\n        measure = time.monotonic() - start\r\n        print(f\"With all at once (masked data): num_indexes={i+1} => {measure:.5f}s\")\r\n\r\n    print(\"---\")\r\n\r\n    # Testing 'df.loc' one at a time using a list of indexes, on masked data.\r\n    for i in range(10):\r\n        indexes = random.sample(list(df.index), k=i+1)\r\n        start = time.monotonic()\r\n        pd.concat([df.loc[[idx]] for idx in indexes])\r\n        measure = time.monotonic() - start\r\n        print(f\"With one at a time (masked data): num_indexes={i+1} => {measure:.5f}s\")\r\n\r\n```\r\nprinted result:\r\n```\r\n\r\nWith all at once (masked data): num_indexes=1 => 0.00045s\r\nWith all at once (masked data): num_indexes=2 => 0.00048s\r\nWith all at once (masked data): num_indexes=3 => 0.00052s\r\nWith all at once (masked data): num_indexes=4 => 0.00050s\r\nWith all at once (masked data): num_indexes=5 => 0.64931s\r\nWith all at once (masked data): num_indexes=6 => 0.65066s\r\nWith all at once (masked data): num_indexes=7 => 0.65181s\r\nWith all at once (masked data): num_indexes=8 => 0.80003s\r\nWith all at once (masked data): num_indexes=9 => 0.65251s\r\nWith all at once (masked data): num_indexes=10 => 0.66629s\r\n---\r\nWith one at a time (masked data): num_indexes=1 => 0.00081s\r\nWith one at a time (masked data): num_indexes=2 => 0.00134s\r\nWith one at a time (masked data): num_indexes=3 => 0.00114s\r\nWith one at a time (masked data): num_indexes=4 => 0.00173s\r\nWith one at a time (masked data): num_indexes=5 => 0.00132s\r\nWith one at a time (masked data): num_indexes=6 => 0.00191s\r\nWith one at a time (masked data): num_indexes=7 => 0.20084s\r\nWith one at a time (masked data): num_indexes=8 => 0.00180s\r\nWith one at a time (masked data): num_indexes=9 => 0.00201s\r\nWith one at a time (masked data): num_indexes=10 => 0.00169s\r\n```\r\n\n\n### Installed Versions\n\n<details>\r\n\r\ncommit : https:\/\/github.com\/pandas-dev\/pandas\/commit\/a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython : 3.9.18.final.0\r\npython-bits : 64\r\nOS : Linux\r\nOS-release : 5.15.0-88-generic\r\nVersion : https:\/\/github.com\/pandas-dev\/pandas\/issues\/98~20.04.1-Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023\r\nmachine : x86_64\r\nprocessor : x86_64\r\nbyteorder : little\r\nLC_ALL : None\r\nLANG : en_US.UTF-8\r\nLOCALE : en_US.UTF-8\r\n\r\npandas : 2.1.4\r\nnumpy : 1.26.3\r\npytz : 2023.3.post1\r\ndateutil : 2.8.2\r\nsetuptools : 68.2.2\r\npip : 23.3.1\r\nCython : None\r\npytest : None\r\nhypothesis : None\r\nsphinx : None\r\nblosc : None\r\nfeather : None\r\nxlsxwriter : None\r\nlxml.etree : None\r\nhtml5lib : None\r\npymysql : None\r\npsycopg2 : None\r\njinja2 : None\r\nIPython : 8.18.1\r\npandas_datareader : None\r\nbs4 : None\r\nbottleneck : None\r\ndataframe-api-compat: None\r\nfastparquet : None\r\nfsspec : None\r\ngcsfs : None\r\nmatplotlib : None\r\nnumba : None\r\nnumexpr : None\r\nodfpy : None\r\nopenpyxl : None\r\npandas_gbq : None\r\npyarrow : None\r\npyreadstat : None\r\npyxlsb : None\r\ns3fs : None\r\nscipy : None\r\nsqlalchemy : None\r\ntables : None\r\ntabulate : None\r\nxarray : None\r\nxlrd : None\r\nzstandard : None\r\ntzdata : 2023.4\r\nqtpy : None\r\npyqt5 : None\r\n\r\n<\/details>\r\n\n\n### Prior Performance\n\n_No response_","comments":[],"labels":["Performance","Needs Triage"]},{"title":"ENH: background_gradient accepts a normalization parameter","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nits really hard to normalize df style background gradients like other colormap accepting attributes\n\n### Feature Description\n\nParameters:\r\n\r\nnorm\r\n\r\n\r\ncmapstr or colormap\r\nMatplotlib colormap.\r\n\r\nlowfloat\r\nCompress the color range at the low end. This is a multiple of the data range to extend below the minimum; good values usually in [0, 1], defaults to 0.\r\n\r\nhighfloat\r\nCompress the color range at the high end. This is a multiple of the data range to extend above the maximum; good values usually in [0, 1], defaults to 0.\r\n\r\naxis{0, 1, \u201cindex\u201d, \u201ccolumns\u201d, None}, default 0\r\nApply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None.\r\n\r\nsubsetlabel, array-like, IndexSlice, optional\r\nA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function.\r\n\r\ntext_color_thresholdfloat or int\r\nLuminance threshold for determining text color in [0, 1]. Facilitates text\r\n\r\nvisibility across varying background colors. All text is dark if 0, and\r\n\r\nlight if 1, defaults to 0.408.\r\n\r\nvminfloat, optional\r\nMinimum data value that corresponds to colormap minimum value. If not specified the minimum value of the data (or gmap) will be used.\r\n\r\nvmaxfloat, optional\r\nMaximum data value that corresponds to colormap maximum value. If not specified the maximum value of the data (or gmap) will be used.\r\n\r\ngmaparray-like, optional\r\nGradient map for determining the background colors. If not supplied will use the underlying data from rows, columns or frame. If given as an ndarray or list-like must be an identical shape to the underlying data considering axis and subset. If given as DataFrame or Series must have same index and column labels considering axis and subset. If supplied, vmin and vmax should be given relative to this gradient map.\r\n\n\n### Alternative Solutions\n\nadd a parameter that takes a norm object\n\n### Additional Context\n\n_No response_","comments":[],"labels":["Enhancement","Needs Triage"]},{"title":"ENH: add NDArrayBackedExtensionArray to public API","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nReviving https:\/\/github.com\/pandas-dev\/pandas\/pull\/45544 as I'd like to use it for an UncertaintyArray.","comments":["Can we make this \"public\" with some kind of documentation that we may adjust it in non-user-facing ways more aggressively than we do with regular APIs?","> Can we make this \"public\" with some kind of documentation that we may adjust it in non-user-facing ways more aggressively than we do with regular APIs?\r\n\r\nadded a note to the class docstring","I'm unsure what to do about this error as `to_numpy` is an inherited method:\r\n```\r\nError: \/home\/runner\/work\/pandas\/pandas\/pandas\/core\/arrays\/base.py:542:EX01:pandas.api.extensions.NDArrayBackedExtensionArray.to_numpy:No examples section found\r\n```\r\n\r\nAlso not sure what to do about \r\n```\r\nError: \/home\/runner\/work\/pandas\/pandas\/pandas\/core\/arrays\/datetimes.py:179:EX03:pandas.arrays.DatetimeArray:flake8 error: line 2, col 4: E121 continuation line under-indented for hanging indent\r\n```\r\nas I haven't changed that file.","Given that the Pandas team was finalizing the 2.2 release, you may have hit the docs system at a bad time.  I've had that happen to me.  Might make sense to try merging with latest and resubmitting.","I tried jumping in to see if I could fix the missing toctree problem, but two things defeated me:\r\n\r\n1. In the normal pandas build process (running `doc\/make.py --warnings-are-errors`) a child process segfaulted on Mac OS 14.1.2 in a Python 3.11.4 enviroment\r\n2. When I tried running `doc\/make.py --warnings-are-errors --single reference\/api\/pandas.api.extensions.NDArrayBackedExtensionArray.argmax.rst` (a command that failed with the toctree warning), that single file built successfully.\r\n\r\nI'm trying to see if I can isolate a reproducer for (1) above.","cc @jreback @jorisvandenbossche from original PR","This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["API Design","ExtensionArray","Stale"]},{"title":"ENH: Exp function for Series and DataFrame","body":"### Feature Type\r\n\r\n- [X] Adding new functionality to pandas\r\n\r\n- [ ] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nCould we add a `.exp` method (in the spirit of `.add`, `.sub`, `.pow` methods)?  By default would use `np.exp(1)` as the base.  But could take `base` as an additional argument.\r\n\r\n### Feature Description\r\n\r\nAdd \"exponentiation\" method for Series and DataFrames\r\n\r\n### Alternative Solutions\r\n\r\nequivalent of `ser.apply(np.exp)`\r\n\r\n### Additional Context\r\n\r\n_No response_","comments":["i'm happy to take this unless there is a good reason against this feature","take","Thanks for the request but I would be -1 on this feature so far as `np.exp` can already accept DataFrame and Series objects and pandas is trying to minimize redundant ways to accomplish the same task:\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: import pandas as pd\r\nnp.exp\r\nIn [3]: np.exp(pd.DataFrame(np.eye(3)))\r\nOut[3]: \r\n          0         1         2\r\n0  2.718282  1.000000  1.000000\r\n1  1.000000  2.718282  1.000000\r\n2  1.000000  1.000000  2.718282\r\n\r\nIn [4]: np.exp(pd.Series([1, 2]))\r\nOut[4]: \r\n0    2.718282\r\n1    7.389056\r\ndtype: float64\r\n```","@mroeschke , I appreciate your take on this, but I lean the other way.  \r\n\r\nIf I have a series and I want to exponentiate it, I would save a few keystrokes with `ser.exp()` compared with `np.exp(ser)`.\r\n\r\nBut more important than the \"keystroke savings\" is the \"flow\" you get from chaining similar steps with similar syntax.  I strongly prefer `ser.sub(1).mul(0.5).exp()` to `np.exp(ser.sub(1).mul(0.5))`.  The three steps here feel similar to me intuitively, and so I feel like the \"correct\" solution should let me chain them with similar syntax.\r\n\r\nRegarding your comment about trying to avoid multiple ways to do the same thing, we have (and I would want to keep):\r\n```\r\nser + 1\r\nser.add(1)\r\n\r\nser ** 2\r\nser.pow(2)\r\n```\r\netc.  There are lots of places where chaining a number of steps this way is readable and intuitive.  I'd love to be able to hit `exp` in the same way. ","Thanks for the feedback. While I see the flow argument, the project has been a bit weary on expanding on the large API of pandas, especially for functionality that is stylistic with known alternatives (pandas defines `__array_ufunc__` to allow any numpy mathematical function (ufunc))","Sigh.  Feels like a weird \"half-done\" current state at the moment: `add`, `sub`, `mul`, `div`, `pow`...but poor old `exp` left out in the cold.","Last pitch: the methods I mention above all do optional alignment on a level of the index.  So you can do `ser.add(other_ser, level=my_level)`.  Noting that you can want to do \"raise a base to the power of my series\" for a base other than `np.e`, you might want the use case of `ser.exp(base=other_ser, level=my_level)`.  Don't think the other approaches do that particularly well."],"labels":["Enhancement","Closing Candidate"]},{"title":"BUG: Docs won't build (Unexpected warning in `user_guide\/timeseries.rst`)","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\ngit clone https:\/\/github.com\/pandas-dev\/pandas.git\r\ncd pandas\r\npython -m pip install -r requirements-dev.txt\r\npython -m pip install -ve . --no-build-isolation --config-settings editable-verbose=true\r\ncd doc\r\n\r\npython make.py html\r\npython make.py clean\r\npython make.py --single user_guide\/timeseries.rst\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nRunning `python make.py html` to build the whole documentation, it fails when building `user_guide\/timeseries.rst`:\r\n```\r\nSphinx parallel build error:\r\nRuntimeError: Unexpected warning in `\/home1\/hhz\/pandas_main\/pandas\/doc\/source\/user_guide\/timeseries.rst` line 218\r\n```\r\nThen run `python make.py --single user_guide\/timeseries.rst` to see details:\r\n```\r\nWARNING: ources... [100%] user_guide\/timeseries\r\n>>>-------------------------------------------------------------------------\r\nWarning in \/home1\/hhz\/pandas_main\/pandas\/doc\/source\/user_guide\/timeseries.rst at block ending on line 218\r\nSpecify :okwarning: as an option in the ipython:: block to suppress this message\r\n----------------------------------------------------------------------------\r\n<ipython-input-44-ffc04f3bd908>:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\r\n  pd.to_datetime(pd.Series([\"Jul 31, 2009\", \"Jan 10, 2010\", None]))\r\n<<<-------------------------------------------------------------------------\r\n\r\nException occurred:\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/IPython\/sphinxext\/ipython_directive.py\", line 602, in process_input\r\n    raise RuntimeError(\r\nRuntimeError: Unexpected warning in `\/home1\/hhz\/pandas_main\/pandas\/doc\/source\/user_guide\/timeseries.rst` line 218\r\nThe full traceback has been saved in \/tmp\/sphinx-err-mnsayjoe.log, if you want to report the issue to the developers.\r\nPlease also report this if it was a user error, so that a better error message can be provided next time.\r\nA bug report can be filed in the tracker at <https:\/\/github.com\/sphinx-doc\/sphinx\/issues>. Thanks!\r\n```\r\nThe UserWarning for `pd.to_datetime(pd.Series([\"Jul 31, 2009\", \"Jan 10, 2010\", None]))` seems to be strange because no warning occurs when I directly run this code in ipython. Full traceback in `\/tmp\/sphinx-err-mnsayjoe.log`:\r\n```\r\n# Platform:         linux; (Linux-6.2.0-39-generic-x86_64-with-glibc2.35)\r\n# Sphinx version:   7.2.6\r\n# Python version:   3.11.0 (CPython)\r\n# Docutils version: 0.20.1\r\n# Jinja2 version:   3.1.2\r\n# Pygments version: 2.17.2\r\n\r\n# Last messages:\r\n#   [new config]\r\n#   2 added, 0 changed, 0 removed\r\n\r\n#   reading sources... [ 50%]\r\n#   index\r\n#   \r\n\r\n#   reading sources... [100%]\r\n#   user_guide\/timeseries\r\n#   \r\n\r\n# Loaded extensions:\r\n#   sphinx.ext.mathjax (7.2.6)\r\n#   alabaster (0.7.13)\r\n#   sphinxcontrib.applehelp (1.0.7)\r\n#   sphinxcontrib.devhelp (1.0.5)\r\n#   sphinxcontrib.htmlhelp (2.0.4)\r\n#   sphinxcontrib.serializinghtml (1.1.9)\r\n#   sphinxcontrib.qthelp (1.0.6)\r\n#   contributors (0.1)\r\n#   IPython.sphinxext.ipython_directive (unknown version)\r\n#   IPython.sphinxext.ipython_console_highlighting (unknown version)\r\n#   matplotlib.sphinxext.plot_directive (3.8.2)\r\n#   sphinx.ext.autodoc.preserve_defaults (7.2.6)\r\n#   sphinx.ext.autodoc.type_comment (7.2.6)\r\n#   sphinx.ext.autodoc.typehints (7.2.6)\r\n#   sphinx.ext.autodoc (7.2.6)\r\n#   sphinx.ext.autosummary (7.2.6)\r\n#   numpydoc (1.6.0)\r\n#   sphinx_copybutton (0.5.2)\r\n#   sphinx_design (0.5.0)\r\n#   sphinx.ext.coverage (7.2.6)\r\n#   sphinx.ext.doctest (7.2.6)\r\n#   sphinx.ext.extlinks (7.2.6)\r\n#   sphinx.ext.ifconfig (7.2.6)\r\n#   sphinx.ext.intersphinx (7.2.6)\r\n#   sphinx.ext.linkcode (7.2.6)\r\n#   sphinx.ext.todo (7.2.6)\r\n#   nbsphinx (0.9.3)\r\n#   pydata_sphinx_theme (unknown version)\r\n\r\n# Traceback:\r\nTraceback (most recent call last):\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/cmd\/build.py\", line 298, in build_main\r\n    app.build(args.force_all, args.filenames)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/application.py\", line 355, in build\r\n    self.builder.build_update()\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 293, in build_update\r\n    self.build(to_build,\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 313, in build\r\n    updated_docnames = set(self.read())\r\n                           ^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 420, in read\r\n    self._read_serial(docnames)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 441, in _read_serial\r\n    self.read_doc(docname)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/builders\/__init__.py\", line 498, in read_doc\r\n    publisher.publish()\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/core.py\", line 234, in publish\r\n    self.document = self.reader.read(self.source, self.parser,\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/io.py\", line 105, in read\r\n    self.parse()\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/readers\/__init__.py\", line 76, in parse\r\n    self.parser.parse(self.input, document)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/sphinx\/parsers.py\", line 81, in parse\r\n    self.statemachine.run(inputlines, document, inliner=self.inliner)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 169, in run\r\n    results = StateMachineWS.run(self, input_lines, input_offset,\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 233, in run\r\n    context, next_state, result = self.check_line(\r\n                                  ^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 445, in check_line\r\n    return method(match, context, next_state)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 3024, in text\r\n    self.section(title.lstrip(), source, style, lineno + 1, messages)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 325, in section\r\n    self.new_subsection(title, lineno, messages)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 391, in new_subsection\r\n    newabsoffset = self.nested_parse(\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 279, in nested_parse\r\n    state_machine.run(block, input_offset, memo=self.memo,\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 195, in run\r\n    results = StateMachineWS.run(self, input_lines, input_offset)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 233, in run\r\n    context, next_state, result = self.check_line(\r\n                                  ^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 445, in check_line\r\n    return method(match, context, next_state)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 2785, in underline\r\n    self.section(title, source, style, lineno - 1, messages)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 325, in section\r\n    self.new_subsection(title, lineno, messages)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 391, in new_subsection\r\n    newabsoffset = self.nested_parse(\r\n                   ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 279, in nested_parse\r\n    state_machine.run(block, input_offset, memo=self.memo,\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 195, in run\r\n    results = StateMachineWS.run(self, input_lines, input_offset)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 233, in run\r\n    context, next_state, result = self.check_line(\r\n                                  ^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/statemachine.py\", line 445, in check_line\r\n    return method(match, context, next_state)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 2355, in explicit_markup\r\n    nodelist, blank_finish = self.explicit_construct(match)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 2367, in explicit_construct\r\n    return method(self, expmatch)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 2104, in directive\r\n    return self.run_directive(\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/docutils\/parsers\/rst\/states.py\", line 2154, in run_directive\r\n    result = directive_instance.run()\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/IPython\/sphinxext\/ipython_directive.py\", line 1033, in run\r\n    rows, figure = self.shell.process_block(block)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/IPython\/sphinxext\/ipython_directive.py\", line 732, in process_block\r\n    self.process_input(data, input_prompt, lineno)\r\n  File \"\/home1\/hhz\/miniconda3\/envs\/pandas_main\/lib\/python3.11\/site-packages\/IPython\/sphinxext\/ipython_directive.py\", line 602, in process_input\r\n    raise RuntimeError(\r\nRuntimeError: Unexpected warning in `\/home1\/hhz\/pandas_main\/pandas\/doc\/source\/user_guide\/timeseries.rst` line 218\r\n```\r\n\r\n### Expected Behavior\r\n\r\nHTML doc successfully built.\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : b2bca5e96545c8b9fb50dbc45ffd71eb71bd2306\r\npython                : 3.11.0.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.2.0-39-generic\r\nVersion               : #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.3.0.dev0+46.gb2bca5e965\r\nnumpy                 : 1.26.3\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : 3.0.5\r\npytest                : 7.4.4\r\nhypothesis            : 6.92.2\r\nsphinx                : 7.2.6\r\nblosc                 : 1.11.1\r\nfeather               : None\r\nxlsxwriter            : 3.1.9\r\nlxml.etree            : 5.0.0\r\nhtml5lib              : 1.1\r\npymysql               : 1.4.6\r\npsycopg2              : 2.9.9\r\njinja2                : 3.1.2\r\nIPython               : 8.19.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : 1.3.7\r\ndataframe-api-compat  : None\r\nfastparquet           : 2023.10.1\r\nfsspec                : 2023.12.2\r\ngcsfs                 : 2023.12.2post1\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : 2.8.8\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.2\r\npyreadstat            : 1.2.6\r\npython-calamine       : None\r\npyxlsb                : 1.0.10\r\ns3fs                  : 2023.12.2\r\nscipy                 : 1.11.4\r\nsqlalchemy            : 2.0.25\r\ntables                : 3.9.2\r\ntabulate              : 0.9.0\r\nxarray                : 2023.12.0\r\nxlrd                  : 2.0.1\r\nzstandard             : 0.22.0\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["> The UserWarning for `pd.to_datetime(pd.Series([\"Jul 31, 2009\", \"Jan 10, 2010\", None]))` seems to be strange because no warning occurs when I directly run this code in ipython.\r\n\r\nThis suggests to me you are using two different pythons when you build docs vs when you run in ipython. When you build and install pandas, what python is used?","@rhshadrach \r\nI directly run the commands `python make.py` and `ipython` in the same conda environment. I have checked that they use the same python.\r\n```\r\n$ which ipython\r\n\/home1\/hhz\/miniconda3\/envs\/pandas_main\/bin\/ipython\r\n$ which python\r\n\/home1\/hhz\/miniconda3\/envs\/pandas_main\/bin\/python\r\n$ cat \/home1\/hhz\/miniconda3\/envs\/pandas_main\/bin\/ipython\r\n#!\/home1\/hhz\/miniconda3\/envs\/pandas_main\/bin\/python\r\n# -*- coding: utf-8 -*-\r\nimport re\r\nimport sys\r\nfrom IPython import start_ipython\r\nif __name__ == '__main__':\r\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\r\n    sys.exit(start_ipython())\r\n```","Very odd. I would suggest to try recreating your environment, but from the commands above I believe you've already done that."],"labels":["Docs"]},{"title":"TST: Ensure base ExtensionArray tests do not use any fixtures in `conftest.py`","body":"geopandas currently needs to copy over some fixtures used in our base EA tests which is not ideal for portability.\r\n\r\nhttps:\/\/github.com\/geopandas\/geopandas\/blob\/08c23312edd8b0df25621f0fc77417840efe1fb5\/geopandas\/tests\/test_extension_array.py#L246-L247\r\n\r\nIdeally we should have a check that these tests do no use any external fixtures defined in `pandas\/conftest.py` or ensure `pandas\/tests\/extension\/conftest.py` contains the needed fixtures and instruct authors to copy over this file as well. ","comments":["As a way to have some test in our own code base that might remind us about this: we could run the tests for one of our own extension arrays outside of the main test directory. \r\n\r\nFor example, we have some tests in `scripts\/tests` that are run, and those won't pick up the `pandas\/conftest.py`, I think. It doesn't exactly fit in there (as it is not testing a utility, but being an actual test in itself), but putting it there makes that we can easily use the workflow that already run those tests anyway (or without having to add a specific extra step in the CI setup. Not that this would be that complicated, though)\r\n\r\nConcretely, the tests in `pandas\/tests\/extension\/test_datetime.py` seem fairly complete and not too complex (not much overrides or skips). We could duplicate the content of that file with some minor edits in a file like `scripts\/tests\/test_extension_array_tests_fixtures.py`. That would ensure whenever we use a new fixture in the extension tests, it reminds us about this setup, and ensure we use fixtures from `pandas\/tests\/extension\/conftest.py`.\r\n","I've noticed at least the following fixtures are problematic in this regards:\r\n\r\n  - all_boolean_reductions\r\n  - all_numeric_reductions\r\n  - comparison_op\r\n  - all_arithmetic_operators\r\n  - sort_by_key\r\n  - groupby_apply_op\r\n  - all_numeric_accumulations"],"labels":["Testing","ExtensionArray"]},{"title":"ENH: Merging two tables where one of the tables is a cyclical subset","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nI would like to make a suggestion for a feature that might already exist, but I'm not sure. It might be really stupid, I guess. During my time using Pandas, I find myself merging two tables that don't actually require the sophisticated merge operations, as there's a simpler cyclical relationship. Let me give a simple example:\n\n### Feature Description\n\nTable 1 (left) is a date range table, with day of the weeks marked in another column:\r\n```\r\nimport pandas as pd, numpy as np\r\n\r\nleft_table = pd.DataFrame({'datetime' :pd.date_range('01-01-2000', freq = 'D', periods = 100)})\r\nleft_table['day_of_week'] = left_table.datetime.dt.day_of_week\r\n```\r\nSo far we have for the left table:\r\n```\r\n     datetime  day_of_week\r\n0  2000-01-01            5\r\n1  2000-01-02            6\r\n2  2000-01-03            0\r\n..        ...          ...\r\n97 2000-04-07            4\r\n98 2000-04-08            5\r\n99 2000-04-09            6\r\n```\r\n\r\nsay we have a right table, which has values that correspond to each day of the week:\r\n```\r\nright_table = pd.DataFrame({'day_of_week':np.arange(7), 'value':np.random.randn(7)})\r\n```\r\nSo we have that our 'right' table looks like this:\r\n```\r\n   day_of_week     value\r\n0            0 -1.491759\r\n1            1 -0.730272\r\n2            2 -0.986385\r\n3            3  1.324833\r\n4            4 -0.974399\r\n5            5  1.016339\r\n6            6  1.663364\r\n```\r\n\r\nIn this instance, one may desire to merge these two tables together:\r\n```\r\njoined_table = left_table.merge(right_table, on = 'day_of_week', how = 'inner').sort_values(by = 'datetime').reset_index(drop = True)\r\n```\r\nAnd we get the expected result:\r\n```\r\n     datetime  day_of_week     value\r\n0  2000-01-01            5 -0.403335\r\n1  2000-01-02            6  0.724089\r\n2  2000-01-03            0  0.522325\r\n..        ...          ...       ...\r\n97 2000-04-07            4  0.611570\r\n98 2000-04-08            5 -0.403335\r\n99 2000-04-09            6  0.724089\r\n```\r\n(at this point a user may want to drop the day_of_week column, as it acted only as a bridge for the join).\r\nI notice, however, this this is a special case where a merge operation is overkill. This is because the right table could be duplicated many times over, and then pd.concat() could be used along the first axis to perform something akin to a merge. This is because the smaller right table is an ordered subset of the left, and the left follows a predictable pattern.\r\n\r\nMy implementation (probably not great, but illustrates the concept) therefore looks like this:\r\n```\r\ndef cyclical_join(left, right, left_on, right_on, right_values_key):\r\n\r\n    #we want to find the index of the first element in the (bigger) left table that matches the first element in our right table\r\n    start_ind = 0\r\n    for j,i in enumerate(left[left_on]):\r\n        if i == right[right_on][0]:\r\n            start_ind = j\r\n            break\r\n        else:\r\n            pass\r\n\r\n    new_vals = list(right[right_values_key])\r\n    temp_left = left.copy(deep = True)\r\n\r\n    #we then need to sort out new column using its cyclical properties\r\n    temp_left['_join'] = np.array(([] if start_ind == 0 else new_vals[-start_ind:]) + new_vals * ((len(left)-start_ind)\/\/len(right)) + new_vals[:(len(left) - start_ind) % len(right)])\r\n    return temp_left\r\n```\r\nto check the output is correct:\r\n```\r\njoined_cyclical = cyclical_join(left_table, right_table, left_on = 'day_of_week', right_on = 'day_of_week', right_values_key = 'value').rename(columns = {'_join':'value'})\r\n(joined_table == joined_cyclical).all()\r\n```\r\nWe see the result of our comparison:\r\n```\r\ndatetime       True\r\nday_of_week    True\r\nvalue          True\r\n```\r\nI think this is a useful addition, and it's faster than a merge (although of course is a specific use-case) by some margin. For our use case where the left table has 100 rows, we see:\r\n\r\nFirst case, 100 rows, I just do the merge, I don't care about the shuffled indices or resorting on the bridge column:\r\n```\r\n%timeit left_table.merge(right_table, on = 'day_of_week', how = 'inner')\r\n2.66 ms \u00b1 70.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nSecond case, I do the resorting etc.\r\n```\r\n%timeit left_table.merge(right_table, on = 'day_of_week', how = 'inner').sort_values(by = 'datetime').reset_index(drop = True)\r\n4.08 ms \u00b1 258 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nThird case: I use the cyclical join. This doesn't care about the order of the bridge, and maintains the order of the left table:\r\n```\r\n%timeit cyclical_join(left_table, right_table, left_on = 'day_of_week', right_on = 'day_of_week', right_values_key = 'value')\r\n587 \u00b5s \u00b1 13.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```\r\n\r\nIt's a about 4-5x faster in this case. We can see on larger tables, where I've used 100,000 rows:\r\n```\r\nleft_table = pd.DataFrame({'datetime' :pd.date_range('01-01-1900', freq = 'D', periods = 100000)})\r\nleft_table['day_of_week'] = left_table.datetime.dt.day_of_week\r\n\r\n#our right table is still valid here\r\n```\r\nFourth case, we merge these tables:\r\n```\r\n%timeit left_table.merge(right_table, on = 'day_of_week', how = 'inner')\r\n11.8 ms \u00b1 356 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nFifth case, we cyclically merge them:\r\n```\r\n%timeit cyclical_join(left_table, right_table, left_on = 'day_of_week', right_on = 'day_of_week', right_values_key = 'value')\r\n10.2 ms \u00b1 306 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nI suspect the drop-off in performance for the larger arrays has something to do with my implementation of `cyclical_merge` being sub-optimal. However, it's nice that this method maintains the order of your other columns in the left table.\r\n\r\nI can foresee this being quite a useful operation for many users.\n\n### Alternative Solutions\n\nI could not find an existing solution. but perhaps this is because I simply don't know the name of the operation if it already commonly exists!\n\n### Additional Context\n\n_No response_","comments":[],"labels":["Enhancement","Needs Triage"]},{"title":"BUG: Inconsistent datetime dtype precision with assign() using list vs direct value","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nimport datetime\r\n\r\ndt = datetime.datetime(2024, 1, 4, 12, 0, 0)\r\n\r\ndf = pd.DataFrame(index=[0])\r\n\r\ndf_list = df.assign(datetime=[dt])\r\ndf_value = df.assign(datetime=dt)\r\n\r\npd.testing.assert_frame_equal(df_list, df_value)\n```\n\n\n### Issue Description\n\nThe dtype of the ` datetime`  column should be the same in both cases, but it is different:\r\n\r\n```\r\nAssertionError: Attributes of DataFrame.iloc[:, 0] (column name=\"datetime\") are different\r\n\r\nAttribute \"dtype\" are different\r\n[left]:  datetime64[ns]\r\n[right]: datetime64[us]\r\n```\r\n\r\n\n\n### Expected Behavior\n\nNo assertion error.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit                : 2522b0ab1b803396a3d8dbc0c936d9b64e239bcf\r\npython                : 3.11.5.final.0\r\npython-bits           : 64\r\nOS                    : Linux\r\nOS-release            : 6.5.0-14-generic\r\nVersion               : #14-Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 14 14:59:49 UTC 2023\r\nmachine               : x86_64\r\nprocessor             : x86_64\r\nbyteorder             : little\r\nLC_ALL                : None\r\nLANG                  : en_US.UTF-8\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.3.0.dev0+43.g2522b0ab1b\r\nnumpy                 : 2.0.0.dev0+git20240102.d906b52\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : None\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : None\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : None\r\nIPython               : 8.15.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : None\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : None\r\nnumba                 : None\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : None\r\npandas_gbq            : None\r\npyarrow               : None\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : None\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.4\r\nqtpy                  : None\r\npyqt5                 : None\r\n\r\n<\/details>\r\n","comments":["Should be addressed by #55901"],"labels":["Bug","Non-Nano"]},{"title":"Move interchange protocol implementation into a separate project","body":"Some of us talked a little bit about this when we met in Basel. The interchange protocol was added to pandas a while ago, but we believe that it would be beneficial to remove it from the pandas main repo into a separate repository.\r\n\r\nThe main issue is that bugfixes to the interchange protocol are currently tied to the pandas release cycle, which means that users might have to pin the pandas version if they run into a bug that they care about. Having it as a dependency decouples the releases, which can make the protocol much more responsive and pins are no longer tied to the main pandas repository if necessary at all.\r\n\r\nThe package would be a pure and lightweight Python dependency which makes it easier to contribute for others, because the testsuite is not slowed down by the pandas testsuite.\r\n\r\nThe new optional dependency is extremly lightweight, no additional dependencies brought in and only around 30kb in size, so it really doesn't matter much.\r\n\r\nThose are really small downsides for improved interoperability and the decoupling of release cycles for both components.\r\n\r\n\r\ncc @pandas-dev\/pandas-core ","comments":["Sure, no objections to removing it from the pandas repo\r\n\r\nI could take it into https:\/\/github.com\/data-apis\/dataframe-api-compat and maintain it, if people are OK with that. There should be enough projects in the community and people in the consortium interested in this that we can keep it alive \ud83d\udcaa \r\n\r\nI think there's only a handful of repos* currently using it anyway (plotly, seaborn, vscode), it'd be pretty easy to manually make PRs to all of them. I could take this on too\r\n\r\nI think that scikit-learn and altair wouldn't need to change anything:\r\n- the former only uses the interchange protocol for not-pandas:\r\n  https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/4ce8e19859cb8b2f2bef197ed5b28beea44ee4b4\/sklearn\/utils\/validation.py#L281-L288\r\n- the latter only uses the interchange protocol for not-pandas, and uses pyarrow's `from_dataframe` method","Would we still have some sort of testing burden for the interchange protocol even after it's removed?\r\n(e.g. do we need to run a CI job against df-api-compat?)","there could be just a simple test, like there is for the api standard (which also defers to `dataframe-api-compat`):\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/8fb8b9fbbe04733336e48503dbfe5abdea8ac0c1\/pandas\/tests\/test_downstream.py#L307-L323","How often does the issue pop up where tying the fixes to a release is problematic? I _think_ that is actually preferable; syncing this across two projects seems more problematic to me\r\n\r\n> The package would be a pure and lightweight Python dependency \r\n\r\nThis is true today, although I've toyed around with moving part of the implementation down to Cython to have the pandas buffers adhere to the Python buffer protocol. I don't think that is possible in a pure Python package. See https:\/\/github.com\/pandas-dev\/pandas\/pull\/55671","plotly currently do this\r\n\r\n```python\r\n        if hasattr(args[\"data_frame\"], \"__dataframe__\") and version.parse(\r\n            pd.__version__\r\n        ) >= version.parse(\"2.0.2\"):\r\n            import pandas.api.interchange\r\n\r\n            df_not_pandas = args[\"data_frame\"]\r\n            args[\"data_frame\"] = df_not_pandas.__dataframe__()\r\n            # According interchange protocol: `def column_names(self) -> Iterable[str]:`\r\n            # so this function can return for example a generator.\r\n            # The easiest way is to convert `columns` to `pandas.Index` so that the\r\n            # type is similar to the types in other code branches.\r\n            columns = pd.Index(args[\"data_frame\"].column_names())\r\n            needs_interchanging = True\r\n        elif hasattr(args[\"data_frame\"], \"to_pandas\"):\r\n            args[\"data_frame\"] = args[\"data_frame\"].to_pandas()\r\n            columns = args[\"data_frame\"].columns\r\n```\r\n\r\nand other issues have come up since\r\n\r\nI think it would've been cleaner to just set a minimum version on dataframe-api-compat, and handle different pandas versions in there\r\n\r\n>  I've toyed around with moving part of the implementation down to Cython to have the pandas buffers adhere to the Python buffer protocol\r\n\r\nOK yeah, if there's something like that which ties it to living in pandas, then it probably shouldn't be removed, thanks for bringing this up","IIUC the PandasBuffer (which seems like the thing that would get a cython buffer implementation mixed into it) takes an ndarray in `__init__` and doesn't seem to rely on anything pandas-specific.  So I don't see that potential refactor as requiring this stuff to live in pandas (though it would negate the \"pure-python\" part of the description above).\r\n\r\nReally the reason to split this out is \"This shouldn't be our problem.\"","@WillAyd could you please expand on why it would be beneficial for the buffers to adhere to the Python buffer protocol?\r\n\r\nFurthermore, would it be possible to take that out into a non-pure-Python package, or would it still need to live in pandas?","The advantage of adhering to the Python protocol is that it allows other packages to access the raw data of a dataframe purely from Python. It doesn't necessarily need to live in pandas, but you would end up duplicating a lot of the same things that we already have in pandas (ex: build system, CI) just in another package.","Thanks\r\n\r\nJust for my understanding, would this require other libraries also adhere to the Python buffer protocol in order to be useful?","No other libraries would not need to do anything. An object that adheres to the buffer protocol can have its raw bytes accessed through standard Python objects. If you look at the tests added in the above PR you can see how that works with a memoryview","Sure but I mean, would consumers because to consume a column's raw bytes through standard Python objects in a library-agnostic manner if other libraries don't change anything?\r\n\r\nFor example, I can currently write\r\n```python\r\nif hasattr(df, '__dataframe__'):\r\n    df_xch = df.__dataframe__()\r\n    schema = {name: col.get_buffers()['data'][1][0] for (name, col) in zip(df_xch.column_names(), df_xch.get_columns())}\r\n```\r\nand know that it will give me the schema of any dataframe which implements the interchange protocol.\r\n\r\nIf pandas buffers were to adhere to the Python buffer protocol, then IIUC you'd also be able to do, say, `bytearray(memoryview(df_xch.get_column_by_name('a').get_buffers()[\"data\"][0]))`. But, if not all library which implement the interchange protocol also adhere to the python buffer protocol, then you'd have no guarantee that that code would work in a library-agnostic manner, right?","Yea I don't think it replaces or changes any parts of the dataframe protocol; I think of it more as a lightweight convenience that a third party can use to perform smaller tasks or aid in debugging a larger implementation of the dataframe protocol","In theory it could be brought up to the DataFrame Interchange Protocol spec to add Python buffer protocol support as a general requirement, but not sure all implementations would want to do that (for pyarrow it would be easy, since our main Buffer object already implements the buffer protocol). \r\n\r\n---\r\n\r\nIf we move it out, would the idea be that it becomes a _required_ dependency of pandas, and not optional? (I assume so, otherwise consumers cannot have the guarantee calling `__dataframe__` on a pandas object would work)\r\n","it could still be optional, anyone using it (e.g. plotly, seaborn) would need to make it required","But in general, we want to allow projects to be able to accept a pandas object, without depending on pandas. \r\nOf course we could ensure that this new `pandas-interchange-protocol` package doesn't actually have a hard dependency on pandas, so they could add a dependency on that package without depending on pandas. But it does require every project that wants to use the interchange protocol to add such dependencies, which feels a bit strange?","If a library already depends on pandas and does:\r\n```python\r\nif isinstance(df, pd.DataFrame):\r\n   pass\r\nelif hasattr(df, '__dataframe__'):\r\n   df = pd.api.interchange.from_dataframe(df)\r\n```\r\nthen instead of just having pandas as a dependency, they would need to also have the interchange protocol package as a dependency. Users need not change anything.\r\n\r\nHypothetically* speaking, if a library does not already depend on pandas, does not have a special path for pandas, and does, say\r\n```python\r\nif hasattr(df, '__dataframe__'):\r\n   df = pyarrow.interchange.from_dataframe(df)\r\n```\r\nthen they could either:\r\n- accept that pandas users are going to get a runtime error, telling them to install an optional dependency. pandas users already get this if they try to do `df.to_excel`, `df.plot`, `df.to_markdown`, so it doesn't feel terrible\r\n- only add the interchange protocol package as a dependency (and not pandas!). If we can keep it pure-Python and under 50 kilobytes, I don't foresee objections, it's a small price to pay in exchange for interoperability. They could even just vendor it.\r\n\r\n*I don't think any such libraries currently exist","On the call today @jorisvandenbossche brought up the C data interface that pyarrow is pushing for, which looks further along developed and probably would make the interchange protocol obsolete. If that materializes I would definitely be in favor of deprecating \/ removing this. \r\n\r\nI guess the only theoretical advantage this would have over the Arrow approach is that we would have the flexibility to serialize non-Arrow types, but I'm not sure if that is really useful in practice","I'm confused, doesn't the dataframe interchange protocol use the Arrow C Data Interface?\r\n\r\nIf you inspect the third element of the `dtype` property of a `Column`, you'll see a format string from the Arrow C Data Interface\r\n```python\r\nIn [1]: df = pd.DataFrame({'a': [pd.Timestamp('2020-01-01', tz='Asia\/Kathmandu')]})\r\n\r\nIn [2]: df\r\nOut[2]:\r\n                          a\r\n0 2020-01-01 00:00:00+05:45\r\n\r\nIn [3]: df.__dataframe__().get_column_by_name('a').dtype\r\nOut[3]: (<DtypeKind.DATETIME: 22>, 64, 'tsn:Asia\/Kathmandu', '=')\r\n```\r\n`'tsn:Asia\/Kathmandu'` means \"timestamp [nanoseconds] with timezone \u201cAsia\/Kathmandu\u201d\", according to https:\/\/arrow.apache.org\/docs\/format\/CDataInterface.html#data-type-description-format-strings\r\n\r\nI'm probably missing something, but could you please elaborate on how it would make the dataframe interchange protocol obsolete?","The C data interface documents the comparison\n\nhttps:\/\/arrow.apache.org\/docs\/format\/CDataInterface\/PyCapsuleInterface.html#comparison-to-dataframe-interchange-protocol\n\nThere is definitely a lot of overlap. The difference would really come down to how important we think it is to exchange non-arrow types ","> Hypothetically* speaking, if a library does not already depend on pandas, does not have a special path for pandas ... *I don't think any such libraries currently exist\r\n\r\nTrue, but of course it's a bit of the point of the interchange protocol to allow this more easily\r\n\r\n----\r\n\r\n> On the call today @jorisvandenbossche brought up the C data interface that pyarrow is pushing for, which looks further along developed and probably would make the interchange protocol obsolete.\r\n\r\nWhile there is certainly overlap, it doesn't make it fully obsolete, there are definitely still use case for both. In fact, I actually also proposed to add the Arrow protocol to the DataFrame Interchange protocol: https:\/\/github.com\/data-apis\/dataframe-api\/issues\/279 \/ https:\/\/github.com\/data-apis\/dataframe-api\/pull\/342. \r\nThe interchange protocol still gives you a nicer Python API to do some inspection, while the Arrow capsule protocol just gives you an opaque capsule (unless you use a library to consume those). So there can be a use case to first get `__dataframe__` to inspect the column names, or number of rows, .. select the columns you need, etc, and then when you actually want to consume the data, use the `__arrow_c_array\/stream__` on the dataframe interchange object to access the actual data (instead of accessing the data through the columns and buffers of the interchange protocol). In summary, it's mostly an alternative to the manual access of individual buffer addresses for consumers, but not necessarily for the top-level dataframe object returned by `__dataframe__`.\r\n\r\n> I guess the only theoretical advantage this would have over the Arrow approach is that we would have the flexibility to serialize non-Arrow types, but I'm not sure if that is really useful in practice\r\n\r\nIndeed, that's the main disadvantage. For example, _if_ you have one dataframe library that implements missing values with a sentinel or byte mask, and interchanges data with another dataframe library that uses the same representation, then passing through Arrow incurs some extra overhead (because it only has one way of representing missing values), while going through the columns\/buffers of the interchange protocol allows you to get exactly the data as they are represented by the producing library.\r\n\r\n> I'm confused, doesn't the dataframe interchange protocol use the Arrow C Data Interface?\r\n\r\nThe dataframe interchange protocol uses the type _format string notation_ of the Arrow C Data Interface, as a way to have a more detailed type description than the generic enum (first element of the `dtype` tuple). But that's only a format string, it further doesn't actually use the core of the Arrow C Data Interface, i.e. the C struct with the actual buffer pointers. \r\n\r\n---\r\n\r\nTo mention it here as well, I have a PR to add the Arrow C Data Interface (PyCapsule protocol) to pandas: https:\/\/github.com\/pandas-dev\/pandas\/pull\/56587","Thanks all for comments (and ensuing conversations), interesting stuff\r\n\r\n> The interchange protocol still gives you a nicer Python API to do some inspection\r\n\r\nIndeed! I do think it is nice to be able to do\r\n```python\r\npandas_df = pd.api.interchange.from_dataframe(some_other_df)\r\n```\r\nand to know that, so long as `some_other_df` implements `__dataframe__`, then `some_other_df` can be converted to pandas\r\n\r\nIf I've understood Joris' proposal in https:\/\/github.com\/pandas-dev\/pandas\/pull\/56587, then this would allow for the Python API to be kept, but for it to use (where possible) the Arrow C Data Interface\r\n\r\nThis way, plotly \/ seaborn \/ altair could keep saying \"if you input something which isn't pandas\/pyarrow, we can agnostically convert to them\"","The main open issues have been addressed. There's only 2 left, and they're very minor - I have a PR open for one, and will open one for the other next week (already got it working locally)\r\n\r\nIt's really not that bad to deal with IMO\r\n\r\nAt this point, given that the interchange protocol already has users, and that splitting it out would require those users to take on an extra dependency, I think it may not be too bad to just keep it?"],"labels":["Needs Discussion","Interchange"]},{"title":"DEPR: EA._pad_or_backfill not having a limit_area argument","body":"Ref: #56616\r\n\r\nCurrently we allow EAs to have `_pad_or_backfill` methods that do not accept a `limit_area` argument, and will only raise if e.g. `Series.ffill` is called with `limit_area` specified as either `\"inside\"` or `\"outside\"`. We should deprecate this behavior, informing EA authors that override this method they need to add such an argument. The current plan:\r\n\r\n - Add a DeprecationWarning in 3.1\r\n - Switch to FutureWarning in 3.2\r\n - Enforce the deprecation in 4.0","comments":[],"labels":["Missing-data","Deprecate","ExtensionArray"]},{"title":"TST: test_setitem_integer_with_missing_raises contains duplicate parametrization","body":"https:\/\/github.com\/pandas-dev\/pandas\/blob\/c4b6bed5346521617b8361801cad0bf854c40994\/pandas\/tests\/extension\/base\/setitem.py#L211-L216\r\n\r\nOne of these parametrizations should probably be `(pd.array([0, 1, 2, pd.NA], dtype=\"Int64\"), True),` but that fails several tests that inherit this test. The task is to investigate whether the failure is expected or OK to xfail","comments":[" pytest pandas\/tests\/extension\/base\/setitem.py  I was trying this command to debug this but no tests were run. Any command to run it in command line?","take"],"labels":["Bug","Testing","ExtensionArray"]},{"title":"BUG: `RollingGroupby.agg` returns no columns when column selected in list is one of the groupby columns","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nX = pd.DataFrame({\"group\": [1, 2, 2, 3, 3, 3], \"other\": [1, 1, 1, 1, 1, 1]})\r\nX.groupby([\"group\"])[[\"group\"]].rolling(2, min_periods=1).agg(\"count\")\n```\n\n\n### Issue Description\n\nA DataFrame with no columns is returned.\r\n\r\n```\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(1, 0), (2, 1), (2, 2), (3, 3), (3, 4), (3, 5)]\r\n```\r\n\r\n---\r\n\r\nAs an FYI, `X.groupby([\"group\"])[\"group\"].rolling(2, min_periods=1).agg(\"count\")` seems fine\r\n```\r\ngroup   \r\n1      0    1.0\r\n2      1    1.0\r\n       2    2.0\r\n3      3    1.0\r\n       4    2.0\r\n       5    2.0\r\nName: group, dtype: float64\r\n```\r\nas does\r\nX.groupby([\"group\"])[[\"other\"]].rolling(2, min_periods=1).agg(\"count\")\r\n```\r\n         other\r\ngroup         \r\n1     0    1.0\r\n2     1    1.0\r\n      2    2.0\r\n3     3    1.0\r\n      4    2.0\r\n      5    2.0\r\n```\r\nas does\r\nX.groupby([\"group\"])[\"other\"].rolling(2, min_periods=1).agg(\"count\")\r\n```\r\ngroup   \r\n1      0    1.0\r\n2      1    1.0\r\n       2    2.0\r\n3      3    1.0\r\n       4    2.0\r\n       5    2.0\r\nName: other, dtype: float64\r\n```\r\n\r\n---\r\n\r\nAlso, the problem does not seem to persist if we don't perform a non collapsing aggregation: \r\n\r\n`X.groupby([\"group\"])[[\"group\"]].agg(\"count\")`\r\n```\r\n       group\r\ngroup       \r\n1          1\r\n2          2\r\n3          3\r\n```\r\n\r\nbut does if we perform expanding aggregation:\r\n\r\n`X.groupby([\"group\"])[[\"group\"]].expanding(min_periods=1).agg(\"count\")`\r\n```\r\nEmpty DataFrame\r\nColumns: []\r\nIndex: [(1, 0), (2, 1), (2, 2), (3, 3), (3, 4), (3, 5)]\r\n```\n\n### Expected Behavior\n\nIt would output the same as when one does \r\n```python\r\nX.groupby([\"group\"])[[\"other\"]].rolling(2, min_periods=1).agg(\"count\")\r\n```\r\nbut with `\"group\"` as the column name.\n\n### Installed Versions\n\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.11.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 22.4.0\r\nVersion             : Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3\/RELEASE_ARM64_T6000\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.0\r\npip                 : 23.0.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : 8.19.0\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n```\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, related to https:\/\/github.com\/pandas-dev\/pandas\/pull\/40341#discussion_r603001314 (cc @mroeschke, @jbrockmendel).\r\n\r\nThe rest of groupby will keep the grouping column if you select it, e.g.\r\n\r\n    df[[\"a\", \"b\"]].groupby(\"a\")[[\"a\", \"b\"]].sum()\r\n\r\nwill sum both `a` and `b` in the result. I think we should agree with that here."],"labels":["Bug","Window"]},{"title":"BUG: `SeriesGroupBy.plot` does not respect `cmap` parameter","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nchunk = pd.DataFrame(index=[0, 1, 2])\r\n\r\ndf = pd.concat([chunk.assign(y=i, i=i) for i in range(3)])\r\n\r\ndf.groupby(\"i\")[\"y\"].plot(legend=True, cmap=\"tab20\");\n```\n\n\n### Issue Description\n\nThe above code snippet demonstrates that `cmap=\"tab20\"` is not respected. The parameter is respected in `DataFrame.plot`, which can be checked by viewing\r\n\r\n```python\r\ndf.plot(cmap=\"tab20\")\r\n```\n\n### Expected Behavior\n\nThe `cmap` parameter should be correctly handled\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.2.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.1.0\r\nVersion             : Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:24 PDT 2023; root:xnu-10002.41.9~6\/RELEASE_ARM64_T6000\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.19.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.4\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"DOC: Cheat Sheet of Pandas is not present on homepage of Pandas docs website.","body":"### Pandas version checks\n\n- [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https:\/\/pandas.pydata.org\/docs\/dev\/)\n\n\n### Location of the documentation\n\nhttps:\/\/pandas.pydata.org\/docs\/index.html\n\n### Documentation problem\n\nThe cheat sheet is not present on pandas.pydata.org (website). I need to open it from the codebase every time.\n\n### Suggested fix for documentation\n\nWe can put cheat sheets of pandas on the Pandas documentation website or a link that can redirect them to the cheat sheet.","comments":["I wonder where is the best place to add this link? Shall we add this to the index page, or sections like \"getting started\" or \"user guide\"?\r\nBy the way, shall we also consider adding this link to github README?","It's a little obscure but it can be found at the bottom of the getting started page.\r\nhttps:\/\/pandas.pydata.org\/getting_started.html.\r\n\r\nIt definetely can be placed in a better location, though.","Thanks @lithomas1 \r\nThis link can also be found at the bottom of the getting started page of documentation, under the subtitle \"Tutorials\". It is hard to notice, though.\r\nhttps:\/\/pandas.pydata.org\/docs\/getting_started\/index.html\r\nMaybe it's better to show this link together with a picture, just like the section \"Coming from\u2026\" in the same page?","@Huanghz2001 \r\nI have took your suggestion and added a section similar to \"Coming from..\" in the same page and have made the changes in the file  \"pandas\/blob\/main\/doc\/source\/getting_started\/index.rst\"\r\n![Screenshot 2024-01-05 012242](https:\/\/github.com\/pandas-dev\/pandas\/assets\/89439095\/1d37974f-6bb5-4bcf-9f08-2e75b253da45)\r\n"],"labels":["Docs"]},{"title":"BUG: Pd.qcut inconsistency when the sample input is equally distributed","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Set a seed for reproducibility\r\nnp.random.seed(42)\r\n\r\n# Number of rows for each group\r\nrows_per_group = 218\r\n\r\n# Create a DataFrame with equal rows for each group\r\ndf = pd.DataFrame({\r\n    'Group': np.repeat(['A', 'B', 'C'], rows_per_group),\r\n    'Column1': np.random.rand(rows_per_group * 3)\r\n})\r\n\r\n# Display the DataFrame\r\nprint(df)\r\n\r\n# Create a rank column\r\ndf[\"Rank\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\")\r\n\r\n# Sort the DataFrame\r\ndf = df.sort_values(by=[\"Group\", \"Rank\"])\r\n\r\n# Sorting each group into 55 intervals (218 rows in each group yields 4 rows in each interval except 2 intervals that only has 3 rows (218 % 55) if we have 55 intervals (218 \/ 55))\r\ndf[\"Interval\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.qcut(x, 55, labels=False, duplicates='drop') + 1)\r\n\r\n# Results: It is random where these 2 intervals choose to have the three rows and it lacks consistency.\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nSo pd.qcut shows inconsistency when the pd.cut is indifferent into putting the last extra\/lacking numbers into intervals. For example. If i have 218 rows in each group by ranking them 1-218 in each group and i want to put them in 55 intervals, that would mean i place 4 values into each interval with the exception of 2 intervals that only have 3 values. The problem is that pd.cut just assigns these 3 values into a random intervals if the values are equally distributed. Could be 24 and 48 could be 2 other intervals. No consistency. Would therefore be nice if it was consistent so that if there was only 1 rest\/lacking value, it goes to the middle interval, lets say round(55\/2). If its 2, it could be the interval under the middle interval, if its three it could be above round(55\/2) and so on.\r\n\r\n### Expected Behavior\r\n\r\nThat there will be consistency in where the last extra\/less values gets put \r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.12.0.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 142 Stepping 9, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en\r\nLOCALE              : Norwegian Bokm\u00e5l_Norway.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : 3.0.6\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : 7.2.6\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.18.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : 2.4.1\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Shall I look into this @Chuck321123 ","@pmhatre1 So i have looked into this once more, and pd.qcut makes more sense when i run ```df[\"Interval2\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.qcut(x, 55, duplicates='drop'))``` Where i removed +1 and added labels=True. I was wrong to say that the intervals with three rows are randomly assigned as in my example they are always 24 and 48. So maybe an added option to distribute it differently would be a suggestion? so that the rest values\/lacking values always either lands in an end interval or in the middle? \r\n\r\nEdit: The most correct way would be to compare these if youre interested:\r\n ```\r\ndf[\"Interval\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.qcut(x, 55, labels=False, duplicates='drop'))\r\n\r\ndf[\"Interval2\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.qcut(x, 55, duplicates='drop'))\r\n\r\ndf[\"Interval3\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.cut(x, 55, labels=False, duplicates='drop'))\r\n\r\ndf[\"Interval4\"] = df.groupby(\"Group\")[\"Column1\"].rank(method=\"first\").transform(lambda x: pd.cut(x, 55, duplicates='drop'))\r\n``` \r\nThe two above are with qcut and the two lower with cut. Same problem there as well"],"labels":["Bug","Needs Triage"]},{"title":"REF: simplify _constructor\/_from_mgr\/_constructor_from_mgr\/...","body":"ATM we have\r\n\r\n- _constructor\r\n- _constructor_sliced\r\n- _constructor_expanddim\r\n- _from_mgr\r\n- _sliced_from_mgr\r\n- _expanddim_from_mgr\r\n- _constructor_from_mgr\r\n- _constructor_sliced_from_mgr\r\n- _constructor_expanddim_from_mgr\r\n\r\nI suspect that we can make do with fewer of these, or at least simplify them (e.g. make some `@final`, `@classmethod`, #51772, make downstream libraries never touch Managers, very slightly improve perf in the non-subclass cases).  Some of these ideas are mutually exclusive:\r\n\r\n1) ATM _from_mgr is a classmethod, all the other *_from_mgr methods are regular methods.  The _constructor_*_from_mgr methods rely on `self._constructor*`, so could not be made classmethods unless those constructor*s also became classmethods.  Does anyone out there rely on _constructor* _not_ being a classmethod?\r\n\r\n2) `_expanddim_from_mgr` and `_sliced_from_mgr` are each only used once, in `_constructor_expanddim_from_mgr` and `_constructor_sliced_from_mgr`, respectively.  Can we just inline these and remove the methods (maybe keep the less-verbose names)?  If not, can we at least `@final` and `@classmethod` them?\r\n\r\n3) Can `_from_mgr` be made `@final`?  If not can `_constructor_from_mgr` be inlined into it?\r\n\r\n4) The 3 listed expanddim methods are there just for `Series.to_frame`.  We could get rid of all three and tell subclasses to just override `to_frame`.  (Doing this would also make it easier to tell subclass authors to handle metadata propagation in to_frame xref #32860)\r\n\r\n5) We can slightly improve perf for non-subclasses by changing `if self._constructor is Series` to `if type(self) is Series` at the cost of slightly hurting perf for subclasses.\r\n\r\n6) Most extreme: we can make it so that a) (most) subclasses don't need to override _any_ *from_mgr methods, b) subclasses `__init__` don't need to know how to handle Managers, c) we don't need the constructor*from_mgr methods and d) (most) subclass authors never touch Managers by writing e.g.\r\n\r\n```\r\ndef _from_mgr(self, mgr, axes):\r\n    df = DataFrame._existing_from_mgr_method(mgr, axes)\r\n    if type(self) is DataFrame:\r\n        return df\r\n    return self._constructor(df)\r\n```\r\n\r\ni.e. assume that the subclass `__init__` knows what to do when given a pandas object.  \r\n\r\n@jorisvandenbossche [writes](https:\/\/github.com\/pandas-dev\/pandas\/pull\/54922#issuecomment-1779545693) that this would inconvenience geopandas since it does inference when given a DataFrame but not when given a Manager.  In cases like that geopandas would need to override _from_mgr (why \"most\" is needed in a) and d) above), but ATM they need to override _constructor_from_mgr, so that wouldn't be a net increase in Stuff They Need To Do.\r\n\r\nThis idea is roughly #53871, which was reverted by #54922.\r\n","comments":["Just chiming in as a maintainer of a package that subclasses Pandas DataFrames and Series'. Apart from having many constructor functions and properties, at the moment it is also not very clear what a subclass can override, should override and must not override. \r\n\r\nThe documentation for subclassing (https:\/\/pandas.pydata.org\/docs\/development\/extending.html#subclassing-pandas-data-structures) only mentions `._constructor`, `._constructor_sliced` and `._constructor_expanddim`. I would argue that there's no way around reading the source code anyway, when subclassing. But from the comments there, it's still very difficult to figure out how everything works.\r\n\r\nUnderstanding the concepts here is fairly difficult if one does not actively participate in core development. And this has become more difficult over the last releases with the addition of new constructor related methods. Therefore, I am in favour of simplifying this system. But additionally, it would then be great to have some more documentation on the purpose of each function or property. Even if it's just in the form of comments in the code.","take","take","take","take"],"labels":["Refactor","Needs Discussion","Subclassing"]},{"title":"BUG: Empty list passed to `Series` returns `object` dtype, but via `DataFrame` returns `float64`","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nIn [1]: import pandas as pd\r\npd.\r\nIn [2]: pd.__version__\r\nOut[2]: '2.1.4'\r\n\r\nIn [3]: pd.Series([])\r\nOut[3]: Series([], dtype: object)\r\n\r\nIn [4]: pd.DataFrame({'a':[]})\r\nOut[4]: \r\nEmpty DataFrame\r\nColumns: [a]\r\nIndex: []\r\n\r\nIn [5]: pd.DataFrame({'a':[]}).dtypes\r\nOut[5]: \r\na    float64\r\ndtype: object\n```\n\n\n### Issue Description\n\nThere seems to be an inconsistency when creating a Series from empty list via `Series` & `DataFrame` constructors. The former yields `object` dtype, the later returns `float64` dtype.\n\n### Expected Behavior\n\nReturn `object` in `DataFrame` constructor\n\n### Installed Versions\n\n<details>\r\n\r\n\/nvme\/0\/pgali\/envs\/cudfdev\/lib\/python3.10\/site-packages\/_distutils_hack\/__init__.py:33: UserWarning: Setuptools is replacing distutils.\r\n  warnings.warn(\"Setuptools is replacing distutils.\")\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.10.13.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.0-88-generic\r\nVersion             : #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.4\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : 3.0.6\r\npytest              : 7.4.3\r\nhypothesis          : 6.91.0\r\nsphinx              : 7.2.6\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.18.1\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.12.1\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : 0.57.1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : 2023.12.1\r\nscipy               : 1.11.4\r\nsqlalchemy          : 2.0.23\r\ntables              : None\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\n<\/details>\r\n","comments":["Same is the behavior for setitem flow too:\r\n\r\n```\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.DataFrame()\r\n\r\nIn [3]: df['a'] = []\r\n\r\nIn [4]: df.dtypes\r\nOut[4]: \r\na    float64\r\ndtype: object\r\n```\r\n","Thanks for the report. I would also expect this to be `object` type from these similar constructions\r\n\r\n```python\r\nIn [4]: pd.DataFrame(columns=[\"a\"]).dtypes\r\nOut[4]: \r\na    object\r\ndtype: object\r\n\r\nIn [5]: pd.DataFrame([], columns=[\"a\"]).dtypes\r\nOut[5]: \r\na    object\r\ndtype: object\r\n\r\nIn [6]: pd.DataFrame({}, columns=[\"a\"]).dtypes\r\nOut[6]: \r\na    object\r\ndtype: object\r\n\r\nIn [7]: pd.DataFrame({\"a\": []}, columns=[\"a\"]).dtypes\r\nOut[7]: \r\na    float64\r\ndtype: object\r\n\r\nIn [8]: pd.DataFrame({\"a\": pd.Series()}).dtypes\r\nOut[8]: \r\na    object\r\ndtype: object\r\n```\r\n\r\nIt looks like this goes through `sanitize_array` where there's this comment\r\n\r\n```python\r\n        if len(data) == 0 and dtype is None:\r\n            # We default to float64, matching numpy\r\n            subarr = np.array([], dtype=np.float64)\r\n```\r\n\r\nI'm not sure if there a reason internally why we need to treat this as float64 but I would expect at least via this constructor route that object is still returned","I would like to take a look at this issue\r\n","> Thanks for the report. I would also expect this to be `object` type from these similar constructions\r\n> \r\n> ```python\r\n> In [4]: pd.DataFrame(columns=[\"a\"]).dtypes\r\n> Out[4]: \r\n> a    object\r\n> dtype: object\r\n> \r\n> In [5]: pd.DataFrame([], columns=[\"a\"]).dtypes\r\n> Out[5]: \r\n> a    object\r\n> dtype: object\r\n> \r\n> In [6]: pd.DataFrame({}, columns=[\"a\"]).dtypes\r\n> Out[6]: \r\n> a    object\r\n> dtype: object\r\n> \r\n> In [7]: pd.DataFrame({\"a\": []}, columns=[\"a\"]).dtypes\r\n> Out[7]: \r\n> a    float64\r\n> dtype: object\r\n> \r\n> In [8]: pd.DataFrame({\"a\": pd.Series()}).dtypes\r\n> Out[8]: \r\n> a    object\r\n> dtype: object\r\n> ```\r\n> \r\n> It looks like this goes through `sanitize_array` where there's this comment\r\n> \r\n> ```python\r\n>         if len(data) == 0 and dtype is None:\r\n>             # We default to float64, matching numpy\r\n>             subarr = np.array([], dtype=np.float64)\r\n> ```\r\n> \r\n> I'm not sure if there a reason internally why we need to treat this as float64 but I would expect at least via this constructor route that object is still returned\r\n\r\nI have debugged and observed that for all the cases except pd.DataFrame({'a': []}) we are getting the length of the data argument in sanitize_array to be 1.\r\n<img width=\"296\" alt=\"p1\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/34889400\/77b9e515-8f6f-4c4c-95c0-18c18e27949c\">\r\n\r\ndoes that mean there is inconsistency with only when an empty dictionary with specified column is passed.\r\nand i think sanitize_array is being called twice for when its Dataframe, and in the second time we are getting float64. As you can see below in the call stack. The first time during initialising we are getting the data to be ['a'] but the second time its empty. when its being called from arrays_to_mgr() which you can see in second call stack.\r\n\r\n<img width=\"293\" alt=\"p2\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/34889400\/f40ad325-2f90-4478-8b44-9e5ea9473c75\">\r\n\r\n<img width=\"287\" alt=\"image\" src=\"https:\/\/github.com\/pandas-dev\/pandas\/assets\/34889400\/20c20c89-6df8-4a9b-8538-390054965761\">\r\n\r\n","Related: on an empty DataFrame, `.values` and `._values` is a float whereas I'd expect object.\r\n\r\n```\r\nprint(pd.DataFrame().values.dtype)\r\n# float64\r\n```\r\n\r\ndue to this line:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/1bf86a35a56405e07291aec8e07bd5f7b8b6b748\/pandas\/core\/internals\/managers.py#L1802\r\n\r\nI ran into this because `DataFrame.stack` uses `._values` to determine the result dtype, and on an empty frame we wind up with float whereas I'd expect object.","> I'm not sure if there a reason internally why we need to treat this as float64 but I would expect at least via this constructor route that object is still returned\r\n\r\nWith both changes (handling the OP and the one I mentioned above), I'm seeing 35 tests fail in the expected way (i.e. there isn't some functionality we definitely don't want to change that breaks). It seems clear to me these changes would make dtypes on empty objects more consistent. The only question on my mind is if this is a bug fix or needs deprecation.","Especially with 3.0 as the next release I would be OK treating this as a \"bug fix\""],"labels":["Bug","Dtype Conversions","DataFrame","Constructors"]},{"title":"BUG: Can't cast pyarrow floats to ints","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\nThis fails with pyarrow types but works with legacy types:\r\n\r\n```pycon\r\n>>> pd.Series([2.3, 2.5], dtype='float64[pyarrow]').astype('int64[pyarrow]')\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\nCell In[100], line 1\r\n----> 1 pd.Series([2.3, 2.5], dtype='float64[pyarrow]').astype('int64[pyarrow]')\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/generic.py:6637, in NDFrame.astype(self, dtype, copy, errors)\r\n   6631     results = [\r\n   6632         ser.astype(dtype, copy=copy, errors=errors) for _, ser in self.items()\r\n   6633     ]\r\n   6635 else:\r\n   6636     # else, only a single dtype is given\r\n-> 6637     new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\r\n   6638     res = self._constructor_from_mgr(new_data, axes=new_data.axes)\r\n   6639     return res.__finalize__(self, method=\"astype\")\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py:431, in BaseBlockManager.astype(self, dtype, copy, errors)\r\n    428 elif using_copy_on_write():\r\n    429     copy = False\r\n--> 431 return self.apply(\r\n    432     \"astype\",\r\n    433     dtype=dtype,\r\n    434     copy=copy,\r\n    435     errors=errors,\r\n    436     using_cow=using_copy_on_write(),\r\n    437 )\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/managers.py:364, in BaseBlockManager.apply(self, f, align_keys, **kwargs)\r\n    362         applied = b.apply(f, **kwargs)\r\n    363     else:\r\n--> 364         applied = getattr(b, f)(**kwargs)\r\n    365     result_blocks = extend_blocks(applied, result_blocks)\r\n    367 out = type(self).from_blocks(result_blocks, self.axes)\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/internals\/blocks.py:754, in Block.astype(self, dtype, copy, errors, using_cow, squeeze)\r\n    751         raise ValueError(\"Can not squeeze with more than one column.\")\r\n    752     values = values[0, :]  # type: ignore[call-overload]\r\n--> 754 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\r\n    756 new_values = maybe_coerce_values(new_values)\r\n    758 refs = None\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/dtypes\/astype.py:237, in astype_array_safe(values, dtype, copy, errors)\r\n    234     dtype = dtype.numpy_dtype\r\n    236 try:\r\n--> 237     new_values = astype_array(values, dtype, copy=copy)\r\n    238 except (ValueError, TypeError):\r\n    239     # e.g. _astype_nansafe can fail on object-dtype of strings\r\n    240     #  trying to convert to float\r\n    241     if errors == \"ignore\":\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/dtypes\/astype.py:179, in astype_array(values, dtype, copy)\r\n    175     return values\r\n    177 if not isinstance(values, np.ndarray):\r\n    178     # i.e. ExtensionArray\r\n--> 179     values = values.astype(dtype, copy=copy)\r\n    181 else:\r\n    182     values = _astype_nansafe(values, dtype, copy=copy)\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/arrays\/base.py:709, in ExtensionArray.astype(self, dtype, copy)\r\n    707 if isinstance(dtype, ExtensionDtype):\r\n    708     cls = dtype.construct_array_type()\r\n--> 709     return cls._from_sequence(self, dtype=dtype, copy=copy)\r\n    711 elif lib.is_np_dtype(dtype, \"M\"):\r\n    712     from pandas.core.arrays import DatetimeArray\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:281, in ArrowExtensionArray._from_sequence(cls, scalars, dtype, copy)\r\n    277 \"\"\"\r\n    278 Construct a new ExtensionArray from a sequence of scalars.\r\n    279 \"\"\"\r\n    280 pa_type = to_pyarrow_type(dtype)\r\n--> 281 pa_array = cls._box_pa_array(scalars, pa_type=pa_type, copy=copy)\r\n    282 arr = cls(pa_array)\r\n    283 return arr\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pandas\/core\/arrays\/arrow\/array.py:497, in ArrowExtensionArray._box_pa_array(cls, value, pa_type, copy)\r\n    495 else:\r\n    496     try:\r\n--> 497         pa_array = pa_array.cast(pa_type)\r\n    498     except (\r\n    499         pa.ArrowInvalid,\r\n    500         pa.ArrowTypeError,\r\n    501         pa.ArrowNotImplementedError,\r\n    502     ):\r\n    503         if pa.types.is_string(pa_array.type) or pa.types.is_large_string(\r\n    504             pa_array.type\r\n    505         ):\r\n    506             # TODO: Move logic in _from_sequence_of_strings into\r\n    507             # _box_pa_array\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/table.pxi:565, in pyarrow.lib.ChunkedArray.cast()\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/compute.py:404, in cast(arr, target_type, safe, options, memory_pool)\r\n    402     else:\r\n    403         options = CastOptions.safe(target_type)\r\n--> 404 return call_function(\"cast\", [arr], options, memory_pool)\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/_compute.pyx:590, in pyarrow._compute.call_function()\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/_compute.pyx:385, in pyarrow._compute.Function.call()\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/error.pxi:154, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~\/.envs\/pd22rc\/lib\/python3.11\/site-packages\/pyarrow\/error.pxi:91, in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Float value 2.3 was truncated converting to int64\r\n```\r\n\r\n\r\n\r\n### Issue Description\r\n\r\nConverting pyarrow floats to integers requires passing through legacy types\r\n\r\n### Expected Behavior\r\n\r\nI would expect the same behavior with legacy pandas types\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n------------------\r\ncommit                : d4c8d82b52045f49a0bb1d762968918a06886ae9\r\npython                : 3.11.6.final.0\r\npython-bits           : 64\r\nOS                    : Darwin\r\nOS-release            : 23.2.0\r\nVersion               : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6000\r\nmachine               : arm64\r\nprocessor             : arm\r\nbyteorder             : little\r\nLC_ALL                : en_US.UTF-8\r\nLANG                  : None\r\nLOCALE                : en_US.UTF-8\r\n\r\npandas                : 2.2.0rc0\r\nnumpy                 : 1.26.2\r\npytz                  : 2023.3.post1\r\ndateutil              : 2.8.2\r\nsetuptools            : 68.2.2\r\npip                   : 23.3.1\r\nCython                : 3.0.7\r\npytest                : None\r\nhypothesis            : None\r\nsphinx                : None\r\nblosc                 : None\r\nfeather               : None\r\nxlsxwriter            : None\r\nlxml.etree            : 4.9.4\r\nhtml5lib              : None\r\npymysql               : None\r\npsycopg2              : None\r\njinja2                : 3.1.2\r\nIPython               : 8.19.0\r\npandas_datareader     : None\r\nadbc-driver-postgresql: None\r\nadbc-driver-sqlite    : None\r\nbs4                   : 4.12.2\r\nbottleneck            : None\r\ndataframe-api-compat  : None\r\nfastparquet           : None\r\nfsspec                : None\r\ngcsfs                 : None\r\nmatplotlib            : 3.8.2\r\nnumba                 : 0.58.1\r\nnumexpr               : None\r\nodfpy                 : None\r\nopenpyxl              : 3.1.2\r\npandas_gbq            : None\r\npyarrow               : 14.0.2\r\npyreadstat            : None\r\npython-calamine       : None\r\npyxlsb                : None\r\ns3fs                  : None\r\nscipy                 : 1.11.4\r\nsqlalchemy            : None\r\ntables                : None\r\ntabulate              : None\r\nxarray                : None\r\nxlrd                  : None\r\nzstandard             : None\r\ntzdata                : 2023.3\r\nqtpy                  : None\r\npyqt5                 : None\r\n<\/details>\r\n","comments":["Hm, I think Arrow casts are safe by default.\r\n\r\nMaybe this can be solved by adding a ``safe`` keyword to astype.\r\n\r\nI'll also add that there's an existing issue for turning on \"safe\" casting by default - even for numpy,\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/issues\/45588."],"labels":["API - Consistency","Arrow","Astype"]},{"title":"ENH: Implement categorical accessor for arrow arrays","body":"Implements the first few methods for the categorical accessor, we could backport, but not sure if I get all of them until the release happens","comments":["This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Categorical","Stale","Arrow"]},{"title":"ENH: Need new semantics for selecting parquet engine in Pandas 3","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [X] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nThis isn't a problem right now, but I just wanted to call it out ahead of 3.0:\r\n\r\n[This behavior](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.to_parquet.html#pandas-dataframe-to-parquet):\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/8b77271393efc678b9897e009c3201e6eeef57fc\/pandas\/io\/parquet.py#L433-L437\r\n\r\nWon't really make sense in 3.0 and onwards,  because:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/8b77271393efc678b9897e009c3201e6eeef57fc\/doc\/source\/whatsnew\/v2.2.0.rst?plain=1#L87\r\n\r\n\n\n### Feature Description\n\nI guess just always use pyarrow unless explicitly told to use fastparquet?\n\n### Alternative Solutions\n\nN\/A\n\n### Additional Context\n\n_No response_","comments":["> I guess just always use pyarrow unless explicitly told to use fastparquet?\r\n\r\nThis would be I suppose the functional equivalent to the current behaviour. Already, many more things are tested with arrow only, which I would say is quite a shame. I may be biased, however.","Thanks for the report. I think we can just change the line\r\n\r\n> The default ``io.parquet.engine`` behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable.\r\n\r\nto something like\r\n\r\n> The default value ``io.parquet.engine`` to use 'pyarrow'.\r\n\r\nDoes that cover everything @davetapley?","I think we should deprecate fast parquet altogether when 3.0 is here, there isn't really much use for it and it doesn't support some things that arrow supports and we use. \r\n\r\nSimplifying code complexity is another benefit of doing this","@rhshadrach yeah, I might have been over thinking it, perhaps it is just a docs change.\r\n\r\nI have no opinion on fast parquet support. "],"labels":["Deprecate","IO Parquet","Blocked"]},{"title":"BUG: inconsistent behaviour of 'to_timedelta' for float 'arg' and different 'unit'","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ntd1 = pd.to_timedelta(1.75, unit='ns')\r\ntd2 = pd.to_timedelta(1.75e-3, unit='us')\r\n\r\nassert td1 == td2, (td1, td2) # fails with AssertionError: (Timedelta('0 days 00:00:00.000000001'), Timedelta('0 days 00:00:00.000000002'))\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\n`to_timedelta` docs are not clear on how sub nanosecond precision should be rounded\/truncated when different `unit` are used in the conversion. \r\n\r\n### Expected Behavior\r\n\r\nEquivalent definitions [^1] of timedelta should give raise to the same result. Current behaviour seems to be \r\n\r\n- truncate if `unit='ns'`\r\n- round otherwise\r\n\r\n[^1]: Here \u201cequivalent\u201c is meant within `float` precision, in fact `1.75 == 1.75e-3 * 1000`\r\n\r\nThis is very confusing, and can give raise to small glitches in the conversions.\r\n\r\nPlease note that this behaviour is not due to the fact that decimal literals have no \u201cexact\u201d `float` representation:\r\n```\r\ntd1 = pd.to_timedelta(2000 \/ 1024, unit='ns')\r\ntd2 = pd.to_timedelta(2 \/ 1024, unit='us')\r\n\r\nassert td1 == td2, (td1, td2)\r\n```\r\nstill fails with\r\n```\r\nAssertionError: (Timedelta('0 days 00:00:00.000000001'), Timedelta('0 days 00:00:00.000000002'))\r\n```\r\nalthough there is no rounding in `2000 \/ 1024` and `2 \/ 1024`.\r\n\r\nPlease note also that truncating sub nanosecond precision is not consistent with [`datetime.timedelta`](https:\/\/docs.python.org\/3\/library\/datetime.html#datetime.timedelta) which has \u00b5s resolution. In fact the docs clearly state that\r\n> If any argument is a float and there are fractional microseconds, the fractional microseconds left over from all arguments are combined and their sum is rounded to the nearest microsecond using round-half-to-even tiebreaker.\r\n\r\nAnalogoulsy `to_timedelta` and the `Timedelta` constructor should always round to the nearest nanosecond using round-half-to-even tiebreaker when `float` args are used.\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.4.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.1.0\r\nVersion             : Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:27 PDT 2023; root:xnu-10002.41.9~6\/RELEASE_X86_64\r\nmachine             : x86_64\r\nprocessor           : i386\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : None.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.1\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.1.2\r\npip                 : 23.3.2\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.3\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.15.0\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.9.0\r\ngcsfs               : None\r\nmatplotlib          : 3.8.0\r\nnumba               : 0.59.0rc1\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : 3.1.2\r\npandas_gbq          : None\r\npyarrow             : 13.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.3\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : 2023.8.0\r\nxlrd                : 2.0.1\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n```\r\n\r\n<\/details>\r\n\r\n\r\n\r\n","comments":["Another strange edge case, reported in #46819 \r\n```\r\n>>> pd.to_timedelta(2.5225, 's').asm8\r\nnumpy.timedelta64(2522499999,'ns')\r\n>>> decimal.Decimal(2.5225)*10**9\r\nDecimal('2522499999.999999964472863212')\r\n```\r\nbut\r\n```\r\n>>> pd.to_timedelta(2.5223, 's').asm8\r\nnumpy.timedelta64(2522300000,'ns')\r\n>>> decimal.Decimal(2.5223)*10**9\r\nDecimal('2522299999.999999986499688021')\r\n```\r\n\r\nWhy is `2.5225` seconds truncated to `2522499999` nanoseconds, while `2.5223` seconds rounded up to `2522300000` nanoseconds? Behaviour here seems quite erratic.","The problem should be in line 223\u2013225 and 228 below: \r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/12d69c8b5739e7070b8ca2ff86587f828c9a96c0\/pandas\/_libs\/tslibs\/conversion.pyx#L214-L232\r\n\r\nIn fact\r\n```\r\n>>> 0.5225 * 1e9\r\n522499999.99999994\r\n>>> 0.5223 * 1e9\r\n522300000.0\r\n```\r\ndespite the fact that \r\n```\r\n>>> round(0.5225, 9) == 0.5225\r\nTrue\r\n```\r\n\r\nI would say that the bug is the naive assumption that\r\n```\r\ntrunc(round(x, p) * 10**p)\r\n```\r\ngive raise to the same results in decimal and binary floating point:\r\n```\r\n>>> math.trunc(round(0.5225, 6) * 10**6)\r\n522499\r\n>>> math.trunc(round(decimal.Decimal(0.5225), 6) * 10**6)\r\n522500\r\n```","After some more experimentation I arrived at some preliminary remarks.\r\n\r\n1. In `to_timedelta` there are two different possible paths:\r\n    1. truncation  if `p == 0`\r\n    1. rounding if `p > 0`\r\n2. The rounding artihmetic is flawed.\r\n\r\nSo actually we have here two separate issues.\r\n\r\nPoint 1. could be considered a desing choice: it is open to discussion if other more coherent coherent behaviours (always round, always truncate) are worth the risk of breaking current code.\r\n\r\nPoint 2. is clearly a bug, which is orthogonal to 1. However, it makes no sense to submit a PR for 2. alone, if also 1. has to be addressed.","Please see my POC implementation with correct rounding but preserving truncation in the ns \u2192 ns case:\r\n<https:\/\/github.com\/miccoli\/pandas\/tree\/GH%2356629> "],"labels":["Bug","Needs Triage"]},{"title":"ENH\/PERF: dispatch is_monotonic_increasing \/ decreasing ?","body":"Is it worth dispatching `is_monotonic_increasing` \/ `is_monotonic_decreasing` for EAs?\r\n\r\nThe cython implemention is early-stopping, but the benefit disappears if the data needs to be copied into an object array as in the example below:\r\n\r\n```\r\nimport pandas as pd\r\n\r\nvalues = [f\"val_{i:07}\" for i in range(1_000_000)]\r\nser = pd.Series(values, dtype=\"string[pyarrow_numpy]\")\r\n\r\n%timeit ser.is_monotonic_increasing\r\n# 219 ms \u00b1 20.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit pc.all(pc.greater_equal(ser.array._pa_array[1:], ser.array._pa_array[:-1]))\r\n# 19.2 ms \u00b1 585 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n\r\nrandom order example with cython early-stopping:\r\n\r\n```\r\nser2 = ser.sample(frac=1.0)\r\n\r\n%timeit ser2.is_monotonic_increasing\r\n# 152 ms \u00b1 4.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n%timeit pc.all(pc.greater_equal(ser2.array._pa_array[1:], ser2.array._pa_array[:-1]))\r\n# 15 ms \u00b1 300 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```","comments":["+1 for anything that's non-numeric\r\n\r\nHow does this look if we convert to numpy without copying? I think similar but want to be sure","+1.  e.g. pyarrow EAs would be able to cache it.  And ExtensionEngine has a hack going through _rank.","> pyarrow EAs would be able to cache it.\r\n\r\n@jbrockmendel - thats an interesting idea. I assume the same could be done for `is_unique`. I suspect the current mutability (e.g. setitem) would complicate the caching a bit?","yah calling `__setitem__` would have to invalidate the cache.  though i imagine that a lot of the relevant info is cached on the pa.ChunkedArray object"],"labels":["Performance","Needs Discussion","ExtensionArray"]},{"title":"ENH: Add option to use absolute value in pct_change","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n```\r\nimport pandas as pd\r\ntest = pd.DataFrame()\r\ntest['testdata'] = [-68, 5144,10000]\r\ntest['pct_changes'] = test['testdata'].pct_change()\r\nprint (test['pct_changes'])\r\n0          NaN\r\n**1   -76.647059**\r\n2     0.944012\r\nName: pct_changes, dtype: float64\r\n\r\n\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\ncorrect result for row 1 should be **76.647059 and not -76.647059.** It seems the current implementation of pct_change() lacks the abs() function and thus returns the wrong sign when the base is negative\r\n\r\n\r\n### Expected Behavior\r\n\r\npercentage_change = (new_value - old_value) \/ abs(old_value)\r\n(5144 - (-68)) \/ abs(-68) = 76.647059\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.6.final.0\r\npython-bits         : 64\r\nOS                  : Linux\r\nOS-release          : 5.15.133.1-microsoft-standard-WSL2\r\nVersion             : #1 SMP Thu Oct 5 21:02:42 UTC 2023\r\nmachine             : x86_64\r\nprocessor           : x86_64\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.24.3\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.3\r\nhtml5lib            : 1.1\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.17.2\r\npandas_datareader   : 0.10.0\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.9.1\r\ngcsfs               : None\r\nmatplotlib          : 3.8.0\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 13.0.0\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.2\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : 2.0.1\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report. Taking the absolute value is common but not universal. It also gives counter-intuitive results - e.g. going from -20 to 20 is a smaller percent change than going from -10 to 20 (200% vs 300%).\r\n\r\nIn my opinion, one should not think of \"percent change\" as a measure how much something is getting more positive, but rather how much it's moving away from zero. Positive percent change means it's moving more away from zero, negative percent change means it's moving toward (and possibly crossing) 0. With this interpretation, not taking absolute value is correct.\r\n\r\nSince it's fairly common to take the absolute value however, I wouldn't be opposed to adding an argument.","I've reworked this issue as an enhancement request.","can I work on this","cc @pandas-dev\/pandas-core - any thoughts on adding an argument here?","isn't there an absolute function that can be applied to the result to return exactly the intent without the need for an argument?\r\n\r\nI dont believe percent change should be absolute by default and it feels more pythonic, to me, to manipulate the outcome with existing functions than adding a keyword","@attack68 - we need to apply `abs` to the denominator alone, not the entire result. This can currently be accomplished with `.shift`, `.abs`, a division, and a multiplication, but it's a bit messy.","Adding an argument allows keeping the current behavior, which can also be non-intuitive:\r\n```python\r\n>>> import pandas as pd\r\n>>> s=pd.Series([-40, -30, -20])\r\n>>> s.pct_change()\r\n0         NaN\r\n1   -0.250000\r\n2   -0.333333\r\ndtype: float64\r\n```\r\nIt seems odd to report -0.25 when the value increased by 25%\r\n\r\nOne could also argue that computing percent change relative to a negative number is not well defined, and we should raise an exception.  \r\n\r\nSo maybe the extra argument should have 3 options:\r\n- \"Keep sign\"  (current behavior)\r\n- \"Ignore sign\"  (take the absolute value)\r\n- \"Raise exception\" (if any number is negative, raise an exception)\r\n\r\nAdvantage of the third option is that it might help identify an error in your dataset or data pipeline.\r\n\r\n","> It seems odd to report -0.25 when the value increased by 25%\r\n\r\nI don't think so. Ref: https:\/\/github.com\/pandas-dev\/pandas\/issues\/56618#issuecomment-1869598122","> > It seems odd to report -0.25 when the value increased by 25%\r\n> \r\n> I don't think so. Ref: [#56618 (comment)](https:\/\/github.com\/pandas-dev\/pandas\/issues\/56618#issuecomment-1869598122)\r\n\r\nIt probably depends on what you are measuring in the original series, and how you interpret the result.  Your example was flipping the sign between 2 adjacent entries.  My example is keeping the sign negative between two entries.  \r\n\r\nIt seems to me that a positive percent change means things increased, and a negative percent change means things decreased.  In my example, seeing a -0.25 when the value increased seems counter-intuitive.  And if the sign flipped, I'm not sure how to interpret the result (hence my suggestion to raise an exception)\r\n","> It probably depends on what you are measuring in the original series, and how you interpret the result. Your example was flipping the sign between 2 adjacent entries. My example is keeping the sign negative between two entries.\r\n\r\nI don't think this is significant. In your example, we are going from -40 to -30. We agree going from 40 to 30 or 40 to 50 is a -25% and 25% change respectively. My claim here is that you should interpret the negative sign not as \"getting more positive\" or \"getting more negative\", but rather, moving toward 0. In this interpretation, going from -40 to -30 being a -25% change is not counter-intuitive.\r\n\r\n> It seems to me that a positive percent change means things increased, and a negative percent change means things decreased\r\n\r\nIt does - as long as you consider the \"size of a number\" to be its absolute value. For example, -100000 is a large number when compared to 0.1.","I think there are already similar issues to this that have been closed \/ rejected in the past:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/issues\/40911\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/issues\/22596\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/pull\/44102\r\n\r\nI don't think there is a universal agreement on what these values would represent, nor do I think expanding API for this is worth it","Thanks @WillAyd. No strong opinion here - but from #22596 the closing comment has 14 down votes and 4 confused emojis. This is also the fourth time this request has come up. I do think that user feedback is something to consider.","> @attack68 - we need to apply `abs` to the denominator alone, not the entire result. This can currently be accomplished with `.shift`, `.abs`, a division, and a multiplication, but it's a bit messy.\r\n\r\nGood point. No opinion either way then.","Do you have a suggestion on what the new argument should be?\r\n\r\nPersonally, between\r\n- `s.pct_change(denominator='ignore sign')`\r\n- `s.diff()\/s.shift().abs()`\r\n\r\nI think I'd find the second one clearer","> Do you have a suggestion on what the new argument should be?\r\n> \r\n> Personally, between\r\n> \r\n> * `s.pct_change(denominator='ignore sign')`\r\n> * `s.diff()\/s.shift().abs()`\r\n> \r\n> I think I'd find the second one clearer\r\n\r\nAgree I would do the second by default and probably not even assume that a `pct_change` function exists. To avoid op risk I would probably also avoid it due the ambiguity highligted in this thread and the definite nature of the more fundamental ops.","I think the fundamental issue here is that pct change doesn't make much sense when the sign of the a series is not consistent (and in almost all applications, strictly positive).  I can see some real downsides of letting errors in code or data go unnoticed if `abs` was applied by the denominator by default.  \r\n\r\nI agree that the existence of `s.diff()\/s.shift().abs()` as a simple one-liner alternative should be sufficient for users who want a modified `pct_change` behavior. ","I read through the discussions in the 3 links that @WillAyd provided.\r\n\r\nWhile `s.diff()\/s.shift().abs()` is the workaround, I don't think we can assume that users would be aware of the ambiguities that would occur  by using `pct_change()` as currently implemented on a `Series` that has inconsistent signs.\r\n\r\nI'd like us to consider either removing `pct_change()` from the API (which then forces people to implement it differently) or changing the current behavior to produce a warning (or raise an exception) if the `Series` has numbers with mixed signs.  Right now, without such a warning, people would never know to use the workaround.\r\n","could just document `s.diff()\/s.shift().abs()` in the `pct_change` docs, instead of removing `pct_change`?","> I'd like us to consider either removing `pct_change()` from the API (which then forces people to implement it differently) \r\n\r\nI also thought removing it might be the way to go - trim down the library with a structured deprecation notice advising how best to replicate the behaviour. \r\n\r\n","> could just document `s.diff()\/s.shift().abs()` in the `pct_change` docs, instead of removing `pct_change`?\r\n\r\nI don't think that helps the users discover when they should use the workaround, so that's why I also suggested raising a warning or exception if there are mixed signs in the series.\r\n","> I don't think that helps the users discover when they should use the workaround, so that's why I also suggested raising a warning or exception if there are mixed signs in the series.\r\n\r\nThe current behavior of `pct_change` with mixed signs is a mathematically correct and interpretable result. I'm -1 on raising a warning in such a case.","> The current behavior of `pct_change` with mixed signs is a mathematically correct and interpretable result. I'm -1 on raising a warning in such a case.\r\n\r\nYes, but from the multiple people that have raised issues indicating they were confused with the result, that indicates the values are subject to interpretation, and there is even a debate on what is \"mathematically correct\".   IMHO, if you have mixed signs in your series, it is a possible indication that you have an error in your data, and that the results may come out in an unexpected way.\r\n\r\nWe could also add an argument such as `check_for_mixed_signs: bool = False` as the default, and if the value is `True`, then we check for mixed signs and raise a warning.  Or do as suggested in the issue, which is to create an argument that lets users choose the behavior.  At least by creating an argument, we are making people more aware of the issue should they encounter it."],"labels":["Enhancement","Numeric Operations","Needs Discussion"]},{"title":"BUG: incompatible dtype when creating bool column with df.at","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame()\r\ncondition = [True, False, True, True]\r\n\r\nfor i in range(len(condition)):\r\n    df.at[i,'truth'] = condition[i]\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhile making a column from a loop, I get an incompaitible dtype warning.\r\n\r\n> FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'True' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\r\n>   df.at[i,'truth'] = condition[i]\r\n\r\nPandas most probably complaints about casting to `float64` because `np.NaN` is considered a float and pandas defaults to making a column filled with `np.NaN` \r\n\r\n### Expected Behavior\r\n\r\nNo warning\r\n\r\n### Installed Versions\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.6.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : AMD64 Family 23 Model 24 Stepping 1, AuthenticAMD\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_India.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.0\r\npip                 : 23.2.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.18.1\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : None\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n\r\n<\/details>\r\n","comments":[],"labels":["Bug","Needs Triage"]},{"title":"DEPR: List of deprecations to be removed in 4.0","body":"# Deprecations for 4.0\r\n-  #56680\n- [ ] #57375\n- [ ] #57372\n- [ ] #57473\n- [ ] #57275\n- [ ] #57347\n- [ ] #57699\n- [ ] #57627","comments":["If we label enforcements of deprecations as deprecations, they will be added here. Maybe we should add a new label \"Enforcement\"?"],"labels":["Deprecate"]},{"title":"BUG: read_html has unexpected behavior parsing th & td with colspan attribute.","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [x] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\nhtmlTable = \"\"\"\r\n<table>\r\n  <thead>\r\n    <tr>\r\n     <th colspan=\"3\">Header 1<\/th>\r\n     <th colspan=\"3\">Header 2<\/th>\r\n     <th colspan=\"3\">Header 3<\/th>\r\n    <\/tr>\r\n  <\/thead>\r\n  <tbody>\r\n   <tr>\r\n    <td>1<\/td>\r\n    <td>2<\/td>\r\n    <td>3<\/td>\r\n    <td>4<\/td>\r\n    <td>5<\/td>\r\n    <td>6<\/td>\r\n    <td>7<\/td>\r\n    <td>8<\/td>\r\n    <td>9<\/td>\r\n   <\/tr>\r\n  <\/tbody>\r\n<\/table>\r\n\"\"\"\r\ndf = pd.read_html(StringIO(htmlTable), index_col=0, keep_default_na=False)[0]\r\nprint(df)\n```\n\n\n### Issue Description\n\nHere is the output:\r\n\r\n```\r\nHeader 1   Header 1.1   Header 1.2  Header 2  Header 2.1  Header 2.2  Header 3  Header 3.1  Header 3.2\r\n1          2            3           4         5           6           7         8           9\r\n```\n\n### Expected Behavior\n\n```\r\nHeader 1                            Header 2                          Header 3\r\n1          2            3           4         5           6           7         8           9\r\n```\r\n\r\nNote:\r\n`In the real scenario I get the duplicated header names as 'Unnamed: 1,2,3'`.\r\n\r\nExample:\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/18900523\/4eb736da-e65b-4a7e-8c00-78f9e22e2976)\r\n\n\n### Installed Versions\n\n```\r\npython: 3.11.6.final.0\r\npip: 23.2.1\r\npandas : 2.1.1 or 2.1.4\r\nnumpy: 1.26.0\r\nxlsxwriter: 3.1.7\r\nlxml.etree : 4.9.3\r\nhtml5lib: 1.1\r\nbs4: 4.12.2\r\n```\r\n","comments":["Please can you assign this issue to me","> Please can you assign this issue to me\r\n\r\n@naman8989 This option is not visible to me.","It looks like in your expected output, you are expecting to have multiple columns with the label of `None`, is that right? Working with DataFrames with duplicate column labels can be very difficult, I don't think this should be the behavior of read_html.\r\n\r\n> In the real scenario I get the duplicated header names as 'Unnamed: 1,2,3'\r\n\r\nI'm guessing you haven't been able to find a reproducible example that does this, is that right?","My question is, Why `pandas.read_html` then export the parsed `DataFrame` into excel, doesn't respect the `colspan` attributes.\r\n\r\nCan you confirm that, The reproducible example will be parsed then exported to excel & give the same result as if it was rendered as HTML table and `colspan`ed cells will be merged?\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/18900523\/87e4a6c9-375e-4322-b935-cbf39df57c16)\r\n","> Why `pandas.read_html` then export the parsed `DataFrame` into excel, doesn't respect the `colspan` attributes.\r\n\r\nThere are two separate operations - not all users will take the result of `read_html` and put it into Excel. We can't modify the behavior of `read_html` in this way without hurting other operations pandas users might do with the results.\r\n\r\n> Can you confirm that, The reproducible example will be parsed then exported to excel & give the same result as if it was rendered as HTML table and colspaned cells will be merged?\r\n\r\nNo - that does not give the same result. But I also don't think it should."],"labels":["Bug","IO HTML","Needs Triage","Closing Candidate"]},{"title":"QST: Roadmap for deprecations of Period types","body":"### Research\n\n- [X] I have searched the [[pandas] tag](https:\/\/stackoverflow.com\/questions\/tagged\/pandas) on StackOverflow for similar questions.\n\n- [X] I have asked my usage related question on [StackOverflow](https:\/\/stackoverflow.com).\n\n\n### Link to question on StackOverflow\n\nNA\n\n### Question about pandas\n\n**Background**\r\n\r\nThere have been several issues raised related to the `Period` types, such as:\r\n\r\n- https:\/\/github.com\/pandas-dev\/pandas\/issues\/54235\r\n- https:\/\/github.com\/pandas-dev\/pandas\/issues\/53446\r\n\r\nAnd the latter deprecation of the business-day period has already been implemented.\r\n\r\nThis is of course related to the enhancement allowing non-ns units to datetimes, see e.g.:\r\n\r\n- https:\/\/github.com\/pandas-dev\/pandas\/issues\/46587\r\n- https:\/\/numpy.org\/doc\/stable\/reference\/arrays.datetime.html#datetime-units\r\n\r\n**Questions**\r\n\r\n1. Given the fact that deprecations of `Period` types have already begun, it would be useful to understand what the expected roadmap for `Period` is.  Is it expected that it will be removed as suggested in #54235?  Is there a plan for how to move forward on the \"sticking points\" listed there (especially the missing units: Week-with anchor, quarter, Year-with-anchor)?\r\n\r\n2. The current deprecation of business day periods in #53446 now requires bifurcation of code from `Period` to `Timestamp` in the special case of business days - if you are using `Period`s, you can't wholesale switch to `Timestamp`s yet (at least if you have e.g. Quarters), but you also can no longer stick with just `Period`s.\r\n\r\n**Tentative request**\r\n\r\nTo me, it would make sense to revert the deprecation of the BDay `Period` dtype until there is a more comprehensive roadmap for `Period` types and a path to make a more wholesale switch.  But my apologies if I missed something fundamental here about why that deprecation is important in itself.\r\n","comments":["cc @jbrockmendel ","I don't think there is a clear roadmap for deprecating Period entirely; not much interest in #54235.  We did recently deprecate support for PeriodIndex in resample.\r\n\r\n@ChadFulton can you elaborate a bit on how the deprecation of Period[B] is a pain point?","Thanks @jbrockmendel.  Currently I use `Period` objects almost exclusively, since for the economic data that I mostly work with, it doesn't make sense to have a time component.  And I have to say that Pandas does a really excellent job of handling `Period`s, so a big thanks for all the work that goes into that.\r\n\r\nSo one practical example of how I use the information in `Period`: when plotting a line of data associated with a period, there are different conventions for where to place the mark, with the most common being start, end, and center.  A toy example of this would be something like:\r\n\r\n```python\r\nix = pd.period_range(start='2000-01-01', end='2000-01-31', freq='B')\r\ndta = pd.Series(np.arange(len(ix)), index=ix)\r\n\r\ncenter = ix.start_time + (ix.end_time - ix.start_time) \/ 2\r\nplt.plot(center, dta)\r\n```\r\n\r\nOverall, my hope is that we don't lose Periods, because I think that they represent a distinct and useful concept that is not captured by non-ns units for datetime.  But I understand that it's a maintenance burden.\r\n\r\nSo more specifically here, my general feeling was just that because `Period` is not the same as \"`Timestamp` + non-ns unit\" (with one simple example being `start_time` and `end_time` attributes), it didn't necessarily make sense to me to remove the `freq='B'` `Period`s (at least in isolation, but also, I hope, at all \ud83d\ude0a).","Thanks for fleshing this out.  The plotting example is useful (in fact ATM the dt64 plotting code currently converts to period internally, which we need to change before the Period[B] deprecation can be enforced.  That is a non-trivial re-write that isn't likely to happen without funding, so may not happen for 3.0).\r\n\r\n> it didn't necessarily make sense to me to remove the freq='B' Periods (at least in isolation, but also, I hope, at all \ud83d\ude0a)\r\n\r\n\"B\" is an outlier in the Period code; getting rid of it will allow non-trivial (though not-huge) code simplifications.  More importantly from my perspective, getting rid of it is a blocker to moving Period from using `freq` to `unit`, which I'm hopeful will put a stop to a long history of user confusion (see issues listed in #53446).  So deprecating \"B\" is not necessarily a prelude to removing all Periods.\r\n\r\nThe immediate question is whether to revert that deprecation.  Plotting is one use case that is inconvenienced here.  Are there others?\r\n\r\n(The rest here is not super-relevant, just things that came to mind while reading your comment)\r\n\r\n> because I think that they represent a distinct and useful concept that is not captured by non-ns units for datetime\r\n\r\nThis wording reminds me of years-old disagreement I used to have with jreback.  My position was that for units where we have both, `Timestamp[unit]` is isomorphic to `Period[unit]`.  IIRC the context for that disagreement was a request for a `date` dtype where I thought `Period[D]` handled the use case just fine.  (Not sure if this paragraph is relevant to this discussion, but it came to mind)\r\n\r\n> with one simple example being start_time and end_time attributes\r\n\r\nWith the introduction of non-nano, there is now a question of what datetime64\/Timestamp unit to return for Period properties.  Keeping it as nanos is fine in most cases, but ATM that will raise for very-large Periods, which can be alleviated by returning a lower-resolution unit.  This is a rare enough corner case that I haven't been interested in bothering with it.","Thanks again for your thoughts on this. Following up with a second specific use case that may help the conversation:\r\n\r\n> The immediate question is whether to revert that deprecation. Plotting is one use case that is inconvenienced here. Are there others?\r\n\r\nAnother use case that shows up as a problem is that `Timestamp`  elements do not allow for a `freq` attribute and `'B'` is not a valid unit. So there appears to me to be currently (i.e. after the deprecation) no way to identify a single object that is a `'B'` frequency.\r\n\r\nThis leads to several secondary problems related to calling `reset_index()` or `to_series()` on a `DatetimeIndex` and losing the frequency:\r\n\r\n```python\r\nindex = pd.date_range('2000', periods=10, freq='B', name='B_index')\r\nx = pd.DataFrame(0, columns=['a', 'b'], index=index)\r\nprint(x.index.freq)\r\ny = x.reset_index().set_index('B_index')\r\nprint(y.index.freq)\r\n```\r\n\r\nyields\r\n\r\n```python\r\n<BusinessDay>\r\nNone\r\n```\r\n\r\n\r\nwhile when using Periods:\r\n\r\n```python\r\nindex = pd.period_range('2000', periods=10, freq='B', name='B_index')\r\nx = pd.DataFrame(0, columns=['a', 'b'], index=index)\r\nprint(x.index.freq)\r\ny = x.reset_index().set_index('B_index')\r\nprint(y.index.freq)\r\n```\r\n\r\nyields\r\n\r\n```python\r\n<BusinessDay>\r\n<BusinessDay>\r\n```\r\n\r\nOf course this same behavior would happen with any frequency, not just `'B'`, but my point is just that with any other frequency I could simply use the `PeriodIndex` to ensure the frequency information is not lost.  With this deprecation, that is no longer possible with `'B'` frequencies.","Also, related to the eventual `freq` -> `unit` conversion, I would argue that until this is complete, deprecations like https:\/\/github.com\/pandas-dev\/pandas\/issues\/9586 are also perhaps not ideal since they make the behavior of the `freq` attribute inconsistent:\r\n\r\nFor example:\r\n\r\n```python\r\nimport pandas as pd\r\nix = pd.date_range(start='2000', periods=10, freq='YE')\r\nix.to_period()\r\n# ^ works and gives Y-DEC\r\n\r\npd.period_range(start='2000', periods=10, freq=ix.freq)\r\n# ^ works and gives Y-DEC\r\n\r\npd.period_range(start='2000', periods=10, freq=ix.freqstr) \r\n# ^ raises ValueError: Invalid frequency: YE-DEC, failed to parse with error\r\n#          message: ValueError(\"for Period, please use 'Y-DEC' instead of 'YE-DEC'\")\r\n\r\n```"],"labels":["Usage Question","Period","Needs Info"]},{"title":"BUG: dt64[non_nano] + some_offsets incorrectly rounding","body":"```python\r\nimport pandas as pd\r\n\r\ndti  = pd.date_range(\"2016-01-01\", periods=3, unit=\"s\")\r\n\r\ndti + pd.offsets.CustomBusinessDay(offset=pd.Timedelta(1))\r\ndti + pd.offsets.BusinessHour(offset=pd.Timedelta(1))\r\ndti + pd.offsets.CustomBusinessHour(offset=pd.Timedelta(1))\r\ndti + pd.offsets.CustomBusinessMonthBegin(offset=pd.Timedelta(1))\r\ndti + pd.offsets.CustomBusinessMonthEnd(offset=pd.Timedelta(1))\r\n```\r\n\r\nEach of the additions above incorrectly round the result when calling `as_unit(self.unit)` [here](https:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/core\/arrays\/datetimes.py#L819).\r\n\r\nThe best solution would be to implement _apply_array for all of these offsets and make the pointwise path in _add_offset unnecessary.\r\n\r\nNext best would be something like\r\n\r\n```\r\nres_unit = self.unit\r\nif hasattr(offset, \"offset\"):\r\n    unit = Timedelta(offset.offset).unit\r\n    res_unit = max(self.unit, unit)  # <- not actually \"max\"; we probably have a helper for this\r\ndtype = tz_to_dtype(self.tz, unit=res_unit)\r\nresult = type(self)._from_sequence(res_values, dtype=dtype)\r\n``` \r\n","comments":[],"labels":["Bug","Numeric Operations","Frequency","Non-Nano"]},{"title":"BUG: Inconsistent behavior while constructing a Series with large integers in a int64 masked array","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import numpy.ma as ma\r\n\r\nIn [3]: mx = ma.masked_array([4873214862074861312, 4875446630161458944, 4824652147895424384, 0, 3526420114272476800], mask=[0, 0, 0, 1, 0])\r\n\r\nIn [4]: pd.Series(mx, dtype='Int64')\r\nOut[4]: \r\n0    4873214862074861568\r\n1    4875446630161459200\r\n2    4824652147895424000\r\n3                   <NA>\r\n4    3526420114272476672\r\ndtype: Int64\r\n\r\nIn [5]: mx.data - pd.Series(mx, dtype='Int64')\r\nOut[5]: \r\n0    -256\r\n1    -256\r\n2     384\r\n3    <NA>\r\n4     128\r\ndtype: Int64\n```\n\n\n### Issue Description\n\nWhile creating a series object from a masked array of large-ish integers (less than max of `Int64`), the output doesn't match the input. This has probably something to do with float downcast\/upcast somewhere. \r\n\r\nProbably related (?) https:\/\/github.com\/pandas-dev\/pandas\/issues\/30268, https:\/\/github.com\/pandas-dev\/pandas\/pull\/50757\n\n### Expected Behavior\n\n`mx.data - pd.Series(mx, dtype='Int64')` should be all zero with `<NA>` for the mask.\n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.6.final.0\r\npython-bits         : 64\r\nOS                  : Darwin\r\nOS-release          : 23.2.0\r\nVersion             : Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu-10002.61.3~2\/RELEASE_ARM64_T6000\r\nmachine             : arm64\r\nprocessor           : arm\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : en_US.UTF-8\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.1\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 68.2.2\r\npip                 : 23.3.1\r\nCython              : 3.0.5\r\npytest              : 7.4.3\r\nhypothesis          : 6.88.3\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : 1.1\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.17.2\r\npandas_datareader   : None\r\nbs4                 : 4.12.2\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : 2023.12.2\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, confirmed on main. Further investigation and PRs to fix are welcome!","take"],"labels":["Bug","Dtype Conversions","Constructors","NA - MaskedArrays"]},{"title":"PERF: pd.merge(on=index) is faster than pd.merge(left_index=True, right_index=True) if index is duplicated","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this issue exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this issue exists on the main branch of pandas.\r\n\r\n\r\n### Reproducible Example\r\n```\r\nimport pandas as pd  # 2.1.4 or 2.2.0.dev0+925.gac170fdc35 (current master)\r\n\r\ndf1 = pd.DataFrame({\r\n    \"key\": [x \/\/ 100 for x in range(1_000_000)],\r\n    \"val1\": 10\r\n}).set_index(\"key\")\r\n\r\ndf2 = pd.DataFrame({\r\n    \"key\": [x \/\/ 100 for x in range(200_000, 800_000)],\r\n    \"val2\": 20\r\n}).set_index(\"key\")\r\n\r\n%timeit -r7 -n3 pd.merge(df1, df2, left_index=True, right_index=True, how=\"inner\")\r\n# 2.1.4: 1.57 s \u00b1 247 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n# main: 2.1 s \u00b1 366 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n\r\n%timeit -r7 -n3 pd.merge(df1, df2, on=[\"key\"], how=\"inner\")\r\n# 2.1.4: 1.25 s \u00b1 14.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n# main: 2.06 s \u00b1 19.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n```\r\n### Installed Versions\r\n\r\n<details>\r\nExceptions: \r\n\r\n```\r\nModuleNotFoundError: No module named 'PySide6'\r\nQtBindingsNotFoundError: No Qt bindings could be found\r\n```\r\n<\/details>\r\n\r\n\r\n### Prior Performance\r\n\r\nBoth merges give the same output (verified using `pd.testing.assert_frame_equal`).\r\n\r\n`pd.merge(..., on=index)`\r\n* 2.1.4: 20% faster, more stable (14.3 ms vs 247 ms std. dev.)\r\n* master: average time is almost same, but there's massive regression: it's almost 65% slower than 2.1.4. \r\n\r\nThis behaviour is observed only when index has duplicated values in both dataframes. The expected behavior is at least the same execution time regardless of the provided arguments.\r\n\r\nMaster regression may be caused by [#56523](https:\/\/github.com\/pandas-dev\/pandas\/pull\/56523) (@lukemanley @phofl FYI). ","comments":["cc @lukemanley any idea where this is coming from?","I took a quick look to see if #56523 is the cause and it doesn't seem to be. \r\n\r\n@starhel - would you mind posting the timings for your example with `sort=True` passed? The reason I ask is that \r\nprior to 2.2 many:many joins were always sorting which was a bug fixed in #54611. I realize the keys in your example are already sorted, but it still goes down a different path with `sort=True`. Given the current implementation, I believe that passing `sort=True` will improve performance in the case of many:many joins and might explain at least some of the 2.1.4 -> main difference you are seeing.","It's quite counter-intuitive that returning sorted data is faster than preserving the input order, but that's what the test results show.\r\n```python\r\n%timeit -r7 -n3 pd.merge(df1, df2, left_index=True, right_index=True, how=\"inner\")\r\n# 1.93 s \u00b1 23.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n%timeit -r7 -n3 pd.merge(df1, df2, on=[\"key\"], how=\"inner\")\r\n# 2.03 s \u00b1 31.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n%timeit -r7 -n3 pd.merge(df1, df2, left_index=True, right_index=True, how=\"inner\", sort=True)\r\n# 1.2 s \u00b1 3.19 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n%timeit -r7 -n3 pd.merge(df1, df2, on=[\"key\"], how=\"inner\", sort=True)\r\n# 1.31 s \u00b1 18.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 3 loops each)\r\n```\r\n\r\nFrom the test, it also appears that perf issue is fixed on the main branch; merging by index is faster than merging by column. In my opinion, it's worth adding to the documentation that sort=True may be faster (according to my test, only for both duplicated indexes) than sort=False.\r\n"],"labels":["Performance","Reshaping"]},{"title":"QST: Questions about HDF5 support of pandas","body":"### Research\n\n- [X] I have searched the [[pandas] tag](https:\/\/stackoverflow.com\/questions\/tagged\/pandas) on StackOverflow for similar questions.\n\n- [X] I have asked my usage related question on [StackOverflow](https:\/\/stackoverflow.com).\n\n\n### Link to question on StackOverflow\n\nhttps:\/\/stackoverflow.com\/questions\/77677944\/how-to-read-hdf5-file-with-pandas\n\n### Question about pandas\n\nI have some some data in HDF5 format, which can be loaded with `h5py` or `PyTables` without problem.\r\nBut I can't manage to read it with `pd.read_hdf` or `pd.HDFStore`.\r\nSome examples data file can be found in [here](https:\/\/gwosc.org\/archive\/links\/O3GK_4KHZ_R1\/G1\/1270281618\/1271462418\/simple\/).\r\n\r\nI saw [this post](https:\/\/stackoverflow.com\/questions\/33641246\/pandas-cant-read-hdf5-file-created-with-h5py\/33644128#33644128) claimed that \"pandas interaction with HDF files is limited to specific structures that pandas understands\".  But that is about 8 years ago,and I can't find any information on the official documents.\r\n\r\nSo my questions:\r\n- Is it still true that pandas only support HDF5 file with specific structures?\r\n- If not, how can I load my file?\r\n- If yes, please add warning in the official documents!\r\n","comments":[],"labels":["Usage Question","Needs Triage"]},{"title":"TST: assert_produces_warning multiple warnings\/messages","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nNA\n```\n\n\n### Issue Description\n\nSometimes when deprecating something I will find a test like:\r\n\r\n```\r\ndef test_foo():\r\n    with tm.assert_produces_warning(UnrelatedWarning, match=unrelated_msg):\r\n        thing_that_i_am_deprecating()\r\n```\r\n\r\nWhen this happens, the only viable option I'm aware of is to change `UnrelatedWarning` to `(UnrelatedWarning, FutureWarning)`.  But this doesn't a) check that we get _both_ or b) check that we get a reasonable message for the FutureWarning.\r\n\r\nThe request here is for a way to specify a list of (warning, msg) pairs and check that we get all of them and no others.  _Or_ to make `@pytest.mark.filterwarnings(\"ignore:my_message:FutureWarning\")` actually work in this context.\n\n### Expected Behavior\n\nNA\n\n### Installed Versions\n\n<details>\r\n\r\nReplace this line with the output of pd.show_versions()\r\n\r\n<\/details>\r\n","comments":["take"],"labels":["Enhancement","Testing"]},{"title":"ENH: Allow to set axis name in pd.concat","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\n`pd.concat()` has the parameter `names` which works fine when using the `keys` parameter together with the `names` parameter. For example:\r\n\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame([[1, 2, 3],\r\n                   [10, 10, 10],\r\n                   ], columns=[\"A\", \"B\", \"C\"]\r\n                  ).rename_axis(\"class\")\r\n\r\npd.concat([df, df.agg([\"sum\"])],\r\n            keys=[\"a_key\", \"b_key\"],\r\n            names=[\"foo\", \"class\"]\r\n            )\r\n```\r\nOutput:\r\n```\r\n              A   B   C\r\nfoo   class            \r\na_key 0       1   2   3\r\n      1      10  10  10\r\nb_key sum    11  12  13\r\n```\r\nThe name of index level 1 is now `\"class\"` (same as it was before).\r\nHowever, using the `names` parameter alone doesn't change anything, no error appears and no changes are made to the name of the index. This is a bit in accordance to the docs, which say to the `names` parameter:\r\n\r\n> Names for the levels in the resulting hierarchical index.\r\n\r\n### Feature Description\r\n\r\nI find it more satisfying if `names` could be used in a single index too, just to set the name of the resulting single index (i.e. rename the index axis).\r\n\r\n```\r\npd.concat([df, df.agg([\"sum\"])], names=[\"class\"])\r\n```\r\nExpected output:\r\n```\r\nclass  A   B   C\r\n0       1   2   3\r\n1      10  10  10\r\nsum    11  12  13\r\n```\r\nMy suggestion is that using the `names` parameter without the `keys` parameter would set the name of the resulting axis for the single index (if a single index is returned). On a side note, I don't know what is usally used as argument in such a case `names=[\"class\"]` or `names=\"class\"`, I think you know more than me.\r\n\r\n### Alternative Solutions\r\n\r\nThe following example works right now but only because the name of the index in the first object of `pd.concat([df, df.agg([\"sum\"])` is already `\"class\"`:\r\n```pd.concat([df, df.agg([\"sum\"]).rename_axis(\"class\")])```\r\n\r\nOtherwise one would need to use:\r\n```pd.concat([df, df.agg([\"sum\"])]).rename_axis(\"class\")```\r\n\r\nThe proposed solution seems to be more readable:\r\n```pd.concat([df, df.agg([\"sum\"])], names=[\"class\"])```\r\n\r\n### Additional Context\r\n\r\nIf this would be implemented, the docs for the parameter `names` would need to be adapted, e.g. to something like:\r\nNames for the levels in the resulting single index or Multiindex.","comments":["Thanks for the request, this seems reasonable to me. At least I don't see a reason why it'd be beneficial to ignore the names argument in the case of a non-MultiIndex."],"labels":["Enhancement","Reshaping","Needs Discussion"]},{"title":"BUG: `read_sql()` does not handle `date` nor `time` datatypes correctly when using pyarrow (but works for `datetime`)","body":"### Pandas version checks\r\n\r\n- [X] I have checked that this issue has not already been reported.\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\r\n\r\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\r\n\r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport sqlalchemy as sa\r\nimport pandas as pd\r\n\r\ne = sa.create_engine(db_uri)    # Any Microsoft SQL Server uri\r\nwith e.connect() as conn:\r\n    # SQL Server query -- If using another RDBMS, please adjust.\r\n    q = \"SELECT CAST(CURRENT_TIMESTAMP as date) D, CAST(CURRENT_TIMESTAMP as time) T, CURRENT_TIMESTAMP DT\"\r\n\r\n    # Execute query with SQLAlchemy\r\n    r = conn.exec_driver_sql(q)\r\n\r\n    # Result metadata has the correct data types for each column\r\n    print(r.cursor.description)\r\n    # (('D', <class 'datetime.date'>, None, 10, 10, 0, True), ('T', <class 'datetime.time'>, None, 16, 16, 7, True), ('DT', <class 'datetime.datetime'>, None, 23, 23, 3, False))\r\n\r\n    # Execute query with read_sql()\r\n    df = pd.read_sql(sql=q, con=db_uri, dtype_backend=\"pyarrow\")\r\n\r\n    # Dataframe metadata has the correct data type only for the last column (datetime).\r\n    print(df.dtypes)\r\n    # D            string[pyarrow]\r\n    # T            string[pyarrow]\r\n    # DT    timestamp[ns][pyarrow]\r\n```\r\n\r\n\r\n### Issue Description\r\n\r\nWhen used with `pyarrow` backend Pandas `read_sql()` does not handle `date` nor `time` datatypes correctly.\r\n\r\nBut it works for `datetime` datatypes.\r\n\r\nThe above example uses Microsoft SQL Server but can be reproduced in any other database.\r\n\r\n### Expected Behavior\r\n\r\n```python \r\nprint(df.dtypes)\r\n# D        date64[ns][pyarrow]\r\n# T        time64[ns][pyarrow]\r\n# DT    timestamp[ns][pyarrow]\r\n```\r\n\r\n### Installed Versions\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.7.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.20348\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : en_US.UTF-8\r\nLOCALE              : pt_BR.cp1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 69.0.2\r\npip                 : 23.3.1\r\nCython              : None\r\npytest              : None\r\nhypothesis          : None\r\nsphinx              : None\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : None\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : None\r\nIPython             : None\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : 1.3.7\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : None\r\nnumba               : 0.58.1\r\nnumexpr             : 2.8.8\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : None\r\nsqlalchemy          : 2.0.23\r\ntables              : None\r\ntabulate            : None\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None","comments":["can I work on this"],"labels":["Bug","IO SQL","Needs Triage","Arrow"]},{"title":"DEPR: Deprecate xlrd and pyxlsb excel engines","body":"We added calamine support for 2.2, so we can theoretically deprecate these 2 engines in favour of calamine to reduce our dependencies and just code that isn't needed anymore","comments":["+1\r\n\r\nIn general I'm okay with supporting any \"good enough\" engine even if it means we have multiple engines for certain formats, but neither of these fit that criteria in my opinion. I would wait a bit for users to give feedback on calamine though. Also might think about deprecating the default to become calamine first, then deprecating these altogether?","I like the idea of making calamine the default first and then deprecating (similar transition had taken place when the default engine was changed for xlsx files)","For my edification, is calamine better in some way than the others?  IIRC xlrd is not maintained; is that accurate?","> xlrd is not maintained; is that accurate?\r\n\r\nYes - and for pyxlsb, it doesn't offer some features that I would consider basic, like [reading datetimes](https:\/\/github.com\/pandas-dev\/pandas\/blob\/ac170fdc3570b55bfcaf44b8587825bcb274bc60\/pandas\/tests\/io\/excel\/test_readers.py#L146). "],"labels":["IO Excel","Deprecate","Needs Discussion"]},{"title":"DEPR: object inference in to_stata","body":"- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\r\n- [ ] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [ ] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\nThe \"do X instead\" won't actually work until 3.0, so this has to wait until then (and can't write the whatsnew until the 3.0 file exists). Putting this here so I don't forget about it.","comments":["This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Deprecate","IO Stata","Stale"]},{"title":"ENH: Add limit_area to groupby.ffill\/bfill","body":"#56531 is adding these arguments to Series\/DataFrame ffill\/bfill due to the deprecation in interpolate. While groupby doesn't have interpolate and so there is no loss of functionality, if an efficient algorithm can implement limit_area I think we should do it for API consistency.","comments":[],"labels":["Enhancement","Groupby","Missing-data","Needs Discussion","API - Consistency"]},{"title":"ENH: Add limit_area to Resample.ffill and Resample.bfill","body":"#56531 is adding these arguments to Series\/DataFrame ffill\/bfill due to the deprecation in interpolate. Resample also has ffill\/bfill deprecated in interpolate and we should add arguments these arguments as well.","comments":[],"labels":["Enhancement","Missing-data","Resample"]},{"title":"DEPR: SparseDtype","body":"Discussed briefly at the sprint.  Sparse is not much used, has a bunch of xfailed tests, and adds about 1MB of .so files.\r\n\r\nThe suggestion here is to deprecate Sparse internally and help [TBD] to implement a 3rd party EA.\r\n\r\nIIRC the main user of Sparse is one of the scikits (or pydata\/something?) and their actual use case would be better handled with a 2D implementation.  Anyone know the appropriate person to ping to get a view on this?\r\n\r\nPeople who use Sparse, please comment so we know you exist!\r\n","comments":["I use it! I find the functionality quite essential for working with high-cardinality categorical features, where it can reduce memory usage 100x or so. I do find the documentation a bit short and some behavior suprising, for example in this question https:\/\/stackoverflow.com\/questions\/77931609\/why-does-pandas-sum-give-wrong-answers-for-sparse-dataframe\/77932016#77932016. "],"labels":["Sparse","Deprecate","Needs Discussion"]},{"title":"CLN: remove unused entries from _lite_rule_alias","body":"xref #56346\r\n\r\nremove unused entries `\"min\", \"ms\", \"us\", \"ns\"` from _lite_rule_alias","comments":["This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Frequency","Stale"]},{"title":"VOTE: Voting issue for PDEP-8: Inplace methods in pandas","body":"Issue to track the votes for PDEP-8: Inplace methods in pandas\r\n\r\nPull request with the PDEP discussion:  https:\/\/github.com\/pandas-dev\/pandas\/pull\/51466\r\n\r\nRendered PDEP for easy reading: https:\/\/github.com\/pandas-dev\/pandas\/blob\/2654fe9ba91dd44ed66f0c246d4ed0c5dde7e391\/web\/pandas\/pdeps\/0008-inplace-methods-in-pandas.md (or a website preview at https:\/\/jorisvandenbossche.github.io\/pandas-website-preview\/pdeps\/0008-inplace-methods-in-pandas.html)\r\n\r\nCast your vote in a comment below.\r\n * +1: approve.\r\n* 0: abstain.\r\n   * Reason: A one sentence reason is required.\r\n* -1: disapprove\r\n   * Reason: A one sentence reason is required.\r\n       - A disapprove vote requires prior participation in the linked issues\/PRs.\r\n\r\nVoting will close in 15 days, i.e., on December 29, 2023.\r\n\r\n@pandas-dev\/pandas-core \r\n","comments":["And starting with my own +1 (so I am not forgotten in the count ;))","+1 :)","+1","+1","+1","group 2 inplace returning self\nwhen did this change? \n\ni was ok with the proposal of leaving inplace in group2 (even though we should just kill it entirely ) until i saw this","> group 2 inplace returning self, when did this change?\r\n\r\nIt was mentioned as an \"open question\" from the beginning (with one of the options to return self), but updated to just propose that option of returning self more recently. I summarized the changes I pushed to the proposal on the PR: https:\/\/github.com\/pandas-dev\/pandas\/pull\/51466#issuecomment-1782982457 mentioning the open questions and that I was planning to update the text about returning self, and then https:\/\/github.com\/pandas-dev\/pandas\/pull\/51466#issuecomment-1808158093 stating that I had updated the text that way. Further, it was also mentioned in the summary of the PDEP I sent to the pandas-dev mailing a month ago.\r\n\r\nLet's leave any further discussion on the PR itself -> https:\/\/github.com\/pandas-dev\/pandas\/pull\/51466","+1","+1","+1","+1","+1","+1","-1 i'll repeat what i stated on the PR (which has had no response)\n\nreturning self on an inplace method is a terrible idea which has happened before -\nsee #1893 - it took years to reverse this\n\nHaving these return self also now adds a HUGE amount of complexity to the api. you have the standard inplace methods, such as insert and .loc which return None, now you are adding a different case for these inplace methods. this also differs from the standard library where inplace methods return None.\n\ntbh I would either:\n\nremove all inplace everywhere; the supposed performance benefefits of having 4 methods with inplace is dubious when compared to the added complexity \n\nif you insist on the above, then reanme these to ffill_inplace and so on (to avoid the typing issue); don't love this, these still should return None","+1\r\n\r\nBut I think type annotations should not be the motivation for returning `self` for `inplace=True`: yes, it would simplify the type annotations, but keyword-only+overloads can handle this complexity. (off-topic: if the goal is to make pandas typing friendlier, avoiding `@inherit_names` would be more important)","using Quorum definition from #53576 of the 11 voting members and the voting period of 15 days then under the not yet ratified voting process then PDEP-8 should have been accepted.\r\n\r\nI'm not suggesting for 1 minute that we dismiss @jreback concerns just to fit in with the draft process. But the purpose of the PDEP process is to facilitate the decision process and avoid discussions going stale.\r\n\r\n> Once the voting period ends, any voter may tally the votes in a comment, using the format: w-x-y-z, where w stands for the total of approving, x of abstaining, z of disapproving votes cast, and z of number of voting members who did not respond to the VOTE issue. The tally of the votes will state if a quorum has been reached or not.\r\n\r\n12 approve (technically 1 approval outside voting window)\r\n0 abstain\r\n1 disapprove\r\n\r\ncan I suggest that we close this vote. potentially make some changes to the proposal that are acceptable to @jreback and then perhaps start the vote again. \r\n\r\nUntil we have all agreed on the voting process then we need to follow the governance in existence.\r\n","I actually just saw @simonjayhawkins comments. This PDEP should not be rejected just because I put a -1 out here. That is the entire point of this process. I'll re-iterate what I was trying to achieve here.\r\n\r\nI am fully +1 on removing `inplace` everywhere if possible\r\nI don't really love the fact that we have `inplace` on a small (4) subset of methods but could live with that.\r\nI don't think the return value should change on those methods, again If the group thinks this is fine, then ok.\r\n\r\nSo I would propose to either:\r\n- push the PDEP thru as is\r\n- cleave the 4 methods that are slightly controvertial and execute on the rest of the PDEP.\r\n\r\ncc @jorisvandenbossche ","> This PDEP should not be rejected just because I put a -1 out here.\r\n\r\nagree.\r\n\r\nmy thinking was that the PDEP is supposed to be a consensus seeking process and so if we go to the vote and there are objections, these objections should have been discussed thoroughly and the outcome basically agreed in the PDEP discussion. This is my understanding of why -1 votes should have a one-liner with the reason and have participated in the discussion so that there is no surprises.\r\n\r\nfrom the PDEP... \"Our workflow was created to support and enable a consensus seeking process\"\r\n\r\nIMHO it is the process that has failed here.\r\n\r\n> * cleave the 4 methods that are slightly controvertial and execute on the rest of the PDEP.\r\n\r\nMy comments were to reinvigorate a discussion\/vote that had gone stale so that the PDEP could be accepted. I was expecting that this outcome was the probable solution to getting the bulk of the PDEP accepted as the \"offending paragraph\" could immediately be included in a PDEP revision in a follow up."],"labels":["Vote"]},{"title":"PERF: casting to the new String dtype could be faster by leveraging pyarrow","body":"The following examples are of course not exactly equivalent, but to illustrate the potential improvement that is possible:\r\n\r\n```python\r\npd.options.future.infer_string = True\r\nser = pd.Series(np.random.randn(10_000_000))\r\n```\r\n\r\n```\r\nIn [11]: %timeit ser.astype(\"string\")\r\n9.15 s \u00b1 789 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [12]: %timeit pa.array(ser).cast(pa.string())\r\n1.29 s \u00b1 23.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n(apart from being faster, it also uses a less memory, because it doesn't create python strings for all elements as an intermediary)\r\n\r\nCurrently, casting to the string dtype, eventually passes the values to `ArrowStringArray._from_sequence`, which then converts the numpy float array to strings with `lib.ensure_string_array`.  ","comments":["Is it intentional that arrow converts floats to integers when casting them to strings if none of them have any decimals?\r\n\r\n```\r\nser = Series([1.0, 2.0, 3.0])\r\n\r\nser.astype(\"string\")\r\n```\r\n\r\n```\r\n0    1\r\n1    2\r\n2    3\r\ndtype: string\r\n```"],"labels":["Performance","Strings"]},{"title":"BUG: Did not expect new dtype datetime64[ns] to equal self.dtype datetime64[ns]","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [ ] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nchange_date[\"latest_change_at\"].fillna(np.datetime64('2001-01-01T00:00:00'), inplace=True)\r\n\r\n\r\nAssertionError: Did not expect new dtype datetime64[ns] to equal self.dtype datetime64[ns]. Please report a bug at https:\/\/github.com\/pandas-dev\/pandas\/issues.\n```\n\n\n### Issue Description\n\nOriginal code was:\r\nchange_date[\"latest_change_at\"].fillna(\"2001-01-01T00:00:00Z\", inplace=True)\r\nThis is my error:\r\nValue '2001-01-01T00:00:00Z' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\r\n\r\nSo I tried to change the values for the fillna to datetime64[ns] by running this:\r\nchange_date[\"latest_change_at\"].fillna(np.datetime64('2001-01-01T00:00:00'), inplace=True)\r\n\n\n### Expected Behavior\n\nThis should cast.\n\n### Installed Versions\n\nPandas 2.1.4\r\n","comments":["Hi, thanks for your report. Can you give us a full reproducer that includes the construction of ``change_date``?"],"labels":["Bug","Dtype Conversions","Needs Info"]},{"title":"ENH: pandas mutate, add R's mutate functionality to enable users to easily create new columns in data frames","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [ ] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nI wish feature engineering (i.e. creating new columns from old ones) could be more efficient and convenient in pandas.\r\nMainly, common ways of adding features to dataframes in pandas include\r\n\r\n1. using chained `.assign` statements (which are hard to debug and contain many hard-to-read lambda expressions) or\r\n2. calling `df['new_column'] = ...` repeatedly in some `add_features` function, this is better for debugging purposes but also hard to read and inconvenient as the user always has to type quotes and the word `df`.\r\n\r\nIn R's `mutate` function, the series are accessible directly from the scope which makes code much more readable (debugging in R is something else to discuss).\n\n### Feature Description\n\nWe could easily add this functionality by providing a context manager (perhaps pd.mutate, to follow R's naming here) which temporarily moves all columns of a dataframe into the caller's `locals`, allows the caller to create new pd.Series while calling and then (upon the context manager's __exit__) all those new pd.Series (or the modified old ones) could be formed to a data frame again.\r\n\r\nThis makes feature engineering much more convenient, efficient and likely also more debuggable that using chained .assign statements (in the debugger, one could directly access all the pd.Series in that scope).\r\nA minimal example implementation could look like the following:\r\n\r\n```python\r\n\r\n# %%\r\n\r\nfrom multiprocessing import context\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(\r\n    {\r\n        \"col_a\": [1, 2, 3, 4, 5],\r\n        \"col_b\": [10, 20, 30, 40, 50],\r\n    }\r\n)\r\n\r\ndf\r\n\r\n# %%\r\n\r\nimport inspect\r\nfrom contextlib import contextmanager\r\nfrom copy import deepcopy\r\n\r\nclass mutate_df:\r\n    def __init__(self, df: pd.DataFrame):\r\n        self.df = df\r\n\r\n    def __enter__(self):\r\n        frame = inspect.currentframe().f_back\r\n        self.scope_keys = deepcopy(self._extract_locals_keys_from_frame(frame))\r\n\r\n        for col in self.df.columns:\r\n            if col in self.scope_keys:\r\n                # Maybe give a warning here?\r\n                pass\r\n\r\n            frame.f_locals[col] = self.df[col]\r\n\r\n    def _extract_locals_keys_from_frame(self, frame):\r\n        s = {\r\n            str(key)\r\n            for key in frame.f_locals.keys()\r\n            if not key.startswith(\"_\")\r\n        }\r\n        return s\r\n\r\n    def __exit__(self, exc_type, exc_val, exc_tb):\r\n        if exc_type:\r\n            raise exc_type(exc_val)\r\n\r\n        frame = inspect.currentframe().f_back\r\n        current_keys = self._extract_locals_keys_from_frame(frame)\r\n\r\n        added_keys = current_keys - self.scope_keys\r\n        added_keys = set.union(added_keys, set(self.df.columns))\r\n\r\n        for key in added_keys:\r\n            try:\r\n                val = frame.f_locals[key]\r\n                self.df[key] = val\r\n\r\n            except:\r\n                pass\r\n\r\n        return True\r\n\r\nwith mutate_df(df):\r\n    # All of df's columns are available in the scope\r\n    # as pd.Series objects\r\n\r\n    # Set the entire column to one value\r\n    c = 10\r\n    # Use columns defined previously\r\n    col_c = col_a * 20\r\n\r\n    # Create new columns\r\n    rolling_mean = col_b.rolling(2).mean()\r\n    col_b_cumsum = col_b.cumsum()\r\n\r\ndf\r\n\r\n\r\n# %%\r\n\r\n```\r\n\r\nThe drawback of this feature is that we are fiddling with the caller's `locals` which is not the most elegant.\r\nHowever, I believe that feature engineering like this is much better to debug and makes the code more readable (than using chained `.assign`s or repeatedly calling `df['new_feature'] = 2 * df['old_feature'] ** 2`).\r\n\r\nTherefore I think this feature would make life easier and pandas more useful (and users faster) in data science tasks.\n\n### Alternative Solutions\n\nOne might want to handle the `locals` better here to make the usage of this feature less error-prone.\r\nPerhaps one would want to cache previous `locals` and then *only* have the dataframe's columns as the locals in the caller's scope.\r\n\r\nThis would make debugging even more clean, because if a user sets a breakpoint in such a `with pd.mutate` statement, then that user sees all the columns in the scope's locals clearly instead of having to inspect the dataframe's columns values in the debugger.\n\n### Additional Context\n\n_No response_","comments":["We have a solution for the lambdas within assign in the pipeline, so mutate is unlikely to get added\r\n\r\ncc @MarcoGorelli ","Seems interesting but unless I am misunderstanding, these two things contradict one another\r\n\r\n>We could easily add this functionality by providing a context manager (perhaps pd.mutate, to follow R's naming here) which temporarily moves all columns of a dataframe into the caller's locals, allows the caller to create new pd.Series while calling and then (upon the context manager's exit) **all those new pd.Series (or the modified old ones) could be formed to a data frame again.**\r\n\r\n```\r\n    # Set the entire column to one value\r\n    c = 10\r\n```\r\n\r\nSo any constants assigned in scope of the context manager become columns also? Or is it only assignments where the RHS is a `pandas.Series` instance? Your example reads like the former, which I think is less than ideal because then can't use intermediate variables within the context manager unless we explicit call `del` on them to remove from the scope. Given explicit use of `del` is pretty rare and some may consider unpythonic, I think that's a going to hold this back greatly.","I don't think they necessarily contradict each other, because:\r\n\r\n1. Users could define locals they do not want to be added as columns via `_some_constant` (i.e. by prefixing it with an underscore), this also fits in nicely with the notion of \"private\" functions in python.\r\n2. We could add a keyword argument to the context manager to allow `pd.mutate(df, reset_locals=False, convert_non_series_to_column=True)` where the latter would allow constants to be assigned to series that have the constant value (as in `.assign`),\r\n3. People could either _define constants before_ the context manager or _within functions that could be called from within the context manager_ and return the desired `pd.Series`. Remember that the scope the context manager uses does not include locals defined in functions which are called from within that scope. As an example:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndf: pd.DataFrame = <some df>\r\nsome_constant: int = 42\r\n\r\ndef make_new_col(df) -> pd.Series:\r\n    # This will of course not be assigned to the dataframe\r\n    locally_defined_constant: int = 101\r\n\r\n    return df['new_col'] * locally_defined_constant\r\n\r\nwith pd.mutate(df):\r\n    # Assigning a pd.Series directly adds it to the columns\r\n    new_col = pd.Series([\"a\", \"b\", ...])\r\n\r\n    constant_series = 42 # This will become a pd.Series having the constant value 42\r\n\r\n    other_col = make_new_col(df)\r\n```\r\n\r\nAnd @phofl , I think this context manager is not meant to replace `.assign` but rather provide a new, more easily debuggable and clean way of creating new columns in dataframes.\r\n\r\nFurther, I think this would also fit nicely in with pandas' notion of series and dataframes and users could more natively modify a dataframe's \"scope\".\r\nAlso, I think a `kwarg` like the `reset_locals` would make sense to users as they then actually enter the \"dataframes scope\", but we might want to set it to `False` by default. If this is set, previous `locals()` would be removed to cleanly have only the dataframe's columns, and `df` of course, in the `locals()`.\r\n\r\nWe would just have to make sure to clean up the newly assigned columns from the `locals()` scope so users don't mix up their scope when using this context manager.","> I don't think they necessarily contradict each other, because:\r\n> \r\n> 1. Users could define locals they do not want to be added as columns via `_some_constant` (i.e. by prefixing it with an underscore), this also fits in nicely with the notion of \"private\" functions in python.\r\n> 2. We could add a keyword argument to the context manager to allow `pd.mutate(df, reset_locals=False, convert_non_series_to_column=True)` where the latter would allow constants to be assigned to series that have the constant value (as in `.assign`),\r\n> 3. People could either _define constants before_ the context manager or _within functions that could be called from within the context manager_ and return the desired `pd.Series`. Remember that the scope the context manager uses does not include locals defined in functions which are called from within that scope. As an example:\r\n> \r\n> ```python\r\n> import pandas as pd\r\n> \r\n> df: pd.DataFrame = <some df>\r\n> some_constant: int = 42\r\n> \r\n> def make_new_col(df) -> pd.Series:\r\n>     # This will of course not be assigned to the dataframe\r\n>     locally_defined_constant: int = 101\r\n> \r\n>     return df['new_col'] * locally_defined_constant\r\n> \r\n> with pd.mutate(df):\r\n>     # Assigning a pd.Series directly adds it to the columns\r\n>     new_col = pd.Series([\"a\", \"b\", ...])\r\n> \r\n>     constant_series = 42 # This will become a pd.Series having the constant value 42\r\n> \r\n>     other_col = make_new_col(df)\r\n> ```\r\n\r\nAh, thank you for the clarification. I for one find `.assign` to be verbose at times, particular when I want to make a new column that depends on an another one and would invole doing a `.apply` (nested lambda yuck!). My other concern was the magicalness of it, but after reading the implementation details of `.query` to support the use of `@` for injecting locals, I don't think its that big of a deal.\r\n\r\nI like the proposal more now.\r\n","Yes, I agree this context manager would be slightly 'hacky' but I think it would boost productivity and improve code readability much more, which I think is more important.\r\n_Another great advantage would be debugging._\r\n\r\nImagine a user sets a breakpoint the first line within the context manager. If we were to clean up previous `locals()`, that user would then get a clear overview seeing all columns as `pd.Series` in the debugger's variable explorer.\r\n\r\nThis makes it much more obvious and explicit to understand column assignment in detail than using (1) chained `.assign` statements (which are basically impossible to debug) or (2) any function that repeatedly calls `df['new_col'] = ...`, which is easier to debug but even in this case, the user would only see created columns by viewing the `df`'s columns explicitly rather than seeing them implicitly in the debugger's variable explorer."],"labels":["Enhancement","Needs Triage"]},{"title":"ENH: `DataFrame.assign` allow passing dictionary directly","body":"### Feature Type\n\n- [X] Adding new functionality to pandas\n\n- [x] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nCurrent signature `DataFrame.assign(**kwargs)` means that only string keys are supported.\n\n### Feature Description\n\nChange signature to be in line with `dict.__init__` \/ `UserDict.__init__`: allow passing a single mapping as well as `**kwargs`.\n\n### Alternative Solutions\n\nOne could change to string and back.\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/72535853\/how-to-assign-to-column-with-non-string-name-or-index\n\n### Additional Context\n\n_No response_","comments":[],"labels":["Enhancement","Needs Discussion"]},{"title":"ENH: Enhanced Parsing in eval Method for Special Characters","body":"### Feature Type\r\n\r\n- [ ] Adding new functionality to pandas\r\n\r\n- [X] Changing existing functionality in pandas\r\n\r\n- [ ] Removing existing functionality in pandas\r\n\r\n\r\n### Problem Description\r\n\r\nHey Pandas team,\r\n\r\nCurrently, the `eval` method treats special characters in column names different than the `query` method.\r\nThe `query` method allows for the use of backticks to enclose column names with special characters in contrast to the `eval` method. This inconsistency can be confusing for users.\r\n\r\nThe proposal is to enhance the parsing capabilities of the `eval` method to recognize and handle backticks in a manner similar to the `query` method. This modification would provide users with more flexibility when working with DataFrame expressions, especially in cases where column names contain special characters.\r\n\r\nHere is an example:\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'my.col': [1, 2, 3]})\r\nqry = df.query('`my.col` > 2')\r\nevl = df.eval('`my.col` = 0')\r\nprint('Original:')\r\nprint(df)\r\nprint('\\nquery: `my.col` > 2')\r\nprint(qry)\r\nprint('\\neval: `my.col` = 0')\r\nprint(evl)\r\n\r\n# Original:\r\n#    my.col\r\n# 0       1\r\n# 1       2\r\n# 2       3\r\n\r\n# query: `my.col` > 2\r\n#    my.col\r\n# 2       3\r\n\r\n# eval: `my.col` = 0\r\n#    my.col  BACKTICK_QUOTED_STRING_my_DOT_col\r\n# 0       1                                  0\r\n# 1       2                                  0\r\n# 2       3                                  0\r\n\r\n# Current behavior in eval\r\n# Creates a column with modified name 'BACKTICK_QUOTED_STRING_my_DOT_col'\r\n```\r\n\r\n### Feature Description\r\n\r\n```\r\n# Proposed behavior in eval\r\n# Directly modifies the 'my.col' column\r\n# df.eval('`my.col` = 0')\r\n#    my.col\r\n# 0       0\r\n# 1       0\r\n# 2       0\r\n```\r\n\r\n### Alternative Solutions\r\n\r\nCan `eval` parsing be adapted to the way `query` parses?\r\n\r\n### Additional Context\r\nPython 3.11.5\r\nPandas 2.1.1","comments":["Thanks for the report; +1 on making this consistent.","take"],"labels":["Enhancement","expressions"]},{"title":"REF: use maybe_convert_objects in pd.array","body":"- [x] closes #52972 (Replace xxxx with the GitHub issue number)\r\n- [x] closes #29973\r\n- [x] [Tests added and passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature\r\n- [x] All [code checks passed](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#pre-commit).\r\n- [ ] Added [type annotations](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/development\/contributing_codebase.html#type-hints) to new arguments\/methods\/functions.\r\n- [ ] Added an entry in the latest `doc\/source\/whatsnew\/vX.X.X.rst` file if fixing a bug or adding a new feature.\r\n\r\n@phofl the only other place where the convert_nullable_whatever option is used is in the sql code that i think you implemented.  will any of the changes here affect that?","comments":["@lithomas1 looked at this function recently so might have feedback","should be fine for the sql use case","This pull request is stale because it has been open for thirty days with no activity. Please [update](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this."],"labels":["Stale"]},{"title":"ENH: get something like a  \"list-like\" type out of infer_dtype when a series with all lists is passed ","body":"### Feature Type\n\n- [ ] Adding new functionality to pandas\n\n- [x] Changing existing functionality in pandas\n\n- [ ] Removing existing functionality in pandas\n\n\n### Problem Description\n\nHi,\r\n\r\nI was trying to use `infer_dtype` to fish for dataframe columns that contain only list like values, but I get only \"mixed\" in that case. To demonstrate the issue:\r\n```python\r\n>>> import pandas as pd\r\n>>> from pandas.api.types import infer_dtype, is_list_like\r\n\r\n>>> s = pd.Series([[1,2], None, [5,6]])\r\n>>> non_list = pd.Series([\"blabla\", None, 234.0])\r\n>>> infer_dtype(s, skipna=True)\r\n'mixed'\r\n>>> all(s.dropna().apply(is_list_like))\r\nTrue\r\n>>> infer_dtype(non_list, skipna=True)\r\n'mixed'\r\n>>> all(non_list.dropna().apply(is_list_like))\r\nFalse\r\n```\r\n \r\nSo for both list-like and truly mixed series I get \"mixed\". Something like \"list-like\" in that case would be much better.\n\n### Feature Description\n\nI'm guessing a patch to `infer_dtype` or an entirely new method, e.g. a `is_list_like()` that takes a series and checks all values.\n\n### Alternative Solutions\n\nOne could do:\r\n```python\r\nimport pandas as pd\r\nfrom pandas.api.types import infer_dtype, is_list_like\r\n\r\ns = pd.Series([[1,2], None, [5,6]])\r\nall(s.dropna().apply(is_list_like))\r\n```\r\n\r\nbut this scans the entire series which is not great for big series\/df columns.\n\n### Additional Context\n\n_No response_","comments":["Sorry for the slow reply.\r\n\r\nI guess this is something that would be improved if pyarrow became a mandatory dependency.\r\n(We could probably infer this as a pyarrow list-dtype)"],"labels":["Enhancement","Dtype Conversions","Arrow","Blocked"]},{"title":"BUG: Inconsistent data type inference in `read_csv` for large CSV files with mixed data types","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\n# Example code to reproduce the issue\r\nimport pandas as pd\r\n\r\n# Imagine this CSV content represents a large file with similar structure\r\ncsv_content = \"\"\"\r\ncolumn1\r\n0\r\n1\r\n2\r\n3\r\n...\r\n'abcd'  # A string value inserted here\r\n...\r\n999996\r\n999997\r\n999998\r\n999999\r\n\"\"\"\n```\n\n\n### Issue Description\n\nI have encountered an issue where Pandas seems to infer different data types for the same numeric values based on the presence of a single string value in a large CSV file (about 100w rows). The columns with numeric values are being inferred as `object` dtype if a single string is inserted somewhere in the column. This affects not only the row with the string but also a significant number of rows around it, the dtype of the 20w(it's different every time) rows before and after have been converted to `object`, leading to inconsistent dtype inference.The numerical values in different rows may have different types.\r\n\r\nAnd read_csv with low_memory=False, the result is normal, the numerical values dtypes will be consistent.\n\n### Expected Behavior\n\n```\r\nraw = pd.read_csv('test.csv')\r\nraw['type'] = raw['key'].map(lambda x: str(type(x)))\r\nprint(raw[raw.key=='abcd'])\r\n          key              type\r\n599999  abcd  <class 'str'>\r\n\r\nprint(raw.type.value_counts())\r\ntype\r\n<class 'int'>    524288\r\n<class 'str'>    475713\r\n\r\nprint(raw[raw.type==\"<class 'int'>\"])\r\n           key              type\r\n0            0  <class 'int'>\r\n1            1  <class 'int'>\r\n2            2  <class 'int'>\r\n3            3  <class 'int'>\r\n4            4  <class 'int'>\r\n...        ...            ...\r\n524283  524283  <class 'int'>\r\n524284  524284  <class 'int'>\r\n524285  524285  <class 'int'>\r\n524286  524286  <class 'int'>\r\n524287  524287  <class 'int'>\r\n\r\nprint(raw[raw.type==\"<class 'str'>\"])\r\n            key              type\r\n524288   524288  <class 'str'>\r\n524289   524289  <class 'str'>\r\n524290   524290  <class 'str'>\r\n524291   524291  <class 'str'>\r\n524292   524292  <class 'str'>\r\n...         ...            ...\r\n999996   999995  <class 'str'>\r\n999997   999996  <class 'str'>\r\n999998   999997  <class 'str'>\r\n999999   999998  <class 'str'>\r\n1000000  999999  <class 'str'>\r\n```\n\n### Installed Versions\n\npandas version 2.1.4\r\nsystem ubuntu 22.04\r\n","comments":["Hi @yinzhedfs,\r\nhave you tried using StringIO to create a file-like object for csv_content?"],"labels":["Bug","Needs Triage"]},{"title":"BUG: bar a line plots are not aligned on the x-axis\/xticks","body":"### Pandas version checks\n\n- [X] I have checked that this issue has not already been reported.\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/pandas.pydata.org\/docs\/whatsnew\/index.html) of pandas.\n\n- [X] I have confirmed this bug exists on the [main branch](https:\/\/pandas.pydata.org\/docs\/dev\/getting_started\/install.html#installing-the-development-version-of-pandas) of pandas.\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    {\r\n        \"bars\": {\r\n            1: 0.5,\r\n            2: 1.0,\r\n            3: 3.0,\r\n            4: 3.5,\r\n            5: 1.5,\r\n        },\r\n        \"pct\": {\r\n            1: 4.0,\r\n            2: 2.0,\r\n            3: 2.0,\r\n            4: 2.0,\r\n            5: 8.0,\r\n        },\r\n    }\r\n)\r\n\r\nax=df[\"bars\"].plot(kind=\"bar\")\r\ndf[\"pct\"].plot(kind=\"line\", ax=ax,)\n```\n\n\n### Issue Description\n\nBar and line plot are not aligned on the x-axis when plotting with Pandas. I saw some somewhat related issues, but they were not exactly this type of plot.\r\n\r\nThis is the plot generated from the sample code above:\r\n\r\n![image](https:\/\/github.com\/pandas-dev\/pandas\/assets\/47297554\/a7e62f94-6d76-4291-861f-cd444a19e3a9)\r\n\n\n### Expected Behavior\n\nLine also starts in index=1, and not index=2 like in the plot above. \n\n### Installed Versions\n\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit              : a671b5a8bf5dd13fb19f0e88edc679bc9e15c673\r\npython              : 3.11.6.final.0\r\npython-bits         : 64\r\nOS                  : Windows\r\nOS-release          : 10\r\nVersion             : 10.0.19045\r\nmachine             : AMD64\r\nprocessor           : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel\r\nbyteorder           : little\r\nLC_ALL              : None\r\nLANG                : None\r\nLOCALE              : English_United States.1252\r\n\r\npandas              : 2.1.4\r\nnumpy               : 1.26.2\r\npytz                : 2023.3.post1\r\ndateutil            : 2.8.2\r\nsetuptools          : 65.5.0\r\npip                 : 23.2.1\r\nCython              : None\r\npytest              : 7.4.3\r\nhypothesis          : None\r\nsphinx              : 7.2.6\r\nblosc               : None\r\nfeather             : None\r\nxlsxwriter          : None\r\nlxml.etree          : 4.9.3\r\nhtml5lib            : None\r\npymysql             : None\r\npsycopg2            : None\r\njinja2              : 3.1.2\r\nIPython             : 8.17.2\r\npandas_datareader   : None\r\nbs4                 : None\r\nbottleneck          : None\r\ndataframe-api-compat: None\r\nfastparquet         : None\r\nfsspec              : None\r\ngcsfs               : None\r\nmatplotlib          : 3.8.2\r\nnumba               : None\r\nnumexpr             : None\r\nodfpy               : None\r\nopenpyxl            : None\r\npandas_gbq          : None\r\npyarrow             : 14.0.1\r\npyreadstat          : None\r\npyxlsb              : None\r\ns3fs                : None\r\nscipy               : 1.11.4\r\nsqlalchemy          : None\r\ntables              : None\r\ntabulate            : 0.9.0\r\nxarray              : None\r\nxlrd                : None\r\nzstandard           : None\r\ntzdata              : 2023.3\r\nqtpy                : None\r\npyqt5               : None\r\n\r\n<\/details>\r\n","comments":["Thanks for the report, confirmed on main. Further investigations and PRs to fix are welcome!","I'm new to the codebase but this caught my interest because I've never encountered it, but it would be really annoying if I did. I'm still trying to understand the code but while reading it I tried to get a sense of where to look by plotting a few graphs myself.\r\n\r\n- In the above example, if it's only lines that are plotted, they are plotted on the correct axes _until_ a bar graph gets added. Then the lines all get shifted by 1. \r\n\r\n- Also if the axes ran from 0-4, the graph is plotted normally, ie with the bars and lines on the correct axes: \r\n\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    {\r\n        \"bars\": {\r\n            0: 0.5,\r\n            1: 1.0,\r\n            2: 3.0,\r\n            3: 3.5,\r\n            4: 1.5,\r\n        },\r\n        \"pct\": {\r\n            0: 4.0,\r\n            1: 2.0,\r\n            2: 2.0,\r\n            3: 2.0,\r\n            4: 8.0,\r\n        },\r\n        \"test\": {\r\n            0: 8.0,\r\n            1: 1.0,\r\n            2: 2.0,\r\n            3: 2.0,\r\n            4: 10.0,\r\n        },\r\n    }\r\n)\r\n\r\nax1 = df[\"pct\"].plot(kind=\"line\")\r\nax2 = df[\"bars\"].plot(kind=\"bar\")\r\nax3 = df[\"test\"].plot(kind=\"line\")\r\n```\r\n\r\nBut if I change my axes for all 3 to 3-7, then I get a shift of 3 instead for the lines (starting at 6 instead of 3): \r\n\r\n![Figure_1](https:\/\/github.com\/pandas-dev\/pandas\/assets\/29156885\/9d3ef6f6-9c1e-42e4-a232-a48bb2eb8261)\r\n\r\nCode to reproduce this to save some editing:\r\n\r\n```\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame(\r\n    {\r\n        \"bars\": {\r\n            3: 0.5,\r\n            4: 1.0,\r\n            5: 3.0,\r\n            6: 3.5,\r\n            7: 1.5,\r\n        },\r\n        \"pct\": {\r\n            3: 4.0,\r\n            4: 2.0,\r\n            5: 2.0,\r\n            6: 2.0,\r\n            7: 8.0,\r\n        },\r\n        \"test\": {\r\n            3: 8.0,\r\n            4: 1.0,\r\n            5: 2.0,\r\n            6: 2.0,\r\n            7: 10.0,\r\n        },\r\n    }\r\n)\r\n\r\nax1 = df[\"pct\"].plot(kind=\"line\")\r\nax2 = df[\"bars\"].plot(kind=\"bar\")\r\nax3 = df[\"test\"].plot(kind=\"line\")\r\n```\r\n\r\nAnnnnnd consistent with expectations, we get a shift of -1 for the lines if the axes all start with -1.","Opened an incomplete PR (logic only, I need to fix CI) for further discussion - this would fix the issue, but should we fix it in the first place?\r\n\r\nTldr, BarPlot uses `self.tick_pos = np.arange(len(data))` to set x, and I've put in some logic that lets it do something similar to LinePlot if the data is a series. ","take","Thanks for the investigations here. We also get incorrect results if all the Series used for the plots do not have the same index, e.g.\r\n\r\n```\r\nbars = pd.Series({\"a\": 0.5, \"b\": 1.0})\r\npct = pd.Series({\"b\": 4.0, \"a\": 2.0})\r\nax = bars.plot(kind=\"bar\")\r\npct.plot(kind=\"line\", ax=ax)\r\n```\r\n\r\nIt does seem reasonable to me for users to expect the order of the bars is preserved, even for numeric indexes:\r\n\r\n```\r\npct = pd.Series({1: 4.0, 10: 2.0, 3: 3.0})\r\npct.plot(kind=\"bar\")\r\n```\r\n\r\nThe bars currently appear with x-ticks 1, 10, 3.\r\n\r\nIt seems difficult to determine appropriate results when there are different indexes in the data in general. I wonder if when plotting and `ax` is provided we can just detect when the xticks do not agree.","Thanks for the thoughtful comment, let me investigate further. \r\n\r\nI found some related issues raised previously: https:\/\/github.com\/pandas-dev\/pandas\/issues\/55508, https:\/\/github.com\/pandas-dev\/pandas\/issues\/50508, https:\/\/github.com\/pandas-dev\/pandas\/issues\/48806","> Thanks for the investigations here. We also get incorrect results if all the Series used for the plots do not have the same index, e.g.\r\n> \r\n> ```\r\n> bars = pd.Series({\"a\": 0.5, \"b\": 1.0})\r\n> pct = pd.Series({\"b\": 4.0, \"a\": 2.0})\r\n> ax = bars.plot(kind=\"bar\")\r\n> pct.plot(kind=\"line\", ax=ax)\r\n> ```\r\n> \r\n> It does seem reasonable to me for users to expect the order of the bars is preserved, even for numeric indexes:\r\n> \r\n> ```\r\n> pct = pd.Series({1: 4.0, 10: 2.0, 3: 3.0})\r\n> pct.plot(kind=\"bar\")\r\n> ```\r\n> \r\n> The bars currently appear with x-ticks 1, 10, 3.\r\n> \r\n> It seems difficult to determine appropriate results when there are different indexes in the data in general. I wonder if when plotting and `ax` is provided we can just detect when the xticks do not agree.\r\n\r\nI tried, and Matplotlib does this chart correctly:\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\nbars = {\"a\": 1.5, \"b\": 3.5}\r\npct = {\"b\": 4.0, \"a\": 2.0}\r\n\r\nfig, ax = plt.subplots()\r\n\r\nax.bar(bars.keys(), bars.values(), color='blue')\r\nax.plot(pct.keys(), pct.values(), color='red')\r\n\r\nplt.show()\r\n\r\n```\r\n\r\n![Unknown](https:\/\/github.com\/pandas-dev\/pandas\/assets\/29156885\/3d6dcde8-3807-4ad9-8939-f489b52c7d26)\r\n\r\n\r\n\r\nHowever for the out of order series, the PR implementation is similar to what Matplotlib currently does (ie bars are out of order): \r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\npct = pd.Series({1: 4.0, 10: 2.0, 3: 3.0})\r\nplt.figure()\r\nplt.bar(pct.index, pct.values, color='blue')\r\nplt.show()\r\n```\r\n![Unknown-2](https:\/\/github.com\/pandas-dev\/pandas\/assets\/29156885\/a6f3ad22-ea0b-482e-9562-2024449e105b)\r\n\r\n","I haven't been able to find documentation on this, but as far as I can tell matplotlib's behavior is:\r\n\r\n - If a series of x values contains both numeric and non-numeric, raise.\r\n - If a series of x values is numeric, use the value to determine location\r\n - If a series of x values is non-numeric, use the position (0, 1, 2, ...) to determine the location\r\n\r\nThe third bullet point above extends to when a series of non-numeric values is added to the plot: if there is a symbol not yet seen, it is added as the next bar. If it's already been seen, it is stacked.\r\n\r\nFrom the [matplotlib docs](https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.axes.Axes.bar.html), I think we may also need to consider \"if x has units (e.g. datetime)\" as a separate case. I've yet to see how that interacts with the logic above.\r\n\r\nIt seems to me we can replicate this behavior (MultiIndex entries treated as tuples and hence non-numeric). One thing I'm still wondering is why we don't just offload this logic to matplotlib's default behavior.","Sorry, been a bit busy and under the weather lately (both concurrently sometimes with an ill toddler). \r\n\r\nIf I understand correctly, the scope of this bug fix has been expanded in order to align the pandas wrapper with what matplotlib does, except where bars are out of order (preserve out of order, which would differ from the matplotlib implementation? ","> to align the pandas wrapper with what matplotlib does\r\n\r\nThe matplotlib behavior certainly seems very reasonable, better than the status quo for pandas, and currently has my support. That said, if there are issues with it, we do not necessarily _have_ to align with it.\r\n\r\n> except where bars are out of order (preserve out of order, which would differ from the matplotlib implementation?\r\n\r\nI don't think so. While a user might want to present 1, 10, 3 in that order for bar charts, it seems more valuable to prefer using numeric dtypes as indications of where on the x-axis to place the bar. This can support e.g. multiple bar charts stacked when they don't have the same exact index. In order for the user to then get 1, 10, 3 in that order, they just need to convert the integers into strings and that will give the desired behavior.","Looks like I'll have some time starting from next Monday to take a serious crack at this."],"labels":["Bug","Visualization"]},{"title":"BUG: ChainedAssignmentError for CoW not working for chained inplace methods when passing `*args` or `**kwargs`","body":"While working on https:\/\/github.com\/pandas-dev\/pandas\/pull\/56402 (ensuring full test coverage for the warning we raise when you do chained inplace methods), we ran into another corner case where the refcount differs, and mixes up the refcount-based inference about whether we are being called in a \"chained\" context and thus have to raise the ChainedAssignment warning. \r\n(a previous case we discovered was when the setitem call is coming from cython code: https:\/\/github.com\/pandas-dev\/pandas\/issues\/51315)\r\n\r\nSpecifically, when passing `*args` or `**kwargs` to the inplace method call, it takes a different execution path in the Python interpreter compared to calling it with manually specified arguments. And starting from Python 3.11, this other execution path gives one reference less.\r\n\r\nExample (with CoW, the `df` will never be updated with such chained method call):\r\n\r\n```python\r\n>>> pd.options.mode.copy_on_write = True\r\n>>> df = DataFrame({\"a\": [1, 2, np.nan], \"b\": 1})\r\n>>> df[\"a\"].fillna(0, inplace=True)\r\nChainedAssignmentError: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\r\nWhen using the Copy-on-Write mode, such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy.\r\n\r\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object.\r\n```\r\n\r\n\r\nThat warning works for all tested Python versions. However, when passing the `args` or `kwargs`, the warning doesn't work on Python >= 3.11:\r\n\r\n```python\r\n>>> pd.options.mode.copy_on_write = True\r\n>>> df = DataFrame({\"a\": [1, 2, np.nan], \"b\": 1})\r\n>>> args = (0, )\r\n>>> df[\"a\"].fillna(*args, inplace=True)\r\n# no warning\r\n```\r\n\r\nFor now we decided to punt on this specific corner case, but creating this issue to we keep track of what we learned and have something to reference to when this might come up later.\r\n ","comments":["Some more details on the inner Python interpreter details about why this is happening: when calling the function with `*args` or `**kwargs`, i.e. with a variable number of arguments, the opcode [`CALL_FUNCTION_EX`](https:\/\/docs.python.org\/3.11\/library\/dis.html#opcode-CALL_FUNCTION_EX) is being used.  \r\nWhen calling the function in a standard way with specified keywords, prior to Python 3.11 the opcode `CALL_FUNCTION_KW` was being used. But with Python 3.11, this has been optimized, and this opcode is replaced with `KW_NAMES` + [`CALL`](https:\/\/docs.python.org\/3.11\/library\/dis.html#opcode-CALL) opcodes. And apparently this optimized implementation results in less references to the object on which the method is called.\r\n\r\n<details>\r\n<summary>Example disassembly (opcodes) for Python 3.10 vs 3.11 for the above fillna call<\/summary>\r\nPython 3.10 (both cases have ref count of 4):\r\n\r\n```\r\nIn [7]: dis.dis(lambda: df['a'].fillna(1, inplace=True))\r\n  1           0 LOAD_GLOBAL              0 (df)\r\n              2 LOAD_CONST               1 ('a')\r\n              4 BINARY_SUBSCR\r\n              6 LOAD_ATTR                1 (fillna)\r\n              8 LOAD_CONST               2 (1)\r\n             10 LOAD_CONST               3 (True)\r\n             12 LOAD_CONST               4 (('inplace',))\r\n             14 CALL_FUNCTION_KW         2\r\n             16 RETURN_VALUE\r\n\r\nIn [8]: dis.dis(lambda: df['a'].fillna(*args, inplace=True))\r\n  1           0 LOAD_GLOBAL              0 (df)\r\n              2 LOAD_CONST               1 ('a')\r\n              4 BINARY_SUBSCR\r\n              6 LOAD_ATTR                1 (fillna)\r\n              8 LOAD_GLOBAL              2 (args)\r\n             10 LOAD_CONST               2 ('inplace')\r\n             12 LOAD_CONST               3 (True)\r\n             14 BUILD_MAP                1\r\n             16 CALL_FUNCTION_EX         1\r\n             18 RETURN_VALUE\r\n```\r\n\r\nPython 3.11:\r\n\r\n```\r\nIn [8]: dis.dis(lambda: df['a'].fillna(*args, inplace=True))\r\n  1           0 RESUME                   0\r\n              2 LOAD_GLOBAL              1 (NULL + df)\r\n             14 LOAD_CONST               1 ('a')\r\n             16 BINARY_SUBSCR\r\n             26 LOAD_ATTR                1 (fillna)\r\n             36 LOAD_GLOBAL              4 (args)\r\n             48 LOAD_CONST               2 ('inplace')\r\n             50 LOAD_CONST               3 (True)\r\n             52 BUILD_MAP                1\r\n             54 CALL_FUNCTION_EX         1\r\n             56 RETURN_VALUE\r\n\r\nIn [9]: dis.dis(lambda: df['a'].fillna(1, inplace=True))\r\n  1           0 RESUME                   0\r\n              2 LOAD_GLOBAL              0 (df)\r\n             14 LOAD_CONST               1 ('a')\r\n             16 BINARY_SUBSCR\r\n             26 LOAD_METHOD              1 (fillna)\r\n             48 LOAD_CONST               2 (1)\r\n             50 LOAD_CONST               3 (True)\r\n             52 KW_NAMES                 4\r\n             54 PRECALL                  2\r\n             58 CALL                     2\r\n             68 RETURN_VALUE\r\n```\r\n\r\n<\/details>\r\n\r\nWe actually noticed this different ref count for the standard case, and therefore use a different threshold for Python 3.11+:\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/400ae748367cf9a356b8433c1aa59f34033f02a4\/pandas\/compat\/_constants.py#L21\r\n\r\nBut that means that for the args\/kwargs use case, we incorrectly don't trigger a warning.\r\n\r\n_If_ we want, we probably would be able to inspect the stack to try to figure out whether the method was called with args\/kwargs or not, if we are OK with paying the code complexity for this and the runtime overhead. Short-term we probably won't include this, but if it turns out to be a more common case, we can always try that in the future. \r\nI experimented a little bit with this, so posting here the code for posterity.\r\n\r\n<details>\r\n\r\n```python\r\ndef _is_using_args_kwargs(code_object, method):\r\n    # simple case of df.method(..) without args\/kwargs\r\n    for instr in dis.get_instructions(code_object):\r\n        if instr.opname == \"LOAD_METHOD\" and instr.argval == method:\r\n            print(\"we have LOAD_METHOD (fillna)\")\r\n            return False\r\n\r\n    # fallback for more complex cases\r\n    instructions = list(dis.get_instructions(code_object))\r\n    instr_call_ex = [i for i, instr in enumerate(instructions) if instr.opname == \"CALL_FUNCTION_EX\"]\r\n    if not instr_call_ex:\r\n        print(\"we have no CALL_FUNCTION_EX\")\r\n        return False\r\n\r\n    method_with_ex = False\r\n    for case in instr_call_ex:\r\n        for i in range(case, 0, -1):\r\n            if instructions[i].opname == \"LOAD_ATTR\":\r\n                method_with_ex = instructions[i].argval == method\r\n                if method_with_ex:\r\n                    print(\"we have CALL_FUNCTION_EX with prepending LOAD_ATTR (fillna)\")\r\n                break\r\n        if method_with_ex:\r\n            return True\r\n    \r\n    return method_with_ex\r\n```\r\n\r\nand this works as \r\n\r\n```\r\nIn [6]: import dis\r\n\r\nIn [7]: d = dis.Bytecode(\"df['a'].fillna(*args, inplace=True)\")\r\n\r\nIn [10]: _is_using_args_kwargs(d.codeobj, method=\"fillna\")\r\nOut[10]: True\r\n\r\nIn [11]: d = dis.Bytecode(\"df['a'].fillna(0, inplace=True)\")\r\n\r\nIn [12]: _is_using_args_kwargs(d.codeobj, method=\"fillna\")\r\nOut[12]: False\r\n\r\nIn [13]: d = dis.Bytecode(\"df['a'].fillna(0, inplace=True).reset_index(*args)\")\r\n\r\nIn [14]: _is_using_args_kwargs(d.codeobj, method=\"fillna\")\r\nOut[14]: False\r\n```\r\n\r\nFor the actual method, it could be integrated with something like:\r\n\r\n```diff\r\n--- a\/pandas\/core\/generic.py\r\n+++ b\/pandas\/core\/generic.py\r\n@@ -6,6 +6,7 @@ from copy import deepcopy\r\n import datetime as dt\r\n from functools import partial\r\n import gc\r\n+import inspect\r\n from json import loads\r\n import operator\r\n import pickle\r\n@@ -7304,11 +7306,20 @@ class NDFrame(PandasObject, indexing.IndexingMixin):\r\n                 and self._is_view_after_cow_rules()\r\n             ):\r\n                 ctr = sys.getrefcount(self)\r\n                 ref_count = REF_COUNT\r\n                 if isinstance(self, ABCSeries) and _check_cacher(self):\r\n                     # see https:\/\/github.com\/pandas-dev\/pandas\/pull\/56060#discussion_r1399245221\r\n                     ref_count += 1\r\n+                if PY311:\r\n+                    calling_frame = inspect.currentframe().f_back\r\n+                    if _is_using_args_kwargs(calling_frame.f_code):\r\n+                        ref_count += 1\r\n                 if ctr <= ref_count:\r\n                     warnings.warn(\r\n                         _chained_assignment_warning_method_msg,\r\n                         FutureWarning,\r\n```\r\n\r\n<\/details>\r\n"],"labels":["Bug","Copy \/ view semantics"]}]
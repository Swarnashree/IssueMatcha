[{"title":"DOC: undocumented behavior for converting month timedelta64 to days","body":"### Issue with current documentation:\r\n\r\nI have three syntaxes that attempt to convert a timedelta64 in month into days. As is well explained in the documentation:\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/arrays.datetime.html#arrays-datetime\r\nthis is not possible because we  do not know how many days a month holds:\r\n\r\n`np.timedelta64(1, 'M') \/ np.timedelta64(1, 'D')`\r\n`Cannot get a common metadata divisor for Numpy datetime metadata [M] and [D] because they have incompatible nonlinear base time units.`\r\n\r\n`timedelta64(timedelta64(1, 'M'), 'D')`\r\n`Cannot cast NumPy timedelta64 scalar from metadata [M] to [D] according to the rule 'same_kind'`\r\n\r\nbut this yields:\r\n\r\n`timedelta64(1, 'M').astype(timedelta64(1, 'D'))`\r\n`numpy.timedelta64(30,'D')`\r\n\r\nI am surprised that there is no error in the latter case but I am mostly interested to find out which rules are used to determine 30 in this case. For example:\r\n\r\n`timedelta64(3, 'M').astype(timedelta64(1, 'D'))`\r\n`numpy.timedelta64(91,'D')`\r\n\r\nwhich indicates to me that there is some kind of heuristics used but I have not been able to find its documentation.\r\n\r\nThank you for your help,\r\n\r\nSacha\r\n\r\n### Idea or request for content:\r\n\r\n_No response_","comments":[],"labels":["04 - Documentation"]},{"title":"ENH: Clarify error message for invalid array indices","body":"This enhancement updates the error message thrown during array indexing\r\nwith invalid objects. Previously, the message did not explicitly mention\r\nthat tuples of valid indexing objects are also acceptable. This could lead\r\nto confusion, especially when debugging legacy code or for users less\r\nfamiliar with NumPy's indexing rules.\r\n\r\nThe new error message now includes \"tuples of these objects\" to clearly\r\nindicate that tuples containing any combination of valid indices (integers,\r\nslices, ellipsis, numpy.newaxis, and integer or boolean arrays) are valid\r\nindexing methods. This change aims to reduce misunderstanding and improve\r\nthe developer experience by providing more direct guidance on valid\r\nindexing types.\r\n\r\nSee ticket #26115","comments":[],"labels":["01 - Enhancement"]},{"title":"ENH: inherit numerical dtypes from abstract ones.","body":"I couldn't quite stand how we are working around not having abstract dtypes by adding weird promotors in #26105, so thought one should just make them, especially since (as I found out) all the basic infrastructure is there.\r\n\r\nSo, this let's all numerical legacy dtypes inherit from the abstract types defined for python `int`, `float` and `complex`. This follows what was mentioned to be the plan all along in the comments in `abstractdtype`, though it does not yet change the names throughout, since I wanted to be sure first I'm not barking up the wrong tree (it took the better part of the day already, mostly to realize that the abstract dtypes better inherit from `PyArrayDescr_Type`, otherwise nothing works).\r\n\r\nAnyway, with this, the `StringDType` multiplication promotors are now their logical selves, the work-around can be removed, and everything works.","comments":["Nice! Thank you for taking this on. I wanted to but have several other things that are higher priority, in particular opening at least a WIP PR for pandas with benchmarks. I'll try to look over this on Monday if I don't get to it sooner.\r\n\r\nOne thought: might it make sense to define an abstract python string dtype? There are a number of spots it makes sense to define promoters for both bytes and unicode in stringdtype, and third-party libraries might want to write promoters for bytes, unicode, and stringdtype all in one go.","> One thought: might it make sense to define an abstract python string dtype? There are a number of spots it makes sense to define promoters for both bytes and unicode in stringdtype, and third-party libraries might want to write promoters for bytes, unicode, and stringdtype all in one go.\r\n\r\nCertainly for 'U' and 'T', which can just be `PyArray_PyStrAbstractDType` (using the current name; really should just delete the \"Py\" part, I think!). Though definitely in a separate PR!","Nice, there is a wrinkle however that I have to think about:  At this point we are using these dtypes also as pretty much concrete ones in the ufunc machinery.  I.e. we have loops registered that must *only* match Python integers.\r\nSo, while this *was* my original intention always, it clashes with those loops, because with this we have to find a new way make a distinction so that they don't match all integer arrays!\r\nThat might be a second `AnyInt` of which `PyInt` is a subclass (they can be basically the same thing I guess), or maybe we have another idea on the ufunc side?\r\n(E.g. only these are special, if we avoid `->abstract` checks in the lookup, we could maybe even register them with `int` as a type.)\r\n\r\nBecause of that I was wondering about creating a general factory for abstract DTypes where you can `DType.register()` (like a Python ABC).\r\nUnfortunately it is *not* possible to create a subclass of ABCMeta because it has a `__new__`, but C-extension metaclasses don't support `__new__` (you can *only* have factories), that is restriction may be partiall my fault, but it is also just the truth: ABCMeta `__new__` would not be called!  (I actually wanted a way to do this, but I don't think Python has it yet.)\r\n\r\nSo, you can do it, but if you don't want to do C-side monkey-patching to live in a future that doesn't even exist in Python dev yet, you basically have to roll a minimal ABCMeta implementation or copy the Python one.","Aha, I initially thought that this doesn't cause problems yet, but it actually does:\r\n```\r\na = np.ones(10000, dtype=np.int64)\r\nb = np.ones(10000, dtype=np.int32)\r\n%timeit a == b\r\n```\r\nis more than 10x slower in the current setup because it is promoted to `O,O->?` rather than `int64,int64->?`.\r\n\r\nOn the plus side, at least that gives us a clear example of what we are dealing with.  Thoughs:\r\n* The pyint->object promoter *could* be a loop also not a promoter (maybe slightly tedious but not a big deal).  That only helps if either:\r\n  1. matching behavior differs for promoters vs. arraymethods.\r\n  2. that loop needs to truly deal with all combinations (we still have the option to \"normalize\" away a python integer here for a loop that doesn't implement the scalar special path).\r\n* (additionally to 2. above, I think) we can probably say that the promoter is just wrong, it should just do the right promotion for *any* integer dtype, or maybe it needs the `matches` return capability from the other PR to say that it really only cares about the actual python scalars.\r\n\r\nNone of that seems quite ideal, although, if we accept the \"dual\" meaning slighty maybe the setup is pretty decent as it is.  We may want the \"matches\" and we probably want that point 2., but with that it seems we can tweak things enough that the exact scalar match doable, but you don't have to worry about it normally.\r\n\r\n---\r\n\r\n**EDIT: I should say that the changes here are great and all look good!  The only issue is this subtle problem of \"dual use\" when registering vs. what is passed in as the loop DType.**","I've also been looking at consequences, but started with `StringDType` itself. There, `PyArray_PyIntAbstractDType` was actually used as the catch-all for general integers for `find`, `startswith`, and `replace` (and reverse\/similar versions), so those don't work with numpy ints or arrays in current master, but do work with this PR - I added some tests that now succeed.","I need to think a bit about the equality example - would some of this be solved by your earlier suggestion, of the python int being its own \"`PyArray_PyIntDType`\" dtype that also inherits from `PyArray_IntAbstractDType`?","> of the python int being its own \"PyArray_PyIntDType\" dtype that also inherits from PyArray_IntAbstractDType?\r\n\r\nYes, it would solve this (either with or without also inheriting).  We could make that part a fallback like the reduction case: if nothing was found for it, `PyIntDType` is promoted to the default integer and we try again.","> OK, by going through all places where NPY_DT_is_abstract was used, I found the two where really that was just a short-cut for \"could be python scalar\"\r\n\r\nSorry for not getting back earlier:  That sounds fine, I was considering that they maybe shouldn't be marked \"abstract\".  In a sense they still are, as you say, they don't have any instance you can attach to an array.\r\nI think renaming them is fine, we don't have to talk about abstract vs. not abstract TBH for now.  To some degree maybe we don't even need the distinction (I think I had used it for common-dtype logic, but somehow it didn't turn out helpful).  In other words it migth make more sense to use it only for \"only has subclasses that are attachable\".\r\n\r\n> Also, I think it would still be good to have this in 2.0\r\n\r\nYes, I tend to agree: it should have no user-facing changes, but could be significant enough help for dtypes.\r\n\r\n> Note that there is a comment in the file that there is only one free slot, so if we need to expose all 6, then I need some help in how to adjust things\r\n\r\nAck...  that is annoying.  Actually, I think we can just place them in the normal table the same as:\r\n```\r\n    'PyArrayDescr_Type':                (3, \"PyArray_DTypeMeta\"),\r\n```\r\nIn principle the other ones could be \"moved\" over also, but that block is not bad there either.\r\n\r\n---\r\n\r\nAnyway, will have to look closer at it soon, but thanks a lot for looking into this, I had gotten a bit lost in having some that support `.register()`, etc. but this solves a lot and is much simpler.  "],"labels":["01 - Enhancement","component: numpy.dtype"]},{"title":"BUG: Exception text when slicing with an invalid object does not report tuple as an option","body":"### Describe the issue:\n\nWhen slicing an array with an invalid object, like a list, the exception text is:\r\n\r\n\"IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\"\r\n\r\nwhich is generated on line 606 of _core\/src\/multiarray\/mapping.c.\r\n\r\nThis was appropriate when lists of indices were accepted or detected as deprecated, but is misleading now. Would it be better to report something like the following:\r\n\r\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`), **tuples of these objects**, and integer or boolean arrays are valid indices\n\n### Reproduce the code example:\n\n```python\nimport numpy\r\n\r\nmyarray = numpy.zeros((5,5))\r\n\r\n# generates misleading exception\r\nslice = myarray[[slice(None),slice(None)]]\r\n\r\n# correct\r\nslice = myarray[(slice(None),slice(None))]\n```\n\n\n### Error message:\n\n```shell\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n```\n\n\n### Python and NumPy Versions:\n\nPython version: all\r\n\r\nNumpy version: all up to current main branch (commit 20185fd).\r\n\r\n\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\nSeeing the exception, especially debugging legacy code that uses lists of slices, it's easy to waste time checking the individual indices, rather than the acutal indexing object.","comments":[],"labels":["00 - Bug"]},{"title":"BLD: Do not use -O3 flag when building in debug mode","body":"Dispatch sources currently get built with -O3 even in debug mode which made debugging in gdb hard. \r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":[],"labels":["16 - Development","36 - Build"]},{"title":"MAINT: Remove unnecessarily defensive code from dlpack deleter","body":"This is a tp_dealloc, they don't need to care about errors. Even the assert is absurdly defensive:  Python will already set a SystemError anyway!","comments":["This seems like the kind of thing that's useful to do, but shouldn't be backported?","Agreed no need to backport.  I don't care if it is either, though (both simple and also earl enough).","> but shouldn't be backported?\r\n\r\nIf it isn't too risky and is very simple, it can be useful to keep code common in the release branch."],"labels":["03 - Maintenance","09 - Backport-Candidate"]},{"title":"f2py fails on Windows - wrong path in meson","body":"### Steps to reproduce:\n\nHi! I try to compile Fortran extension with f2py, but it fails with strange error. The crucial lines in output are:\r\n```\r\nProgram C:      ools\\miniforge3\\envs\\py12\\python.exe found: NO\r\n\r\nmeson.build:11:22: ERROR: C:    ools\\miniforge3\\envs\\py12\\python.exe not found\r\n```\r\nIt seems that \"\\t\" is replaced with tabulator at some point, and path is not found. Is this numpy or meson bug?\n\n### Error message:\n\n```shell\n(py12) PS C:\\Users\\mwojc\\git\\ffnet\\ffnet\\fortran> f2py.exe -m '_ffnet' -c 'ffnet.f'\r\nCannot use distutils backend with Python>=3.12, using meson backend instead.\r\nUsing meson backend\r\nWill pass --lower to f2py\r\nSee https:\/\/numpy.org\/doc\/stable\/f2py\/buildtools\/meson.html\r\nReading fortran codes...\r\n        Reading file 'ffnet.f' (format:fix,strict)\r\nPost-processing...\r\n        Block: _ffnet\r\n                        Block: prop\r\n                        Block: sqerror\r\n                        Block: grad\r\n                        Block: recall\r\n                        Block: diff\r\n                        Block: func\r\n                        Block: func2\r\n                        Block: pikaiaff\r\n                        Block: normcall\r\n                        Block: normdiff\r\n                        Block: normcall2\r\n                        Block: normdiff2\r\n                        Block: momentum\r\n                        Block: rprop\r\n                        Block: setin\r\n                        Block: getout\r\n                        Block: mapa\r\n                        Block: dmapa\r\n                        Block: vmapa\r\n                        Block: mmapa\r\nApplying post-processing hooks...\r\n  character_backward_compatibility_hook\r\nPost-processing (stage 2)...\r\nBuilding modules...\r\n    Building module \"_ffnet\"...\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"prop\"...\r\n          units = prop(x,conec,units,[n,u])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"sqerror\"...\r\n          sqerr = sqerror(x,conec,units,inno,outno,input,targ,[n,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"grad\"...\r\n          xprime = grad(x,conec,bconecno,units,inno,outno,input,targ,[n,bn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"recall\"...\r\n          output = recall(x,conec,units,inno,outno,input,[n,u,i,o])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"diff\"...\r\n          deriv = diff(x,conec,dconecno,dconecmk,units,inno,outno,input,[n,dn,u,i,o])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"func\"...\r\n          sqerr = func(x,conec,bconecno,units,inno,outno,input,targ,[n,bn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"func2\"...\r\n          sqerr,xprime = func2(x,conec,bconecno,units,inno,outno,input,targ,[n,bn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"pikaiaff\"...\r\n          isqerr = pikaiaff(x,ffn,conec,units,inno,outno,input,targ,bound1,bound2,[n,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"normcall\"...\r\n          output = normcall(x,conec,units,inno,outno,eni,deo,input,[n,u,i,o])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"normdiff\"...\r\n          deriv = normdiff(x,conec,dconecno,dconecmk,units,inno,outno,eni,ded,input,[n,dn,u,i,o])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"normcall2\"...\r\n          output = normcall2(x,conec,units,inno,outno,eni,deo,input,[n,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"normdiff2\"...\r\n          deriv = normdiff2(x,conec,dconecno,dconecmk,units,inno,outno,eni,ded,input,[n,dn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"momentum\"...\r\n          x = momentum(x,conec,bconecno,units,inno,outno,input,targ,eta,moment,maxiter,[n,bn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"rprop\"...\r\n          x,xmi = rprop(x,conec,bconecno,units,inno,outno,input,targ,a,b,mimin,mimax,xmi,maxiter,[n,bn,u,i,o,p])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"setin\"...\r\n          units = setin(input,inno,eni,units,[i,u])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"getout\"...\r\n          output = getout(units,outno,deo,[u,o])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n                Creating wrapper for Fortran function \"mapa\"(\"mapa\")...\r\n        Constructing wrapper function \"mapa\"...\r\n          mapa = mapa(f,a,b,c,d)\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n                Creating wrapper for Fortran function \"dmapa\"(\"dmapa\")...\r\n        Constructing wrapper function \"dmapa\"...\r\n          dmapa = dmapa(f,a,b,c,d)\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"vmapa\"...\r\n          vout = vmapa(vin,a,b,c,d,[n])\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"_ffnet-f2pywrappers.f\"\r\n        Constructing wrapper function \"mmapa\"...\r\n          mmout = mmapa(mmin,a,b,c,d,[m,n])\r\n    Wrote C\/API module \"_ffnet\" to file \".\\_ffnetmodule.c\"\r\n    Fortran 77 wrappers are saved to \".\\_ffnet-f2pywrappers.f\"\r\nThe Meson build system\r\nVersion: 1.4.0\r\nSource dir: C:\\Users\\mwojc\\AppData\\Local\\Temp\\tmp1axuanha\r\nBuild dir: C:\\Users\\mwojc\\AppData\\Local\\Temp\\tmp1axuanha\\bbdir\r\nBuild type: native build\r\nProject name: _ffnet\r\nProject version: 0.1\r\nFortran compiler for the host machine: gfortran (gcc 5.3.0 \"GNU Fortran (Rev5, Built by MSYS2 project) 5.3.0\")\r\nFortran linker for the host machine: gfortran ld.bfd 2.25.1\r\nC compiler for the host machine: cc (gcc 5.3.0 \"cc (Rev5, Built by MSYS2 project) 5.3.0\")\r\nC linker for the host machine: cc ld.bfd 2.25.1\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram C:      ools\\miniforge3\\envs\\py12\\python.exe found: NO\r\n\r\nmeson.build:11:22: ERROR: C:    ools\\miniforge3\\envs\\py12\\python.exe not found\r\n\r\nA full log can be found at C:\\Users\\mwojc\\AppData\\Local\\Temp\\tmp1axuanha\\bbdir\\meson-logs\\meson-log.txt\r\nTraceback (most recent call last):\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Scripts\\f2py-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\site-packages\\numpy\\f2py\\f2py2e.py\", line 766, in main\r\n    run_compile()\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\site-packages\\numpy\\f2py\\f2py2e.py\", line 738, in run_compile\r\n    builder.compile()\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\site-packages\\numpy\\f2py\\_backends\\_meson.py\", line 178, in compile\r\n    self.run_meson(self.build_dir)\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\site-packages\\numpy\\f2py\\_backends\\_meson.py\", line 171, in run_meson\r\n    self._run_subprocess_command(setup_command, build_dir)\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\site-packages\\numpy\\f2py\\_backends\\_meson.py\", line 167, in _run_subprocess_command\r\n    subprocess.run(command, cwd=cwd, check=True)\r\n  File \"C:\\tools\\miniforge3\\envs\\py12\\Lib\\subprocess.py\", line 571, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['meson', 'setup', 'bbdir']' returned non-zero exit status 1.\n```\n\n\n### Additional information:\n\n_No response_","comments":["NumPy issue: The template building doesn't account for escapes for example [here](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/f2py\/_backends\/_meson.py#L77).  Not immediately sure what the pattern for that is normally.  Maybe using `{!r}` rather than `'{}'` (although may need an additional `str()`)?","Actually, maybe the solution is to use triple quotes everywhere strings are pasted?  Since it sounds like that behaves like [raw strings](https:\/\/mesonbuild.com\/Release-notes-for-0-46-0.html#string-escape-character-sequence-update) in meson.\r\n(I suppose that'll still fail if someone has a file which contains triple quotes :))","I get this error too. Minimal reproduce examples: \r\nhttps:\/\/github.com\/Serge3leo\/numpy-f2py-240323\/tree\/pip-bug (pip)\r\nhttps:\/\/github.com\/Serge3leo\/numpy-f2py-240323 (miniconda)\r\n\r\nWorkaround:\r\n```\r\nff=`python -c \"import numpy.f2py; print(numpy.f2py.__file__)\"`\r\nmbt=`dirname $ff`\/_backends\/meson.build.template\r\n\r\nsed -e \"\/import('python').find_installation(\/s\/'\\${python}', \/\/\" -i -- \"$mbt\"\r\n```\r\n\r\nDiff:\r\n```\r\n--- \"C:\\\\Miniconda\\\\envs\\\\test\\\\Lib\\\\site-packages\\\\numpy\\\\f2py\/_backends\/meson.build.template.orig\"\t2024-02-06 13:27:04.925110800 +0000\r\n+++ \"C:\\\\Miniconda\\\\envs\\\\test\\\\Lib\\\\site-packages\\\\numpy\\\\f2py\/_backends\/meson.build.template\"\t2024-03-23 20:07:44.848754500 +0000\r\n@@ -8,7 +8,7 @@\r\n                           ])\r\n fc = meson.get_compiler('fortran')\r\n \r\n-py = import('python').find_installation('${python}', pure: false)\r\n+py = import('python').find_installation(pure: false)\r\n py_dep = py.dependency()\r\n \r\n incdir_numpy = run_command(py,\r\n```"],"labels":["component: numpy.f2py","32 - Installation","Meson"]},{"title":"DOC: Clarify randint and integers doc","body":"Improve doc strings to include multiple signatures Clean up docstring generally\r\n\r\ncloses #9573\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Need to see how this renders in the docbuild.\r\n","Doesn't render correctly. The first docstring line gets appended to the signature. This might be a numpydoc problem, not sure how multiple signatures should be marked up.","Maybe @rossbar has a suggestion (or maybe it should also use `[]` style in the signature)."],"labels":["04 - Documentation","09 - Backport-Candidate"]},{"title":"API: Default to hidden visibility for API tables","body":"This adds a new ``NPY_API_SYMBOL_ATTRIBUTE`` but defaults to using hidden visibility (always the case on windows!).\r\n\r\nThat actually makes the situation \"worse\" for ``eigenpy`` in some degree, since it forces them to adapt, but it also allows them to decide to just roll with it (by actually also exporting it on windows which).\r\n\r\nSince it aligns windows and linux it seems like a good idea?  OTOH, who knows how many projects get away with just not caring about windows...\r\n\r\nI tried to reorganize the docs a bit on how to import the array API...\r\n\r\n\r\n---\r\n\r\n**Should be ready, but for discussion: we have to make a decision that this is better and ideally improves the ``eigenpy`` situation (if just be clarifying it).  EDIT: See gh-26098 and gh-26091.**","comments":["This should be backported?","Maybe?  It only really matters if we think it helps clarify the situation for 2.0 and I am not certain.\r\nI.e. it forces\/nudges users to not share the API table between shared libraries.  Something that is shaky on windows and could be an anti-pattern if the shared libraries are not all part of one project.\r\n\r\nMaybe @mattip has an opinion on this?\r\n\r\nOtherwise, I think this may be nice but if it causes breakage it is probably niche enough that we can get away with it in a 2.1 release.","Sounds like it needs more experimentation, so 2.1 is probably the better option.","This seems like a good idea to me. It prevents potential problems I think, and having Windows and other OSes behave consistently here has to be better.\r\n\r\nRegarding symbol visibility, I'll note that:\r\n- Meson turns on `-fvisibility=hidden` (`inlineshidden` for C++) by default: https:\/\/mesonbuild.com\/Python-module.html#methods. Extension modules should only need the `PyInit_xxx` symbol. \r\n- SciPy uses a linker script in addition to really hide everything to the extent possible (originally implemented by Pauli to deal with symbol clashes in some corner cases): https:\/\/github.com\/scipy\/scipy\/blob\/7b3831a394d012176f95596df971b8bb7248d6c0\/meson.build#L119-L131","Ah nice point, maybe another reason that adding it explicitly may be good to make downstream setups not accidentally rely on somtehing that is brittle (by build setup\/system rules).\r\n(I.e. if we create pain now, at least we are likely to safe pain elsewhere because eventually someone will run into it.)\r\n\r\n> Meson turns on -fvisibility=hidden (inlineshidden for C++) \r\n\r\nI guess we don't actually use it?  Would be a nice cleanup (not *now*!) to remove all the `NPY_NO_EXPORT` and only mark the functions that we do want to export."],"labels":["30 - API"]},{"title":"BUG: Always pass explicit copy arg to `__array__`","body":"Addresses https:\/\/github.com\/numpy\/numpy\/issues\/25941#issuecomment-2010343170\r\n\r\nHi @ngoldbaum,\r\n\r\nWith this PR, explicit `copy` argument is always passed to `__array__`. \r\n`NPY_ARRAY_ENSURECOPY` check was missing from flags processing, to cover all supported values for `copy`:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/7f1c8cbe0c6fa4e554b3b1e4d7dc2f03fababbec\/numpy\/_core\/src\/multiarray\/ctors.c#L2417-L2420","comments":["You need to be very careful here.  Right now:\r\n* The ensure-copy flag seems still set, so I assume another copy is guaranteed to be made even if `copy=True` was passed and honored.\r\n* If `copy=True` is passed but *not yet supported*, you retry without it, but that code doesn't enforce a copy at all.\r\n* (I am not sure that the _other_ code paths, e.g. conversion from buffers, ensure the copy correctly either, or if recursive calls in the discovery disable `copy` since a copy has to be made in either case.)\r\n\r\nThis would also significantly escalate the amount of `DeprecationWarnings` we see, so I am not sure I think it is worth to backporting (if that was the plan)?","> If copy=True is passed but not yet supported, you retry without it, but that code doesn't enforce a copy at all.\r\n\r\nEDIT: deleted a bad idea I had here originally\r\n\r\nAlso would you be up for adding docs for implementers on how to handle the `copy` keyword? For `copy=True`, we should tell implementers they need to make a copy if `copy=True` because in the future numpy will stop making copies. If for some reason it's impossible to copy or copying doesn't make sense, then an error has to be raised. Here \"copying\" specifically means that `__array__` should return a new numpy array that owns its own data. That means any array-like defining `__array__`  that stores its data outside of a numpy array *has* to always copy in `__array__` and therefore should probably always error if `copy=False` is passed.","> This would also significantly escalate the amount of DeprecationWarnings we see, so I am not sure I think it is worth to backporting (if that was the plan)?\r\n\r\nAh this is a good point. If you run the scipy or scikit-learn tests with this PR applied, do you see a bunch of new deprecation warnings? I think I agree with Sebastian here, we\u2019re going to explicitly pass a `copy` keyword argument more often with this change, since before if someone did:\r\n\r\n```\r\nnp.array(arraylike_that_doesnt_implement_copy, copy=True)\r\n```\r\n\r\nThe `copy` keyword wouldn\u2019t get passed in to `__array__` at all, because we were implicitly passing `None` by ignoring `True`, and for the default case we don\u2019t pass the keyword at all.\r\n\r\nSo another way to generating a bunch of new unnecessary downstream warnings or errors would be to only print the deprecation warning if someone explicitly passed in `copy=False`. For `copy=True` most libraries were already doing the correct thing, and if they don\u2019t make a copy then numpy will. In the future we could deprecate not having `copy` and `dtype` at all, and not just when `copy=False` is passed once there\u2019s more widespread availability in downstream implementations.","It occurs to me we should also update the release notes discussion for this change if we want it to be in the 2.0 release.","I had the impression that the comment this PR is trying to address was quite minor, and non-critical for 2.0. What was critical for 2.0 was updating the docs. And then for 2.1 we can work on the change needed to ensure that there will never be two copies. Does that sound right?","> I had the impression that the comment this PR is trying to address was quite minor, and non-critical for 2.0\r\n\r\nRight, I agree.  The only thing that may be nice for 2.0 is adding a single half sentence to the documentation saying: `copy=True` *must* ensure a copy is returned (however, as of 2.0, NumPy never passes `True`!).\r\nAnything more is unnecessary and would create more churn (and in the first PR we said we should avoid exactly that churn to force quick `copy=` adoption).\r\n\r\nHowever, there is something else here (should maybe make a new issue):\r\n```\r\nclass m:\r\n    def __array__(self):\r\n        return np.arange(3)\r\n\r\nnp.array(m(), copy=False)\r\n```\r\nGives:\r\n```\r\n<ipython-input-3-a3c23c535a07>:1: UserWarning: __array__ should implement 'dtype' and 'copy' keywords\r\n  np.array(m(), copy=False)\r\nOut[3]: array([0, 1, 2])\r\n```\r\nWhich seems wrong.  It must fail (the `copy=False` was clearly not honored) with an error that suggest that the user probably meant `asarray` (`copy=None`), but if not the `__array__` needs to implement the `copy` kwarg.\r\n\r\nI am not even sure we need the warning in NumPy 2.0 (i.e. this PR needs it but `copy=False` fails anyway, and `copy=None` doesn't pass it), but if we have it would be better as a `DeprecationWarning`?"],"labels":["00 - Bug"]},{"title":"BUG:High CPU load with periodical matrix calculations","body":"### Describe the issue:\n\nHigh CPU load when using simple numpy matrix operation periodically\r\nIn the example i am only calculating one matrix inverse every 20ms but I have 3\/8 CPU cores under full load at all times\n\n### Reproduce the code example:\n\n```python\nimport time\r\nimport numpy\r\nimport psutil\r\n\r\ndef test_numpy():\r\n    N = 100\r\n    data = [numpy.random.rand(4, 4) for _ in range(N)]\r\n    cpu = 0\r\n    for m in data:\r\n        numpy.linalg.inv(m)\r\n        time.sleep(0.05)\r\n        cpu += psutil.cpu_percent()\r\n    print(f\"\\ncpu core usage: {round(cpu \/ N * psutil.cpu_count())}%\")\n```\n\n\n### Error message:\n\n```shell\nNo error. But very high CPU load on the overall system even for small calculations\n```\n\n\n### Python and NumPy Versions:\n\n1.24.4\r\n3.11.8\n\n### Runtime Environment:\n\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/jan\/dev\/pyurctrl\/build\/venv\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\n\n### Context for the issue:\n\nperiodic calculations on robot telemetry are slowing down the overall system performance dramatically. ","comments":["This will be caused by the overhead of the 'timing' loop you created, not the cost of a numpy calculation.\r\n\r\n```\r\nimport numpy as np\r\ndata = np.random.rand(4, 4)\r\n%timeit np.linalg.inv(data)\r\n```\r\ngives\r\n```\r\n2.51 \u00b5s \u00b1 34 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n```\r\nso any extra delay\/slowdown is the print, asking for cpu load, and the sleep.\r\n","this is just a minimal example. in my real application i am experiencing the same while only calculating after receiving data. \r\nusing scipy.linalg.inv i have a total cpu core load of 10-20% in this application. using numpy.linalg.inv the total cpu core load is between 280-320% and my cpu fans are spinning up","![image](https:\/\/github.com\/numpy\/numpy\/assets\/45814838\/df2420f3-c2f4-49aa-b7eb-146e8b0a36f2)\r\n\r\ni can also see this behavior inside htop. without the printing and psuti.\r\nTime.sleep should not contribute to cpu load at all\r\n\r\nHere is the same using scipy:\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/45814838\/bc428975-7363-4426-b9ab-2e5d8b1544e1)\r\n","What version of blas are you using in scipy and numpy, i.e. what does this output?\r\n```\r\nscipy.show_config('dicts')['Build Dependencies']['blas']\r\nnumpy.show_config('dicts')['Build Dependencies']['blas']\r\n```"],"labels":["00 - Bug"]},{"title":"BUG: test_lazy_load raises RecursionError with Python 3.13.0a5","body":"### Describe the issue:\n\nI try to build numpy 1.26.4 in Fedora Linux with Python 3.13.0a5 containing the fix for `@LIBPYTHON@` usage (https:\/\/github.com\/python\/cpython\/pull\/116746). test_lazy_load unexpectedly raises Recursion Error.\n\n### Reproduce the code example:\n\n```python\nnone\n```\n\n\n### Error message:\n\n```shell\n________________________________ test_lazy_load ________________________________\r\n\r\n    @pytest.mark.filterwarnings(\"ignore:The NumPy module was reloaded\")\r\n    def test_lazy_load():\r\n        # gh-22045. lazyload doesn't import submodule names into the namespace\r\n        # muck with sys.modules to test the importing system\r\n        old_numpy = sys.modules.pop(\"numpy\")\r\n    \r\n        numpy_modules = {}\r\n        for mod_name, mod in list(sys.modules.items()):\r\n            if mod_name[:6] == \"numpy.\":\r\n                numpy_modules[mod_name] = mod\r\n                sys.modules.pop(mod_name)\r\n    \r\n        try:\r\n            # create lazy load of numpy as np\r\n            spec = find_spec(\"numpy\")\r\n            module = module_from_spec(spec)\r\n            sys.modules[\"numpy\"] = module\r\n            loader = LazyLoader(spec.loader)\r\n            loader.exec_module(module)\r\n            np = module\r\n    \r\n            # test a subpackage import\r\n>           from numpy.lib import recfunctions\r\n\r\nloader     = <importlib.util.LazyLoader object at 0x7f07a3f64c80>\r\nmod        = <module 'pydoc' from '\/usr\/lib64\/python3.13\/pydoc.py'>\r\nmod_name   = 'pydoc'\r\nmodule     = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\nnp         = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\nnumpy_modules = {'numpy.__config__': <module 'numpy.__config__' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/p...lddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/core\/_dtype_ctypes.py'>, ...}\r\nold_numpy  = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\nspec       = ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, origin='\/build...ule_search_locations=['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy'])\r\n\r\n..\/..\/..\/..\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/tests\/test_lazyloading.py:30: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n<frozen importlib._bootstrap>:1360: in _find_and_load\r\n    ???\r\n        import_    = <built-in function __import__>\r\n        module     = <object object at 0x7f07ea940060>\r\n        name       = 'numpy.lib'\r\n<frozen importlib._bootstrap>:1316: in _find_and_load_unlocked\r\n    ???\r\n        import_    = <built-in function __import__>\r\n        name       = 'numpy.lib'\r\n        parent     = 'numpy'\r\n        parent_module = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\n        parent_spec = None\r\n        path       = None\r\n<frozen importlib.util>:209: in __getattribute__\r\n    ???\r\n        __dict__   = {'AxisError': <class 'numpy.exceptions.AxisError'>, 'ComplexWarning': <class 'numpy.exceptions.ComplexWarning'>, 'Modu...ng': <class 'numpy.exceptions.ModuleDeprecationWarning'>, 'TooHardError': <class 'numpy.exceptions.TooHardError'>, ...}\r\n        __spec__   = ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, origin='\/build...ule_search_locations=['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy'])\r\n        attr       = '__path__'\r\n        attrs_now  = {'AxisError': <class 'numpy.exceptions.AxisError'>, 'ComplexWarning': <class 'numpy.exceptions.ComplexWarning'>, 'Modu...ng': <class 'numpy.exceptions.ModuleDeprecationWarning'>, 'TooHardError': <class 'numpy.exceptions.TooHardError'>, ...}\r\n        attrs_then = {'__cached__': '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__pycach...packages\/numpy\/__init__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, ...}\r\n        attrs_updated = {}\r\n        key        = '__cached__'\r\n        loader_state = {'__class__': <class 'module'>, '__dict__': {'__cached__': '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/l...t 0x7f07a3e33c20>, ...}, 'is_loading': True, 'lock': <unlocked _thread.RLock object owner=0 count=0 at 0x7f07a3492740>}\r\n        original_name = 'numpy'\r\n        self       = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\n        value      = '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__pycache__\/__init__.cpython-313.pyc'\r\n..\/..\/..\/..\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py:113: in <module>\r\n    from . import version\r\n        AxisError  = <class 'numpy.exceptions.AxisError'>\r\n        ComplexWarning = <class 'numpy.exceptions.ComplexWarning'>\r\n        ModuleDeprecationWarning = <class 'numpy.exceptions.ModuleDeprecationWarning'>\r\n        TooHardError = <class 'numpy.exceptions.TooHardError'>\r\n        VisibleDeprecationWarning = <class 'numpy.exceptions.VisibleDeprecationWarning'>\r\n        _CopyMode  = <enum '_CopyMode'>\r\n        _NoValue   = <no value>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__pycache__\/__init__.cpython-313.pyc'\r\n        __doc__    = '\\nNumPy\\n=====\\n\\nProvides\\n  1. An array object of arbitrary homogeneous items\\n  2. Fast mathematical operations ov...en\\navailable as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\\nExceptions to this rule are documented.\\n\\n'\r\n        __file__   = '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>\r\n        __name__   = 'numpy'\r\n        __package__ = 'numpy'\r\n        __path__   = ['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy']\r\n        __spec__   = ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, origin='\/build...ule_search_locations=['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy'])\r\n        _globals   = <module 'numpy._globals' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/_globals.py'>\r\n        _utils     = <module 'numpy._utils' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/_utils\/__init__.py'>\r\n        exceptions = <module 'numpy.exceptions' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/exceptions.py'>\r\n        sys        = <module 'sys' (built-in)>\r\n        warnings   = <module 'warnings' from '\/usr\/lib64\/python3.13\/warnings.py'>\r\n<frozen importlib._bootstrap>:1412: in _handle_fromlist\r\n    ???\r\n        fromlist   = ('version',)\r\n        import_    = <built-in function __import__>\r\n        module     = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\n        recursive  = False\r\n        x          = 'version'\r\n<frozen importlib.util>:209: in __getattribute__\r\n    ???\r\n        __dict__   = {'AxisError': <class 'numpy.exceptions.AxisError'>, 'ComplexWarning': <class 'numpy.exceptions.ComplexWarning'>, 'Modu...ng': <class 'numpy.exceptions.ModuleDeprecationWarning'>, 'TooHardError': <class 'numpy.exceptions.TooHardError'>, ...}\r\n        __spec__   = ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, origin='\/build...ule_search_locations=['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy'])\r\n        attr       = 'version'\r\n        attrs_now  = {'AxisError': <class 'numpy.exceptions.AxisError'>, 'ComplexWarning': <class 'numpy.exceptions.ComplexWarning'>, 'Modu...ng': <class 'numpy.exceptions.ModuleDeprecationWarning'>, 'TooHardError': <class 'numpy.exceptions.TooHardError'>, ...}\r\n        attrs_then = {'__cached__': '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__pycach...packages\/numpy\/__init__.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, ...}\r\n        attrs_updated = {'AxisError': <class 'numpy.exceptions.AxisError'>, 'ComplexWarning': <class 'numpy.exceptions.ComplexWarning'>, 'Modu...ng': <class 'numpy.exceptions.ModuleDeprecationWarning'>, 'TooHardError': <class 'numpy.exceptions.TooHardError'>, ...}\r\n        key        = 'AxisError'\r\n        loader_state = {'__class__': <class 'module'>, '__dict__': {'__cached__': '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/l...t 0x7f07a3e33c20>, ...}, 'is_loading': True, 'lock': <unlocked _thread.RLock object owner=0 count=0 at 0x7f07a3492740>}\r\n        original_name = 'numpy'\r\n        self       = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\n        value      = <class 'numpy.exceptions.AxisError'>\r\n..\/..\/..\/..\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py:113: in <module>\r\n    from . import version\r\n        AxisError  = <class 'numpy.exceptions.AxisError'>\r\n        ComplexWarning = <class 'numpy.exceptions.ComplexWarning'>\r\n        ModuleDeprecationWarning = <class 'numpy.exceptions.ModuleDeprecationWarning'>\r\n        TooHardError = <class 'numpy.exceptions.TooHardError'>\r\n        VisibleDeprecationWarning = <class 'numpy.exceptions.VisibleDeprecationWarning'>\r\n        _CopyMode  = <enum '_CopyMode'>\r\n        _NoValue   = <no value>\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__pycache__\/__init__.cpython-313.pyc'\r\n        __doc__    = '\\nNumPy\\n=====\\n\\nProvides\\n  1. An array object of arbitrary homogeneous items\\n  2. Fast mathematical operations ov...en\\navailable as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\\nExceptions to this rule are documented.\\n\\n'\r\n        __file__   = '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>\r\n        __name__   = 'numpy'\r\n        __package__ = 'numpy'\r\n        __path__   = ['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy']\r\n        __spec__   = ModuleSpec(name='numpy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f07a3e33c20>, origin='\/build...ule_search_locations=['\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy'])\r\n        _globals   = <module 'numpy._globals' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/_globals.py'>\r\n        _utils     = <module 'numpy._utils' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/_utils\/__init__.py'>\r\n        exceptions = <module 'numpy.exceptions' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/exceptions.py'>\r\n        sys        = <module 'sys' (built-in)>\r\n        warnings   = <module 'warnings' from '\/usr\/lib64\/python3.13\/warnings.py'>\r\n<frozen importlib._bootstrap>:1412: in _handle_fromlist\r\n    ???\r\nE   RecursionError: maximum recursion depth exceeded\r\n        fromlist   = ('version',)\r\n        import_    = <built-in function __import__>\r\n        module     = <module 'numpy' from '\/builddir\/build\/BUILDROOT\/numpy-1.26.4-2.fc41.x86_64\/usr\/lib64\/python3.13\/site-packages\/numpy\/__init__.py'>\r\n        recursive  = False\r\n        x          = 'version'\r\n!!! Recursion detected (same locals & position)\n```\n\n\n### Python and NumPy Versions:\n\nPython 3.13.0a5, numpy 1.26.4 (from Fedora)\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["What happens when you try to build and test HEAD or 2.0.0b1?","I attempted to build 2.0.0b1 as RPM but it was far from trivial and I didn't succeed. I was also not successful with building numpy locally, because not all of the dependencies are available for Python 3.13, so I can't really tell.","@tacaswell in your numpy `main` builds for Python 3.13, do you see this test failing?","Yes I see this.  I see a total of 9 errors [7 of which are `undefined symbol: _PyErr_WriteUnraisableMsg`, 1 of which is meson, and this] and 95 failures [that all look meson or f2py related].\r\n\r\nPlus having to delete the `numpy\/distutils` directory to get pytest to find the test without import errors.","Great, thanks. Then I guess we have to figure out if this test is not written in a future-proof way, or if it's a regression in `importlib`."],"labels":["00 - Bug"]},{"title":"DOC: confusing documentation for Python level `np.intp` and C-level `npy_intp`","body":"### Issue with current documentation:\r\n\r\nIn #24888 the `npy_intp` type was updated to alias `Py_ssize_t` (instead of `intptr_t` as previously). Its documentation was updated accordingly:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/56dab5053eedbd0a34b8c5aa8bf66c46a8b15cb5\/doc\/source\/reference\/c-api\/dtype.rst?plain=1#L387-L398\r\n\r\nHowever, in the same file, the `NPY_INTP` enumerated type still refers to the equivalence with `intptr_t` (size of `void*`):\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/56dab5053eedbd0a34b8c5aa8bf66c46a8b15cb5\/doc\/source\/reference\/c-api\/dtype.rst?plain=1#L171-L175\r\n\r\nfollowed by:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/56dab5053eedbd0a34b8c5aa8bf66c46a8b15cb5\/doc\/source\/reference\/c-api\/dtype.rst?plain=1#L290-L296\r\n\r\nFurthermore, the documentation for the Python-level dtype with matching name `np.intp` also refers to equivalence with `intptr_t`:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/56dab5053eedbd0a34b8c5aa8bf66c46a8b15cb5\/doc\/source\/reference\/arrays.scalars.rst?plain=1#L376-L384\r\n\r\n\r\n### Idea or request for content:\r\n\r\nSo I am confused:\r\n\r\n- are the documentation sections paragraph for the enumeration value and Python-level dtype object up to date?\r\n- if so, I think it would be important to add a note to make the type discrepancy explicit (while explaining that this should not be a problem to confused `Py_ssize_t` with `intptr_t` on most platforms).\r\n- if this not intended (that is, if the Python dtype and the enumeration value are now all aligned on `Py_ssize_t`\/`npy_intp`), then those paragraphs should be updated.\r\n\r\n\r\nNote: the documentation of `npy_uintp` `np.uintp` would deserve a similar clarification.\r\n","comments":["We talked about this at the triage meeting and were left a little confused. Are there systems we support where `Py_ssize_t` isn't the same size as `intptr_t`?\r\n\r\nWe can add some notes where we talk about how it's the same size as a pointer that this isn't strictly true according to the C standard.","I would expect that the documentation follows the code, i.e., if `np.intp` is defined as `Py_ssize_t` then say so regardless of the fact that on most platforms ist\u2018s the same as `intptr_t`.\r\nLooking at https:\/\/github.com\/numpy\/numpy\/blob\/c3f091d403e0becda4887b2600df89b98f08473b\/numpy\/_core\/include\/numpy\/npy_common.h#L201 this seems the case.\r\n\r\nMy guess is that this part of the doc was just not updated with the change to Py_ssize_t.","> My guess is that this part of the doc was just not updated with the change to Py_ssize_t.\r\n\r\nThat's also my guess, although I am not familiar enough with the code based to understand how `numpy.intp` is define and whether it relates to `npy_intp` by construction or not.","Yes, its the same type, I added `p` for pointer, not that it matters on any typical hardware."],"labels":["04 - Documentation"]},{"title":"BUG: Undefined symbol when compiling shared library for numpy 2.0","body":"### Describe the issue:\r\n\r\nI am getting some undefined symbol error when compiling a shared library (C Python Module) that is linking against yet another shared library (C Python Module), both compiled against Numpy 2.0. Everything was fine before the update of Numpy to 2.0.0b1.\r\n\r\nDoes anyone have any idea regarding what is causing this issue?\r\n\r\n### Reproduce the code example:\r\n\r\nI don't have a MRE. I'm running the following [Github Action pipeline](https:\/\/github.com\/duburcqa\/jiminy\/blob\/dev\/.github\/workflows\/manylinux.yml)\r\n\r\nI could write one if really necessary. Basically I compile the library Eigenpy and then I linked my own module on it. It is the latter that contains undefined symbols.\r\n\r\n### Error message:\r\n\r\n```shell\r\nImportError: \/home\/runner\/.local\/lib\/python3.10\/site-packages\/jiminy_py\/core\/core.cpython-310-x86_64-linux-gnu.so: undefined symbol: EIGENPY_ARRAY_APIPyArray_RUNTIME_VERSION\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\nPython 3.10, Numpy 2.0.0b1\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["> Everything was fine before the update.\r\n\r\nWhat update?","Before the migration to Numpy 2.0.0b1.","It's hard to say without more info and spending a lot of time to understand the build of your project, but one thing to check is if you're using a build of eigen that was compiled against numpy 2.0.","> It's hard to say without more info and spending a lot of time to understand the build of your project\r\n\r\nYes, that is what I was afraid of... I was hoping it could be useful for someone involved in the development of numpy 2.0.\r\n\r\n>  using a build of eigen that was compiled against numpy 2.0.\r\n\r\nI'm building everything myself from source, so yes I'm sure I'm using the exact same numpy version (2.0.0b1).","What I don't understand is that `PyArray_RUNTIME_VERSION` should not be a symbol in the first place no ? By looking at the source code it looks like a preprocessor macro.","It could also be an issue eigen will need to deal with, it looks like it\u2019s trying to re-export the entire numpy C API.","OK I think the difference is here:\r\n\r\n### Numpy 2.0.0b1\r\n\r\n```cpp\r\n#if defined(NO_IMPORT) || defined(NO_IMPORT_ARRAY)\r\nextern void **PyArray_API;\r\nextern int PyArray_RUNTIME_VERSION;\r\n#else\r\n#if defined(PY_ARRAY_UNIQUE_SYMBOL)\r\nvoid **PyArray_API;\r\nint PyArray_RUNTIME_VERSION;\r\n#else\r\nstatic void **PyArray_API = NULL;\r\nstatic int PyArray_RUNTIME_VERSION = 0;\r\n#endif\r\n#endif\r\n```\r\n\r\n### Numpy 1.26.4\r\n\r\n```cpp\r\n#if defined(PY_ARRAY_UNIQUE_SYMBOL)\r\n#define PyArray_API PY_ARRAY_UNIQUE_SYMBOL\r\n#endif\r\n\r\n#if defined(NO_IMPORT) || defined(NO_IMPORT_ARRAY)\r\nextern void **PyArray_API;\r\n#else\r\n#if defined(PY_ARRAY_UNIQUE_SYMBOL)\r\nvoid **PyArray_API;\r\n#else\r\nstatic void **PyArray_API=NULL;\r\n#endif\r\n#endif\r\n```","[Here](https:\/\/github.com\/stack-of-tasks\/eigenpy\/blob\/458c0e195253568a20b4ed1342cd4df28e2373a2\/include\/eigenpy\/numpy.hpp#L11) Is what Eigenpy is doing:\r\n\r\n```cpp\r\n\r\n#ifndef PY_ARRAY_UNIQUE_SYMBOL\r\n#define PY_ARRAY_UNIQUE_SYMBOL EIGENPY_ARRAY_API\r\n#endif\r\n\r\n#include <numpy\/numpyconfig.h>\r\n#ifdef NPY_1_8_API_VERSION\r\n#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\r\n#endif\r\n\r\n#include <numpy\/ndarrayobject.h>\r\n#include <numpy\/ufuncobject.h>\r\n```\r\n\r\nDoes it make sense to you or is it hard to tell ?","The point is that you use Eigens(?) `PY_ARRAY_UNIQUE_SYMBOL` together with `NPY_NO_IMPORT` from a different compilation unit **and** Eigen has not been recompiled with NumPy 2.\r\n\r\nIf that is a serious use-case I have to think a bit how to deal with it.","So eigenpy is defining `PY_ARRAY_UNIQUE_SYMBOL` to be `EIGENPY_ARRAY_API`, then it `#include`'s the numpy headers. `__multiarray_api.h` has this block:\r\n\r\n```\r\n#if defined(PY_ARRAY_UNIQUE_SYMBOL)\r\n    #define PyArray_API PY_ARRAY_UNIQUE_SYMBOL\r\n    #define _NPY_VERSION_CONCAT_HELPER2(x, y) x ## y\r\n    #define _NPY_VERSION_CONCAT_HELPER(arg) \\\r\n        _NPY_VERSION_CONCAT_HELPER2(arg, PyArray_RUNTIME_VERSION)\r\n    #define PyArray_RUNTIME_VERSION \\\r\n        _NPY_VERSION_CONCAT_HELPER(PY_ARRAY_UNIQUE_SYMBOL)\r\n#endif\r\n```\r\n\r\nwhich presumably your build hits.\r\n\r\nI'm not sure offhand if we test this sort of build, ping @seberg since he did this work on adding `PyArray_RUNTIME_VERSION`","> The point is that you use Eigens(?) PY_ARRAY_UNIQUE_SYMBOL together with NPY_NO_IMPORT from a different compilation unit\r\n\r\nIt is indeed defining `NO_IMPORT_ARRAY` [here](https:\/\/github.com\/stack-of-tasks\/eigenpy\/blob\/458c0e195253568a20b4ed1342cd4df28e2373a2\/include\/eigenpy\/fwd.hpp#L87):\r\n```cpp\r\n#define NO_IMPORT_ARRAY\r\n#include \"eigenpy\/numpy.hpp\"\r\n#undef NO_IMPORT_ARRAY\r\n```\r\n\r\n> and Eigen has not been recompiled with NumPy 2.\r\n\r\nEigenpy **has**  been recompiled with NumPy 2.\r\n\r\n> If that is a serious use-case I have to think a bit how to deal with it.\r\n\r\nIt is hard to tell for sure if this is intended or not... But using Eigenpy to expose Eigen for other libraries is definitely a serious usecase. The library is used by pinocchio, hpp-fcl, crocoddyl... and many others INRIA projects.","I think this pattern is coming from `Boost::Python` which introduces it [here](https:\/\/github.com\/boostorg\/python\/blob\/6c3f3ecacf66f61d799d80294bbf59ceb84daf8a\/include\/boost\/python\/numpy\/internal.hpp#L20). Both Eigenpy and my own library are using `Boost::Python` over `pybind11` to generate bindings for historical reasons.","Yeah, but boost python is header only.  While here it seems this isn't a single compilation unit.  At least that is my only explanation.\r\n\r\nThe only way this makes sense to me is that for some reason you are finding an older `eigenpy` version at runtime.  It is not even clear to me that it makes any sense at all for `eigenpy`'s `numpy.hpp` to be public API at all.\r\n\r\nSo, I am not clear whether we need a solution that always works, a work-around for these odder cases where it is necessary for some reason, or nothing at all and some part of the project just has not been recompiled with NumPy 2 (e.g. compilation cache). ","> The only way this makes sense to me is that for some reason you are finding an older eigenpy version at runtime\r\n\r\nIndeed ! This is it... I'm explicitly doing this and I forgot about it... Sorry about that.","Closing then, let us know in a new issue if you have any other issues handling numpy 2.0.",">  It is not even clear to me that it makes any sense at all for eigenpy's numpy.hpp to be public API at all.\r\n\r\nGood point. Is it difficult to make it private API ?\r\n\r\nMaybe I should not support this scenario in the first place, but I would love to be able to let the user use its own version of eigenpy at runtime without causing such undefined symbol error. It was working fine so far.","> Closing then, let us know in a new issue if you have any other issues handling numpy 2.0.\r\n\r\nYou want me to open a new issue to continue this discussion ? If you guys have any advice about how this scenario should be handled.","Ah sorry for closing a little hastily. I thought you didn't want to do this.\r\n\r\nSo I think what will work going forward is to build against numpy 2.0, and then use those builds to support numpy 1.x and numpy 2.0. The numpy ABI is now backward compatible - we can expose older ABIs if you compile against newer numpy. It's not possible for the other way to work, so I don't think what you're trying to do will work with numpy 2.0. Maybe sebastian can comment further.","Yes, this is what I am currently doing, with great success actually ! What does not work is:\r\n* I build eigenpy against numpy 2\r\n* I build my library against numpy 2, and linked with eigenpy too\r\n* At runtime, the user decides to use its own binaries of eigenpy (same version tag obviously) and I want to be able to use its symbol instead of loading my own instance of the shared library concurrently\r\n\r\nThis scenario makes sense in the context of Boost::Python actually, the \"global\" registry of from-to python converter works requires to have a unique instance of boost python shared liabry to be loaded at the same time, otherwise inter-operability between modules would be broken.","> Good point. Is it difficult to make it private API ?\r\n\r\nI don't know?  Within the private API, you *can* still include `ndarraytypes.h` which should give you most things you need (in case you need part of the NumPy stuff to be public).\r\n*Late EDIT: Actually that part isn't quite true :(, exporting any NumPy symbols has issues at last in theory since we everyone to use the headers you used.*\r\n\r\nMost of the stuff in the `numpy.hpp` seems for internal use (e.g. to register new dtypes), but it also is maybe intentional to make `call_PyArray_SimpleNew` public?!  But it is a static inline function so it requires using the NumPy-API via `eigenpy` if used downstream?\r\n\r\n> Maybe I should not support this scenario in the first place\r\n\r\nI don't know.  It seems to me that leaking NumPy API out is bad (i.e. any include of `ndarrayobject.h` should be confined to a private header).  But I am not surprised that this happens.\r\n\r\nWe can easily work around this, by just defining `PyArray_RUNTIME_VERSION` to something else (maybe slightly slower).\r\n\r\nBut a caveat remains: The *caller* of `import_array` promises that every user is NumPy 2 ready (and we assume that any user of NumPy headers calls `import_array` at some point at least to some degree).\r\n\r\nThat is a potential issue, but I guess one we just have to live with, and honestly, one that I suspect nobody is likely to run into in practice.","Thank you for your very insightful rely! Now I understand better how to get around the issue. I will try to propose a MR for eigenpy that makes it private and see if it helps.\r\n\r\n> That is a potential issue, but I guess one we just have to live with, and honestly, one that I suspect nobody is likely to run into in practice.\r\n\r\nI'm not 100% sure if this means that my usecase is not supported, but i can assure you that the problem I'm running into is real. My library is one among many other relying on Eigenpy. Mine is the only one related to INRIA that got some traction. From this perspective, i want to be able to ensure inter-operability with all of them without requiring the user to build my library from source while all the others are already distributed as pre-compiled binaries. It is easy for me to add as requirement a specific eigenpy version, but I have no way to enforce the version of numpy used to compile it. This means that I have to encourage all the maintainers of all these projects to distributed wheels compiled against numpy 2.0 if I want to be able to do the same with mine. That is going to be challenging... i would say a \"dirty\" workaround would be fine for me.","Well we'll need to find a solution whether in NumPy or not, but it is still unclear where\/what that is.\r\n\r\nI think that if you just add a n`#include \"numpy\/ndarrayobject.h\"` before the other includes things will magically work, since you are probably not using the NumPy API (directly) in a way that requires an explicit import (i.e. you use it through a `boost::python::numpy` but that seems its own compilation unit).\r\n\r\n(I.e. you would get a static define for these symbols, and would be expected to call `import_array()`, but since you don't actually need that API at all, things are fine.)\r\n\r\nI still suspect `eigenpy` shouldn't transitively expose the NumPy API (which it does by setting unique-symbol and including `ndarrayobject.h` for you).  And I also don't think it needs to at all.\r\nBut, while I think eigenpy doesn't really need to, I am not sure that no-one else has a better reason.\r\n\r\n(I wish I there was a better way to do this unique symbol dance.)","> i.e. you use it through a boost::python::numpy but that seems its own compilation unit\r\n\r\nUnfortunately, this is a wrong assumption. Both Eigenpy and my own library are using barebone C-API of Numpy because not everything has been exposed in `boost::python::numpy`, typically when it comes to custom scalar type registration or memory alignment handling. ","> Both Eigenpy and my own library are using barebone C-API\r\n\r\nLet me summarize the situation here a bit:\r\n* NumPy is set up to allow a project to have multiple c\/cpp files which share one API table with a single `import_array()` call.  This is the \"Unique Symbol + No Import\" mechanism.\r\n  * This is really mainly meant for multiple C-files which are bundled into a single `.so` I think.  Not really for sharing across library lines (but nothing prevents that).\r\n* `eigenpy` sets the unique symbol, which means it *shares* it's API table with any project that includes it (i.e. at runtime you must find the `eigenpy` `.so`).\r\n  * N.B.: The point of the NumPy setup is exactly to avoid any direct runtime symbol lookup, the API is instead shared through Python API.\r\n\r\nThis means that:\r\n* `eigenpy` *transitively* exposes NumPy C-API to user (although I doubt they need this, i.e. users could include numpy themselves).  *However, `eigenpy` does this even if users do not need any NumPy API.*\r\n* In your situation, you *do* need the NumPy API, and you could take care of the `ndarrayobject.h` and `import_array()` call yourself in principle.  In fact, a (maybe strange) approach would be to:\r\n  * Add `#include \"eigenpy\/numpy.hpp\"` before other includes that might pull in NumPy.\r\n  * Add `if (_import_array() < 0) { throw ... }` somewhere in the module init (where you call `eigenpy::enableEigenPy()`)\r\n  That will define the symbol locally to *your* library\/project, which, while using the same name, should overshadow those from `eigenpy`, I think.\r\n\r\n---\r\n\r\n**\"Simple\" Solution:**  We can easily redefine `PyArray_RUNTIME_VERSION` to be one of:\r\n```\r\n\/\/ First could be 0 in some cases, but its fine 0 means \"old\", second is a function call.\r\n#define PyArray_RUNTIME_VERSION ((int)PyArray_API[1])\r\n#define PyArray_RUNTIME_VERSION PyArray_GetNDArrayCVersion()\r\n```\r\nboth of which should be fine in practice.\r\n\r\n**Problem:**\r\n* Due to this transitive export you never call `import_array()` from *your* library which means you have *no* compatibility checks:\r\n  * If your library is compiled with NumPy 1.x it might not notice it is running in NumPy 2 and just break\/return wrong things.\r\n  * You cannot really use new NumPy API because you never do the runtime check if that API is actually available.\r\n* Unfortunately, I guess the NumPy headers make it somewhat easy to do this even unintended.\r\n\r\nSo we can change the above.  Things will work, but you lose those checks and balances.  I suspect we can live with it, but...\r\n\r\n---\r\n\r\n... What bothers me most, is not what we do right now, but that I would like a way forward so that `eigenpy` stops transitively exporting the NumPy API, ideally without losing anything.\r\n\r\nFor example, a pattern of `#define DEFINE_EIGENPY_ENABLE` which *must* be added to be able to call `eigenpy::enableEigenPy();` and making `eigenpy::enableEigenPy` a **static ~inline~** entry-point which calls `import_array()\/import_numpy()` as static ~inline~ would work.  But in practice, I am not sure if some libraries don't even get away without it entirely, so not sure how well just overshadowing works.","Maybe we should get some eigenpy people in on this: @jcarpent or @jorisv?","Thank you @seberg , all these details are helping a lot. I will wait for a feedback from eigenpy team before deciding going for one direction or the other.","One new observation: Windows *defaults* to private symbols for shared libraries.  Thus, it seems like your code is only working because you *force* static linking of `eigenpy` on windows.\r\nAnd I guess there is a good chance the only reason you do this is the NumPy API use!?","> Windows defaults to private symbols for shared libraries. Thus, it seems like your code is only working because you force static linking of eigenpy on windows.\r\n\r\nIndeed I do, but only for historically reason (it was easier at the beginning to ensure that the C module can be loaded on any machine if it has no external dependency at runtime). Now, I'm \"repairing\" the generated wheel using `machomachomangler` (much like what `auditwheel` does on Linux), so it is not an issue anymore. But since it was working this way, I never tried to go back to shared library on Windows. Especially because the scenario I mentioned before only makes sense on Unix, since everybody that is using Eigenpy for serious research and business is using MacOS or Linux but never Windows, so inter-operability is much less revenant in this case.","(sorry for the long post...)\r\n\r\n> but only for historically reason\r\n\r\nNo, this is the reason for this old PR https:\/\/github.com\/stack-of-tasks\/eigenpy\/commit\/7a57c1983021e4718d769ff2c6d093fbdc7a045d, but that isn't enough at all, since `eigenpy` makes all of it's internals public and they randomly use the NumPy API.  You *must* link `eigenpy` statically on windows unless you either:\r\n* deal with NumPy includes before eigenpy does so.\r\n* Or just happen to not use any API which might need NumPy.\r\n\r\nI also do wonder how much you actually care about this behavior:  The wheels you package must include `eigenpy` so presumably you can make sure it is compiled with the newest NumPy.  `conda-forge`, etc. will also be able to just handle those dependencies, fine I presume.\r\n\r\n---\r\n\r\nFor NumPy, I think we ideally \"break\" `eigenpy` here.  That is, _default_ to the windows behavior and hide the import away (up to you to undo that).  That doesn't make existing usage safer (it could crash in weird ways if not recompiled for NumPy 2).\r\n\r\nNow, since compilers optimize things away that are unused, I could imagine things are actually somewhat fine after that, although it would be nice if `eigenpy` could at least only include `ndarraytypes.h` rather than `(nd)arrayobject.h` if imported as public API (and remove the `#define Py_ARRAY_UNIQUE_SYMBOL` and `#define NPY_NO_IMPORT`).\r\nI don't think that works for all files, but maybe for all files which get included by the `fwd.hpp`.\r\n\r\nThat will mean that _some_ `eigenpy` features will fail to link\/compile.  Most likely nobody will notice that part (you and pinoccio seem to only really call a few initialize functions from it.\r\n\r\nOf course `jiminy-py` uses the NumPy API!  So it'll have to implement the `import_array` call and the unique-symbol, etc. which is currently implicitly done by eigenpy.\r\n*If you add that to `jiminy-py` it will just work perfectly with any `eigenpy` version.*\r\n\r\n---\r\n\r\nIt might be nice to simplify the include pattern a bit at least for C++, I can imagine that would be easy (i.e. a bit similar to what `pybind11` is doing).\r\n\r\nI also wish we had a `numpy\/stubs.h` header so that a project like `eigenpy` could define its function with `struct PyArray_Descr *func(struct PyArrayObject *arr)` without including the full `ndarraytypes.h` header.","> You must link eigenpy statically on windows\r\n\r\nYes, I agree with you. What I meant is that I just never tried to link dynamically because I wanted to link statically from the start to improve portability. I never had in mind to link dynamically. But yes for sure, it would not work anyway.","I am considering escalating this a bit for you (because I think it clarifies things and is generally better), see gh-26103.\r\nI.e. breaking the linking always, unless you add a define.  I am not sure that is the right thing, but this whole windows oddity about windows needing static linking anyway makes me feel this is all very brittle.\r\n\r\nBut I am not sure... I am also fine with just \"helping\" you here (could be tied into that same define also).  Although I am not even sure that the failing use-case is important (dynamically linking with an eigenpy that was compiled with an older NumPy version than your own library)."],"labels":["00 - Bug"]},{"title":"ENH: Add partition\/rpartition ufunc for string dtypes","body":"Closes #25993.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["I guess the disadvantage of the boolean is that `sum(partition(string, sep), initial='') != string`. I guess to reproduce the input, one would do something like `np.where(result[1], result[0] +  sep + result[2], result[0])`? I'd still slightly prefer returning an array of string, but seeing that there is an easy solution, I won't insist.","I tend to agree. While implementing it and writing the tests, I was at some points a bit confused as to what the original input string was, cause it wasn't immediately clear from the result truple. The trade-off is 1 vs 4 bytes per element, if the input is UTF-32, so not a huge amount of memory, for having a cleaner way to reconstruct the original string. \r\n\r\n@ngoldbaum What do you think?","> The trade-off is 1 vs 4 bytes per element, if the input is UTF-32, so not a huge amount of memory, for having a cleaner way to reconstruct the original string.\r\n\r\nAnd we diverge less from the CPython API too. Go for it and drop the boolean idea :)","FWIW I\u2019m planning to go over this and add stringdtype support tomorrow.","I started on this but got distracted by other fires and didn\u2019t finish it. Will work more tomorrow.","The failures on 32 bit architectures are real. Going to try to set up a 32 bit windows python environment to reproduce it..."],"labels":["01 - Enhancement"]},{"title":"TYP: Make array _ShapeType covariant","body":"Fixes #25729.  Applies to main ndarray class as well as record arrays, masked arrays, masked record arrays, mmaps, and matrices.\r\n\r\nI didn't see anything in the dev guide about testing static types for PRs, but this change fixes the MWE in the linked issue.  While there hasn't been much discussion on that issue, #16544 has renewed interest, so submitting this in case it's  helpful.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["@BvB93 Ping.","- [x] Add Alias\r\n- [x] Expand & rewrite change notes\r\n- [x] Add `TypeVar` bounds\r\n- [x] Add documentation for new alias","Alright @BvB93 , I made all the requested changes and CI is green - ready for another review. (a) Added type alias, (b) bound the TypeVar, (c) to demonstrate the use, `np.shape` is now typed to return the shape `TypeVar`, (d) described this in the docs and in expanded change notes, including some of the interplay between `TypeVar` and `TypeVarTuple`.  Hope I've been clear enough without going into too much detail.\r\n\r\nBig thanks to @jorenham for being equally interested in this and helping me think through some of the details!"],"labels":["Static typing"]},{"title":"TST: failures in `TestCholesky::test_basic_property` on aarch64","body":"While [building](https:\/\/github.com\/conda-forge\/numpy-feedstock\/pull\/312) 2.0.0b1 for conda-forge, the builds for linux-aarch64 all fail with:\r\n```\r\n=========================== short test summary info ============================\r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape3] - AssertionError: \r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape4] - AssertionError: \r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape3] - AssertionError: \r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape4] - AssertionError: \r\n4 failed, 45545 passed, 1071 skipped, 82 xfailed, 7 xpassed in 4036.05s (1:07:16)\r\n```\r\n\r\nIt's _possible_ that this is a problem with the emulation in QEMU, c.f. this scipy [issue](https:\/\/github.com\/scipy\/scipy\/issues\/19210). But I'd thought I'd flag it at least, as it's not a minor tolerance violation, but rather complete garbage:\r\n```\r\nMismatched elements: 1617 \/ 2500 (64.7%)\r\nMax absolute difference among violations: 89.25262\r\nMax relative difference among violations: 2200.9631\r\n```\r\n\r\n<details>\r\n<summary>More detailed error logs<\/summary>\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape3] - AssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0.00298023\r\n(50, 50) <class 'numpy.float32'>\r\n[[ 44.06959     -1.2917366   -0.61489445 ...  -1.7078063   -0.06026965\r\n    3.5968263 ]\r\n [ -1.2917366   50.031647    -0.93625164 ...  -9.246351   -12.977635\r\n   -0.22433573]\r\n [ -0.61489445  -0.93625164  56.479916   ...   3.1758978    6.96665\r\n   -2.8806841 ]\r\n ...\r\n [ -1.7078063   -9.246351     3.1758978  ...  53.048283     6.9221125\r\n    6.9285684 ]\r\n [ -0.06026965 -12.977635     6.96665    ...   6.9221125   49.258278\r\n  -11.654396  ]\r\n [  3.5968263   -0.22433573  -2.8806841  ...   6.9285684  -11.654396\r\n   47.500088  ]]\r\n[[ 6.638493    0.          0.         ...  0.          0.\r\n   0.        ]\r\n [-0.1945828   7.070628    0.         ...  0.          0.\r\n   0.        ]\r\n [-0.0926256  -0.13496326  7.5135293  ...  0.          0.\r\n   0.        ]\r\n ...\r\n [-0.25725812 -1.3147925   0.3959019  ...  0.7488741   0.\r\n   0.        ]\r\n [-0.00907881 -1.8356787   0.8941284  ... -0.11000964  0.70787036\r\n   0.        ]\r\n [ 0.5418137  -0.01681719 -0.3770223  ... -0.01939702  0.88379747\r\n   1.6186445 ]]\r\nMismatched elements: 1617 \/ 2500 (64.7%)\r\nMax absolute difference among violations: 89.25262\r\nMax relative difference among violations: 2200.9631\r\n ACTUAL: array([[ 4.406959e+01, -1.291737e+00, -6.148944e-01, ..., -9.883090e+00,\r\n        -2.005572e+00,  7.965497e+00],\r\n       [-1.291737e+00, -7.090253e+00,  1.104174e+00, ..., -9.404308e-03,...\r\n DESIRED: array([[ 44.06959 ,  -1.291737,  -0.614894, ...,  -1.707806,  -0.06027 ,\r\n          3.596826],\r\n       [ -1.291737,  50.031647,  -0.936252, ...,  -9.246351, -12.977635,...\r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[False-float32-shape4] - AssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0.000178814\r\n(3, 10, 10) <class 'numpy.float32'>\r\n[[[10.734333   -4.8768277  -3.1554568  -4.5997534   2.1030364\r\n   -1.2435193   3.0306861  -4.149522   -2.9620435   0.03625464]\r\n  [-4.8768277   8.896948    3.812659   -0.03711339 -1.0476841\r\n    4.170913    0.7686802   0.3364729   1.0283238  -3.826606  ]\r\n  [-3.1554568   3.812659    3.9959366   1.1625074   0.8306916\r\n    0.39898828 -0.77481    -1.9865611  -0.5670059  -2.2059839 ]\r\n  [-4.5997534  -0.03711339  1.1625074   6.6847258  -0.24326262\r\n   -0.47384936 -1.5542715   4.114234   -1.7963581   1.5196537 ]\r\n  [ 2.1030364  -1.0476841   0.8306916  -0.24326262  3.89964\r\n   -4.6199813   1.9657652  -1.6450504  -0.18314792  0.349001  ]\r\n  [-1.2435193   4.170913    0.39898828 -0.47384936 -4.6199813\r\n   11.839529   -2.9300923   0.8218101  -1.1914967  -1.5600258 ]\r\n  [ 3.0306861   0.7686802  -0.77481    -1.5542715   1.9657652\r\n   -2.9300923   7.020295    1.0565685   1.5246283  -3.3805852 ]\r\n  [-4.149522    0.3364729  -1.9865611   4.114234   -1.6450504\r\n    0.8218101   1.0565685  10.698549    4.4092803   0.4416819 ]\r\n  [-2.9620435   1.0283238  -0.5670059  -1.7963581  -0.18314792\r\n   -1.1914967   1.5246283   4.4092803   8.2551985  -1.8104365 ]\r\n  [ 0.03625464 -3.826606   -2.2059839   1.5196537   0.349001\r\n   -1.5600258  -3.3805852   0.4416819  -1.8104365   6.6920257 ]]\r\n\r\n [[ 5.9013734  -0.11575395 -1.8299209  -0.09370685  0.29582188\r\n    0.94792557 -1.7730689   3.0220997  -2.5740616   2.1073136 ]\r\n  [-0.11575395 10.544705    0.79119295 -5.9150043  -1.6428163\r\n   -1.3827156   0.25873902 -1.2621315  -0.7767093   4.0993524 ]\r\n  [-1.8299209   0.79119295 16.183662    6.329022   -1.1351309\r\n   -2.710929    6.666974    0.74811184  2.4979637   1.5011857 ]\r\n  [-0.09370685 -5.9150043   6.329022   13.666377   -2.3753624\r\n    0.70267767  4.4945393   1.8476337  -2.3819492   1.1581259 ]\r\n  [ 0.29582188 -1.6428163  -1.1351309  -2.3753624   6.571176\r\n   -4.508656   -2.573465   -0.43898493  5.2973313  -1.8500149 ]\r\n  [ 0.94792557 -1.3827156  -2.710929    0.70267767 -4.508656\r\n    6.8080935   0.086738    2.382368   -5.498594   -0.53745675]\r\n  [-1.7730689   0.25873902  6.666974    4.4945393  -2.573465\r\n    0.086738    6.919965   -0.40823123  1.9532615  -0.06429972]\r\n  [ 3.0220997  -1.2621315   0.74811184  1.8476337  -0.43898493\r\n    2.382368   -0.40823123  5.7033362  -2.0585406   0.21069823]\r\n  [-2.5740616  -0.7767093   2.4979637  -2.3819492   5.2973313\r\n   -5.498594    1.9532615  -2.0585406   9.462108   -3.9543934 ]\r\n  [ 2.1073136   4.0993524   1.5011857   1.1581259  -1.8500149\r\n   -0.53745675 -0.06429972  0.21069823 -3.9543934   7.435262  ]]\r\n\r\n [[ 4.6643376   2.6854048   4.0593524   0.1535468  -1.5118827\r\n    3.397957   -2.9920995   0.80777925 -1.7318133   0.82475716]\r\n  [ 2.6854048   7.829547   -0.5087376   4.3746657  -1.4713306\r\n   -4.861691   -6.0652037   0.99772763 -4.4356337   5.25942   ]\r\n  [ 4.0593524  -0.5087376  13.68554    -5.680109   -4.2268295\r\n    8.056314   -6.3996224  -0.8219479  -1.7537458   1.7455587 ]\r\n  [ 0.1535468   4.3746657  -5.680109   15.329586   -1.0279889\r\n   -9.684909   -4.311326   -0.32577878 -8.493642    7.0410876 ]\r\n  [-1.5118827  -1.4713306  -4.2268295  -1.0279889   6.4806786\r\n   -1.5806129   3.722421    1.541739   -0.32345185 -3.0850477 ]\r\n  [ 3.397957   -4.861691    8.056314   -9.684909   -1.5806129\r\n   15.264503    3.3202677  -0.3396074   4.926388   -7.346789  ]\r\n  [-2.9920995  -6.0652037  -6.3996224  -4.311326    3.722421\r\n    3.3202677  11.730729    1.6513809   7.1151867  -8.045567  ]\r\n  [ 0.80777925  0.99772763 -0.8219479  -0.32577878  1.541739\r\n   -0.3396074   1.6513809   4.239041   -0.10079275 -0.07716311]\r\n  [-1.7318133  -4.4356337  -1.7537458  -8.493642   -0.32345185\r\n    4.926388    7.1151867  -0.10079275 12.2900095  -7.07966   ]\r\n  [ 0.82475716  5.25942     1.7455587   7.0410876  -3.0850477\r\n   -7.346789   -8.045567   -0.07716311 -7.07966     9.485415  ]]]\r\n[[[ 3.2763293   0.          0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-1.4885036   2.584822    0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.96310735  0.9204      1.4903773   0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-1.4039351  -0.8228325   0.38091183  1.9726999   0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 0.641888   -0.03568195  0.9942048   0.12664968  1.5753931\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.3795465   1.3950504  -0.83908963  0.23359051 -2.2355907\r\n    1.9982008   0.          0.          0.          0.        ]\r\n  [ 0.9250249   0.83006996 -0.43472758  0.3006057   1.1398793\r\n   -0.8125721   1.7990714   0.          0.          0.        ]\r\n  [-1.2665155  -0.599167   -1.7813463   1.2782736   0.4796653\r\n    0.22821037  0.67006654  1.7880605   0.          0.        ]\r\n  [-0.90407383 -0.12279119 -0.8888414  -1.4336116   0.9255097\r\n    0.14752597  0.87395006  1.3292007   1.0810157   0.        ]\r\n  [ 0.01106563 -1.4740415  -0.5626888   0.27203056  0.5168721\r\n    0.5606847  -1.4603251  -0.657102   -0.46530387  0.87528366]]\r\n\r\n [[ 2.4292743   0.          0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.0476496   3.2469115   0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.75327885  0.23262091  3.9448855   0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.03857401 -1.8222985   1.7044525   2.7274456   0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 0.12177376 -0.50417566 -0.2347646  -1.059335    2.263813\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 0.39020938 -0.42012918 -0.5879161   0.3498526  -2.0034354\r\n    1.4133387   0.          0.          0.          0.        ]\r\n  [-0.729876    0.06897654  1.5465921   0.7171502  -0.58618873\r\n   -0.08172157  1.768033    0.          0.          0.        ]\r\n  [ 1.2440339  -0.37046087  0.4490354   0.16688555 -0.2186787\r\n    1.067539   -0.18652947  1.6021042   0.          0.        ]\r\n  [-1.0596011  -0.25476483  0.44590706 -1.3371879   1.7607771\r\n   -0.6612598   1.3828325   0.3352417   0.8516632   0.        ]\r\n  [ 0.86746633  1.2752694   0.47098336  0.9946084  -0.06559554\r\n   -0.3839536  -0.58293706 -0.30378163 -0.96378136  1.5273129 ]]\r\n\r\n [[ 2.1597078   0.          0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 1.2434112   2.5066862   0.          0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 1.8795841  -1.1352971   2.977214    0.          0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 0.0710961   1.7099324  -1.3006988   3.272437    0.\r\n    0.          0.          0.          0.          0.        ]\r\n  [-0.7000404  -0.23971593 -1.0691853  -0.5986391   2.1051443\r\n    0.          0.          0.          0.          0.        ]\r\n  [ 1.5733411  -2.719926    0.6755185  -1.3039918  -0.5650853\r\n    1.707355    0.          0.          0.          0.        ]\r\n  [-1.3854187  -1.7323902  -1.9354969  -1.1514527  -0.20018531\r\n    0.2816568   1.2723097   0.          0.          0.        ]\r\n  [ 0.37402248  0.21249725 -0.43117726 -0.39009395  0.5510192\r\n   -0.1500179   1.1054928   1.4722989   0.          0.        ]\r\n  [-0.8018739  -1.3717611  -0.60590625 -2.102138   -1.4820251\r\n   -0.41726547 -0.11362489  0.19627525  1.5991338   0.        ]\r\n  [ 0.38188368  1.9087278   1.0730667   1.572491   -0.12896962\r\n   -0.8804634  -0.07867548  0.32361656 -0.51923645  0.9485597 ]]]\r\nMismatched elements: 253 \/ 300 (84.3%)\r\nMax absolute difference among violations: 20.29966\r\nMax relative difference among violations: 237.33156\r\n ACTUAL: array([[[10.734334, -4.876828, -3.155457, -4.599753,  2.103036,\r\n         -1.243519,  3.030686, -4.149522,  0.      ,  0.      ],\r\n        [-4.876828,  2.244246,  1.433589,  8.771067,  1.423618,...\r\n DESIRED: array([[[10.734333, -4.876828, -3.155457, -4.599753,  2.103036,\r\n         -1.243519,  3.030686, -4.149522, -2.962044,  0.036255],\r\n        [-4.876828,  8.896948,  3.812659, -0.037113, -1.047684,...\r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape3] - AssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0.00298023\r\n(50, 50) <class 'numpy.float32'>\r\n[[ 44.06959     -1.2917366   -0.61489445 ...  -1.7078063   -0.06026965\r\n    3.5968263 ]\r\n [ -1.2917366   50.031647    -0.93625164 ...  -9.246351   -12.977635\r\n   -0.22433573]\r\n [ -0.61489445  -0.93625164  56.479916   ...   3.1758978    6.96665\r\n   -2.8806841 ]\r\n ...\r\n [ -1.7078063   -9.246351     3.1758978  ...  53.048283     6.9221125\r\n    6.9285684 ]\r\n [ -0.06026965 -12.977635     6.96665    ...   6.9221125   49.258278\r\n  -11.654396  ]\r\n [  3.5968263   -0.22433573  -2.8806841  ...   6.9285684  -11.654396\r\n   47.500088  ]]\r\n[[ 6.638493   -0.1945828  -0.0926256  ... -0.25725812 -0.00907881\r\n   0.5418137 ]\r\n [ 0.          7.070628   -0.13496326 ... -1.3147925  -1.8356787\r\n  -0.01681719]\r\n [ 0.          0.          7.5135293  ...  0.3959019   0.8941284\r\n  -0.3770223 ]\r\n ...\r\n [ 0.          0.          0.         ...  0.7488741  -0.11000964\r\n  -0.01939702]\r\n [ 0.          0.          0.         ...  0.          0.70787036\r\n   0.88379747]\r\n [ 0.          0.          0.         ...  0.          0.\r\n   1.6186445 ]]\r\nMismatched elements: 1617 \/ 2500 (64.7%)\r\nMax absolute difference among violations: 89.25262\r\nMax relative difference among violations: 2200.9631\r\n ACTUAL: array([[ 4.406959e+01, -1.291737e+00, -6.148944e-01, ..., -9.883090e+00,\r\n        -2.005572e+00,  7.965497e+00],\r\n       [-1.291737e+00, -7.090253e+00,  1.104174e+00, ..., -9.404308e-03,...\r\n DESIRED: array([[ 44.06959 ,  -1.291737,  -0.614894, ...,  -1.707806,  -0.06027 ,\r\n          3.596826],\r\n       [ -1.291737,  50.031647,  -0.936252, ...,  -9.246351, -12.977635,...\r\nFAILED linalg\/tests\/test_linalg.py::TestCholesky::test_basic_property[True-float32-shape4] - AssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0.000178814\r\n(3, 10, 10) <class 'numpy.float32'>\r\n[[[10.734333   -4.8768277  -3.1554568  -4.5997534   2.1030364\r\n   -1.2435193   3.0306861  -4.149522   -2.9620435   0.03625464]\r\n  [-4.8768277   8.896948    3.812659   -0.03711339 -1.0476841\r\n    4.170913    0.7686802   0.3364729   1.0283238  -3.826606  ]\r\n  [-3.1554568   3.812659    3.9959366   1.1625074   0.8306916\r\n    0.39898828 -0.77481    -1.9865611  -0.5670059  -2.2059839 ]\r\n  [-4.5997534  -0.03711339  1.1625074   6.6847258  -0.24326262\r\n   -0.47384936 -1.5542715   4.114234   -1.7963581   1.5196537 ]\r\n  [ 2.1030364  -1.0476841   0.8306916  -0.24326262  3.89964\r\n   -4.6199813   1.9657652  -1.6450504  -0.18314792  0.349001  ]\r\n  [-1.2435193   4.170913    0.39898828 -0.47384936 -4.6199813\r\n   11.839529   -2.9300923   0.8218101  -1.1914967  -1.5600258 ]\r\n  [ 3.0306861   0.7686802  -0.77481    -1.5542715   1.9657652\r\n   -2.9300923   7.020295    1.0565685   1.5246283  -3.3805852 ]\r\n  [-4.149522    0.3364729  -1.9865611   4.114234   -1.6450504\r\n    0.8218101   1.0565685  10.698549    4.4092803   0.4416819 ]\r\n  [-2.9620435   1.0283238  -0.5670059  -1.7963581  -0.18314792\r\n   -1.1914967   1.5246283   4.4092803   8.2551985  -1.8104365 ]\r\n  [ 0.03625464 -3.826606   -2.2059839   1.5196537   0.349001\r\n   -1.5600258  -3.3805852   0.4416819  -1.8104365   6.6920257 ]]\r\n\r\n [[ 5.9013734  -0.11575395 -1.8299209  -0.09370685  0.29582188\r\n    0.94792557 -1.7730689   3.0220997  -2.5740616   2.1073136 ]\r\n  [-0.11575395 10.544705    0.79119295 -5.9150043  -1.6428163\r\n   -1.3827156   0.25873902 -1.2621315  -0.7767093   4.0993524 ]\r\n  [-1.8299209   0.79119295 16.183662    6.329022   -1.1351309\r\n   -2.710929    6.666974    0.74811184  2.4979637   1.5011857 ]\r\n  [-0.09370685 -5.9150043   6.329022   13.666377   -2.3753624\r\n    0.70267767  4.4945393   1.8476337  -2.3819492   1.1581259 ]\r\n  [ 0.29582188 -1.6428163  -1.1351309  -2.3753624   6.571176\r\n   -4.508656   -2.573465   -0.43898493  5.2973313  -1.8500149 ]\r\n  [ 0.94792557 -1.3827156  -2.710929    0.70267767 -4.508656\r\n    6.8080935   0.086738    2.382368   -5.498594   -0.53745675]\r\n  [-1.7730689   0.25873902  6.666974    4.4945393  -2.573465\r\n    0.086738    6.919965   -0.40823123  1.9532615  -0.06429972]\r\n  [ 3.0220997  -1.2621315   0.74811184  1.8476337  -0.43898493\r\n    2.382368   -0.40823123  5.7033362  -2.0585406   0.21069823]\r\n  [-2.5740616  -0.7767093   2.4979637  -2.3819492   5.2973313\r\n   -5.498594    1.9532615  -2.0585406   9.462108   -3.9543934 ]\r\n  [ 2.1073136   4.0993524   1.5011857   1.1581259  -1.8500149\r\n   -0.53745675 -0.06429972  0.21069823 -3.9543934   7.435262  ]]\r\n\r\n [[ 4.6643376   2.6854048   4.0593524   0.1535468  -1.5118827\r\n    3.397957   -2.9920995   0.80777925 -1.7318133   0.82475716]\r\n  [ 2.6854048   7.829547   -0.5087376   4.3746657  -1.4713306\r\n   -4.861691   -6.0652037   0.99772763 -4.4356337   5.25942   ]\r\n  [ 4.0593524  -0.5087376  13.68554    -5.680109   -4.2268295\r\n    8.056314   -6.3996224  -0.8219479  -1.7537458   1.7455587 ]\r\n  [ 0.1535468   4.3746657  -5.680109   15.329586   -1.0279889\r\n   -9.684909   -4.311326   -0.32577878 -8.493642    7.0410876 ]\r\n  [-1.5118827  -1.4713306  -4.2268295  -1.0279889   6.4806786\r\n   -1.5806129   3.722421    1.541739   -0.32345185 -3.0850477 ]\r\n  [ 3.397957   -4.861691    8.056314   -9.684909   -1.5806129\r\n   15.264503    3.3202677  -0.3396074   4.926388   -7.346789  ]\r\n  [-2.9920995  -6.0652037  -6.3996224  -4.311326    3.722421\r\n    3.3202677  11.730729    1.6513809   7.1151867  -8.045567  ]\r\n  [ 0.80777925  0.99772763 -0.8219479  -0.32577878  1.541739\r\n   -0.3396074   1.6513809   4.239041   -0.10079275 -0.07716311]\r\n  [-1.7318133  -4.4356337  -1.7537458  -8.493642   -0.32345185\r\n    4.926388    7.1151867  -0.10079275 12.2900095  -7.07966   ]\r\n  [ 0.82475716  5.25942     1.7455587   7.0410876  -3.0850477\r\n   -7.346789   -8.045567   -0.07716311 -7.07966     9.485415  ]]]\r\n[[[ 3.2763293  -1.4885036  -0.96310735 -1.4039351   0.641888\r\n   -0.3795465   0.9250249  -1.2665155  -0.90407383  0.01106563]\r\n  [ 0.          2.584822    0.9204     -0.8228325  -0.03568195\r\n    1.3950504   0.83006996 -0.599167   -0.12279119 -1.4740415 ]\r\n  [ 0.          0.          1.4903773   0.38091183  0.9942048\r\n   -0.83908963 -0.43472758 -1.7813463  -0.8888414  -0.5626888 ]\r\n  [ 0.          0.          0.          1.9726999   0.12664968\r\n    0.23359051  0.3006057   1.2782736  -1.4336116   0.27203056]\r\n  [ 0.          0.          0.          0.          1.5753931\r\n   -2.2355907   1.1398793   0.4796653   0.9255097   0.5168721 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    1.9982008  -0.8125721   0.22821037  0.14752597  0.5606847 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          1.7990714   0.67006654  0.87395006 -1.4603251 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          1.7880605   1.3292007  -0.657102  ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          1.0810157  -0.46530387]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          0.          0.87528366]]\r\n\r\n [[ 2.4292743  -0.0476496  -0.75327885 -0.03857401  0.12177376\r\n    0.39020938 -0.729876    1.2440339  -1.0596011   0.86746633]\r\n  [ 0.          3.2469115   0.23262091 -1.8222985  -0.50417566\r\n   -0.42012918  0.06897654 -0.37046087 -0.25476483  1.2752694 ]\r\n  [ 0.          0.          3.9448855   1.7044525  -0.2347646\r\n   -0.5879161   1.5465921   0.4490354   0.44590706  0.47098336]\r\n  [ 0.          0.          0.          2.7274456  -1.059335\r\n    0.3498526   0.7171502   0.16688555 -1.3371879   0.9946084 ]\r\n  [ 0.          0.          0.          0.          2.263813\r\n   -2.0034354  -0.58618873 -0.2186787   1.7607771  -0.06559554]\r\n  [ 0.          0.          0.          0.          0.\r\n    1.4133387  -0.08172157  1.067539   -0.6612598  -0.3839536 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          1.768033   -0.18652947  1.3828325  -0.58293706]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          1.6021042   0.3352417  -0.30378163]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          0.8516632  -0.96378136]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          0.          1.5273129 ]]\r\n\r\n [[ 2.1597078   1.2434112   1.8795841   0.0710961  -0.7000404\r\n    1.5733411  -1.3854187   0.37402248 -0.8018739   0.38188368]\r\n  [ 0.          2.5066862  -1.1352971   1.7099324  -0.23971593\r\n   -2.719926   -1.7323902   0.21249725 -1.3717611   1.9087278 ]\r\n  [ 0.          0.          2.977214   -1.3006988  -1.0691853\r\n    0.6755185  -1.9354969  -0.43117726 -0.60590625  1.0730667 ]\r\n  [ 0.          0.          0.          3.272437   -0.5986391\r\n   -1.3039918  -1.1514527  -0.39009395 -2.102138    1.572491  ]\r\n  [ 0.          0.          0.          0.          2.1051443\r\n   -0.5650853  -0.20018531  0.5510192  -1.4820251  -0.12896962]\r\n  [ 0.          0.          0.          0.          0.\r\n    1.707355    0.2816568  -0.1500179  -0.41726547 -0.8804634 ]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          1.2723097   1.1054928  -0.11362489 -0.07867548]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          1.4722989   0.19627525  0.32361656]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          1.5991338  -0.51923645]\r\n  [ 0.          0.          0.          0.          0.\r\n    0.          0.          0.          0.          0.9485597 ]]]\r\nMismatched elements: 253 \/ 300 (84.3%)\r\nMax absolute difference among violations: 20.29966\r\nMax relative difference among violations: 237.33156\r\n ACTUAL: array([[[10.734334, -4.876828, -3.155457, -4.599753,  2.103036,\r\n         -1.243519,  3.030686, -4.149522,  0.      ,  0.      ],\r\n        [-4.876828,  2.244246,  1.433589,  8.771067,  1.423618,...\r\n DESIRED: array([[[10.734333, -4.876828, -3.155457, -4.599753,  2.103036,\r\n         -1.243519,  3.030686, -4.149522, -2.962044,  0.036255],\r\n        [-4.876828,  8.896948,  3.812659, -0.037113, -1.047684,...\r\n4 failed, 45545 passed, 1071 skipped, 82 xfailed, 7 xpassed in 4036.05s (1:07:16)\r\n```\r\n\r\n<\/details>","comments":["> It's _possible_ that this is a problem with the emulation in QEMU, c.f. this scipy [issue](https:\/\/github.com\/scipy\/scipy\/issues\/19210).\r\n\r\nIt's somewhat likely I'd say, since we don't see this issue on our own aarch64 CI \/ wheel builds (native builds on Cirrus CI).\r\n\r\nThe other main suspect would be the OpenBLAS build for aarch64 used in conda-forge. It may be worth testing with Netlib BLAS\/LAPACK to rule that out?","> The other main suspect would be the OpenBLAS build for aarch64 used in conda-forge. It may be worth testing with Netlib BLAS\/LAPACK to rule that out?\r\n\r\nAFAIU the two things are not mutually exclusive. It can be both OpenBLAS-only _and_ QEMU-related (which is the case for the scipy failures), due to the way various optimizations get translated to CPU instructions, which get emulated through QEMU.\r\n\r\nIn any case, I can run the full BLAS flavour analysis for 2.0.0b1.","The slightly surprising thing for me is that a BLAS issue (which seems like) wouldn't show up when doing the same build on NumPy 1.26.\r\n\r\nSo the changes here might be in the strided complex matmul, @mhvk do you remember if there are changes that are related?\r\n\r\nOTOH, even then, since we don't see it in CI, I think being related to QEMU is very likely.","I had some changes to `matmul.c.src` at some point, but those didn't make it in, except a change in call signature of the matrix-vector pieces calling `gemv`, where I removed `NPY_UNUSED` arguments. I just checked again, and those remove the right ones. So, that seems exceedingly unlikely.","> In any case, I can run the full BLAS flavour analysis for 2.0.0b1.\r\n\r\nIn Line with the experience for SciPy, the failure did not appear when running against netlib LAPACK.\r\n\r\nI'm okay with skipping this test on aarch for now. "],"labels":["component: numpy.linalg"]},{"title":"Fate of `numpy.version` in 2.0","body":"While looking at the [reference](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/tests\/test_public_api.py) (effectively) for numpy's public API, I noticed:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/6059db15f06555b246f3743db3152050b50722b1\/numpy\/tests\/test_public_api.py#L141\r\n\r\nI haven't followed the finalization of 2.0 closely, so apologies if this is superfluous, but I thought I'd flag it. It wasn't mentioned in https:\/\/github.com\/numpy\/numpy\/issues\/25918 for example.\r\n\r\nCC @rgommers @charris @seberg","comments":["I'll vote for not doing anything, the sum of all changes is already large when it comes to spending downstream annoyance chips.","I agree with not doing anything. It was a minor cleanup opportunity, but the window for regular API changes for 2.0 has closed. We've asked downstream projects to do their RC1 and hopefully have our own RC1 out later this week.\r\n\r\nI see a few more minor inaccuracies in that test, I'll open a PR to fix this comment and those other things."],"labels":["62 - Python API"]},{"title":"ENH: Half-sized StringDType?","body":"### Proposed new feature or change:\n\n(Mostly for @ngoldbaum probably)\r\n\r\nThe new `StrngDTyype` is quite beautiful but also fairly inefficient for arrays that contain mostly smaller strings, since it uses at least the 16 bytes per array element that store its information. In principle, an 8-byte version of `StringDType` exists, as that is used on 32 bit systems, but it cannot directly be used on 64 bit systems, since it cannot store the pointer to a long string stored outside the arena. However, this could be remedied if, instead, this pointer itself is stored inside the arena (and, thus, the actual array entry continues to contain the offset in the arena).\r\n\r\nOverall, one could envision a string dtype with a slightly changed logic also for short string replacements, which would ensure that offsets to the arena are always kept:\r\n1. Initialization is exactly the same as is: short strings are stored in the array, everything else in the arena. The minimum length of an arena entry is thus 8 bytes.\r\n2. If a short string is replaced by a longer one (at least 8 bytes), a new entry in the arena is added.\r\n3. If an arena string is replaced by a shorter one, it is kept in the arena, even if short enough to be stored in the array, so that the arena offset is not lost (and repeated string replacements can thus not lead the arena to continue to grow; it can never have more entries than the size of the array).\r\n4. If an arena string is replaced by string too long to be stored in the existing entry, it is stored on the heap, with the corresponding pointer stored in the arena entry (which is at least 8 bytes in size, so this fits). The offset into the arena is thus now used to find the pointer to the heap.\r\n5. If an entry on the heap is replaced by one that fits in the corresponding arena entry, it will be stored there.\r\n\r\nCompared to the existing `StringDType`, this shorter type would be equally efficient for first-time entries, but slightly less efficient for short or long string replacements, as those both require an extra lookup. It also has the limitation that the arena cannot be larger than 4 GB. It would generally use quite a bit less memory if the array is mostly short strings except if they are mostly 8-15 bytes long.\r\n\r\nWould this be worth implementing, perhaps with a different initialization option for `StringDType`?","comments":["This is very interesting, thank you for writing it up.\r\n\r\nStoring heap string pointers in the arena never occurred to me, I agree that does naturally let you reduce the size of the packed string struct.\r\n\r\nWhy would the arena have a 4 GB size limit? Couldn't we still store a 7-byte integer offset in the packed string, outside the flags?\r\n\r\nAnother nice aspect of this choice is if we disable mutability and drop the short string optimization and the need for flags along with it, this is exactly the arrow variable-length string memory layout.","> Why would the arena have a 4 GB size limit? Couldn't we still store a 7-byte integer offset in the packed string, outside the flags?\r\n\r\nThe limit would only happen for the short version of the dtype, where one uses just 8 bytes (i.e., the 32 bit version on a 64-bit machine, which would work in this scheme). I had been thinking about that mostly because I know many of the astronomy strings use small numbers of characters, so was hoping for `StringDType` to beat `U` at least, and hence be able to get rid of our hacks where we use `S` to represent ascii strings.\r\n\r\n> Another nice aspect of this choice is if we disable mutability and drop the short string optimization and the need for flags along with it, this is exactly the arrow variable-length string memory layout.\r\n\r\nI do really like the short strings, though! I think in the general case, they will reduce memory usage considerably...\r\n","Sorry, I perhaps missed your point: for an 8-byte dtype, if we use the full 7-byte offset in the packed string, then where is the size stored?","I\u2019m asking about the 8-byte version. Maybe I\u2019m not understanding the struct layout you\u2019re proposing. If you drop the need to store a pointer in the packed string, wouldn\u2019t you always be storing a 7-byte integer offset and one byte of flags?\r\n\r\nThe reason I bring up the arrow layout is it makes zero-copy interoperability with the arrow ecosystem easy. This is in a possible future where numpy gains better thread safety support by (for example) \u201cfreezing\u201d arrays or creating them with mutability disabled. Short string support isn\u2019t going away.","Ah missed your comment while I was writing mine, so 32 bit size and offset in the packed string.","Couldn\u2019t we do this with just a 7 byte offset and 1 byte of flags, by storing the string size in the arena too?\r\n\r\nThat does add some extra overhead to check string sizes, but we actually don\u2019t expose in the public API that you can get the string size directly from the packed string without unpacking it.\r\n\r\nI do really like your proposal but am trying to think of ways to avoid 32 bit int limits, which may be hit in practice for real-world workflows.","Ah, now you're ahead of me! I was indeed confused how a `flags-byte, 7-byte offset` would work, given that one needs the size. Though I also realized that in principle one can store both the length of the allocated space and the current size in the arena. As long as one keeps the medium flag, that would in fact not be much of an increase in size of the arena - easily compensated by the decrease in size of the part in the array. It would probably be wise in that case to have something explicit to indicate an empty string (perhaps the short size bits?). \r\n\r\nOr I guess one could use 6 bytes for the offset and keep 1 for the size, so that only for long strings the actual size is stored in the arena ? That does feel a bit less clean, though.\r\n\r\nMore generally, do you think it is worth exploring this with actual code?","> The reason I bring up the arrow layout is it makes zero-copy interoperability with the arrow ecosystem easy.\r\n\r\nI'm actually a little confused about this: I'm not sure I found the right link, but from [this one](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#variable-size-binary-layout), it seemed that the arrow format does not store sizes in its arena, so I guess from arrow to us would have to be via an arena filled with pointers. On the other hand, from us to arrow would seem to work, since they allow offsets & sizes in the \"views\".","> More generally, do you think it is worth exploring this with actual code?\r\n\r\nVery yes. The main goal of this whole project was reducing memory usage, so halving the minimum memory overhead is a very worthy goal.\r\n\r\n> Or I guess one could use 6 bytes for the offset and keep 1 for the size, so that only for long strings the actual size is stored in the arena ? That does feel a bit less clean, though.\r\n\r\nI agree it would be best to keep the complexity down and only have one representation, but I agree one way to do this would be to only \"degrade\" to a longer offset field if needed. The problem with that is you'd only hit this mode for either very long strings or very large arrays which seems like a recipe for bugs that only affect workflows dealing with lots of data.\r\n\r\n> but from [this one](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#variable-size-binary-layout), it seemed that the arrow format does not store sizes in its arena, so I guess from arrow to us would have to be via an arena filled with pointers\r\n\r\nThat's the right link. They have an array of offsets and a sidecar array for the string data, which corresponds to our arena. My thought was that an immutable ndarray would degrade to exactly this format.\r\n\r\nBut also I see now that within the last year arrow added a [variable-sized binary view layout](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#variable-size-binary-view-layout) which is a lot more similar to our layout and may be worth looking at more closely.\r\n","Partially just to remind myself while thinking about the implementation: One annoyance of storing the size in the arena, is that even for medium-sized strings, one has to keep space for a larger size, in case it gets replaced by a longer one that has to be stored on the heap. So, taking 7 bytes to be enough for the size, the minimum size for storing a medium string becomes 16 bytes, which is an overhead of 8 bytes for an 8-byte long string (which would just not be short any more).\r\n\r\nFlags could be:\r\n8 missing\r\n7 in arena\r\n6 long string in arena\r\n5 on heap\r\n\r\nAll clear means short string."],"labels":["01 - Enhancement","15 - Discussion","component: numpy.strings"]},{"title":"DOC: Inaccessible contrast for a:visited links in admonitions","body":"### Issue with current documentation:\r\n\r\n1. Visit https:\/\/numpy.org\/neps\/nep-0029-deprecation_policy.html\r\n2. If you've not visited the link \"SPEC 0 \u2014 Minimum Supported Versions\" before, it looks like:\r\n\r\n<img width=\"747\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/1324225\/7fd0d136-69fc-447b-9573-dbda945c18c4\">\r\n\r\n3. Click the link \"SPEC 0 \u2014 Minimum Supported Versions\"\r\n4. Return to https:\/\/numpy.org\/neps\/nep-0029-deprecation_policy.html\r\n5. It looks like:\r\n\r\n<img width=\"741\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/1324225\/cb41d42e-9557-487d-8369-2e8ff82744c8\">\r\n\r\nThe contrast of purple on the dark background is low: it's ratio is 1.51, below the WCAG AA level of 4.5:\r\n\r\n<img width=\"235\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/1324225\/c96a6f11-e869-4d66-b73d-7c1ba36a5602\">\r\n\r\nThis comes from [`basic.css`](https:\/\/numpy.org\/neps\/_static\/basic.css), which I think is from Sphinx:\r\n\r\n```css\r\na:visited {\r\n    color: #551A8B;\r\n}\r\n```\r\n\r\nIf I disable the rule, it looks like this, with a good ratio of 5.37:\r\n\r\n<img width=\"733\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/1324225\/528908b6-2052-4be0-9b80-adc810eb48eb\">\r\n\r\nThe docs use PyData Sphinx Theme 0.13.3, but I can't reproduce the problem with these links (and inspecting the elements to edit in a link):\r\n\r\n* https:\/\/pydata-sphinx-theme.readthedocs.io\/en\/v0.13.3\/examples\/kitchen-sink\/admonitions.html\r\n* https:\/\/pydata-sphinx-theme.readthedocs.io\/en\/stable\/examples\/kitchen-sink\/admonitions.html (0.15.2)\r\n\r\n","comments":["Related: https:\/\/github.com\/scientific-python\/scientific-python-hugo-theme\/issues\/457","This is fixed in the most recent versions of the theme (with the updated colour palette). So once the pin on 0.13 is removed this should no longer be a problem. \n\nIt's also related to https:\/\/github.com\/numpy\/numpy\/issues\/25927\n"],"labels":["04 - Documentation"]},{"title":"ENH: Optimize np.power(x, 2) for double and float type","body":"We optimize `np.power(x, e)` by adding a fast path for the case `e=2` and `x` is double or float.\r\n\r\nUpdated benchmark (linux):\r\n\r\nMain\r\n```\r\nx**1 54.387 [us]\r\nx**2 50.409 [us]\r\nx**3 1999.226 [us]\r\nx**1.123 1982.337 [us]\r\nx**0.5 206.666 [us]\r\npower(x, 1) 1980.208 [us]\r\npower(x, 2) 1987.717 [us]\r\npower(x, 3) 1982.347 [us]\r\npower(x, 1.123) 1979.270 [us]\r\npower(x, 0.5) 1981.093 [us]\r\npower(x_float32, 1) 917.876 [us]\r\npower(x_float32, 2) 937.346 [us]\r\npower(x_float32, 3) 1006.258 [us]\r\npower(x_float32, 1.123) 916.753 [us]\r\npower(x_float32, 0.5) 918.313 [us]\r\npower(double_size10, 1) 1.116 [us]\r\npower(double_size10, 2) 1.127 [us]\r\npower(double_size10, 3) 1.172 [us]\r\npower(double_size10, 1.123) 1.010 [us]\r\npower(double_size10, 0.5) 0.941 [us]\r\n```\r\nPR\r\n```\r\nx**1 55.415 [us]\r\nx**2 51.346 [us]\r\nx**3 1953.280 [us]\r\nx**1.123 1953.009 [us]\r\nx**0.5 206.721 [us]\r\npower(x, 1) 1947.996 [us]\r\npower(x, 2) 58.777 [us]\r\npower(x, 3) 1951.243 [us]\r\npower(x, 1.123) 1949.528 [us]\r\npower(x, 0.5) 406.441 [us]\r\npower(x_float32, 1) 900.470 [us]\r\npower(x_float32, 2) 53.788 [us]\r\npower(x_float32, 3) 900.063 [us]\r\npower(x_float32, 1.123) 903.382 [us]\r\npower(x_float32, 0.5) 205.104 [us]\r\npower(double_size10, 1) 1.087 [us]\r\npower(double_size10, 2) 0.912 [us]\r\npower(double_size10, 3) 1.218 [us]\r\npower(double_size10, 1.123) 0.918 [us]\r\npower(double_size10, 0.5) 0.759 [us]\r\n```\r\n<details><summary>Benchmark script<\/summary>\r\n\r\n```\r\nimport sys\r\ndel(sys.path[0])\r\nimport numpy as np\r\nprint(np, np.__version__)\r\nnp.show_config()\r\n\r\nimport timeit\r\n\r\nexponents = [1, 2, 3, 1.123, .5]\r\n\r\nxf=np.arange(100_000.).astype(np.float64)\r\nnf = 1_000\r\nfor e in exponents:\r\n    dt=timeit.timeit(f'xf**{e}', globals=globals(), number=nf)\r\n    print(f'x**{e} {1e6*dt\/nf:.3f} [us]')\r\n\r\nfor e in exponents:\r\n    dt=timeit.timeit(f'np.power(xf, {e})', globals=globals(), number=nf)\r\n    print(f'power(x, {e}) {1e6*dt\/nf:.3f} [us]')\r\n\r\nf=np.arange(100_000.).astype(np.float32)\r\nfor e in exponents:\r\n    dt=timeit.timeit(f'np.power(f, {e})', globals=globals(), number=nf)\r\n    print(f'power(x_float32, {e}) {1e6*dt\/nf:.3f} [us]')\r\n\r\n\r\nnf = 1_000_000\r\ny=np.arange(10.)\r\nfor e in exponents:\r\n    dt=timeit.timeit(f'np.power(y, {e})', globals=globals(), number=nf)\r\n    print(f'power(double_size10, {e}) {1e6*dt\/nf:.3f} [us]')\r\n```\r\n<\/details>\r\n\r\nOld benchmark (windows)\r\n```\r\nimport numpy as np\r\nimport timeit\r\n\r\nxf=np.arange(100_000+.0)\r\nnf = 2_000\r\n\r\nfor e in [1, 2, 3]:\r\n    dt=timeit.timeit(f'xf**{e}', globals=globals(), number=nf)\r\n    print(f'x**{e} {1e6*dt\/nf:.3f} [us]')\r\n\r\nfor e in [1, 2, 3]:\r\n    dt=timeit.timeit(f'np.power(xf, {e})', globals=globals(), number=nf)\r\n    print(f'power(x, {e}) {1e6*dt\/nf:.3f} [us]')\r\ndt=timeit.timeit('np.power(xf, 2.)', globals=globals(), number=nf)\r\nprint(f'power(x, 2.) {1e6*dt\/nf:.3f} [us]')\r\n```\r\nResults on main:\r\n```\r\nx**1 131.168 [us]\r\nx**2 75.814 [us]\r\nx**3 1509.223 [us]\r\npower(x, 1) 306.960 [us]\r\npower(x, 2) 1620.303 [us]\r\npower(x, 3) 1571.338 [us]\r\npower(x, 2.) 1493.646 [us]\r\n```\r\nResults on this PR:\r\n```\r\nx**1 119.589 [us]\r\nx**2 77.959 [us]\r\nx**3 1445.394 [us]\r\npower(x, 1) 257.166 [us]\r\npower(x, 2) 58.722 [us]\r\npower(x, 3) 1406.567 [us]\r\npower(x, 2.) 57.718 [us]\r\n```\r\n\r\nNotes:\r\n\r\n* Maybe the fast path for exponent 2 should be outside the SIMD check. On my system SIMD is not selected\/available, so I cannot benchmark this.\r\n* Benchmark results are a bit unstable on my machine, so it would be good to reproduce this\r\n* We could special case also other often used exponents. For example -1, 0, .5, 1, 3, 4. But we should check whether that gives the exact same results as the current implementation.\r\n* Depending on the compiler the option `ffast-math` can make `pow` much faster for special cases such as `e=2`. See for example https:\/\/stackoverflow.com\/a\/2940800\r\n\r\nRelated: #26045, #9363, #6399, #22651, #7786\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["All tests pass, but this PR does change behavior. A minimal example:\r\n```\r\nz=np.frombuffer(b'\\xb6\\xfe\\x10\\x16\\xe1\\xa7\\xda?')\r\nprint(z, np.power(z, 2) - z*z)\r\n```\r\nOn main `np.power(z, 2)` and `z*z` slightly differ, with this PR they are exactly equal.","Given the dramatic performance improvement, I think accepting some small change in output is ok. Is it possible to hint to the compiler to generalize this a bit? Or maybe manually unroll up to e.g. `n=5`?\r\n\r\nIt would be good to see some benchmarks to capture the overhead of the extra branching for non-integer powers, if any.\r\n\r\nAdded the triage review label to make sure this gets looked at during a triage meeting.","> We optimize np.power(x, e) by adding a fast path for the case e=2 and x is double.\r\n\r\nYour patch seems to do this for float as well. Or am I missing something? ","> > We optimize np.power(x, e) by adding a fast path for the case e=2 and x is double.\r\n> \r\n> Your patch seems to do this for float as well. Or am I missing something?\r\n\r\nIt indeed handles both double and float.","> Given the dramatic performance improvement, I think accepting some small change in output is ok. Is it possible to hint to the compiler to generalize this a bit? Or maybe manually unroll up to e.g. `n=5`?\r\n> \r\n> It would be good to see some benchmarks to capture the overhead of the extra branching for non-integer powers, if any.\r\n\r\nI added some tests with smaller arrays to see whether the overhead impacts the non-integer case (results updated in first comment). It seems the impact is small or none, but it might depend on the compiler\/platform and would be good to benchmark on an independent system.\r\n\r\nAdding more special cases is possible, but we need to decide which cases are important as each added case adds a bit more complexity (and overhead). I now have `e=2` and `e=.5`. Other cases to consider are -1 and -2 (but this might require some though about whether cases like 0 are handles correctly) and indeed `e=3, 4, 5,  ...`\r\n","The impact is largest in some degenerate cases.  Small arrays are dominated by other overheads anyway.\r\nUsage of `where=[0, 1] * N` may be the worst but is rather niche and ridiculous.  Things like `arr = np.empty((large, 2))[::2, :]` might run into it (but I am actually not sure, it may be that it goes into buffered paths making the inner-loop large).","> Adding more special cases is possible, but we need to decide which cases are important\r\n\r\n@eendebakpt out of curiosity, why optimize for `np.power(x, 2)`? one could as well just use `x * x`, ","The following script can be used to generate some cases where `z*z` differs from `np.power(z, e)` (results might be platform dependent).\r\n```\r\nimport sympy\r\nimport numpy as np\r\nimport random\r\nfrom codecs import decode\r\nimport struct\r\n\r\n\r\ndef bin_to_float(b):\r\n    \"\"\" Convert binary string to a float. \"\"\"\r\n    bf = int(b, 2).to_bytes(8)  # 8 bytes needed for IEEE 754 binary64.\r\n    return struct.unpack('>d', bf)[0]\r\n\r\ndef check_float(z: float, e: int = 3):\r\n    symbolic_z= sympy.sympify(z)\r\n\r\n    symbolic_product = 1\r\n    z_product = 1\r\n    for _ in range(e):\r\n        symbolic_product *= symbolic_z\r\n        z_product *= z\r\n    delta = symbolic_product - z_product\r\n    delta_pow = symbolic_product - np.power(z, e).item()\r\n    if  delta_pow!=delta:\r\n       \r\n        print(f'{z}: delta product {delta}, delta pow(., {e}) {delta_pow}')\r\n    return delta_pow!=delta\r\n\r\nz=np.frombuffer(b'\\xb6\\xfe\\x10\\x16\\xe1\\xa7\\xda?')\r\nz=float(z.item())\r\ncheck_float(z)\r\n\r\nx=np.random.rand(40_000,)\r\nn=0\r\nfor z in x:\r\n    symbolic_z= sympy.sympify(z)\r\n    n+= check_float(z)\r\nprint(f'{n}\/{x.size} = {n\/x.size}')\r\n\r\nfor ii in range(10_000):\r\n    z = bin_to_float(bin(random.getrandbits(64)))\r\n    check_float(z)\r\n```\r\nIn the cases generated with this script the values for `z*z` agree well with `np.power(z, 2)`, but already for `e=3` there are large differences. For example for `z=-15656763.486516586`. In the cases generated the calculated product is a better approximation than `np.power(z, 3)`.\r\n\r\n","> > Adding more special cases is possible, but we need to decide which cases are important\r\n> \r\n> @eendebakpt out of curiosity, why optimize for `np.power(x, 2)`? one could as well just use `x * x`,\r\n\r\nThat is a fair point. There are several issues (links in the first comment) where users experience a slow np.power (also for other exponents), and this would be one way to provide a performance improvement. Replacing `np.power(x, 2)` with `x*x` is not always possible or elegant (for example when `np.power` is used inside a method where the exponent is one of the arguments).\r\n\r\nBut whether numpy should special case for certain exponents (and if so which ones) is not clear to me. It is on the agenda for the next triage meeting I believe, that also seems like a good place to discuss.","> @eendebakpt out of curiosity, why optimize for `np.power(x, 2)`? one could as well just use `x * x`,\r\n\r\nNumpy already optimizes `x ** 2` to call `np.square` - I think in any higher level language one wants to try to avoid people having to think about performance too much, but rather focus on code correctness.","We talked about this at the triage meeting and there wasn't a ton of appetite for adding special handling for anything besides 0.5 and 2, mostly because of concerns about accuracy.\r\n\r\nI did some github code searches for uses with common variable names like `x` and `np.power(..., 2)` definitely comes up a lot (3200 results with `x`), `np.power(..., 0.5)` less so (a few hundred results), so if there's any marginal cost to supporting square roots, I would drop that. `np.power(... 3)` seems to be relatively popular (1000 results), so if that can be supported without precision loss that seems worth doing as well, but nothing more.","@ngoldbaum Thanks for the feedback! I implemented `e=2` and `e=.5`. \r\n\r\nI tried some experiments to determine the accuracy. Not sure there are representative, but here are some results.\r\n\r\nFor `e=3` the precision loss seems small (cases found so far are always within one ULP), but the results can be less precise. So unless there is an expert that can confirm adding `e=3` is ok, I will leave it out of this PR. One example for type float32 (the blue data points are a bit further from the exact values than the red ones):\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/883786\/385eae97-7eda-4f58-b846-530f58bcb0ed)\r\n\r\nFor `e=0.5` all the differences between `sqrt` and `np.power` seem to equal far off from the exact value (and all within one ULP). An example: \r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/883786\/900efd46-e91d-4eb0-89c6-59c1b5488cd7)\r\n\r\n"],"labels":["triaged"]},{"title":"BUG: `beta` random generator can output exact zeros","body":"### Describe the issue:\n\nThe PDF for the Beta distribution is exactly zero for the extreme values 0 and 1. Hence, a random number generator that samples from it should not sample points where the probability mass is zero - i.e. it should be bounded in the **open** interval `(0,1)`.\r\n\r\nAs a comparison, R's `rbeta` will never output exact zeros, but in some cases in can output exact ones, which perhaps _could_ be reasonable given that numerical precision close to 1 is more limited than close to zero:\r\n```r\r\nset.seed(123)\r\nx = rbeta(1e6, shape1=0.010582142677289599, shape2=0.21254801747114022)\r\nsum((x == 0) | (x == 1))\r\n```\r\n```\r\n20\r\n```\r\n\r\nNumpy OTOH will output exact zeros with a non-negligible probability if the parameters are small:\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nrng = np.random.default_rng(seed=123)\r\nx = rng.beta(0.010582142677289599, 0.21254801747114022, size=int(1e6))\r\nnp.sum(x == 0), np.sum(x == 1)\n```\n\n\n### Error message:\n\n```shell\n(387, 13)\r\n```\n```\n\n\n### Python and NumPy Versions:\n\n1.26.1\r\n3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]\n\n### Runtime Environment:\n\n[{'numpy_version': '1.26.1',\r\n  'python': '3.11.0 (main, Mar  1 2023, 18:26:19) [GCC 11.2.0]',\r\n  'uname': uname_result(system='Linux', node='debian', release='6.1.0-18-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01)', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/david\/anaconda3\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 20,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\n\n### Context for the issue:\n\n_No response_","comments":["Does the value of the beta distribution of the pdf not depend on the values of alpha and beta? In particular for the values provided the pdf is non-zero at 0 and 1.\r\n\r\n```\r\nfrom scipy.stats import beta as beta_dist\r\nimport matplotlib.pyplot as plt\r\nx=np.linspace(0, 1, 1000)\r\nplt.figure(10);plt.clf()\r\n\r\nalpha, beta=0.010582142677289599, 0.21254801747114022\r\np=beta_dist.pdf(x, alpha, beta)\r\nplt.plot(x, p, label=f'{alpha=} {beta=}')\r\n\r\nalpha, beta=0.5, 0.5\r\np=beta_dist.pdf(x, alpha, beta)\r\nplt.plot(x, p, label=f'{alpha=} {beta=}')\r\n\r\nalpha, beta=2, 2 \r\np=beta_dist.pdf(x, alpha, beta)\r\nplt.plot(x, p, label=f'{alpha=} {beta=}')\r\nplt.legend()\r\n```\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/883786\/5a5f87e8-e9e7-4fbc-90de-cf43049c5a39)\r\n\r\n","Per [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Beta_distribution), the PDF of the beta distribution is as follows:\r\n$P(x|\\alpha,\\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta - 1}$\r\n\r\nwith $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}$\r\n\r\nIf $x=0$, the PDF will involve powers of zero, which will be either inifite (for $\\alpha<1$), one (for $\\alpha=1$), or zero (for $\\alpha>1$).\r\n\r\nIf the distribution were to be defined in such a way that zeros are allowed, then it means a random sampler would only ever choose zeros when $\\alpha<1$, which is not one's idea of a beta distribution, and not what numpy is outputting.","Numpy can only output floating point numbers. If numpy would sample perfectly, and then convert those samples to floating point I would expect zeros both for $\\alpha < 1$ and $\\alpha > 1$.\r\n\r\nThe implementation of the beta distribution sampler is here https:\/\/github.com\/numpy\/numpy\/blob\/7f1c8cbe0c6fa4e554b3b1e4d7dc2f03fababbec\/numpy\/random\/src\/distributions\/distributions.c#L411. The algorithm is not perfect (also see https:\/\/github.com\/numpy\/numpy\/issues\/688), but it does seem to give a reasonable sampling. \r\n\r\nA test to compare the CDF from a large sample with the true CDF\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.stats import beta as beta_dist\r\n\r\nrng = np.random.default_rng(seed=123)\r\nalpha, beta = 0.010582142677289599, 0.21254801747114022\r\n\r\nsamples = rng.beta(alpha, beta, size=int(100e6))\r\n\r\n# from samples determine a CDF\r\ncount, bins_count = np.histogram(samples, bins=np.linspace(0, 1., 1500)) \r\nsampled_pdf = count \/ sum(count) \r\nsampled_cdf = np.cumsum(sampled_pdf)   \r\n\r\n# calculate the CDF \r\ncentres  = ( bins_count[1:]+bins_count[:-1])\/2\r\np=beta_dist.cdf(centres, alpha, beta)\r\n\r\n# plot results\r\nplt.figure(10); plt.clf()\r\nplt.plot(centres, sampled_cdf, label=\"CDF from samples\") \r\nplt.plot(centres, p, '-',  label=f'CDF {alpha=} {beta=}')\r\nplt.legend() \r\n\r\nplt.figure(20); plt.clf()\r\nplt.plot(centres, p - sampled_cdf, '.',  label=\"delta\") \r\nplt.ylabel('Delta between CDF and sampled CDF') \r\n```\r\nResults in\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/883786\/bedf330f-d5ec-49b0-aa03-e88ac977fcd7)\r\n\r\nSo indeed the density of very small samples (including zero) is a bit high.","Not all infinities are the same. These singularities are not Dirac deltas. In the realm of pure real numbers, the probability _mass_ of any real number (like 0, 0.5, or 1) from a distribution without Dirac deltas is 0 (as the set of a single real number has measure 0). The only things that have nonzero probability mass are intervals, and there is no step-like behavior at 0 or 1. The CDF just has infinite slope exactly on 0, but the CDF doesn't jump up discontinuously as it would with a Dirac delta.\r\n\r\nFWIW, this graph is plotting the wrong thing. The empirical CDF from the histogram approximates the true CDF evaluated at the _right_ edge of each bin, not the CDF evaluated at the center. `p = beta_dist.cdf(bins_count[1:], alpha, beta)`. The resulting residual plot looks much nicer.\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/46135\/c5f105b3-99ed-4a5d-ae91-8f0fe17cfbc5)\r\n\r\nFinite floating point implementations of probability distributions are always approximations. The floating point number that you get out at the end does not really represent the actual real number that has the same value. It's more fuzzy than that. I'd love to say that it represents some interval _around_ the real number with the same value, but that's not actually achievable in practice with practical implementations of nontrivial nonuniform distributions. But it's close to being true. The probability of getting the floating point value of `0.0` from `rng.beta(a, b)` does not represent the probability of getting real number 0 from $B(\\alpha, \\beta)$ (which is, of course 0). It's closer to the probability of getting a real number in the interval $[0, \\epsilon]$ for some $\\epsilon$ (sadly, not `np.finfo(float).eps`, but any means). And that's a finite, well-behaved value even if you keep that interval bounded or unbounded.","While it _might_ be the case that floating points of exactly zero are sometimes the correct answer given that they cannot represent every real number (even if overrepresented in this particular example), in practice one of the usages of random number generators is to pass aggregations of the results to statistical functionalities like PDF \/ CDF \/ other generators \/ etc.; for which values of zero in the specific case of a beta distribution are rather problematic.","0 and 1 are quite valid outputs of a (real, ideal) beta distribution (the CDF is perfectly well-defined and continuous on the boundary of the interval as well as the interior). If specific downstream applications have extra requirements, they can apply them as needed."],"labels":["00 - Bug"]},{"title":"BUG: Cannot create zero-length array-like object via __array_interface__ providing null pointer as data","body":"### Describe the issue:\n\nI am trying to create a custom array-like object that stores zero-length one-dimensional array using `__array_interface__`. As the array is zero-length and therefore does not contain any data, I provide a null pointer as the `data` field of `__array_interface__` (i.e. `\"data\": (0, False)`). Then my object is seemingly treated as a scalar by `np.array`, despite the fact that `shape` is `(0,)`. If I provide anything different from 0 as the data, everything works.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nclass TestArray:\r\n    def __init__(self):\r\n        self.__array_interface__ = {\r\n            \"data\": (0, False),\r\n            \"strides\": None,\r\n            \"descr\": [(\"\", \"<f8\")],\r\n            \"typestr\": \"<f8\",\r\n            \"shape\": (0,),\r\n            \"version\": 3,\r\n        }\r\n\r\nnp.array(TestArray())\n```\n\n\n### Error message:\n\n```shell\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[24], line 1\r\n----> 1 np.array(TestArray())\r\n\r\nTypeError: float() argument must be a string or a real number, not 'TestArray'\n```\n\n\n### Python and NumPy Versions:\n\n1.26.1\r\n3.11.6 (main, Oct  2 2023, 13:45:54) [GCC 12.3.0]\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["Hmmm, there is some strange \"scalar\" logic, for `data=NULL` (not sure how much for data not being passed).\r\n\r\nNumPy requires an allocation itself for empty arrays also, although even `NULL` should probably just work (by the fact that NumPy just allocates the data), at least if `strides` is also NULL.\r\n\r\nSo the workaround is to not use NULL, which you may have noticed.  Otherwise, this needs a bit of thought, and mostly cleaning out...","Thanks! Is that correct that it is a safe workaround to use arbitrary non-zero number as `data` here?","Arbitrary data should be fine, may be better if it points to at least one byte of valid memory."],"labels":["00 - Bug"]},{"title":"DOC: Mention `np.lib.NumPyVersion()` in the migration guide","body":"Some users will have to do conditional imports, SciPy uses `np.lib.NumPyVersion(np.__version__) > \"2.0.0\"` which seems like a decent pattern.\r\n\r\nI think we should mention this pattern as one possible way to do things in the migration guide.","comments":["Good idea - except things will work a bit smoother over the next 1-2 months if it's `>= \"2.0.0b1\"`.","Hello, is this issue open. I would like to work on this issue.","Thanks for the offer @sharsha315, but please pick an issue that is not labelled with the 2.0.0 milestone.","Thanks for the information @rgommers."],"labels":["04 - Documentation"]},{"title":"Handle warning about private API and pybind11 before 2.0.0rc1","body":"In 2.0.0b1 we still have a very verbose and scary-looking warning like this:\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.0.dev0 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled against NumPy 2.0.\r\n...\r\npybind11 note: You may see this message if using pybind11,\r\nthis is not problematic at pre-release time\r\nit indicates the need for a new pybind11 release.\r\n```\r\n\r\nFull warning:\r\n\r\n<details>\r\n\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.0.dev0 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled against NumPy 2.0.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\neither downgrade NumPy or update the failing module (if available).\r\n\r\nNOTE: When testing against pre-release versions of NumPy 2.0\r\nor building nightly wheels for it, it is necessary to ensure\r\nthe NumPy pre-release is used at build time.\r\nThe main way to ensure this is using no build isolation\r\nand installing dependencies manually with NumPy.\r\nFor cibuildwheel for example, this may be achieved by using\r\nthe flag to pip:\r\n    CIBW_BUILD_FRONTEND: pip; args: --no-build-isolation\r\ninstalling NumPy with:\r\n    pip install --pre --extra-index-url https:\/\/pypi.anaconda.org\/scientific-python-nightly-wheels\/simple\r\nin the `CIBW_BEFORE_BUILD` step.  Please compare with the\r\nsolutions e.g. in astropy or matplotlib for how to make this\r\nconditional for nightly wheel builds using expressions.\r\nIf you do not worry about using pre-releases of all\r\ndependencies, you can also use `--pre --extra-index-url` in the\r\nbuild frontend (instead of build isolation).\r\nThis will become unnecessary as soon as NumPy 2.0 is released.\r\n\r\nIf your dependencies have the issue, check whether they\r\nhave nightly wheels build against NumPy 2.0.\r\n\r\npybind11 note: You may see this message if using pybind11,\r\nthis is not problematic at pre-release time\r\nit indicates the need for a new pybind11 release.\r\n\r\n```\r\n\r\n<\/details>\r\n\r\nWe need to finalize this before `2.0.0rc1` in one of two ways:\r\n1. Turn the warning into a hard error,\r\n2. Silence the warning.\r\n\r\nAt https:\/\/github.com\/numpy\/numpy\/issues\/25918#issuecomment-1987316018 we discussed a bit. @seberg said:\r\n_\"That huge warning is seems only semi-useful, it would be nice to make it a hard error by the time we release, otherwise we don't protect users from potentially bad results\/crashes due to a module that wasn't recompiled. I don't care much about `rc1` specifically it just seems like a plausible\/reasonable warning shot for pybind11.\"_\r\n\r\nI said: _\"Well, it must be done either before `rc1` or not at all. The way it looks now, it actually seems fairly safe to disable the warning, leaving the compat shim in place for 2.0, and make it a hard error on `main` in a month or so. Probably best to wait and see how a `pybind11` release is coming along, and decide in a week or so?\"_\r\n\r\nThe outcome will depend on https:\/\/github.com\/pybind\/pybind11\/issues\/5009. If `pybind11` manages to get a release out soon enough, we may be able to turn the warning into an error. If not, then it'll be too disruptive to do so.","comments":["I am not sure I can agree that silencing the warning is an option.  Remember that without the fixes at least in some parts, SciPy would have just returned incorrect results.\r\nI can accept keeping the warning for `pybind11`s sake, but it seems to me users must have a fighting chance to know that why they may be seeing random nonsense or crashes."],"labels":["17 - Task","54 - Needs decision"]},{"title":"DOC:  `NpzFile` is documented to ignore files without a `.npy` extension - but it doesn't","body":"### Issue with current documentation:\r\n\r\nSee the docs here: https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.lib.npyio.NpzFile.html#numpy.lib.npyio.NpzFile\r\n\r\nBut int he code, Npy files are silently loaded https:\/\/github.com\/numpy\/numpy\/blob\/82eca7bae8249f2576d28dae0800147546d0604d\/numpy\/lib\/_npyio_impl.py#L199C1-L200C37\r\n\r\n\r\n### Idea or request for content:\r\n\r\nMy suggestion would be to ignore them or even better raise an exception.","comments":[],"labels":["04 - Documentation"]},{"title":"BUG: numpy.power result is nondeterministic when AVX512 enabled","body":"### Describe the issue:\n\n`numpy.power` result is nondeterministic when the exponent is a view with certain strides and AVX512 is available. Problem goes away with `NPY_DISABLE_CPU_FEATURES=\"AVX512F\"`\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nfor r in range(1000):\r\n    for c in range(1, 10):\r\n        src = np.linspace(0, 1, num=r * c).reshape(r, c)\r\n        for j in range(0, c):\r\n            x = src[:, j]\r\n            if not np.array_equal(np.power(1.2, x), np.power(1.2, x)):\r\n                print(f\"diff! {(r,c,j)=}; {x.__array_interface__=}\")\r\n                break\n```\n\n\n### Error message:\n\n```shell\nSnippet of results:\r\ndiff! (r,c,j)=(3, 3, 1); x.__array_interface__={'data': (94083650133912, False), 'strides': (24,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (3,), 'version': 3}\r\ndiff! (r,c,j)=(134, 5, 4); x.__array_interface__={'data': (94083654872944, False), 'strides': (40,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (134,), 'version': 3}\r\ndiff! (r,c,j)=(134, 8, 2); x.__array_interface__={'data': (94083654872928, False), 'strides': (64,), 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (134,), 'version': 3}\n```\n\n\n### Python and NumPy Versions:\n\n1.26.4\r\n3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n\n### Runtime Environment:\n\n[{'numpy_version': '1.26.4',\r\n  'python': '3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) '\r\n            '[GCC 12.3.0]',\r\n  'uname': uname_result(system='Linux', node='<redacted>, release='6.1.0-18-cloud-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01)', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX',\r\n                                'AVX512_CLX',\r\n                                'AVX512_CNL',\r\n                                'AVX512_ICL'],\r\n                      'not_found': ['AVX512_KNL', 'AVX512_KNM', 'AVX512_SPR']}},\r\n {'filepath': '<redacted>\/conda_package_mkl\/lib\/libmkl_rt.so.2',\r\n  'internal_api': 'mkl',\r\n  'num_threads': 48,\r\n  'prefix': 'libmkl_rt',\r\n  'threading_layer': 'intel',\r\n  'user_api': 'blas',\r\n  'version': '2023.2-Product'},\r\n {'filepath': '<redacted>\/conda_package_llvm_openmp\/lib\/libomp.so',\r\n  'internal_api': 'openmp',\r\n  'num_threads': 96,\r\n  'prefix': 'libomp',\r\n  'user_api': 'openmp',\r\n  'version': None}]\n\n### Context for the issue:\n\nThe result is unexpectedly nondeterministic and causes flaky tests.","comments":["Occasionally, the strided input array overlaps in memory with the output\/destination array, preventing SIMD from being used. The output of SIMD can differ from the scalar version by at most 4 ULP. You could adjust your test to account this for this difference.   ","@seberg is it concerning to see this check fail? https:\/\/github.com\/numpy\/numpy\/blob\/71ab90631eed8667b33bf80ffde761df0adff9b7\/numpy\/_core\/src\/umath\/loops_umath_fp.dispatch.c.src#L247 \r\n\r\nI always assumed we never hit these conditions and they were there just as a sanity check. ","> I always assumed we never hit these conditions and they were there just as a sanity check.\r\n\r\nAccumulations do hit it, although we know that ahead of time really and it should be a very clear pattern of 1 stride\/step difference between inputs.\r\nWhat I have been saying about that part is that it would be plausible to explicitly pick a different loop ahead of time for accumulations.","If it fails with the above code, then I would assume `npyv_loadable_stride_` is failing, which seems acceptable (not sure about the conditions). "],"labels":["00 - Bug","component: SIMD"]},{"title":"DOC: add nit-picky mode to NEP building","body":"Now that regular doc builds are using nit-picky mode, we could get NEPs to also build cleanly. Curently there are [39 warnings](https:\/\/app.circleci.com\/pipelines\/github\/numpy\/numpy\/25555\/workflows\/8ccf567d-fe17-4b11-9114-cad4faba20b2\/jobs\/38840), mostly in NEP 13.\r\n\r\nOnce the warnings are gone, we should add a `-n` option to make sure they don't come back.\r\n\r\nFor some examples, see [all the great work @F3eQnxN3RriK](https:\/\/github.com\/numpy\/numpy\/pulls?q=is%3Apr+author%3AF3eQnxN3RriK) (and others) did to get the regular docs build clean.","comments":[],"labels":["component: documentation"]},{"title":"MAINT: What should we do about np.strings.partition?","body":"As of NumPy 1.x, an array returned from `np.char.partition` has one more dimension than the input array, and includes the separator string n times, where n is the number of items in the input array.\r\n\r\n```python3\r\n>>> import numpy as np\r\n>>> arr = np.array(['hello,world', 'something,else'])\r\n>>> np.char.partition(arr, ',')\r\narray([['hello', ',', 'world'],\r\n       ['something', ',', 'else']], dtype='<U9')\r\n```\r\n\r\nIn the context of calling `partition` on arrays, it doesn't make much sense to me to return the separator n times. So, the question is, should we maybe diverge from the CPython behavior and do something less wasteful than that in NumPy? If so, will we make it to rc1? I can work on a PR as soon as we got some consensus on what to return instead, but if we don't make it, let's remove `np.strings.partition` for now and leave it in `np.char`, so that we have freedom over what its API is going to be.\r\n\r\nTo me, returning the following 3-tuple makes more sense (maybe others have better ideas):\r\n1. bool array that signifies whether the separator was found (1 byte per item vs 4 bytes per item for unicode arrays)\r\n2. string array for all the items before the separator\r\n3. string array for all the items after the separator\r\n\r\ncc @ngoldbaum @mhvk ","comments":["That's a tricky one. Certainly, in your example having 9x4 bytes for each comma or empty string seems excessive. I think if we do anything, though, it should follow the order of CPython, i.e., string array of before, string array of separator\/empty, string array of after.\r\n\r\nFor me, what complicates this further is that `np.char.split` returns an object array of list:\r\n```\r\nIn [6]: np.char.partition(np.array(['0 1 2', 'a']), ' ')\r\nOut[6]: \r\narray([['0', ' ', '1 2'],\r\n       ['a', '', '']], dtype='<U3')\r\n\r\nIn [7]: np.char.split(np.array(['0 1 2', 'a']))\r\nOut[7]: array([list(['0', '1', '2']), list(['a'])], dtype=object)\r\n```\r\nI don't think it is very nice if `.split()` returns something where you have to index the element to get the parts, while for `.partition()` one first has to index the part (to get all elements).\r\n\r\np.s. Since this is not obvious, perhaps for now leaving it just in `np.char` is not a bad idea...","The problem with `split` is the API contract forces us to produce a ragged array.\r\n\r\n`partition` is different because you'll always get 3 entries in the trailing dimension.\r\n\r\nI agree it would be best if we could figure out a uniform way to handle this. For `split` we could do things like setting entries that come \"after\" the last split to empty string or force people specify `maxsplit`, but fundamentally the python API doesn't really make sense for an ndarray library. I'm not sure offhand what's best.\r\n\r\npyarrow doesn't have a `str.partition` equivalent, but it looks like they handle this in `pyarrow.compute.split_pattern` by returning a `ListArray` (e.g. a ragged array):\r\n\r\n```\r\nIn [4]: a\r\nOut[4]:\r\n<pyarrow.lib.StringArray object at 0x104eaa860>\r\n[\r\n  \"hello\",\r\n  \"world\"\r\n]\r\n\r\nIn [5]: pa.compute.split_pattern(a, pattern=\"l\")\r\nOut[5]:\r\n<pyarrow.lib.ListArray object at 0x158561840>\r\n[\r\n  [\r\n    \"he\",\r\n    \"\",\r\n    \"o\"\r\n  ],\r\n  [\r\n    \"wor\",\r\n    \"d\"\r\n  ]\r\n]\r\n```\r\n\r\nFor the specific question posed in the issue, I think returning a boolean array instead of an array containing empty strings or the partition character is probably best, since I suspect that most people will just convert the array to a bool anyway to see where there are valid splits.","I guess one thing to consider is whether there is any specific implementation that would allow one to write a routine that works on arrays, and automatically works on single strings as well. This is partially what makes me like the tuple route but also makes me feel that the middle one, for separator, should be a string array, with separator or empty as entries. It means one can more easily reconstruct things.","For some historical context, the reason the separator was returned in the python implementation is [to use it as a boolean flag](https:\/\/marc.info\/?l=python-dev&m=112529473001162&w=2). Reconstructing the original string is still trivial since the user knows the separator they passed to `partition` in the first place. And to me, it makes more sense to return a boolean if the intention is to signal that a match was found or not.\r\n\r\nThat said, keeping consistency with the single-string case is also very worth doing so if the only way to keep that is to return a string array of separators or empty string, that's fine with me.\r\n\r\nI also looked a little bit at the `split` question, it looks like at least ONNX defines a [padded split](https:\/\/github.com\/onnx\/onnx\/blob\/a600e2fe60fcec561a448aabacc43033e5025220\/onnx\/reference\/ops\/op_string_split.py#L25) wrapper around `char.split`, so there's at least one piece of prior art where someone went out of their way to not get the object array of lists, despite the memory overhead.","> For split we could do things like setting entries that come \"after\" the last split to empty string or force people specify maxsplit\r\n\r\nI think we should probably leave `split` alone and have it return an array of dtype `object`. Calling `split` on an ndarray makes little sense and, to me, the current behavior is the least confusing to the end user.\r\n\r\n> That said, keeping consistency with the single-string case is also very worth doing so if the only way to keep that is to return a string array of separators or empty string, that's fine with me.\r\n\r\nI don't understand what exactly consistency means in this context. Would this be consistent?\r\n\r\n```python3\r\n>>> import numpy as np\r\n>>> a = np.array(['hello,world'])\r\n>>> np.strings.partition(a, ',')\r\n(array(['hello'], dtype='<U5'), array([ True]), array(['world'], dtype='<U5'))\r\n>>> a = np.str_('hello,world')\r\n>>> np.strings.partition(a, ',')\r\n(np.str_('hello'), np.True_, np.str_('world'))\r\n```\r\n\r\n> I also looked a little bit at the split question, it looks like at least ONNX defines a [padded split](https:\/\/github.com\/onnx\/onnx\/blob\/a600e2fe60fcec561a448aabacc43033e5025220\/onnx\/reference\/ops\/op_string_split.py#L25) wrapper around char.split, so there's at least one piece of prior art where someone went out of their way to not get the object array of lists, despite the memory overhead.\r\n\r\nThis is interesting. Maybe worth adding it as a separate API then? So have both `numpy.strings.split` and `numpy.string.padded_split`?","> Would this be consistent?\r\n\r\nI think so, yes. You can pass a single string and an array into the function and you get back the correct result either way.\r\n\r\n>  Maybe worth adding it as a separate API then? So have both numpy.strings.split and numpy.string.padded_split?\r\n\r\nYeah, let's do this and not try to force `split` to have an API that makes sense for an array library. \r\n\r\nMaybe we could do the same for the new API we're spitballing for `partition`? Although I can't think of a good name to use. That way we can leave `partition` in `np.strings`, but then when we come up with a new function that makes more sense, add it and add a note to the `partition` docstring that the other function is probably what people want. You could also do it with a keyword argument, but that's likely less discoverable.\r\n\r\nMy only concern with hiding `partition` and `rpartition` in numpy 2.0 is it means `np.strings` isn't quite a drop-in replacement for `np.char`. But also [it's not exactly a popular function](https:\/\/github.com\/search?q=%22np.char.partition%22&type=code) so maybe that's fine.","I think in my example I wasn't quite clear, if one uses `np.string.partition` then there is of course no problem, but I was, arguably wrongly, thinking of a function that takes some string input `s` and then doing `s.partition(...)` - and how it would be nice if the result could be interpreted independently of whether `s` was `str` or array-like. But of course, this would only work for `chararray`, so is not directly relevant to a new *function* in `np.strings`.\r\n\r\nAnyway, overall I think for both `partition` and `split`, what is returned by `str` is just not quite useful for what needs to be returned by an array routine. In that respect, it would seem fine to delay this a bit, and think, e.g., where there would be a more general routine that allows one to select which part, or how many splits, one wants.","It would be good to get this off the list of blocking items for `2.0.0rc1`. It should require only a diff of a few lines I think, e.g. adding an underscore to the names to ensure they are not exposed publicly yet. Could you open a PR @lysnikolaou?","@rgommers Opened #26033.","Closed by https:\/\/github.com\/numpy\/numpy\/pull\/26033.","Should we maybe keep this open until `partition` is implemented as a ufunc?","Ah sure, sorry for closing prematurely","Changed the milestone so it doesn\u2019t show up as a 2.0 blocker."],"labels":["component: numpy.strings"]},{"title":"BUG: `np.polynomial.Polynomail.fit` gets failed Singular Value Decomposition for `deg=0` when there is only 1 datapoint to fit","body":"### Describe the issue:\n\nHello,\r\nI found that calling `np.polynomial.Polynomial.fit` fails with a `LinalgError` (see below) when `x` and `y` represent a single datapoint and `deg=0` (average of the data). As soon as `x` and `y` have at least 2 entries, everything works again.\r\n\r\n`np.polyfit` on the other hand, handles this very special case properly.\r\n\r\nIn the reproducible example, there are 3 combinations yielding:\r\n- \u274c fails (`do_fail=True` \u27a1\ufe0f size 1, `poly_degree=0`)\r\n![grafik](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/011746b7-e9bf-40da-8126-7a56f541eaa0)\r\n- \u2705 works (`do_fail=False` \u27a1\ufe0f size 2, `poly_degree=0`)\r\n![grafik](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/51430cdb-93da-40d9-aeb7-16ec0d246753)\r\n- \u2705 works (`do_fail=False` \u27a1\ufe0f size 2, `poly_degree=1`)\r\n![grafik](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/8ba6c499-faaa-4d48-823d-9ffc0ff86328)\r\n\n\n### Reproduce the code example:\n\n```python\n### Imports ###\r\n\r\nimport traceback\r\n\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\n\r\n### Test ###\r\n\r\n# the xy-data to fit are set up\r\ndo_fail = True\r\npoly_degree = 0  # 0 will fail if `do_fail=True`, 1 will work\r\n\r\nif do_fail:\r\n    x_fit = np.array([9.0])\r\n    y_fit = np.array([1.0])\r\n\r\nelse:\r\n    x_fit = np.array([8.0, 9.0])\r\n    y_fit = np.array([1.0, 2.0])\r\n\r\n# `np.polyfit` will not fail\r\npoly_old = np.polyfit(x_fit, y_fit, deg=poly_degree)\r\n\r\n# `np.polynomial.Polynomial.fit` will fail for size 1 and `deg`=0\r\ntry:\r\n    poly_new = np.polynomial.Polynomial.fit(x_fit, y_fit, deg=poly_degree)\r\nexcept np.linalg.LinAlgError:\r\n    poly_new = None\r\n    traceback.print_exc()\r\n\r\n\r\n### Visualisation ###\r\n\r\nx_predict = np.linspace(start=0.0, stop=10.0, num=101)\r\nplt.scatter(x_fit, y_fit, marker=\"o\", c=\"blue\", s=100.0, label=\"data to fit\")\r\nplt.plot(\r\n    x_predict,\r\n    np.polyval(poly_old, x_predict),\r\n    label=\"np.polyfit\",\r\n    color=\"blue\",\r\n    linewidth=5.0,\r\n)\r\n\r\nif poly_new is not None:\r\n    plt.plot(\r\n        x_predict,\r\n        poly_new(x_predict),\r\n        label=\"np.polynomial.Polynomial.fit\",\r\n        color=\"cyan\",\r\n        linestyle=\"--\",\r\n        linewidth=3.0,\r\n    )\r\n# end if\r\n\r\nplt.legend()\r\n\r\nplt.show()\n```\n\n\n### Error message:\n\n```shell\nSomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\polyutils.py:303: RuntimeWarning: divide by zero encountered in scalar divide\r\n  off = (old[1]*new[0] - old[0]*new[1])\/oldlen\r\nSomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\polyutils.py:304: RuntimeWarning: divide by zero encountered in scalar divide\r\n  scl = newlen\/oldlen\r\nSomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\polyutils.py:372: RuntimeWarning: invalid value encountered in add\r\n  return off + scl*x\r\n ** On entry to DLASCLS parameter number  4 had an illegal value\r\nTraceback (most recent call last):\r\n  File \"SomeUserPath\\numpy_poly.py\", line 23, in <module>\r\n    poly_new = np.polynomial.Polynomial.fit(x_fit, y_fit, deg=poly_degree)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"SomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\_polybase.py\", line 1037, in fit\r\n    res = cls._fit(xnew, y, deg, w=w, rcond=rcond, full=full)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"SomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\polynomial.py\", line 1362, in polyfit\r\n    return pu._fit(polyvander, x, y, deg, rcond, full, w)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"SomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\polynomial\\polyutils.py\", line 664, in _fit\r\n    c, resids, rank, s = np.linalg.lstsq(lhs.T\/scl, rhs.T, rcond)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"SomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\linalg\\linalg.py\", line 2326, in lstsq\r\n    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"SomeUserPath\\.venv311\\Lib\\site-packages\\numpy\\linalg\\linalg.py\", line 124, in _raise_linalgerror_lstsq\r\n    raise LinAlgError(\"SVD did not converge in Linear Least Squares\")\r\nnumpy.linalg.LinAlgError: SVD did not converge in Linear Least Squares\n```\n\n\n### Python and NumPy Versions:\n\n`numpy`: 1.26.4\r\n`python`: 3.11.6 (tags\/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]\n\n### Runtime Environment:\n\n```bash\r\n[{'numpy_version': '1.26.4',\r\n  'python': '3.11.6 (tags\/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 '\r\n            '64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='SomeUsersPC', release='10', version='10.0.22631', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': SomeUserPath\\\\.venv311\\\\Lib\\\\site-packages\\\\numpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n```\n\n### Context for the issue:\n\nI ran into this issue when running a `pytest` on a project that caused this relatively **unlikely edge case**.\r\nThe **priority** for this is thus **low** I guess because this will probably not occur in real world applications.\r\n\r\nHowever, `np.polyfit` handles it correctly, so it would be more a matter of consistency.\r\n\r\nThanks for your time! \ud83d\ude43 ","comments":["I can confirm the issue. It seems the problem is in `np.polynimial.polyutils.get_domain` which for the example above results in \r\n```\r\nIn [6]: pu.getdomain([9.0])\r\nOut[6]: array([9., 9.])\r\n```\r\n\r\nFitting a degree zero polynomial to a single datapoint is probably not used a lot, but it does not seem unreasonable. I will create a PR to fix this."],"labels":["00 - Bug"]},{"title":"Action wheel builds do not happen when a tag is pushed.","body":"This turned up when building wheels for v2.0.0b1, I needed to trigger them manually. I assume this is because `push:` has been removed from `.github\/workflows\/wheels.yml`.\r\n\r\nAnother potential problem is that the commit has both a tag and `[wheel build]`. I also wonder why only use the first line of the commit when looking for `[wheel build]`. I note the `git log -1 --grep '\\[wheel build\\]'` will also find the directive. ","comments":["Xref https:\/\/github.com\/numpy\/numpy\/pull\/24535","[here's](https:\/\/github.com\/refnx\/refnx\/blob\/c5282834a5055348af57755db6dda0ddef436d02\/.github\/workflows\/release.yml#L6) how I run a build when a tag is pushed, for a personal project.","Even after #24535 there are a ton of 15 second runs of the wheel builder action, it doesn't seem to have bought much. @rgommers.","> Even after #24535 there are a ton of 15 second runs of the wheel builder action, it doesn't seem to have bought much. @rgommers.\r\n\r\nThat is not the case. There are way fewer triggers on this repo, and almost none on forks - it is now possible to find the actual runs on the first page of the action. And the email notifications went away completely. There are still some triggers on this repo, but that didn't seem avoidable given the setup.","I'm tempted to revert the changes in #24535 just to get a release out. It would also help if manually running builds in a maintenance branch uploaded to staging.","Please don't do that. The problem is https:\/\/github.com\/numpy\/numpy\/pull\/25981#issuecomment-1987653985. I'll open a PR asap.","> It would also help if manually running builds in a maintenance branch uploaded to staging.\r\n\r\nGood point. That requires some extra config. I just learned that it's possible to add options to manual triggers; https:\/\/docs.github.com\/en\/actions\/using-workflows\/workflow-syntax-for-github-actions#onworkflow_dispatchinputs. Perhaps choosing nighty vs. staging manually is useful? "],"labels":["component: CI"]},{"title":"Question: Can record or structured array fields be contiguous?","body":"Dear Numpy Team,\r\n\r\nI frequently encounter situations where I need to load data from a Pandas DataFrame into NumPy arrays, perform computations, and then update the DataFrame.\r\n\r\nTypically, I have two approaches:\r\n\r\n1) Loading required indexes or columns of the DataFrame explicitly into NumPy arrays.\r\n   This method ensures contiguous arrays but involves more lines of code and can be cumbersome.\r\n\r\n2) Loading required part of the DataFrame to a record array.\r\n   This approach is more elegant and requires less code, but the fields of the record array are not contiguous, leading to potential performance drawbacks.\r\n\r\nHere's an example showing both methods:\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Creating a sample DataFrame\r\ndates = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\r\nsymbols = ['A', 'B', 'C']\r\ndata = {'name': np.random.choice(symbols, len(dates)),\r\n        'date': dates,\r\n        'value1': np.random.rand(len(dates)),\r\n        'value2': np.random.randint(1, 10, len(dates))}\r\ndf = pd.DataFrame(data).set_index(['name', 'date']).sort_index()\r\n\r\n# 1) Export pandas DataFrame to \"ordinary\" numpy arrays\r\nnames = df.index.get_level_values('name').to_numpy('<U1')\r\ndates = df.index.get_level_values('date').to_numpy('<M8[D]')\r\nvalues1 = df['value1'].to_numpy()\r\nvalues2 = df['value2'].to_numpy()\r\n\r\nprint(\"NumPy Arrays:\")\r\nprint(\"  C_CONTIGUOUS:\", dates.flags['C_CONTIGUOUS'])\r\nprint(\"  F_CONTIGUOUS:\", dates.flags['F_CONTIGUOUS'])\r\n# C_CONTIGUOUS: True\r\n# F_CONTIGUOUS: True\r\n\r\n# 2) Export pandas DataFrame to record array\r\nrecarr = df.to_records(index_dtypes={'name': '<U1', 'date': '<M8[D]'})\r\n\r\nprint(\"\\nRecord Array Field:\")\r\nprint(\"  C_CONTIGUOUS:\", recarr.date.flags['C_CONTIGUOUS'])\r\nprint(\"  F_CONTIGUOUS:\", recarr.date.flags['F_CONTIGUOUS'])\r\n# C_CONTIGUOUS: False\r\n# F_CONTIGUOUS: False\r\n```\r\nThe choice between these methods often comes down to a trade-off between code simplicity and performance.\r\nWhile the record array approach is elegant, its non-contiguous fields can impact performance, as shown in the provided performance example.\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport numba as nb\r\n\r\n@nb.njit(['f8[::1](f8[::1], f8[::1])',  # contiguous\r\n          'f8[:](f8[:], f8[::1])'])     # strided\r\ndef func_add(arr, other):\r\n    res = np.empty_like(arr)\r\n    for i in range(arr.size):\r\n        res[i] = arr[i] + other[i]\r\n    return res\r\n\r\n# Create a sample heterogeneous DataFrame with 100,000 rows\r\ndf = pd.DataFrame({\r\n    'A': np.random.randint(0, 100, 100_000),\r\n    'B': np.random.rand(100_000),\r\n    'C': np.random.choice(['a', 'b', 'c', 'd', 'e'], 100_000)})\r\n\r\n# strided recarray column\r\nrecord_array = df.to_records(index=False)\r\nrecarr_col = record_array.B\r\n\r\n# contiguous array\r\narr = np.array(recarr_col)\r\n\r\n# another contiguous array\r\nother = arr.copy()\r\n\r\n# warmup\r\nfunc_add(arr, other)\r\nfunc_add(recarr_col, other)\r\n\r\n# time\r\n%timeit func_add(arr, other)         # contiguous\r\n%timeit func_add(recarr_col, other)  # strided\r\n# 88.5 \u00b5s \u00b1 2.52 \u00b5s per loop  # contiguous\r\n# 201 \u00b5s \u00b1 15.2 \u00b5s per loop   # strided\r\n\r\n# Python: 3.12.0\r\n# Numpy: 1.26.4\r\n# Pandas: 2.1.4\r\n```\r\nThese kind of interactions between dataframes and arrays are pretty common and as such important in data transformation pipelines.\r\nIs it possible to load record or structured arrays with contiguous fields out of the box?\r\n\r\nRegards Oyibo","comments":["In the first case you are creating a `dates` array with the single dtype `'<M8[D]'`, in the second a `recarr` with a record dtype `(numpy.record, [('name', '<U1'), ('date', '<M8[D]'), ('value1', '<f8'), ('value2', '<i8')])`. The layout of the record dtype is \"rowlike\" instead of \"columnlike\". This is the default for NumPy. You could try to convince pandas to use subarrays instead in `df.to_records`, but then indexing them would be a bit strange. \r\n\r\nIn any case, this issue is one with the conversions Pandas does, and not really one for NumPy.","@mattip Thank you for your response.\r\nI have used the export of arrays from pandas dataframes as an example to illustrate the use case. The question is not about pandas.\r\n\r\nThe default behavior of NumPy structured or record arrays is to have contiguous memory layout for the array itself, but the fields within the structured or record array are not contiguous. This is unfavorable for field-wise computations.\r\nIs it possible to create structured or record arrays with contiguous fields or is this feature not supported?\r\n```\r\n import numpy as np\r\n\r\n# Create a structured array\r\nstructured_array = np.array([(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 35)],\r\n                           dtype=[('ID', 'i4'), ('Name', 'U10'), ('Age', 'i4')])\r\nstructured_array_field = structured_array['Age']\r\nstructured_array_field.flags['C_CONTIGUOUS']\r\nstructured_array_field.flags['F_CONTIGUOUS']\r\n# False\r\n# False\r\n\r\n# Create a record array\r\nrecord_array = np.rec.array(structured_array)\r\nrecord_array_field = record_array.Age\r\nrecord_array_field.flags['C_CONTIGUOUS']\r\nrecord_array_field.flags['F_CONTIGUOUS']\r\n# False\r\n# False\r\n```\r\n","It is not possible in the general case where the different elements of the structure have different sizes, but if they are the same one can use the following rather tricky option:\r\n```\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import DummyArray\r\n\r\nxyz = np.arange(15.).reshape(3, 5)  # x, y, z, each 5 elements\r\nprint(f\"x={xyz[0]}, y={xyz[1]}, z={xyz[2]}\")\r\n\r\nnew_dtype = np.dtype({\r\n    'names': ['x', 'y', 'z'],\r\n    'formats': ['<f8', '<f8', '<f8'],\r\n    'offsets': [0, 1*5*8, 2*5*8],\r\n    'itemsize': 3*5*8\r\n})\r\ninterface = dict(xyz.__array_interface__)\r\ninterface[\"descr\"] = interface[\"typestr\"] = f\"|V{new_dtype.itemsize}\"\r\ninterface[\"shape\"] = xyz.shape[1:]\r\ninterface[\"strides\"] = xyz.strides[1:]\r\ns = np.asarray(DummyArray(interface, base=xyz))\r\ns.dtype = new_dtype\r\n\r\nprint(f\"{s['x']=}, {s['y']=}, {s['z']=}\")\r\n```\r\n(Actually used in astropy's `Distribution` class - https:\/\/github.com\/astropy\/astropy\/blob\/4b727adcd13d73d9f5efcb2105e016453effb4e4\/astropy\/uncertainty\/core.py#L92-L104)\r\n\r\nThe reason that this trick does not work for fields with different itemsize is that in that case the offsets between different elements in each field are no longer the same.\r\n\r\nIndeed, this is also why it is non-trivial to add support for this in numpy - almost all of numpy is built on arrays having strides and working with those, i.e., it counts on array elements having a fixed size. This is easy to do in the \"row\" format, but not in \"column\" format.","Of course, a different solution is just to use sub-arrays,\r\n```\r\nxyz_dtype = np.dtype([('x', '(5,)f8'), ('y', '(5,)f8'), ('z', '(5,)f8')])\r\ns2 = xyz.ravel().view(xyz_dtype)\r\nprint(f\"{s2['x']=}, {s2['y']=}, {s2['z']=}\")\r\n```\r\n\r\nFor both my possibilities,\r\n```\r\ns['x'].flags['C_CONTIGUOUS']\r\n#  True\r\ns2['x'].flags['C_CONTIGUOUS']\r\n# True\r\n```","Thank you very much @mhvk for these insightful examples.\r\nI have implemented your first solution for homogeneous arrays to structured arrays for row-like and column-like layouts, and it seems to work pretty fine. In the column-like case I've made a transposed copy of the input array. I have to look deeper into it to adjust the strides and avoid this copy.\r\n\r\nAlthough this is already pretty awesome, I'm wondering if it's possible to achieve similar functionality for structured arrays with heterogeneous data. Is there any chance at all?\r\n```\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import DummyArray\r\n\r\n\r\ndef convert_to_structured_contig_array(\r\n        arr: np.ndarray,\r\n        names: list,\r\n        rowlike: bool = True) -> np.ndarray:\r\n    \"\"\"\r\n    Convert a 2D homogeneous NumPy array into a structured array with contiguous fields.\r\n\r\n    Parameters:\r\n    - arr: NumPy array\r\n        The input 2D homogeneous array.\r\n    - names: list\r\n        List of strings specifying the field names for the structured array.\r\n    - rowlike: bool, optional (default=True)\r\n        If True, each row of the input array represents a single item in the structured array.\r\n        If False, each column of the input array represents a single item.\r\n\r\n    Returns:\r\n    - structured_array: NumPy array\r\n        The structured array with contiguous fields.\r\n\r\n    Example:\r\n        print('rowlike')\r\n        arr = np.arange(12.).reshape(3, 4)\r\n        names = ['x', 'y', 'z']\r\n        rowlike = True\r\n        struct_arr = convert_to_structured_contig_array(arr, names, rowlike)\r\n        print(arr, '\\n', struct_arr.reshape(-1, 1))\r\n        print('contiguous:', struct_arr['x'].flags['C_CONTIGUOUS'])\r\n        # contiguous: True\r\n\r\n        print('columnlike')\r\n        arr = np.arange(12.).reshape(4, 3)\r\n        names = ['x', 'y', 'z']\r\n        rowlike = False\r\n        struct_arr = convert_to_structured_contig_array(arr, names, rowlike)\r\n        print(arr)\r\n        print(struct_arr.reshape(-1, 1))\r\n        print('contiguous:', struct_arr['x'].flags['C_CONTIGUOUS'])\r\n        # contiguous: True\r\n    \"\"\"\r\n    base = arr if rowlike else arr.T.copy()\r\n    nrows, ncols = base.shape\r\n    nrows_new = ncols\r\n    ncols_new = nrows\r\n\r\n    # validate input\r\n    assert (ncols_new == len(names)) and (arr.ndim == 2)\r\n\r\n    # Create a new data type with contiguous fields\r\n    data_type = arr.dtype\r\n    itemsize = data_type.itemsize\r\n    new_dtype = np.dtype({\r\n        'names': names,\r\n        'formats': [data_type.name] * ncols_new,\r\n        'offsets': np.arange(ncols_new) * nrows_new * itemsize,\r\n        'itemsize': nrows * ncols * itemsize,\r\n    })\r\n\r\n    # Create a new interface for the contiguous array\r\n    interface = dict(base.__array_interface__)\r\n    interface[\"descr\"] = interface[\"typestr\"] = f\"|V{new_dtype.itemsize}\"\r\n    interface[\"shape\"] = (nrows_new,)\r\n    interface[\"strides\"] = base.strides[1:]\r\n\r\n    # Create the structured array with contiguous fields\r\n    contig_array = np.asarray(DummyArray(interface, base=base))\r\n    contig_array.dtype = new_dtype\r\n    return contig_array\r\n```\r\nEdit 1: \r\nSecond method to convert a homogenuous array into structured subarrays\r\n```\r\ndef convert_to_structured_subarrays(\r\n        arr: np.ndarray,\r\n        names: list,\r\n        rowlike: bool = True) -> np.ndarray:\r\n    \"\"\"Convert a 2D homogeneous NumPy array into a structured array of subarrays.\"\"\"\r\n    base = arr if rowlike else arr.T\r\n    nrows, ncols = base.shape\r\n    nrows_new = ncols\r\n    ncols_new = nrows\r\n\r\n    # validate input\r\n    assert (ncols_new == len(names)) and (arr.ndim == 2)\r\n\r\n    # Create a new data type with contiguous fields\r\n    dt_name = arr.dtype.name\r\n    fmt = f'({nrows_new},){dt_name}'\r\n    struct_dtype = np.dtype([(name, fmt) for name in names])\r\n    return base.ravel().view(struct_dtype)\r\n```\r\nEdit 2: \r\nSecond method to convert a heterogenuous array into structured subarrays as @mattip suggested, too.\r\n```\r\ndef convert_structured_to_subarrays(arr: np.ndarray) -> np.ndarray:\r\n    \"\"\"\r\n    Convert a heterogeneous NumPy array into a structured array of subarrays.\r\n\r\n    Parameters:\r\n    - arr: NumPy array\r\n        The input heterogeneous array.\r\n\r\n    Returns:\r\n    - structured_array: NumPy array\r\n        The structured array with contiguous subarrays.\r\n\r\n    Example:\r\n        struct_arr = np.array([(1, 2.), (3, 4.), (5, 6.)], dtype=[('x', 'i8'), ('y', 'f8')])\r\n        struct_subarrays = convert_structured_to_subarrays(struct_arr)\r\n        print(struct_arr)\r\n        print(struct_subarrays)\r\n        print('contiguous:', struct_subarrays['x'].flags['C_CONTIGUOUS'])\r\n        # contiguous: True\r\n    \"\"\"\r\n    # generate structured dtype\r\n    nrows = arr.size\r\n    data_type = arr.dtype\r\n    names = data_type.names\r\n    dtypes = [data_type.fields[name][0] for name in names]\r\n    struct_dtype = np.dtype([(name, dty, nrows) for name, dty in zip(names, dtypes)])\r\n\r\n    # fill empty array\r\n    struct_arr = np.empty(1, dtype=struct_dtype)\r\n    for name in names:\r\n        struct_arr[name] = arr[name]\r\n    return struct_arr\r\n```\r\n"],"labels":["33 - Question"]},{"title":"DOC: Add interactive examples with jupyterlite-sphinx","body":"I'd like to assess interest in adding interactive examples to NumPy's docs. @mattip mentioned in https:\/\/github.com\/scipy\/scipy\/issues\/19729#issuecomment-1969621588, that this was briefly discussed in one of the NumPy community meetings.\r\n\r\nFor background info. The [jupyterlite-sphinx](https:\/\/jupyterlite-sphinx.readthedocs.io\/en\/latest\/) extension offers the [try_examples](https:\/\/jupyterlite-sphinx.readthedocs.io\/en\/latest\/directives\/try_examples.html) directive, which adds a button which can be used to swap the rendered examples sections for a docstring in-place with an embedded Jupyterlite notebook. The notebooks run in-browser using a wasm based kernel like [pyodide](https:\/\/github.com\/jupyterlite\/pyodide-kernel) .\r\n\r\n I've deployed SciPy's docs with this extension in action here: https:\/\/steppi.github.io\/scipy\/reference\/index.html\r\n\r\nHere's a direct link to a page with an example: https:\/\/steppi.github.io\/scipy\/reference\/generated\/scipy.integrate.quad.html#scipy.integrate.quad.\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/20019 was recently merged, adding jupyterlite-sphinx to SciPy's docs. Examples are currently disabled (it's possible to enable or disable them for deployed docs without rebuilding), but we plan to roll them out within the next two weeks.\r\n\r\nI'd be interested in getting feedback on the extension, and if there are any changes or new features you'd like to see before it could be used for NumPy. NumPy being able to ship interactive examples would need to wait for Pyodide support for NumPy 2.0, which I suspect would not be too difficult. My understanding is the primary blocker making updates of SciPy in Pyodide difficult is the dependence on Fortran.","comments":[],"labels":["01 - Enhancement","component: documentation"]},{"title":"BUG: Fix bug in numpy.pad()","body":"See #25926 for the bug\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["- Closes #25926 ","Looks good to me! The code for `np.pad` is a bit complex, maybe refactoring can improve that (but probably not for this PR).","Please squash on merge."],"labels":["00 - Bug","09 - Backport-Candidate"]},{"title":"BUG: seeing uninitialized memory in ufuncs","body":"### Describe the issue:\n\nI do not know what happens, but I see different behavior when I repeat the same operation. I don't know exactly if this is a bug, but if it is, it could be serious because the behavior is undefined for very basic functionality in my eyes. Please close this if I am just wrong or if my usage is wrong. In this case, an assertion could solve my issue.\r\n\n\n### Reproduce the code example:\n\n```python\n#https:\/\/www.python.org\/\r\n#Python 3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0] on linux\r\n#Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport numpy as np\r\na = np.array([0.,0.,0.])\r\nb = np.divide(a,a.max(), where=~np.all(np.equal(a, 0.)))\r\n\r\nprint(b)\r\n#[6.9424691e-310 6.9424691e-310 6.9424600e-310]\r\nprint(b>0)\r\n#[ True  True  True]\r\nprint(b)\r\n#[6.9424691e-310 6.9424691e-310 6.9424600e-310]\r\n\r\na = np.array([0.,0.,0.])\r\nb = np.divide(a,a.max(), where=~np.all(np.equal(a, 0.)))\r\n\r\nprint(b)\r\n#[0. 0. 0.]\r\nprint(b>0)\r\n#[False False False]\r\nprint(b)\r\n#[0. 0. 0.]\r\n\r\nnp.__version__\r\n'1.21.6'\r\n\r\nimport sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.21.6\r\n3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0]\n```\n\n\n### Error message:\n\n```shell\nLines 12 and 22 yield different results for the exact same operation.\n```\n\n\n### Python and NumPy Versions:\n\n1.21.6\r\n3.10.5 (main, Jul 22 2022, 17:09:35) [GCC 9.4.0]\n\n### Runtime Environment:\n\nI used the www.phython.org online python 3.10 interpreter for verification.\n\n### Context for the issue:\n\nI found undefined behavior in a larger project and it took me a week to trace the bug because, in my case, this is a very basic functionality. I faced the wrong results in testing a functionality unrelated to this behavior; I accidentally created that case and had no idea that such behavior could possibly occur.","comments":["Where is tricky: it does not touch memory where the condition is falsy. So you are seeing uninitialized memory. See the [documentation](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.divide.html) of the where argument.\r\n\r\nThis comes up often enough that maybe we should add a warning if `where` is used and `out` is `None`. If we do that, what type of warning should it be?","I appreciate this clarification, and I understand the underlying problem you laid out. However, it is unclear why the memory is initialized in the second run.\r\n\r\n```\r\nimport numpy as np\r\n\r\na = np.array([0.,0.,0.])\r\n\r\nb = np.divide(a,a.max(), out=np.array([6.,3.,1.]), where=~np.all(np.equal(a, 0.)))\r\n\r\nprint(b)\r\n>>> [6. 3. 1.]\r\n\r\na = np.array([0.,0.,0.])\r\nb = np.divide(a,a.max(), where=~np.all(np.equal(a, 0.)))\r\n\r\nprint(b)\r\n>>> [0. 0. 0.]\r\n```\r\n\r\nThe behavior still differs between runs. Seemingly, one time, it is initialized with whatever is in the memory, and the second time, it zeros. After my initial question, I thought it was the same memory, but after this example, it seemingly zeros. However, using \"out\" with correctly initialized outputs solved my issue. But I don't know if this behavior is intended.  A warning should remind the user once to initialize the output efficiently.  \r\n\r\nThank you best regards\r\n\r\n","I just ran into this same issue, it was maddening.  I narrowed it down to two cases:\r\n\r\n# Unusual Behavior Example\r\n\r\n**The first and last elements of the resulting \"c\" array are unitialized memory and will just contain whatever values happen to be in RAM at the moment:**  \r\n\r\n```\r\nimport numpy as np\r\n\r\na = np.array([0,1,0])\r\nb = np.array([1,1,1])\r\n\r\naddCondition = np.where(a != 0, True, False)\r\nc = np.add(a, b, where=addCondition)\r\n```\r\n\r\n# Fixed Example\r\n\r\n**If I initialize the array first, I get what I expected (when the condition is False, just use the element from the first array instead of adding both):**\r\n\r\n```\r\nimport numpy as np\r\n\r\na = np.array([0,1,0])\r\nb = np.array([1,1,1])\r\n\r\naddCondition = np.where(a != 0, True, False)\r\nc = np.array(a)\r\nnp.add(a, b, out=c, where=addCondition)\r\n```"],"labels":["33 - Question"]},{"title":"BUG: sliding_window_view raises `ValueError` if in some dimension the window is longer than the array by 1","body":"### Describe the issue:\n\nObserve the shapes of the following arrays:\r\n```python\r\n>>> from numpy.lib.stride_tricks import sliding_window_view\r\n>>> sliding_window_view([0, 1, 2, 3], 2)\r\narray([[0, 1],\r\n       [1, 2],\r\n       [2, 3]])\r\n>>> sliding_window_view([0, 1, 2], 2)\r\narray([[0, 1],\r\n       [1, 2]])\r\n>>> sliding_window_view([0, 1], 2)\r\narray([[0, 1]])\r\n```\r\nIf `x` is a 1-dimensional array and `window_shape` is a counting number less than or equal to `len(x) + 1` then `sliding_window_view(x, window_shape)` ought to have shape `(len(x) - window_shape + 1, window_shape)`.\r\n\r\nHowever, there is a bug if the result would have length 0:\r\n```python\r\n>>> sliding_window_view([0], 2)\r\n```\r\nThe result _ought_ to be\r\n```python\r\narray([], shape=(0, 2), dtype=int64)\r\n```\r\nInstead we get\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/jdawson\/Documents\/bugs\/numpy-sliding\/.venv\/lib\/python3.12\/site-packages\/numpy\/lib\/stride_tricks.py\", line 332, in sliding_window_view\r\n    raise ValueError(\r\nValueError: window shape cannot be larger than input array shape\r\n```\r\nThis bug is present in higher-dimensional cases also.\r\n\r\nFinally, note that it is correct to raise `ValueError` if in some dimension the window shape is longer than the array by 2 or more.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nnp.lib.stride_tricks.sliding_window_view([0], 2)\n```\n\n\n### Error message:\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/jdawson\/Documents\/bugs\/numpy-sliding\/sliding_bug.py\", line 4, in <module>\r\n    np.lib.stride_tricks.sliding_window_view([0], 2)\r\n  File \"\/home\/jdawson\/Documents\/bugs\/numpy-sliding\/.venv\/lib\/python3.12\/site-packages\/numpy\/lib\/stride_tricks.py\", line 332, in sliding_window_view\r\n    raise ValueError(\r\nValueError: window shape cannot be larger than input array shape\n```\n\n\n### Python and NumPy Versions:\n\n1.26.4\r\n3.12.2 (main, Feb 25 2024, 16:36:57) [GCC 9.4.0]\n\n### Runtime Environment:\n\n[{'numpy_version': '1.26.4',\r\n  'python': '3.12.2 (main, Feb 25 2024, 16:36:57) [GCC 9.4.0]',\r\n  'uname': uname_result(system='Linux', node='5420', release='5.15.0-97-generic', version='#107~20.04.1-Ubuntu SMP Fri Feb 9 14:20:11 UTC 2024', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX',\r\n                                'AVX512_CLX',\r\n                                'AVX512_CNL',\r\n                                'AVX512_ICL'],\r\n                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/jdawson\/Documents\/bugs\/numpy-sliding\/.venv\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\n\n### Context for the issue:\n\nThe bug necessitates extra code around `sliding_window_view` to handle this case.","comments":["This does make a lot of sense to me, but on first glance, I am surprisingly unsure if that generalization is maybe surprising after all.\r\nI think the right question is whether there are use-cases where not using a value at all would be wrong (i.e. normally you use all values at least once).  I can't think of any, but I am not sure; clearly your use-case not using the values, that is what you want.\r\n\r\nNot sure why it should matter whether the window size is exactly 1 smaller at most conceptually.  Why not use `max(0, ...)` for the new dimension?\r\n\r\nA helpful thing would be to see what the prior art is in pandas\/scipy?  E.g. pandas rolling window operations and scipy.ndimage filters?","> A helpful thing would be to see what the prior art is in pandas\/scipy? E.g. pandas rolling window operations and scipy.ndimage filters?\r\n\r\nThe Python standard library has [`itertools.pairwise`](https:\/\/docs.python.org\/3\/library\/itertools.html#itertools.pairwise), for windows of length 2. For an input of length 1 its output has length 0:\r\n```python\r\n>>> import itertools\r\n>>> list(itertools.pairwise([0, 1, 2]))\r\n[(0, 1), (1, 2)]\r\n>>> list(itertools.pairwise([0, 1]))\r\n[(0, 1)]\r\n>>> list(itertools.pairwise([0]))\r\n[]\r\n```\r\n[`pandas.Series.rolling`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.rolling.html) does a different kind of operation from `sliding_window_view`: its output is as long as its input, padded with NaNs as necessary.\r\n\r\n> Not sure why it should matter whether the window size is exactly 1 smaller at most conceptually. Why not use `max(0, ...)` for the new dimension?\r\n\r\nIf (length of window) > (length of array) + 1 both raising an exception, the current behaviour, and your suggestion, which agrees with `itertools.pairwise`, are reasonable. However, changing from the former to the latter is a change of functionality, whereas my pull request for the case that (length of window) = (length of array) + 1 resolves a bug and so should be less controversial.\r\n\r\nWhether\r\n\r\n(length of result) = (length of array) - (length of window) + 1\r\n\r\nor\r\n\r\n(length of result) = max((length of array) - length of window) + 1, 0),\r\n\r\nif\r\n\r\n(length of window) = (length of array) + 1\r\n\r\nthen\r\n\r\n(length of result) = 0.","> resolves a bug and so should be less controversial\r\n\r\nI don't understand the difference in why one should be a bug and the other not, to me they seem the same:  In either case there is valid \"boundary\" data that is not part of the result.\r\nThe `pairwise` is indeed prior art, it seems `ndimage` is not (like `rolling` it always returns the same length as the input, and has a parameter which might mask boundary values `min_periods`).","> I don't understand the difference in why one should be a bug and the other not, to me they seem the same: In either case there is valid \"boundary\" data that is not part of the result.\r\n\r\nBoth\r\n\r\n1. The behaviour that (length of result) = (length of array) - (length of window) + 1; and\r\n2. The behaviour that (length of result) = max((length of array) - length of window) + 1, 0)\r\n\r\nare reasonable.\r\n\r\nHowever, the current behaviour, that (length of result) = (length of array) - (length of window) + 1 unless (length of result) would equal 0 is not reasonable. There ought not to be such an arbitrary exception.","What doth hinder this to be merged?","@seberg, the following table shows how the length of the result in a dimension depends on the lengths of the array and the window in that dimension, under the three behaviours discussed.\r\n\r\n|  | window < array + 1 | window = array + 1 | window > array + 1 |\r\n| --- | --- | --- | --- |\r\n| now | array - window + 1 | ValueError | ValueError |\r\n| this request | array - window + 1 | 0 | ValueError |\r\n| `max(0, ...)` | array - window + 1 | 0 | 0 |\r\n\r\nBoth behaviours for window > array + 1 are reasonable; this request does not change the behaviour in that case; it simply corrects the mistake in the case that window = array + 1. ","I desire a review of this request.","Sorry, most of our reviewer bandwidth is going toward fixing urgent problems required for 2.0.0rc1."],"labels":["00 - Bug"]},{"title":"Expected behaviour regarding new `copy` keyword in `__array__` implementations","body":"Moving from https:\/\/github.com\/numpy\/numpy\/pull\/25922, which added this to the docs:\r\n\r\n> NumPy will pass ``copy`` to the ``__array__`` special method in situations where\r\nit would be set to a non-default value (e.g. in a call to\r\n``np.asarray(some_object, copy=False)``). Currently, if an\r\nunexpected keyword argument error is raised after this, NumPy will print a\r\nwarning and re-try without the ``copy`` keyword argument. Implementations of\r\nobjects implementing the ``__array__`` protocol should accept a ``copy`` keyword\r\nargument with the same meaning as when passed to `numpy.array` or\r\n`numpy.asarray`.\r\n\r\nThinking through how to update some `__array__` implementations, I am wondering: what are the expectations from numpy how an `__array__` implementations handles this keyword fully or partially? \r\n\r\nWhen `copy=True` is being passed, does numpy assume that the `__array__` implementation already made that copy? But what if the implementation defers dtype casting to numpy (which is currently possible), then it might be a waste to already copy up front?\r\n\r\n_Originally posted by @jorisvandenbossche in https:\/\/github.com\/numpy\/numpy\/pull\/25922#discussion_r1512762568_\r\n            ","comments":["\r\nDeprecated though, right? The signature should already have included the `dtype` keyword, even though it was still working to not have that.\r\n\r\n> then it might be a waste to already copy up front?\r\n\r\nWhat we want to achieve I think is that only a single copy is ever made. We can have two now in many cases, because the other library may copy from its internal implementation into an array, and then numpy will make another copy. The only way to get it right is for the other library to do the copy if `copy=True`, and for numpy not to do so if the method call including `copy=True` succeeded.\r\n\r\nThe old behavior is:\r\n- the only contract of `__array__` is that it must return a `numpy.ndarray` instance,\r\n- the implementer of `__array__` may or may not make a copy in the process of producing an array\r\n- NumPy _may_ make a copy of the returned array if needed when `copy=False`, and _must_ make a copy if `copy=True` (it cannot know otherwise if it has exclusive ownership of the underlying data)\r\n- end result for `copy=True` is: either 1 or 2 copies made (NumPy cannot know which)\r\n\r\nThe intended new behavior is:\r\n- `copy` is always passed on to the `__array__` implementer, with the same semantics as for `asarray` (`False`: \"never copy\", `None`: \"copy-if-needed\", `True`: ensure data is a semantic copy \/ unique to returned array) \r\n- that implementer _must_ ensure that the data can not be used anymore by anything other than the numpy array it's returning (that usually means \"copy the data in memory\" but it can also mean something like \"invalidate any other reference to this data\")\r\n- end result:\r\n  - `copy=False`: guarantee that no data is copied in memory\r\n  - `copy=None`: 0 or 1 copies made, data may be a view\r\n  - `copy=True`: 0 or 1 copies made, data is guaranteed to not be a view\r\n\r\nIt looks like we need to do two things:\r\n1. Make the expectations on implementers of `__array__` clearer in our docs,\r\n2. Ensure we actually do not make a second copy if `copy=True` if passing along the copy keyword in the call to `__array__` succeeded.\r\n\r\nThis change of `__array__` kinda came in at the end of a long PR\/discussion about the `asarray`\/`array` behavior, so I didn't realize until now that this is actually also a performance improvement, potentially avoiding an unnecessary copy.","One additional aspect of the old behaviour is illustrated as follows:\r\n\r\n```python\r\nclass MyArray:\r\n    def __init__(self, arr):\r\n        self._ndarray = arr\r\n\r\n    def __array__(self, dtype=None):\r\n        return self._ndarray\r\n\r\nIn [7]: arr = MyArray(np.array([1, 2, 3]))\r\n\r\nIn [8]: np.asarray(arr, dtype=\"float64\")\r\nOut[8]: array([1., 2., 3.])\r\n```\r\n\r\nSo when `__array__` returns an np.ndarray that doesn't match the requested dtype, then numpy does the casting. In all of pandas \/ geopandas \/ shapely, I know we have some `__array__` implementations that do this. \r\n\r\nWhen a `copy` keyword is added, I assume we can't keep such a simple implementation if we want to correctly honor the keyword. Numpy will now assume the library did correctly honor the keyword? For example, with the above code, when doing `np.asarray(arr, dtype=\"int64\", copy=True)`, i.e. with a matching dtype, numpy doesn't need to do a cast and thus not need to copy, and then will assume that copy already happened? \r\n","It seems that on current main, numpy still does a copy afterwards, so `np.asarray(arr, dtype=\"int64\", copy=True)` actually gives a copy even with the `__array__` implementation above. But I assume numpy will want to change that to avoid the additional copy?","> But I assume numpy will want to change that to avoid the additional copy?\r\n\r\nRight now it's actually not possible to get to the code path in numpy that calls `__array__` with `copy=True`, but in the future we'll want to restructure things to avoid the extra copy.","This should probably be fixed:\r\n\r\n```\r\nimport numpy as np\r\n\r\nclass HasArray:\r\n    def __array__(self, values=None, dtype=None, copy=None):\r\n        print(copy)\r\n        return\r\n\r\narr = HasArray()\r\n\r\nnp.array(arr, copy=True)\r\n```\r\n\r\nRight now the `print(copy)` will print `None` where it really should print `True`.\r\n\r\nWe also need to update the docs to cover how implementers should handle this."],"labels":["component: numpy._core"]},{"title":"error: no member named 'aligned_alloc' in the global namespace; did you mean simply 'aligned_alloc'?","body":"### Steps to reproduce:\r\n\r\npip install git+https:\/\/github.com\/numpy\/numpy\r\n\r\n### Error message:\r\n\r\n```shell\r\n      [275\/339] Compiling C++ object numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o\r\n      FAILED: numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o\r\n      c++ -Inumpy\/fft\/_pocketfft_umath.cpython-311.so.p -Inumpy\/fft -I..\/numpy\/fft -Inumpy\/_core -I..\/numpy\/_core -Inumpy\/_core\/include -I..\/numpy\/_core\/include -I..\/numpy\/_core\/src\/common -I\/data\/data\/com.termux\/files\/usr\/include\/python3.11 -I\/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde\/meson_cpu -fvisibility=hidden -fvisibility-inlines-hidden -fcolor-diagnostics -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++17 -O3 -ftrapping-math -DNPY_HAVE_CLANG_FPSTRICT -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -fPIC -MD -MQ numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o -MF numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o.d -o numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o -c ..\/numpy\/fft\/_pocketfft_umath.cpp\r\n      In file included from ..\/numpy\/fft\/_pocketfft_umath.cpp:24:\r\n      ..\/numpy\/fft\/pocketfft\/pocketfft_hdronly.h:163:15: error: no member named 'aligned_alloc' in the global namespace; did you mean simply 'aligned_alloc'?\r\n        163 |   void *ptr = ::aligned_alloc(align,(size+align-1)&(~(align-1)));\r\n            |               ^~~~~~~~~~~~~~~\r\n            |               aligned_alloc\r\n      ..\/numpy\/fft\/pocketfft\/pocketfft_hdronly.h:160:14: note: 'aligned_alloc' declared here\r\n        160 | inline void *aligned_alloc(size_t align, size_t size)\r\n            |              ^\r\n      1 error generated.\r\n```\r\n\r\nFull build log:\r\n\r\n<details>\r\n\r\n```shell\r\n+ \/data\/data\/com.termux\/files\/home\/numpy\/bin\/python \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/vendored-meson\/meson\/meson.py setup \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=\/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde\/meson-python-native-file.ini\r\n      The Meson build system\r\n      Version: 1.2.99\r\n      Source dir: \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\r\n      Build dir: \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde\r\n      Build type: native build\r\n      Project name: NumPy\r\n      Project version: 2.0.0.dev0+git20240305.2c95083\r\n      C compiler for the host machine: cc (clang 17.0.6 \"clang version 17.0.6\")\r\n      C linker for the host machine: cc ld.lld 17.0.6\r\n      C++ compiler for the host machine: c++ (clang 17.0.6 \"clang version 17.0.6\")\r\n      C++ linker for the host machine: c++ ld.lld 17.0.6\r\n      Cython compiler for the host machine: cython (cython 3.0.8)\r\n      Host machine cpu family: aarch64\r\n      Host machine cpu: aarch64\r\n      Program python found: YES (\/data\/data\/com.termux\/files\/home\/numpy\/bin\/python)\r\n      Found pkg-config: \/data\/data\/com.termux\/files\/usr\/bin\/pkg-config (0.29.2)\r\n      Run-time dependency python found: YES 3.11\r\n      Has header \"Python.h\" with dependency python-3.11: YES\r\n      Compiler for C supports arguments -fno-strict-aliasing: YES\r\n      Compiler for C supports arguments -ftrapping-math: YES\r\n      Message: During parsing cpu-dispatch: The following CPU features were ignored due to platform incompatibility or lack of support:\r\n      \"XOP FMA4\"\r\n      Test features \"NEON NEON_FP16 NEON_VFPV4 ASIMD\" : Supported\r\n      Test features \"ASIMDHP\" : Supported\r\n      Test features \"ASIMDFHM\" : Supported\r\n      Test features \"SVE\" : Supported\r\n      Configuring npy_cpu_dispatch_config.h using configuration\r\n      Message:\r\n      CPU Optimization Options\r\n        baseline:\r\n          Requested : min\r\n          Enabled   : NEON NEON_FP16 NEON_VFPV4 ASIMD\r\n        dispatch:\r\n          Requested : max -xop -fma4\r\n          Enabled   : ASIMDHP ASIMDFHM SVE\r\n\r\n      Library m found: YES\r\n      Run-time dependency scipy-openblas found: NO (tried pkgconfig)\r\n      Found CMake: \/data\/data\/com.termux\/files\/usr\/bin\/cmake (3.28.3)\r\n      WARNING: CMake Toolchain: Failed to determine CMake compilers state\r\n      Run-time dependency openblas found: NO (tried pkgconfig, pkgconfig, pkgconfig, system and cmake)\r\n      Run-time dependency flexiblas found: NO (tried pkgconfig and cmake)\r\n      Run-time dependency blis found: NO (tried pkgconfig and cmake)\r\n      Run-time dependency blas found: YES unknown\r\n      ..\/numpy\/meson.build:135: WARNING: Project targets '>=1.2.99' but uses feature introduced in '1.3.0': dep 'blas' custom lookup.\r\n      Message: BLAS symbol suffix:\r\n      Run-time dependency openblas found: NO (tried pkgconfig, pkgconfig, pkgconfig, system and cmake)\r\n      Run-time dependency flexiblas found: NO (tried pkgconfig and cmake)\r\n      Run-time dependency lapack found: NO (tried pkgconfig and system)\r\n      Program _build_utils\/process_src_template.py found: YES (\/data\/data\/com.termux\/files\/home\/numpy\/bin\/python \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/numpy\/_build_utils\/process_src_template.py)\r\n      Program _build_utils\/tempita.py found: YES (\/data\/data\/com.termux\/files\/home\/numpy\/bin\/python \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/numpy\/_build_utils\/tempita.py)\r\n      Configuring __config__.py using configuration\r\n      Checking for size of \"short\" : 2\r\n      Checking for size of \"int\" : 4\r\n      Checking for size of \"long\" : 8\r\n      Checking for size of \"long long\" : 8\r\n      Checking for size of \"float\" : 4\r\n      Checking for size of \"double\" : 8\r\n      Checking for size of \"long double\" : 16\r\n      Checking for size of \"size_t\" : 8\r\n      Checking for size of \"size_t\" : 8 (cached)\r\n      Checking for size of \"wchar_t\" : 4\r\n      Checking for size of \"off_t\" : 8\r\n      Checking for size of \"Py_intptr_t\" with dependency python-3.11: 8\r\n      Checking for size of \"PY_LONG_LONG\" with dependency python-3.11: 8\r\n      Has header \"complex.h\" : YES\r\n      Checking for type \"complex float\" : YES\r\n      Checking for size of \"complex float\" : 8\r\n      Checking for type \"complex double\" : YES\r\n      Checking for size of \"complex double\" : 16\r\n      Checking for type \"complex long double\" : YES\r\n      Checking for size of \"complex long double\" : 32\r\n      Checking for function \"sin\" with dependency -lm: YES\r\n      Checking for function \"cos\" with dependency -lm: YES\r\n      Checking for function \"tan\" with dependency -lm: YES\r\n      Checking for function \"sinh\" with dependency -lm: YES\r\n      Checking for function \"cosh\" with dependency -lm: YES\r\n      Checking for function \"tanh\" with dependency -lm: YES\r\n      Checking for function \"fabs\" with dependency -lm: YES\r\n      Checking for function \"floor\" with dependency -lm: YES\r\n      Checking for function \"ceil\" with dependency -lm: YES\r\n      Checking for function \"sqrt\" with dependency -lm: YES\r\n      Checking for function \"log10\" with dependency -lm: YES\r\n      Checking for function \"log\" with dependency -lm: YES\r\n      Checking for function \"exp\" with dependency -lm: YES\r\n      Checking for function \"asin\" with dependency -lm: YES\r\n      Checking for function \"acos\" with dependency -lm: YES\r\n      Checking for function \"atan\" with dependency -lm: YES\r\n      Checking for function \"fmod\" with dependency -lm: YES\r\n      Checking for function \"modf\" with dependency -lm: YES\r\n      Checking for function \"frexp\" with dependency -lm: YES\r\n      Checking for function \"ldexp\" with dependency -lm: YES\r\n      Checking for function \"expm1\" with dependency -lm: YES\r\n      Checking for function \"log1p\" with dependency -lm: YES\r\n      Checking for function \"acosh\" with dependency -lm: YES\r\n      Checking for function \"asinh\" with dependency -lm: YES\r\n      Checking for function \"atanh\" with dependency -lm: YES\r\n      Checking for function \"rint\" with dependency -lm: YES\r\n      Checking for function \"trunc\" with dependency -lm: YES\r\n      Checking for function \"exp2\" with dependency -lm: YES\r\n      Checking for function \"copysign\" with dependency -lm: YES\r\n      Checking for function \"nextafter\" with dependency -lm: YES\r\n      Checking for function \"cbrt\" with dependency -lm: YES\r\n      Checking for function \"log2\" with dependency -lm: YES\r\n      Checking for function \"pow\" with dependency -lm: YES\r\n      Checking for function \"hypot\" with dependency -lm: YES\r\n      Checking for function \"atan2\" with dependency -lm: YES\r\n      Checking for function \"csin\" with dependency -lm: YES\r\n      Checking for function \"csinh\" with dependency -lm: YES\r\n      Checking for function \"ccos\" with dependency -lm: YES\r\n      Checking for function \"ccosh\" with dependency -lm: YES\r\n      Checking for function \"ctan\" with dependency -lm: YES\r\n      Checking for function \"ctanh\" with dependency -lm: YES\r\n      Checking for function \"creal\" with dependency -lm: YES\r\n      Checking for function \"cimag\" with dependency -lm: YES\r\n      Checking for function \"conj\" with dependency -lm: YES\r\n      Checking for function \"strtoll\" : YES\r\n      Checking for function \"strtoull\" : YES\r\n      Checking for function \"cabs\" with dependency -lm: YES\r\n      Checking for function \"cabsf\" with dependency -lm: YES\r\n      Checking for function \"cabsl\" with dependency -lm: YES\r\n      Checking for function \"cacos\" with dependency -lm: YES\r\n      Checking for function \"cacosf\" with dependency -lm: YES\r\n      Checking for function \"cacosl\" with dependency -lm: NO\r\n      Checking for function \"cacosh\" with dependency -lm: YES\r\n      Checking for function \"cacoshf\" with dependency -lm: YES\r\n      Checking for function \"cacoshl\" with dependency -lm: NO\r\n      Checking for function \"carg\" with dependency -lm: YES\r\n      Checking for function \"cargf\" with dependency -lm: YES\r\n      Checking for function \"cargl\" with dependency -lm: YES\r\n      Checking for function \"casin\" with dependency -lm: YES\r\n      Checking for function \"casinf\" with dependency -lm: YES\r\n      Checking for function \"casinl\" with dependency -lm: NO\r\n      Checking for function \"casinh\" with dependency -lm: YES\r\n      Checking for function \"casinhf\" with dependency -lm: YES\r\n      Checking for function \"casinhl\" with dependency -lm: NO\r\n      Checking for function \"catan\" with dependency -lm: YES\r\n      Checking for function \"catanf\" with dependency -lm: YES\r\n      Checking for function \"catanl\" with dependency -lm: NO\r\n      Checking for function \"catanh\" with dependency -lm: YES\r\n      Checking for function \"catanhf\" with dependency -lm: YES\r\n      Checking for function \"catanhl\" with dependency -lm: NO\r\n      Checking for function \"cexp\" with dependency -lm: YES\r\n      Checking for function \"cexpf\" with dependency -lm: YES\r\n      Checking for function \"cexpl\" with dependency -lm: NO\r\n      Checking for function \"clog\" with dependency -lm: NO\r\n      Checking for function \"clogf\" with dependency -lm: NO\r\n      Checking for function \"clogl\" with dependency -lm: NO\r\n      Checking for function \"cpow\" with dependency -lm: NO\r\n      Checking for function \"cpowf\" with dependency -lm: NO\r\n      Checking for function \"cpowl\" with dependency -lm: NO\r\n      Checking for function \"csqrt\" with dependency -lm: YES\r\n      Checking for function \"csqrtf\" with dependency -lm: YES\r\n      Checking for function \"csqrtl\" with dependency -lm: YES\r\n      Checking for function \"csin\" with dependency -lm: YES (cached)\r\n      Checking for function \"csinf\" with dependency -lm: YES\r\n      Checking for function \"csinl\" with dependency -lm: NO\r\n      Checking for function \"csinh\" with dependency -lm: YES (cached)\r\n      Checking for function \"csinhf\" with dependency -lm: YES\r\n      Checking for function \"csinhl\" with dependency -lm: NO\r\n      Checking for function \"ccos\" with dependency -lm: YES (cached)\r\n      Checking for function \"ccosf\" with dependency -lm: YES\r\n      Checking for function \"ccosl\" with dependency -lm: NO\r\n      Checking for function \"ccosh\" with dependency -lm: YES (cached)\r\n      Checking for function \"ccoshf\" with dependency -lm: YES\r\n      Checking for function \"ccoshl\" with dependency -lm: NO\r\n      Checking for function \"ctan\" with dependency -lm: YES (cached)\r\n      Checking for function \"ctanf\" with dependency -lm: YES\r\n      Checking for function \"ctanl\" with dependency -lm: NO\r\n      Checking for function \"ctanh\" with dependency -lm: YES (cached)\r\n      Checking for function \"ctanhf\" with dependency -lm: YES\r\n      Checking for function \"ctanhl\" with dependency -lm: NO\r\n      Checking for function \"isfinite\" with dependency -lm: YES\r\n      Header \"Python.h\" has symbol \"isfinite\" with dependency python-3.11: YES\r\n      Checking for function \"isinf\" with dependency -lm: YES\r\n      Header \"Python.h\" has symbol \"isinf\" with dependency python-3.11: YES\r\n      Checking for function \"isnan\" with dependency -lm: YES\r\n      Header \"Python.h\" has symbol \"isnan\" with dependency python-3.11: YES\r\n      Checking for function \"signbit\" with dependency -lm: YES\r\n      Header \"Python.h\" has symbol \"signbit\" with dependency python-3.11: YES\r\n      Checking for function \"fallocate\" : YES\r\n      Header \"Python.h\" has symbol \"HAVE_FTELLO\" with dependency python-3.11: YES\r\n      Header \"Python.h\" has symbol \"HAVE_FSEEKO\" with dependency python-3.11: YES\r\n      Checking for function \"backtrace\" : NO\r\n      Checking for function \"madvise\" : YES\r\n      Has header \"features.h\" : YES\r\n      Has header \"xlocale.h\" : YES\r\n      Has header \"dlfcn.h\" : YES\r\n      Has header \"execinfo.h\" : NO\r\n      Has header \"libunwind.h\" : NO\r\n      Has header \"sys\/mman.h\" : YES\r\n      Checking for function \"strtold_l\" : YES\r\n      Compiler for C supports arguments -O3: YES\r\n      Has header \"endian.h\" : YES\r\n      Has header \"sys\/endian.h\" : YES\r\n      Header \"inttypes.h\" has symbol \"PRIdPTR\" : YES\r\n      Compiler for C supports function attribute visibility:hidden: YES\r\n      Configuring config.h using configuration\r\n      Configuring _numpyconfig.h using configuration\r\n      Configuring npymath.ini using configuration\r\n      Configuring mlib.ini using configuration\r\n      Configuring numpy.pc using configuration\r\n      Generating multi-targets for \"_umath_tests.dispatch.h\"\r\n        Enabled targets: ASIMDHP, baseline\r\n      Generating multi-targets for \"argfunc.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"x86_simd_argsort.dispatch.h\"\r\n        Enabled targets:\r\n      Generating multi-targets for \"x86_simd_qsort.dispatch.h\"\r\n        Enabled targets:\r\n      Generating multi-targets for \"x86_simd_qsort_16bit.dispatch.h\"\r\n        Enabled targets:\r\n      Generating multi-targets for \"highway_qsort.dispatch.h\"\r\n        Enabled targets: SVE, ASIMD\r\n      Generating multi-targets for \"highway_qsort_16bit.dispatch.h\"\r\n        Enabled targets: ASIMDHP\r\n      Generating multi-targets for \"loops_arithm_fp.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_arithmetic.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_comparison.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_exponent_log.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_hyperbolic.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_logical.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_minmax.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_modulo.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_trigonometric.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_umath_fp.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_unary.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_unary_fp.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_unary_fp_le.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_unary_complex.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"loops_autovec.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Generating multi-targets for \"_simd.dispatch.h\"\r\n        Enabled targets: baseline\r\n      Build targets in project: 65\r\n      WARNING: Project specifies a minimum meson_version '>=1.2.99' but uses features which were added in newer versions:\r\n       * 1.3.0: {'dep 'blas' custom lookup'}\r\n\r\n      NumPy 2.0.0.dev0+git20240305.2c95083\r\n\r\n        User defined options\r\n          Native files: \/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde\/meson-python-native-file.ini\r\n          buildtype   : release\r\n          b_ndebug    : if-release\r\n          b_vscrt     : md\r\n\r\n      Found ninja-1.11.1 at \/data\/data\/com.termux\/files\/usr\/bin\/ninja\r\n      + \/data\/data\/com.termux\/files\/usr\/bin\/ninja\r\n      [1\/339] Generating 'numpy\/_core\/libnpymath.a.p\/ieee754.c'\r\n      [2\/339] Generating numpy\/_core\/npy_math_internal.h with a custom command\r\n      [3\/339] Generating numpy\/_core\/_umath_doc_generated with a custom command\r\n      [4\/339] Generating 'numpy\/_core\/libnpymath.a.p\/npy_math_complex.c'\r\n      [5\/339] Generating numpy\/generate-version with a custom command\r\n      Saving version to numpy\/version.py\r\n      [6\/339] Generating numpy\/_core\/__umath_generated with a custom command\r\n      [7\/339] Generating numpy\/__init__.cython-30.pxd with a custom command\r\n      [8\/339] Generating numpy\/__init__.pxd with a custom command\r\n      [9\/339] Generating numpy\/__init__.py with a custom command\r\n      [10\/339] Generating 'numpy\/_core\/_multiarray_tests.cpython-311.so.p\/_multiarray_tests.c'\r\n      [11\/339] Generating 'numpy\/_core\/_umath_tests.cpython-311.so.p\/_umath_tests.c'\r\n      [12\/339] Generating 'numpy\/_core\/_multiarray_tests.cpython-311.so.p\/templ_common.h'\r\n      [13\/339] Generating 'numpy\/_core\/libargfunc.dispatch.h_baseline.a.p\/npy_sort.h'\r\n      [14\/339] Generating 'numpy\/_core\/libargfunc.dispatch.h_baseline.a.p\/arraytypes.h'\r\n      [15\/339] Generating 'numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/arraytypes.h'\r\n      [16\/339] Generating 'numpy\/_core\/libhighway_qsort.dispatch.h_ASIMD.a.p\/arraytypes.h'\r\n      [17\/339] Generating 'numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/npy_sort.h'\r\n      [18\/339] Generating 'numpy\/_core\/libargfunc.dispatch.h_baseline.a.p\/argfunc.dispatch.c'\r\n      [19\/339] Generating 'numpy\/_core\/libhighway_qsort_16bit.dispatch.h_ASIMDHP.a.p\/arraytypes.h'\r\n      [20\/339] Generating 'numpy\/_core\/libhighway_qsort.dispatch.h_ASIMD.a.p\/npy_sort.h'\r\n      [21\/339] Generating 'numpy\/_core\/libloops_arithm_fp.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [22\/339] Generating 'numpy\/_core\/libhighway_qsort_16bit.dispatch.h_ASIMDHP.a.p\/npy_sort.h'\r\n      [23\/339] Generating numpy\/_core\/__ufunc_api with a custom command\r\n      [24\/339] Generating 'numpy\/_core\/libloops_arithm_fp.dispatch.h_baseline.a.p\/loops.h'\r\n      [25\/339] Generating 'numpy\/_core\/libloops_arithmetic.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [26\/339] Generating 'numpy\/_core\/libloops_arithmetic.dispatch.h_baseline.a.p\/loops_arithmetic.dispatch.c'\r\n      [27\/339] Compiling C object numpy\/_core\/libnpymath.a.p\/meson-generated_ieee754.c.o\r\n      [28\/339] Generating 'numpy\/_core\/libloops_arithm_fp.dispatch.h_baseline.a.p\/loops_arithm_fp.dispatch.c'\r\n      [29\/339] Generating 'numpy\/_core\/libloops_comparison.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [30\/339] Generating numpy\/_core\/__multiarray_api with a custom command\r\n      [31\/339] Generating 'numpy\/_core\/libloops_arithmetic.dispatch.h_baseline.a.p\/loops.h'\r\n      [32\/339] Generating 'numpy\/_core\/libloops_comparison.dispatch.h_baseline.a.p\/loops.h'\r\n      [33\/339] Generating 'numpy\/_core\/libloops_exponent_log.dispatch.h_baseline.a.p\/loops.h'\r\n      [34\/339] Generating 'numpy\/_core\/libloops_exponent_log.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [35\/339] Generating 'numpy\/_core\/libloops_comparison.dispatch.h_baseline.a.p\/loops_comparison.dispatch.c'\r\n      [36\/339] Compiling C object numpy\/_core\/_multiarray_tests.cpython-311.so.p\/src_common_npy_hashtable.c.o\r\n      [37\/339] Compiling C object numpy\/_core\/libnpymath.a.p\/src_npymath_npy_math.c.o\r\n      [38\/339] Compiling C object numpy\/_core\/lib_umath_tests.dispatch.h_baseline.a.p\/src_umath__umath_tests.dispatch.c.o\r\n      [39\/339] Linking static target numpy\/_core\/lib_umath_tests.dispatch.h_baseline.a\r\n      [40\/339] Compiling C object numpy\/_core\/libnpymath.a.p\/meson-generated_npy_math_complex.c.o\r\n      [41\/339] Compiling C object numpy\/_core\/lib_umath_tests.dispatch.h_ASIMDHP.a.p\/src_umath__umath_tests.dispatch.c.o\r\n      [42\/339] Compiling C object numpy\/_core\/_multiarray_tests.cpython-311.so.p\/src_common_npy_argparse.c.o\r\n      [43\/339] Linking static target numpy\/_core\/lib_umath_tests.dispatch.h_ASIMDHP.a\r\n      [44\/339] Linking static target numpy\/_core\/lib_umath_tests_mtargets.a\r\n      [45\/339] Compiling C object numpy\/_core\/_multiarray_tests.cpython-311.so.p\/src_common_mem_overlap.c.o\r\n      [46\/339] Compiling C object numpy\/_core\/_multiarray_tests.cpython-311.so.p\/meson-generated__multiarray_tests.c.o\r\n      [47\/339] Compiling C object numpy\/_core\/_umath_tests.cpython-311.so.p\/src_common_npy_cpu_features.c.o\r\n      [48\/339] Compiling C object numpy\/_core\/_struct_ufunc_tests.cpython-311.so.p\/src_umath__struct_ufunc_tests.c.o\r\n      [49\/339] Linking target numpy\/_core\/_struct_ufunc_tests.cpython-311.so\r\n      [50\/339] Compiling C object numpy\/_core\/_operand_flag_tests.cpython-311.so.p\/src_umath__operand_flag_tests.c.o\r\n      [51\/339] Linking target numpy\/_core\/_operand_flag_tests.cpython-311.so\r\n      [52\/339] Compiling C object numpy\/_core\/_umath_tests.cpython-311.so.p\/meson-generated__umath_tests.c.o\r\n      [53\/339] Linking target numpy\/_core\/_umath_tests.cpython-311.so\r\n      [54\/339] Compiling C++ object numpy\/_core\/libnpymath.a.p\/src_npymath_halffloat.cpp.o\r\n      [55\/339] Linking static target numpy\/_core\/libnpymath.a\r\n      [56\/339] Linking target numpy\/_core\/_multiarray_tests.cpython-311.so\r\n      [57\/339] Compiling C object numpy\/_core\/_rational_tests.cpython-311.so.p\/src_umath__rational_tests.c.o\r\n      [58\/339] Generating 'numpy\/_core\/libloops_exponent_log.dispatch.h_baseline.a.p\/loops_exponent_log.dispatch.c'\r\n      [59\/339] Linking target numpy\/_core\/_rational_tests.cpython-311.so\r\n      [60\/339] Generating 'numpy\/_core\/libloops_hyperbolic.dispatch.h_baseline.a.p\/loops.h'\r\n      [61\/339] Generating 'numpy\/_core\/libloops_hyperbolic.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [62\/339] Compiling C object numpy\/_core\/libloops_arithmetic.dispatch.h_baseline.a.p\/meson-generated_loops_arithmetic.dispatch.c.o\r\n      [63\/339] Generating 'numpy\/_core\/libloops_hyperbolic.dispatch.h_baseline.a.p\/loops_hyperbolic.dispatch.c'\r\n      [64\/339] Linking static target numpy\/_core\/libloops_arithmetic.dispatch.h_baseline.a\r\n      [65\/339] Generating 'numpy\/_core\/libloops_logical.dispatch.h_baseline.a.p\/loops.h'\r\n      [66\/339] Generating 'numpy\/_core\/libloops_logical.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [67\/339] Generating 'numpy\/_core\/libloops_logical.dispatch.h_baseline.a.p\/loops_logical.dispatch.c'\r\n      [68\/339] Compiling C object numpy\/_core\/libargfunc.dispatch.h_baseline.a.p\/meson-generated_argfunc.dispatch.c.o\r\n      [69\/339] Linking static target numpy\/_core\/libargfunc.dispatch.h_baseline.a\r\n      [70\/339] Compiling C object numpy\/_core\/libloops_arithm_fp.dispatch.h_baseline.a.p\/meson-generated_loops_arithm_fp.dispatch.c.o\r\n      [71\/339] Compiling C object numpy\/_core\/libloops_exponent_log.dispatch.h_baseline.a.p\/meson-generated_loops_exponent_log.dispatch.c.o\r\n      [72\/339] Linking static target numpy\/_core\/libloops_arithm_fp.dispatch.h_baseline.a\r\n      [73\/339] Linking static target numpy\/_core\/libloops_exponent_log.dispatch.h_baseline.a\r\n      [74\/339] Generating 'numpy\/_core\/libloops_minmax.dispatch.h_baseline.a.p\/loops.h'\r\n      [75\/339] Generating 'numpy\/_core\/libloops_modulo.dispatch.h_baseline.a.p\/loops.h'\r\n      [76\/339] Generating 'numpy\/_core\/libloops_modulo.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [77\/339] Generating 'numpy\/_core\/libloops_minmax.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [78\/339] Generating 'numpy\/_core\/libloops_modulo.dispatch.h_baseline.a.p\/loops_modulo.dispatch.c'\r\n      [79\/339] Generating 'numpy\/_core\/libloops_minmax.dispatch.h_baseline.a.p\/loops_minmax.dispatch.c'\r\n      [80\/339] Compiling C++ object numpy\/_core\/libhighway.a.p\/src_highway_hwy_targets.cc.o\r\n      [81\/339] Compiling C object numpy\/_core\/libloops_hyperbolic.dispatch.h_baseline.a.p\/meson-generated_loops_hyperbolic.dispatch.c.o\r\n      [82\/339] Linking static target numpy\/_core\/libhighway.a\r\n      [83\/339] Linking static target numpy\/_core\/libloops_hyperbolic.dispatch.h_baseline.a\r\n      [84\/339] Generating 'numpy\/_core\/libloops_trigonometric.dispatch.h_baseline.a.p\/loops.h'\r\n      [85\/339] Generating 'numpy\/_core\/libloops_trigonometric.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [86\/339] Generating 'numpy\/_core\/libloops_trigonometric.dispatch.h_baseline.a.p\/loops_trigonometric.dispatch.c'\r\n      [87\/339] Generating 'numpy\/_core\/libloops_umath_fp.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [88\/339] Generating 'numpy\/_core\/libloops_umath_fp.dispatch.h_baseline.a.p\/loops.h'\r\n      [89\/339] Generating 'numpy\/_core\/libloops_umath_fp.dispatch.h_baseline.a.p\/loops_umath_fp.dispatch.c'\r\n      [90\/339] Generating 'numpy\/_core\/libloops_unary.dispatch.h_baseline.a.p\/loops.h'\r\n      [91\/339] Compiling C object numpy\/_core\/libloops_trigonometric.dispatch.h_baseline.a.p\/meson-generated_loops_trigonometric.dispatch.c.o\r\n      [92\/339] Compiling C object numpy\/_core\/libloops_comparison.dispatch.h_baseline.a.p\/meson-generated_loops_comparison.dispatch.c.o\r\n      [93\/339] Generating 'numpy\/_core\/libloops_unary.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [94\/339] Linking static target numpy\/_core\/libloops_trigonometric.dispatch.h_baseline.a\r\n      [95\/339] Linking static target numpy\/_core\/libloops_comparison.dispatch.h_baseline.a\r\n      [96\/339] Generating 'numpy\/_core\/libloops_unary_fp.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [97\/339] Generating 'numpy\/_core\/libloops_unary_fp.dispatch.h_baseline.a.p\/loops.h'\r\n      [98\/339] Generating 'numpy\/_core\/libloops_unary.dispatch.h_baseline.a.p\/loops_unary.dispatch.c'\r\n      [99\/339] Compiling C object numpy\/_core\/libloops_modulo.dispatch.h_baseline.a.p\/meson-generated_loops_modulo.dispatch.c.o\r\n      [100\/339] Linking static target numpy\/_core\/libloops_modulo.dispatch.h_baseline.a\r\n      [101\/339] Generating 'numpy\/_core\/libloops_unary_fp_le.dispatch.h_baseline.a.p\/loops.h'\r\n      [102\/339] Generating 'numpy\/_core\/libloops_unary_fp.dispatch.h_baseline.a.p\/loops_unary_fp.dispatch.c'\r\n      [103\/339] Generating 'numpy\/_core\/libloops_unary_fp_le.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [104\/339] Generating 'numpy\/_core\/libloops_unary_fp_le.dispatch.h_baseline.a.p\/loops_unary_fp_le.dispatch.c'\r\n      [105\/339] Generating 'numpy\/_core\/libloops_unary_complex.dispatch.h_baseline.a.p\/loops.h'\r\n      [106\/339] Compiling C object numpy\/_core\/libloops_umath_fp.dispatch.h_baseline.a.p\/meson-generated_loops_umath_fp.dispatch.c.o\r\n      [107\/339] Linking static target numpy\/_core\/libloops_umath_fp.dispatch.h_baseline.a\r\n      [108\/339] Compiling C object numpy\/_core\/libloops_logical.dispatch.h_baseline.a.p\/meson-generated_loops_logical.dispatch.c.o\r\n      [109\/339] Generating 'numpy\/_core\/libloops_unary_complex.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [110\/339] Linking static target numpy\/_core\/libloops_logical.dispatch.h_baseline.a\r\n      [111\/339] Generating 'numpy\/_core\/libloops_unary_complex.dispatch.h_baseline.a.p\/loops_unary_complex.dispatch.c'\r\n      [112\/339] Generating 'numpy\/_core\/libloops_autovec.dispatch.h_baseline.a.p\/loops_utils.h'\r\n      [113\/339] Generating 'numpy\/_core\/libloops_autovec.dispatch.h_baseline.a.p\/loops.h'\r\n      [114\/339] Generating 'numpy\/_core\/libloops_autovec.dispatch.h_baseline.a.p\/loops_autovec.dispatch.c'\r\n      [115\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/arraytypes.h'\r\n      [116\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/npy_sort.h'\r\n      [117\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/arraytypes.c'\r\n      [118\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/einsum.c'\r\n      [119\/339] Compiling C object numpy\/_core\/libloops_unary_fp_le.dispatch.h_baseline.a.p\/meson-generated_loops_unary_fp_le.dispatch.c.o\r\n      [120\/339] Linking static target numpy\/_core\/libloops_unary_fp_le.dispatch.h_baseline.a\r\n      [121\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/einsum_sumprod.c'\r\n      [122\/339] Compiling C object numpy\/_core\/libloops_unary_complex.dispatch.h_baseline.a.p\/meson-generated_loops_unary_complex.dispatch.c.o\r\n      [123\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/nditer_templ.c'\r\n      [124\/339] Linking static target numpy\/_core\/libloops_unary_complex.dispatch.h_baseline.a\r\n      [125\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/scalartypes.c'\r\n      [126\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/templ_common.h'\r\n      [127\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/loops_utils.h'\r\n      [128\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/loops.h'\r\n      [129\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/funcs.inc'\r\n      [130\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/loops.c'\r\n      [131\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/matmul.h'\r\n      [132\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/matmul.c'\r\n      [133\/339] Compiling C object numpy\/_core\/libloops_unary_fp.dispatch.h_baseline.a.p\/meson-generated_loops_unary_fp.dispatch.c.o\r\n      [134\/339] Linking static target numpy\/_core\/libloops_unary_fp.dispatch.h_baseline.a\r\n      [135\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/lowlevel_strided_loops.c'\r\n      [136\/339] Compiling C object numpy\/_core\/libloops_unary.dispatch.h_baseline.a.p\/meson-generated_loops_unary.dispatch.c.o\r\n      [137\/339] Linking static target numpy\/_core\/libloops_unary.dispatch.h_baseline.a\r\n      [138\/339] Generating 'numpy\/_core\/_multiarray_umath.cpython-311.so.p\/scalarmath.c'\r\n      [139\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_einsum.c.o\r\n      [140\/339] Compiling C object numpy\/_core\/libloops_minmax.dispatch.h_baseline.a.p\/meson-generated_loops_minmax.dispatch.c.o\r\n      [141\/339] Linking static target numpy\/_core\/libloops_minmax.dispatch.h_baseline.a\r\n      [142\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_nditer_templ.c.o\r\n      [143\/339] Compiling C++ object numpy\/_core\/libhighway_qsort_16bit.dispatch.h_ASIMDHP.a.p\/src_npysort_highway_qsort_16bit.dispatch.cpp.o\r\n      [144\/339] Linking static target numpy\/_core\/libhighway_qsort_16bit.dispatch.h_ASIMDHP.a\r\n      [145\/339] Compiling C object numpy\/_core\/libloops_autovec.dispatch.h_baseline.a.p\/meson-generated_loops_autovec.dispatch.c.o\r\n      [146\/339] Linking static target numpy\/_core\/libloops_autovec.dispatch.h_baseline.a\r\n      [147\/339] Compiling C++ object numpy\/_core\/libhighway_qsort.dispatch.h_ASIMD.a.p\/src_npysort_highway_qsort.dispatch.cpp.o\r\n      [148\/339] Linking static target numpy\/_core\/libhighway_qsort.dispatch.h_ASIMD.a\r\n      [149\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_scalartypes.c.o\r\n      [150\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_abstractdtypes.c.o\r\n      [151\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_alloc.c.o\r\n      [152\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_loops.c.o\r\n      [153\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_einsum_sumprod.c.o\r\n      [154\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_matmul.c.o\r\n      [155\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_arrayobject.c.o\r\n      [156\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_array_converter.c.o\r\n      [157\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_arraytypes.c.o\r\n      [158\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_array_assign_scalar.c.o\r\n      [159\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_array_assign_array.c.o\r\n      [160\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_array_coercion.c.o\r\n      [161\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_arraywrap.c.o\r\n      [162\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_array_method.c.o\r\n      [163\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_arrayfunction_override.c.o\r\n      [164\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_compiled_base.c.o\r\n      [165\/339] Compiling C++ object numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/src_npysort_highway_qsort.dispatch.cpp.o\r\n      [166\/339] Linking static target numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a\r\n      [167\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_common_dtype.c.o\r\n      [168\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_calculation.c.o\r\n      [169\/339] Linking static target numpy\/_core\/lib_multiarray_umath_mtargets.a\r\n      [170\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_common.c.o\r\n      [171\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_buffer.c.o\r\n      [172\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_convert.c.o\r\n      [173\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_convert_datatype.c.o\r\n      [174\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_datetime_busdaycal.c.o\r\n      [175\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_datetime_strings.c.o\r\n      [176\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_conversion_utils.c.o\r\n      [177\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_datetime_busday.c.o\r\n      [178\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_descriptor.c.o\r\n      [179\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_scalarmath.c.o\r\n      [180\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_dlpack.c.o\r\n      [181\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_ctors.c.o\r\n      [182\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_getset.c.o\r\n      [183\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_dtype_traversal.c.o\r\n      [184\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_dtypemeta.c.o\r\n      [185\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_datetime.c.o\r\n      [186\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_public_dtype_api.c.o\r\n      [187\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_flagsobject.c.o\r\n      [188\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_hashdescr.c.o\r\n      [189\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_legacy_dtype_implementation.c.o\r\n      [190\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_dtype_transfer.c.o\r\n      [191\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_dragon4.c.o\r\n      [192\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_methods.c.o\r\n      [193\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_iterators.c.o\r\n      [194\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_item_selection.c.o\r\n      [195\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_nditer_constr.c.o\r\n      [196\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_nditer_pywrap.c.o\r\n      [197\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_refcount.c.o\r\n      [198\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_sequence.c.o\r\n      [199\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_multiarraymodule.c.o\r\n      [200\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_scalarapi.c.o\r\n      [201\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_number.c.o\r\n      [202\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_mapping.c.o\r\n      [203\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/meson-generated_lowlevel_strided_loops.c.o\r\n      [204\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_nditer_api.c.o\r\n      [205\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_shape.c.o\r\n      [206\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_stringdtype_dtype.c.o\r\n      [207\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_usertypes.c.o\r\n      [208\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_strfuncs.c.o\r\n      [209\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_vdot.c.o\r\n      [210\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_temp_elide.c.o\r\n      [211\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_stringdtype_utf8_utils.c.o\r\n      [212\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_stringdtype_static_string.c.o\r\n      [213\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_conversions.c.o\r\n      [214\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_field_types.c.o\r\n      [215\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_stringdtype_casts.c.o\r\n      [216\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_growth.c.o\r\n      [217\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_mergesort.cpp.o\r\n      [218\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_readtext.c.o\r\n      [219\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_radixsort.cpp.o\r\n      [220\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_str_to_int.c.o\r\n      [221\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_heapsort.cpp.o\r\n      [222\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_stream_pyobject.c.o\r\n      [223\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npymath_arm64_exports.c.o\r\n      [224\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_rows.c.o\r\n      [225\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_gil_utils.c.o\r\n      [226\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_multiarray_textreading_tokenize.cpp.o\r\n      [227\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_array_assign.c.o\r\n      [228\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_ucsnarrow.c.o\r\n      [229\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_mem_overlap.c.o\r\n      [230\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_npy_hashtable.c.o\r\n      [231\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_npy_argparse.c.o\r\n      [232\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_npy_longdouble.c.o\r\n      [233\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_binsearch.cpp.o\r\n      [234\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_cblasfuncs.c.o\r\n      [235\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_ufunc_override.c.o\r\n      [236\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_npy_cpu_features.c.o\r\n      [237\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_numpyos.c.o\r\n      [238\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_npy_cpu_dispatch.c.o\r\n      [239\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_common_python_xerbla.c.o\r\n      [240\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_legacy_array_method.c.o\r\n      [241\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_ufunc_type_resolution.c.o\r\n      [242\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_extobj.c.o\r\n      [243\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_dispatching.c.o\r\n      [244\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_override.c.o\r\n      [245\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_reduction.c.o\r\n      [246\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_special_integer_comparisons.cpp.o\r\n      [247\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_wrapping_array_method.c.o\r\n      [248\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_string_ufuncs.cpp.o\r\n      [249\/339] Generating 'numpy\/_core\/lib_simd.dispatch.h_baseline.a.p\/_simd_inc.h'\r\n      [250\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_clip.cpp.o\r\n      [251\/339] Generating 'numpy\/_core\/lib_simd.dispatch.h_baseline.a.p\/_simd_data.inc'\r\n      [252\/339] Generating 'numpy\/_core\/lib_simd.dispatch.h_baseline.a.p\/_simd.dispatch.c'\r\n      [253\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_ufunc_object.c.o\r\n      [254\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath__scaled_float_dtype.c.o\r\n      [255\/339] Compiling C object numpy\/_core\/_simd.cpython-311.so.p\/src__simd__simd.c.o\r\n      [256\/339] Compiling C object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_umathmodule.c.o\r\n      [257\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_python_xerbla.c.o\r\n      [258\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_litemodule.c.o\r\n      [259\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c.c.o\r\n      [260\/339] Compiling C object numpy\/_core\/_simd.cpython-311.so.p\/src_common_npy_cpu_features.c.o\r\n      [261\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_selection.cpp.o\r\n      [262\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_umath_stringdtype_ufuncs.cpp.o\r\n      [263\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_config.c.o\r\n      [264\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_timsort.cpp.o\r\n      [265\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_lapack.c.o\r\n      [266\/339] Compiling C++ object numpy\/_core\/_multiarray_umath.cpython-311.so.p\/src_npysort_quicksort.cpp.o\r\n      [267\/339] Linking target numpy\/_core\/_multiarray_umath.cpython-311.so\r\n      [268\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_python_xerbla.c.o\r\n      [269\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_f2c.c.o\r\n      [270\/339] Compiling C object numpy\/_core\/lib_simd.dispatch.h_baseline.a.p\/meson-generated__simd.dispatch.c.o\r\n      [271\/339] Linking static target numpy\/_core\/lib_simd.dispatch.h_baseline.a\r\n      [272\/339] Linking static target numpy\/_core\/lib_simd_mtargets.a\r\n      [273\/339] Linking target numpy\/_core\/_simd.cpython-311.so\r\n      [274\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_c_lapack.c.o\r\n      [275\/339] Compiling C++ object numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o\r\n      FAILED: numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o\r\n      c++ -Inumpy\/fft\/_pocketfft_umath.cpython-311.so.p -Inumpy\/fft -I..\/numpy\/fft -Inumpy\/_core -I..\/numpy\/_core -Inumpy\/_core\/include -I..\/numpy\/_core\/include -I..\/numpy\/_core\/src\/common -I\/data\/data\/com.termux\/files\/usr\/include\/python3.11 -I\/data\/data\/com.termux\/files\/usr\/tmp\/pip-req-build-ku9xsfks\/.mesonpy-sksxgrde\/meson_cpu -fvisibility=hidden -fvisibility-inlines-hidden -fcolor-diagnostics -DNDEBUG -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -std=c++17 -O3 -ftrapping-math -DNPY_HAVE_CLANG_FPSTRICT -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -fPIC -MD -MQ numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o -MF numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o.d -o numpy\/fft\/_pocketfft_umath.cpython-311.so.p\/_pocketfft_umath.cpp.o -c ..\/numpy\/fft\/_pocketfft_umath.cpp\r\n      In file included from ..\/numpy\/fft\/_pocketfft_umath.cpp:24:\r\n      ..\/numpy\/fft\/pocketfft\/pocketfft_hdronly.h:163:15: error: no member named 'aligned_alloc' in the global namespace; did you mean simply 'aligned_alloc'?\r\n        163 |   void *ptr = ::aligned_alloc(align,(size+align-1)&(~(align-1)));\r\n            |               ^~~~~~~~~~~~~~~\r\n            |               aligned_alloc\r\n      ..\/numpy\/fft\/pocketfft\/pocketfft_hdronly.h:160:14: note: 'aligned_alloc' declared here\r\n        160 | inline void *aligned_alloc(size_t align, size_t size)\r\n            |              ^\r\n      1 error generated.\r\n      [276\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_f2c_d_lapack.c.o\r\n      [277\/339] Compiling C++ object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/umath_linalg.cpp.o\r\n      [278\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_f2c_c_lapack.c.o\r\n      [279\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_z_lapack.c.o\r\n      [280\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_s_lapack.c.o\r\n      [281\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_blas.c.o\r\n      [282\/339] Compiling C object numpy\/linalg\/lapack_lite.cpython-311.so.p\/lapack_lite_f2c_d_lapack.c.o\r\n      [283\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_f2c_z_lapack.c.o\r\n      [284\/339] Compiling C object numpy\/linalg\/_umath_linalg.cpython-311.so.p\/lapack_lite_f2c_s_lapack.c.o\r\n      ninja: build stopped: subcommand failed.\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n```\r\n\r\n<\/details>\r\n\r\n### Additional information:\r\n\r\nI cannot understand what could be an actual problem and how to resolve it. I tried numpy from PyPI before, but it always shows errors like \u00abCannot import \"PyExc_ValueError\"\u00bb or \u00abModule have no element\" ndarray\u00bb.\r\n\r\nSystem:\r\nTerminal: TERMUX 0.118.0\r\nOS: XOS V10.0.0 (Based on Android 11)\r\nDevice: Infinix NOTE 12\r\n\r\n(I'm sorry if i missed something that's the first time im creating issue on github). ","comments":["Thanks for the report @Ree1352. This may be an actual regression, if not on Termux then perhaps on Windows - it looks the same as https:\/\/github.com\/scipy\/scipy\/pull\/19761. I'll have a closer look at this. We just switched to the C++ pocketfft version, so the code using `::aligned_alloc` is new (and has given other issues in SciPy before IIRC, e.g. with Mingw).\r\n\r\nYou are on Termux, which has always been a bit problematic since it doesn't have normal C99 support. See gh-22935 and gh-10808. If you could look at these two issues and determine whether numpy `1.24.4` and `1.25.2` work for you (maybe with adding the `MATHLIB=m` workaround mentioned in one of those issues), that'd be great. We'd be happy to help Termux users since it seems to be fairly popular, but know next to nothing about it.","Okay, I checked and the latest compat fix for Windows that SciPy needed (https:\/\/github.com\/mreineck\/pocketfft\/commit\/9d3ab05a7fffbc71a492bc6a17be034e83e8f0fe) is already included in NumPy's main branch. So this issue is also specific to Termux. I'd be happy to review a PR to add a define like in the linked pocketfft commit to make things work for Termux, but I can't test on that platform.\r\n\r\nRe aligned allocation in general, that is now used in at least two places: Highway has its own implementation in `hwy\/aligned_allocator.cc|h`, while the pocketfft code uses `::aligned_alloc`. That same code has been in SciPy for quite a while, so it should be robust enough for 2.0, if not perfectly portable as this issue shows.","I'm also running into this on osx while [building](https:\/\/github.com\/conda-forge\/numpy-feedstock\/pull\/312) numpy 2.0.0b1 for conda-forge:\r\n```\r\n[467\/517] Compiling C++ object numpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p\/_pocketfft_umath.cpp.o\r\nFAILED: numpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p\/_pocketfft_umath.cpp.o \r\nx86_64-apple-darwin13.4.0-clang++ -Inumpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p -Inumpy\/fft -I..\/numpy\/fft -Inumpy\/_core -I..\/numpy\/_core -Inumpy\/_core\/include -I..\/numpy\/_core\/include -I..\/numpy\/_core\/src\/common -I$PREFIX\/include\/python3.11 -I$SRC_DIR\/builddir\/meson_cpu -fvisibility=hidden -fvisibility-inlines-hidden -fcolor-diagnostics -DNDEBUG -Wall -Winvalid-pch -std=c++17 -O3 -ftrapping-math -DNPY_HAVE_CLANG_FPSTRICT -msse -msse2 -msse3 -mssse3 -DNPY_HAVE_SSE2 -DNPY_HAVE_SSE -DNPY_HAVE_SSE3 -DNPY_HAVE_SSSE3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -fmessage-length=0 -isystem $PREFIX\/include -fdebug-prefix-map=$SRC_DIR=\/usr\/local\/src\/conda\/numpy-2.0.0b1 -fdebug-prefix-map=$PREFIX=\/usr\/local\/src\/conda-prefix -D_FORTIFY_SOURCE=2 -isystem $PREFIX\/include -mmacosx-version-min=10.9 -MD -MQ numpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p\/_pocketfft_umath.cpp.o -MF numpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p\/_pocketfft_umath.cpp.o.d -o numpy\/fft\/_pocketfft_umath.cpython-311-darwin.so.p\/_pocketfft_umath.cpp.o -c ..\/numpy\/fft\/_pocketfft_umath.cpp\r\nIn file included from ..\/numpy\/fft\/_pocketfft_umath.cpp:24:\r\n..\/numpy\/fft\/pocketfft\/pocketfft_hdronly.h:163:15: error: no member named 'aligned_alloc' in the global namespace; did you mean simply 'aligned_alloc'?\r\n  void *ptr = ::aligned_alloc(align,(size+align-1)&(~(align-1)));\r\n              ^~~~~~~~~~~~~~~\r\n              aligned_alloc\r\n```\r\n\r\nLooking at the [code](https:\/\/github.com\/mreineck\/pocketfft\/blob\/0f7aa1225b065938fc263b7914df16b8c1cbc9d7\/pocketfft_hdronly.h#L155-L160) in question\r\n```C\r\n\/\/ the __MINGW32__ part in the conditional below works around the problem that\r\n\/\/ the standard C++ library on Windows does not provide aligned_alloc() even\r\n\/\/ though the MinGW compiler and MSVC may advertise C++17 compliance.\r\n\/\/ aligned_alloc is only supported from MacOS 10.15.\r\n#if (__cplusplus >= 201703L) && (!defined(__MINGW32__)) && (!defined(_MSC_VER)) && (MAC_OS_X_VERSION_MIN_REQUIRED >= MAC_OS_X_VERSION_10_15)\r\ninline void *aligned_alloc(size_t align, size_t size)\r\n```\r\nthere's already a guard to only use this on macOS >=10.15, but it looks like `MAC_OS_X_VERSION_MIN_REQUIRED` might not be defined (correctly).","> there's already a guard to only use this on macOS >=10.15, but it looks like MAC_OS_X_VERSION_MIN_REQUIRED might not be defined (correctly).\n\nI provided this fix and it was just a guess. It seemed to work at the time but I wouldn't be surprised if it isn't completely robust.","See https:\/\/github.com\/scipy\/pocketfft\/pull\/1","Should NumPy switch to vendoring the `pocketfft` maintained in the SciPy organisation ?","Upstream now has one commit with Tyler's shim for MacOS, and another which disables the conditional aligned_alloc completely.\n\n> Done! You can now choose between cb9988c86f4b005de5f77594026a8ba9efec45ee (the original PR) and 33ae5dc94c9cdc7f1c78346504a85de87cadaa12 (fully disabled aligned_alloc)","> Should NumPy switch to vendoring the pocketfft maintained in the SciPy organisation ?\r\n\r\nMaybe we should vendor the upstream repo https:\/\/github.com\/mreineck\/pocketfft. In any case we should synchronize the code with that repo by applying the commits, I suggest we be conservative and disable the `allignd_alloc`.","> Maybe we should vendor the upstream repo\r\n\r\nYou already do: https:\/\/github.com\/numpy\/numpy\/tree\/main\/numpy\/fft. We vendor a fork under the SciPy org as a precaution in case the upstream repo goes inactive, but shouldn't matter much."],"labels":["32 - Installation"]},{"title":"`np.vectorize` fails with output dtype `datetime64[ns]`","body":"### Describe the issue:\r\n\r\nThis came out from @mathause  's investigation in https:\/\/github.com\/pydata\/xarray\/issues\/8802#issuecomment-1975370245\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\notype = \"datetime64[ns]\"\r\narr = np.array(['2024-01-01', '2024-01-02', '2024-01-03'], dtype='datetime64[ns]')\r\nnp.vectorize(lambda x: x, signature=\"(i)->(j)\", otypes=[otype])(arr)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[1], line 4\r\n      2 otype = \"datetime64[ns]\"\r\n      3 arr = np.array(['2024-01-01', '2024-01-02', '2024-01-03'], dtype='datetime64[ns]')\r\n----> 4 np.vectorize(lambda x: x, signature=\"(i)->(j)\", otypes=[otype])(arr)\r\n\r\nFile ~\/envs\/test-xarray\/lib\/python3.10\/site-packages\/numpy\/lib\/function_base.py:2372, in vectorize.__call__(self, *args, **kwargs)\r\n   2369     self._init_stage_2(*args, **kwargs)\r\n   2370     return self\r\n-> 2372 return self._call_as_normal(*args, **kwargs)\r\n\r\nFile ~\/envs\/test-xarray\/lib\/python3.10\/site-packages\/numpy\/lib\/function_base.py:2365, in vectorize._call_as_normal(self, *args, **kwargs)\r\n   2362     vargs = [args[_i] for _i in inds]\r\n   2363     vargs.extend([kwargs[_n] for _n in names])\r\n-> 2365 return self._vectorize_call(func=func, args=vargs)\r\n\r\nFile ~\/envs\/test-xarray\/lib\/python3.10\/site-packages\/numpy\/lib\/function_base.py:2446, in vectorize._vectorize_call(self, func, args)\r\n   2444 \"\"\"Vectorized call to `func` over positional `args`.\"\"\"\r\n   2445 if self.signature is not None:\r\n-> 2446     res = self._vectorize_call_with_signature(func, args)\r\n   2447 elif not args:\r\n   2448     res = func()\r\n\r\nFile ~\/envs\/test-xarray\/lib\/python3.10\/site-packages\/numpy\/lib\/function_base.py:2506, in vectorize._vectorize_call_with_signature(self, func, args)\r\n   2502         outputs = _create_arrays(broadcast_shape, dim_sizes,\r\n   2503                                  output_core_dims, otypes, results)\r\n   2505     for output, result in zip(outputs, results):\r\n-> 2506         output[index] = result\r\n   2508 if outputs is None:\r\n   2509     # did not call the function even once\r\n   2510     if otypes is None:\r\n\r\nValueError: Cannot convert from specific units to generic units in NumPy datetimes or timedeltas\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n```shell\r\n1.26.4\r\n3.10.1 (main, Dec 15 2021, 17:45:54) [GCC 9.3.0]\r\n```\r\n### Runtime Environment:\r\n```shell\r\n[{'numpy_version': '1.26.4',\r\n  'python': '3.10.1 (main, Dec 15 2021, 17:45:54) [GCC 9.3.0]',\r\n  'uname': uname_result(system='Linux', node='ip-172-34-24-37', release='5.15.0-1051-aws', version='#56~20.04.1-Ubuntu SMP Tue Nov 28 15:43:31 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX'],\r\n                      'not_found': ['AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/ubuntu\/envs\/test-xarray\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 4,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n```\r\n### Context for the issue:\r\n\r\n_No response_","comments":["I think this happens because numpy creates the target array with `np.dtype(\"datetime64[ns]\").char`, i.e. `dtype('<M8')` and not `dtype('<M8[ms]')`\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/8f22d5aea1516c7228232988e015ff217a6c7c4a\/numpy\/lib\/_function_base_impl.py#L2333\r\n","I wonder what breaks if you drop the `char` there. Looking at the `Vectorize` class, it seems to only pass `otypes` entries as values for `dtype` keywords, so dtype instances should be fine.","I will work on this."],"labels":["00 - Bug"]},{"title":"ENH: Convert tanh from C universal intrinsics to C++ using Highway","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nThis is another patch demonstrating how the current NumPy SIMD code could be converted to Highway, similar to #25781. All tests pass on my local AVX512 and AVX2 machine.\r\n\r\nOn x86, AVX2 has a major performance improvement, while AVX512 shows a mix of small regressions and speedups.\r\n\r\n### AVX512\r\nAVX512 stays within 10% of baseline, most being within 5%.\r\n```\r\n| Change   | Before [15691c33] <main>   | After [ddbf7be8] <tanh-hwy>   |   Ratio | Benchmark (Parameter)                                                    |\r\n|----------|----------------------------|-------------------------------|---------|--------------------------------------------------------------------------|\r\n| +        | 14.4\u00b10.1\u03bcs                 | 15.5\u00b10.01\u03bcs                   |    1.08 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 1, 'f')        |\r\n| +        | 46.9\u00b10.01\u03bcs                | 49.5\u00b12\u03bcs                      |    1.06 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 1, 'd')        |\r\n| +        | 8.22\u00b10.1\u03bcs                 | 8.44\u00b10.05\u03bcs                   |    1.03 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 2, 'f')        |\r\n| +        | 893\u00b10.1\u03bcs                  | 899\u00b11\u03bcs                       |    1.01 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 1, 'd') |\r\n| +        | 894\u00b10.7\u03bcs                  | 904\u00b10.9\u03bcs                     |    1.01 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 2, 'd') |\r\n| +        | 882\u00b10.07\u03bcs                 | 883\u00b10.5\u03bcs                     |    1    | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 2, 'd') |\r\n| -        | 178\u00b10.04\u03bcs                 | 178\u00b10.2\u03bcs                     |    1    | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 2, 'f') |\r\n| -        | 17.2\u00b10.01\u03bcs                | 17.1\u00b10.01\u03bcs                   |    0.99 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 2, 'f')        |\r\n| -        | 172\u00b10.2\u03bcs                  | 171\u00b10.04\u03bcs                    |    0.99 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 2, 'f') |\r\n| -        | 167\u00b13\u03bcs                    | 165\u00b10.03\u03bcs                    |    0.99 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 2, 'e') |\r\n| -        | 4.90\u00b10.01\u03bcs                | 4.83\u00b10\u03bcs                      |    0.98 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 1, 'f')        |\r\n| -        | 169\u00b13\u03bcs                    | 165\u00b10.06\u03bcs                    |    0.98 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 1, 'e') |\r\n| -        | 176\u00b16\u03bcs                    | 169\u00b10.02\u03bcs                    |    0.96 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 2, 'e') |\r\n| -        | 19.1\u00b11\u03bcs                   | 17.9\u00b10.01\u03bcs                   |    0.94 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 1, 'd')        |\r\n```\r\n\r\n### AVX2\r\nAVX2 shows a major performance improvement; thanks @jan-wassenberg for the idea to avoid slow gathers with the LUTs.\r\n```\r\n| Change   | Before [15691c33] <main>   | After [0267cd21] <tanh-hwy>   |   Ratio | Benchmark (Parameter)                                                    |\r\n|----------|----------------------------|-------------------------------|---------|--------------------------------------------------------------------------|\r\n| -        | 1.83\u00b10.01ms                | 1.50\u00b10ms                      |    0.82 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 2, 'd') |\r\n| -        | 429\u00b10.3\u03bcs                  | 351\u00b10.8\u03bcs                     |    0.82 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 2, 'f') |\r\n| -        | 410\u00b10.2\u03bcs                  | 333\u00b12\u03bcs                       |    0.81 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 1, 'f') |\r\n| -        | 415\u00b10.2\u03bcs                  | 337\u00b10.7\u03bcs                     |    0.81 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 2, 'f') |\r\n| -        | 1.84\u00b10.01ms                | 1.49\u00b10ms                      |    0.81 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 1, 'd') |\r\n| -        | 430\u00b10.3\u03bcs                  | 350\u00b10.2\u03bcs                     |    0.81 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 4, 1, 'f') |\r\n| -        | 1.81\u00b10.01ms                | 1.44\u00b10ms                      |    0.8  | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 1, 'd') |\r\n| -        | 1.81\u00b10.01ms                | 1.44\u00b10ms                      |    0.8  | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'tanh'>, 1, 2, 'd') |\r\n| -        | 3.46\u00b10ms                   | 2.22\u00b10ms                      |    0.64 | bench_ufunc.UFunc.time_ufunc_types('tanh')                               |\r\n| -        | 117\u00b10.01\u03bcs                 | 43.4\u00b10.04\u03bcs                   |    0.37 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 2, 'f')        |\r\n| -        | 115\u00b10.04\u03bcs                 | 41.2\u00b10.01\u03bcs                   |    0.36 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 1, 'f')        |\r\n| -        | 498\u00b18\u03bcs                    | 149\u00b10.08\u03bcs                    |    0.3  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 1, 'd')        |\r\n| -        | 499\u00b18\u03bcs                    | 150\u00b10.5\u03bcs                     |    0.3  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 4, 2, 'd')        |\r\n| -        | 475\u00b18\u03bcs                    | 110\u00b15\u03bcs                       |    0.23 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 2, 'd')        |\r\n| -        | 102\u00b10.02\u03bcs                 | 23.1\u00b10.02\u03bcs                   |    0.23 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 2, 'f')        |\r\n| -        | 475\u00b19\u03bcs                    | 103\u00b10.05\u03bcs                    |    0.22 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 1, 'd')        |\r\n| -        | 99.7\u00b10.02\u03bcs                | 19.7\u00b10.01\u03bcs                   |    0.2  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'tanh'>, 1, 1, 'f')        |\r\n```\r\n \r\n","comments":["> This is another patch demonstrating how the current NumPy SIMD code could be converted to Highway, similar to #25781. All tests pass on my local AVX512 and AVX2 machine.\r\n> \r\n> ### AVX512\r\n> AVX512 stays within 10% of baseline, most being within 5%.\r\n>\r\n> ### AVX2\r\n> AVX2 has almost exact performance parity.\r\n\r\nI'd suggest that the 1-10% variation could also be just due to it being low numbers of `\u03bcs` and the code being slightly different. We've had other similar things happen just by introducing code which leads to a few `\u03bcs` differences in unrelated code.\r\n\r\nI'll run this on some AArch64, and see what we get, this is very exciting though and I look forward to reviewing this further \ud83d\ude38 \r\n\r\n","cc @jan-wassenberg because the review box doesn't seem to want me to request review that way \ud83d\ude40 ","Oh nice, great to see the AVX2 speedup, congrats!"],"labels":["01 - Enhancement","component: SIMD"]},{"title":"Make \"Replaced-By\" and \"Replaces\" metadata in NEPs render as links","body":"This follows up on the suggestion in https:\/\/github.com\/numpy\/numpy\/pull\/25920#issuecomment-1975800919. When a NEP is marked as superseded, you have to add metadata like:\r\n```\r\n:Status: Superseded\r\n:Replaced-By: xx\r\n```\r\nwhere xx is the integer number of the NEP that superseded this NEP. Similarly, in the superseding NEP you have to add:\r\n```\r\n:Replaces: yy\r\n```\r\nThis is rendered as a plain text integer. It's be great if instead it was rendered as `NEP xx` as a link.\r\n\r\nThat these are plain integers is actually enforced in the doc build (which can be run with `cd doc\/neps && make html`) at https:\/\/github.com\/numpy\/numpy\/blob\/main\/doc\/neps\/tools\/build_index.py.\r\n\r\ngh-25920 tweaked that code a bit so instead of a single `int` it can be a comma-separated list of ints. But turning them into proper links requires a bit more work.\r\n\r\nThere's a couple of ways this could be done. For example:\r\n- keep the numbers as integers and generate link under the hood in `build_index.py`, or\r\n- replace the integers with ```:ref:`NEPxx` ``` directly in the `.rst` files of the NEPs\r\n\r\nAs long as the build works without warnings and the tooling code doesn't grow super complex, any solution should be fine here.\r\n\r\nMarking this as a task that's suitable for a sprint, it seems doable in a half a day or so and doesn't require knowledge of NumPy internals, only Python and reStructuredText.","comments":[],"labels":["17 - Task","component: documentation","component: NEP","sprintable"]},{"title":"Color and font choice for typesetting inline code - drop the pink\/red","body":"After spending a fair bit of time staring at the 2.0.0 release notes, the use of the bright red\/pink color and the too-large font for text that is typeset as inline code became really jarring. \r\n\r\n<img width=\"457\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/9ad613ef-35c1-4869-849f-5aacd62e8004\">\r\n\r\nCompare with the [docs of pydata-sphinx-theme](https:\/\/pydata-sphinx-theme.readthedocs.io) itself:\r\n\r\n<img width=\"469\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/75906777-b1ba-453c-967c-a271f798fc34\">\r\n\r\nThat is far more pleasant to read. The theme authors have spent a lot of time polishing these choices; our bright pink\/red is a leftover from the initial theme (originally from Pandas).\r\n\r\n**Proposal: switch color choices and font sizes to the theme defaults.**","comments":["Got some info: we need at least 0.14.0 for the color changes (we currently have it pinned at 0.13.3), and https:\/\/github.com\/scipy\/scipy\/issues\/19958 is a similar issue for SciPy that touches on some issues that the NumPy docs are also going to have (being worked on).\r\n\r\nUpgrading to the latest version (0.15.2) locally gives the more muted colors, and deleting `doc\/source\/_static\/numpy.css` files the font size problem. There are some other rendering issues, but the colors\/fonts look way better:\r\n\r\n<img width=\"459\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/893313ed-2069-46c0-a3fe-404ac18cd3ee\">\r\n"],"labels":["01 - Enhancement","component: documentation"]},{"title":"BUG: `np.pad(... mode=\"reflect\")` and `np.pad(... mode=\"symmetric\")` sometimes drop values for multiple reflections","body":"### Describe the issue:\r\n\r\nHello,\r\nI have encountered a problem with `np.pad` when using the `mode=\"reflect\"`, which should _\"Pad[s] with the reflection of the vector mirrored on the first and last values of the vector along each axis.\"_\r\n\r\n#### Expected behaviour\r\nThe series `a b c d e f g` should be extended to something like `... c d e f g f e d c b | a b c d e f g | f e d c b a b c d e f g f ...` where the original series is located between the two pipes `|`, as shown in the following figure which can be reproduced by the code example below:\r\n![Padding_Working_Example](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/58217656-25a2-43e3-b628-a8e06edb3ee5)\r\n\r\n#### Observed behaviour\r\nFor **some combinations** of `pad_width=(pad_left, pad_right)`, an **erroneous outcome** is returned, **but no `Exception`** is raised. Somehow, the padding drops values after padding with multiples of `series.size` leading to an inconsistent repetition behaviour. Extending the `pad_width` reveals that the respective value is dropped again after a few successful repetitions of the padding sequence as depicted in the following (dropped values indicated by question marks):\r\n![Padding_Failed_Example](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/81ad95e7-766a-4787-b368-47c0061733b5)\r\n\r\n#### Expected causes\r\nThe error must occur somewhere [here](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/numpy\/lib\/arraypad.py#L296-L378) in combination with [this while loop](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/numpy\/lib\/arraypad.py#L862-L869) that iteratively updates the indices used for padding. Unfortunately, I didn't have the time to dig deeper into it with single step debugging. For the context section however, I added a `.txt`-file that lists failed combinations of (`series_size - pad_left - pad_right`). From what I see, the combination `(pad_left=b, pad_right=a)` fails when `(pad_left=a, pad_right=b)` also did, so in this sense it seems symmetric.\r\n\r\nThanks for your time!\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n### Imports ###\r\n\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\n\r\n### Padding of a series ###\r\n\r\nseries = np.array([1, 2, 3, 4, 5], dtype=np.float64)\r\n# pad_width = (10, 10)  # will run and produce expected outcome\r\npad_width = (7, 105)  # will run and NOT produce the expected outcome\r\nseries_padded = np.pad(\r\n    array=series,\r\n    pad_width=pad_width,\r\n    mode=\"reflect\",\r\n)\r\n\r\n### Visualisation ###\r\n\r\nplt.plot(\r\n    np.arange(0, series.size),\r\n    series,\r\n    color=\"red\",\r\n    linewidth=3.0,\r\n    label=\"original\",\r\n    zorder=1,\r\n)\r\nplt.plot(\r\n    np.arange(-pad_width[0], series.size + pad_width[1]),\r\n    series_padded,\r\n    color=\"blue\",\r\n    linewidth=5.0,\r\n    linestyle=\"--\",\r\n    label=f\"padded (pad_left = {pad_width[0]}, pad_right = {pad_width[1]})\",\r\n    zorder=0,\r\n)\r\nplt.legend()\r\n\r\nplt.show()\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\n**NumPy:** `1.26.4`\r\n**Python:** `3.11.6 (tags\/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]`\r\n\r\n### Runtime Environment:\r\n\r\n```bash\r\n[{'numpy_version': '1.26.4',\r\n  'python': '3.11.6 (tags\/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 '\r\n            '64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='SomeUserName', release='10', version='10.0.22631', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': 'C:\\\\Users\\\\...\\\\.venv311\\\\Lib\\\\site-packages\\\\numpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\n  ```\r\n\r\n### Context for the issue:\r\n\r\nI figured out this issue while testing a Python package I am working on that implements a similar feature, but via a sparse matrix multiplication which does not show this issue.\r\nBelow, I attach a textfile with all the combination of `series.size - pad_left - pad_right` where the error was observed. Maybe this helps in finding a common pattern for debugging. The parameter combinations tested are also listed in there. If you want, I can extend the test:\r\n\r\n[Reflect_Padding_Failed_Combinations.txt](https:\/\/github.com\/numpy\/numpy\/files\/14478341\/ReflectPaddingFailedCombinations.txt)\r\n\r\nHowever, it is **not critical** I guess since this is probably **not the standard use case**. I only went that far for testing to really make sure, everything works as expected.","comments":["Hello, I'd like to take this issue. I've successfully reproduced the bug and implemented a solution on my local environment. I'll be submitting a pull request shortly.","Problem resolved and test cases were added. Need review to merge. Thanks.","@EngineerEricXie Thanks for tackling this so quickly!","While I was reading the code, I was wondering about the massive amount of logic of the `mode=\"reflect\"`. I know it has to handle the cases of ND-Arrays and their respective axis. It goes into a [while-loop](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/numpy\/lib\/arraypad.py#L862-L869), but if only the left and only the right extension are added one at a time, the full extension can be computed in one go with a known number of iterations.\r\n\r\nCurrently, my 1D-implementation as subsitute for `np.pad(... mode =\"reflect\")` looks like this:\r\n```python\r\ndef extend_mirrored_boundary(\r\n    series: np.ndarray, ext_size_left: int, ext_size_right: int\r\n) -> np.ndarray:\r\n    \"\"\"\r\n    Extends a series by mirroring it at the boundaries, i.e., the series\r\n    ``a b c d e f g`` is extended to ``... d c b | a b c d e f g | f e d ...``.\r\n\r\n    Parameters\r\n    ----------\r\n    series : np.ndarray of shape (series_size, )\r\n        The series to be extended.\r\n    ext_size_left, ext_size_right : int\r\n        The size of the extension at the left and right boundaries, respectively.\r\n\r\n    Returns\r\n    -------\r\n    extended_series : np.ndarray of shape (num_series_points + ext_size_left + ext_size_right,)\r\n        The extended series.\r\n\r\n    Notes\r\n    -----\r\n    This test does not rely on `np.pad` due to a bug in the implementation of the\r\n    `\"reflect\"` mode. See https:\/\/github.com\/numpy\/numpy\/issues\/25926.\r\n\r\n    \"\"\"\r\n\r\n    # a nested function for computing the mirrored extension is defined\r\n    # it works on local variables that are defined individually for the left and right\r\n    # extensions\r\n    def compute_mirrored_extension() -> list[np.ndarray]:\r\n        # the full extensions are handled\r\n        vals_to_concat = [\r\n            extension_for_even if iter_i % 2 == 0 else extension_for_odd\r\n            for iter_i in range(0, n_full_ext)\r\n        ]\r\n\r\n        # the remainder extension is handled\r\n        vals_to_concat.append(extension_remainder)\r\n        return vals_to_concat\r\n\r\n    # the respective extensions for the left and right boundaries are computed\r\n    left_full_ext = series[:0:-1]\r\n    right_full_ext = series[-2::-1]\r\n\r\n    # the left extension is computed and added to the series (if needed)\r\n    series_concat = [series]\r\n    full_ext_size = max(series.size - 1, 1)  # to avoid division by zero\r\n    if ext_size_left > 0:\r\n        n_full_ext, remainder_ext = divmod(ext_size_left, full_ext_size)\r\n        extension_for_even = left_full_ext\r\n        extension_for_odd = np.flip(right_full_ext)\r\n        extension_remainder = (\r\n            extension_for_even[full_ext_size - remainder_ext : :]\r\n            if n_full_ext % 2 == 0\r\n            else extension_for_odd[full_ext_size - remainder_ext : :]\r\n        )\r\n\r\n        series_concat = compute_mirrored_extension()[::-1] + series_concat\r\n\r\n    # end if\r\n\r\n    # the right extension is computed and added to the series (if needed)\r\n    if ext_size_right > 0:\r\n        n_full_ext, remainder_ext = divmod(ext_size_right, full_ext_size)\r\n        extension_for_even = right_full_ext\r\n        extension_for_odd = np.flip(left_full_ext)\r\n        extension_remainder = (\r\n            extension_for_even[0:remainder_ext]\r\n            if n_full_ext % 2 == 0\r\n            else extension_for_odd[0:remainder_ext]\r\n        )\r\n\r\n        series_concat = series_concat + compute_mirrored_extension()\r\n\r\n    # end if\r\n\r\n    return np.concatenate(series_concat)\r\n```\r\nand it passes the tests, even when the `series` has only 1 element.\r\n\r\nYet, I'm not 100% sure if there is a more efficient way and that's why I'm asking this question more out of curiosity \ud83d\ude43 Maybe this can also help me enhance my logic a bit \ud83d\ude05 ","@EngineerEricXie I dug a little deeper and also found that the `mode=\"symmetric\"` has the same problem (since it comes in the same `if` and relies on the same utility functions):\r\n\r\nExample of a failure (I simply replaced the `mode` in the reproducible example):\r\n![Symmetric_Padding_Failed_Example](https:\/\/github.com\/numpy\/numpy\/assets\/128370921\/b77d232c-6c94-483e-af14-9b1b07ab50c2)\r\n\r\nAgain, I will add a `.txt`-file that lists failed combinations of `(series_size - pad_left - pad_right)`:\r\n\r\n[SymmetricPaddingFailedCombinations.txt](https:\/\/github.com\/numpy\/numpy\/files\/14547751\/SymmetricPaddingFailedCombinations.txt)\r\n\r\nWith this, the tests will probably also have to cover the case of `mode=\"symmetric\"`.\r\n\r\nHope this finds you well \ud83d\ude43 ","@IruNikZe Thanks for pointing out the problem in `mode=symmetric`. The logic behind the `mode=reflect` can be demonstrated by the following figures.\r\nFirst, the unpadded value were set to be zero. Therefore we can see that the value except original value is zero.\r\n![Figure_1](https:\/\/github.com\/numpy\/numpy\/assets\/161030123\/b6ec773b-d225-45b2-b6c5-2df299cec21c)\r\nThen, the code fill in the left and right vlaue by reflection (reflect one value on the left with reflection axis on index 1)(reflect two value on the right with reflection axis on index 3)\r\n![Figure_2_draw](https:\/\/github.com\/numpy\/numpy\/assets\/161030123\/64c91bb8-9382-4ed4-8e41-bd34705b72f3)\r\nThen the code reflects the right 4 values with reflection axis on index 5.\r\n![Figure_3_draw](https:\/\/github.com\/numpy\/numpy\/assets\/161030123\/7a468a53-3bea-4945-b588-da3b2943517c)\r\nLastly, the code reflects the right 2 value with reflection axis index 8\r\n![Figure_4_draw](https:\/\/github.com\/numpy\/numpy\/assets\/161030123\/6e21f843-d92e-48bd-bcee-cff5ee7372fa)\r\nThe code will compare the size of yellow_line<->green_line and green_line <-> orange_line to copy only the needed value. The original bug for both `mode=reflect` and `mode=symmetric` comes from the setting of yellow_line, where in the orignal code is the edge value (ex. the yellow_line is set to be on index 0 for the second figure for the original code, which will make the reflection contain value that is not in the period)","Thanks again @EngineerEricXie for this very detailed explanation and adding the test \u2705 "],"labels":["00 - Bug","component: numpy.lib"]},{"title":"ENH: Optimize np.strings.expandtabs to avoid unnecessary copies","body":"As noted in https:\/\/github.com\/numpy\/numpy\/pull\/25891#discussion_r1504990588, we can optimize `expandtabs` to avoid unnecessary `memcpy`s.","comments":[],"labels":["01 - Enhancement","component: numpy._core"]},{"title":"BUG: Sum per axis is wrong after copy of data is C (default)","body":"### Describe the issue:\r\n\r\nWhen you compare results of operations after copy with order of 'C' and 'F' then results will be different when you have multiple columns\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef fn(x):\r\n    return (x).sum(axis=0, keepdims=True)\r\n\r\ndef fun(data, n = 100_000):\r\n    x = np.tile(data, (n,1))\r\n    rf = fn(x.copy(order='F'))\r\n    rc = fn(x.copy(order='C'))\r\n    print(f\"F order: {rf}, C order {rc}\")\r\n\r\ndata =  np.array([[-0.16385849, -0.06833044],[ 0.14264219, -0.06838101]], dtype=np.float32)\r\nfun(data) # F order: [[ -2121.6326 -13671.142 ]], C order [[ -2117.505 -13671.854]]\r\nfun(data[:, 0:1]) # F order: [[-2121.6326]], C order [[-2121.6326]]\r\nfun(data[:, 1:2]) # F order: [[-13671.142]], C order [[-13671.142]]\r\n```\r\n\r\n\r\n```python\r\nn = 100_000_000\r\na = np.array((np.full(n, 1, dtype=np.float32), np.full(n, 0.01, dtype=np.float32), np.full(n, -1, dtype=np.float32))).T\r\nprint(a.shape) # (100000000, 3)\r\nprint(a.sum(axis=0)) # [ 1.0000000e+08  1.0000628e+06 -1.0000000e+08]\r\nprint(a.copy().sum(axis=0)) # [ 16777216.    262144. -16777216.]\r\nprint(a.copy(order=\"C\").sum(axis=0)) # [ 16777216.    262144. -16777216.]\r\nprint(a.copy(order=\"F\").sum(axis=0)) # [ 1.0000000e+08  1.0000628e+06 -1.0000000e+08]\r\n```\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\n1.26.4\r\n3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["You can also have such an example:\r\n```\r\nn = 1000_000\r\na = np.array((np.full(n, 1, dtype=np.float32), np.full(n, 0.0000001, dtype=np.float32), np.full(n, -1, dtype=np.float32)))\r\nprint(a.copy(order=\"F\").sum()) # 0.11920929\r\nprint(a.copy(order=\"C\").sum()) # 0\r\n\r\nprint(a.copy(order=\"F\").T.sum()) # 0.11920929\r\nprint(a.copy(order=\"C\").T.sum()) # 0.0\r\n\r\nprint(a.T.copy(order=\"F\").sum()) # 0.0\r\nprint(a.T.copy(order=\"C\").sum()) # 0.11920929\r\n```","from the docs of the numpy for sum: https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.sum.html\r\n\r\n> Technically, to provide the best speed possible, the improved precision is only used when the summation is along the fast axis in memory. Note that the exact precision may vary depending on other parameters. In contrast to NumPy, Python\u2019s math.fsum function uses a slower but more precise approach to summation. Especially when summing a large number of lower precision floating point numbers, such as float32, numerical errors can become significant. In such cases it can be advisable to use dtype=\u201dfloat64\u201d to use a higher precision for the output.","and yet another example of per axis sum:\r\n```\r\nn = 100_000_000\r\na = np.array((np.full(n, 1, dtype=np.float32), np.full(n, 0.01, dtype=np.float32), np.full(n, -1, dtype=np.float32))).T\r\nprint(a.shape) # (100000000, 3)\r\nprint(a.copy(order=\"F\").sum(axis=0)) # [ 1.0000000e+08  1.0000628e+06 -1.0000000e+08]\r\nprint(a.copy(order=\"C\").sum(axis=0)) # [ 16777216.    262144. -16777216.]\r\n```","You have already linked the documentation with the explanation.  Do you think there is more to do here?","So i have gave some more information in the other ticket. One think where the documentation is wrong:\r\n> This improved precision is always provided when no axis is given\r\n\r\nHowever in this case this is not true:\r\n```\r\nn = 1000_000\r\na = np.array((np.full(n, 1, dtype=np.float32), np.full(n, 0.0000001, dtype=np.float32), np.full(n, -1, dtype=np.float32)))\r\nprint(a.copy(order=\"F\").sum()) # 0.11920929\r\nprint(a.copy(order=\"C\").sum()) # 0\r\n\r\nprint(a.copy(order=\"F\").T.sum()) # 0.11920929\r\nprint(a.copy(order=\"C\").T.sum()) # 0.0\r\n\r\nprint(a.T.copy(order=\"F\").sum()) # 0.0\r\nprint(a.T.copy(order=\"C\").sum()) # 0.11920929\r\n```","Could numpy use the same logic which is in torch cpu, which looks like is both faster and have correct results?\r\nOr at least have a flag to use pair wise sum for better calculations"],"labels":["00 - Bug","57 - Close?"]},{"title":"BUG: Ensure correct handling of multidimensional q by nanquantile","body":"Fixes #25731.\r\nKey changes applied to the following function `def _nanquantile_ureduce_func()` in `\/numpy\/lib\/_nanfunctions_impl.py`\r\nIn particular the results from `result = _nanquantile_1d(part, q, overwrite_input, method, weights=wgt)` were not being reshaped properly. Tests passed on local plus contributed additional test to `numpy\/lib\/tests\/test_nanfunctions.py` to validate consistent behavior between quantile and nanquantile. \r\nSome additional details included below to facilitate code-review.\r\nOriginal behavior that was highlighted in #25731.\r\n\r\n```\r\na = np.ones((3, 4))\r\nq = np.full((1, 2), 0.5)\r\n \r\nprint(np.quantile(a, q, axis=1).shape)\r\n# (1, 2, 3)  correct\r\nprint(np.nanquantile(a, q, axis=1).shape)\r\n# (1, 3, 2) mangled up\r\n```\r\nTo ensure consistent behavior with `np.quantile\/np.percentile` the axes contributed by `q` should end up at the beginning of the result. \r\n\r\nThe key issue was `def _nanquantile_ureduce_func()` in `\/numpy\/lib\/_nanfunctions_impl.py`.  All the computations seem correct (in my understanding), up untill `result = _nanquantile_1d(part, q, overwrite_input, method, weights=wgt)`. After this, the `result` was being reshaped using `result = np.moveaxis(result, axis, 0)`. This resulted in the mangled up shape highlighted in the original bug.\r\n\r\nThe proposed solution explicitly computes the locations of axis, before applying `np.moveaxis`\r\n```\r\nnum_dims = len(a.shape)+len(q.shape)-1\r\nnew_axes = list(range(len(q.shape), num_dims))\r\nnew_axes[axis:axis] = range(len(q.shape)) \r\nresult = np.moveaxis(result, source=range(num_dims), destination=new_axes)\r\n```\r\nNow, the return values and shapes of `np.nanquantile` are consistent with `np.quantile`. This has been verified using newly contributed tests included in `test_consistency_with_quantile` in `numpy\/lib\/tests\/test_nanfunctions.py`. Furthermore, all pre-existing tests pass. ","comments":[],"labels":["00 - Bug"]},{"title":"DOC: update the note for \"insert\" so that it points to the explanation of why `...0...` and `...[0]...` are wildly different.","body":"### Issue with current documentation:\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.insert.html#numpy.insert currently has a note that reads:\r\n\r\n>Note that for higher dimensional inserts obj=0 behaves very different from obj=[0] just like arr[:,0,:] = values is different from arr[:,[0],:] = values.\r\n\r\nWhich doesn't let readers find out what the specific difference in behaviour is, or why that difference exists.\n\n### Idea or request for content:\n\nLink out to whatever page(s) explain this difference in behaviour, so that people aren't left with more questions than they had before they saw that note =)","comments":[],"labels":["04 - Documentation"]},{"title":"build issue of numpy 1.26.0 with openblas 0.3.20 on windows.","body":"### Describe the issue:\n\nI have built openblas and tried building numpy 1.26.0 wheel after setting PKG_CONFIG_PATH to openblas.pc .\r\nI also installed rtools from gfortran support ans using visual studio to build. \r\n\r\nopenblas build report - \r\n```\r\nOpenBLAS build complete. (BLAS CBLAS LAPACK LAPACKE)\r\n\r\n            OS               ... WINNT\r\n            Architecture     ... x86_64\r\n            BINARY           ... 64bit\r\n            C compiler       ... GCC  (cmd & version : cc.exe (Built by Jeroen for the R-project) 10.3.0)\r\n            Fortran compiler ... GFORTRAN  (cmd & version : GNU Fortran (Built by Jeroen for the R-project) 10.3.0)\r\n            Library Name     ... libopenblas_zenp-r0.3.20.a (Multi-threading; Max num-threads is 2)\r\n            \r\n```\r\n            \r\nbuilding wheel usinf - `python -m build --wheel --no-isolation .`\r\n\r\ngetting below error -\r\n```\r\nLibrary m found: NO\r\nFound pkg-config: C:\\rtools40\\ucrt64\\bin\\pkg-config.EXE (0.29.2)\r\nRun-time dependency openblas found: YES 0.3.20\r\nChecking if \"CBLAS\" with dependency openblas: links: NO\r\nFound CMake: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.EXE (3.17.20032601)\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nRun-time dependency cblas found: NO (tried pkgconfig and cmake)\r\n\r\nnumpy\\1.26.0\\numpy-1.26.0\\numpy\\meson.build:189:4: ERROR: Problem encountered: No CBLAS interface detected! Install a BLAS library with CBLAS support, or use the `allow-noblas` build option (note, this may be up to 100x slower for some linear algebra operations). \r\n```\r\n\r\nPlease help if I am missing anything.\n\n### Reproduce the code example:\n\n```python\n`python -m build --wheel --no-isolation .`\n```\n\n\n### Error message:\n\n```shell\nLibrary m found: NO\r\nFound pkg-config: C:\\rtools40\\ucrt64\\bin\\pkg-config.EXE (0.29.2)\r\nRun-time dependency openblas found: YES 0.3.20\r\nChecking if \"CBLAS\" with dependency openblas: links: NO\r\nFound CMake: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.EXE (3.17.20032601)\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nRun-time dependency cblas found: NO (tried pkgconfig and cmake)\r\n\r\nnumpy\\1.26.0\\numpy-1.26.0\\numpy\\meson.build:189:4: ERROR: Problem encountered: No CBLAS interface detected! Install a BLAS library with CBLAS support, or use the `allow-noblas` build option (note, this may be up to 100x slower for some linear algebra operations). \r\n```\n```\n\n\n### Python and NumPy Versions:\n\n3.12.1\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["steps mentioned in CI job (https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/azure-steps-windows.yml) regarding openblas is quite not clear to me .\r\n\r\n```\r\n$target=$(python -c \"import tools.openblas_support as obs; plat=obs.get_plat(); ilp64=obs.get_ilp64(); target=f'openblas_{plat}.zip'; obs.download_openblas(target, plat, ilp64);print(target)\")\r\n    unzip -o -d c:\/opt\/ $target\r\n```\r\n\r\nthis peice of code is using import tools.openblas_support , how this package was installed?","As Ralf mentioned on the mailing list:\r\n\r\n> Please don't build 1.26.0, but 1.26.4. The CBLAS detection issue you are\r\nhitting there has been made more robust in 1.26.2-3, so it should go away\r\n\r\nIs there a reason why you can't build the most recent patch release? In general we don't support issues reported against old patch releases, particularly when there have been bugfixes for the issue.","Thank you for the update. \r\nI tried building numpy 1.26.4 , I noticed it needed scipy_openblas64 and this packages contains the openblas library.\r\nWhen I tried to use openblas 0.3.20 built by me after setting PKG_CONFIG_PATH, seems build did not recognise my openblas library. \r\n\r\nscipy_openblas64 is must for numpy 1.26.4? How do I create a whl file including openblas library?  ","You do not have to use scipy-openblas, you can use (almost) any other BLAS package. But you must ensure that the OpenBLAS you use can be found by pkg-config or cmake. See the [section on blas and lapack in the documentation](https:\/\/numpy.org\/devdocs\/building\/blas_lapack.html#blas-and-lapack). If you are using MSVC to build NumPy, you must be sure your OpenBLAS pkg-config script points to a import library that MSVC can use (a `.lib` suffix, not an `.so`, `.dll`, nor `.a`.","I have created a openblas library for windows using - https:\/\/github.com\/arrayfire\/arrayfire\/wiki\/CBLAS-for-Windows\r\nand made .lib for Visual Studio to compile with OpenBLAS.\r\n\r\n```\r\ncd OpenBLAS\/exports\r\nlib.exe \/machine:x64 \/def:libopenblas.def\r\n```\r\n\r\n\r\n```\r\nOpenBLAS build complete. (BLAS CBLAS LAPACK LAPACKE)\r\n\r\n            OS               ... WINNT\r\n            Architecture     ... x86_64\r\n            BINARY           ... 64bit\r\n            C compiler       ... GCC  (cmd & version : cc.exe (Built by Jeroen for the R-project) 10.3.0)\r\n            Fortran compiler ... GFORTRAN  (cmd & version : GNU Fortran (Built by Jeroen for the R-project) 10.3.0)\r\n            Library Name     ... libopenblas_zenp-r0.3.20.a (Multi-threading; Max num-threads is 2)\r\n```\r\n\r\nsetting PKG_CONFIG_PATH\r\n`set PKG_CONFIG_PATH=C:\\Temp\\3063972971\\python\\lib\\pkgconfig`\r\n\r\n\r\ncontent of  C:\\Temp\\3063972971\\python\\lib\\pkgconfig\\openblas.pc \r\n```\r\nlibdir=C:\/Temp\/3063972971\/python\/lib\r\nincludedir=C:\/Temp\/3063972971\/python\/include\r\nopenblas_config= USE_64BITINT= DYNAMIC_ARCH= DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= ZEN MAX_THREADS=2\r\nversion=0.3.20\r\nextralib=-defaultlib:advapi32 -lgfortran -lgfortran\r\nName: openblas\r\nDescription: OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version\r\nVersion: ${version}\r\nURL: https:\/\/github.com\/xianyi\/OpenBLAS\r\nLibs: -L${libdir} -lopenblas\r\nLibs.private: ${extralib}\r\nCflags: -I${includedir}\r\n\r\n\r\n```\r\nlib content -\r\n```\r\nls C:\/Temp\/3063972971\/python\/lib | grep openblas\r\nlibopenblas.a\r\nlibopenblas.dll.a\r\nlibopenblas.exp\r\nlibopenblas.lib\r\nlibopenblas_zenp-r0.3.20.a\r\n```\r\n\r\n\r\nwhich cl\r\n`\/c\/Program Files (x86)\/Microsoft Visual Studio\/2019\/Professional\/VC\/Tools\/MSVC\/14.27.29110\/bin\/HostX86\/x64\/cl`\r\n\r\n\r\nBut when I tried to built and install using - `python -m pip install . -v -Csetup-args=\"--vsenv\" `\r\nit still says openblas found: NO\r\n\r\nLog snippet - \r\n  ```\r\nLibrary m found: NO\r\n  Fetching value of define \"__MINGW32__\" :\r\n  Found pkg-config: C:\\rtools40\\ucrt64\\bin\\pkg-config.EXE (0.29.2)\r\n  Run-time dependency scipy-openblas found: NO (tried pkgconfig)\r\n  Run-time dependency mkl found: NO (tried pkgconfig and system)\r\n  Run-time dependency accelerate found: NO (tried system)\r\n  Found CMake: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.EXE (3.17.20032601)\r\n  WARNING: CMake Toolchain: Failed to determine CMake compilers state\r\n  WARNING: CMake Toolchain: Failed to determine CMake compilers state\r\n  Run-time dependency openblas found: NO (tried pkgconfig, pkgconfig, pkgconfig, system and cmake)\r\n  Run-time dependency flexiblas64 found: NO (tried pkgconfig and cmake)\r\n  Run-time dependency blis found: NO (tried pkgconfig and cmake)\r\n  Run-time dependency blas found: NO (tried pkgconfig and system)\r\n  Run-time dependency mkl found: NO (tried pkgconfig and system)\r\n  Run-time dependency accelerate found: NO (tried system)\r\n  WARNING: CMake Toolchain: Failed to determine CMake compilers state\r\n  Run-time dependency openblas found: NO (tried pkgconfig, pkgconfig, pkgconfig, system and cmake)\r\n  Run-time dependency flexiblas found: NO (tried pkgconfig and cmake)\r\n  Run-time dependency lapack found: NO (tried pkgconfig and system)\r\n\r\n```\r\nAlso at last I am hitting below Error -\r\n```\r\n\r\n[368\/503] Compiling C++ object numpy\/core\/_multiarray_umath.cp312-win_amd64.pyd.p\/src_npysort_binsearch.cpp.obj\r\nFAILED: numpy\/core\/_multiarray_umath.cp312-win_amd64.pyd.p\/src_npysort_binsearch.cpp.obj\r\n\"cl\" \"-Inumpy\\core\\_multiarray_umath.cp312-win_amd64.pyd.p\" \"-Inumpy\\core\" \"-I..\\numpy\\core\" \"-Inumpy\\core\\include\" \"-I..\\numpy\\core\\include\" \"-I..\\numpy\\core\\src\\common\" \"-I..\\numpy\\core\\src\\multiarray\" \"-I..\\numpy\\core\\src\\npymath\" \"-I..\\numpy\\core\\src\\umath\" \"-IC:\\temp\\3063972971\\python\\Include\" \"-IC:\/temp\/3063972971\/python\/include\" \"-IC:\/temp\/3063972971\/python\/include\/openssl\" \"-IC:\/temp\/3063972971\/python\\python_3rdparty\\numpy\\1.26.4\\numpy-1.26.4\\.mesonpy-xmru26c7\\meson_cpu\" \"-DNDEBUG\" \"\/MD\" \"\/nologo\" \"\/showIncludes\" \"\/utf-8\" \"\/Zc:__cplusplus\" \"\/W2\" \"\/EHsc\" \"\/std:c++17\" \"\/permissive-\" \"\/O2\" \"\/Gw\" \"-DNPY_HAVE_SSE2\" \"-DNPY_HAVE_SSE\" \"-DNPY_HAVE_SSE3\" \"-DNPY_INTERNAL_BUILD\" \"-DHAVE_NPY_CONFIG_H\" \"-D_FILE_OFFSET_BITS=64\" \"-D_LARGEFILE_SOURCE=1\" \"-D_LARGEFILE64_SOURCE=1\" \"-D__STDC_VERSION__=0\" \"\/Fdnumpy\\core\\_multiarray_umath.cp312-win_amd64.pyd.p\\src_npysort_binsearch.cpp.pdb\" \/Fonumpy\/core\/_multiarray_umath.cp312-win_amd64.pyd.p\/src_npysort_binsearch.cpp.obj \"\/c\" ..\/numpy\/core\/src\/npysort\/binsearch.cpp\r\n..\/numpy\/core\/src\/npysort\/binsearch.cpp(338): error C2131: expression did not evaluate to a constant\r\n..\/numpy\/core\/src\/npysort\/binsearch.cpp(309): note: failure was caused by a read of an uninitialized symbol\r\n..\/numpy\/core\/src\/npysort\/binsearch.cpp(309): note: see usage of 'binsearch_base<noarg>::value_type::typenum'\r\n..\/numpy\/core\/src\/npysort\/binsearch.cpp(395): note: see reference to class template instantiation 'binsearch_t<noarg>'\r\n```","Cool.\r\n\r\nI think you may need to change the lib name in `openblas.pc`, since the MSVC linker does not automatically add `lib`:\r\n```diff\r\n- Libs: -L${libdir} -lopenblas\r\n+ Libs: -L${libdir} -llibopenblas ","Still same issue after making suggested changes in `openblas.pc`","1 strange thing I noticed while building numpy 1.26.0 and 1.26.4 is that  same` openblas.pc` set in `PKG_CONFIG_PATH` and build numpy 1.26.0, I see pkgconfig finds openblas correctly - \r\n\r\n```\r\nLibrary m found: NO\r\nFound pkg-config: C:\\rtools40\\ucrt64\\bin\\pkg-config.EXE (0.29.2)\r\nRun-time dependency openblas found: YES 0.3.20\r\nChecking if \"CBLAS\" with dependency openblas: links: NO\r\nFound CMake: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin\\cmake.EXE (3.17.20032601)\r\nWARNING: CMake Toolchain: Failed to determine CMake compilers state\r\nRun-time dependency cblas found: NO (tried pkgconfig and cmake)\r\n\r\nnumpy\\1.26.0\\numpy-1.26.0\\numpy\\meson.build:189:4: ERROR: Problem encountered: No CBLAS interface detected! Install a BLAS library with CBLAS support, or use the `allow-noblas` build option (note, this may be up to 100x slower for some linear algebra operations).\r\n```\r\n\r\n\r\nbut when try to build 1.26.4 it fails to find the opeblas lib.\r\n\r\n\r\n","You should have a more detailed log in `.\/build\/meson-logs`. Try to find there what is going on."],"labels":["32 - Installation"]},{"title":"NumPy runtime error when embedding Python 312 in C++ using CMake","body":"### Steps to reproduce:\r\n\r\nUse this CMake File:\r\n\r\n```cmake\r\nset(SAM_ENGINE \"sam_engine\" CACHE STRING \"SAM Engine library name.\")\r\n\r\n# Check https:\/\/cmake.org\/cmake\/help\/latest\/module\/FindPython.html for more information.\r\nfind_package(Python 3.12 REQUIRED COMPONENTS Interpreter Development NumPy)\r\n\r\nadd_library(${SAM_ENGINE} SHARED STATIC\r\n    ${SAM_ENGINE_SRC_FILES} \r\n)\r\n\r\ntarget_link_libraries(${SAM_ENGINE} PUBLIC \r\n    ${Python_LIBRARIES}\r\n)\r\n\r\ntarget_include_directories(${SAM_ENGINE} PUBLIC \r\n    \"${CMAKE_CURRENT_SOURCE_DIR}\/\" \r\n    \"${Python_INCLUDE_DIRS}\/\" \r\n)\r\n```\r\n\r\nC++ code:\r\n\r\n```cpp\r\nPy_Initialize();\r\n\r\n\/\/ Append the directory containing your Python module to sys.path\r\nPyObject* sysPath = PySys_GetObject((char*)\"path\");\r\nPyList_Append(sysPath, PyUnicode_FromString(PE_ENGINE_SCRIPTS_DIR));\r\n\r\nPEEngine::pModule = PyImport_ImportModule(\"get_pe_features\");\r\nif (PEEngine::pModule == NULL) {\r\n    std::cout << \"Error: Failed to import Python module\" << std::endl;\r\n    PyErr_Print();\r\n}\r\n```\r\n\r\n### Error message:\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\__init__.py\", line 24, in <module>\r\n    from . import multiarray\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\multiarray.py\", line 10, in <module>\r\n    from . import overrides\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\overrides.py\", line 8, in <module>\r\n    from numpy.core._multiarray_umath import (\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\n```\r\n\r\nFull error message:\r\n\r\n```\r\n$ .\/tests\/Debug\/sam_engine_tests.exe --gtest_filter=PEEngineTest.TestGetPEFeatures\r\nNote: Google Test filter = PEEngineTest.TestGetPEFeatures\r\n[==========] Running 1 test from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from PEEngineTest\r\n[ RUN      ] PEEngineTest.TestGetPEFeatures\r\nError: Failed to import Python module\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\__init__.py\", line 24, in <module>\r\n    from . import multiarray\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\multiarray.py\", line 10, in <module>\r\n    from . import overrides\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\overrides.py\", line 8, in <module>\r\n    from numpy.core._multiarray_umath import (\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py\", line 144, in <module>\r\n    from numpy.__config__ import show as show_config\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__config__.py\", line 4, in <module>\r\n    from numpy.core._multiarray_umath import (\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\__init__.py\", line 50, in <module>\r\n    raise ImportError(msg)\r\nImportError:\r\n\r\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\r\n\r\nImporting the numpy C-extensions failed. This error can happen for\r\nmany reasons, often due to issues with your setup or how NumPy was\r\ninstalled.\r\n\r\nWe have compiled some common reasons and troubleshooting tips at:\r\n\r\n    https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\r\n\r\nPlease note and check the following:\r\n\r\n  * The Python version is: Python3.12 from \"C:\\Users\\saifs\\Desktop\\scorpion-antimalware\\.build\\tests\\Debug\\sam_engine_tests.exe\"\r\n  * The NumPy version is: \"1.26.4\"\r\n\r\nand make sure that they are the versions you expect.\r\nPlease carefully study the documentation linked above for further help.\r\n\r\nOriginal error was: No module named 'numpy.core._multiarray_umath'\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\/saifs\/Desktop\/scorpion-antimalware\/sam-engine\/PE-engine\/scripts\\get_pe_features.py\", line 1, in <module>\r\n    from ember import PEFeatureExtractor\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ember\\__init__.py\", line 6, in <module>\r\n    import numpy as np\r\n  File \"C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py\", line 149, in <module>\r\n    raise ImportError(msg) from e\r\nImportError: Error importing numpy: you should not try to import numpy from\r\n        its source directory; please exit the numpy source tree, and relaunch\r\n        your python interpreter from there.\r\n[       OK ] PEEngineTest.TestGetPEFeatures (262 ms)\r\n[----------] 1 test from PEEngineTest (262 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test suite ran. (263 ms total)\r\n[  PASSED  ] 1 test.\r\n```\r\n\r\n\r\n### Additional information:\r\n\r\nI have a Windows VM, a fresh one, and only one Python version with these packages installed:\r\n```\r\nSuccessfully installed colorama-0.4.6 ember-0.1.0 joblib-1.3.2 lief-0.14.1 lightgbm-4.3.0 numpy-1.26.4 pandas-2.2.1 python-dateutil-2.8.2 pytz-2024.1 scikit-learn-1.4.1.post1 scipy-1.12.0 six-1.16.0 threadpoolctl-3.3.0 tqdm-4.66.2 tzdata-2024.1\r\n```\r\n\r\nMy env variables:\r\n```\r\nPYTHONPATH: None\r\nPATH: C:\\Users\\saifs\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\saifs\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\Git\\cmd;C:\\Program Files\\CMake\\bin;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit;C:\\Qt\\6.6.1\\msvc2019_64\\lib;C:\\Qt\\6.6.1\\msvc2019_64\\bin;C:\\Program Files (x86)\\googletest-distribution\\bin;C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312;C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Python312\\Scripts;C:\\Users\\saifs\\AppData\\Local\\Programs\\Python\\Launcher;C:\\Users\\saifs\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\saifs\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\Git\\usr\\bin\\vendor_perl;C:\\Program Files\\Git\\usr\\bin\\core_perl\r\n```\r\n\r\nI tried everything nearly, so please can anyone suggest something?\r\n\r\nI think it is worth noting that, my CMake command ran without any errors as well as the build process is perfectly fine. The problem occurs at runtime.\r\n","comments":["I think this is very similar to #13051 and maybe #12977. I reviewed the whole conversation but nothing helpful."],"labels":["32 - Installation"]},{"title":"CI: Compiler sanitizers tests are hanging intermittently","body":"The actions label is `gcc_sanitizers`. All of the test runs show errors, some of which look valid, but none cause the test to fail. I have to wonder where the bogus values come from, are they byproducts of the sanitizer? See https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8008572322\/job\/21875289661 for examples.\r\n\r\nI also note that the time was normally around 20 minutes, it is now well in excess of 2 hours.  Something has changed.\r\n\r\n","comments":["I believe all of those errors are from UBSan and do not fail that job until #24209 is fixed.\r\n\r\n> I also note that the time was normally around 20 minutes, it is now well in excess of 2 hours. Something has changed.\r\n\r\nThe job you linked to ran in 20 minutes. Do you have a job where it took hours to run?","Ah like this one: https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8008729929\/job\/21875802671\r\n\r\nYes, there's a heisenbug that crashes the test runner every so often. It only happens with the compiler sanitizers job and may be a bug in the GCC sanitizer implementation, I haven't been able to reproduce it on clang. It might also be a real issue.","I cancelled that one, it wasn't about to finish any time soon.\r\n\r\nEDIT: But was still running.","The crash is such that the test run doesn't actually end, it times out after six hours. I agree, not great!","Reopening until we are sure the test is no longer hanging. It also seems there is [a failure](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8261417495\/job\/22598680115#step:6:159) that is not picked up by the pytest mechanism\r\n```\r\nnumpy\/_core\/tests\/test_api.py::test_copyto_fromscalar ..\/numpy\/_core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x6020000c7212 for type 'unsigned int', which requires 4 byte alignment\r\n0x6020000c7212: note: pointer points here\r\n 00 00  00 01 00 00 00 01 00 00  00 00 00 00 00 00 00 00  00 11 00 00 04 00 00 00  07 00 00 3c 00 00\r\n              ^ \r\nPASSED\r\nnumpy\/_core\/tests\/test_api.py::test_copyto PASSED\r\nnumpy\/_core\/tests\/test_api.py::test_copyto_permut ..\/numpy\/_core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x6020001d4492 for type 'unsigned int', which requires 4 byte alignment\r\n0x6020001d4492: note: pointer points here\r\n 00 00  00 01 00 01 00 01 00 01  00 00 00 00 00 00 00 00  03 11 00 00 09 00 00 00  07 00 00 3c 00 00\r\n              ^ \r\nPASSED\r\n```","Actually, searching that log for \"runtime error\" shows many of them...","I think most of them are somehwat intentional.  I.e. some code choses to ignore alignment on platforms where we know that is OK (and probably better), but the sanitizers complain it anyway.\r\n\r\nNot sure what to do about those, maybe those code-paths were just optimizations from a time long past, and using a safe code-path the compiler will do fast code anyway.\r\n(I am also OK to just ignore the issue, since unaligned arrays are pretty rare either way.)","Those are all UBsan errors that won\u2019t fail the build until #24209 is fixed.","Ahh, thanks, I missed that. I changed the title of #24209 so a search for sanitizer makes it more prominent.","I just looked at one of the recent failures. It looks like this test is failing in a new way, where if you look in the raw logs there are many many lines like:\r\n\r\n```\r\n2024-03-13T10:24:04.4788196Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4788381Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4788558Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4788738Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4788928Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4789110Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4789286Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4789470Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4789652Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4789834Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790017Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790193Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790375Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790560Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790744Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4790933Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:04.4791116Z AddressSanitizer:DEADLYSIGNAL\r\n```\r\n\r\nI'm not sure why this is getting printed to stderr every 20 microseconds or so, and only on some test runs. It actually seems to start before the tests even begin executing:\r\n\r\n```\r\n2024-03-13T10:23:59.5807981Z Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n2024-03-13T10:23:59.5853061Z Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\r\n2024-03-13T10:23:59.7449512Z Installing collected packages: sortedcontainers, typing_extensions, pluggy, iniconfig, execnet, attrs, pytest, hypothesis, pytest-xdist\r\n2024-03-13T10:24:00.3361522Z Successfully installed attrs-23.2.0 execnet-2.0.2 hypothesis-6.99.5 iniconfig-2.0.0 pluggy-1.4.0 pytest-8.1.1 pytest-xdist-3.5.0 sortedcontainers-2.4.0 typing_extensions-4.10.0\r\n2024-03-13T10:24:00.6105621Z \u001b[92m\u001b[1mInvoking `build` prior to running tests:\u001b[0m\r\n2024-03-13T10:24:00.9137798Z \u001b[94m\u001b[1m$ \/opt\/hostedtoolcache\/Python\/3.11.8\/x64\/bin\/python vendored-meson\/meson\/meson.py compile -C build\u001b[0m\r\n2024-03-13T10:24:00.9166244Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9167759Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9168550Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9169412Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9170111Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9170756Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9171404Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9175971Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9176791Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9177516Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9178080Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9178603Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9179115Z AddressSanitizer:DEADLYSIGNAL\r\n2024-03-13T10:24:00.9179624Z AddressSanitizer:DEADLYSIGNAL\r\n```\r\n\r\nI guess if this gets to be too annoying we can disable the tests. We could also look into using the clang sanitizers, which might be more stable than the gcc sanitizers.","It's now failing on every run in the same way. I still don't understand why this is happening so I've manually disabled the workflow in the github actions settings.\r\n\r\nI *think* if we build numpy with clang we should be able to use the clang sanitizers which are generally better tested (google uses them internally on all code)."],"labels":["component: CI"]},{"title":"BUG: incompatible pointer type for `npy_int32t` and `npy_uint32` on i686","body":"### Describe the issue:\n\nOn i686, numpy defines:\r\n* `npy_int32` type as `long int`.\r\n*  `npy_uint32` type as `long unsigned int`.\r\nThese are incompatible with the stdint definitions of:\r\n* `int32_t` as `int`\r\n* `uint32_t` as `unsigned int`\r\n\r\nThis is causing h5py to fail to build on Fedora 40 due to the use of `-Werror=incompatible-pointer-types`.  I think it's fairly reasonable to expect that the `npy_*int*` types to be completely interchangeable with the `stdint` types.\n\n### Reproduce the code example:\n\n```python\nfrom numpy cimport int8_t, uint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t\r\n...\r\n    def read_direct_chunk(self, offsets, PropID dxpl=None, unsigned char[::1] out=None):\r\n...\r\n        cdef uint32_t filters\r\n...\r\n            H5Dread_chunk(dset_id, dxpl_id, offset, &filters, chunk_buffer)\n```\n\n\n### Error message:\n\n```shell\n\/builddir\/build\/BUILD\/h5py-3.10.0\/serial\/h5py\/defs.c: In function \u2018__pyx_f_4h5py_4defs_H5Dread_chunk\u2019:\r\n\/builddir\/build\/BUILD\/h5py-3.10.0\/serial\/h5py\/defs.c:14922:85: error: passing argument 4 of \u2018H5Dread_chunk\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\r\n14922 |         __pyx_v_r = H5Dread_chunk(__pyx_v_dset_id, __pyx_v_dxpl_id, __pyx_v_offset, __pyx_v_filters, __pyx_v_buf);\r\n      |                     ^~~~~~~~~~~~~~~\r\n      |                     |\r\n      |                     __pyx_t_5numpy_uint32_t * {aka long unsigned int *}\r\nIn file included from \/usr\/include\/hdf5.h:25,\r\n                 from \/builddir\/build\/BUILD\/h5py-3.10.0\/serial\/h5py\/api_compat.h:27,\r\n                 from \/builddir\/build\/BUILD\/h5py-3.10.0\/serial\/h5py\/defs.c:1246:\r\n\/usr\/include\/H5Dpublic.h:1003:92: note: expected \u2018uint32_t *\u2019 {aka \u2018unsigned int *\u2019} but argument is of type \u2018__pyx_t_5numpy_uint32_t *\u2019 {aka \u2018long unsigned int *\u2019}\r\n 1003 | H5_DLL herr_t H5Dread_chunk(hid_t dset_id, hid_t dxpl_id, const hsize_t *offset, uint32_t *filters,\r\n      |                  ~~~~~~~~~~^~~~~~~\n```\n\n\n### Python and NumPy Versions:\n\n1.26.2\r\n3.12.2 (main, Feb  7 2024, 00:00:00) [GCC 14.0.1 20240127 (Red Hat 14.0.1-0)]\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["Thanks for the report @opoplawski.\r\n\r\n> I think it's fairly reasonable to expect that the `npy_*int*` types to be completely interchangeable with the `stdint` types.\r\n\r\nAgreed. I don't know what the history of the current definition. My guess is it's been like that since forever, and originally there was a hypothetical concern about `int` in theory being able to be 16-bit on some unknown OS. Maybe @charris remembers?\r\n\r\nLet me link two issues which I think have the same root cause:\r\n- https:\/\/github.com\/scipy\/scipy\/issues\/19993 (SciPy doesn't build with GCC 14 as a result of `typedef npy_int32 F_INT` in `f2py`, resulting in the same `incompatible-pointer-types` error (also Fedora 40)\r\n- gh-25777 reports two compile warnings from an `f2py` example in our docs; the first one is due to this `long int` vs. `int` issue\r\n\r\n> This is causing h5py to fail to build on Fedora 40 due to the use of `-Werror=incompatible-pointer-types`.\r\n\r\nCan you clarify whether this is the GCC 14 default, or if Fedora is adding a `-Werror` flag here?","> > This is causing h5py to fail to build on Fedora 40 due to the use of `-Werror=incompatible-pointer-types`.\r\n> \r\n> Can you clarify whether this is the GCC 14 default, or if Fedora is adding a `-Werror` flag here?\r\n\r\nI can confirm this is the GCC 14 default, see https:\/\/gcc.gnu.org\/gcc-14\/porting_to.html#c and https:\/\/wiki.gentoo.org\/wiki\/Modern_C_porting. I think we've hit this too, we just didn't end up reporting it yet as I hadn't dug into it.","Thanks! Okay, then I think we need to do something about this very soon (just not sure what yet). I'm pretty sure there is still at least one bug in `f2py` lurking, and in SciPy we've got this warning silenced in `linalg` targets for reasons I can't quite remember.\r\n\r\nIn our Cython support (https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/__init__.cython-30.pxd) we have:\r\n```cython\r\ncdef extern from \"numpy\/arrayobject.h\":\r\n    ctypedef signed int         npy_int32\r\n\r\n# Typedefs that matches the runtime dtype objects in\r\n# the numpy module.\r\n\r\nctypedef npy_int32      int32_t\r\n```\r\n\r\nThe typedefs in https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/_core\/include\/numpy\/npy_common.h#L796 are:\r\n```C\r\n#ifdef PY_LONG_LONG\r\ntypedef PY_LONG_LONG npy_longlong;\r\n#else\r\ntypedef long npy_longlong;\r\n#endif \r\n\r\n\/* various size check to pick one of these: *\/\r\n#elif NPY_BITSOF_LONG == 32\r\n    typedef long npy_int32;\r\n#elif NPY_BITSOF_LONGLONG == 32\r\n#  ifndef NPY_INT32\r\n    typedef npy_longlong npy_int32;\r\n...\r\n#elif NPY_BITSOF_INT == 32\r\n#ifndef NPY_INT32\r\n    typedef int npy_int32;\r\n...\r\n#elif NPY_BITSOF_SHORT == 32\r\n#ifndef NPY_INT32\r\n...\r\n#elif NPY_BITSOF_CHAR == 32\r\n#ifndef NPY_INT32\r\n    typedef short npy_int32;\r\n    typedef signed char npy_int32;\r\n```\r\n\r\nSo it doesn't look like the intent is to define `npy_int32` as `long` or `long int` if `int` is also 32 bits long. However, I'm also not sure if it's possible to do this in a way that works on all platforms and allow mixing `npy_int32` with `int` or other builtin types that are guaranteed to exist.\r\n\r\nIf we would swap the check to first check if `int` is 32-bit and only try `long` if that's not the case, that should fix this case. Hard to tell what it would break though.\r\n\r\nThe C standard does not have any rules for what `int32_t` aliases with if it exists, right?","Or maybe all of that code is obsolete, and now that we require C99 we can throw it away and do this unconditionally:\r\n```C\r\ntypedef int32_t npy_int32;\r\ntypedef uint32_t npy_uint32;\r\n```","For anyone wanting to work on this: the CI run to check is the azure pipelines `Linux_Python_39_32bit_full_with_asserts` workflow, which can be run locally via docker\r\n```\r\ndocker run -v $(pwd):\/numpy -e CFLAGS=\"-msse2 -std=c99 -UNDEBUG\" \\\r\n            -e F77=gfortran-5 -e F90=gfortran-5 quay.io\/pypa\/manylinux2014_i686 \\\r\n            \/bin\/bash -xc \"source \/numpy\/tools\/ci\/run_32_bit_linux_docker.sh\"\r\n```","> Or maybe all of that code is obsolete, and now that we require C99 we can throw it away and do this unconditionally:\r\n\r\nI don't mind changing this, just to note that these get floated in the same way all the way to Python\/ctypes\/buffer protocol, etc. as they end up in the character codes.  So changing the mapping means changing it in a few places."],"labels":["00 - Bug","component: numpy._core"]},{"title":"BUG: Test_X86_Features.test_features fails on Fedora Copr builders: AVX512_SPR actual-True, desired-False","body":"### Describe the issue:\r\n\r\nHello, I have noticed that the Test_X86_Features.test_features repeatedly (but not always) fails in Fedora Linux Copr builders. I've recently reproduced that with 1.26.2, but can try with 1.26.4 as well, if there is an indication that it should matter.\r\n\r\nI've isolated the failure to one particular builder configuration.\r\n\r\n```\r\nCPU info:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             4\r\nOn-line CPU(s) list:                0-3\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8488C\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 2\r\nSocket(s):                          1\r\nStepping:                           8\r\nBogoMIPS:                           4800.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          96 KiB (2 instances)\r\nL1i cache:                          64 KiB (2 instances)\r\nL2 cache:                           4 MiB (2 instances)\r\nL3 cache:                           105 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-3\r\n```\r\n\r\n### Error message:\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n_______________________ Test_X86_Features.test_features ________________________\r\n\r\nself = <numpy.core.tests.test_cpu_features.Test_X86_Features object at 0x7f498e01c3e0>\r\n\r\n    def test_features(self):\r\n        self.load_flags()\r\n        for gname, features in self.features_groups.items():\r\n            test_features = [self.cpu_have(f) for f in features]\r\n>           assert_features_equal(__cpu_features__.get(gname), all(test_features), gname)\r\nE           AssertionError: Failure Detection\r\nE            NAME: 'AVX512_SPR'\r\nE            ACTUAL: True\r\nE            DESIRED: False\r\nE           \r\n###########################################\r\n### Extra debugging information\r\n###########################################\r\n-------------------------------------------\r\n--- NumPy Detections\r\n-------------------------------------------\r\n{MMX: True, SSE: True, SSE2: True, SSE3: True, SSSE3: True, SSE41: True, POPCNT: True, SSE42: True, AVX: True, F16C: True, XOP: False, FMA4: False, FMA3: True, AVX2: True, AVX512F: True, AVX512CD: True, AVX512ER: False, AVX512PF: False, AVX5124FMAPS: False, AVX5124VNNIW: False, AVX512VPOPCNTDQ: True, AVX512VL: True, AVX512BW: True, AVX512DQ: True, AVX512VNNI: True, AVX512IFMA: True, AVX512VBMI: True, AVX512VBMI2: True, AVX512BITALG: True, AVX512FP16: True, AVX512_KNL: False, AVX512_KNM: False, AVX512_SKX: True, AVX512_CLX: True, AVX512_CNL: True, AVX512_ICL: True, AVX512_SPR: True, VSX: False, VSX2: False, VSX3: False, VSX4: False, VX: False, VXE: False, VXE2: False, NEON: False, NEON_FP16: False, NEON_VFPV4: False, ASIMD: False, FPHP: False, ASIMDHP: False, ASIMDDP: False, ASIMDFHM: False}\r\n-------------------------------------------\r\n--- SYS \/ CPUINFO\r\n-------------------------------------------\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 143\r\nmodel name\t: Intel(R) Xeon(R) Platinum 8488C\r\nstepping\t: 8\r\nmicrocode\t: 0x2b000571\r\ncpu MHz\t\t: 3792.916\r\ncache size\t: 107520 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 31\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd ida arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid cldemote movdiri movdir64b md_clear serialize amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nbugs\t\t: spectre_v1 spectre_v2 spec_store_bypass swapgs eibrs_pbrsb\r\nbogomips\t: 4800.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 46 bits physical, 48 bits virtual\r\npower management:\r\nE           \r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 143\r\nmodel name\t: Intel(R) Xeon(R) Platinum 8488C\r\nstepping\t: 8\r\nmicrocode\t: 0x2b000571\r\ncpu MHz\t\t: 3759.239\r\ncache size\t: 107520 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 1\r\ncpu cores\t: 2\r\napicid\t\t: 2\r\ninitial apicid\t: 2\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 31\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid....\r\n-------------------------------------------\r\n--- SYS \/ AUXV\r\n-------------------------------------------\r\nAT_SYSINFO_EHDR:      0x7ffdbc3d4000\r\nAT_MINSIGSTKSZ:       11952\r\nAT_HWCAP:             1f8bfbff\r\nAT_PAGESZ:            4096\r\nAT_CLKTCK:            100\r\nAT_PHDR:              0x561a72b70040\r\nAT_PHENT:             56\r\nAT_PHNUM:             13\r\nAT_BASE:              0x7fa02ec94000\r\nAT_FLAGS:             0x0\r\nAT_ENTRY:             0x561a72b72970\r\nAT_UID:               1001\r\nAT_EUID:              1001\r\nAT_GID:               135\r\nAT_EGID:              135\r\nAT_SECURE:            0\r\nAT_RANDOM:            0x7ffdbc228689\r\nAT_HWCAP2:            0x2\r\nAT_EXECFN:            \/bin\/true\r\nAT_PLATFORM:          x86_64\r\nAT_??? (0x1b): 0x1c\r\nAT_??? (0x1c): 0x20\r\n\r\nfeatures   = ['AVX512F', 'AVX512CD', 'AVX512BW', 'AVX512DQ', 'AVX512VL', 'AVX512IFMA', ...]\r\ngname      = 'AVX512_SPR'\r\nself       = <numpy.core.tests.test_cpu_features.Test_X86_Features object at 0x7f498e01c3e0>\r\ntest_features = [True, True, True, True, True, True, ...]\r\n\r\n..\/..\/..\/..\/BUILDROOT\/numpy-1.26.2-4.fc41.x86_64\/usr\/lib64\/python3.12\/site-packages\/numpy\/core\/tests\/test_cpu_features.py:77: AssertionError\r\n```\r\n\r\n### Python and NumPy Versions:\r\n\r\n1.26.2\r\n3.12.2, 3.12.1, 3.13.0a4\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":[],"labels":["00 - Bug","component: SIMD"]},{"title":"ENH: Add round to significant figures in package ","body":"### Proposed new feature or change:\r\n\r\nCode Suggestion:\r\n\r\n```python\r\ndef round_to_sig_figs(value, sig_figs=3):\r\n   \"\"\"Round a number to the selected number of significant figures.\r\n\r\n   Round a number to the selected number of significant figures.\r\n\r\n   Parameters\r\n   ----------\r\n   value: int or float\r\n       The number that will be rounded to the selected number of\r\n       significant figures.\r\n   sig_figs: int, default=3\r\n       The number significant figures that the 'value' variable\r\n       will be rounded too. If sig_fig=0, it will return 0.0.\r\n\r\n   Returns\r\n   -------\r\n   value_rounded_to_sig_figs: float\r\n       The input 'value' variable rounded to the selected number\r\n       significant figures. If sig_fig=0, it will return 0.0.\r\n   \"\"\"\r\n   if value == 0:\r\n      value_rounded_to_sig_figs = 0\r\n\r\n   else:\r\n      value_rounded_to_sig_figs = float(\r\n         round(value, sig_figs - int(math.floor(math.log10(abs(value)))) - 1)\r\n      )\r\n\r\n   return value_rounded_to_sig_figs\r\n```","comments":["Thanks for the suggestion @bc118. This is a utility that can be written in a few lines of Python code on top of `np.round`, so it looks to me like it doesn't meet the bar for inclusion in numpy.\r\n\r\nAdditionally, would this function even be useful for arrays, or only for floats? ","IMO it should be included, as it is helpful for statistics and all applications, and it is annoying to add in every code that people write. Many people in the community wonder why this is not added, along with other things.  \r\n\r\nIf you want to round arrays also, feel free to add this.  I am not opposed to anything, I am just suggesting it be added in general.  It would be useful for arrays for the same reason, after printing and reimporting data, or in linear math arrays....  My intent was to show a general example and let you implement it how the others are implimented.","Why have any other round feature, like decimals, which are even more simple  already in numpy? \r\n\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.around.html\r\n\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.round.html\r\n\r\nTo be honest, round decimals are less valuable than round to sig fig when you have limited bytes for numbers, and you can never be sure how large the whole numbers are going to be.,\r\n","We follow the python lead, which has `__round__` in its [object model](https:\/\/docs.python.org\/3\/reference\/datamodel.html#object.__round__). The usual way to approach adding new functionality in NumPy, before the Array API, was to [ask the mailing list](https:\/\/numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion) for comment. These days we tend to defer to the Array API, whose [round](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.round.html) does not even have a `decimals` kwarg, much less a `significant_digits` one.","@bc118 \"significant figures\" is a useful function, particularly where formatting matters.\r\nIntegrating a feature like this into numpy could indeed reduce repetitive implementations.\r\n\r\nHowever, it's crucial to ensure that such a function covers all necessary cases.\r\nCode examples (i.e. on stackoverflow) are sometimes error prone and in this specific case you could add a check for np.isfinite(value) to catch nan & inf values. You could add a remark that rounding rule is ties to equal (i.e. 2.5->2, 3.5->4). \r\nThere are probably even more things to consider and there is a whole Python package called \"rounders\" dedicated to this topic.\r\nhttps:\/\/github.com\/mdickinson\/rounders\/tree\/main\r\n\r\nIf a function like \"significant figures\" is used in many packages then robustness and reliability is paramount.\r\nThis would make the case to include such functions into the Python math standard library or Numpy's library.\r\n\r\nThe discussion around a similar feature request for Python's math library shows the complexity involved.\r\nhttps:\/\/discuss.python.org\/t\/rounding-to-significant-figures-feature-request-for-math-library\/16395\/87\r\n\r\nThe fact that \"significant figures\" didn't make it over the math library bar suggests that the criteria for inclusion are beyond just utility reasons.\r\nNevertheless, feature requests like the \"significant figures\" function are valuable contributions.\r\n\r\n@mattip given that the Array API guides the behavior of array methods, how does it impact the development and inclusion of standalone functions in numpy?\r\nAre there specific considerations or guidelines that numpy follows to ensure consistency between array methods and standalone functions? What are numpy's main standards for inclusion of standalone functions?\r\n\r\nI appreciate any insights.\r\n\r\nBest regards, Oyibo","> given that the Array API guides the behavior of array methods, how does it impact the development and inclusion of standalone functions in numpy?\r\n\r\nI don't think it does in cases like this feature request. This is certainly too niche for a standard. Whether it deserves inclusion in `numpy` or `numpy.lib.xxx` can still be decided by the numpy team just fine. If there's any change, it's that the bar (which was already high) went up a bit more with all the care we've taken with the Python API cleanups and array API standard addition in NumPy 2.0\r\n\r\n> If a function like \"significant figures\" is used in many packages then robustness and reliability is paramount.\r\nThis would make the case to include such functions into the Python math standard library or Numpy's library.\r\n\r\nOne could also say that if there are _that_ many ways to do it, pointing users at that `rounders` package is just fine. \r\n\r\n> Are there specific considerations or guidelines that numpy follows to ensure consistency between array methods and standalone functions? What are numpy's main standards for inclusion of standalone functions?\r\n\r\nndarray methods: we don't want any more, adding a new method will be highly exceptional. Functions are preferred.\r\n\r\nFor functions, they should at least be:\r\n1. widely applicable\r\n2. usually nontrivial to implement (if it's a few lines of pure Python\/NumPy code, we tend to avoid adding it)\r\n3. fit well in NumPy's existing scope\r\n\r\n","Personally, I'm less interested in adding a function that rounds the numbers themselves (that is, a function that takes an `ArrayLike` and returns an `ArrayLike`).\r\n\r\nHowever, I'd love to have the [option to _format_](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.printoptions.html) float scalars and arrays rounded to a given number of significant digits rather than number digits after the decimal place. And preferably with [engineering notation](https:\/\/en.wikipedia.org\/wiki\/Engineering_notation).","@rgommers thank you for your response. \r\nGiven the criteria provided, it appears unlikely the function \"significant figures\" meets the standards for inclusion into NumPy.\r\nPersonally, I find the reference to an alternative existing packages acceptable.\r\nThank you and the NumPy team for your dedication to the library's development."],"labels":["01 - Enhancement"]},{"title":"ENH : np.argmax is unusually time consuming for multidimensional array","body":"### Describe the issue:\n\nBoth np.amax and np.argmax are expected to have a time complexity of O(n)\uff0cso they should have similar computational times.\r\nHowever, they only exhibit comparable performance on 1D arrays. \r\nFor 2D or higher dimensional arrays, np.amax consistently outperforms np.argmax by a factor of 8x or more. It's strange.\r\n\r\n\n\n### Reproduce the code example:\n\n```python\nimport timeit\r\n\r\nstmt1 = \"np.argmax(a, axis=0)\"\r\nstmt2 = \"np.amax(a, axis=0)\"\r\nsetup_1d = \"import numpy as np; a = np.random.rand(3*768*768)\"\r\nsetup_2d = \"import numpy as np; a = np.random.rand(3,768*768)\"\r\n\r\nexecution_time1 = timeit.timeit(stmt1, setup=setup_2d, number=1000)\r\nprint(f\"Execution time for np.argmax on 2d array: {execution_time1} seconds\")\r\n\r\nexecution_time2 = timeit.timeit(stmt2, setup=setup_2d, number=1000)\r\nprint(f\"Execution time for np.amax on 2d array: {execution_time2} seconds\")\r\n\r\nexecution_time1 = timeit.timeit(stmt1, setup=setup_1d, number=1000)\r\nprint(f\"Execution time for np.argmax on 1d array: {execution_time1} seconds\")\r\n\r\nexecution_time2 = timeit.timeit(stmt2, setup=setup_1d, number=1000)\r\nprint(f\"Execution time for np.amax on 1d array: {execution_time2} seconds\")\n```\n\n\n### Error message:\n\n```shell\nExecution time for np.argmax on 2d array: 16.13085489999503 seconds\r\nExecution time for np.amax on 2d array: 2.400201399810612 seconds\r\nExecution time for np.argmax on 1d array: 0.6763406000100076 seconds\r\nExecution time for np.amax on 1d array: 0.4886799002997577 seconds\n```\n\n\n### Python and NumPy Versions:\n\nPython: 3.10.13\r\nNumpy: 1.26.4\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["@penzaijun - It may be due to the fact that argmax will flatten the array if its more than one dimensional first and then return the indices of the maximum value. The relevant documentation - https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.argmax.html. ","@amentee \r\n\r\nThank you for the reply !\r\n\r\nAccording to the documentation, argmax will flatten the array only when axis is None. But for all tests above, I have set axis=0. \r\n\r\nAdditionally, I tested the time consumption of argmax on 2d arrays without setting axis=0. The result is 0.680 seconds, very close to the time consumption of directly performing argmax on 1d arrays with the same size. It seems that flattening the array does not cost too much time.   \r\n\r\nI should correct my statement: the time consumption becomes abnormally high only when argmax is performed on multidimensional arrays along a specified axis.\r\n\r\n**Additional Test:**\r\nexecution_time3 = timeit.timeit(\"np.argmax(a)\", setup=setup_2d, number=1000)\r\nprint(f\"Execution time for np.argmax on 2d array with axis==None: {execution_time3} seconds\")\r\n\r\n**Test Result:**\r\nExecution time for np.argmax on 2d array with axis==None: 0.6801762999966741 seconds\r\n\r\n","It seems that they have quite different implementations. The list of array method implementations in C can be found in [methods.c](https:\/\/github.com\/numpy\/numpy\/blob\/e920ca42d98c4911c2294784f8809b38735923a4\/numpy\/_core\/src\/multiarray\/methods.c#L2853). From this, you can see that `argmax` uses a custom implementation (the main part of which can be found in [calculation.c](https:\/\/github.com\/numpy\/numpy\/blob\/e920ca42d98c4911c2294784f8809b38735923a4\/numpy\/_core\/src\/multiarray\/calculation.c#L40)), whereas `amax` (which is the same as `max`) is defined as the [`reduce`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.ufunc.reduce.html) method of the `maximum` ufunc. This method has a general definition for the ufunc base class in [ufunc_object.c](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/_core\/src\/umath\/ufunc_object.c#L3449).\r\n\r\nIn either case you have to loop over all elements along the given axis so I'd guess the discrepancy is due to the fact that ufuncs like `maximum` have highly optimized implementations."],"labels":["00 - Bug"]},{"title":"BUG: masked array division broken with np.seterr(under=\"raise\") ","body":"### Describe the issue:\n\nWhen `np.seterr(under=\"raise\")` is set, division does not work with masked arrays due to an internal multiplication underflow.\n\n### Reproduce the code example:\n\n```python\nnp.seterr(under=\"raise\")\r\nx=np.arange(0, 3, 0.1)\r\nX = np.ma.array(x)\r\nx2 = x \/ 2.0  # <- works\r\nX2 = X \/ 2.0  # <- fails\n```\n\n\n### Error message:\n\n```shell\nFile \/usr\/lib\/python3.11\/site-packages\/numpy\/ma\/core.py:4275, in MaskedArray.__truediv__(self, other)\r\n   4273 if self._delegate_binop(other):\r\n   4274     return NotImplemented\r\n-> 4275 return true_divide(self, other)\r\n\r\nFile \/usr\/lib\/python3.11\/site-packages\/numpy\/ma\/core.py:1171, in _DomainedBinaryOperation.__call__(self, a, b, *args, **kwargs)\r\n   1169 domain = ufunc_domain.get(self.f, None)\r\n   1170 if domain is not None:\r\n-> 1171     m |= domain(da, db)\r\n   1172 # Take care of the scalar case first\r\n   1173 if not m.ndim:\r\n\r\nFile \/usr\/lib\/python3.11\/site-packages\/numpy\/ma\/core.py:858, in _DomainSafeDivide.__call__(self, a, b)\r\n    856 a, b = np.asarray(a), np.asarray(b)\r\n    857 with np.errstate(invalid='ignore'):\r\n--> 858     return umath.absolute(a) * self.tolerance >= umath.absolute(b)\r\n\r\nFloatingPointError: underflow encountered in multiply\n```\n\n\n### Python and NumPy Versions:\n\nPython 3.11, numpy 1.26.3\n\n### Runtime Environment:\n\n```\r\n[{'numpy_version': '1.26.3',\r\n  'python': '3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]',\r\n  'uname': uname_result(system='Linux', node='belanna', release='6.7.4-arch1-1', version='#1 SMP PREEMPT_DYNAMIC Mon, 05 Feb 2024 22:07:49 +0000', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL',\r\n                                    'AVX512_SPR']}}]\r\n```\n\n### Context for the issue:\n\nThis was reported against Matplotlib in https:\/\/github.com\/matplotlib\/matplotlib\/issues\/27770\r\n\r\n@2sn  will have to provide any additional context.","comments":["This can be worked around by using `X2 = X*0.5` instead.","This domain stuff is fatally flawed, but I guess `np.errstate(all=\"ignore\")` makes sense in any domain related calculation especially.","I'm working on this issue"],"labels":["00 - Bug","component: numpy.ma","sprintable"]},{"title":"BUG: Behavior of `atleast_3d` is surprising ","body":"### Describe the issue:\r\n\r\nAdding axis to the right is perplexing, specially when `atleast_2d`  adds to the left and numpy broadcasting behavior always grows to the left.\r\n\r\nIs there a good reason for this? I see there's a PR to customize behavior via kwarg but the default behavior is kept.\r\n\r\nI suspect this is more of a bug\/ historical accident than a feature but happy to be corrected. This is relevant for libraries that try to emulate numpy API but understandably don't want to frustrate users with obscure behavior.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nNot needed\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\nLatest and main, it's documented behavior \r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["IIRC, it is an odd behavior. There was a discussion of it long ago, but the function itself goes back to 2002 before there was NumPy. I assume someone wanted it broadcasting on the last (-1) axis for some reason. Note that dimensions of length 1 are always broadcast.","This might be an argument for reviving gh-18386 with `atleast_nd`...","Would it be reasonable to change the behavior in 2.0?","Maybe sensible to start warning in 2.0 and replacing with `atleast_nd` in due course? ","The origin of the semantics are to coerce `(H, W)` intensity images to a `(H, W, C)` channels-at-the-end \"3D\" shape, a common use case.","But still - the difference with `atleast_2d` would be very difficult to predict without knowing that history. \r\n\r\nPerhaps rename (with deprecation) to `hw2hwc` or prefer`np.atleast_nd(hw, 3, pos=-1)` or `hw[..., None] if hw.ndim == 2 else hw` for that case?","Adding `atleast_nd` sounds useful and fine. Deprecating `atleast_3d` certainly should not happen before `atleast_nd` has existed for several releases (window of NEP 29 \/ SPEC 0), and is probably not a good idea given it's used heavily and has matching implementations in other libraries. A doc note seems more appropriate.\r\n\r\n"],"labels":["00 - Bug"]},{"title":"BUG: The F2PY statement `fortranname` does not work for functions. ","body":"### Describe the issue:\r\n\r\n# F2PY - Fortranname\r\n\r\n\r\nAs I understand it, the statement `fortranname` should work for both\r\nsubroutines and functions. However, The F2PY statement `fortranname`\r\nseems only to work for subroutines and not for functions.\r\n\r\n### Reproduce the code example:\r\n\r\n## Subroutine\r\n\r\nThe F2PY statement `fortranname` works in subroutines.\r\n\r\n`subfortranname.f`\r\n\r\n```fortran\r\n      SUBROUTINE SUBFORTRANNAME(A,B,C)\r\n      REAL*8 A, B, C\r\n      C = A + B\r\n      END SUBROUTINE\r\n```\r\n\r\n`subfortranname.pyf`\r\n\r\n``` python\r\npython module subfortranname ! in \r\n    interface  ! in :subfortranname\r\n        subroutine subfortranname_default(a,b,c) ! in :subfortranname:subfortranname.f\r\n            fortranname subfortranname\r\n            real*8 :: a\r\n            real*8 :: b\r\n            real*8, intent(out) :: c\r\n        end subroutine subfortranname_default\r\n    end interface \r\nend python module subfortranname\r\n```\r\n\r\n`test_subfortranname.py`\r\n\r\n``` python\r\nimport subfortranname\r\n\r\nprint(subfortranname.__doc__)\r\n```\r\n\r\n``` bash\r\nThis module 'subfortranname' is auto-generated with f2py (version:1.26.4).\r\nFunctions:\r\n    c = subfortranname_default(a,b)\r\n.\r\n```\r\n\r\n## Function\r\n\r\nThe F2PY statement `fortranname` does not work in functions.\r\n\r\n`funcfortranname.f`\r\n\r\n``` fortran\r\n      REAL*8 FUNCTION FUNCFORTRANNAME(A,B)\r\n      REAL*8 A, B\r\n      FUNCFORTRANNAME = A + B\r\n      RETURN\r\n      END FUNCTION\r\n```\r\n\r\n`funcfortranname.pyf`\r\n\r\n``` python\r\npython module funcfortranname ! in \r\n    interface  ! in :funcfortranname\r\n        function funcfortranname_default(a,b) ! in :funcfortranname:funcfortranname.f\r\n            fortranname funcfortranname\r\n            real*8 :: a\r\n            real*8 :: b\r\n            real*8, intent(out) :: funcfortranname\r\n        end function funcfortranname_default\r\n    end interface \r\nend python module funcfortranname\r\n```\r\n\r\n`test_funcfortranname.py`\r\n\r\n``` python\r\nimport funcfortranname\r\n\r\nprint(funcfortranname.__doc__)\r\n```\r\n\r\n``` bash\r\nTraceback (most recent call last):\r\n  File \"\/home\/johannes\/Sandbox\/f2py\/fortranname\/test_f.py\", line 1, in <module>\r\n    import funcfortranname\r\nImportError: \/home\/johannes\/Sandbox\/f2py\/fortranname\/funcfortranname.cpython-310-x86_64-linux-gnu.so: undefined symbol: funcfortranname_default_\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\nPython 3.10.12 and 3.12.1\r\nNumpy 1.26.4\r\n\r\n```bash\r\n1.26.4\r\n3.12.1 (main, Dec 10 2023, 15:07:36) [GCC 11.4.0]\r\n```\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":[],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"ENH: Print Option: Always show an array's dtype","body":"### Proposed new feature or change:\r\n\r\n#### Motive \r\n\r\nThe default dtype of array is platform-dependant. ( #9464 )\r\n\r\nWhen running tests in a continuous integration context, that are ran on multiple platforms (Windows, macOS, Linux), the fact that the default dtypes of arrays can vary must be taken into account.\r\n\r\nThe issue appears for tests relying on numpy's arrays representations. Indeed, the default dtype of the array is not displayed in the array representation. This means that an expected output representation is now dependant on the platform. Writing OS-specific tests is now unavoidable.\r\n\r\nWhat I would like is being able to write platform independent repeatable outputs that can be used for automated testing.\r\n\r\n#### Example\r\n\r\n##### Actual\r\n\r\nOn my machine, the default dtype for integer arrays is `int64`. Here are some examples of array creations and their representations:\r\n```python\r\nIn [3]: import numpy as np\r\n\r\nIn [4]: np.array([1, 2, 3])\r\nOut[4]: array([1, 2, 3])\r\n\r\nIn [5]: np.array([1, 2, 3], dtype=np.int64)\r\nOut[5]: array([1, 2, 3])\r\n\r\nIn [6]: np.array([1, 2, 3], dtype=np.int32)\r\nOut[6]: array([1, 2, 3], dtype=int32)\r\n```\r\n\r\nWe can see that:\r\n- When creating an array with no dtype kwarg, the default dtype is used. The array representation solely is not enough to know the actual dtype.\r\n- When creating an array with a dtype kwarg matching the default integer dtype of the platform, the resulting array representation is the same, and dtype is also implicit.\r\n- The last case is the most explicit: the user provides the expected dtype, and the representation reflects that. This only works for non-default dtypes.\r\n\r\n##### Desired \r\n\r\n```python\r\nIn [3]: import numpy as np\r\n\r\nIn [4]: np.array([1, 2, 3])\r\nOut[4]: array([1, 2, 3], dtype=int64)\r\n\r\nIn [5]: np.array([1, 2, 3], dtype=np.int64)\r\nOut[5]: array([1, 2, 3], dtype=int64)\r\n\r\nIn [6]: np.array([1, 2, 3], dtype=np.int32)\r\nOut[6]: array([1, 2, 3], dtype=int32)\r\n```\r\n\r\nThe dtype is always printed out, and the default dtype does not influence the representation. So, since the default dtype depends on the platform, and the representation depends on the dtype, the chain is broken and the representation does not depend anymore on the platform. Writing platform independant tests relying on representation is now easier.\r\n\r\nfrom\r\n`platform <- default dtype <- repr` => `platform <- repr`\r\nto\r\n`platform <- default dtype <\/- repr` => `platform <\/- repr`\r\n\r\n\r\n#### Existing solutions I looked for \r\n\r\n##### `np.set_printoptions`\r\n\r\nI first looked into https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.set_printoptions.html \r\nI experimented with kwarg `legacy='1.13'` and `legacy='1.21`, without success. Also, even if I were successful, I would have dislike relying on a kwarg named `legacy`, strongly implying it should not be used anymore in new code.\r\n\r\n#### Proposed solution\r\n\r\nAdding a new `dtype` printing option\r\n\r\n```python\r\nimport numpy\r\n\r\n\r\nnp.set_printoptions(dtype=\"default\") # current behaviour\r\nnp.set_printoptions(dtype=\"always\") # always print dtype\r\nnp.set_printoptions(dtype=\"never\") # never print dtype\r\n```\r\n\r\n#### Technical Analysis \r\n\r\nThe function [`_array_repr_implementation`](https:\/\/github.com\/numpy\/numpy\/blob\/d35cd07ea997f033b2d89d349734c61f5de54b0d\/numpy\/core\/arrayprint.py#L1487) implements the array representation logic. We can see the logic where it adds the suffix, and there is no way to force print the dtype, or force not printing it.\r\n\r\nAllow to override this param could be helpful: \r\n```python\r\ndef _array_repr_implementation(\r\n        arr, max_line_width=None, precision=None, suppress_small=None,\r\n+       skipdtype: bool | None = None,       \r\n        array2string=array2string):\r\n        ...\r\n- skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0\r\n+ if skipdtype is None:\r\n+     skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0\r\n```\r\n\r\nRole of the proposed new `skipdtype` three-valued kwarg:\r\n- `None`: current behaviour, platform-dependant\r\n- `False`: always print the `, dtype=...` suffix\r\n- `True`: never print the `, dtype=...` suffix\r\n\r\n#### Additional links\r\n\r\n- https:\/\/github.com\/pydata\/xarray\/pull\/8702\r\n","comments":["FWIW, I changed things so in NumPy 2.0 the default on windows is 64bit also.  It still is 32bit on 32bit platforms, though, so it doesn't remove the platform issue fully.  Just hopefully the worst caveat.\r\n\r\nI don't have an opinion on always printing it.  But since we hide it, having an option in the printoptions for it seems very reasonable to me.  (Not sure I think there is much reason to always hide it.)"],"labels":["01 - Enhancement"]},{"title":"BUG: f2py appears to ignore Fortran compiler flags","body":"### Describe the issue:\n\nI'm collaborating on a code that compiles some fixed-form Fortran subroutines using `f2py`. My collaborators are using Python 3.11 and earlier, whereas I'm failing to build the subroutines on Python 3.12, presumably because I have to use the Meson backend (which I installed through my package manager).\r\n\r\nSpecifically, an example subroutine fails to build with `gfortran` without the `-ffixed-form` flag. But even if I pass this to `f2py`, I get the same error that `gfortran` gives me *without* the flag, as if it's not being passed.\r\n\r\nI'm not 100% sure how all these form fields are going to render in the issue, but taking the Fortran source below as `mwe.f95`, I get (`$` represents the terminal prompt):\r\n```\r\n$ gfortran -c mwe.f95 \r\nmwe.f95:1:22:\r\n\r\n    1 |       subroutine test(\r\n      |                      1\r\nError: Invalid character in name at (1)\r\nmwe.f95:2:7:\r\n\r\n    2 |      & x)\r\n      |       1\r\nError: Invalid character in name at (1)\r\nmwe.f95:4:9:\r\n\r\n    4 |       end subroutine test\r\n      |         1\r\nError: Expecting END PROGRAM statement at (1)\r\nf951: Error: Unexpected end of file in \u2018mwe.f95\u2019\r\n$ gfortran -ffixed-form -c mwe.f95  # no output\r\n$ f2py -c --f77flags=-ffixed-form --f90flags=-ffixed-form -m mwe mwe.f95\r\n< full error in the appropriate field but relevant detail is... >\r\n...\r\n\r\n[3\/6] Compiling Fortran object mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o\r\nFAILED: mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o \r\ngfortran -Imwe.cpython-312-x86_64-linux-gnu.so.p -I. -I.. -I\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/core\/include -I\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/src -I\/usr\/include\/python3.12 -fvisibility=hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -O3 -fPIC -Jmwe.cpython-312-x86_64-linux-gnu.so.p -o mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o -c ..\/mwe.f95\r\n..\/mwe.f95:1:22:\r\n\r\n    1 |       subroutine test(\r\n      |                      1\r\nError: Invalid character in name at (1)\r\n..\/mwe.f95:2:7:\r\n\r\n    2 |      & x)\r\n      |       1\r\nError: Invalid character in name at (1)\r\n..\/mwe.f95:4:9:\r\n\r\n    4 |       end subroutine test\r\n      |         1\r\nError: Expecting END PROGRAM statement at (1)\r\nf951: Error: Unexpected end of file in \u2018..\/mwe.f95\u2019\r\n...\r\nsubprocess.CalledProcessError: Command '['meson', 'compile', '-C', 'bbdir']' returned non-zero exit status 1.\r\n```\r\nNote that the build succeeds if I change the source file extension to `.f`, perhaps because the fixed-form is inferred.\n\n### Reproduce the code example:\n\n```python\n! mwe.f95\r\n      subroutine test(\r\n     & x)\r\n          real*8 x\r\n      end subroutine test\n```\n\n\n### Error message:\n\n```shell\nCannot use distutils backend with Python>=3.12, using meson backend instead.\r\nUsing meson backend\r\nWill pass --lower to f2py\r\nSee https:\/\/numpy.org\/doc\/stable\/f2py\/buildtools\/meson.html\r\nReading fortran codes...\r\n\tReading file 'mwe.f95' (format:free)\r\nLine #1 in mwe.f95:\"      subroutine test( \"\r\n\tanalyzeline: No name\/args pattern found for line.\r\nPost-processing...\r\n\tBlock: mwe\r\n\t\t\tBlock: unknown_subroutine\r\nApplying post-processing hooks...\r\n  character_backward_compatibility_hook\r\nPost-processing (stage 2)...\r\nBuilding modules...\r\n    Building module \"mwe\"...\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"mwe-f2pywrappers.f\"\r\n        Constructing wrapper function \"unknown_subroutine\"...\r\n          unknown_subroutine()\r\n    Wrote C\/API module \"mwe\" to file \".\/mwemodule.c\"\r\nThe Meson build system\r\nVersion: 1.3.1\r\nSource dir: \/tmp\/tmpjodywqy7\r\nBuild dir: \/tmp\/tmpjodywqy7\/bbdir\r\nBuild type: native build\r\nProject name: mwe\r\nProject version: 0.1\r\nFortran compiler for the host machine: gfortran (gcc 13.2.1 \"GNU Fortran (GCC) 13.2.1 20231205 (Red Hat 13.2.1-6)\")\r\nFortran linker for the host machine: gfortran ld.bfd 2.40-14\r\nC compiler for the host machine: cc (gcc 13.2.1 \"cc (GCC) 13.2.1 20231205 (Red Hat 13.2.1-6)\")\r\nC linker for the host machine: cc ld.bfd 2.40-14\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram \/usr\/bin\/python3 found: YES (\/usr\/bin\/python3)\r\nFound pkg-config: YES (\/usr\/bin\/pkg-config) 1.9.5\r\nRun-time dependency python found: YES 3.12\r\nLibrary quadmath found: YES\r\nBuild targets in project: 1\r\n\r\nFound ninja-1.11.1 at \/usr\/bin\/ninja\r\nINFO: autodetecting backend as ninja                                                                                                                                                          \r\nINFO: calculating backend command to run: \/usr\/bin\/ninja -C \/tmp\/tmpjodywqy7\/bbdir\r\nninja: Entering directory `\/tmp\/tmpjodywqy7\/bbdir'\r\n[3\/6] Compiling Fortran object mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o\r\nFAILED: mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o \r\ngfortran -Imwe.cpython-312-x86_64-linux-gnu.so.p -I. -I.. -I\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/core\/include -I\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/src -I\/usr\/include\/python3.12 -fvisibility=hidden -fdiagnostics-color=always -D_FILE_OFFSET_BITS=64 -Wall -O3 -fPIC -Jmwe.cpython-312-x86_64-linux-gnu.so.p -o mwe.cpython-312-x86_64-linux-gnu.so.p\/mwe.f95.o -c ..\/mwe.f95\r\n..\/mwe.f95:1:22:\r\n\r\n    1 |       subroutine test(\r\n      |                      1\r\nError: Invalid character in name at (1)\r\n..\/mwe.f95:2:7:\r\n\r\n    2 |      & x)\r\n      |       1\r\nError: Invalid character in name at (1)\r\n..\/mwe.f95:4:9:\r\n\r\n    4 |       end subroutine test\r\n      |         1\r\nError: Expecting END PROGRAM statement at (1)\r\nf951: Error: Unexpected end of file in \u2018..\/mwe.f95\u2019\r\n[5\/6] Compiling C object mwe.cpython-312-x86_64-linux-gnu.so.p\/15b943306aebd8c2a7f428d911374b526ccafd56_.._.._f2py_src_fortranobject.c.o\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"\/usr\/bin\/f2py\", line 33, in <module>\r\n    sys.exit(load_entry_point('numpy==1.24.4', 'console_scripts', 'f2py')())\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/f2py2e.py\", line 766, in main\r\n    run_compile()\r\n  File \"\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/f2py2e.py\", line 738, in run_compile\r\n    builder.compile()\r\n  File \"\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/_backends\/_meson.py\", line 178, in compile\r\n    self.run_meson(self.build_dir)\r\n  File \"\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/_backends\/_meson.py\", line 173, in run_meson\r\n    self._run_subprocess_command(compile_command, build_dir)\r\n  File \"\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy\/f2py\/_backends\/_meson.py\", line 167, in _run_subprocess_command\r\n    subprocess.run(command, cwd=cwd, check=True)\r\n  File \"\/usr\/lib64\/python3.12\/subprocess.py\", line 571, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['meson', 'compile', '-C', 'bbdir']' returned non-zero exit status 1.\n```\n\n\n### Python and NumPy Versions:\n\n1.26.3\r\n3.12.1 (main, Dec 18 2023, 00:00:00) [GCC 13.2.1 20231205 (Red Hat 13.2.1-6)]\r\n\n\n### Runtime Environment:\n\n[{'numpy_version': '1.26.3',\r\n  'python': '3.12.1 (main, Dec 18 2023, 00:00:00) [GCC 13.2.1 20231205 (Red '\r\n            'Hat 13.2.1-6)]',\r\n  'uname': uname_result(system='Linux', node='localhost.localdomain', release='6.7.3-200.fc39.x86_64', version='#1 SMP PREEMPT_DYNAMIC Thu Feb  1 03:29:52 UTC 2024', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/wball\/.local\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 4,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'},\r\n {'filepath': '\/usr\/lib64\/libgomp.so.1.0.0',\r\n  'internal_api': 'openmp',\r\n  'num_threads': 4,\r\n  'prefix': 'libgomp',\r\n  'user_api': 'openmp',\r\n  'version': None}]\r\nNone\r\n\n\n### Context for the issue:\n\nThis appears to be a regression in `f2py`, in that something that was working under defaults in Python 3.11 (with distutils) appears to broken in Python 3.12 (with Meson). I suspect this is actually a Meson issue but I have limited experience with it.\r\n\r\nI have access to a few different systems to try a few tests. If the intention isn't to support what I'm trying, it should be documented. I couldn't (at least in the ~20 minutes I spent paging through) find anything relevant in the [f2py docs](https:\/\/numpy.org\/doc\/stable\/f2py\/index.html).","comments":[],"labels":["00 - Bug"]},{"title":"ENH: Convert fp32 sin\/cos from C universal intrinsics to C++ using Highway","body":"This patch is to experiment with highway and see how we can leverage its intrinsics using static dispatch. I would think these are the minimum requirements: \r\n\r\n- [ ]  passes CI on all the relevant platforms:   AVX512_SKX, [AVX2, FMA3], VSX4, VSX3, VSX2, NEON_VFPV4, VXE2, VX. All tests pass on my local AVX512 and AVX2 machines. \r\n- [ ] No performance regressions. \r\n\r\nOn x86, both AVX-512 and AVX2 seem to have performance regressions. Yet to figure out where they are coming from. \r\n\r\n### AVX-512 benchmarks\r\n\r\nThese are about 1.5x slower even when built with `-march=skylake-avx512`. If we use just `-mavx512f -mavx512bw`, etc, then its about 4x slower. \r\n\r\n```\r\n| Change   | Before [5867ee6b] <main>   | After [d5596c17] <sincos-hwy>   |   Ratio | Benchmark (Parameter)                                                   |\r\n|----------|----------------------------|---------------------------------|---------|-------------------------------------------------------------------------|\r\n| +        | 7.48\u00b10\u03bcs                   | 11.5\u00b10.01\u03bcs                     |    1.53 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 1, 'f')        |\r\n| +        | 7.70\u00b10.06\u03bcs                | 11.5\u00b10.02\u03bcs                     |    1.49 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 1, 'f')        |\r\n| +        | 11.8\u00b10.1\u03bcs                 | 16.3\u00b10.04\u03bcs                     |    1.39 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 2, 'f')        |\r\n| +        | 11.9\u00b10.1\u03bcs                 | 16.3\u00b10.09\u03bcs                     |    1.38 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 2, 'f')        |\r\n| +        | 23.2\u00b10.06\u03bcs                | 28.0\u00b10.01\u03bcs                     |    1.21 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 4, 2, 'f')        |\r\n| +        | 23.3\u00b10.2\u03bcs                 | 28.1\u00b10.01\u03bcs                     |    1.21 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 4, 2, 'f')        |\r\n| +        | 20.4\u00b10.02\u03bcs                | 24.4\u00b10.09\u03bcs                     |    1.2  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 4, 1, 'f')        |\r\n| +        | 20.3\u00b10.01\u03bcs                | 24.5\u00b10.02\u03bcs                     |    1.2  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 4, 1, 'f')        |\r\n| -        | 66.1\u00b10.02\u03bcs                | 62.2\u00b10.07\u03bcs                     |    0.94 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'cos'>, 4, 2, 'f') |\r\n| -        | 67.2\u00b10.01\u03bcs                | 62.2\u00b10.03\u03bcs                     |    0.93 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'sin'>, 4, 2, 'f') |\r\n| -        | 64.6\u00b10.03\u03bcs                | 59.1\u00b10.01\u03bcs                     |    0.91 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'cos'>, 4, 1, 'f') |\r\n| -        | 65.2\u00b10.02\u03bcs                | 59.1\u00b10.04\u03bcs                     |    0.91 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'sin'>, 4, 1, 'f') |\r\n| -        | 60.6\u00b10.04\u03bcs                | 54.4\u00b10.02\u03bcs                     |    0.9  | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'cos'>, 1, 2, 'f') |\r\n| -        | 59.0\u00b10.05\u03bcs                | 51.6\u00b10.04\u03bcs                     |    0.88 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'cos'>, 1, 1, 'f') |\r\n| -        | 61.5\u00b10.03\u03bcs                | 54.1\u00b10.06\u03bcs                     |    0.88 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'sin'>, 1, 2, 'f') |\r\n| -        | 59.9\u00b10.01\u03bcs                | 51.8\u00b10.05\u03bcs                     |    0.86 | bench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'sin'>, 1, 1, 'f') |\r\n```\r\n\r\n### AVX2 benchmarks\r\n\r\nThese are about 1.34x slower when built using `-march=skylake`. If we use `-maxv2` or even `-march=haswell`, then these seem to be 4x slower. \r\n\r\n```\r\n| Change   | Before [5867ee6b] <main>   | After [d5596c17] <sincos-hwy>   |   Ratio | Benchmark (Parameter)                                            |\r\n|----------|----------------------------|---------------------------------|---------|------------------------------------------------------------------|\r\n| +        | 12.7\u00b10.1\u03bcs                 | 17.1\u00b10.1\u03bcs                      |    1.34 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 1, 'f') |\r\n| +        | 13.1\u00b10.01\u03bcs                | 17.2\u00b10.2\u03bcs                      |    1.31 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 1, 'f') |\r\n| +        | 40.5\u00b10.02\u03bcs                | 49.7\u00b10.2\u03bcs                      |    1.23 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 4, 2, 'f') |\r\n| +        | 36.7\u00b10.2\u03bcs                 | 45.1\u00b10.2\u03bcs                      |    1.23 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 4, 1, 'f') |\r\n| +        | 37.2\u00b10.04\u03bcs                | 45.2\u00b10.4\u03bcs                      |    1.22 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 4, 1, 'f') |\r\n| +        | 40.3\u00b10.01\u03bcs                | 49.3\u00b10.2\u03bcs                      |    1.22 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 4, 2, 'f') |\r\n| +        | 18.5\u00b10.1\u03bcs                 | 22.2\u00b10.2\u03bcs                      |    1.2  | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 2, 'f') |\r\n| +        | 18.5\u00b10.02\u03bcs                | 21.8\u00b10.2\u03bcs                      |    1.18 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 2, 'f') |\r\n```","comments":["cc @jan-wassenberg ","Ok, so I played a bit of spot the difference and left some comments, and I ran some benchmarks quickly - it looks like with ASIMD, there's regressions with these:\r\n\r\n```\r\nbench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 4, 2, 'f')\r\nbench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 4, 2, 'f')\r\nbench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'sin'>, 4, 2, 'f')\r\nbench_ufunc_strides.UnaryFPSpecial.time_unary(<ufunc 'cos'>, 4, 2, 'f')\r\nbench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 2, 'f')\r\n```\r\n\r\nThis indicates the gather\/scatter aren't as optimal as the NumPy ones; I wonder if we can blend the NumPy loads and stores with the Highway code here \ud83e\udd14 ",">> This indicates the gather\/scatter aren't as optimal as the NumPy ones; I wonder if we can blend the NumPy loads and stores with the Highway code here \ud83e\udd14\r\n\r\nThat is extremely likely. My Gather\/Scatter were just a quick and dirty way to make this work. I will eventually move to using the highway implementation. ","Please explain to me, where did this conclusion come from? the current speed-up is related to special cases (the libm fallback has been improved somehow) which affects both contiguous and non-contiguous. So I think maybe the regression is related to it.\r\n\r\n","@seiko2plus I am seeing slowdown for strided cases as well. I just meant this could be a result of my `GatherIndexN` and `ScatterIndexN `functions which just perform a simple scalar loop to load and store. It's only a guess, I will need to take a deeper look into it. \r\n\r\n```\r\n| +        | 11.8\u00b10.1\u03bcs                 | 16.3\u00b10.04\u03bcs                     |    1.39 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 2, 'f')        |\r\n| +        | 11.9\u00b10.1\u03bcs                 | 16.3\u00b10.09\u03bcs                     |    1.38 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 2, 'f')   \r\n```","> -march=skylake-avx512. If we use just -mavx512f -mavx512bw, etc, then its about 4x slower.\r\n\r\nRight, Highway checks for multiple CPU flags before using AVX3. -march=skx is sufficient here, but for the individual -mavx512, that would require a long list.\r\n","Moving the `hn::StoreMaskBits` to inside the if condition helped perf by a little bit, now we are just about 1.2x slower. \r\n\r\n```\r\n| +        | 7.47\u00b10.01\u03bcs                | 9.11\u00b10.05\u03bcs                     |    1.22 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 1, 'f')        |\r\n| +        | 7.68\u00b10.03\u03bcs                | 9.32\u00b10.04\u03bcs                     |    1.21 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 1, 'f')        |\r\n\r\n```","> Moving the `hn::StoreMaskBits` to inside the if condition helped perf by a little bit, now we are just about 1.2x slower.\r\n> \r\n> ```\r\n> | +        | 7.47\u00b10.01\u03bcs                | 9.11\u00b10.05\u03bcs                     |    1.22 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'sin'>, 1, 1, 'f')        |\r\n> | +        | 7.68\u00b10.03\u03bcs                | 9.32\u00b10.04\u03bcs                     |    1.21 | bench_ufunc_strides.UnaryFP.time_unary(<ufunc 'cos'>, 1, 1, 'f')        |\r\n> ```\r\n\r\nCool, I have added some additional suggestions :)","@jan-wassenberg just looking at the CI failures and it seems there's some compiler incompatibility on PPC (https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8087779139\/job\/22100478961?pr=25781#step:8:504) and an abort on Z13 (https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8087779139\/job\/22100479504?pr=25781) any ideas? \r\n\r\nThere's also a failure on `armhf` but I can take a look at that when I get a minute (https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8087779139\/job\/22100478151?pr=25781)","hm, the Z13 error is `realloc(): invalid next size\r\nFatal Python error: Aborted`\r\nSeems unrelated to SIMD; are we possibly corrupting the heap?\r\n\r\nThe latter at least I can help with. We are missing HWY_ATTR:\r\n\r\n> Additionally, each function that calls Highway ops (such as Load) must either be prefixed with HWY_ATTR, OR reside between HWY_BEFORE_NAMESPACE() and HWY_AFTER_NAMESPACE(). Lambda functions currently require HWY_ATTR before their opening brace.","> The latter at least I can help with. We are missing HWY_ATTR:\r\n\r\nAdding HWY_ATTR fixed the build errors on ppc64le. Why did it fail only for this platform though? \r\n\r\nNeed help with debugging 3 more failures:  \r\n \r\n1. The [crash ](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8366054700\/job\/22905373846?pr=25781) on s390x platforms. Ping @seiko2plus \r\n2. armhf failure: looks like accuracy problems ping @Mousius \r\n3. @jan-wassenberg any idea why the cygwin CI fails with this error? \r\n\r\n```\r\n..\/numpy\/_core\/src\/highway\/hwy\/ops\/generic_ops-inl.h: In function 'void hwy::N_AVX2::StoreInterleaved3(hwy::N_AVX2::VFromD<D>, hwy::N_AVX2::VFromD<D>, hwy::N_AVX2::VFromD<D>, D, hwy::N_AVX2::TFromD<D>*)':\r\n..\/numpy\/_core\/src\/highway\/hwy\/ops\/generic_ops-inl.h:1320:14: error: expected unqualified-id before numeric constant\r\n 1320 |   const auto B0 = TableLookupBytesOr0(v0, shuf_B0);\r\n```\r\n\r\n\r\n\r\n\r\n","hm, strange. Neither the x86 implementation of TableLookupBytesOr0, nor the quoted line and the one before it, have a numeric constant. Which compiler is cygwin using?"],"labels":["01 - Enhancement"]},{"title":"DOC: clarification about what SeedSequence can and cannot do","body":"### Issue with current documentation:\r\n\r\nI want to be sure that what SeedSequence can and cannot do is clear from the [doc](https:\/\/numpy.org\/doc\/stable\/reference\/random\/parallel.html#seedsequence-spawning). What I get from the doc is:\r\n1. SeedSequence can create a good initial state even though I use 0 as my initial seed. This means that even if I use 0 as my seed I will not get any degenerate behavior from my RNG and the subsequent generated numbers will appear random and independent. Is this what is meant by SeedSequence can \"ensure that low-quality seeds are turned into high quality initial states (at least, with very high probability).\"?\r\n2. The issue with using small seeds is that this is not covering the initial state space (even with SeedSequence) and therefore we might end up having people running studies with the same initial state which would create biases? So this is mainly to ensure that everyone is unique number?\r\n\r\n### Idea or request for content:\r\n\r\nMaybe clarify what is meant by SeedSequence can \"ensure that low-quality seeds are turned into high quality initial states (at least, with very high probability)\", as it is also asked to provide large unique numbers for seeds. ","comments":["1. Yes, that is the understanding we intended to convey. We go into detail about the problem being avoided in the subsequent paragraph. Without `SeedSequence` or some other conditioning of the seed (for instance, `RandomState` was already fine in this respect), `0` or `12345` would have made a bad _single_ instance of `MT19937` because of its zero-land properties. Similarly, `0x987...blahmanybitsblah...12345` and `0x987...blahmanybitsblah...12346` would have made a bad single _pair_ of streams for any PRNG except for the weak-crypto ones like `Philox`.\r\n\r\n2. The _remaining_ issue with small seeds is twofold. First, people don't pull them randomly, even from the set of small integers. Instead, they use convenience seeds like `0`, `10`, `42`, `12345`, `31415926`. These get repeated a lot.\r\n\r\nSecond, even if they did pull randomly from the set of small 32-bit integers and everyone got a unique 32-bit number, the space of 32-bit integers is small relative to the state spaces of modern PRNGs. Even without exact duplicates, [there are biases](https:\/\/www.pcg-random.org\/posts\/cpp-seeding-surprises.html#bias) with any particular 4294967296-sized subset of that state space. With `SeedSequence` and our recommended practice of `secrets.randbits(128)`, we still have a default bottleneck of 128 bits, which is still smaller than our state spaces, but 128 bits is big enough that I don't care much about the remaining bias (and 128 bits is about the most I think I can get away with forcing people to copy-paste).\r\n\r\nItem 2 is entirely orthogonal to Item 1. Our recommendation for big seeds is up in the Quick Start, where I don't really want to overwhelm people with too many details. We discuss Item 1 in the design section about `SeedSequence` where we can afford to go into details, but because the two issues are fairly orthogonal, I would not have thought to do so there. But I can see reading that discussion and coming away with the impression that `SeedSequence` dealt with _all_ of the issues with small seeds. A small following note to say that we still recommend `secrets.randbits(128)` seeds for community practice reasons would probably be reasonable.","Thanks a lot @rkern for the explanation this is very clear! Yes I guess that for most users there is no need to get into that much details. I had overlooked the biases one would get with seeds smaller than the state space and hence the additional reason for having large seeds."],"labels":["04 - Documentation"]},{"title":"BUG: f2py module allocatable array example gives two compile warnings","body":"### Describe the issue:\r\n\r\nThe example for 'basic support' for f90 module allocatable arrays, as quoted in the `f2py` documentation, gives two compile warnings. Use of the compiled .so file as described further in the same documentation works fine to me, without errors.\r\n\r\nonline version of f2py doc: https:\/\/numpy.org\/doc\/stable\/f2py\/python-usage.html#allocatable-arrays\r\nRelated issue: https:\/\/github.com\/numpy\/numpy\/issues\/19157\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n! fortran module code as given at https:\/\/numpy.org\/doc\/stable\/f2py\/python-usage.html#allocatable-arrays\r\nmodule mod\r\n  real, allocatable, dimension(:,:) :: b \r\ncontains\r\n  subroutine foo\r\n    integer k\r\n    if (allocated(b)) then\r\n       print*, \"b=[\"\r\n       do k = 1,size(b,1)\r\n          print*, b(k,1:size(b,2))\r\n       enddo\r\n       print*, \"]\"\r\n    else\r\n       print*, \"b is not allocated\"\r\n    endif\r\n  end subroutine foo\r\nend module mod\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n$ f2py -c -m allocarr allocarr.f90\r\nrunning build\r\nrunning config_cc\r\nINFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\r\nrunning config_fc\r\nINFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\r\nrunning build_src\r\nINFO: build_src\r\nINFO: building extension \"allocarr\" sources\r\nINFO: f2py options: []\r\nINFO: f2py:> \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.c\r\ncreating \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\r\nReading fortran codes...\r\n        Reading file 'allocarr.f90' (format:free)\r\nPost-processing...\r\n        Block: allocarr\r\n                        Block: mod\r\n                                Block: foo\r\nApplying post-processing hooks...\r\n  character_backward_compatibility_hook\r\nPost-processing (stage 2)...\r\n        Block: allocarr\r\n                Block: unknown_interface\r\n                        Block: mod\r\n                                Block: foo\r\nBuilding modules...\r\n    Building module \"allocarr\"...\r\n                Constructing F90 module support for \"mod\"...\r\n                  Variables: b\r\ngetarrdims:warning: assumed shape array, using 0 instead of ':'\r\ngetarrdims:warning: assumed shape array, using 0 instead of ':'\r\n            Constructing wrapper function \"mod.foo\"...\r\n              foo()\r\n    Wrote C\/API module \"allocarr\" to file \"\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.c\"\r\n    Fortran 90 wrappers are saved to \"\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarr-f2pywrappers2.f90\"\r\nINFO:   adding '\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/fortranobject.c' to sources.\r\nINFO:   adding '\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11' to include_dirs.\r\ncopying \/usr\/lib\/python3\/dist-packages\/numpy\/f2py\/src\/fortranobject.c -> \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\r\ncopying \/usr\/lib\/python3\/dist-packages\/numpy\/f2py\/src\/fortranobject.h -> \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\r\nINFO:   adding '\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarr-f2pywrappers2.f90' to sources.\r\nINFO: build_src: building npy-pkg config files\r\n\/usr\/lib\/python3\/dist-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n  warnings.warn(\r\nrunning build_ext\r\nINFO: customize UnixCCompiler\r\nINFO: customize UnixCCompiler using build_ext\r\nINFO: get_default_fcompiler: matching types: '['arm', 'gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor', 'fujitsu']'\r\nINFO: customize ArmFlangCompiler\r\nWARN: Could not locate executable armflang\r\nINFO: customize Gnu95FCompiler\r\nINFO: Found executable \/usr\/bin\/gfortran\r\nINFO: customize Gnu95FCompiler\r\nINFO: customize Gnu95FCompiler using build_ext\r\nINFO: building 'allocarr' extension\r\nINFO: compiling C sources\r\nINFO: C compiler: x86_64-linux-gnu-gcc -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC\r\n\r\ncreating \/tmp\/tmp2q0rmg2g\/tmp\r\ncreating \/tmp\/tmp2q0rmg2g\/tmp\/tmp2q0rmg2g\r\ncreating \/tmp\/tmp2q0rmg2g\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\r\nINFO: compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11 -I\/usr\/lib\/python3\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python3.11 -c'\r\nINFO: x86_64-linux-gnu-gcc: \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.c\r\nINFO: x86_64-linux-gnu-gcc: \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/fortranobject.c\r\n\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.c: In function \u2018f2py_setup_mod\u2019:\r\n\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.c:186:31: warning: assignment to \u2018f2py_init_func\u2019 {aka \u2018void (*)(int *, long int *, void (*)(char *, long int *), int *)\u2019} from incompatible pointer type \u2018void (*)(int *, int *, void (*)(char *, int *), int *)\u2019 [-Wincompatible-pointer-types]\r\n  186 |   f2py_mod_def[i_f2py++].func = b;\r\n      |                               ^\r\nINFO: compiling Fortran 90 module sources\r\nINFO: Fortran f77 compiler: \/usr\/bin\/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops\r\nFortran f90 compiler: \/usr\/bin\/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\r\nFortran fix compiler: \/usr\/bin\/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\r\nINFO: compile options: '-I\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11 -I\/usr\/lib\/python3\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python3.11 -c'\r\nextra options: '-J\/tmp\/tmp2q0rmg2g\/ -I\/tmp\/tmp2q0rmg2g\/'\r\nINFO: gfortran:f90: allocarr.f90\r\nINFO: compiling Fortran sources\r\nINFO: Fortran f77 compiler: \/usr\/bin\/gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -O3 -funroll-loops\r\nFortran f90 compiler: \/usr\/bin\/gfortran -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\r\nFortran fix compiler: \/usr\/bin\/gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -O3 -funroll-loops\r\nINFO: compile options: '-I\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11 -I\/usr\/lib\/python3\/dist-packages\/numpy\/core\/include -I\/usr\/include\/python3.11 -c'\r\nextra options: '-J\/tmp\/tmp2q0rmg2g\/ -I\/tmp\/tmp2q0rmg2g\/'\r\nINFO: gfortran:f90: \/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarr-f2pywrappers2.f90\r\n\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarr-f2pywrappers2.f90:37:10:\r\n\r\n   37 |       use mod, only : b\r\n      |          1\r\nWarning: Unused module variable \u2018b\u2019 which has been explicitly imported at (1) [-Wunused-variable]\r\nINFO: \/usr\/bin\/gfortran -Wall -g -Wall -g -shared \/tmp\/tmp2q0rmg2g\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarrmodule.o \/tmp\/tmp2q0rmg2g\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/fortranobject.o \/tmp\/tmp2q0rmg2g\/allocarr.o \/tmp\/tmp2q0rmg2g\/tmp\/tmp2q0rmg2g\/src.linux-x86_64-3.11\/allocarr-f2pywrappers2.o -L\/usr\/lib\/gcc\/x86_64-linux-gnu\/12 -L\/usr\/lib\/gcc\/x86_64-linux-gnu\/12 -L\/usr\/lib\/x86_64-linux-gnu -lgfortran -o .\/allocarr.cpython-311-x86_64-linux-gnu.so\r\nRemoving build directory \/tmp\/tmp2q0rmg2g\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\nVersions including Debian\/bookworm patch IDs:\r\n````\r\npython3-numpy                          1:1.24.2-1                             amd64\r\npython3                                3.11.2-1+b1                            amd64\r\ngfortran-12                            12.2.0-14                              amd64\r\ngcc-12                                 12.2.0-14                              amd64\r\n````\r\n\r\n### Speculation:\r\n\r\nIt looks like the first warning is an issue of a mismatch between `long int` on the python side and `int` on the fortran side.\r\n\r\nThe second warning is probably normally a feature, but in this situation invalid.","comments":[],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"DOC: Array interface protocol does not recognize bit field typestr and could mention AttributeError use","body":"### Describe the issue:\r\n\r\nI am trying to implement the array interface protocol for Polars Series. Polars represents its boolean arrays (and validity buffers) as bit-packed booleans, e.g. 8 booleans per byte. I figured I would need to specify a bit field (e.g. `t` type) to do this, however, it seems numpy does not recognize any way of defining a bit field. I've tried various combinations (with `descr` too) to get this to work, but I can't figure it out.\r\n\r\nCould it be that NumPy does not handle bit field types correctly?\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\n\r\nclass wrapper:\r\n    pass\r\n\r\n\r\ninterface = {\r\n    \"shape\": (3,),\r\n    \"typestr\": \"|t1\",\r\n    \"version\": 3,\r\n}\r\nwrapper.__array_interface__ = interface\r\n\r\nresult = np.array(wrapper, copy=False)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"\/home\/stijn\/code\/polars\/py-polars\/repro.py\", line 19, in <module>\r\n    result = np.array(wrapper, copy=False)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: data type '|t1' not understood\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n1.26.3\r\n3.12.0 (main, Nov 16 2023, 08:47:32) [GCC 11.4.0]\r\n\r\n### Runtime Environment:\r\n\r\n[{'numpy_version': '1.26.3',\r\n  'python': '3.12.0 (main, Nov 16 2023, 08:47:32) [GCC 11.4.0]',\r\n  'uname': uname_result(system='Linux', node='frosty', release='6.1.0-1029-oem', version='#29-Ubuntu SMP PREEMPT_DYNAMIC Tue Jan  9 21:07:34 UTC 2024', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Prescott',\r\n  'filepath': '\/home\/stijn\/code\/polars\/.venv\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n\r\n### Context for the issue:\r\n\r\nI am trying to improve interoperability between the Polars dataframe library and NumPy.","comments":["The buffer protocol is defined by Python, not numpy, and it has a fixed [enumerated list](https:\/\/docs.python.org\/3\/library\/struct.html#format-characters) of types it supports (yeah that's the struct module docs, but the buffer protocol docs link there...). Unfortunately bitfields aren't one of the supported types. Neither are datetimes, which is one reason why cython doesn't support typed memoryviews for datetimes.\r\n\r\nThis seems to be coming up a lot and we've asked upstream if there's any appetite to expand the buffer protocol to allow defining extensible types. See https:\/\/discuss.python.org\/t\/buffer-protocol-and-arbitrary-data-types\/26256. So far there haven't been any takers and some have expressed the opinion that numpy, cython, etc should add support for the arrow in-memory interface instead.","Thanks for the reply!\r\n\r\nApologies for messing up the wording: I meant to refer to the NumPy array interface protocol defined here:\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/arrays.interface.html\r\n\r\nThe NumPy docs here clearly define a \"bit field\", which I figured could be used for bit-packed booleans. However, I cannot get it to work at all.","NumPy doesn't have build in bitfield support at this time and it would need to be padded to full bytes to be compatible with the array ABI.\r\nI am not sure why it is documented, I guess someone had ambitions a long time ago.","Thanks for the clarification. In that case, I think the logic for checking the `__array_interface__` should be updated. It should not fail on unrecognized type strings - rather, it should fall back to using `__array__()`. From the [docs](https:\/\/numpy.org\/devdocs\/user\/basics.interoperability.html#using-arbitrary-objects-in-numpy):\r\n\r\n> For both the buffer and the `__array_interface__` protocols, the object describes its memory layout and NumPy does everything else (zero-copy if possible). If that\u2019s not possible, the object itself is responsible for returning a ndarray from `__array__()`.\r\n\r\nCurrently, the step that checks the array interface fails on the unrecognized bit types, and it does not fall back to the `__array__()` method:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport polars as pl\r\n\r\ns = pl.Series([True, False])\r\n\r\narr = np.asarray(s)\r\nprint(arr)\r\n# TypeError: data type '|t1' not understood\r\n```\r\n\r\nThis means we currently cannot implement the `__array_protocol__` as it gets in the way of the other interop implementation (`__array__`).","You *can* actually implement `__array_interface__`, you just have to make sure it is a property and raise an `AttributeError` when NumPy can't do it.\r\nI can be convinced that NumPy should fall back (ideally bribed with a PR), but I do think it risks hiding bugs when the typestring is invalid due to an actual problem.","Ah, good to know that I can raise an AttributeError to force the fallback. Then I have what I need for now!\r\n\r\nI'll leave it up to you whether this issue should stay open or not.\r\n\r\nI would say at least the docs should be updated to remove the bit field entry. And maybe it should be explicitly stated in the interop docs that the fallback only happens when an AttributeError is raised (if that is indeed the case)."],"labels":["04 - Documentation"]},{"title":"DOC: documentation for `invert` is incorrect, as it does not return two's complement for signed integers","body":"### Issue with current documentation:\r\n\r\nThe documentation for [`invert`](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.invert.html) states (first sentence):\r\n> Compute bit-wise inversion, or bit-wise NOT, element-wise.\r\n\r\nTwo paragraphs down, it states:\r\n> For signed integer inputs, the two\u2019s complement is returned.\r\n\r\nThese are not equivalent things (going by the common definition of two's complement, as found on e.g. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Two%27s_complement). A test shows the bit-wise NOT is returned, not the two's complement.\r\n\r\nThe notes even mention\r\n> numpy.bitwise_not is an alias for [invert](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.invert.html#numpy.invert):\r\n>\r\n>     np.bitwise_not is np.invert\r\n>     True\r\n\r\nwhich makes it even clearer. \r\n\r\nIt's likely a result of a mistaken thought of equivalency.\r\n\r\nThe issue was brought to light at [this StackOverflow question](https:\/\/stackoverflow.com\/questions\/77925955\/unexpected-behavior-of-numpy-invert\/77926058#77926058).\r\n\r\n\r\nNote that the example section also mentions two's complement, but differently:\r\n> When using signed integer types the result is the two\u2019s complement of the result for the unsigned type:\r\n\r\nPerhaps this is what actually is meant, but I find it confusing to read (\"the two's complement of the result for the unsigned type\" implies the two's complement of the unsigned type was taken. I guess it means something like the two's complement of the bit-wise NOT of the unsigned type, after converting the that bit-wise NOT result to a signed type?)","comments":["Whoa, that is old documentation. I suspect that function is not much used."],"labels":["04 - Documentation"]},{"title":"ENH: Reduce size of distributed wheels","body":"### Proposed new feature or change:\n\n- OpenBLAS wheels\r\n  Reduction: %\r\n  Related issue:\r\n- Random \/ Cython\r\n  Reduction: %\r\n  Related issue:\r\n- Increase minimum SIMD architecture\r\n  Reduction: %\r\n  Related issue:","comments":["`libopenblas64_.so` is by far the biggest fish here. For a regular x86-64 manylinux wheel, we're at 14.8 MB for numpy 1.26.2, and >50% of that is in `numpy.libs`.\r\n\r\nIf you build a wheel without vendored libopenblas (through `python -m build`), then it's about 7.8 MB. If you unzip it, you find the following:\r\n- total unzipped size: 28.9 MB\r\n- `_multiarray_umath.so`: 8.2 MB\r\n- `_simd.so`: 3.4 MB\r\n- `_core\/tests\/`: 3.3 MB\r\n- all other `tests\/` combined: ~3.4 MB\r\n- `numpy\/random\/*.so`: 3.2 MB\r\n\r\nIt looks like separating out the tests could be the easiest and most significant win after separating out `libopenblas`. The two static libraries are only ~200 kb together so not that relevant. ","Would we make the tests optionally installable, or how would users validate that their NumPy installation is working correctly?","> Would we make the tests optionally installable, or how would users validate that their NumPy installation is working correctly?\r\n\r\nWe could publish a `numpy-tests` package indeed. And\/or a build-time switch like `-Dinclude-tests=true`.","So the idea would be that most \"only CI testing\" people would get a non-optimized NumPy without tests, and \"normal users\" would download OpenBLAS?","I don't see a realistic way to do that, one cannot distinguish between CI jobs and other regular uses. Also, it's not at all clear that most usage is for CI. It seems unlikely actually - all we know is that ~90% of jobs are for Linux. 200 million CI jobs\/month would be a lot. The distribution over Python versions, with 3.12 still very low 4 months after it was released, rather suggests that it's large-scale production deployments of applications and data pipelines:\r\n\r\n<img width=\"205\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/a26efc46-07c4-4b1d-9da4-2131c3b09366\">\r\n","Another alternative for the test suite, if we want it to be included by default, is to compress it.","I think I've figured out how to remove the tests for pandas, if you're interested.\r\n\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/pull\/53007 is my attempt.\r\n\r\nBasically my strategy is use ``.gitattributes`` to remove tests from the sdist (and use setuptools to build wheels for the tests separetely), then build the wheel from the sdist without tests.","Interesting, thanks for sharing @lithomas1. \r\n\r\nI'd be more inclined to keep tests in the sdist (distros need it after all) and use a build flag to allow removing tests from wheels. But I'm certainly not sure that that's the way to go before trying.\r\n\r\n> Another alternative for the test suite, if we want it to be included by default, is to compress it.\r\n\r\nThat could help a bit, but probably not all that much compared to not shipping them - wheels are already compressed after all (they're zip archives).","Yeap, sorry for the noise. Compression advantage of something like zstd over gz is only a few 100k.\r\n\r\nI wish wheels supported better compression. Looks like [they considered it](https:\/\/discuss.python.org\/t\/making-the-wheel-format-more-flexible-for-better-compression-speed\/3810\/74?page=3) but [never got zstd or similar into the standard lib](https:\/\/mail.python.org\/archives\/list\/python-ideas@python.org\/thread\/VQIFA7WTNRAOYZGTVP4WZC2CD36KYIVY\/)."],"labels":["03 - Maintenance"]},{"title":"Bring back `np.object` to be an alias to `np.object_` ","body":"This is a follow-up to gh-22021. `np.object` was removed several releases back. It is the nicer name though, and we should probably bring it back so `object_` and `object` are the same thing (and longer-term we can prefer `object`.\r\n\r\nIt is another alias in the namespace, but underscored names really are no good. Since `object`, while not our favorite dtype, is here to stay it deserves the obvious name without an underscore.\r\n\r\nWhen this is done, it'd be good to look at the PR that closed gh-22021 and do a similar thing (making the preferred name the actual one in the code, rather than the alias).","comments":[],"labels":["17 - Task"]},{"title":"BUG: Nanquantile doens't handle multidimensional `q` correctly","body":"Quantile\/percentile insert the dimensions of `q` at the beginning of the result.  Nanquantile seems not to be written to correctly support multi-dimensional `q` and ends up splitting it up:\r\n```python\r\na = np.ones((3, 4))\r\nq = np.full((1, 2), 0.5)\r\n \r\nprint(np.quantile(a, q, axis=1).shape)\r\n# (1, 2, 3)  correct\r\nprint(np.nanquantile(a, q, axis=1).shape)\r\n# (1, 3, 2) mangled up\r\n```\r\nThe axes contributed by `q` of shape `(1, 2)` should end up at the beginning but get split.\r\n\r\n(Reported by @aschaffer)","comments":["`nanquantile()` seems to insert `q` dimensions into the selected `axis`, instead of at the beginning, like `quantile\/percentile` do."],"labels":["00 - Bug","component: numpy.lib"]},{"title":"ENH: Should _ShapeType be covariant?","body":"### Proposed new feature or change:\r\n\r\nI believe [`_ShapeType`](https:\/\/github.com\/numpy\/numpy\/blob\/2a52c5778ceb5be4bdd7e54925d87f16f748f9d2\/numpy\/__init__.pyi#L1383) should be covariant.  For instance, the following code does not typecheck because `_ShapeType` is invariant:\r\n\r\n```python\r\nfrom typing import NewType\r\nimport numpy as np\r\nfrom numpy.typing import NBitBase\r\n\r\nTime = NewType(\"Time\", int)\r\nSeries = NewType(\"Series\", int)\r\nAnyFloat = np.dtype[np.floating[NBitBase]]\r\n\r\narr: np.ndarray[tuple[Time, Series], AnyFloat] = np.arange(4.0).reshape((2, 2))\r\n\r\ndef foo(a: np.ndarray[tuple[int, ...], AnyFloat]) -> None: ...\r\n\r\nfoo(arr)\r\n```\r\n\r\nI don't think there's any problems making it covariant, because I would assume it will be replaced by `TypeVarTuple` ([PEP 646](https:\/\/peps.python.org\/pep-0646\/)), and tuples are covariant.  In the interim, there's a few functions that change the shape of the array, such as `__getitem__`, but it seems those return `ndarray[Any,]` except in the overload:\r\n```python\r\n    @overload\r\n    def __getitem__(self: NDArray[void], key: list[str]) -> ndarray[_ShapeType, _dtype[void]]: ...\r\n```\r\nI'm not sure what situation this is handling, since I've never seen `self` typed.\r\n\r\nThere's also the `chararray`, `memmap`, `matrix`,  `recordarray`, `MaskedArray`, and `MaskedRecord` types (of which the latter two define an identical `_ShapeType` type variable), but their methods also appear to return `Any` as the shape of any return that isn't the same shape.\r\n\r\nWould you mind if I PR'd this and saw if it passes type checking?","comments":[],"labels":["Static typing"]},{"title":"BUG: f2py python front end gives a fatal error when array size is given before array in agreement with fortran backend","body":"### Describe the issue:\r\n\r\nSUMMARY:\r\n\r\nA beginner using f2py will expect that if the fortran backend lists parameters including an array size and an array, in that order, then a python front end that uses the fortran backend compiled by f2py should also list the array size and array in that same order. However, doing this gives a fatal error in python. The user is required to either omit the array size or provide the two parameters in the reverse order.\r\n\r\nREPRODUCE:\r\n\r\nWith appropriate file names:\r\n````\r\ngfortran bug0.f; .\/a.out; f2py -c -m param_order bug0.f; python3 bug0py.py\r\n````\r\ngives\r\n````\r\nnine =         9\r\n`````\r\nfrom running the fortran main program but\r\n````\r\nTraceback (most recent call last):\r\n  File \"\/MYDIR\/bug0py.py\", line 7, in <module>\r\n    param_order.get_fact(k, j_size, j_array)\r\nparam_order.error: (shape(j_array, 0) == j_size) failed for 1st keyword j_size: get_fact:j_size=45\r\n````\r\nfrom running the python front end.\r\n\r\nCommenting out the line `param_order.get_fact(k, j_size, j_array)` and uncommenting either of the lines\r\n````\r\n#param_order.get_fact(k, j_array, j_size)\r\n#param_order.get_fact(k, j_array)\r\n````\r\nand doing `gfortran bug0.f; .\/a.out; f2py -c -m param_order bug0.f; python3 bug0py.py` again gives \r\n````\r\nnine =      9\r\n````\r\nfrom both the fortran main program and the python front end.\r\n\r\n\r\n### Reproduce the code example:\r\n\r\nfortran `bug0.f`:\r\n````\r\n      program learn_f2py\r\n      integer :: k\r\n      integer, dimension(3) :: j_array\r\n      k = 55\r\n      j_size = 3\r\n      j_array(2) = 845\r\n      call get_fact(k, j_size, j_array)\r\n      end\r\n\r\n      subroutine get_fact(k, j_size, j_array)\r\n!f2py integer, intent(in) :: k, j_size\r\n      integer, dimension(j_size) :: j_array\r\n!f2py intent(in) :: j_array\r\n! implicit typing\r\n      nine = (k + j_array(2))\/100\r\n      write(6,'(a,i10)')'nine =',nine\r\n      return\r\n      end\r\n````\r\n\r\npython `bug0py.py`:\r\n````\r\nimport param_order\r\nk = 55\r\nj_array = (45, 845, 1845) # python list object\r\nj_size = 3\r\n\r\n#This fails fatally, despite the parameters being in the correct order:\r\nparam_order.get_fact(k, j_size, j_array)\r\n\r\n#This works, despite the parameters being in the wrong order:\r\n#param_order.get_fact(k, j_array, j_size)\r\n\r\n#This works:\r\n#param_order.get_fact(k, j_array)\r\n````\r\n\r\n\r\n### Error message:\r\n\r\n````\r\nTraceback (most recent call last):\r\n  File \"\/MYDIR\/bug0py.py\", line 7, in <module>\r\n    param_order.get_fact(k, j_size, j_array)\r\nparam_order.error: (shape(j_array, 0) == j_size) failed for 1st keyword j_size: get_fact:j_size=45\r\n````\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n````\r\nDebian bookworm\r\nii  python3-numpy                                 1:1.24.2-1                              amd64\r\nii  python3                                       3.11.2-1+b1                             amd64\r\n````\r\n\r\n\r\n### Context for the issue:\r\n\r\nDISCUSSION:\r\n\r\nThis behaviour is not obvious to the beginning user and ( see https:\/\/github.com\/numpy\/numpy\/issues\/25616 ) is not documented. The minimal working example that I have shown gives an error message that quickly gives a clue to what is wrong, since it is clear that `j_size` was supposed to be 3 but is seen by python as 45, and we know where the 45 comes from. However, if `j_array` starts with the value 0, the user is less likely to guess the explanation.\r\n\r\nApart from improved documentation, an alternative to heavy refactoring of `f2py` could be to provide a warning or error during the `f2py` compile step that an array size is provided earlier than the array itself and that the user should consider either (i) removing the array size in the python call, or (ii) placing it after the array in both the fortran and python sides. If the user is expected to leave the fortran side as unchanged as possible, then (ii) will not be an option, b ut (i) will be.\r\n","comments":["If you play around with a long list of arrays and array sizes as parameters on the fortran side, you should find that on the python side, as long as all the arrays are listed first (in their same order when ignoring the sizes), and all the sizes are either omitted or listed second (in their same order when ignoring the arrays themselves), then calling the function from fortran or python should work and give the same results. That's no longer a minimal working example, though.\r\n"],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"ENH: optionally transpose rows and columns in A for faster & equivalent matrix_rank computation","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nThis is a simple addition to ``matrix_rank`` in the ``numpy.linalg._linalg.py`` module that optionally performs a transpose of the final two dimensions such that the matrix rank is computed with a smaller number of columns, which speeds it up but is mathematically equivalent. \r\n\r\nHere's some code with a test (using the original ``matrix_rank`` method):\r\n```python\r\nimport numpy as np\r\nimport time\r\n\r\ndef check_matrix_rank(R, C, rank=None, num_tests=1, batch=1):\r\n    rank = np.min(R, C) if rank is None else rank\r\n    if batch==1:\r\n        left_sv = np.random.normal(0, 1, (R, rank))\r\n        right_sv = np.random.normal(0, 1, (rank, C))\r\n        data = left_sv @ right_sv\r\n    else:\r\n        left_sv = [np.random.normal(0, 1, (R, rank)) for _ in range(batch)]\r\n        right_sv = [np.random.normal(0, 1, (rank, C)) for _ in range(batch)]\r\n        data = np.stack([lsv @ rsv for lsv, rsv in zip(left_sv, right_sv)])\r\n\r\n    def message(msg, data, rank, measured_rank, duration, verbose=False):\r\n        per_test = duration\/num_tests\r\n        if per_test < 1:\r\n            dur_message = f\"duration per measurement (ms): {1000*per_test}\"\r\n        else:\r\n            dur_message = f\"duration per measurement: {per_test}\"\r\n        if verbose:\r\n            print(msg, \r\n                  \"data shape:\", data.shape, \r\n                  \"expected\/measured rank:\", f\"{rank}\/{measured_rank}\", \r\n                  dur_message)\r\n        else:\r\n            print(msg, dur_message)\r\n    \r\n    def process_data(data):\r\n        if data.shape[-2]*200 < data.shape[-1]:\r\n            return np.swapaxes(data, -2, -1)\r\n        return data\r\n    \r\n    t = time.time()\r\n    for _ in range(num_tests):\r\n        measured_rank = np.linalg.matrix_rank(data)\r\n    message(\"original matrix_rank:\", data, rank, measured_rank, time.time() - t)\r\n    \r\n    t = time.time()\r\n    for _ in range(num_tests):\r\n        udata = process_data(data)\r\n        measured_rank = np.linalg.matrix_rank(udata)\r\n    message(\"with potential transpose:\", udata, rank, measured_rank, time.time() - t)\r\n\r\n\r\nB = 100\r\n\r\n# asymmetric matrices\r\nR = [10, 100]\r\nC = [int(r*201) for r in R]\r\nrank = [r for r in R]\r\nnum_tests = 1\r\n\r\nprint('Checks for asymmetric batched matrices:')\r\nfor r, c, rnk in zip(R, C, rank):\r\n    print(f'Checks for R={r}, C={c}, rank={rnk}:')\r\n    check_matrix_rank(r, c, rnk, num_tests, batch=B)\r\n    print('')\r\n\r\nprint('')\r\n\r\nprint('Checks for asymmetric matrices (no batch):')\r\nfor r, c, rnk in zip(R, C, rank):\r\n    print(f'Checks for R={r}, C={c}, rank={rnk}:')\r\n    check_matrix_rank(r, c, rnk, num_tests, batch=1)\r\n    print('')\r\n\r\n```\r\n\r\n### Results\r\nOverall, there's a 1.5-2x speed up for asymmetric matrices and similar performance (or very slightly slower) for similar sized matrices). When the input is a batched set of matrices, the speed up is almost 3x. \r\n\r\nI'm not exactly sure how to document this because it's just a simple line to speed up the computation. I suppose there may be cases where the results differ, but that will only be numerical errors so a speed up of almost 2x for large asymmetric matrices is probably worth it. For a first pass, I just added a note in the ``matrix_rank`` method.\r\n\r\nAll tests have passed including linting on my local build.\r\n","comments":[],"labels":["01 - Enhancement"]},{"title":"DOC: How do I get the coefficients from a `Polynomial` object?","body":"### Issue with current documentation:\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial\r\n\r\nThis page mentions `coef` as a parameter, but it doesn't say anything about `coef` as a property.\r\n\r\n`coef` does not give the coefficients of the polynomial, as demonstrated by this code:\r\n\r\n```python3\r\nimport numpy as np\r\nfrom numpy.polynomial import Polynomial\r\n\r\ninputs = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\r\noutputs = [1.0, 2.0, 4.0, 8.0, 16.0, 32.0]\r\n\r\ndegree = 5\r\npoly_fit = Polynomial.fit(inputs, outputs, degree)\r\n\r\ncoefficients = poly_fit.coef\r\n\r\nprint(\"Coefficients:\", coefficients)\r\n# Coefficients: [5.66015625 9.80338542 8.3984375  4.8828125  2.44140625 0.81380208]\r\n\r\n# evaluate the polynomial at x=0\r\n# (using 3 different methods)\r\n\r\nprint(np.polyval(coefficients, 0))  # 0.8138020833332782\r\nprint(np.polyval(coefficients[::-1], 0))  # 5.6601562500000036\r\nprint(poly_fit(0))  # 1.000000000000024\r\n```\r\n\r\nNone of the values from `coef` are the constant x**0 coefficient (that should be something close to 1).\r\n`poly_fit(0)` correctly evaluates the polynomial at zero. But how do I see the coefficients?\n\n### Idea or request for content:\n\n_No response_","comments":["Those _are_ the coefficients -- of the scaled and offset polynomials used for numerical reasons. To see the unscaled version, do `poly_fit.convert().coef`. That is mentioned in the documentation of the return value.","When I don't understand what I'm seeing with a `Polynomial` object, I go to the documentation of the `Polynomial` class, not the documentation of the function that gave me the instance.\r\n\r\nI see now with more searches that there has been a lot of confusion around this documentation problem for many years.\r\nhttps:\/\/github.com\/numpy\/numpy\/issues\/9533 among others\r\n\r\nThere are multiple problems with the documentation around this:\r\n\r\nIt's not clear enough that `Polynomial` inherits from `ABCPolyBase`. That should be somewhere above the parameters to the `__init__` function (since, in some cases, the vast majority of the documentation for a class could be in the parent class).\r\n\r\nThere should be a link to `ABCPolyBase` documentation that includes the `coef` property. (How is anyone supposed to see any of that documentation? Not only is it not linked, it doesn't show up in documentation search either.)\r\n\r\nThe `coef` property documentation should say something like \"Series coefficients in order of increasing degree, scaled and shifted according to `domain` and `window`. If the coefficients for the unscaled and unshifted basis polynomials are of interest, do `polynomial.convert().coef`.\"\r\nThat's more appropriate than putting it in the documentation of a function that returns a `Polynomial` instance. (How many functions are there that return an instance of `ABCPolyBase`? Do you put that in all of them?)\r\n"],"labels":["04 - Documentation"]},{"title":"DOC: Add documentation explaining our promotion rules","body":"This adds a dedicated page about promotion rules.\r\n\r\n---\r\n\r\nPlease don't hesitate to rip it apart, I haven't tried to really iterate yet, but I think it would be useful to get feedback.  (Also don't hesitate to push changes\/fixes!)\r\n\r\nMy thought was to try and not dwell too long on the technical details, but start with the things that I thought are more interesting to very new users.  That might mean the plot (taken from the NEP, but without the text), may be a little bit underexplained.\r\n\r\nThere is also the question if more examples would be relevant, although I don't really want to overdo it (in the NEP the big table is important, for a new user with fresh eyes, I am not quite sure what is.)\r\n\r\nI need to figure out if we have any docs that need to be removed\/refer here (beyond `result_type, `promote_types`, and maybe the release note\/migration guide as a link).\r\n\r\n*Doctest will probably fail for now, and references might be bad... let's fix it as we iterate*\r\n\r\n---\r\n\r\n@mdhaber maybe you can have a look?","comments":[],"labels":["04 - Documentation"]},{"title":"DOC: quantile: correct\/simplify documentation","body":"gh-24588 noted inaccuracies in the `quantile` documentation. This PR corrects these and simplifies the descriptions of the various methods. It also refers users of `percentile`, `nanquantile`, `nanpercentile` to the documentation of `quantile` to reduce redunandancy and ensure that the notes of these functions do not get out of sync.\r\n\r\nCloses gh-24588\r\n\r\n@lorentzenchr ","comments":["Finally.\r\n\r\n<img width=\"693\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/6570539\/0eca1716-54be-4f95-9b7f-16d1c2249e5a\">\r\n\r\nI guess it needed to be the simplified table markup and not include a reference link. @lorentzenchr @melissawm I think this is ready for a look when you have a moment. [Link to the proposed `quantile` docs.](https:\/\/output.circle-artifacts.com\/output\/job\/d43c275a-6f11-4f88-9748-7065c1ee7e0b\/artifacts\/0\/doc\/build\/html\/reference\/generated\/numpy.quantile.html#numpy-quantile)","> and not include a reference link.\r\n\r\nSorry for catching this late - for future reference some numpydoc features don't work inside tables - there is a warning to this effect [buried deep in the docs](https:\/\/numpydoc.readthedocs.io\/en\/latest\/format.html#references)"],"labels":["04 - Documentation","triage review"]},{"title":"ENH: Add ``axis`` kwarg to np.tile","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nAdds a keyword argument `axis` allowing to specify which axes to tile:\r\n\r\n``` python\r\n>>> A = np.arange(3)[None, :]\r\n>>> A\r\n[[0 1 2]]\r\n>>> np.tile(A, (1, 2))\r\n[[0 1 2 0 1 2]]\r\n>>> np.tile(A, 2, axis=-1)\r\n[[0 1 2 0 1 2]]\r\n>>> np.tile(A, (2, 2))\r\n[[0 1 2 0 1 2]\r\n [0 1 2 0 1 2]]\r\n>>> np.tile(A, (2, 2), axis=(0, 1))\r\n[[0 1 2 0 1 2]\r\n [0 1 2 0 1 2]]\r\n```\r\n\r\nfixes https:\/\/github.com\/numpy\/numpy\/issues\/8879\r\n\r\nThis is my first PR to numpy, so let me know if there are any issues.","comments":["> The implementation looks good. But I am not sure this functionally should be in numpy. The pr adds some convenience in the input arguments, but this can also be done with a small wrapper around the np.tile\r\n\r\nThis change makes sense in my opinion to have parity with `np.sum` and `np.roll` (and possibly others) which also accept an `axis` kwarg that can be a tuple of ints.\r\n\r\n``` python\r\n>>> x = np.random.random((3, 3))\r\n>>> x\r\narray([[0.394, 0.653, 0.309],\r\n       [0.368, 0.196, 0.859],\r\n       [0.439, 0.505, 0.007]])\r\n\r\n>>> np.roll(x, (1, 1), axis=(0, 1))\r\narray([[0.007, 0.439, 0.505],\r\n       [0.309, 0.394, 0.653],\r\n       [0.859, 0.368, 0.196]])\r\n\r\n>>> np.sum(x, axis=(0, 1))\r\n3.72948997730025\r\n```","Any news on this?  Still think this is worth considering due to the principle of Least Astonishment.","Hmm, I now see @ngoldbaum's reply on the mailing list. He makes a good point that now that there is an API that all array libraries have agreed on, it would be better not to deviate from that unnecessarily.","The further discussion does suggest it is worth proposing it. To be honest, if one were to start from scratch, I think one would have defined `np.tile(array, rep: int, axis: int)` and then expand to both `rep` and `axis` being tuples..."],"labels":["01 - Enhancement"]},{"title":"TSK: Follow-up things for stringdtype","body":"### Proposed new feature or change:\r\n\r\nA number of points arose in discussion of #25347 that were deemed better done in follow-up, to not distract from the meat of that (very large) PR. This issue is to remind us of those.\r\n1. [x] The doc for `StringDType` has a size argument that was needed in development but not for actual release. It can be removed. This should be done before the NumPy 2.0 release because it is public API. (Though might it be useful for a short version that only uses arena? see below. Probably can put it back if needed...)\r\n    - https:\/\/github.com\/numpy\/numpy\/pull\/25856\r\n3. [ ] The `add` ufunc needs a promoter so addition with a python string works.\r\n4. [ ] Add a cython interface for the stringdtype C API.\r\n5. [x] It is likely better to use a flag for strings \"long strings\" (stored outside of the numpy array proper) instead of one for short ones (stored inside), so that an all-zero entry correctly implies a zero-length string (see https:\/\/github.com\/numpy\/numpy\/pull\/25347#issuecomment-1910321015)\r\n6. [ ] Refactor the flags in the vstring implementation to use bitfields. This will improve clarity and eliminate complicated  bitflag math.\r\n7. [ ] Possibly, the arena should have more recoverable flags\/size information (see https:\/\/github.com\/numpy\/numpy\/pull\/25347#issuecomment-1910321015)\r\n8. [ ] Investigate refactoring `new_stringdtype_instance` into `tp_init`\r\n9. [ ] Replace the long `#define` in `casts.c` with templating (or `.c.src`)\r\n10. [x] Replace ufunc wrappers with passing functions into `*auxdata` (see [here](https:\/\/github.com\/numpy\/numpy\/pull\/25347#discussion_r1471915449), [here](https:\/\/github.com\/numpy\/numpy\/pull\/25347#discussion_r1471928789), [here](https:\/\/github.com\/numpy\/numpy\/pull\/25347#discussion_r1471930959), and [here](https:\/\/github.com\/numpy\/numpy\/pull\/25347#discussion_r1471932261)) [e.g., `minimum` and `maximum`; the various comparison functions; the templated unary functions; `find`, `rfind` and maybe `count`; `startswith` and `endswith`; `lstrip`, `rstrip` and `strip`, plus `whitespace` versions]. Attempt in gh-25796\r\n11. [ ] Check `array2string` formatter overrides.\r\n12. [ ] Adjust error messages referring to \"object array\" (e.g., `a.view('2u8')` currently errors with `\"Cannot change data-type for object array.\"`).\r\n\r\nThings possibly for longer term\r\n- Support in structured arrays (perhaps not super useful, but could be seen as similar to `object`).\r\n- Expose more of the currently private `NpyString` API. This will depend on feedback from users.\r\n- Fix `longdouble` to string, which is listed as broken in a `TODO` item in `casts.c`. isn't `dragon` able to deal with it?\r\n- Add a DType API callback that triggers when the initial filling of a newly created array (e.g. after `PyArray_FromAny` finishes filling a new array). We could use this to trim the arena buffer to the exact size needed by the array. Currently we over-allocate because the buffer grows exponentially with a growth factor of 1.25.\r\n- Might it make sense on 64bit systems, where normally the size is 16 bytes, to have a 8-byte version (short strings up to 7, only arena allocations for long ones; might use the `size` argument...).\r\n- In principle, `.view(StringDType())` could be possible in some cases (e.g., to change the NA behaviour). Would need to share the allocator (and thus have reference counts for that...).\r\n- Dealing with array scalars vs `str` scalars - see also more general discussion about array scalars in gh-24897.\r\n\r\nSmall warts, possibly not solvable\r\n- `StringDType` is added to `add_dtype_helper` late in the initialization of `multiarraymodule`; can this be easier?\r\n- Can the cases of having and not having gil be factored out, so that one doesn't get the kind of hybrid stuff in `load_non_nullable_string` with its `has_gil` argument.\r\n- to have `dtype.hasobject` be true is logical but not quite the right name.\r\n","comments":["Added a few things I had on my personal list. Thanks for opening this and for all the review this week!","@asmeurer pointed out to me that we should add a proper scalar type. Right now stringdtype's scalar is `str`, which doesn't inherit from `np.generic`, so stringdtype ends up as an oddball in the API from the perspective of the type hierarchy of the scalar types.\r\n\r\nWe can fix this by defining a proper scalar type that wraps a single packed string entry and exposes an API that is duck-compatible with `str`, making use of the ufunc implementations we're adding.\r\n\r\nThis may also lead to performance improvements, since scalar indexing won't need to copy to python's internal string representation.\r\n\r\nI don't think this needs to happen for 2.0 since this is something we can improve later and I doubt it will have major user-facing impacts, since object string arrays also effectively have a `str` scalar.","> so stringdtype ends up as an oddball in the API from the perspective of the type hierarchy of the scalar types.\r\n\r\nI personally don't see this as a big issue.  Although it might be friendly to not convret to scalar as often as we currently do if it is a string scalar (bad timeline too though).","I think the main problem may be that people expect generally that they can treat `array[something]` as array-ish, with a `.shape`, etc. In that sense we probably do need a scalar type (or, perhaps preferably, just not drop to a scalar in the first place...).\r\n\r\nIf we're going to be \"not quite right\" for 2.0, should we perhaps err on the side of not creating a `str` object?","Are you saying that we should create a 0D array instead?","If so, I think it's much more important for the scalar to be duck-compatible with `str` than with `ndarray`. Especially if the goal is replace object string arrays in downstream packages.","Yes, I was, I really dislike array scalars and wish everything was 0-D arrays instead. But you make a good point that we want to make sure that object arrays can easily be replaced...\r\n\r\nEDIT: because I dislike how the type that comes out of `__getitem__` or `.sum()` depends on the arguments. Why should `axis=None` give me a different instance than `axis=-1`? And it is just a hassle if one subclasses `ndarray`...","I also completely agree that scalars are terrible for a dozen different reasons, and wish numpy just had 0-d arrays. But I guess this was too ambitious for NumPy 2.0. As it stands, scalars exist. Nathan makes a good point that object scalars are already kind of specially broken because they aren't even numpy types, and this new dtype replaces object `str` arrays for various use-cases (tbf, it also replaces `np.str_` arrays for many use-cases too). But at least if you are working with object arrays you probably (or at least hopefully) are doing it on purpose and can be careful about this. For every other dtype, the scalar is at least array-like in most respects. And this is documented too https:\/\/numpy.org\/doc\/stable\/reference\/arrays.scalars.html#numpy.object_:\r\n\r\n> The object type is also special because an array containing [object_](https:\/\/numpy.org\/doc\/stable\/reference\/arrays.scalars.html#numpy.object_) items does not return an [object_](https:\/\/numpy.org\/doc\/stable\/reference\/arrays.scalars.html#numpy.object_) object on item access, but instead returns the actual object that the array item refers to.\r\n\r\nMaking stringdtype scalars subclass from both `str` and `generic` seems like the best option from a usability point of view, although if actually making it a real `str` subclass is not good for performance, just making it duck type and defining `__instancecheck__` is probably good enough. ","Even subclassing from generic gives you crazy things, like indexing must be string indexing and not array indexing!  In other words: IMO, part of the problem with scalars is that they pretend to be array likes, even though it is currently vital for much code that they do (because we create them too often).\r\nPlus, the ABI is likely odd, so care has to be taken (numpy strings pull it off, so not sure if it is easy or not).\r\n\r\nI envisioned we could create a *new* `np.NumPyScalar`, fully abstract, class you can register with.  But not sure it matters here.\r\n\r\nIn short, I don't care about the hierarchy at all, but I agree there are two things:\r\n1. You cannot write `np.array(scalar)` and `np.array(str_array[0])` without a `dtype` because we do not pick the new dtype for those for BC reasons.\r\n2. Just like for `object` dtype, the fact that we don't preserve 0-D arrays is more harmful for this DType compared to numerical dtypes where the scalars somewhat duck-type as arrays.\r\n\r\nI suggest moving discussion about point 2 to gh-24897.  I could imagine making my try-0d-preservation opt-in more agressively for all but current dtypes (minus object maybe), even if we don't dare change it otherwise without 2.0.\r\nAnd yes, maybe we should _never_ convert to scalars and make that something users always would have to do explicitly with `.item()` or so, but not sure I think that is as realistic."],"labels":["17 - Task"]},{"title":"BUG: Type hints missing for `np.array` with builtin types ","body":"### Describe the issue:\n\nType hints for something like `np.array(1)` do not correctly hint the array's `dtype`.\r\n\r\nE.g.:\r\n``` bash\r\n mypy -c 'import numpy; x = numpy.array(1); reveal_type(x)'\r\n```\r\n\r\nwill output:\r\n```\r\n<string>:1: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n```\r\n\r\nThe same output is generated for `float` inputs (e.g. `numpy.array(1.0)`), `bool` inputs (e.g. `numpy.array(True)`), and `str` inputs (e.g. `numpy.array(\"a\")`).\r\n\r\nThe expected behaviour is that the `dtype` would be inferred from the input type. E.g. for the `int` input case, the revealed type expected is:\r\n```\r\n<string>:1: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[numpy.signedinteger[numpy._typing._64Bit]]]\"\r\n```\r\n\r\n## Possible solution\r\nI have found that adding new overloads to `multiarray.pyi` here resolves this.\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/ac8b7ac9e1b47bfc8211ca2895086030e7f71ed4\/numpy\/_core\/multiarray.pyi#L190-L200\r\n\r\nFor example, adding the following overload for mapping `int` to `int_`:\r\n``` python\r\n@overload\r\ndef array(\r\n    object: _ArrayLike[int],\r\n    dtype: None = ...,\r\n    *,\r\n    copy: bool | _CopyMode = ...,\r\n    order: _OrderKACF = ...,\r\n    subok: bool = ...,\r\n    ndmin: int = ...,\r\n    like: None | _SupportsArrayFunc = ...,\r\n) -> NDArray[int_]: ...\r\n```\r\nwill result in the correct `dtype` revealed by mypy. Similar overloads can be added for other types (`float` -> `float64`, `bool` -> `bool_`, etc.)\r\n\n\n### Reproduce the code example:\n\n```python\nSee code above.\n```\n\n\n### Error message:\n\n_No response_\n\n### Python and NumPy Versions:\n\n1.26.3\r\n3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\nI believe using `np.array` on built-in types without any other parameters is a common use case. Adding type hints for these cases would help with type-narrowing for libraries which use static type checkers.","comments":["This case was previously mentioned in https:\/\/github.com\/numpy\/numpy\/issues\/19252#issue-921589619:\r\n\r\n\r\n> Three examples wherein dtype-support is substantially more difficult to implement.\r\n> \r\n> ```python\r\n> AR_3 = np.array(1.0)\r\n> AR_4 = np.array(1, dtype=float)\r\n> AR_5 = np.array(1, dtype=\"f8\")\r\n> \r\n> if TYPE_CHECKING:\r\n>     reveal_type(AR_3)  # note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n>     reveal_type(AR_4)  # note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n>     reveal_type(AR_5)  # note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n> ```"],"labels":["00 - Bug","Static typing"]},{"title":"API: make numpy.lib._arraysetops.intersect1d work on multiple arrays","body":"Intersect1d can be used with multiple arrays now and also returns the indices of all arrays when using `return_indices=True`.\r\n\r\nI need this for my own work, so I figured I might as well share the code with the community. Thats why I didnt check on the mailing list first.","comments":["I dont think the failing test is on me. Can someone restart this please or tell me what needs to change?","The doctest failure is real:\r\n\r\n```\r\nFile \"venv\/lib\/python3.11\/site-packages\/numpy\/__init__.py\", line ?, in intersect1d\r\nFailed example:\r\n    intersect1d(*ars, return_indices=True)\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"\/home\/circleci\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/doctest.py\", line 1351, in __run\r\n        exec(compile(example.source, filename, \"single\",\r\n      File \"<doctest intersect1d[2]>\", line 1, in <module>\r\n        intersect1d(*ars, return_indices=True)\r\n        ^^^^^^^^^^^\r\n    NameError: name 'intersect1d' is not defined\r\n```\r\n\r\nYou can run the doctests locally with `spin python tools\/refguide_check.py --doctests`.\r\n\r\nThe mypy failure is unrelated, I restarted that one.\r\n\r\nBy the way, this isn't mergeable without:\r\n\r\n* Some tests\r\n* API changes need more justification. The NumPy API is already quite complicated so we don't add things just because a contributor needed it for their work, they also have to justify why it's generally useful. You should ping the mailing list about this to get feedback from a broad cross-section of the community that doesn't track github every day.\r\n* API changes need a release note.\r\n\r\n"],"labels":["56 - Needs Release Note.","30 - API"]},{"title":"BUG: Masked array default fill value can overflow","body":"### Describe the issue:\n\nFor both signed and unsigned integers the default fill value is 99999, while for floats it is 1e20.\r\n\r\nThis is problematic for (u)int[8,16] as well as half floats, which do not contain the default fill value in their valid range.\n\n### Reproduce the code example:\n\n```python\n>>> arr = np.ma.array([1, 2, 3], mask=[1, 0, 1], dtype=np.int8)\r\n>>> arr.filled()\r\narray([63,  2, 63], dtype=int8)\r\n\r\n>>> arr = np.ma.array([1, 2, 3], mask=[1, 0, 1], dtype=np.float16)\r\n>>> arr.filled()\r\n\/Users\/goldbaum\/Documents\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/ma\/core.py:3873: RuntimeWarning: overflow encountered in cast\r\n  np.copyto(result, fill_value, where=m)\r\nOut[16]: array([inf,  2., inf], dtype=float16)\n```\n\n\n### Error message:\n\n```shell\nN\/A\n```\n\n\n### Python and NumPy Versions:\n\nNumpy 2.0 dev on python 3.11.\n\n### Runtime Environment:\n\nN\/A\n\n### Context for the issue:\n\nI don't think this is an urgent but didn't see an issue describing this behavior so I'm filing this for future searchers.\r\n\r\nThat said, this does complicate the NEP 50 implementation because we need to have a number of workarounds so that this continues to work. I think it would be better to choose a default fill value that fits in the range of the data (`[i,f]info.max`?) for these types, but I have no idea what that entails for backward compatibility.","comments":[" If a specific fill value is absent in `filled`, it gets the default fill value for that dtype. The catch is that it gets the default fill value for that \"kind\" of dtype. below code from numpy.ma core code is given for reference:\r\n`default_filler = {'b': True,\r\n                  'c': 1.e20 + 0.0j,\r\n                  'f': 1.e20,\r\n                  'i': 999999,\r\n                  'O': '?',\r\n                  'S': b'N\/A',\r\n                  'u': 999999,\r\n                  'V': b'???',\r\n                  'U': 'N\/A'\r\n                  }`\r\n                  \r\nYou can see that for any kind of float (float16, float32, float64) the same fill value is being used. For a fill value of \"1.e20\" you always get the runtime error while typecasting (say to float16). I tried the below code to reproduce that issue:\r\n`converted_value = np.array(1e+20, dtype=np.float64).astype(np.float16)`\r\n\r\nOne workaround is to define fill values to each specific dtype. Another solution I tried that worked was to replace default value of float to 'inf', which did not raise any issue for any specific float dtypes.\r\n`fill_value = np.inf`\r\n","The main issue here isn't so much the implementation of changing the default fill value, or even coming up with better default fill values, it's whether or not changing this will break user code. The default fill value is used in a number of places implicitly in operations so it's not clear to me if changing the default will have unintended consequences elsewhere.","Since the fill value is not determined by the user, we should be able to change the default values without major code breakage. But there are chances of user code that depends on the default values (can't think of an example right off the bat). Maybe change and document it?","Also this warning is not seen in earlier versions of Numpy (I tried with v1.23.5). So it can be changed, although I need to verify how it's being handled in earlier versions."],"labels":["00 - Bug"]},{"title":"ENH: add matvec and vecmat gufuncs","body":"This PR adds new `matvec` and `vecmat` gufunc to calculate the matrix-vector and vector-matrix product, to add to plain matrix multiplication with `matmul` and the inner vector product with `vecdot`. \r\n\r\nFixes #12348\r\n\r\nNote that for complex numbers, `vecmat` is defined as `x\u2020A`, i.e., the complex conjugate of the vector is taken. This seems to be the standard and is what we used for `vecdot` too (`x\u2020x`). However, it is *not* what `matmul` does for vector-matrix or indeed vector-vector products. I think this is a bug in matmul, which I'm happy to fix here. But I'll post to the mailing list to get feedback.\r\n\r\nSeparately, with these functions available, in principle these could be used in `__matmul__` and the specializations in `np.matmul` removed. But that can be a separate PR (if it is wanted at all).\r\n\r\n*Summary of [mailing list discussion](https:\/\/mail.python.org\/archives\/list\/numpy-discussion@python.org\/thread\/LX36TEAENIDSSFAKS6YLXXLWVOBABSEC\/)*\r\n- Behaviour of `np.matmul` for vector-matrix should not be adjusted. Some surprise that \"matrix multiplcation\" deals with vectors at all.\r\n- Less obvious consensus on what `@` should do.\r\n- Moderate support for having special `matvec` and `vecmat` functions.","comments":["Refactored to avoid duplication of code: `matvec` now uses the `matmul_noblas` loops internally (as does the matrix-vector implementation in `matmul` itself), and `vecmat` creates a loop that uses the loops in `vecdot`, letting their outer loop deal with the matrix dimension.","@charris - since you were the one originally asking for it, what do you think of this implementation? It is now quite lightweight, since it reuses inner loops.","macos failure unrelated","Moving milestone to 2.1, if we get this in we should change it again, but at least I don't think it needs to be prioritized.","Yes, not critical for 2.0 for sure, but maybe more logical to clear the milestone rather than set it to 2.1 if there still is a chance. This PR has become fairly simple after my refactor."],"labels":["01 - Enhancement","component: numpy._core"]},{"title":"MAINT: Refactor C++ Sort Operations","body":"  The refactored sort operations are now encapsulated within the `np::sort` namespace and no longer rely on tags.\r\n  This patch also introduced new C++ datatypes that cover all NumPy built-in datatypes.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":[],"labels":["03 - Maintenance"]},{"title":"DOC: f2py command to generate c wrapper respecting pyf file directives","body":"### Issue with current documentation:\r\n\r\nPython 3.11.4, numpy 1.26.3, macOS Sonoma 14.2.1 (Intel)\r\n\r\nThe documentation does not explain how to generate a c wrapper that uses custom directives in a pyf file or it is not possible and the documentation contradicts itself. \r\n\r\nIm trying to move a [fortran project](https:\/\/github.com\/scottransom\/pyslalib)  from numpy.disutils to meson. The previous developers created a long pyf file with custom function annotations. The example for how to move to meson requires the [c wrapper generation step](https:\/\/numpy.org\/doc\/stable\/f2py\/buildtools\/meson.html#automating-wrapper-generation).  [Extension module construction](https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#extension-module-construction) mentions using pyf files as a source for c wrapper generation.\r\n\r\n Running ```poetry run Python3 -m numpy.f2py -m slalib --lower slalib.pyf addet.f cc2s.f dc62s.f dr2af.f dvn.f etrms.f h2fk5.f nutc80.f prebn.f rverot.f tpv2c.f afin.f cc62s.f dcc2s.f dr2tf.f dvxv.f euler.f hfk5z.f oap.f prec.f rvgalc.f ue2el.f airmas.f cd2tf.f dcmpf.f drange.f e2h.f evp.f idchf.f oapqk.f preces.f rvlg.f ue2pv.f altaz.f cldj.f dcs2c.f dranrm.f earth.f fitxy.f idchi.f obs.f precl.f rvlsrd.f unpcd.f amp.f clyd.f dd2tf.f ds2c6.f ecleq.f fk425.f imxv.f pa.f prenut.f rvlsrk.f v2tp.f ampqk.f combn.f de2h.f ds2tp.f ecmat.f fk45z.f intin.f pav.f pv2el.f s2tp.f vdv.f aop.f cr2af.f deuler.f dsep.f ecor.f fk524.f invf.f pcd.f pv2ue.f sep.f veri.f aoppa.f cr2tf.f dfltin.f dsepv.f eg50.f fk52h.f kbj.f pda2h.f pvobs.f sepv.f vers.f aoppat.f cs2c.f dh2e.f dt.f el2ue.f fk54z.f m2av.f pdq2h.f pxy.f slalib-f2pywrappers.f vn.f aopqk.f cs2c6.f dimxv.f dtf2d.f epb.f fk5hz.f map.f permut.f range.f smat.f vxv.f atmdsp.f ctf2d.f djcal.f dtf2r.f epb2d.f flotin.f mappa.f pertel.f ranorm.f subet.f wait.f atms.f ctf2r.f djcl.f dtp2s.f epco.f galeq.f mapqk.f pertue.f rcc.f supgal.f xy2xy.f atmt.f daf2r.f dm2av.f dtp2v.f epj.f galsup.f mapqkz.f planel.f rdplan.f svd.f zd.f av2m.f dafin.f dmat.f dtps2c.f epj2d.f ge50.f moon.f planet.f refco.f svdcov.f bear.f dat.f dmoon.f dtpv2c.f epv.f geoc.f mxm.f plante.f refcoq.f svdsol.f caf2r.f dav2m.f dmxm.f dtt.f eqecl.f gmst.f mxv.f plantu.f refro.f tp2s.f caldj.f dbear.f dmxv.f dv2tp.f eqeqx.f gmsta.f nut.f pm.f refv.f tp2v.f calyd.f dbjin.f dpav.f dvdv.f eqgal.f h2e.f nutc.f polmo.f refz.f tps2c.f gresid.F random.F```\r\n \r\n Results in a c wrapper that does not respect the directives in the pyf file. \r\n For example the sla_refro function has parameter 10 (ref) marked as intent(out) but the c wrapper keeps the parameter\r\n \r\n [Other options](https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#other-options) mentions that numpy.f2py -m can't be used with pyf files explaining the behaviour but contradicting the extension module construction section.\r\n \r\na simpler way to reproduce this is to alter the fib example file like so:\r\n\r\n```\r\nC FILE: FIB1.F\r\n      SUBROUTINE FIB(A,N)\r\nC\r\nC     CALCULATE FIRST N FIBONACCI NUMBERS\r\nC\r\n      INTEGER N\r\n      INTEGER A\r\n      A = N + 1\r\n\r\n      END\r\nC END FILE FIB1.F\r\n```\r\nediting the pyf file to include \r\n```\r\npython module fibx ! in \r\n    interface  ! in :fibx\r\n        subroutine fib(a,n) ! in :fibx:fib1.f\r\n            integer intent(out) :: a\r\n            integer :: n\r\n        end subroutine fib\r\n    end interface \r\nend python module fibx\r\n```\r\nand finally running ```poetry run Python3 -m numpy.f2py -m fibx --lower fib1.pyf fib1.f```\r\n\r\nThe resulting .c file still has \"a\" marked as an input\r\n\r\n p.s. I believe that the pyf file is correct as [The smart way](https:\/\/numpy.org\/doc\/stable\/f2py\/f2py.getting-started.html#the-smart-way) results in a correct wrap.\r\n\r\n### Idea or request for content:\r\n\r\nEither an example of how to generate a c wrapper using a pyf file or the mention of signature files should be removed from [Extension module construction](https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#extension-module-construction) to avoid confusion","comments":["The notes section of https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#numpy.f2py.get_include seems relevant. That's what SciPy does in `scipy.linalg`, which has a lot of `.pyf` files. This may be a gap in the docs indeed (@HaoZeke may be able to judge better than I can).","> The notes section of https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#numpy.f2py.get_include seems relevant. That's what SciPy does in `scipy.linalg`, which has a lot of `.pyf` files. This may be a gap in the docs indeed (@HaoZeke may be able to judge better than I can).\r\n\r\nI may be misunderstanding but I don't think that my issue comes from missing c sources as those are supplied by numpy when I compile the __module.c. And the resulting .so file is working except that it is supposed to transform a call by reference to a call by value\/a return value which it doesn't. The __module.c looks like I never supplied my signature (.pyf) file and f2py is guessing the necessary c wraps. I know however that my signature file is correct as skipping the intermediate step of generating the __module.c with `numpy.f2py -c` results in a `.so` file that corresponds to my signature file. There must be some kind for difference between the way `f2py -c` and `f2py -m` generate the c file as I interrupted the execution of `f2py -c` to grab the .c file from `\/var\/...\/` and it is correct. There may be an option missing in [Extension module construction](https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#extension-module-construction).\r\n","Thanks for bringing this up @JulianBothorn. The documentation needs an update. The solution here is to just not pass any Fortran files if the signature is provided.\r\n\r\nSo:\r\n```bash\r\n# Incorrect, .pyf is ignored!\r\npython -m numpy.f2py -m fibx --lower fib1.pyf fib1.f\r\n```\r\nAs noted, this generates the `.pyf` \"on the fly\" as it were, so the resulting `.c` file is unexpected:\r\n\r\n```bash\r\ngrep \"Parameters\" fibxmodule.c -A 3 -B 2\r\nstatic char doc_f2py_rout_fibx_fib[] = \"\\\r\nfib(a,n)\\n\\nWrapper for ``fib``.\\\r\n\\n\\nParameters\\n----------\\n\"\r\n\"a : input int\\n\"\r\n\"n : input int\";\r\n\/* extern void F_FUNC(fib,FIB)(int*,int*); *\/\r\n```\r\n\r\nThe correct approach is to not provide any Fortran sources here.\r\n\r\n```bash\r\n# Correct\r\n\u276f python -m numpy.f2py fib1.pyf\r\nReading fortran codes...\r\n\tReading file 'fib1.pyf' (format:free)\r\nPost-processing...\r\n\tBlock: fibx\r\n\t\t\tBlock: fib\r\nApplying post-processing hooks...\r\n  character_backward_compatibility_hook\r\nPost-processing (stage 2)...\r\nBuilding modules...\r\n    Building module \"fibx\"...\r\n    Generating possibly empty wrappers\"\r\n    Maybe empty \"fibx-f2pywrappers.f\"\r\n        Constructing wrapper function \"fib\"...\r\n          a = fib(n)\r\n    Wrote C\/API module \"fibx\" to file \".\/fibxmodule.c\"\r\n```\r\n\r\nThen the generated `.c` file is correct in terms of the signature:\r\n\r\n```bash\r\ngrep \"Parameters\" fibxmodule.c -A 3 -B 2\r\n#\r\nstatic char doc_f2py_rout_fibx_fib[] = \"\\\r\na = fib(n)\\n\\nWrapper for ``fib``.\\\r\n\\n\\nParameters\\n----------\\n\"\r\n\"n : input int\\n\"\r\n\"\\nReturns\\n-------\\n\"\r\n\"a : int\";\r\n```\r\n\r\nA PR to the existing documentation clarifying this behavior would be most welcome.\r\n\r\nIt might be more verbose for many cases but I wonder if the F2PY CLI shouldn't warn more, e.g. if there are `.pyf` and Fortran files a warning would be nice.. (PRs also welcome for this)."],"labels":["04 - Documentation","component: numpy.f2py"]},{"title":"BUG: Scalar `__format__` should use dragon4 printing for floats","body":"### Describe the issue:\r\n\r\nNumpy float value is not rounded with python round function in f-string, but is rounded in every other scenario. In the code example below, first print row doesn't work as expected, but second one does.\r\nNot sure if this is a bug and if it is a numpy bug, please let me know what exactly is happening here.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\na = 0.123456\r\nb = np.float32(a)\r\n\r\nprint(f\"Python float: {round(a, 3)}, Numpy float: {round(b, 3)}\")\r\nprint(\"Python float:\", round(a, 3), \"Numpy float:\", round(b, 3))\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nPython float: 0.123, Numpy float: 0.12300000339746475\r\nPython float: 0.123 Numpy float: 0.123\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\nPython 3.11.4\r\nnp.__version__ 1.26.2\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["Example with float16\r\n```\r\nb = np.float16(a)\r\nprint(f\"Python float: {round(a, 3)}, Numpy float: {round(b, 3)}\")\r\n\r\nPython float: 0.123, Numpy float: 0.1240234375\r\n```","Just to note: It isn't the rounding tha isn't working, that is fine: The value *is* correctly rounded (to the given precision possible).\r\n\r\nIt is the `__format__` implementation that falls back to Python `float` formatting (I guess), which is bad.","Also, due to that fact, I would expect `f\"{b:1.3}f` works mostly fine (except for long double when you want full precision), since it goes via Python float.  And that is should be the preferable spelling anyway."],"labels":["00 - Bug"]},{"title":"BUG: branch choices for `geomspace` on complex arguments","body":"### Describe the issue:\r\n\r\nThe docs for [`np.geomspace`](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.geomspace.html#numpy-geomspace) state:\r\n\r\n> If the inputs or dtype are complex, the output will follow a logarithmic spiral in the complex plane. (There are an infinite number of spirals passing through two points; the output will follow the shortest such path.)\r\n\r\nBut the function does not appear to obey this rule. I'm not sure how it is in fact choosing branch cuts.\r\n\r\nSearching a bit, this may be altered by #25441 but I believe that is not in any released version yet.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nnp.geomspace(-1j, 0.001 + 1j, 5)   # semicircle to the right, via +1\r\nnp.geomspace(-1j, -0.001 + 1j, 5)  # semicircle to the left, via -1, as documented\r\n\r\nx = 1.2 + 3.4j  # also x = 1+1j\r\ndelta = 0.01j\r\nnp.geomspace(x, -x + delta, 5)  # crosses real line near 3.5, middle point 3.4-1.2j\r\nnp.geomspace(x, -x - delta, 5)  # goes the same way!\r\n```\r\n\r\n\r\n### Error message:\r\n\r\nNo error message. Numerical result is:\r\n\r\n```python\r\n>>> x = 1.2 + 3.4j  # also x = 1+1j\r\n>>> delta = 0.01j\r\n>>> np.geomspace(x, -x + delta, 5)  # crosses real line near 3.5, middle point 3.4-1.2j\r\narray([ 1.2       +3.4j       ,  3.2509223 +1.5538648j ,\r\n        3.39499673-1.20000116j,  1.55032927-3.24738677j,\r\n       -1.2       -3.39j      ])\r\n>>> np.geomspace(x, -x - delta, 5)  # goes the same way!\r\narray([ 1.2       +3.4j       ,  3.25445784+1.55740034j,\r\n        3.40499673-1.20000115j,  1.56093588-3.25799337j,\r\n       -1.2       -3.41j      ])\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n```\r\n1.26.3\r\n3.11.7 (main, Jan 16 2024, 14:42:22) [Clang 14.0.0 (clang-1400.0.29.202)]\r\n```\r\n\r\n### Runtime Environment:\r\n\r\n```\r\n[{'numpy_version': '1.26.3',\r\n  'python': '3.11.7 (main, Jan 16 2024, 14:42:22) [Clang 14.0.0 '\r\n            '(clang-1400.0.29.202)]',\r\n  'uname': uname_result(system='Darwin', node='ArmBook.local', release='21.6.0', version='Darwin Kernel Version 21.6.0: Sun Nov  6 23:29:57 PST 2022; root:xnu-8020.240.14~1\/RELEASE_ARM64_T8101', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP'],\r\n                      'not_found': ['ASIMDFHM']}},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/opt\/homebrew\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n```\r\n\r\n### Context for the issue:\r\n\r\n","comments":["I can confirm the documentation does not match the implementation. I am not sure what the use case is for a complex `np.geomspace`, or why the shortest path is preferred, so not sure which one needs to be changed.\r\n\r\nA fix that modifies the code so the shortest path is used, is in https:\/\/github.com\/numpy\/numpy\/compare\/main...eendebakpt:geomspace, but it is not very elegant (it ad-hoc modifies the phase of the result after the main calculation). Another option would be to deal with the radius and phase of the start and stop separately. For the radius use the normal logspace, for the phase part use a linspace (making sure to take the shortest path module 2*pi).\r\n\r\n@endolith Any ideas on this?\r\n","I did a quick test on `-dev`, which includes my #25441, and now it seems more reasonable,\r\n```\r\nIn [2]: np.geomspace(x, -x + delta, 5)\r\nOut[2]: \r\narray([ 1.2       +3.4j       , -1.5538648 +3.2509223j ,\r\n       -3.39499673+1.20000116j, -3.24738677-1.55032927j,\r\n       -1.2       -3.39j      ])\r\n\r\nIn [3]: np.geomspace(x, -x - delta, 5)\r\nOut[3]: \r\narray([ 1.2       +3.4j       ,  3.25445784+1.55740034j,\r\n        3.40499673-1.20000115j,  1.56093588-3.25799337j,\r\n       -1.2       -3.41j      ])\r\n```\r\nI remember my pleasant surprise about \"... how the code in geomspace is substantially simplified by using the new definition!\", but it seems it was actually more than that!\r\n\r\nWould you be able to confirm that the issue is solved on numpy-dev?","@eendebakpt I had no use case in mind for complex inputs, and [wasn't sure if they even needed to be supported](https:\/\/github.com\/numpy\/numpy\/pull\/7268#issuecomment-186503421).  I think the \"shortest such path\" description was more an observation than a specification.  I don't care what it does with complex inputs.  The main goal of `geomspace` was just more intuitive arguments than `logspace`.","@mhvk I created a regression test that passes: #25715, so it seems the issue has been resolved on current dev."],"labels":["00 - Bug"]},{"title":"BUG: SVML lacks CET support","body":"### Describe the issue:\n\nSVML includes assembler that requires instrumentation \/ annotation to support CET. Somewhat similar to annotation for non-executable stack support https:\/\/github.com\/numpy\/SVML\/commit\/dd60c04e13f922676f2fd80189341ef2d9237ef4.  For specific requirements see https:\/\/www.intel.com\/content\/dam\/develop\/external\/us\/en\/documents\/catc17-introduction-intel-cet-844137.pdf or the CET chapter of Intel\u00ae 64 and IA-32 Architectures Software Developer\u2019s Manual.  I did not report this on https:\/\/github.com\/numpy\/SVML as it has no issue tracker.\n\n### Reproduce the code example:\n\n```python\nLDFLAGS='-Wl,-z,cet-report=error' python -m build --wheel --no-isolation -Csetup-args=\"-Dblas=cblas\" -Csetup-args=\"-Dlapack=lapack\"\n```\n\n\n### Error message:\n\n```shell\n[497\/498] Linking target numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so\r\nFAILED: numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so \r\nc++  -o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_arraytypes.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_einsum.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_einsum_sumprod.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_lowlevel_strided_loops.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_nditer_templ.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_scalartypes.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_loops.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_matmul.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/meson-generated_scalarmath.c.o ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acos_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acos_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acos_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acosh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acosh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_acosh_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asin_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asin_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asin_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asinh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asinh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_asinh_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan2_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan2_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan2_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atan_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atanh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atanh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_atanh_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cbrt_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cbrt_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cbrt_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cos_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cos_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cos_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cosh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cosh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_cosh_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp2_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp2_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp2_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_exp_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_expm1_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_expm1_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_expm1_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log10_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log10_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log10_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log1p_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log1p_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log1p_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log2_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log2_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log2_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_log_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_pow_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_pow_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_pow_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sin_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sin_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sin_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sinh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sinh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_sinh_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tan_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tan_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tan_d_ha.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tanh_d_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tanh_s_la.s ..\/..\/numpy\/core\/src\/umath\/svml\/linux\/avx512\/svml_z0_tanh_d_ha.s numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_abstractdtypes.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_alloc.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_arrayobject.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_array_coercion.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_array_method.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_array_assign_scalar.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_array_assign_array.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_arrayfunction_override.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_buffer.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_calculation.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_compiled_base.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_common.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_common_dtype.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_convert.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_convert_datatype.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_conversion_utils.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_ctors.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_datetime.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_datetime_strings.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_datetime_busday.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_datetime_busdaycal.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_descriptor.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_dlpack.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_dtypemeta.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_dragon4.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_dtype_transfer.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_dtype_traversal.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_experimental_public_dtype_api.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_flagsobject.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_getset.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_hashdescr.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_item_selection.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_iterators.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_legacy_dtype_implementation.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_mapping.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_methods.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_multiarraymodule.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_nditer_api.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_nditer_constr.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_nditer_pywrap.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_number.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_refcount.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_sequence.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_shape.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_scalarapi.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_strfuncs.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_temp_elide.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_typeinfo.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_usertypes.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_vdot.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_quicksort.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_mergesort.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_timsort.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_heapsort.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_radixsort.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_selection.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npysort_binsearch.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_conversions.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_field_types.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_growth.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_readtext.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_rows.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_stream_pyobject.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_str_to_int.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_multiarray_textreading_tokenize.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_npymath_arm64_exports.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_array_assign.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_mem_overlap.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_npy_argparse.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_npy_hashtable.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_npy_longdouble.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_ucsnarrow.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_ufunc_override.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_numpyos.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_npy_cpu_features.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_cblasfuncs.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_common_python_xerbla.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_ufunc_type_resolution.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_clip.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_dispatching.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_extobj.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_legacy_array_method.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_override.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_reduction.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_ufunc_object.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_umathmodule.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_string_ufuncs.cpp.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath_wrapping_array_method.c.o numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so.p\/src_umath__scaled_float_dtype.c.o -Wl,--as-needed -Wl,--allow-shlib-undefined -Wl,-O1 -shared -fPIC -Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now -flto=auto -Wl,-z,cet-report=error -march=x86-64 -mtune=generic -O2 -pipe -fno-plt -fexceptions -Wp,-D_FORTIFY_SOURCE=2 -fstack-clash-protection -fcf-protection -Wformat -Werror=format-security -Wp,-D_GLIBCXX_ASSERTIONS -g -ffile-prefix-map=\/build\/python-numpy\/src=\/usr\/src\/debug\/python-numpy -flto=auto -ffat-lto-objects -Wl,--start-group numpy\/core\/libnpymath.a numpy\/core\/lib_multiarray_umath_mtargets.a \/usr\/lib\/libcblas.so -Wl,--end-group\r\n..\/..\/numpy\/core\/src\/multiarray\/experimental_public_dtype_api.c:389:1: warning: type of \u2018PyUFunc_AddLoopFromSpec\u2019 does not match original declaration [-Wlto-type-mismatch]\r\n  389 | PyUFunc_AddLoopFromSpec(PyUFuncObject *ufunc, PyObject *info, int ignore_duplicate);\r\n      | ^\r\n..\/..\/numpy\/core\/src\/umath\/dispatching.c:154:1: note: type mismatch in parameter 3\r\n  154 | PyUFunc_AddLoopFromSpec(PyObject *ufunc, PyArrayMethod_Spec *spec)\r\n      | ^\r\n..\/..\/numpy\/core\/src\/umath\/dispatching.c:154:1: note: type \u2018void\u2019 should match type \u2018int\u2019\r\n..\/..\/numpy\/core\/src\/umath\/dispatching.c:154:1: note: \u2018PyUFunc_AddLoopFromSpec\u2019 was previously declared here\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.h:15:1: warning: type of \u2018arr_interp_complex\u2019 does not match original declaration [-Wlto-type-mismatch]\r\n   15 | arr_interp_complex(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);\r\n      | ^\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.c:667:1: note: type mismatch in parameter 5\r\n  667 | arr_interp_complex(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,\r\n      | ^\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.c:667:1: note: \u2018arr_interp_complex\u2019 was previously declared here\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.h:13:1: warning: type of \u2018arr_interp\u2019 does not match original declaration [-Wlto-type-mismatch]\r\n   13 | arr_interp(PyObject *, PyObject *const *, Py_ssize_t, PyObject *, PyObject *);\r\n      | ^\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.c:497:1: note: type mismatch in parameter 5\r\n  497 | arr_interp(PyObject *NPY_UNUSED(self), PyObject *const *args, Py_ssize_t len_args,\r\n      | ^\r\n..\/..\/numpy\/core\/src\/multiarray\/compiled_base.c:497:1: note: \u2018arr_interp\u2019 was previously declared here\r\n\/usr\/bin\/ld: \/tmp\/ccdesNTJ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc1AdOSy.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cclPkj1Q.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccPROftU.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccZcsBiE.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cchxKNoC.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc6RJeDn.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc4djqBJ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccnjTr5k.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccWovo3i.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc6bj2ev.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccrtRE3y.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccUWVVhv.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc6zPwzb.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccF4pYxP.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccKQ7Wfm.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccVvCbmC.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccG8crii.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccekIpup.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccaFudNQ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cclk12ZQ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc2J849w.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc1iLD1R.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cct0Jz6v.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccRnDONn.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccbCrjVs.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccwKfhtB.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cceKWOn4.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccQgNpvd.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccNl7wUH.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccnPS8mt.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccyHBNNE.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccIv0Gp1.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccpYsIHP.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccOh9mIQ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccxDwKHZ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccIcXIIS.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccWdLJQV.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccrXlQKb.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccYk7ged.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cchOzA5h.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccEtSdJB.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccjpU3B8.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cchbjt8w.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccNNbOym.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccx7J5Ka.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccrCwfxe.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccbpwazx.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc8HoZ5c.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccpc4fDs.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc4geSEV.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccncpsKo.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccPqNxBH.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccKrR1eP.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cceXnMkA.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccgLeIaP.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccYOkuj5.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc4jwQ6C.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccawrqPz.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc8mrCQY.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccf6MxzS.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccT7LwbJ.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccASmgX3.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccYl0HY1.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/cc3c0DQB.o: error: missing IBT and SHSTK properties\r\n\/usr\/bin\/ld: \/tmp\/ccucSyfa.o: error: missing IBT and SHSTK properties\r\ncollect2: error: ld returned 1 exit status\r\n[498\/498] Linking target numpy\/random\/_generator.cpython-311-x86_64-linux-gnu.so\r\nninja: build stopped: subcommand failed.\n```\n\n\n### Python and NumPy Versions:\n\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.26.3\r\n3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]\r\n```\n\n### Runtime Environment:\n\n```\r\n>>> import numpy; print(numpy.show_runtime())\r\n[{'numpy_version': '1.26.3',\r\n  'python': '3.11.6 (main, Nov 14 2023, 09:36:21) [GCC 13.2.1 20230801]',\r\n  'uname': uname_result(system='Linux', node='arch', release='6.6.12-1-stable', version='#1 SMP Fri, 19 Jan 2024 15:03:04 +0000', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL',\r\n                                    'AVX512_SPR']}}]\r\nNone\r\n```\n\n### Context for the issue:\n\nCan be worked around by disabling SVML with `-Csetup-args=\"-Ddisable-svml=true\"`\r\nSee also https:\/\/github.com\/numpy\/numpy\/issues\/24221 for how the issue can be produced at run time if LDFLAGS='-Wl,-z,cet-report=error' is not used to detect such issues at build time.","comments":[],"labels":["00 - Bug"]},{"title":"BUG: `sqrt(ma.masked)` returns non-masked array","body":"### Describe the issue:\r\n\r\nIn numpy 2.0 nightly, applying sqrt() to a fully masked scalar array undoes the mask.\r\nnumpy 1.26.3 behaves correctly.\r\nOther ufuncs seem to behave correctly (but I haven't performed a thorough test for all of them).\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> a = np.ma.masked_array(1, mask=True)\r\n>>> type(a)\r\nnumpy.ma.core.MaskedArray\r\n\r\n>>> np.sqrt(a)\r\n# numpy 2.0\r\nmasked_array(data=1.,\r\n             mask=False,\r\n             fill_value=1e+20)\r\n# numpy 1.26.3\r\nmasked_array(data=--,\r\n             mask=True,\r\n       fill_value=1e+20,\r\n            dtype=float64)\r\n\r\n>>> np.sqrt(np.ma.masked)\r\n# numpy 2.0\r\nmasked_array(data=0.,\r\n             mask=False,\r\n             fill_value=1e+20)\r\n# numpy 1.26.3\r\nmasked_array(data=--,\r\n             mask=True,\r\n       fill_value=1e+20,\r\n            dtype=float64)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\nCPython 3.12.1 x86_64\r\nnumpy 2.0.0.dev0+git20240113.d2f60ff\r\n\r\nfrom\r\npython -m pip install --no-deps --pre -i https:\/\/pypi.anaconda.org\/scientific-python-nightly-wheels\/simple numpy \r\nas of 2024-01-19\r\n\r\n\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["Weird, I think the reason why this is so cryptic, is that something fails with a `TypeError` while dealing with the `context`, and then it succeeds when not passing it.\r\n\r\n(Shows how `try\/except` clauses to be forgiving are not helpful sometimes...)","The error seems to be:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 np.sqrt(np.ma.masked)\r\n\r\nFile ~\/forks\/numpy\/build-install\/usr\/lib\/python3.10\/site-packages\/numpy\/ma\/core.py:6686, in MaskedConstant.__array_wrap__(self, obj, context, return_scalar)\r\n   6685 def __array_wrap__(self, obj, context=None, return_scalar=False):\r\n-> 6686     return self.view(MaskedArray).__array_wrap__(obj, context)\r\n\r\nFile ~\/forks\/numpy\/build-install\/usr\/lib\/python3.10\/site-packages\/numpy\/ma\/core.py:3131, in MaskedArray.__array_wrap__(self, obj, context, return_scalar)\r\n   3127 except KeyError:\r\n   3128     # Domain not recognized, use fill_value instead\r\n   3129     fill_value = self.fill_value\r\n-> 3131 np.copyto(result, fill_value, where=d)\r\n   3133 # Update the mask\r\n   3134 if m is nomask:\r\n\r\nTypeError: Cannot cast scalar from dtype('float64') to dtype('bool') according to the rule 'safe'\r\n```\r\n(Although now I have to look at my other code, I didn't want to see it at this stage ;)).","OK, the problem there is the `d`, which is *not* a valid mask (it is the float value).  That seems right (not sure when\/why it changed).","@seberg do you think this should be possible to fix in a weekend? If so I can take a look at it this weekend :) "],"labels":["00 - Bug","06 - Regression"]},{"title":"BUG: Complex printing tests fail on Windows ARM64","body":"### Describe the issue:\r\n\r\n6 out of 16 tests fail in `test_print.py` when running on Windows on ARM. They appear to all be related to complex numbers. If I've read the results correctly, it only affects `np.cdouble` and `np.clongdouble` and not `np.complex64`.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\npytest numpy\/_core\/tests\/test_print.py\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n__________ test_complex_types[complex128] __________\r\n\r\ntp = <class 'numpy.complex128'>\r\n\r\n    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_types(tp):\r\n        \"\"\"Check formatting of complex types.\r\n\r\n            This is only for the str function, and only for simple types.\r\n            The precision of np.float32 and np.longdouble aren't the same as the\r\n            python float precision.\r\n\r\n        \"\"\"\r\n        for x in [0, 1, -1, 1e20]:\r\n            assert_equal(str(tp(x)), str(complex(x)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\n            assert_equal(str(tp(x*1j)), str(complex(x*1j)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\n>           assert_equal(str(tp(x + x*1j)), str(complex(x + x*1j)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\nE           AssertionError:\r\nE           Items are not equal: Failed str formatting for type <class 'numpy.complex128'>\r\nE            ACTUAL: '(1+0j)'\r\nE            DESIRED: '(1+1j)'\r\n\r\ntp         = <class 'numpy.complex128'>\r\nx          = 1\r\n\r\nnumpy\\_core\\tests\\test_print.py:65: AssertionError\r\n__________ test_complex_types[clongdouble] __________\r\n\r\ntp = <class 'numpy.clongdouble'>\r\n\r\n    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_types(tp):\r\n        \"\"\"Check formatting of complex types.\r\n\r\n            This is only for the str function, and only for simple types.\r\n            The precision of np.float32 and np.longdouble aren't the same as the\r\n            python float precision.\r\n\r\n        \"\"\"\r\n        for x in [0, 1, -1, 1e20]:\r\n            assert_equal(str(tp(x)), str(complex(x)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\n            assert_equal(str(tp(x*1j)), str(complex(x*1j)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\n>           assert_equal(str(tp(x + x*1j)), str(complex(x + x*1j)),\r\n                         err_msg='Failed str formatting for type %s' % tp)\r\nE           AssertionError:\r\nE           Items are not equal: Failed str formatting for type <class 'numpy.clongdouble'>\r\nE            ACTUAL: '(1+0j)'\r\nE            DESIRED: '(1+1j)'\r\n\r\ntp         = <class 'numpy.clongdouble'>\r\nx          = 1\r\n\r\nnumpy\\_core\\tests\\test_print.py:65: AssertionError\r\n__________ test_complex_inf_nan[complex128] __________\r\n\r\ndtype = <class 'numpy.complex128'>\r\n\r\n    @pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_inf_nan(dtype):\r\n        \"\"\"Check inf\/nan formatting of complex types.\"\"\"\r\n        TESTS = {\r\n            complex(np.inf, 0): \"(inf+0j)\",\r\n            complex(0, np.inf): \"infj\",\r\n            complex(-np.inf, 0): \"(-inf+0j)\",\r\n            complex(0, -np.inf): \"-infj\",\r\n            complex(np.inf, 1): \"(inf+1j)\",\r\n            complex(1, np.inf): \"(1+infj)\",\r\n            complex(-np.inf, 1): \"(-inf+1j)\",\r\n            complex(1, -np.inf): \"(1-infj)\",\r\n            complex(np.nan, 0): \"(nan+0j)\",\r\n            complex(0, np.nan): \"nanj\",\r\n            complex(-np.nan, 0): \"(nan+0j)\",\r\n            complex(0, -np.nan): \"nanj\",\r\n            complex(np.nan, 1): \"(nan+1j)\",\r\n            complex(1, np.nan): \"(1+nanj)\",\r\n            complex(-np.nan, 1): \"(nan+1j)\",\r\n            complex(1, -np.nan): \"(1+nanj)\",\r\n        }\r\n        for c, s in TESTS.items():\r\n>           assert_equal(str(dtype(c)), s)\r\nE           AssertionError:\r\nE           Items are not equal:\r\nE            ACTUAL: '(inf+0j)'\r\nE            DESIRED: '(inf+1j)'\r\n\r\nTESTS      = {(inf+0j): '(inf+0j)', infj: 'infj', (-inf+0j): '(-inf+0j)', -infj: '-infj', ...}\r\nc          = (inf+1j)\r\ndtype      = <class 'numpy.complex128'>\r\ns          = '(inf+1j)'\r\n\r\nnumpy\\_core\\tests\\test_print.py:99: AssertionError\r\n__________ test_complex_inf_nan[clongdouble] __________\r\n\r\ndtype = <class 'numpy.clongdouble'>\r\n\r\n    @pytest.mark.parametrize('dtype', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_inf_nan(dtype):\r\n        \"\"\"Check inf\/nan formatting of complex types.\"\"\"\r\n        TESTS = {\r\n            complex(np.inf, 0): \"(inf+0j)\",\r\n            complex(0, np.inf): \"infj\",\r\n            complex(-np.inf, 0): \"(-inf+0j)\",\r\n            complex(0, -np.inf): \"-infj\",\r\n            complex(np.inf, 1): \"(inf+1j)\",\r\n            complex(1, np.inf): \"(1+infj)\",\r\n            complex(-np.inf, 1): \"(-inf+1j)\",\r\n            complex(1, -np.inf): \"(1-infj)\",\r\n            complex(np.nan, 0): \"(nan+0j)\",\r\n            complex(0, np.nan): \"nanj\",\r\n            complex(-np.nan, 0): \"(nan+0j)\",\r\n            complex(0, -np.nan): \"nanj\",\r\n            complex(np.nan, 1): \"(nan+1j)\",\r\n            complex(1, np.nan): \"(1+nanj)\",\r\n            complex(-np.nan, 1): \"(nan+1j)\",\r\n            complex(1, -np.nan): \"(1+nanj)\",\r\n        }\r\n        for c, s in TESTS.items():\r\n>           assert_equal(str(dtype(c)), s)\r\nE           AssertionError:\r\nE           Items are not equal:\r\nE            ACTUAL: '(inf+0j)'\r\nE            DESIRED: '(inf+1j)'\r\n\r\nTESTS      = {(inf+0j): '(inf+0j)', infj: 'infj', (-inf+0j): '(-inf+0j)', -infj: '-infj', ...}\r\nc          = (inf+1j)\r\ndtype      = <class 'numpy.clongdouble'>\r\ns          = '(inf+1j)'\r\n\r\nnumpy\\_core\\tests\\test_print.py:99: AssertionError\r\n__________test_complex_type_print[complex128] __________\r\n\r\ntp = <class 'numpy.complex128'>\r\n\r\n    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_type_print(tp):\r\n        \"\"\"Check formatting when using print \"\"\"\r\n        # We do not create complex with inf\/nan directly because the feature is\r\n        # missing in python < 2.6\r\n        for x in [0, 1, -1, 1e20]:\r\n            _test_redirected_print(complex(x), tp)\r\n\r\n        if tp(1e16).itemsize > 8:\r\n            _test_redirected_print(complex(1e16), tp)\r\n        else:\r\n            ref = '(1e+16+0j)'\r\n            _test_redirected_print(complex(1e16), tp, ref)\r\n\r\n>       _test_redirected_print(complex(np.inf, 1), tp, '(inf+1j)')\r\n\r\ntp         = <class 'numpy.complex128'>\r\nx          = 1e+20\r\n\r\nnumpy\\_core\\tests\\test_print.py:152:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nx = (inf+1j), tp = <class 'numpy.complex128'>, ref = '(inf+1j)'\r\n\r\n    def _test_redirected_print(x, tp, ref=None):\r\n        file = StringIO()\r\n        file_tp = StringIO()\r\n        stdout = sys.stdout\r\n        try:\r\n            sys.stdout = file_tp\r\n            print(tp(x))\r\n            sys.stdout = file\r\n            if ref:\r\n                print(ref)\r\n            else:\r\n                print(x)\r\n        finally:\r\n            sys.stdout = stdout\r\n\r\n>       assert_equal(file.getvalue(), file_tp.getvalue(),\r\n                     err_msg='print failed for type%s' % tp)\r\nE       AssertionError:\r\nE       Items are not equal: print failed for type<class 'numpy.complex128'>\r\nE        ACTUAL: '(inf+1j)\\n'\r\nE        DESIRED: '(inf+0j)\\n'\r\n\r\nfile       = <_io.StringIO object at 0x000001FEED9B0640>\r\nfile_tp    = <_io.StringIO object at 0x000001FEED9B0280>\r\nref        = '(inf+1j)'\r\nstdout     = <_io.TextIOWrapper name='<tempfile._TemporaryFileWrapper object at 0x000001FEEAA7FA40>' mode='r+' encoding='utf-8'>\r\ntp         = <class 'numpy.complex128'>\r\nx          = (inf+1j)\r\n\r\nnumpy\\_core\\tests\\test_print.py:118: AssertionError\r\n__________ test_complex_type_print[clongdouble] __________\r\n\r\ntp = <class 'numpy.clongdouble'>\r\n\r\n    @pytest.mark.parametrize('tp', [np.complex64, np.cdouble, np.clongdouble])\r\n    def test_complex_type_print(tp):\r\n        \"\"\"Check formatting when using print \"\"\"\r\n        # We do not create complex with inf\/nan directly because the feature is\r\n        # missing in python < 2.6\r\n        for x in [0, 1, -1, 1e20]:\r\n            _test_redirected_print(complex(x), tp)\r\n\r\n        if tp(1e16).itemsize > 8:\r\n            _test_redirected_print(complex(1e16), tp)\r\n        else:\r\n            ref = '(1e+16+0j)'\r\n            _test_redirected_print(complex(1e16), tp, ref)\r\n\r\n>       _test_redirected_print(complex(np.inf, 1), tp, '(inf+1j)')\r\n\r\ntp         = <class 'numpy.clongdouble'>\r\nx          = 1e+20\r\n\r\nnumpy\\_core\\tests\\test_print.py:152:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nx = (inf+1j), tp = <class 'numpy.clongdouble'>, ref = '(inf+1j)'\r\n\r\n    def _test_redirected_print(x, tp, ref=None):\r\n        file = StringIO()\r\n        file_tp = StringIO()\r\n        stdout = sys.stdout\r\n        try:\r\n            sys.stdout = file_tp\r\n            print(tp(x))\r\n            sys.stdout = file\r\n            if ref:\r\n                print(ref)\r\n            else:\r\n                print(x)\r\n        finally:\r\n            sys.stdout = stdout\r\n\r\n>       assert_equal(file.getvalue(), file_tp.getvalue(),\r\n                     err_msg='print failed for type%s' % tp)\r\nE       AssertionError:\r\nE       Items are not equal: print failed for type<class 'numpy.clongdouble'>\r\nE        ACTUAL: '(inf+1j)\\n'\r\nE        DESIRED: '(inf+0j)\\n'\r\n\r\nfile       = <_io.StringIO object at 0x000001FEED9B0C40>\r\nfile_tp    = <_io.StringIO object at 0x000001FEED9B0B80>\r\nref        = '(inf+1j)'\r\nstdout     = <_io.TextIOWrapper name='<tempfile._TemporaryFileWrapper object at 0x000001FEEAA7FA40>' mode='r+' encoding='utf-8'>\r\ntp         = <class 'numpy.clongdouble'>\r\nx          = (inf+1j)\r\n\r\nnumpy\\_core\\tests\\test_print.py:118: AssertionError\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n2.0.0.dev0+git20240118.a7c6be5\r\n3.12.1 (tags\/v3.12.1:2305ca5, Dec  7 2023, 22:12:47) [MSC v.1937 64 bit (ARM64)]\r\n\r\n\r\n### Runtime Environment:\r\n\r\n```\r\n[{'numpy_version': '2.0.0.dev0+git20240118.a7c6be5',\r\n  'python': '3.12.1 (tags\/v3.12.1:2305ca5, Dec  7 2023, 22:12:47) [MSC v.1937 '\r\n            '64 bit (ARM64)]',\r\n  'uname': uname_result(system='Windows', node='mars', release='11', version='10.0.22621', machine='ARM64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': [],\r\n                      'not_found': []}}]\r\nNone\r\n```\r\n(PS, this function prints already, so you can remove `print` from the PR template.)\r\n\r\n### Context for the issue:\r\n\r\nI've started working on building Matplotlib on Windows on Arm.","comments":["Missed that `numpy\/polynomial\/tests\/test_printing.py::test_complex_coefficients` also fails the same way:\r\n```\r\n__________ test_complex_coefficients __________\r\n\r\n    def test_complex_coefficients():\r\n        \"\"\"Test both numpy and built-in complex.\"\"\"\r\n        coefs = [0+1j, 1+1j, -2+2j, 3+0j]\r\n        # numpy complex\r\n        p1 = poly.Polynomial(coefs)\r\n        # Python complex\r\n        p2 = poly.Polynomial(array(coefs, dtype=object))\r\n        poly.set_default_printstyle('unicode')\r\n>       assert_equal(str(p1), \"1j + (1+1j)\u00b7x - (2-2j)\u00b7x\u00b2 + (3+0j)\u00b7x\u00b3\")\r\nE       AssertionError:\r\nE       Items are not equal:\r\nE        ACTUAL: '1j + (1+0j)\u00b7x - (2+0j)\u00b7x\u00b2 + (3+0j)\u00b7x\u00b3'\r\nE        DESIRED: '1j + (1+1j)\u00b7x - (2-2j)\u00b7x\u00b2 + (3+0j)\u00b7x\u00b3'\r\n\r\ncoefs      = [1j, (1+1j), (-2+2j), (3+0j)]\r\np1         = Polynomial([ 0.+1.j,  1.+1.j, -2.+2.j,  3.+0.j], domain=[-1,  1], window=[-1,  1], symbol='x')\r\np2         = Polynomial([1j, (1+1j), (-2+2j), (3+0j)], dtype=object, domain=[-1,  1], window=[-1,  1], symbol='x')\r\n\r\nnumpy\\polynomial\\tests\\test_printing.py:251: AssertionError\r\n```","Oops, missed one more in `numpy\/_core\/tests\/test_arrayprint.py::TestPrintOptions::test_legacy_mode_scalars`, but it's more of the same:\r\n```\r\n__________ TestPrintOptions.test_legacy_mode_scalars __________\r\n\r\nself = <numpy._core.tests.test_arrayprint.TestPrintOptions object at 0x0000027CF6D3B1D0>\r\n\r\n    def test_legacy_mode_scalars(self):\r\n        # in legacy mode, str of floats get truncated, and complex scalars\r\n        # use * for non-finite imaginary part\r\n        np.set_printoptions(legacy='1.13')\r\n        assert_equal(str(np.float64(1.123456789123456789)), '1.12345678912')\r\n        assert_equal(str(np.complex128(complex(1, np.nan))), '(1+nan*j)')\r\n\r\n        np.set_printoptions(legacy=False)\r\n        assert_equal(str(np.float64(1.123456789123456789)),\r\n                     '1.1234567891234568')\r\n>       assert_equal(str(np.complex128(complex(1, np.nan))), '(1+nanj)')\r\nE       AssertionError:\r\nE       Items are not equal:\r\nE        ACTUAL: '(1+0j)'\r\nE        DESIRED: '(1+nanj)'\r\n\r\nself       = <numpy._core.tests.test_arrayprint.TestPrintOptions object at 0x0000027CF6D3B1D0>\r\n\r\nnumpy\\_core\\tests\\test_arrayprint.py:895: AssertionError\r\n\r\n```","Or even more simply:\r\n```\r\n$ python\r\nPython 3.12.1 (tags\/v3.12.1:2305ca5, Dec  7 2023, 22:12:47) [MSC v.1937 64 bit (ARM64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> np.complex64(1, 2)\r\nnp.complex64(1+2j)\r\n>>> np.complex128(1, 2)\r\nnp.complex128(1+0j)\r\n```\r\neven though:\r\n```\r\n>>> val = np.complex64(1, 2)\r\n>>> val.real\r\nnp.float32(1.0)\r\n>>> val.imag\r\nnp.float32(2.0)\r\n>>> val = np.complex128(1, 2)\r\n>>> val.real\r\nnp.float64(1.0)\r\n>>> val.imag\r\nnp.float64(2.0)\r\n>>>\r\n```\r\nso this is specifically about printing.","I did some printf debugging on a arm64 windows VM and I don't think this is necessarily printing specific. In particular, if I put a `printf` inside `npy_cimag` like so:\r\n\r\n```\r\ndiff --git a\/numpy\/_core\/include\/numpy\/npy_math.h b\/numpy\/_core\/include\/numpy\/npy_math.h\r\nindex 216b173fde..c532eb963b 100644\r\n--- a\/numpy\/_core\/include\/numpy\/npy_math.h\r\n+++ b\/numpy\/_core\/include\/numpy\/npy_math.h\r\n@@ -372,6 +372,7 @@ static inline void npy_csetreal(npy_cdouble *z, const double r)\r\n\r\n static inline double npy_cimag(const npy_cdouble z)\r\n {\r\n+    printf(\"z: %f, %f\", ((double *) &z)[0], ((double *) &z)[1]);\r\n     return ((double *) &z)[1];\r\n }\r\n```\r\n\r\nI see in my terminal output when I execute `np.complex128(1, 2)` that the imagingary part is zero in the `npy_cdouble` struct.\r\n\r\nIt looks like `val.imag` uses a different code path via `gentype_imag_get` that interprets the data in the imaginary part as a float and goes through the float scalar repr machinery and somehow that ends up with the correct value.\r\n\r\nThis is reaching the edge of my windows knowledge.","Ping @lysnikolaou, maybe this is related to the complex number refactoring?","This maybe related to issues with x64-translation.\r\nI noticed that on WoA pip tries to compile numpy for me.\r\nPerhaps the OP is running an x64 version.\r\n\r\nI just opened https:\/\/github.com\/numpy\/numpy\/issues\/25858 which is somewhat related.","No, I am running entirely native only."],"labels":["00 - Bug"]},{"title":"Deprecating in-place operations where the out-of-place equivalent would upcast?","body":"In-place operators typically behave the same as their out-of-place equivalents (excluding what happens with views). For example, if `x` and `y` are  `float64` arrays, then `x += y` and `x = x + y` are equivalent statements. However, something unexpected may happen when the right-hand operand has a dtype that would normally cause upcasting when combined with the left-hand operand according to NumPy's casting rules. In such cases, the in-place version doesn't upcast the left-hand operand, but downcasts the right-hand operand. To illustrate the difference:\r\n\r\n```python\r\n>>> # The in-place version:\r\n>>> a = np.array(1, dtype=np.int8)\r\n>>> a += np.array(2**12, dtype=np.int16)\r\n>>> a\r\narray(1, dtype=int8)\r\n\r\n>>> # The out-of-place version:\r\n>>> a + np.array(2**12, dtype=np.int16)\r\n4097\r\n>>> (a + np.array(2**12, dtype=np.int16)).dtype\r\ndtype('int16')\r\n```\r\nThis yields silently incorrect results, and seems undesirable.\r\n\r\nIt is already forbidden to do this with dtypes that aren't the same kind:\r\n```python\r\n>>> a += np.array(2**12, dtype=np.float16)\r\n...\r\nUFuncTypeError: Cannot cast ufunc 'add' output from dtype('float16') to dtype('int8') with casting rule 'same_kind'\r\n```\r\n\r\nIt seems like it would be safer if the behavior would only allow `'safe'` rather than `'same_kind'` casting. Making that change would avoid the silently incorrect results, and recover the property that in-place and out-of-place statements act the same.\r\n\r\nThis came up in the review of NEP 56 (gh-25542). We decided to punt on it there, since it's a little unclear how impactful deprecating unsafe casts are, and there is no hurry in making this change. It does seem desirable though.\r\n\r\nThe right course of action here is probably to implement this change in a branch, and then seeing what fails in test suites of downstream projects to assess the impact.","comments":[],"labels":["component: numpy._core","07 - Deprecation"]},{"title":"DOC: f2py needs to explain fortran vs python differences in parameter lists","body":"### Issue with current documentation:\r\n\r\nhttps:\/\/numpy.org\/doc\/stable\/f2py does not seem to state that (i)  multiple intent(out) parameters should be given as a list at the python level, and (ii) array sizes explicitly listed in the fortran subroutine have to be either ignored or shifted to the end of the python parameter list.\r\n\r\n### Idea or request for content:\r\n\r\nThe documentation of `f2py` would be clearer if there were examples illustrating how (i) multiple `intent(out)` fortran parameters can be given as a python `list` left-hand value, and probably more importantly (ii) array sizes defined in the fortran subroutine e.g. with `dimension(n)` should either be ignored at the python level or optionally listed after the array parameters. Although case (i) for a single `intent(out)` parameter is given, I couldn't find the case of multiple `intent(out)` parameters or array sizes anywhere in the [main f2py web documentation](https:\/\/numpy.org\/doc\/stable\/f2py).\r\n\r\nWith `python3 3.11.2-1+b1` `python3-numpy 1:1.24.2-1` on Debian\/bookworm, the following fortran and python program illustrate (i) and (ii). Compiling and running either the fortran program alone with `gfortran 12.2.0-14` or compiling with `f2py` give return values of 0 to the shell for me, i.e. both pass all checks.\r\n\r\nCase (ii) seems to be a feature, not a bug - since the python calling procedure has to know the size of the array before feeding it to the fortran side, the size does not have to be stated. But it would be useful to have it in the main documentation rather than have users discover it by trial and error. :)\r\n\r\n````\r\n! learn_f2py - simple program for checking how\/if f2py works - fortran backend\r\n!\r\n!     Copyright (C) 2024 Boud Roukema\r\n!     All rights reserved.\r\n! \r\n! Redistribution and use in source and binary forms, with or without\r\n! modification, are permitted provided that the following conditions are\r\n! met:\r\n! \r\n!     * Redistributions of source code must retain the above copyright\r\n!        notice, this list of conditions and the following disclaimer.\r\n! \r\n!     * Redistributions in binary form must reproduce the above\r\n!        copyright notice, this list of conditions and the following\r\n!        disclaimer in the documentation and\/or other materials provided\r\n!        with the distribution.\r\n! \r\n!     * Neither the name of the NumPy Developers nor the names of any\r\n!        contributors may be used to endorse or promote products derived\r\n!        from this software without specific prior written permission.\r\n! \r\n! THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\r\n! \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\r\n! LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\r\n! A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\r\n! OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\r\n! SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\r\n! LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\r\n! DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\r\n! THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\r\n! (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n! OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n!------------------------------------\r\n      program learn_f2py\r\n      integer :: k\r\n      integer, dimension(3) :: j_array\r\n\r\n      m_expected = 720\r\n\r\n      k = 55\r\n      j_size = 3\r\n      j_array(2) = 845\r\n      \r\n      call get_fact(k, j_size, j_array, m_result, mplus9)\r\n      if(m_result .eq. m_expected)then\r\n         i_pass = 0\r\n      else\r\n         i_pass = 1\r\n      endif\r\n      write(6,'(a,i10)')'i_pass = ',i_pass\r\n\r\n#ifdef __GFORTRAN__\r\n      stop(i_pass)\r\n#endif\r\n      end\r\n\r\n      subroutine get_fact(k, j_size, j_array, m_result, mplus9)\r\n!f2py integer, intent(out) :: m_result, mplus9\r\n!f2py integer, intent(in) :: k, j_size\r\n      integer, dimension(j_size) :: j_array\r\n!f2py intent(in) :: j_array\r\n! implicit typing\r\n      m_result=1                   ! to hold the result; override input value\r\n      n=6\r\n      do i=1,n\r\n         !m_result = m_result*i\r\n         m_result = mult(m_result,i)\r\n      enddo\r\n\r\n      nine = (k + j_array(2))\/100 ! should be (55 + 845)\/100 = 9\r\n      \r\n      mplus9 = m_result + nine ! illustrate how to get a second return value\r\n      write(6,'(a,i10,a,i10)')'learn_f2py: ',n,'!  = ',m_result\r\n      write(6,'(a,i10,a,i10)')'learn_f2py: ',n,'! + 9  = ',mplus9\r\n\r\n      return\r\n      end\r\n\r\n      function mult(m,i)\r\n!f2py intent(in) m,i\r\n      mult = m*i\r\n!      i=i+1 ! modifying an input value is allowed in fortran, but unwise \r\n      return\r\n      end\r\n````\r\n\r\n````\r\n# ! learn_f2py - simple program for checking how\/if f2py works - python frontend\r\n#\r\n#     Copyright (C) 2024 Boud Roukema\r\n#     All rights reserved.\r\n# \r\n# Redistribution and use in source and binary forms, with or without\r\n# modification, are permitted provided that the following conditions are\r\n# met:\r\n# \r\n#     * Redistributions of source code must retain the above copyright\r\n#        notice, this list of conditions and the following disclaimer.\r\n# \r\n#     * Redistributions in binary form must reproduce the above\r\n#        copyright notice, this list of conditions and the following\r\n#        disclaimer in the documentation and\/or other materials provided\r\n#        with the distribution.\r\n# \r\n#     * Neither the name of the NumPy Developers nor the names of any\r\n#        contributors may be used to endorse or promote products derived\r\n#        from this software without specific prior written permission.\r\n# \r\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\r\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\r\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\r\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\r\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\r\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\r\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\r\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\r\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\r\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n#------------------------------------\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\n# import the .so shared object library that includes the subroutine get_fact()\r\nimport learn_f2py\r\n\r\nprint(learn_f2py.__doc__)\r\n\r\nprint(\"Python frontend starting ...\")\r\n# implicit declarations, python style\r\nm_expected = 720\r\n\r\nk = 55\r\n#j_array = (45, 845, 1845) # python list object\r\nj_array = np.array([45, 845, 1845]) # numpy array object\r\nj_size = 3\r\n\r\n# Although the fortran subroutine is declared to have five parameters,\r\n# one of them is an array size, and two of them are outputs; so the\r\n# python frontend gives the two outputs as the left-hand side of the\r\n# call to the subroutine, and can choose to *not* inform the fortran\r\n# subroutine of the array size via j_size:\r\nm_result_py, mplus9_py = learn_f2py.get_fact(k, j_array)\r\n\r\nprint(\"python: m_result_py, mplus9_py = \", m_result_py,mplus9_py)\r\n\r\nif(m_result_py == m_expected):\r\n    pass_fail_code = 0\r\nelse:\r\n    pass_fail_code = 1\r\n\r\n# This time we will explicitly feed the array size to the fortran subroutine:\r\nm_result_py, mplus9_py = learn_f2py.get_fact(k, j_array, j_size)\r\n\r\nprint(\"python: m_result_py, mplus9_py = \", m_result_py,mplus9_py)\r\n\r\nif(m_result_py == m_expected):\r\n    pass_fail_code += 0\r\nelse:\r\n    pass_fail_code += 2\r\n    \r\nprint(\"pass_fail_code = \",pass_fail_code)\r\n\r\nsys.exit(pass_fail_code)\r\n````\r\n\r\n\r\nI licensed these with the standard Numpy BSD-3-clause licence, so that anyone is welcome to copy\/paste\/edit into Numpy documentation if desired under ([the default Numpy LICENSE.txt](https:\/\/github.com\/numpy\/numpy\/blob\/main\/LICENSE.txt), i.e. for licensing purposes, I would be considered one of the Numpy developers for this minute contribution).\r\n","comments":["BUG? Should the error that is given if the array size parameter is given explicitly by the python side be considered a bug? My feeling is yes, so I could split it out if others agree. To see this with the example above, replace\r\n````\r\nm_result_py, mplus9_py = learn_f2py.get_fact(k, j_array, j_size)\r\n````\r\nby\r\n````\r\nm_result_py, mplus9_py = learn_f2py.get_fact(k, j_size, j_array)\r\n````\r\nand recompile and run. This gives me the error\r\n````\r\nTraceback (most recent call last):\r\n  File \"\/MYPATH\/learn_f2py.py\", line 53, in <module>\r\n    m_result_py, mplus9_py = learn_f2py.get_fact(k, j_size, j_array)\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nlearn_f2py.error: (shape(j_array, 0) == j_size) failed for 1st keyword j_size: get_fact:j_size=45\r\n````\r\n\r\nIf it's not considered a bug, then it should be made very clear in the documentation, i.e. that if the user wishes to pass the array size from python to fortran, then it must be listed after the array itself in the python parameter list, even if it's listed earlier in the fortran ordering of the list of parameters.\r\n","Thank you for the code samples, would you be willing to open a PR with additions to the documentation? The best approach would be a narrative document, perhaps in the advanced examples section?\r\n\r\nAs for the bug, could you split that into a separate issue? I do believe it does seem like a bug since it is very surprising behavior, however if it has been around long enough changing it would be difficult (it would break a lot of downstream packages if the argument order is shifted).","@HaoZeke   I split out the bug: https:\/\/github.com\/numpy\/numpy\/issues\/25728 with a minimum working example.\r\n\r\nI'm unlikely to have the time to submit a merge request to add my code to the docs, sorry. I deliberately stated the licence to remove any worries about licence compatibility for someone with the time to do an MR. :)\r\n"],"labels":["04 - Documentation","component: numpy.f2py"]},{"title":"BUG: `np.take` cannot deal with 64-bit indices on 32-bit platforms","body":"This looks like a bug, because regular indexing is able to deal with the exact same situation. Running these test cases, the ones where the second argument has `int64` dtype fail in a 32-bit Linux Docker container:\r\n\r\n```python\r\ndef test_take_32_64():\r\n    x32 = np.ones(2, dtype=np.int32)\r\n    x64 = np.ones(2, dtype=np.int64)\r\n    np.take(x32, x64)\r\n\r\ndef test_take_64_32():\r\n    x32 = np.ones(2, dtype=np.int32)\r\n    x64 = np.ones(2, dtype=np.int64)\r\n    np.take(x64, x32)\r\n\r\ndef test_take_32_32():\r\n    x32 = np.ones(2, dtype=np.int32)\r\n    np.take(x32, x32)\r\n\r\ndef test_take_64_64():\r\n    x64 = np.ones(2, dtype=np.int64)\r\n    np.take(x64, x64)\r\n```\r\n\r\nFull tracebacks:\r\n\r\n<details>\r\n\r\n```\r\n_______________________________ test_take_32_64 ________________________________\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:57: in _wrapfunc\r\n    return bound(*args, **kwds)\r\nE   TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n        args       = (array([1, 1], dtype=int64),)\r\n        bound      = <built-in method take of numpy.ndarray object at 0xe28f8aa0>\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1])\r\nDuring handling of the above exception, another exception occurred:\r\nscipy\/_lib\/tests\/test__util.py:25: in test_take_32_64\r\n    np.take(x32, x64)\r\n        x32        = array([1, 1])\r\n        x64        = array([1, 1], dtype=int64)\r\n<__array_function__ internals>:180: in take\r\n    ???\r\n        args       = (array([1, 1]), array([1, 1], dtype=int64))\r\n        dispatcher = <function _take_dispatcher at 0xeed66a00>\r\n        implementation = <function take at 0xeed66a90>\r\n        kwargs     = {}\r\n        public_api = <function take at 0xeed66ad8>\r\n        relevant_args = (array([1, 1]), None)\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:190: in take\r\n    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\r\n        a          = array([1, 1])\r\n        axis       = None\r\n        indices    = array([1, 1], dtype=int64)\r\n        mode       = 'raise'\r\n        out        = None\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:66: in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n        args       = (array([1, 1], dtype=int64),)\r\n        bound      = <built-in method take of numpy.ndarray object at 0xe28f8aa0>\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1])\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:43: in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nE   TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n        args       = (array([1, 1], dtype=int64),)\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1])\r\n        wrap       = <built-in method __array_wrap__ of numpy.ndarray object at 0xe28f8aa0>\r\n\r\n_______________________________ test_take_64_64 ________________________________\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:57: in _wrapfunc\r\n    return bound(*args, **kwds)\r\nE   TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n        args       = (array([1, 1], dtype=int64),)\r\n        bound      = <built-in method take of numpy.ndarray object at 0xdff33f20>\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1], dtype=int64)\r\nDuring handling of the above exception, another exception occurred:\r\nscipy\/_lib\/tests\/test__util.py:38: in test_take_64_64\r\n    np.take(x64, x64)\r\n        x64        = array([1, 1], dtype=int64)\r\n<__array_function__ internals>:180: in take\r\n    ???\r\n        args       = (array([1, 1], dtype=int64), array([1, 1], dtype=int64))\r\n        dispatcher = <function _take_dispatcher at 0xeed66a00>\r\n        implementation = <function take at 0xeed66a90>\r\n        kwargs     = {}\r\n        public_api = <function take at 0xeed66ad8>\r\n        relevant_args = (array([1, 1], dtype=int64), None)\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:190: in take\r\n    return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\r\n        a          = array([1, 1], dtype=int64)\r\n        axis       = None\r\n        indices    = array([1, 1], dtype=int64)\r\n        mode       = 'raise'\r\n        out        = None\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:66: in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n        args       = (array([1, 1], dtype=int64),)\r\n        bound      = <built-in method take of numpy.ndarray object at 0xdff33f20>\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1], dtype=int64)\r\n\/scipy\/test\/lib\/python3.9\/site-packages\/numpy\/core\/fromnumeric.py:43: in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\nE   TypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n        args       = (array([1, 1], dtype=int64),)\r\n        kwds       = {'axis': None, 'mode': 'raise', 'out': None}\r\n        method     = 'take'\r\n        obj        = array([1, 1], dtype=int64)\r\n        wrap       = <built-in method __array_wrap__ of numpy.ndarray object at 0xdff33f20>\r\n=========================== short test summary info ============================\r\nFAILED scipy\/_lib\/tests\/test__util.py::test_take_32_64 - TypeError: Cannot ca...\r\nFAILED scipy\/_lib\/tests\/test__util.py::test_take_64_64 - TypeError: Cannot ca..\r\n```\r\n\r\n<\/details>\r\n\r\nIt's expected that `np.take` has parity here with indexing, and accepts `int64` dtype input also on 32-bit platforms. I came across this in SciPy code, where trying to replace indexing with `take` usage failed unexpectedly. It was code like this:\r\n```python\r\ndef _kpoints(data, k, rng):\r\n    idx = rng.choice(data.shape[0], size=int(k), replace=False)\r\n    return np.take(data, idx, axis=0)\r\n```\r\nThere, `idx` is always 64-bit, and this is a fairly common way of generating indices, so it's cumbersome to have to cast `idx` to `intp` dtype.","comments":["Indexing just uses same-kind casting and accepts that you might get wrong results for strange values.  That would be an OK change here, I think."],"labels":["00 - Bug","component: numpy._core"]},{"title":"DOC: See if we should make a note of `__array_wrap__(..., return_scalar=True)` in subclassing docs","body":"### Describe the issue:\n\nI expected reductions of subclasses of ndarray to return the same type as reductions of ndarray itself.  For example, in the snippet shown below, I the `.sum()` and `.std()` of an integer-valued ndarray have type `numpy.int64` and `numpy.float64`.  But, the same reductions of a subclass instead give values wrapped by the subclass, instead of the 'bare' data types.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\na = np.arange(10)\r\nprint(type(a.sum()))\r\n# <class 'numpy.int64'>\r\nprint(type(a.std()))\r\n# <class 'numpy.float64'>\r\n\r\nclass RealisticInfoArray(np.ndarray):\r\n    # Just lifted from\r\n    # https:\/\/numpy.org\/doc\/stable\/user\/basics.subclassing.html\r\n\r\n    def __new__(cls, input_array, info=None):\r\n        # Input array is an already formed ndarray instance\r\n        # We first cast to be our class type\r\n        obj = np.asarray(input_array).view(cls)\r\n        # add the new attribute to the created instance\r\n        obj.info = info\r\n        # Finally, we must return the newly created object:\r\n        return obj\r\n\r\n    def __array_finalize__(self, obj):\r\n        # see InfoArray.__array_finalize__ for comments\r\n        if obj is None: return\r\n        self.info = getattr(obj, 'info', None)\r\n\r\nb = RealisticInfoArray(a)\r\nprint(type(b.sum()))\r\n# <class '__main__.RealisticInfoArray'>\r\nprint(type(b.std()))\r\n# <class '__main__.RealisticInfoArray'>\r\n\r\n# Here I was expecting, as above\r\n# <class 'numpy.int64'>\r\n# <class 'numpy.float64'>\r\n# or even maybe int and float, respectively.\n```\n\n\n### Error message:\n\n_No response_\n\n### Python and NumPy Versions:\n\n```\r\n1.24.4\r\n3.9.13 (main, Aug 25 2022, 18:29:29) \r\n[Clang 12.0.0 ]\r\n```\n\n### Runtime Environment:\n\nI think this is not what was hoped for, and may be an issue unto itself, but that's for a different time.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n\/var\/folders\/xs\/n80hfp_j4zx4z6gfqjf05wwr0000gp\/T\/ipykernel_19584\/667756739.py in <module>\r\n----> 1 import numpy; print(numpy.show_runtime())\r\n\r\n~\/.local\/lib\/python3.9\/site-packages\/numpy\/lib\/utils.py in show_runtime()\r\n     86     try:\r\n     87         from threadpoolctl import threadpool_info\r\n---> 88         config_found.extend(threadpool_info())\r\n     89     except ImportError:\r\n     90         print(\"WARNING: `threadpoolctl` not found in system!\"\r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in threadpool_info()\r\n    122     In addition, each module may contain internal_api specific entries.\r\n    123     \"\"\"\r\n--> 124     return _ThreadpoolInfo(user_api=_ALL_USER_APIS).todicts()\r\n    125 \r\n    126 \r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in __init__(self, user_api, prefixes, modules)\r\n    338 \r\n    339             self.modules = []\r\n--> 340             self._load_modules()\r\n    341             self._warn_if_incompatible_openmp()\r\n    342         else:\r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in _load_modules(self)\r\n    369         \"\"\"Loop through loaded libraries and store supported ones\"\"\"\r\n    370         if sys.platform == \"darwin\":\r\n--> 371             self._find_modules_with_dyld()\r\n    372         elif sys.platform == \"win32\":\r\n    373             self._find_modules_with_enum_process_module_ex()\r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in _find_modules_with_dyld(self)\r\n    426 \r\n    427             # Store the module if it is supported and selected\r\n--> 428             self._make_module_from_path(filepath)\r\n    429 \r\n    430     def _find_modules_with_enum_process_module_ex(self):\r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in _make_module_from_path(self, filepath)\r\n    513             if prefix in self.prefixes or user_api in self.user_api:\r\n    514                 module_class = globals()[module_class]\r\n--> 515                 module = module_class(filepath, prefix, user_api, internal_api)\r\n    516                 self.modules.append(module)\r\n    517 \r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in __init__(self, filepath, prefix, user_api, internal_api)\r\n    604         self.internal_api = internal_api\r\n    605         self._dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\r\n--> 606         self.version = self.get_version()\r\n    607         self.num_threads = self.get_num_threads()\r\n    608         self._get_extra_info()\r\n\r\n~\/opt\/anaconda3\/lib\/python3.9\/site-packages\/threadpoolctl.py in get_version(self)\r\n    644                              lambda: None)\r\n    645         get_config.restype = ctypes.c_char_p\r\n--> 646         config = get_config().split()\r\n    647         if config[0] == b\"OpenBLAS\":\r\n    648             return config[1].decode(\"utf-8\")\r\n\r\nAttributeError: 'NoneType' object has no attribute 'split'\r\n```\n\n### Context for the issue:\n\nThis causes my software to serialize the resulting reductions in an unexpected, erroneous way.\r\n\r\nIf the behavior is intended, the documentation should explain how to remove the subclass type to coax plain data upon reduction.\r\n\r\nIf the behavior is not intended, it should be fixed.  Unfortunately, a change to the example that addresses my issue has proved beyond my tinkering because of numpy's unusual `__new__` and `__array_finalize__` approach.","comments":["You can implement `__array_wrap__` to convrt to scalar, but that is just guessing until NumPy 2.0 comes out where I want to add `return_scalar=False` to array-wrap so that you can know what to do.\r\n\r\nConversion to scalar can be done via `arr[()]`.  Beyond digging even deeper into `__array_ufunc__`, I can only say: should be fixed in 2.0 and I can't do more because while this is what you want, other subclasses would lose vital information if the default changed here.","Understood.  In that case I would advocate a fix of this non-bug to be an explanation of all of that in the documentation, and a working example of how to get the expected-to-me behavior."],"labels":["04 - Documentation"]},{"title":"TST: np.take out dtype","body":"Fixes gh-25588, gh-16319, gh-22766, and gh-21676\r\n\r\nI'm a numpy newbie. While this PR adds a test case and makes the added test pass, I don't fully see all the consequences of my changes, in particular how `PyArray_FromArray` works.","comments":["@charris I first wanted the CI to finish to prove that the test triggers a failure."],"labels":["05 - Testing"]},{"title":"DOC: remove `numpy.distutils` usages from user-facing docs","body":"`numpy.distutils` has been deprecated for a while, and there are better alternatives now. NumPy itself has switched to Meson. The `f2py` docs go above and beyond already and show how to use Meson, CMake and scikit-build: https:\/\/numpy.org\/doc\/stable\/f2py\/buildtools\/index.html#build-systems.\r\n\r\nHere is one example that is still using `numpy.distutils`: https:\/\/numpy.org\/doc\/stable\/user\/c-info.ufunc-tutorial.html. There may be others. It'd be fine to keep some docs around that use `setuptools` instead of `numpy.distutils`. The docs should probably simply use Meson in most places, and in cases that are more critical (like for `f2py`) mention or show that both Meson\/meson-python and CMake\/scikit-build-core are good options.","comments":[],"labels":["17 - Task","04 - Documentation","component: numpy.distutils","component: documentation"]},{"title":"BUG: ma.power unexpectedly shrinks masks, but other operators don't","body":"### Describe the issue:\n\nThis might be better described as \"unexpected behavior.\" If you have an masked ndarray whose mask if comprised of only `False` values, the power operation will shrink the mask array down to a single bool. Other math operations do not do this (`+`, `-`, `\/`, `*`), i.e., they preserve the shape of the mask array.\r\n\r\nThis introduces errors when you index the mask later on, expecting it to be an ndarray but it is just a bool.\r\n\r\nThe cause appears to be from [this line](https:\/\/github.com\/numpy\/numpy\/blob\/d35cd07ea997f033b2d89d349734c61f5de54b0d\/numpy\/ma\/core.py#L6969) in the `numpy.ma.power` definition. The default `shrink` argument in `numpy.ma.mask_or` is `True`, and we are meeting those criteria in this case.\n\n### Reproduce the code example:\n\n```python\nimport numpy\r\n\r\n# create an example masked array with an all-False mask\r\na = numpy.array((1, 2, 3))  # arbitrary data\r\nb = numpy.zeros(3)  # all-False mask\r\n\r\nm = numpy.ma.masked_array(a, b)\r\n\r\n\r\n# preview the masked array, its mask has 3 Falses\r\n\r\nm\r\n\r\n# result:\r\n# masked_array(data=[1, 2, 3],\r\n#              mask=[False, False, False],\r\n#        fill_value=999999)\r\n\r\n\r\n# the multiply operator preserves the mask\r\n\r\nm * 4\r\n\r\n\r\n# result:\r\n# masked_array(data=[4, 8, 12],\r\n#              mask=[False, False, False],\r\n#        fill_value=999999)\r\n\r\n\r\n# the power operator shrinks the mask\r\n\r\nm ** 2\r\n\r\n# result:\r\n# masked_array(data=[1, 4, 9],\r\n#              mask=False,\r\n#        fill_value=999999)\n```\n\n\n### Error message:\n\n```shell\nBy default there is no error, but you will get an indexing error if you try to slice the mask in the future.\n```\n\n\n### Python and NumPy Versions:\n\nPython 3.7 and 3.10.\r\nNumpy 1.17, 1.24, 1.26\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\nI came across this issue when simulating a vectorized grid of line-sphere intersections. I slice a section of the mask for future calculations, which raises an indexing exception in cases where the entire mask is False, because the power operations shrunk the mask from an array to a bool.\r\n\r\nIt is indeed possible to get around this by making a check on the mask after the power operation, but I lost some time trying to figure this out and I think other users may encounter similar problems.","comments":[],"labels":["00 - Bug"]},{"title":"BUG: np.take cast to out argument","body":"### Describe the issue:\r\n\r\n`np.take(a, indices, out=out)` fails if `out.dtype` is not exactly `a.dtype`. The doc says about `out`:\r\n> It should be of the appropriate shape and dtype.\r\n\r\nBut what is *appropriate*?\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\na = np.arange(3).astype(np.int32)\r\nindices = np.arange(2)\r\nout = np.zeros_like(indices, dtype=np.int64)\r\nnp.take(a, indices, out=out)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile ~\/github\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/_core\/fromnumeric.py:59, in _wrapfunc(obj, method, *args, **kwds)\r\n     58 try:\r\n---> 59     return bound(*args, **kwds)\r\n     60 except TypeError:\r\n     61     # A TypeError occurs if the object does have such a method in its\r\n     62     # class, but its signature is not identical to that of NumPy's. This\r\n   (...)\r\n     66     # Call _wrapit from within the except clause to ensure a potential\r\n     67     # exception has a traceback chain.\r\n\r\nTypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[4], line 1\r\n----> 1 np.take(a, indices, out=out)\r\n\r\nFile ~\/github\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/_core\/fromnumeric.py:192, in take(a, indices, axis, out, mode)\r\n     95 @array_function_dispatch(_take_dispatcher)\r\n     96 def take(a, indices, axis=None, out=None, mode='raise'):\r\n     97     \"\"\"\r\n     98     Take elements from an array along an axis.\r\n     99 \r\n   (...)\r\n    190            [5, 7]])\r\n    191     \"\"\"\r\n--> 192     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)\r\n\r\nFile ~\/github\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/_core\/fromnumeric.py:68, in _wrapfunc(obj, method, *args, **kwds)\r\n     59     return bound(*args, **kwds)\r\n     60 except TypeError:\r\n     61     # A TypeError occurs if the object does have such a method in its\r\n     62     # class, but its signature is not identical to that of NumPy's. This\r\n   (...)\r\n     66     # Call _wrapit from within the except clause to ensure a potential\r\n     67     # exception has a traceback chain.\r\n---> 68     return _wrapit(obj, method, *args, **kwds)\r\n\r\nFile ~\/github\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/_core\/fromnumeric.py:45, in _wrapit(obj, method, *args, **kwds)\r\n     43 except AttributeError:\r\n     44     wrap = None\r\n---> 45 result = getattr(asarray(obj), method)(*args, **kwds)\r\n     46 if wrap:\r\n     47     if not isinstance(result, mu.ndarray):\r\n\r\nTypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n'numpy_version': '2.0.0.dev0+git20240115.bbdd595'\r\n\r\n### Runtime Environment:\r\n\r\nnot important\r\n\r\n### Context for the issue:\r\n\r\n`out = np.take(a, indices)` works, but passing the `out` arg explicitly saves intermediate memory and could be a tiny bit faster.","comments":["It looks like this is happening because inside of the `take` implementation, we explicitly downcast `out` to the same dtype as `a`:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/b0371ef240560e78b651a5d7c9407ae3212a3d56\/numpy\/_core\/src\/multiarray\/item_selection.c#L306-L310\r\n\r\nUltimately the error you're seeing gets triggered from the early return on line 310 in that C file.\r\n\r\nI'm not sure offhand why the `PyArray_FromArray` call is there, although `git blame` says it's been there since 2011. Presumably we'd need to do something smarter at that location to check if `out` is already sufficiently \"wide\" to hold the  output.","I seem to recall an old issue or PR about it.  The `out` casting check is incorrectly reversed or so.","Related issue https:\/\/github.com\/numpy\/numpy\/issues\/16319 and https:\/\/github.com\/numpy\/numpy\/pull\/4246.","Current behaviour is that `np.take` always returns `out` if it is given. The desired behaviour is unclear to me.\r\nIf a user passes an `out` of same shape and dtype as the result, this seems fine. But what should happen if a user passes an out of same shape but different dtype? Possibilities are:\r\n\r\n1. Always error (current main)\r\n2. Only error when out.dtype is smaller\r\n  In this case options for return value are:\r\n  a) Return a separate newly created array\r\n  b) Return out as specified by user and cast resulting values to out.dtype.\r\n "],"labels":["00 - Bug"]},{"title":"DOC: missing examples for np.polynomial.polynomial.polyval","body":"### Issue with current documentation:\r\n\r\nI JUST ran into this issue #22104 and it took me way too long to find that github-issue to finally see an example of how to use the Polynomial correctly. \r\n\r\n### Idea or request for content:\r\n\r\nAdding an example (like the one given in #22104) [here](https:\/\/numpy.org\/doc\/stable\/reference\/routines.polynomials.html#quick-reference) and [here](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polynomial.polynomial.polyval.html), and perhaps a mention [here](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.polynomial.polynomial.Polynomial.html#numpy.polynomial.polynomial.Polynomial) (e.g. adding \"evaluating the polynomial at positions\" instead of, or in additon to, \"Call self as a function\") would have been very helpful.\r\n\r\nP.S. also the explanation\r\n> Polynomial.fit uses scaled and shifted polynomials for numerical reasons. If you want the usual polynomials, you need to use the convert method\r\n\r\nwould be good to have in the docs.","comments":[],"labels":["04 - Documentation"]},{"title":"BUG: `dtype(<generic>).type` has different output than deprecated `obj2sctype`?","body":"### Describe the issue:\r\n\r\nI'm currently working on adapting skimage for NumPy 2.0 during which I was told to replace our usages of the former `np.core.numerictypes.obj2sctype(x)` with the equivalent `np.dtype(x).type`. However, it seems that if `x` is an generic dtype (I think that's what they are called) the result differs. The example below shows the difference on the current nightly wheel of NumPy.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nfrom numpy._core.numerictypes import obj2sctype\r\nassert obj2sctype(np.floating), np.dtype(np.floating).type\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n(numpy.floating, numpy.float64)\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n2.0.0.dev0+git20240109.6e3b923\r\n3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:03:24) [GCC 12.3.0]\r\n\r\n### Runtime Environment:\r\n\r\n<details><summary>Details<\/summary>\r\n<p>\r\n\r\n```\r\n[{'numpy_version': '2.0.0.dev0+git20240109.6e3b923',\r\n  'python': '3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:03:24) '\r\n            '[GCC 12.3.0]',\r\n  'uname': uname_result(system='Linux', node='hue', release='6.6.10-arch1-1', version='#1 SMP PREEMPT_DYNAMIC Fri, 05 Jan 2024 16:20:41 +0000', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/lg\/.local\/lib\/micromamba\/envs\/skimagedev312\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n```\r\n\r\n<\/p>\r\n<\/details> \r\n\r\n### Context for the issue:\r\n\r\nI guess this is kind of an obscure edge case we are hitting in skimage, and I think I can figure out a way around it. But I still wanted to raise the issue in case this was something that was missed. Feel free to close this, if this isn't something to worry about. Though, I'd be curious if there is an alternative in NumPy 2.0 with which the original output can be revoverd.\r\n\r\nSlightly related https:\/\/github.com\/numpy\/numpy\/issues\/17325.","comments":["Could you link to the context\/code?  It doesn't work for the abstract scalar types indeed, but then the abstract scalar types rarely do what you want them to do, except for that odd `issubdtype` I guess...","Sure, have a look at [skimage\/util\/dtype.py#L277](https:\/\/github.com\/scikit-image\/scikit-image\/blob\/c459c2adfc2b15e616c6858e034de178f4ba449a\/skimage\/util\/dtype.py#L277). If I replace it with just `dtype(...).type` a [test checking for `np.float32` passthrough](https:\/\/github.com\/scikit-image\/scikit-image\/blob\/c459c2adfc2b15e616c6858e034de178f4ba449a\/skimage\/util\/tests\/test_dtype.py#L157-L160) would fail because `dtype(np.floating).type` is turned into `np.float64`. \r\n\r\nThough, it seems that we can get by with just passing `dtype` to `np.issubdtype(...)` directly (see [this diff](https:\/\/github.com\/scikit-image\/scikit-image\/commit\/1835386d5a1d165c07348c47753934c9a51cecda#diff-7959905b4c33d2cefa7da06da5df8a54094174bdc11b41e4aef632ee88e029f6R277)). The tests still pass and as far as I can see,`np.issubdtype` applies `dtype(arg2).type` automatically if an argument isn't a subclass of generic...\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/e429f52efa21ef6ee7904eaba0c9e071e36d2f60\/numpy\/_core\/numerictypes.py#L514-L515"],"labels":["00 - Bug"]},{"title":"BUG: Silent int32 overflow in lapack work size computation leads to wrong exception","body":"The issue of np.linalg.eigh returning wrong results or crashing is still real (using numpy 1.26.2):\r\n```python\r\n>>> import numpy as np\r\n>>> n=32767\r\n>>> b=np.random.rand(n)\r\n>>> m_32767=np.diag(b)\r\n>>> m_32767.shape\r\n(32767, 32767)\r\n>>> V_32767=np.linalg.eigh(m_32767)\r\n ** On entry to DSTEDC parameter number  8 had an illegal value\r\n ** On entry to DORMTR parameter number 12 had an illegal value\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/pearu\/miniconda3\/envs\/jax-cuda-dev\/lib\/python3.11\/site-packages\/numpy\/linalg\/linalg.py\", line 1487, in eigh\r\n    w, vt = gufunc(a, signature=signature, extobj=extobj)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/pearu\/miniconda3\/envs\/jax-cuda-dev\/lib\/python3.11\/site-packages\/numpy\/linalg\/linalg.py\", line 118, in _raise_linalgerror_eigenvalues_nonconvergence\r\n    raise LinAlgError(\"Eigenvalues did not converge\")\r\nnumpy.linalg.LinAlgError: Eigenvalues did not converge\r\n```\r\nwhere the exception \"Eigenvalues did not converge\" is very likely wrong and misleading. With n == 32766, the above example works fine.\r\n\r\nThe underlying problem is that when computing `lwork` for the lapack `syevd` function using expression `1 + 6 * n + 2 * n * n`, it will overflow when `n == 32767`. This problem is not unique to `syevd` but it exists for all lapack functions that work array sizes are quadratic wrt input sizes.\r\n\r\nWhile switching to lapack implementations that uses 64-bit integer inputs, the overflow issue is seemingly resolved but in fact it is just harder to reproduce because the critical `n` size will be `2147483647` where the issue re-merges when the `lwork` expression above will overflow for int64.\r\n\r\nI have implemented a solution to the same problem in JAX (https:\/\/github.com\/google\/jax\/pull\/19288) that will lead to an overflow exception rather than wrong results or crashes. I think something similar is appropriate for NumPy as well.\r\n\r\n_Originally posted by @pearu in https:\/\/github.com\/numpy\/numpy\/issues\/13956#issuecomment-1885191876_\r\n            ","comments":["Nice solution, thanks for sharing!\r\n\r\nOn my machine with OpenBLAS 0.3.23, the function call appears to simply hang rather than return an exception, and consumer 100% of all CPU cores available for minutes until I kill the process.","> On my machine with OpenBLAS 0.3.23, the function call appears to simply hang rather than return an exception, and consumer 100% of all CPU cores available for minutes until I kill the process.\r\n\r\nNot surprising. The incorrect value of lwork in lapack functions leads to unpredictable behavior that could be a crash, wrong results, wrong error code, you name it.\r\n\r\nBtw, it is normal that solving the eigenvalue problem of such large matrices on CPU takes time. For instance, in my box with plenty of RAM it takes about 20 minutes with 3600% of CPU utilization. "],"labels":["00 - Bug","component: numpy.linalg"]},{"title":"BUG: cannot build numpy on arm64 Ubuntu 22.04 with asan enabled","body":"When I try to build the current numpy main branch, gcc crashes with an internal compiler error inside highway's `vqsort`:\r\n\r\n\r\n\r\n```\r\n$ spin build --clean --  -Dbuildtype=debug -Db_sanitize=address\r\n<snipping output until the error>\r\n[185\/333] Compiling C++ object numpy\/_...c_npysort_highway_qsort.dispatch.cpp.o\r\nFAILED: numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/src_npysort_highway_qsort.dispatch.cpp.o \r\nc++ -Inumpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p -Inumpy\/_core -I..\/numpy\/_core -Inumpy\/_core\/include -I..\/numpy\/_core\/include -I..\/numpy\/_core\/src\/common -I..\/numpy\/_core\/src\/multiarray -I..\/numpy\/_core\/src\/npymath -I..\/numpy\/_core\/src\/umath -I..\/numpy\/_core\/src\/highway -I\/home\/goldbaum\/.pyenv\/versions\/3.11.7-debug\/include\/python3.11d -I\/home\/goldbaum\/numpy\/build\/meson_cpu -fdiagnostics-color=always -fsanitize=address -fno-omit-frame-pointer -Wall -Winvalid-pch -std=c++17 -O0 -g -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -O3 -DNPY_HAVE_NEON_VFPV4 -DNPY_HAVE_NEON_FP16 -DNPY_HAVE_NEON -DNPY_HAVE_ASIMD -DNPY_HAVE_ASIMDHP -DNPY_HAVE_SVE -march=armv8.2-a+sve+fp16 -DNPY_MTARGETS_CURRENT=SVE -MD -MQ numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/src_npysort_highway_qsort.dispatch.cpp.o -MF numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/src_npysort_highway_qsort.dispatch.cpp.o.d -o numpy\/_core\/libhighway_qsort.dispatch.h_SVE.a.p\/src_npysort_highway_qsort.dispatch.cpp.o -c ..\/numpy\/_core\/src\/npysort\/highway_qsort.dispatch.cpp\r\nduring GIMPLE pass: sanopt\r\nIn file included from ..\/numpy\/_core\/src\/npysort\/highway_qsort.dispatch.cpp:3:\r\n..\/numpy\/_core\/src\/highway\/hwy\/contrib\/sort\/vqsort-inl.h: In function \\u2018void hwy::N_SVE::detail::Recurse(D, Traits, T*, size_t, T*, uint64_t*, size_t) [with D = hwy::N_SVE::Simd<long unsigned int, 32, 0>; Traits = hwy::N_SVE::detail::SharedTraits<hwy::N_SVE::detail::TraitsLane<hwy::N_SVE::detail::OrderAscending<long unsigned int> > >; T = long unsigned int]\\u2019:\r\n..\/numpy\/_core\/src\/highway\/hwy\/contrib\/sort\/vqsort-inl.h:1571:19: internal compiler error: in asan_expand_mark_ifn, at asan.c:3726\r\n 1571 | HWY_NOINLINE void Recurse(D d, Traits st, T* HWY_RESTRICT keys,\r\n      |                   ^~~~~~~\r\n0xffff930073fb __libc_start_call_main\r\n\t..\/sysdeps\/nptl\/libc_start_call_main.h:58\r\n0xffff930074cb __libc_start_main_impl\r\n\t..\/csu\/libc-start.c:392\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <file:\/\/\/usr\/share\/doc\/gcc-11\/README.Bugs> for instructions.\r\n```\r\n\r\n```\r\ngoldbaum@ubuntuvm:~\/numpy$ gcc --version\r\ngcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nCopyright (C) 2021 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n\r\n@Mousius should I be reporting this upstream to highway (or maybe ubuntu or gcc?).","comments":["I guess I should also say I'm trying to build numpy with address sanitizer support turned on, to debug a numpy issue. It looks like the error is happening inside of the asan implementation in gcc.","It does build on AMD64 so I can work around this for today\u2019s debugging session by using my old laptop with an intel x86 CPU. This also means that the gcc or highway issue causing this is arm-specific.","@ngoldbaum I'd duplicate this issue in Highway, might be related to https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=97696 and likely requires a workaround from Highway \ud83d\ude3f Does it work with a newer GCC?","It just occurred to me that we can add this to the compiler check for SVE (https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/distutils\/checks\/cpu_sve.c) that would mean it doesn't try to compile Highway at all if the compiler bug exists.","Sorry, not sure if it works on newer gcc versions, I can't see how to easily install a newer version.","Bummer, looks like the GCC bug is three years old and not fixed yet :\/\r\nTo unblock your progress, you can define -DVQSORT_ENABLED=0 in the compiler flags.\r\nMeanwhile we can set that automatically for Arm asan builds in Highway, for the upcoming release.","A fix for this issue (https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=97696) was submitted to GCC mainline:\r\nhttps:\/\/gcc.gnu.org\/git\/gitweb.cgi?p=gcc.git;h=fca6f6fddb22b8665e840f455a7d0318d4575227"],"labels":["00 - Bug"]},{"title":"BUG: masked_array mean\/sum not aligned with array","body":"### Describe the issue:\n\nIn the  numpy.array, mean method on int array is computed in float64 as described in documentation.\r\nThe behavior for masked_array is different, it is computing mean using array datatype. As a consequence, the resulting mean value is wrong where max_value is exceeded.\r\n\r\nIn my opinion , this could be either a feature of masked_array which is not documented and , to my view, a sneaky  bug.\r\n\r\nAs a work-around, user may add dtype=np.float64 parameter.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\na1 = np.ones((2000,2000), dtype=np.int16)\r\na1=a1*3000\r\na1.mean()\r\nassert(a1.mean() == 3000.0)\r\n# a1.mean() is correct: 3000.0\r\nma1 = np.ma.array(a1)\r\nma1.mean()\r\nassert(ma1.mean() == 3000.0)\r\n# ma1.mean() is -221.225472 instead of 3000.0\n```\n\n\n### Error message:\n\n```shell\nn\/a\n```\n\n\n### Python and NumPy Versions:\n\n1.21.6\r\n3.7.9 (tags\/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]\r\n\r\nI have tested other versions with similar outputs.\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\nunexpected result without error emitted leading to incorrect simulation.","comments":["By explicitly setting the dtype to np.float64 when creating the masked_array","Hello Bharath,\r\nThanks for your additional work-around proposal.\r\nI agree that we can change the array datatype but this does not fix the original issue.\r\nBy the way, in my user code, changing the masked array datatype is not possible.\r\nVincent"],"labels":["00 - Bug"]},{"title":"BUG: `assert_allclose` cannot handle object arrays","body":"This is similar to #9023 but worse, because it's less of a corner case, and more of a full edge. \r\n\r\nWhereas `assert_equal` \"only\" treats nans in object arrays incorrectly, its not even possible to put object arrays into `assert_allclose` at the most basic level:\r\n```\r\n>>> from numpy.testing import assert_allclose\r\n>>> n = np.array([1], dtype=object)\r\n>>> assert_allclose(n.astype(int), n)\r\n[...]\r\nTypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\r\n```\r\n\r\nThis makes it unnecessarily hard to write parametric tests for various corner cases (which are already hard to write at the best of times...).","comments":["Presumably if #9009 were fixed this would \"just work\"."],"labels":["component: numpy.testing"]},{"title":"ENH: allow start-stop array for indices in reduceat","body":"Triggered by #834 seeing some comments again, a draft just to see how it would look to allow `reduceat` to take a set of start, stop indices (treated as slices), to make the interface a bit more easily comprehensible without making a truly new method. It also allows passing in an `initial` to deal with empty slices.\r\n\r\nFixes #834 \r\n\r\nDraft only, since no new test cases yet. Mostly to discuss whether we want this at all, and, if so, what the API should be. So probably best not to worry too much about implementation (the duplication of code, both in `reduceat` itself and with `reduce` is large).\r\n\r\nSample use:\r\n```\r\na = np.arange(12)\r\nnp.add.reduceat(a, ([1, 3, 5], [2, -1, 0]))\r\n# array([ 1, 52,  0])\r\nnp.minimum.reduceat(a, ([1, 3, 5], [2, -1, 0]), initial=10)\r\n# array([ 1,  3, 10])\r\nnp.minimum.reduceat(a, ([1, 3, 5], [2, -1, 0]))\r\n# ValueError: empty slice encountered with reduceat operation for 'minimum', which does not have an identity. Specify 'initial'.\r\n```\r\nWriting it out like this, I think a different order may be useful, i.e., `np.add(a, [(1, 2), (3, -1), (5, 0)])`. The reason I picked the other one was that I liked the idea of triggering it by using `slice(start, stop)`, with both `start` and `stop` possibly arrays and a tuple of two lists was closer to that (although internally it just turns it into an array). The list of tuples suggests more a structured array with start and stop (and step?) entries.\r\n\r\np.s. Fairly trivially extensible to `start, stop, step`.","comments":[],"labels":["01 - Enhancement","component: numpy.ufunc"]},{"title":"Doing something about slow tests again","body":"The test suite has gradually become a bit slower again, and this makes CI jobs take longer - which now is an additional hassle because of the cost (as of now, $2.75 per wheel build run, see https:\/\/github.com\/numpy\/numpy\/issues\/24280#issuecomment-1867993272). Some jobs are quite slow, and the PyPy on Windows one is ridiculously slow, taking 1h 22m.\r\n\r\nHere are the top 200 slowest tests for the `-m full` test suite (which is what the wheel builds run):\r\n\r\n<details>\r\n\r\n```\r\n====================================== slowest 200 durations ======================================\r\n11.75s call     numpy\/lib\/tests\/test_io.py::TestSaveTxt::test_large_zip\r\n7.19s call     numpy\/random\/tests\/test_extending.py::test_cython\r\n6.73s call     numpy\/_core\/tests\/test_mem_overlap.py::test_may_share_memory_easy_fuzz\r\n6.41s call     numpy\/_core\/tests\/test_multiarray.py::TestDot::test_huge_vectordot[complex128]\r\n6.02s call     numpy\/linalg\/tests\/test_linalg.py::TestCond::test_generalized_sq_cases\r\n5.39s call     numpy\/distutils\/tests\/test_build_ext.py::test_multi_fortran_libs_link\r\n5.28s call     numpy\/_core\/tests\/test_mem_overlap.py::test_may_share_memory_harder_fuzz\r\n5.01s setup    numpy\/_core\/tests\/test_cython.py::test_is_timedelta64_object\r\n4.99s call     numpy\/_core\/tests\/test_multiarray.py::TestBool::test_count_nonzero_all\r\n3.90s call     numpy\/_core\/tests\/test_mem_overlap.py::test_diophantine_fuzz\r\n3.58s call     numpy\/lib\/tests\/test_io.py::TestSavezLoad::test_big_arrays\r\n3.46s call     numpy\/tests\/test_warnings.py::test_warning_calls\r\n3.39s call     numpy\/lib\/tests\/test_format.py::test_large_archive\r\n3.18s call     numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_ufunc_call_fuzz\r\n2.88s call     numpy\/testing\/tests\/test_utils.py::TestAssertNoGcCycles::test_asserts\r\n2.88s call     numpy\/testing\/tests\/test_utils.py::TestAssertNoGcCycles::test_passes\r\n2.82s call     numpy\/_core\/tests\/test_dtype.py::TestDTypeMakeCanonical::test_structured\r\n2.36s call     numpy\/lib\/tests\/test_io.py::test_load_refcount\r\n2.24s call     numpy\/lib\/tests\/test_function_base.py::TestLeaks::test_frompyfunc_leaks[bound-20]\r\n2.24s call     numpy\/lib\/tests\/test_function_base.py::TestLeaks::test_frompyfunc_leaks[unbound-0]\r\n2.23s call     numpy\/random\/tests\/test_generator_mt19937.py::TestIntegers::test_integers_small_dtype_chisquared[50000000-5000-uint16-6500.0]\r\n2.10s call     numpy\/random\/tests\/test_generator_mt19937.py::TestRandomDist::test_dirichlet_moderately_small_alpha\r\n2.04s call     numpy\/linalg\/tests\/test_linalg.py::TestDet::test_generalized_sq_cases\r\n1.90s setup    numpy\/f2py\/tests\/test_character.py::TestCharacterString::test_input[1]\r\n1.83s setup    numpy\/f2py\/tests\/test_parameter.py::TestParameters::test_constant_real_single\r\n1.82s setup    numpy\/f2py\/tests\/test_return_character.py::TestFReturnCharacter::test_all_f77[t0]\r\n1.81s setup    numpy\/f2py\/tests\/test_return_logical.py::TestFReturnLogical::test_all_f77[t0]\r\n1.81s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestDimSpec::test_array_size[n]\r\n1.80s setup    numpy\/f2py\/tests\/test_return_integer.py::TestFReturnInteger::test_all_f77[t0]\r\n1.80s setup    numpy\/f2py\/tests\/test_callback.py::TestF77Callback::test_all[t]\r\n1.80s setup    numpy\/f2py\/tests\/test_callback.py::TestF90Callback::test_gh17797\r\n1.80s setup    numpy\/f2py\/tests\/test_return_complex.py::TestFReturnComplex::test_all_f77[t0]\r\n1.80s setup    numpy\/f2py\/tests\/test_return_real.py::TestFReturnReal::test_all_f77[t0]\r\n1.80s setup    numpy\/f2py\/tests\/test_regression.py::TestModuleAndSubroutine::test_gh25337\r\n1.78s setup    numpy\/f2py\/tests\/test_character.py::TestCharacter::test_input[c]\r\n1.77s setup    numpy\/f2py\/tests\/test_common.py::TestCommonWithUse::test_common_gh19161\r\n1.77s setup    numpy\/f2py\/tests\/test_string.py::TestDocStringArguments::test_example\r\n1.76s setup    numpy\/f2py\/tests\/test_character.py::TestMiscCharacter::test_gh18684\r\n1.76s setup    numpy\/f2py\/tests\/test_common.py::TestCommonBlock::test_common_block\r\n1.76s setup    numpy\/f2py\/tests\/test_callback.py::TestF77CallbackPythonTLS::test_all[t]\r\n1.76s setup    numpy\/f2py\/tests\/test_assumed_shape.py::TestAssumedShapeSumExample::test_all\r\n1.76s setup    numpy\/f2py\/tests\/test_assumed_shape.py::TestF2cmapOption::test_all\r\n1.76s setup    numpy\/f2py\/tests\/test_quoted_character.py::TestQuotedCharacter::test_quoted_character\r\n1.75s setup    numpy\/f2py\/tests\/test_string.py::TestFixedString::test_intent_in\r\n1.75s setup    numpy\/f2py\/tests\/test_size.py::TestSizeSumExample::test_all\r\n1.75s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestCrackFortran::test_gh2848\r\n1.75s setup    numpy\/f2py\/tests\/test_f2cmap.py::TestF2Cmap::test_gh15095\r\n1.75s setup    numpy\/f2py\/tests\/test_return_real.py::TestCReturnReal::test_all[t4]\r\n1.75s setup    numpy\/f2py\/tests\/test_callback.py::TestGH25211::test_gh25211\r\n1.75s setup    numpy\/f2py\/tests\/test_data.py::TestDataMultiplierF77::test_data_stmts\r\n1.74s setup    numpy\/f2py\/tests\/test_semicolon_split.py::TestMultiline::test_multiline\r\n1.74s setup    numpy\/f2py\/tests\/test_isoc.py::TestISOC::test_c_double\r\n1.74s call     numpy\/linalg\/tests\/test_linalg.py::TestPinv::test_generalized_nonsq_cases\r\n1.74s setup    numpy\/f2py\/tests\/test_value_attrspec.py::TestValueAttr::test_gh21665\r\n1.74s setup    numpy\/f2py\/tests\/test_character.py::TestStringOptionalInOut::test_gh24662\r\n1.74s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestFunctionReturn::test_function_rettype\r\n1.74s setup    numpy\/f2py\/tests\/test_mixed.py::TestMixed::test_all\r\n1.74s setup    numpy\/f2py\/tests\/test_module_doc.py::TestModuleDocString::test_module_docstring\r\n1.74s setup    numpy\/f2py\/tests\/test_callback.py::TestGH18335::test_gh18335\r\n1.73s setup    numpy\/f2py\/tests\/test_abstract_interface.py::TestAbstractInterface::test_abstract_interface\r\n1.73s setup    numpy\/f2py\/tests\/test_data.py::TestDataWithCommentsF77::test_data_stmts\r\n1.73s setup    numpy\/f2py\/tests\/test_kind.py::TestKind::test_int\r\n1.73s setup    numpy\/f2py\/tests\/test_regression.py::TestNumpyVersionAttribute::test_numpy_version_attribute\r\n1.73s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestNoSpace::test_module\r\n1.73s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestExternal::test_external_as_statement\r\n1.73s setup    numpy\/f2py\/tests\/test_character.py::TestBCCharHandling::test_gh25286\r\n1.73s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestUnicodeComment::test_encoding_comment\r\n1.73s setup    numpy\/f2py\/tests\/test_character.py::TestNewCharHandling::test_gh25286\r\n1.73s call     numpy\/testing\/tests\/test_utils.py::TestAssertNoGcCycles::test_fails\r\n1.73s setup    numpy\/f2py\/tests\/test_character.py::TestStringAssumedLength::test_gh24008\r\n1.73s setup    numpy\/f2py\/tests\/test_docs.py::TestDocAdvanced::test_asterisk1\r\n1.73s call     numpy\/tests\/test_ctypeslib.py::TestAsArray::test_reference_cycles\r\n1.72s setup    numpy\/f2py\/tests\/test_character.py::TestStringScalarArr::test_char\r\n1.72s setup    numpy\/f2py\/tests\/test_string.py::TestString::test_char\r\n1.72s setup    numpy\/f2py\/tests\/test_data.py::TestDataF77::test_data_stmts\r\n1.72s setup    numpy\/f2py\/tests\/test_block_docstring.py::TestBlockDocString::test_block_docstring\r\n1.72s setup    numpy\/f2py\/tests\/test_semicolon_split.py::TestCallstatement::test_callstatement\r\n1.71s setup    numpy\/f2py\/tests\/test_data.py::TestData::test_data_stmts\r\n1.71s setup    numpy\/f2py\/tests\/test_regression.py::TestIntentInOut::test_inout\r\n1.71s setup    numpy\/f2py\/tests\/test_regression.py::TestNegativeBounds::test_negbound\r\n1.69s call     numpy\/_core\/tests\/test_extint128.py::test_divmod_128_64\r\n1.69s call     numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_binary_ufunc_1d_manual\r\n1.64s call     numpy\/_core\/tests\/test_regression.py::TestRegression::test_structarray_title\r\n1.59s call     numpy\/_core\/tests\/test_nditer.py::test_iter_buffered_reduce_reuse\r\n1.58s call     numpy\/_core\/tests\/test_multiarray.py::TestDot::test_huge_vectordot[float64]\r\n1.54s setup    numpy\/f2py\/tests\/test_array_from_pyobj.py::TestIntent::test_in_out\r\n1.18s call     numpy\/array_api\/tests\/test_array_object.py::test_operators\r\n1.17s call     numpy\/linalg\/tests\/test_linalg.py::TestPinv::test_generalized_sq_cases\r\n1.04s call     numpy\/linalg\/tests\/test_linalg.py::test_sdot_bug_8577\r\n1.00s call     numpy\/linalg\/tests\/test_linalg.py::TestEigvals::test_generalized_sq_cases\r\n0.84s call     numpy\/linalg\/tests\/test_linalg.py::TestInv::test_generalized_sq_cases\r\n0.83s call     numpy\/_core\/tests\/test_multiarray.py::TestPickling::test_roundtrip\r\n0.81s call     numpy\/_core\/tests\/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array\r\n0.80s call     numpy\/linalg\/tests\/test_linalg.py::TestSolve::test_generalized_sq_cases\r\n0.72s call     numpy\/linalg\/tests\/test_linalg.py::TestEig::test_generalized_sq_cases\r\n0.71s setup    numpy\/_core\/tests\/test_array_interface.py::test_cstruct\r\n0.68s call     numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_gufunc_fuzz\r\n0.67s call     numpy\/_core\/tests\/test_multiarray.py::TestArrayFinalize::test_lifetime_on_error\r\n0.67s call     numpy\/_core\/tests\/test_multiarray.py::TestAlignment::test_various_alignments\r\n0.66s call     numpy\/linalg\/tests\/test_linalg.py::TestSVD::test_generalized_sq_cases\r\n0.66s call     numpy\/_core\/tests\/test_scalarmath.py::TestModulus::test_float_modulus_exact\r\n0.65s call     numpy\/f2py\/tests\/test_crackfortran.py::TestNameArgsPatternBacktracking::test_nameargspattern_backtracking[@)@bind                         @(@]\r\n0.64s setup    numpy\/_core\/tests\/test_mem_policy.py::test_set_policy\r\n0.62s call     numpy\/lib\/tests\/test_format.py::test_huge_header_npz\r\n0.60s call     numpy\/random\/tests\/test_generator_mt19937_regressions.py::TestRegression::test_shuffle_of_array_of_different_length_strings\r\n0.58s teardown numpy\/typing\/tests\/test_typing.py::test_extended_precision\r\n0.58s call     numpy\/random\/tests\/test_regression.py::TestRegression::test_shuffle_of_array_of_different_length_strings\r\n0.57s call     numpy\/random\/tests\/test_randomstate_regression.py::TestRegression::test_shuffle_of_array_of_different_length_strings\r\n0.57s call     numpy\/random\/tests\/test_regression.py::TestRegression::test_shuffle_of_array_of_objects\r\n0.57s call     numpy\/random\/tests\/test_randomstate_regression.py::TestRegression::test_shuffle_of_array_of_objects\r\n0.55s call     numpy\/random\/tests\/test_generator_mt19937_regressions.py::TestRegression::test_shuffle_of_array_of_objects\r\n0.54s call     numpy\/_core\/tests\/test_limited_api.py::test_limited_api\r\n0.49s call     numpy\/_core\/tests\/test_dtype.py::TestDTypeMakeCanonical::test_make_canonical_hypothesis\r\n0.47s call     numpy\/_core\/tests\/test_multiarray.py::TestUnicodeEncoding::test_round_trip\r\n0.47s call     numpy\/_core\/tests\/test_mem_overlap.py::test_internal_overlap_fuzz\r\n0.46s call     numpy\/_core\/tests\/test_numeric.py::TestCreationFuncs::test_full\r\n0.45s setup    numpy\/f2py\/tests\/test_array_from_pyobj.py::TestSharedMemory::test_hidden[DOUBLE]\r\n0.44s call     numpy\/_core\/tests\/test_numeric.py::TestClip::test_clip_property\r\n0.42s call     numpy\/_core\/tests\/test_multiarray.py::TestCreation::test_zeros_big\r\n0.42s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<structured subarray 1>]\r\n0.42s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<subarray in field>]\r\n0.42s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<structured subarray 1>]\r\n0.42s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<structured subarray 2>]\r\n0.41s call     numpy\/_core\/tests\/test_mem_policy.py::test_owner_is_base\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<subarray>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<subarray>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<subarray in field>]\r\n0.41s call     numpy\/random\/tests\/test_generator_mt19937.py::TestIntegers::test_integers_small_dtype_chisquared[10000000-2500-int16-3300.0]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<subarray in field>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<structured subarray 1>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<subarray in field>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape0-index0-2-<structured subarray 2>]\r\n0.41s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<subarray in field>]\r\n0.40s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<subarray>]\r\n0.40s call     numpy\/_core\/tests\/test_cpu_features.py::TestEnvPrivation::test_runtime_feature_selection\r\n0.40s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<subarray>]\r\n0.39s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_take_and_repeat[<structured subarray 2>]\r\n0.39s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape1-index1-4-<structured subarray 2>]\r\n0.39s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_item_setting[<structured subarray 1>]\r\n0.38s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<structured subarray 2>]\r\n0.38s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<structured subarray 2>]\r\n0.38s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<subarray>]\r\n0.38s call     numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_ufunc_1d_manual\r\n0.38s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<subarray in field>]\r\n0.37s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<subarray>]\r\n0.37s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape2-index2-2-<structured subarray 1>]\r\n0.37s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing[shape3-index3-2-<structured subarray 1>]\r\n0.35s call     numpy\/random\/tests\/test_randomstate.py::test_integer_repeat[multinomial]\r\n0.35s call     numpy\/_core\/tests\/test_regression.py::TestRegression::test_leak_in_structured_dtype_comparison\r\n0.34s call     numpy\/_core\/tests\/test_arrayprint.py::TestArray2String::test_any_text\r\n0.34s call     numpy\/_core\/tests\/test_multiarray.py::TestDot::test_accelerate_framework_sgemv_fix\r\n0.30s call     numpy\/_core\/tests\/test_multiarray.py::test_partition_fp[float16-N95]\r\n0.28s call     numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_ufunc_call_complex_fuzz\r\n0.27s call     numpy\/_core\/tests\/test_multiarray.py::TestMethods::test_partition\r\n0.26s call     numpy\/_core\/tests\/test_umath.py::TestAbsoluteNegative::test_abs_neg_blocked\r\n0.26s call     numpy\/_core\/tests\/test_multiarray.py::TestMethods::test_sort_degraded\r\n0.24s call     numpy\/lib\/tests\/test_function_base.py::TestQuantile::test_quantile_monotonic_hypo\r\n0.24s call     numpy\/_core\/tests\/test_api.py::test_copyto_permut\r\n0.23s call     numpy\/ma\/tests\/test_core.py::TestMaskedArrayMathMethods::test_mean_overflow\r\n0.23s call     numpy\/_core\/tests\/test_mem_policy.py::test_switch_owner[0]\r\n0.23s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_as_parameter_holds_reference\r\n0.23s call     numpy\/tests\/test_reloading.py::test_full_reimport\r\n0.23s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[overlapping]\r\n0.23s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[readonly]\r\n0.23s call     numpy\/_core\/tests\/test_scalarmath.py::TestBaseMath::test_blocked\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[empty]\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[structured]\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[empty-2d]\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[1d]\r\n0.22s call     numpy\/_core\/tests\/test_mem_policy.py::test_switch_owner[1]\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[2d]\r\n0.22s call     numpy\/_core\/tests\/test_mem_policy.py::test_switch_owner[None]\r\n0.22s call     numpy\/_core\/tests\/test_multiarray.py::TestCTypes::test_ctypes_data_as_holds_reference[object]\r\n0.22s call     numpy\/_core\/tests\/test_indexing.py::TestMultiIndexingAutomated::test_multidim\r\n0.22s call     numpy\/lib\/tests\/test_histograms.py::TestHistogramOptimBinNums::test_scott_vs_stone\r\n0.21s call     numpy\/lib\/tests\/test_nanfunctions.py::TestNanFunctions_Median::test_float_special\r\n0.20s call     numpy\/lib\/tests\/test_format.py::test_huge_header[r]\r\n0.20s call     numpy\/_core\/tests\/test_extint128.py::test_safe_binop\r\n0.20s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<subarray in field>]\r\n0.20s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<structured subarray 1>]\r\n0.20s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<subarray>]\r\n0.19s call     numpy\/_core\/tests\/test_umath.py::TestAVXFloat32Transcendental::test_sincos_float32\r\n0.19s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<subarray>]\r\n0.19s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<structured subarray 2>]\r\n0.19s call     numpy\/array_api\/tests\/test_set_functions.py::test_inverse_indices_shape[unique_inverse]\r\n0.19s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[ones-1-<structured subarray 2>]\r\n0.19s call     numpy\/tests\/test_scripts.py::test_pep338\r\n0.18s call     numpy\/_core\/tests\/test_arrayprint.py::TestArray2String::test_refcount\r\n0.18s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<subarray in field>]\r\n0.18s call     numpy\/_core\/tests\/test_multiarray.py::TestIO::test_largish_file[path_obj]\r\n0.18s call     numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_create_delete[zeros-0-<structured subarray 1>]\r\n0.18s call     numpy\/fft\/tests\/test_helper.py::TestFFTShift::test_equal_to_original\r\n0.18s call     numpy\/_core\/tests\/test_multiarray.py::TestIO::test_largish_file[string]\r\n0.18s call     numpy\/distutils\/tests\/test_exec_command.py::TestExecCommand::test_basic\r\n0.18s call     numpy\/_core\/tests\/test_umath_accuracy.py::TestAccuracy::test_validate_transcendentals\r\n0.18s call     numpy\/tests\/test_scripts.py::test_f2py[f2py]\r\n0.18s call     numpy\/_core\/tests\/test_multiarray.py::TestBool::test_count_nonzero\r\n0.17s call     numpy\/linalg\/tests\/test_linalg.py::TestEighCases::test_generalized_herm_cases\r\n0.17s call     numpy\/tests\/test_public_api.py::test_import_lazy_import[testing]\r\n0.17s call     numpy\/linalg\/tests\/test_linalg.py::TestCond::test_sq_cases\r\n============== 45452 passed, 376 skipped, 36 xfailed, 4 xpassed in 343.20s (0:05:43) ==============\r\n```\r\n\r\n<\/details>\r\n\r\nOne of the top offenders is `f2py`, there is already a separate issue for that: gh-25134. Separating out those tests so they don't run at all on wheel builds will take care of that problem.\r\n\r\nFor the rest we should go through and deal with some of the tests in the above list case by case. I'm having a look at `TestStructuredObjectRefcounting` now, which is one of the worst tests.","comments":["I propose that we introduce an `@pytest.mark.xslow` decorator, just like SciPy does. And then run those tests only in a single dedicated CI job, not in the full test suite. That solves the problem with the slowest tests. It makes very little sense for something like `TestSaveTxt::test_large_zip`, which takes ~10 sec. and never fails, to run ~30 times for a single commit where we trigger a wheel build.","> and the PyPy on Windows one is ridiculously slow\r\n\r\nAll the PyPy tests are slow, everything else finishes and the three PyPy tests will still be running. It might be useful to check why. @mattip any ideas?","Can we group parametrized tests?  I suspect there are some excessive parametrization and this doesn't look like it would notice them.","Having an xslow would be fine.  It would be nice to make sure that they are run on release wheels, but OK...","You might also want to consider moving more CI to cron jobs, perhaps the pypy and simd ones, allowing one to trigger those manually by setting a specific label (we do this in astropy for emulated architectures)\r\n\r\nMany PRs only touch python code, perhaps this can be recognized\/labelled automatically, with CI depending on the label?","FWIW, I wouldn't mind such a setup, the one thing would be nice to auto-open an issue on failure I guess.  I wonder if @pllim might just know how to set that up quite quickly?  For C vs. Python, I am not sure I think it is worthwhile, running only the most basic tests on `Documentation` marked PRs would be cool though.\r\n\r\nPyPy could be moved or not (because it does fail occasionally on larger C-changes).  The SIMD\/architecture tests would be nice for chron+explicitly though.\r\n\r\nWth `pytest-durations-extra` and `--functions-durations`, I get this:\r\n```\r\n=============================================================== slowest test functions durations ===============================================================\r\n64.88s numpy\/_pyinstaller\/test_pyinstaller.py::test_pyinstaller\r\n23.46s numpy\/lib\/tests\/test_io.py::TestSaveTxt::test_large_zip\r\n13.21s numpy\/linalg\/tests\/test_linalg.py::TestCond::test_generalized_sq_cases\r\n12.88s numpy\/lib\/tests\/test_io.py::TestSavezLoad::test_big_arrays\r\n12.76s numpy\/_core\/tests\/test_mem_overlap.py::test_may_share_memory_easy_fuzz\r\n10.22s numpy\/_core\/tests\/test_mem_overlap.py::test_may_share_memory_harder_fuzz\r\n9.16s numpy\/_core\/tests\/test_multiarray.py::TestBool::test_count_nonzero_all\r\n8.12s numpy\/_core\/tests\/test_dtype.py::TestStructuredObjectRefcounting::test_structured_object_indexing\r\n7.88s numpy\/_core\/tests\/test_strings.py::test_string_comparisons\r\n7.60s numpy\/distutils\/tests\/test_build_ext.py::test_multi_fortran_libs_link\r\n7.45s numpy\/lib\/tests\/test_format.py::test_large_archive\r\n6.84s numpy\/_core\/tests\/test_multiarray.py::test_partition_int\r\n6.67s numpy\/_core\/tests\/test_mem_overlap.py::test_diophantine_fuzz\r\n6.52s numpy\/lib\/tests\/test_function_base.py::TestLeaks::test_frompyfunc_leaks\r\n6.26s numpy\/random\/tests\/test_extending.py::test_cython\r\n6.05s numpy\/ma\/tests\/test_core.py::TestMaskedArrayArithmetic::test_comparisons_for_numeric\r\n5.43s numpy\/_core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_ufunc_call_fuzz\r\n5.25s numpy\/tests\/test_warnings.py::test_warning_calls\r\n4.91s numpy\/random\/tests\/test_generator_mt19937.py::TestIntegers::test_integers_small_dtype_chisquared\r\n4.73s numpy\/_core\/tests\/test_cython.py::test_is_timedelta64_object\r\n================================================================== sum of all tests durations ==================================================================\r\n575.43s\r\n============================================ 44051 passed, 1784 skipped, 36 xfailed, 4 xpassed in 707.82s (0:11:47) ============================================\r\n```\r\nThe very slowest ons don't seem to affected, OTOH the parametrized ones are often not marked slow and beyond the first ~10 or so, they seem to start to dominate.","Intriguing. Anything called `_fuzz` would seem fair game (and perhaps not particularly logical to run on PRs).","Hello! I am not familiar with numpy tests so I can only say how astropy is doing it and you can adapt it as you see fit for this package.\r\n\r\n1. pyinstaller -- We have it run in a daily cron job over at https:\/\/github.com\/astropy\/astropy\/blob\/b50e9cfc8bf862816ea9ddd7193f0cf801b44c71\/.github\/workflows\/ci_cron_daily.yml#L46\r\n2. slow tests -- saimn implemented markers for \"slow\" and \"hugemem\" in https:\/\/github.com\/astropy\/astropy\/pull\/12764 . The markers are in https:\/\/github.com\/astropy\/pytest-astropy\/blob\/main\/pytest_astropy\/plugin.py . Looks like we still run the \"slow\" tests in regular CI but only for selected jobs but we never run \"hugemem\" in the CI at all but provide instructions for devs to run those manually as needed. This does not mean you have to do it exactly in the same way but the infrastructure is there.\r\n3. Run extra CI via label -- We have opt-in label to run cron job CI in a PR but only as needed because the \"exotic archs\" jobs take a few hours to run. You can see it here: https:\/\/github.com\/astropy\/astropy\/blob\/b50e9cfc8bf862816ea9ddd7193f0cf801b44c71\/.github\/workflows\/ci_cron_weekly.yml#L33\r\n\r\nHope this helps and happy holidays!","> All the PyPy tests are slow, everything else finishes and the three PyPy tests will still be running\r\n\r\nPyPy is known to be slow on c-extensions. I run weekly tests of PyPy HEAD against common or complicated projects' HEAD in a [binary-testing](https:\/\/github.com\/pypy\/binary-testing\/actions). It probably makes sense for NumPy to limit testing of PyPy to a sampling strategy and not on every PR.","> Having an xslow would be fine. It would be nice to make sure that they are run on release wheels, but OK...\r\n\r\nNo need I think, these tests are highly unlikely to fail if they're still run in a regular CI job. So making that a manual step in the release process would be a bit much.\r\n\r\n> FWIW, I wouldn't mind such a setup, the one thing would be nice to auto-open an issue on failure I guess.\r\n\r\nThis type of improvement is certainly of interest as a CI improvement I'd say. It's a bit orthogonal to the main goals of this issue though, which are to speed up wheel builds to (a) reduce Cirrus CI costs, and (b) improve on iteration time when debugging CI issues.\r\n\r\nMoving away from Azure completely falls also in the \"desired CI improvements\" bucket. The new BLAS CI jobs could bespecial-cased too, like SIMD, docs.\r\n\r\n\r\n> Wth `pytest-durations-extra` and `--functions-durations`, I get this:\r\n\r\nnice, that's a helpful tool. \r\n\r\n> It probably makes sense for NumPy to limit testing of PyPy to a sampling strategy and not on every PR.\r\n\r\nThe main problem is PyPy wheel builds, not regular CI. I think we want to keep these wheels. We could look at not running the full test suite though, only the default (fast) tests.","Now that cp312 is mainstream we could probably build all the macos wheels in two matrix entries. cp39-cp312 for <14 (i.e. with openblas) and cp39-cp312 for 14>= (i.e. with accelerate). The CI runners have enough grunt to get through them all in under an hour. That would probably reduce cost, but wouldn't improve iteration speed while debugging.\r\n\r\nTo get through the CI builds quicker for linux_aarch64 we could give each matrix entry more CPU, currently they're only given 1 core each. However, if the CPU is underutilised then one doesn't get efficiency gains.\r\n\r\nw.r.t debugging cirrus-ci - it should be possible to run all the jobs locally if one has a Mac. The cirrus CLI allows one to run the same config on your local computer. I was thinking of writing a guide for how to debug CI configs.","> Now that cp312 is mainstream we could probably build all the macos wheels in two matrix entries\r\n\r\nProbably best not to. It won't make too much difference in overall runtime, just save a couple of minutes for avoiding repo clones and caching conda-forge downloads. Not worth the longer runtime and churn I'd say.\r\n\r\n> To get through the CI builds quicker for linux_aarch64 we could give each matrix entry more CPU, currently they're only given 1 core each. However, if the CPU is underutilised then one doesn't get efficiency gains\r\n\r\n2 cpu's would help I suspect, build time will be almost twice faster, and test suite runtime ~1.7x or so with `pytest-xdist`.\r\n\r\n> I was thinking of writing a guide for how to debug CI configs.\r\n\r\nThat would be very useful I think. Also for other projects to refer to.\r\n\r\n","> w.r.t debugging cirrus-ci - it should be possible to run all the jobs locally if one has a Mac. The cirrus CLI allows one to run the same config on your local computer. I was thinking of writing a guide for how to debug CI configs.\r\n\r\nYes, please!","See https:\/\/github.com\/numpy\/numpy\/wiki\/Debugging-CI-guidelines for some basic guidelines for debugging CI configurations. Bear in mind it's a WIP. @rgommers, this should be good for both scipy and numpy."],"labels":["17 - Task","component: CI"]},{"title":"BUG: crashes in sort\/argsort on macOS arm64","body":"Not sure exactly how long this has been happening, but the macOS arm64 CI job on Cirrus is crashing fairly regularly right now. This happened both before and after the downgrade of the Highway git submodule to 1.0.7\r\n\r\nThe CI logs show things like:\r\n```\r\n________________________ _core\/tests\/test_multiarray.py ________________________\r\n[gw1] darwin -- Python 3.10.11 \/private\/var\/folders\/c2\/b2kmn__12wq57m82ltz0hl1w0000gn\/T\/cibw-run-3g7m_5go\/cp310-macosx_arm64\/venv-test\/bin\/python\r\nworker 'gw1' crashed while running '_core\/tests\/test_multiarray.py::test_sort_float[e-N151]'\r\n```\r\n\r\nIt's not just that one test, there are multiple tests and input values that crash.\r\n\r\nIt could be multiple things, but gh-25247 is probably the first thing to check (Cc @Mousius).","comments":["Could you link to a full Cirrus run @rgommers? ","Sure, here is an example: https:\/\/cirrus-ci.com\/task\/4660394644471808","If it's the main ci run (not a wheel) then this should be visible from PR runs.","I don't think it's related to https:\/\/github.com\/numpy\/numpy\/pull\/25247 for the obvious reason that the compiler used within `cirrus-ci` doesn't support SVE:\r\n```bash\r\nTest features \"NEON NEON_FP16 NEON_VFPV4 ASIMD\" : Supported \r\nTest features \"ASIMDHP\" : Supported \r\nTest features \"ASIMDFHM\" : Supported \r\nTest features \"SVE\" : Unsupported due to Compiler fails against the test code of \"SVE\"\r\nConfiguring npy_cpu_dispatch_config.h using configuration\r\nMessage: \r\nCPU Optimization Options\r\n  baseline:\r\n    Requested : min\r\n    Enabled   : NEON NEON_FP16 NEON_VFPV4 ASIMD\r\n  dispatch:\r\n    Requested : max -xop -fma4\r\n    Enabled   : ASIMDHP ASIMDFHM\r\n```\r\nIt may related to #24018 or #25045\r\n","I've been unable to reproduce this even with the same exact setup, can anyone reproduce this reliably? \ud83e\udd14 @jan-wassenberg can you think of any changes that might've fixed something like this in the next Highway release?","Ah, your testing is with latest Highway? 1.0.7 was Aug30. Since then there were indeed some changes to VQSort:\r\n\r\n- fixing UB in partition https:\/\/github.com\/google\/highway\/commit\/01361d52cd637a3f3c1394c5090798e186900271\r\n- disabling in GCC ASAN due to compiler bug https:\/\/github.com\/google\/highway\/commit\/eb29d601a11f261a58a7e066318e8e42981fb694\r\n- major update of f16 https:\/\/github.com\/google\/highway\/commit\/3bc78b02f46b7210671db3397fc784236909c8bb\r\n- enabling runtime dispatch https:\/\/github.com\/google\/highway\/commit\/cb933022ba6f4dcaf5cd1d67b0627f84095f214a\r\n\r\nThose, especially the first, may well have fixed such a bug.","> Ah, your testing is with latest Highway? 1.0.7 was Aug30. Since then there were indeed some changes to VQSort:\r\n> \r\n> * fixing UB in partition [google\/highway@01361d5](https:\/\/github.com\/google\/highway\/commit\/01361d52cd637a3f3c1394c5090798e186900271)\r\n> * disabling in GCC ASAN due to compiler bug [google\/highway@eb29d60](https:\/\/github.com\/google\/highway\/commit\/eb29d601a11f261a58a7e066318e8e42981fb694)\r\n> * major update of f16 [google\/highway@3bc78b0](https:\/\/github.com\/google\/highway\/commit\/3bc78b02f46b7210671db3397fc784236909c8bb)\r\n> * enabling runtime dispatch [google\/highway@cb93302](https:\/\/github.com\/google\/highway\/commit\/cb933022ba6f4dcaf5cd1d67b0627f84095f214a)\r\n> \r\n> Those, especially the first, may well have fixed such a bug.\r\n\r\nDo you have an update on the next release? We rolled back to `1.0.7` due to compiler issues with the latest `HEAD` and wanting to choose the releases for stability.",":) I understand. Am currently fixing warnings from recent work and hope\/expect we can do a release next week.","Ping @jan-wassenberg. This is problematic since we only see this in the infrequent wheel builds, and the crash prevents the wheel uploads. So downstream projects are not testing against our latest builds.","Acknowledged :) Sorry that I am running behind, am at a conference this week with minimal coding time. I am postponing further work on Highway's ThreadPool until the release is done.","@mattip A quick update, the pre-release work is done and we're currently waiting for two reviewers who should be back tomorrow.","FYI the 1.1.0 release is now done :)","Great, thanks @jan-wassenberg!\r\n\r\n@Mousius are you planning to submit the PR to update to the new Highway version? Would be nice to get this one off the 2.0 milestone.","@jan-wassenberg excellent news! \ud83d\ude38 \r\n@rgommers https:\/\/github.com\/numpy\/numpy\/pull\/25873 \ud83d\ude38 ","perhaps we can close this? has anyone reproduced the crash and confirmed highway 1.1.0 fixes it? ","That crash was happening irregularly in CI (reported several times more in addition to this issue, for example gh-25685 and gh-25686). I wasn't able to reproduce it locally on a macOS M1 machine.\r\n\r\nSo yes, let's go with your suggestion and close this issue - let's keep an eye out for it happening again. ","In the wheel build run on Cirrus CI that just ran on `main` ([CI logs](https:\/\/cirrus-ci.com\/build\/6671602109120512, with commit c3a32b5), 2 out of 4 arm64 jobs crashed in `test_partition` with:\r\n```\r\nworker 'gw2' crashed while running '_core\/tests\/test_multiarray.py::test_partition_int[uint32-N66]'\r\n```\r\nand\r\n```\r\nworker 'gw0' crashed while running '_core\/tests\/test_multiarray.py::test_partition_fp[float32-N82]'\r\n```\r\n\r\nThe other two jobs passed. There doesn't seem to be a correlation with platform or Python versions; of the two failing jobs, one was Python 3.10 on the Sonoma image, the other Python 3.11 on the Monterey image.\r\n\r\nLooking back at previous cron invocations, we had similar issues. On 25 Feb two jobs failed with:\r\n```\r\n_core\/tests\/test_multiarray.py::test_argsort_float[float32-N255]\r\n\r\n_core\/tests\/test_multiarray.py::test_argsort_int[int32-N252]\r\n```\r\n\r\nAnd on 18 Feb one jobs failed with:\r\n```\r\n_core\/tests\/test_multiarray.py::test_sort_float[e-N223]\r\n```\r\n\r\nI'm re-running the two failed jobs now; we still have an issue here though.","@rgommers Are these only on ARM? `np.partition` and `np.argsort` are vectorized only on x86 (avx-512 and avx2). ARM still uses the NumPy scalar version.","Yes, arm64 jobs only. \r\n\r\n> ARM still uses the NumPy scalar version.\r\n\r\nDon't we use Highway for these since gh-24018?","> Don't we use Highway for these since gh-24018?\r\n\r\nonly `np.sort`. not for `np.partition` and `np.argsort`. Although, I just realized the tests `test_partition_int` and `test_partition_fp` call `np.sort`. It is possible it could be coming from there.   \r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/2e3f52faa6244df3bc0f59ee923c04d19e424f9f\/numpy\/_core\/tests\/test_multiarray.py#L10095-L10098","I think that was the assumption here. That's why we wanted to try the new Highway release.","yeah, the fact that we have failure in the sort test `_core\/tests\/test_multiarray.py::test_sort_float[e-N223]` also points to it. ","@Mousius with an arm64 macOS machine the failures should be reproducible locally with a Tart VM image: https:\/\/github.com\/numpy\/numpy\/wiki\/Debugging-CI-guidelines#local-build-of-macos-wheels-using-cibuildwheel-in-a-macos-tart-vm. Maybe you'd be able to give that a try?\r\n\r\n@charris this shouldn't be blocking for release branch creation, but it's likely to be a blocker for the final release (and it's going to require hitting the \"re-try failures\" button until all jobs pass in order to update wheels for 2.0.0rc1).\r\n","@rgommers I'm waiting for the `copy` keyword fallout to settle before branching.\r\n\r\nEDIT: And if we release rc1 with this, I need to remember to note the occasional failures in the release notes.","> @rgommers I'm waiting for the `copy` keyword fallout to settle before branching.\r\n\r\nYes - there's several loose ends we need to get in before branching. Let me open a new issue for it now.","@jan-wassenberg could this be related to your comments in https:\/\/github.com\/numpy\/numpy\/pull\/25781#issuecomment-1974307487 ? I see in our VQSort file, we haven't got any HWY_ATTR \/ HWY_BEFORE_NAMESPACE etc? See: https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/_core\/src\/npysort\/highway_qsort.dispatch.cpp","Good spot, indeed missing there. I think missing ATTR can only cause build failures, though.\r\nIt's probably OK to omit them because the VQSort functions are large enough that they will likely not be inlined.","Thanks @ngoldbaum, for giving me some lldb incantations. Outside of the `tart vm` seems to have tripped it straight away:\r\n\r\n```\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x122600000)\r\n    frame #0: 0x000000010271c520 _multiarray_umath.cpython-311-darwin.so`bool hwy::N_NEON_WITHOUT_AES::detail::MaybePartitionTwoValue<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t*, unsigned long, decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)()))&, hwy::float16_t*) [inlined] void hwy::N_NEON_WITHOUT_AES::StoreU<hwy::N_NEON_WITHOUT_AES::Simd<unsigned short, 8ul, 0>, (void*)0, (void*)0>(v=<unavailable>, (null)=<unavailable>, unaligned=<unavailable>) at arm_neon-inl.h:3295:3 [opt]\r\n   3292\ttemplate <class D, HWY_IF_V_SIZE_D(D, 16), HWY_IF_U16_D(D)>\r\n   3293\tHWY_API void StoreU(Vec128<uint16_t> v, D \/* tag *\/,\r\n   3294\t                    uint16_t* HWY_RESTRICT unaligned) {\r\n-> 3295\t  vst1q_u16(unaligned, v.raw);\r\n   3296\t}\r\n   3297\ttemplate <class D, HWY_IF_V_SIZE_D(D, 16), HWY_IF_U32_D(D)>\r\n   3298\tHWY_API void StoreU(Vec128<uint32_t> v, D \/* tag *\/,\r\nwarning: _multiarray_umath.cpython-311-darwin.so was compiled with optimization - stepping may behave oddly; variables may not be available.\r\n(lldb) bt\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x122600000)\r\n  * frame #0: 0x000000010271c520 _multiarray_umath.cpython-311-darwin.so`bool hwy::N_NEON_WITHOUT_AES::detail::MaybePartitionTwoValue<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t*, unsigned long, decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)()))&, hwy::float16_t*) [inlined] void hwy::N_NEON_WITHOUT_AES::StoreU<hwy::N_NEON_WITHOUT_AES::Simd<unsigned short, 8ul, 0>, (void*)0, (void*)0>(v=<unavailable>, (null)=<unavailable>, unaligned=<unavailable>) at arm_neon-inl.h:3295:3 [opt]\r\n    frame #1: 0x000000010271c520 _multiarray_umath.cpython-311-darwin.so`bool hwy::N_NEON_WITHOUT_AES::detail::MaybePartitionTwoValue<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t*, unsigned long, decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)()))&, hwy::float16_t*) [inlined] void hwy::N_NEON_WITHOUT_AES::StoreU<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, (void*)0>(v=<unavailable>, d=<unavailable>, p=<unavailable>) at arm_neon-inl.h:3457:10 [opt]\r\n    frame #2: 0x000000010271c520 _multiarray_umath.cpython-311-darwin.so`bool hwy::N_NEON_WITHOUT_AES::detail::MaybePartitionTwoValue<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t*, unsigned long, decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)())), decltype(Zero((hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>)()))&, hwy::float16_t*) [inlined] void hwy::N_NEON_WITHOUT_AES::BlendedStore<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>>(v=<unavailable>, m=<unavailable>, d=<unavailable>, p=<unavailable>) at arm_neon-inl.h:3480:3 [opt]\r\n    frame #3: 0x000000010271c520 _multiarray_umath.cpython-311-darwin.so`bool hwy::N_NEON_WITHOUT_AES::detail::MaybePartitionTwoValue<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(d=<unavailable>, st=<unavailable>, keys=0x00000001225ffe4e, num=217, valueL=<unavailable>, valueR=<unavailable>, third=0x000000016fdfbb10, buf=0x000000016fdfbbd0) at vqsort-inl.h:840:3 [opt]\r\n    frame #4: 0x0000000102717ee8 _multiarray_umath.cpython-311-darwin.so`void hwy::N_NEON_WITHOUT_AES::detail::Recurse<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t*, unsigned long, hwy::float16_t*, unsigned long long*, unsigned long) [inlined] bool hwy::N_NEON_WITHOUT_AES::detail::PartitionIfTwoKeys<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(d=<unavailable>, st=<unavailable>, pivot=<unavailable>, keys=0x00000001225ffd90, num=312, idx_second=<unavailable>, second=<unavailable>, third=0x000000016fdfbb10, buf=0x000000016fdfbbd0) at vqsort-inl.h:978:22 [opt]\r\n    frame #5: 0x0000000102717e68 _multiarray_umath.cpython-311-darwin.so`void hwy::N_NEON_WITHOUT_AES::detail::Recurse<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(d=<unavailable>, st=<unavailable>, keys=0x00000001225ffd90, num=312, buf=0x000000016fdfbbd0, state=0x0000600003d93af8, remaining_levels=50) at vqsort-inl.h:1610:9 [opt]\r\n    frame #6: 0x000000010271729c _multiarray_umath.cpython-311-darwin.so`void hwy::N_NEON_WITHOUT_AES::VQSortStatic<hwy::float16_t>(hwy::float16_t*, unsigned long, hwy::SortAscending) at vqsort-inl.h:1813:5 [opt]\r\n    frame #7: 0x00000001027170dc _multiarray_umath.cpython-311-darwin.so`void hwy::N_NEON_WITHOUT_AES::VQSortStatic<hwy::float16_t>(hwy::float16_t*, unsigned long, hwy::SortAscending) [inlined] void hwy::N_NEON_WITHOUT_AES::Sort<hwy::N_NEON_WITHOUT_AES::Simd<hwy::float16_t, 8ul, 0>, hwy::N_NEON_WITHOUT_AES::detail::SharedTraits<hwy::N_NEON_WITHOUT_AES::detail::TraitsLane<hwy::N_NEON_WITHOUT_AES::detail::OrderAscending<hwy::float16_t>>>, hwy::float16_t>(d=<unavailable>, st=<unavailable>, keys=<unavailable>, num=312) at vqsort-inl.h:1845:10 [opt]\r\n    frame #8: 0x00000001027170dc _multiarray_umath.cpython-311-darwin.so`void hwy::N_NEON_WITHOUT_AES::VQSortStatic<hwy::float16_t>(keys=0x00000001225ffd90, num=312, (null)=<unavailable>) at vqsort-inl.h:1904:3 [opt]\r\n    frame #9: 0x0000000102597124 _multiarray_umath.cpython-311-darwin.so`bool quicksort_dispatch<np::Half>(start=0x00000001225ffd90, num=312) at quicksort.cpp:105:9\r\n    frame #10: 0x0000000102597070 _multiarray_umath.cpython-311-darwin.so`quicksort_half(start=0x00000001225ffd90, n=312, __NPY_UNUSED_TAGGEDvarr=0x00000001270efab0) at quicksort.cpp:798:9\r\n    frame #11: 0x0000000102539ad4 _multiarray_umath.cpython-311-darwin.so`_new_sortlike(op=0x00000001270efab0, axis=0, sort=(_multiarray_umath.cpython-311-darwin.so`::quicksort_half(void *, npy_intp, void *) at quicksort.cpp:797), part=0x0000000000000000, kth=0x0000000000000000, nkth=0) at item_selection.c:1255:19\r\n    frame #12: 0x00000001025395bc _multiarray_umath.cpython-311-darwin.so`PyArray_Sort(op=0x00000001270efab0, axis=0, which=NPY_QUICKSORT) at item_selection.c:1549:12\r\n    frame #13: 0x0000000102559a94 _multiarray_umath.cpython-311-darwin.so`array_sort(self=0x00000001270efab0, args=0x00000001001116a0, len_args=0, kwnames=0x0000000101de8080) at methods.c:1285:11\r\n    frame #14: 0x0000000100970d1c Python`method_vectorcall_FASTCALL_KEYWORDS + 96\r\n    frame #15: 0x0000000100a3bfa0 Python`_PyEval_EvalFrameDefault + 40444\r\n    frame #16: 0x0000000100a4072c Python`_PyEval_Vector + 116\r\n    frame #17: 0x0000000100964ab4 Python`PyObject_Vectorcall + 76\r\n    frame #18: 0x00000001024d83fc _multiarray_umath.cpython-311-darwin.so`dispatcher_vectorcall(self=0x0000000101e00a30, args=0x0000000100111600, len_args=-9223372036854775807, kwnames=0x00000001109bf280) at arrayfunction_override.c:588:18\r\n    frame #19: 0x0000000100a3bfa0 Python`_PyEval_EvalFrameDefault + 40444\r\n```\r\n\r\nSo, this answers @seiko2plus's question about whether we're using SIMD. We are \ud83d\ude38 \r\n\r\nIf I compile with `-Db_lundef=false -Db_sanitize=address`, I get import errors when running the tests. I still need to debug that.\r\n\r\nAlso, I ran the tests again, and it's not happening anymore...\r\n","I did see one crash on macosx, python 3.10, in one of the wheel build runs for the 2.0.0b1 release.","There is a similar crash on the [PyPy 3.9 macos arm64 wheel builds](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/8216795820\/job\/22471760587?pr=25977), which repeated the second time I ran it","See https:\/\/github.com\/numpy\/numpy\/pull\/25987\/checks?check_run_id=22490803090 for a sort failure on arm64.","Is it possible we could get better error messages by disabling pytest parallelism?","If we can't figure this out, I think we should revert using Highway in the arm64 sort ufuncs. It has proven unstable, we can't release with this kind of heisenbug.","> Is it possible we could get better error messages by disabling pytest parallelism?\r\n\r\nUnlikely, I don't think `pytest` gives useful output on segfaults either way. \r\n\r\n> If we can't figure this out, I think we should revert using Highway in the arm64 sort ufuncs. It has proven unstable, we can't release with this kind of heisenbug.\r\n\r\nThat sounds right to me - we should do so later this week unless the problem is resolved, and then work towards re-enabling it in `main`.","It seems the PyPy case fails consistently, perhaps exploration could start there.\r\n\r\ncc @seiko2plus @Mousius.","> That sounds right to me - we should do so later this week unless the problem is resolved, and then work towards re-enabling it in `main`.\r\n\r\n@seiko2plus pointed out that we could leave this enabled for non-AArch64, as they don't seem to be erroring. That'd leave most of the code intact and the benefit still available to some users \ud83d\ude38 ","x86-64 is covered by `x86-simd-sort`, right? And architectures other than Intel\/Arm are only tested lightly under Qemu, which could hide a problem like this perhaps? With so few users on those non-standard architectures, we may not get any testing signal in time.","> x86-64 is covered by `x86-simd-sort`, right? \r\n\r\nyes. \r\n\r\n> With so few users on those non-standard architectures, we may not get any testing signal in time.\r\n\r\n@jan-wassenberg might be able to clarify which architectures they test in their CI\/release process for vqsort. NumPy currently uses highway for SVE, ASIMD, ASIMDHP, VSX2. ","> could leave this enabled for non-AArch64, as they don't seem to be erroring\r\n\r\nWhat would be the easiest way to disable Highway sorting code for arm64?","@mattip just remove ASIMD (or is it SVE?) from this line: \r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/e7568e93119197c0a9f29098ccf1a57b61743673\/numpy\/_core\/meson.build#L824","It'd be `ASIMD`, we haven't seen any issues with `SVE` as yet \ud83e\udd14 We could also only enable it for Linux, which we've not seen any issues on?","@Mousius how consistent was the segfault you saw in the [comment above](https:\/\/github.com\/numpy\/numpy\/issues\/25464#issuecomment-1981653866)? Enough that you could see that disabling `ASIMD` solves the problem?","> > x86-64 is covered by `x86-simd-sort`, right?\r\n> \r\n> yes.\r\n> \r\n> > With so few users on those non-standard architectures, we may not get any testing signal in time.\r\n> \r\n> @jan-wassenberg might be able to clarify which architectures they test in their CI\/release process for vqsort. NumPy currently uses highway for SVE, ASIMD, ASIMDHP, VSX2.\r\n\r\nSure, CI is x86, WASM, RVV, Armv7, Armv8 ASIMD (native HW), Arm SVE, Arm SVE2, plus msan\/asan.\r\nRelease: also GCC, POWER8, POWER9, Z14, Z15.","> @Mousius how consistent was the segfault you saw in the [comment above](https:\/\/github.com\/numpy\/numpy\/issues\/25464#issuecomment-1981653866)? Enough that you could see that disabling `ASIMD` solves the problem?\r\n\r\nI saw it once; it's proven very difficult to reproduce again despite the same conditions. Disabling ASIMD should fix it by disabling any optimisation whilst we figure out why clang on darwin is causing issues. Unfortunately, it would also disable ASIMD for Linux, which seems well-tested by Highway, and we haven't seen any failures from the Linux AArch64 wheel builds?\r\n\r\nIf it seems acceptable to limit it to Linux, I'm happy to raise a patch so we can get something in to mitigate this.","Looking back over the cirrus CI runs https:\/\/cirrus-ci.com\/github\/numpy\/numpy, and diving into the last failures, I only see arm64 ones. Can we change the meson build condition to include the platform?","@mattip https:\/\/github.com\/numpy\/numpy\/pull\/26000 is what I'm suggesting.","Thanks for the immediate mitigation, that will take the pressure off. Any progress with this?\r\n> If I compile with `-Db_lundef=false -Db_sanitize=address`, I get import errors when running the tests"],"labels":["00 - Bug"]},{"title":"BUG: `np.log(max_value_for_longdouble)` fails on Linux aarch64","body":"Failure in today's aarch64 linux wheel build run:\r\n\r\n```\r\n___________________________ TestLog.test_log_values ____________________________\r\n[gw0] linux -- Python 3.10.13 \/tmp\/tmp.gkfDdM\/venv\/bin\/python\r\nself = <numpy._core.tests.test_umath.TestLog object at 0xffff9d34f8e0>\r\n    def test_log_values(self):\r\n        x = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\r\n        y = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n        for dt in ['f', 'd', 'g']:\r\n            log2_ = 0.69314718055994530943\r\n            xf = np.array(x, dtype=dt)\r\n            yf = np.array(y, dtype=dt)*log2_\r\n            assert_almost_equal(np.log(xf), yf)\r\n    \r\n        # test aliasing(issue #17761)\r\n        x = np.array([2, 0.937500, 3, 0.947500, 1.054697])\r\n        xf = np.log(x)\r\n        assert_almost_equal(np.log(x, out=x), xf)\r\n    \r\n        # test log() of max for dtype does not raise\r\n        for dt in ['f', 'd', 'g']:\r\n            with np.errstate(all='raise'):\r\n                x = np.finfo(dt).max\r\n>               np.log(x)\r\nE               FloatingPointError: overflow encountered in log\r\ndt         = 'g'\r\nlog2_      = 0.6931471805599453\r\nself       = <numpy._core.tests.test_umath.TestLog object at 0xffff9d34f8e0>\r\nx          = np.longdouble('1.189731495357231765085759326628007e+4932')\r\nxf         = array([ 0.69314718, -0.06453852,  1.09861229, -0.05392834,  0.05325352])\r\ny          = [0, 1, 2, 3, 4, 5, ...]\r\nyf         = array([0.        , 0.69314718, 1.38629436, 2.07944154, 2.77258872,\r\n       3.4657359 , 4.15888308, 4.85203026, 5.54517744, 6.23832463,\r\n       6.93147181], dtype=float128)\r\n..\/venv\/lib\/python3.10\/site-packages\/numpy\/_core\/tests\/test_umath.py:1359: FloatingPointError\r\n```\r\n\r\nThis used to pass until ~1.5 months ago, which is when we had the last successful nightly wheel build. It's a pretty niche failure, so I'm going to skip the `longdouble` check for now.","comments":[],"labels":["00 - Bug","component: numpy._core"]},{"title":"BUG: test error collecting `f2py\/tests\/test_abstract_interface.py`","body":"First time I've seen this one, it occurred in the PyPy on Windows wheel build job (see [CI log](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/7290847257\/job\/19868540074?pr=25446)):\r\n```\r\n  ERROR f2py\/tests\/test_abstract_interface.py - PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n  ERROR gw1\r\n```\r\n\r\nFull traceback:\r\n\r\n<details>\r\n\r\n```\r\n ___________ ERROR collecting f2py\/tests\/test_abstract_interface.py ____________\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\runner.py:341: in from_call\r\n      result: Optional[TResult] = func()\r\n          cls        = <class '_pytest.runner.CallInfo'>\r\n          duration   = 10.200289099999999\r\n          excinfo    = <ExceptionInfo PermissionError(13, 'The process cannot access the file because it is being used by another process') tblen=24>\r\n          func       = <function pytest_make_collect_report.<locals>.<lambda> at 0x0000027f94cf08e0>\r\n          precise_start = 26.6201467\r\n          precise_stop = 36.8204358\r\n          reraise    = None\r\n          result     = None\r\n          start      = 1703177221.1964283\r\n          stop       = 1703177231.3974376\r\n          when       = 'collect'\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\runner.py:372: in <lambda>\r\n      call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\r\n          collector  = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\python.py:531: in collect\r\n      self._inject_setup_module_fixture()\r\n          __class__  = <class '_pytest.python.Module'>\r\n          self       = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\python.py:545: in _inject_setup_module_fixture\r\n      self.obj, (\"setUpModule\", \"setup_module\")\r\n          has_nose   = True\r\n          self       = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\python.py:310: in obj\r\n      self._obj = obj = self._getobj()\r\n          obj        = None\r\n          self       = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\python.py:528: in _getobj\r\n      return self._importtestmodule()\r\n          self       = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\python.py:617: in _importtestmodule\r\n      mod = import_path(self.path, mode=importmode, root=self.config.rootpath)\r\n          importmode = 'prepend'\r\n          self       = <Module test_abstract_interface.py>\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\pathlib.py:565: in import_path\r\n      importlib.import_module(module_name)\r\n          mode       = <ImportMode.prepend: 'prepend'>\r\n          module_name = 'numpy.f2py.tests.test_abstract_interface'\r\n          names      = ['numpy', 'f2py', 'tests', 'test_abstract_interface']\r\n          p          = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy\/f2py\/tests\/test_abstract_interface.py')\r\n          path       = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy\/f2py\/tests\/test_abstract_interface.py')\r\n          pkg_path   = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy')\r\n          pkg_root   = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages')\r\n          root       = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/test_cwd')\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127: in import_module\r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n          level      = 0\r\n          name       = 'numpy.f2py.tests.test_abstract_interface'\r\n          package    = None\r\n  <frozen importlib._bootstrap>:1030: in _gcd_import\r\n      ???\r\n          level      = 0\r\n          name       = 'numpy.f2py.tests.test_abstract_interface'\r\n          package    = None\r\n  <frozen importlib._bootstrap>:1007: in _find_and_load\r\n      ???\r\n          import_    = <function _gcd_import at 0x00007ff8f29dec70>\r\n          module     = <object object at 0x00007ff8f29deb90>\r\n          name       = 'numpy.f2py.tests.test_abstract_interface'\r\n  <frozen importlib._bootstrap>:986: in _find_and_load_unlocked\r\n      ???\r\n          import_    = <function _gcd_import at 0x00007ff8f29dec70>\r\n          name       = 'numpy.f2py.tests.test_abstract_interface'\r\n          parent     = 'numpy.f2py.tests'\r\n          parent_module = <module 'numpy.f2py.tests' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\__init__.py'>\r\n          path       = ['C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests']\r\n          spec       = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py')\r\n  <frozen importlib._bootstrap>:680: in _load_unlocked\r\n      ???\r\n          module     = <module 'numpy.f2py.tests.test_abstract_interface' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py'>\r\n          spec       = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py')\r\n  ..\\venv-test\\lib\\site-packages\\_pytest\\assertion\\rewrite.py:178: in exec_module\r\n      exec(co, module.__dict__)\r\n          cache_dir  = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy\/f2py\/tests\/__pycache__')\r\n          cache_name = 'test_abstract_interface.pypy39-pytest-7.4.0.pyc'\r\n          co         = <code object <module> at 0x0000027f929f2a18, file \"C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py\", line 1>\r\n          fn         = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy\/f2py\/tests\/test_abstract_interface.py')\r\n          module     = <module 'numpy.f2py.tests.test_abstract_interface' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py'>\r\n          ok         = True\r\n          pyc        = WindowsPath('C:\/Users\/runneradmin\/AppData\/Local\/Temp\/cibw-run-52rc4ryf\/pp39-win_amd64\/venv-test\/lib\/site-packages\/numpy\/f2py\/tests\/__pycache__\/test_abstract_interface.pypy39-pytest-7.4.0.pyc')\r\n          self       = <_pytest.assertion.rewrite.AssertionRewritingHook object at 0x0000027ffb8cf130>\r\n          state      = <_pytest.assertion.AssertionState object at 0x0000027ffb8cfc58>\r\n          write      = True\r\n  ..\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\test_abstract_interface.py:4: in <module>\r\n      from . import util\r\n          Path       = <class 'pathlib.Path'>\r\n          __builtins__ = <builtins>\r\n          __cached__ = 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\__pycache__\\\\test_abstract_interface.pypy39.pyc'\r\n          __doc__    = None\r\n          __file__   = 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py'\r\n          __loader__ = <_pytest.assertion.rewrite.AssertionRewritingHook object at 0x0000027ffb8cf130>\r\n          __name__   = 'numpy.f2py.tests.test_abstract_interface'\r\n          __package__ = 'numpy.f2py.tests'\r\n          __spec__   = ModuleSpec(name='numpy.f2py.tests.test_abstract_interface', loader=<_pytest.assertion.rewrite.AssertionRewritingHook o...emp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\test_abstract_interface.py')\r\n          pytest     = <module 'pytest' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\pytest\\\\__init__.py'>\r\n          textwrap   = <module 'textwrap' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\textwrap.py'>\r\n  ..\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\util.py:276: in <module>\r\n      checker.check_compilers()\r\n          CompilerChecker = <class 'numpy.f2py.tests.util.CompilerChecker'>\r\n          IS_WASM    = False\r\n          MesonBackend = <class 'numpy.f2py._backends._meson.MesonBackend'>\r\n          Path       = <class 'pathlib.Path'>\r\n          __builtins__ = <builtins>\r\n          __cached__ = 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\__pycache__\\\\util.pypy39.pyc'\r\n          __doc__    = '\\nUtility functions for\\n\\n- building and importing modules on test time, using a temporary location\\n- detecting if compilers are present\\n- determining paths to tests\\n\\n'\r\n          __file__   = 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\util.py'\r\n          __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x0000027f94d094b0>\r\n          __name__   = 'numpy.f2py.tests.util'\r\n          __package__ = 'numpy.f2py.tests'\r\n          __spec__   = ModuleSpec(name='numpy.f2py.tests.util', loader=<_frozen_importlib_external.SourceFileLoader object at 0x0000027f94d09...\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\f2py\\\\tests\\\\util.py')\r\n          _cleanup   = <function _cleanup at 0x0000027f94d42a20>\r\n          _memoize   = <function _memoize at 0x0000027f94d42c00>\r\n          _module_dir = None\r\n          _module_num = 5403\r\n          asunicode  = <function asunicode at 0x0000027ffbe4d560>\r\n          atexit     = <module 'atexit' (built-in)>\r\n          build_code = <function _memoize.<locals>.wrapper at 0x0000027f94d42d40>\r\n          build_module = <function _memoize.<locals>.wrapper at 0x0000027f94d42ca0>\r\n          check_language = <function check_language at 0x0000027f94d42340>\r\n          checker    = <numpy.f2py.tests.util.CompilerChecker object at 0x0000027f94d40950>\r\n          concurrent = <module 'concurrent' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\concurrent\\\\__init__.py'>\r\n          contextlib = <module 'contextlib' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\contextlib.py'>\r\n          fortran77_code = \"\\nC Example Fortran 77 code\\n      PROGRAM HELLO\\n      PRINT *, 'Hello, Fortran 77!'\\n      END\\n\"\r\n          fortran90_code = \"\\n! Example Fortran 90 code\\nprogram hello90\\n  type :: greeting\\n    character(len=20) :: text\\n  end type greeting\\n\\n  type(greeting) :: greet\\n  greet%text = 'hello, fortran 90!'\\n  print *, greet%text\\nend program hello90\\n\"\r\n          get_module_dir = <function get_module_dir at 0x0000027f94d42ac0>\r\n          get_temp_module_name = <function get_temp_module_name at 0x0000027f94d42b60>\r\n          glob       = <module 'glob' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\glob.py'>\r\n          import_module = <function import_module at 0x0000027ffa535e20>\r\n          numpy      = <module 'numpy' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py'>\r\n          os         = <module 'os' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\os.py'>\r\n          pytest     = <module 'pytest' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\lib\\\\site-packages\\\\pytest\\\\__init__.py'>\r\n          re         = <module 're' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\re.py'>\r\n          shutil     = <module 'shutil' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\shutil.py'>\r\n          subprocess = <module 'subprocess' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\subprocess.py'>\r\n          sys        = <module 'sys' (built-in)>\r\n          tempfile   = <module 'tempfile' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\tempfile.py'>\r\n          temppath   = <function temppath at 0x0000027f85bf6020>\r\n          textwrap   = <module 'textwrap' from 'C:\\\\Users\\\\runneradmin\\\\AppData\\\\Local\\\\pypa\\\\cibuildwheel\\\\Cache\\\\pypy3.9-v7.3.12-win64\\\\Lib\\\\textwrap.py'>\r\n  ..\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\util.py:269: in check_compilers\r\n      self.has_c = futures[0].result()\r\n          executor   = <concurrent.futures.thread.ThreadPoolExecutor object at 0x0000027f94d40988>\r\n          futures    = [<Future at 0x27f94d409c0 state=finished raised PermissionError>, <Future at 0x27f94d40870 state=finished returned bool>, <Future at 0x27f94d40598 state=finished returned bool>]\r\n          self       = <numpy.f2py.tests.util.CompilerChecker object at 0x0000027f94d40950>\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\concurrent\\futures\\_base.py:446: in result\r\n      return self.__get_result()\r\n          self       = None\r\n          timeout    = None\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\concurrent\\futures\\_base.py:391: in __get_result\r\n      raise self._exception\r\n          self       = None\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\concurrent\\futures\\thread.py:58: in run\r\n      result = self.fn(*self.args, **self.kwargs)\r\n          self       = None\r\n  ..\\venv-test\\lib\\site-packages\\numpy\\f2py\\tests\\util.py:229: in check_language\r\n      shutil.rmtree(tmpdir)\r\n          code_snippet = None\r\n          f          = <_io.TextIOWrapper name='C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build' mode='w' encoding='UTF-8'>\r\n          lang       = 'c'\r\n          meson_file = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n          runmeson   = CompletedProcess(args=['meson', 'setup', 'btmp'], returncode=0, stdout=b'The Meson build system\\r\\nVersion: 1.3.0\\r\\nS...\\\\runneradmin\\\\AppData\\\\Local\\\\Temp\\\\cibw-run-52rc4ryf\\\\pp39-win_amd64\\\\venv-test\\\\Scripts\\\\ninja.EXE\\r\\n', stderr=b'')\r\n          tmpdir     = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4'\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\shutil.py:759: in rmtree\r\n      return _rmtree_unsafe(path, onerror)\r\n          ignore_errors = False\r\n          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>\r\n          path       = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4'\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\shutil.py:629: in _rmtree_unsafe\r\n      onerror(os.unlink, fullname, sys.exc_info())\r\n          entries    = [<DirEntry 'btmp'>, <DirEntry 'meson.build'>]\r\n          entry      = <DirEntry 'meson.build'>\r\n          fullname   = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>\r\n          path       = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4'\r\n          scandir_it = <posix.ScandirIterator object at 0x0000027f94d21910>\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\shutil.py:627: in _rmtree_unsafe\r\n      os.unlink(fullname)\r\n  E   PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n          entries    = [<DirEntry 'btmp'>, <DirEntry 'meson.build'>]\r\n          entry      = <DirEntry 'meson.build'>\r\n          fullname   = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n          onerror    = <function rmtree.<locals>.onerror at 0x0000027f94d42660>\r\n          path       = 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4'\r\n          scandir_it = <posix.ScandirIterator object at 0x0000027f94d21910>\r\n  ____________________________ ERROR collecting gw1 _____________________________\r\n  Different tests were collected between gw3 and gw1. The difference is:\r\n  --- gw3\r\n  \r\n  +++ gw1\r\n  \r\n  @@ -32556,8 +32556,6 @@\r\n  \r\n   distutils\/tests\/test_system_info.py::TestSystemInfoReading::test_compile2\r\n   distutils\/tests\/test_system_info.py::TestSystemInfoReading::test_overrides\r\n   distutils\/tests\/test_system_info.py::test_distutils_parse_env_order\r\n  -f2py\/tests\/test_abstract_interface.py::TestAbstractInterface::test_abstract_interface\r\n  -f2py\/tests\/test_abstract_interface.py::TestAbstractInterface::test_parse_abstract_interface\r\n   f2py\/tests\/test_array_from_pyobj.py::TestIntent::test_in_out\r\n   f2py\/tests\/test_array_from_pyobj.py::TestSharedMemory::test_in_from_2seq[BOOL]\r\n   f2py\/tests\/test_array_from_pyobj.py::TestSharedMemory::test_in_from_2casttype[BOOL]\r\n  C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-52rc4ryf\\pp39-win_amd64\\venv-test\\lib\\site-packages\\numpy\\_pytesttester.py:143: DeprecationWarning: \r\n  \r\n    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\r\n    of the deprecation of `distutils` itself. It will be removed for\r\n    Python >= 3.12. For older Python versions it will remain present.\r\n    It is recommended to use `setuptools < 60.0` for those Python versions.\r\n    For more details, see:\r\n      https:\/\/numpy.org\/devdocs\/reference\/distutils_status_migration.html \r\n  \r\n  \r\n    from numpy.distutils import cpuinfo\r\n  To see why this happens see Known limitations in documentation\r\n  ============================== warnings summary ===============================\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n    C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n  \r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127\r\n    C:\\Users\\runneradmin\\AppData\\Local\\pypa\\cibuildwheel\\Cache\\pypy3.9-v7.3.12-win64\\Lib\\importlib\\__init__.py:127: DeprecationWarning: \r\n    \r\n      `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\r\n      of the deprecation of `distutils` itself. It will be removed for\r\n      Python >= 3.12. For older Python versions it will remain present.\r\n      It is recommended to use `setuptools < 60.0` for those Python versions.\r\n      For more details, see:\r\n        https:\/\/numpy.org\/devdocs\/reference\/distutils_status_migration.html \r\n    \r\n    \r\n      return _bootstrap._gcd_import(name[level:], package, level)\r\n  \r\n  -- Docs: https:\/\/docs.pytest.org\/en\/stable\/how-to\/capture-warnings.html\r\n  =========================== short test summary info ===========================\r\n  ERROR f2py\/tests\/test_abstract_interface.py - PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpye0_j0h4\\\\meson.build'\r\n  ERROR gw1\r\n  8 warnings, 2 errors in 55.76s\r\n  Error: Command bash D:\\a\\numpy\\numpy\/tools\/wheels\/cibw_test_command.sh D:\\a\\numpy\\numpy failed with code 1. None\r\n```\r\n\r\n<\/details>\r\n\r\n@HaoZeke this looks like a bug in the test. The subclassing of `util.F2PyTest` seems to be causing the module to be attempted to be built more than once, which won't work when the test suite is run in parallel.","comments":[],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"BUG: pip install fails on ppc64le after VSX4 introducion","body":"### Describe the issue:\r\n\r\nInstallation using pip fails on ppc64le. \r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nconda create -y -n buildenvgxx11 gxx_linux-ppc64le\r\nconda activate buildenvgxx11\r\npip install numpy==1.23.5\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nINFO: compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/multiarray -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath -Inumpy\/core\/include -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/include\/numpy -Ibuild\/src.linux-ppc64le-3.10\/numpy\/distutils\/include -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/npysort -Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/opt\/conda\/include\/python3.10 -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/npymath -c'\r\n      extra options: '-O3 -mcpu=power10 -mtune=power10'\r\n      INFO: powerpc64le-conda-linux-gnu-cc: build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.vsx4.c\r\n      INFO: powerpc64le-conda-linux-gnu-cc: build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_trigonometric.dispatch.vsx4.c\r\n      INFO: powerpc64le-conda-linux-gnu-cc: build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_hyperbolic.dispatch.vsx4.c\r\n      during RTL pass: expand\r\n      In file included from build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_hyperbolic.dispatch.vsx4.c:11:\r\n      numpy\/core\/src\/umath\/loops_hyperbolic.dispatch.c.src: In function 'FLOAT_tanh_VSX4':\r\n      numpy\/core\/src\/umath\/loops_hyperbolic.dispatch.c.src:374:9: internal compiler error: in rs6000_sibcall_aix, at config\/rs6000\/rs6000.c:25670\r\n        374 |         npy_clear_floatstatus_barrier((char*)dimensions);\r\n            |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      Please submit a full bug report,\r\n      with preprocessed source if appropriate.\r\n      See <https:\/\/gcc.gnu.org\/bugs\/> for instructions.\r\n      during RTL pass: expand\r\n      In file included from build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.vsx4.c:11:\r\n      numpy\/core\/src\/umath\/loops_arithmetic.dispatch.c.src: In function 'BYTE_divide_VSX4':\r\n      numpy\/core\/src\/umath\/loops_arithmetic.dispatch.c.src:76:13: internal compiler error: in rs6000_sibcall_aix, at config\/rs6000\/rs6000.c:25670\r\n         76 |             npy_set_floatstatus_overflow();\r\n            |             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      Please submit a full bug report,\r\n      with preprocessed source if appropriate.\r\n      See <https:\/\/gcc.gnu.org\/bugs\/> for instructions.\r\n      error: Command \"<myenv>buildenvgxx11\/bin\/powerpc64le-conda-linux-gnu-cc -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O3 -Wall -mcpu=power8 -fPIC -O3 -isystem \/opt\/conda\/include -mcpu=power8 -fPIC -O3 -isystem \/opt\/conda\/include -mcpu=power8 -mtune=power8 -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O3 -pipe -isystem <myenv>buildenvgxx11\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem <myenv>buildenvgxx11\/include -fPIC -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/multiarray -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath -Inumpy\/core\/include -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/include\/numpy -Ibuild\/src.linux-ppc64le-3.10\/numpy\/distutils\/include -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/npysort -Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/opt\/conda\/include\/python3.10 -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-ppc64le-3.10\/numpy\/core\/src\/npymath -c build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.vsx4.c -o build\/temp.linux-ppc64le-3.10\/build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.vsx4.o -MMD -MF build\/temp.linux-ppc64le-3.10\/build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.vsx4.o.d -O3 -mcpu=power10 -mtune=power10\" failed with exit status 1\r\n      \r\n      \r\n\r\n      ########### EXT COMPILER OPTIMIZATION ###########\r\n      INFO: Platform      :\r\n        Architecture: ppc64le\r\n        Compiler    : unix-like\r\n\r\n      CPU baseline  :\r\n        Requested   : 'min'\r\n        Enabled     : VSX VSX2\r\n        Flags       : -mcpu=power8\r\n        Extra checks: VSX_ASM\r\n\r\n      CPU dispatch  :\r\n        Requested   : 'max -xop -fma4'\r\n        Enabled     : VSX3 VSX4\r\n        Generated   :\r\n                    :\r\n        VSX3        : VSX VSX2\r\n        Flags       : -mcpu=power9 -mtune=power9\r\n        Extra checks: none\r\n        Detect      : VSX3\r\n                    : build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_trigonometric.dispatch.c\r\n                    :\r\n        VSX4        : VSX VSX2 VSX3\r\n        Flags       : -mcpu=power10 -mtune=power10\r\n        Extra checks: VSX4_MMA\r\n        Detect      : VSX4\r\n                    : build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_arithmetic.dispatch.c\r\n                    : build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_trigonometric.dispatch.c\r\n                    : build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_hyperbolic.dispatch.c\r\n                    : build\/src.linux-ppc64le-3.10\/numpy\/core\/src\/umath\/loops_modulo.dispatch.c\r\n      INFO: CCompilerOpt.cache_flush[857] : write cache to path -> \/tmp\/pip-install-kw88t5sf\/numpy_228008dbdfce4094a9475c5197e2ff88\/build\/temp.linux-ppc64le-3.10\/ccompiler_opt_cache_ext.py\r\n      INFO:\r\n      ########### CLIB COMPILER OPTIMIZATION ###########\r\n      INFO: Platform      :\r\n        Architecture: ppc64le\r\n        Compiler    : unix-like\r\n\r\n      CPU baseline  :\r\n        Requested   : 'min'\r\n        Enabled     : VSX VSX2\r\n        Flags       : -mcpu=power8\r\n        Extra checks: VSX_ASM\r\n\r\n      CPU dispatch  :\r\n        Requested   : 'max -xop -fma4'\r\n        Enabled     : VSX3 VSX4\r\n        Generated   : none\r\n      INFO: CCompilerOpt.cache_flush[857] : write cache to path -> \/tmp\/pip-install-kw88t5sf\/numpy_228008dbdfce4094a9475c5197e2ff88\/build\/temp.linux-ppc64le-3.10\/ccompiler_opt_cache_clib.py\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for numpy\r\n\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\nNumpy: 1.23.5, 1.26.2\r\nGCC:  Anaconda GCC 11.2 \r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\nUnable to build Tensorflow on ppc64le - https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62659","comments":["Thanks for the report @cdeepali. Could you try a newer version to see if it's fixed? The last release which used `numpy.distutils` as the build system is `1.25.2`, and the latest release (which uses Meson as the build system) is `1.26.2`. Those are the two versions worth trying.\r\n\r\nI realize that that doesn't immediately solves your issue if TensorFlow pins to 1.23.5, but things should work in general on `ppc64le` for us, and if the most recent versions build you can exclude an issue in your environment being the root cause.","I see this issue with versions 1.25.2 and 1.26.2 as well. Seems to be caused by the addition of VSX4 (Power10) support. - https:\/\/github.com\/numpy\/numpy\/pull\/20821\r\n\r\n\r\n\r\n","Thanks for identifying the root cause @cdeepali. I think @seiko2plus may be able to help here.","@seiko2plus, any suggestions on the above issue. ","It's a bit ambiguous build error, I'm trying to create a minimal producer to report upstream, however, the issue seems related to the gcc flag \"no-plt\".\r\nAnaconda exports the following value of CFLAGS:\r\n```\r\nexport CFLAGS=\"-mcpu=power8 -mtune=power8 -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O3 -pipe -isystem \/root\/anaconda3\/include\"\r\n```\r\nTry to remove `-fno-plt` and rebuild e.g.:\r\n```\r\nexport CFLAGS=\"-mcpu=power8 -mtune=power8 -ftree-vectorize -fPIC -fstack-protector-strong -O3 -pipe -isystem \/root\/anaconda3\/include\"\r\n```"],"labels":["00 - Bug","32 - Installation","component: SIMD"]},{"title":"BUG: `isin` and `in1d` fail when `test_elements` is a length-1 unsigned integer with a value that overflows on cast to a signed integer","body":"### Describe the issue:\r\n\r\nThe NumPy `isin` and `in1d` functions fail when `test_elements` is a length-1 unsigned integer with a value that overflows on cast to a signed integer. The functions perform as expected for values that do not overflow, or when the `test_elelements` array as more than 1 value.\r\n \r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n>>> # `test_elements` is a single value in a uint64 array\r\n>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775807,), dtype=np.uint64)) \r\narray([False, False])\r\n\r\n>>> # as soon as `test_elements` contains a value that overflows, we get an `IndexError` \r\n>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775808,), dtype=np.uint64)) \r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/ariza\/.env312\/lib\/python3.12\/site-packages\/numpy\/lib\/arraysetops.py\", line 699, in in1d\r\n    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n\r\n>>> # however, if there is more than one number in that array, it works as expected\r\n>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((9223372036854775808, 42), dtype=np.uint64))\r\narray([False, False])\r\n\r\n>>> # presumably all numbers between 9223372036854775808 and np.iinfo(np.uint64).max will fail\r\n>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((np.iinfo(np.uint64).max,), dtype=np.uint64))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/ariza\/.env312\/lib\/python3.12\/site-packages\/numpy\/lib\/arraysetops.py\", line 699, in in1d\r\n    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n\r\n>>> # consistently, as soon as we have more than one number it works as expected\r\n>>> np.in1d(np.array([0, 1], dtype=np.int64), np.array((np.iinfo(np.uint64).max, 42), dtype=np.uint64))\r\narray([False, False])\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/ariza\/.env312\/lib\/python3.12\/site-packages\/numpy\/lib\/arraysetops.py\", line 699, in in1d\r\n    outgoing_array[basic_mask] = isin_helper_ar[ar1[basic_mask] -\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```\r\n\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.26.2\r\n3.12.0 (main, Dec  6 2023, 09:10:38) [GCC 9.4.0]\r\n```\r\n\r\n\r\n### Runtime Environment:\r\n\r\n```\r\n>>> import numpy; print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.26.2',\r\n  'python': '3.12.0 (main, Dec  6 2023, 09:10:38) [GCC 9.4.0]',\r\n  'uname': uname_result(system='Linux', node='ariza-is-p15', release='5.15.0-71-generic', version='#78~20.04.1-Ubuntu SMP Wed Apr 19 11:26:48 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n```\r\n\r\n\r\n### Context for the issue:\r\n\r\nI found this error through a Hypothesis-based test run on [StaticFrame](https:\/\/github.com\/static-frame\/static-frame). It is a rarely encountered issue but relevant when having containers with heterogeneous array types.","comments":[],"labels":["00 - Bug"]},{"title":"DOC: `f2py` compatibility with Fortran95 clarification","body":"### Proposed new feature or change:\n\n I do not think f2py needs to cater for defined types, overloading and else structures where C and F95 differ too much-user may switch directly to C\/C++ instead. I understand such structures would be permitted between f90 routines, gfortran or else compiler taking care, but not at python interface serviced by f2py. However, all  f90  scalar and array declarations should be properly processed, including  those involving integer expression and lower bounds for dimensions, and calling\r\nfor dynamic memory allocation and f2py should not complain on valid intrinsic subroutines (e.g. tiny() in my example).\r\nTo better track both fortran and f2py auto-generated C errors, effort should be made to better relate  lines in auto-generated C code to fortran constructs\/line range. Line number in the C code does not tell much from where it comes in Fortran. The extent of compatibility with Fortran95 should be described  IN DETAIL in f2py documentation.","comments":[],"labels":["04 - Documentation","component: numpy.f2py"]},{"title":"BUG: f2py generates error C code out of fortran90","body":"### Describe the issue:\n\nSorry as I cannot be more specific on the bug nature because there is no way to figure to what fortran feature is related\r\nwrong syntax in the C code generated by f2py from fortran90 source, hence example files are quite long. As \"paste drop or click\" does not accept a valid TGZ file, you need to download archive from https:\/\/users.camk.edu.pl\/alex\/#software\r\ntab \"m\". There are 3 .f90 files, to be compiled with build.sh script (note f2py<-->f2py3 name may need editing depending on your system configuration). They were compiled correctly\r\nby numpy 1.21.5 f2py under linux (listing in enclosed archive) and the .so library worked OK in python, they no longer compile correctly in numpy 1.26.2. The error appears during compilation od C code generated by f2py (listing enclosed).\r\nA MSWindow user claims on his installation numpy 1.23.5 compilation worked ok.\r\nI also noticed f2py complained on valid intrinsic procedure tiny()-a separate issue. Included build.sh file may need editing\r\nf2py<-->f2py3, depending on installation. gfortran compiles all 3 .f90 files without complain.\n\n### Reproduce the code example:\n\n```python\nNo short example feasible as bug appears in C code auto-generated by f2py with no clear link to fortran source. See numpy 1.21.5 and 1.26.2 compile listing in the archive put on web:\r\nhttps:\/\/users.camk.edu.pl\/alex\/#software under tab \"m\". Also enclosed is compile script and f90 code, so that results may be reproduced under both numpy versions at your place.\n```\n\n\n### Error message:\n\n_No response_\n\n### Python and NumPy Versions:\n\nnumpy 1.21.5 and 1.26.2, Python 3.10.12 (for newer numpy)\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["Hi @dibnob44, thanks for the bug report. I am a little confused though, because the log from `1.21.5` also seems to have quite a few (important) warnings:\r\n\r\n```bash\r\n************* Conversion to Python3 unfinished ************\r\nrm: cannot remove '*.pyc': No such file or directory\r\nrm: cannot remove '*.so': No such file or directory\r\nrm: cannot remove '*.mod': No such file or directory\r\nrm: cannot remove '*.toc': No such file or directory\r\nrm: cannot remove '*.aux': No such file or directory\r\nrm: cannot remove '*.log': No such file or directory\r\nrunning build\r\nrunning config_cc\r\nunifing config_cc, config, build_clib, build_ext, build commands --compiler options\r\nrunning config_fc\r\nunifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\r\nrunning build_src\r\nbuild_src\r\nbuilding extension \"aov\" sources\r\nf2py options: []\r\nf2py:> working\/src.linux-x86_64-3.10\/aovmodule.c\r\ncreating working\r\ncreating working\/src.linux-x86_64-3.10\r\nReading fortran codes...\r\n\tReading file 'aovconst.f90' (format:free)\r\n\tReading file 'aovsub.f90' (format:free)\r\n\tReading file 'aov.f90' (format:free)\r\nPost-processing...\r\n\tBlock: aov\r\n\t\t\tBlock: aovconst\r\nIn: :aov:aovconst.f90:aovconst\r\nget_parameters: got \"name 'tiny' is not defined\" on '4.*tiny(1.0)'\r\n\t\t\tBlock: aovsub\r\nIn: :aov:aovsub.f90:aovsub\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: sortx\r\nIn: :aov:aovsub.f90:aovsub:sortx\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: cracow\r\nIn: :aov:aovsub.f90:aovsub:cracow\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: peak\r\nIn: :aov:aovsub.f90:aovsub:peak\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: givensacc\r\nIn: :aov:aovsub.f90:aovsub:givensacc\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: givens\r\nIn: :aov:aovsub.f90:aovsub:givens\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\nIn: :aov:aovsub.f90:aovsub:givens\r\nget_parameters: got \"name 'epsilon' is not defined\" on '30.*epsilon(1.)'\r\n\t\t\t\tBlock: sortx_h\r\nIn: :aov:aovsub.f90:aovsub:sortx_h\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: sortx_m_r4\r\nIn: :aov:aovsub.f90:aovsub:sortx_m_r4\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: sortx_m_r8\r\nIn: :aov:aovsub.f90:aovsub:sortx_m_r8\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\tBlock: aov\r\nIn: :aov:aov.f90:aov\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: covar\r\nIn: :aov:aov.f90:aov:covar\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: fitcor\r\nIn: :aov:aov.f90:aov:fitcor\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: totals\r\nIn: :aov:aov.f90:aov:totals\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: fgrid\r\nIn: :aov:aov.f90:aov:fgrid\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: test\r\nIn: :aov:aov.f90:aov:test\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: fouw\r\nIn: :aov:aov.f90:aov:fouw\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: prew\r\nIn: :aov:aov.f90:aov:prew\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: aovmhw\r\nIn: :aov:aov.f90:aov:aovmhw\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: powspw\r\nIn: :aov:aov.f90:aov:powspw\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: aovtrw\r\nIn: :aov:aov.f90:aov:aovtrw\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: aovw\r\nIn: :aov:aov.f90:aov:aovw\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: ortprj\r\nIn: :aov:aov.f90:aov:ortprj\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\n\t\t\t\tBlock: ortint\r\nIn: :aov:aov.f90:aov:ortint\r\nget_parameters: got \"'(' was never closed (<string>, line 1)\" on '4.*tiny(1.0'\r\nPost-processing (stage 2)...\r\n\tBlock: aov\r\n\t\tBlock: unknown_interface\r\n\t\t\tBlock: aovconst\r\n\t\t\tBlock: aovsub\r\n\t\t\t\tBlock: cracow\r\n\t\t\t\tBlock: peak\r\n\t\t\t\tBlock: givensacc\r\n\t\t\t\tBlock: givens\r\n\t\t\t\tBlock: sortx_h\r\n\t\t\t\tBlock: sortx_m_r4\r\n\t\t\t\tBlock: sortx_m_r8\r\n\t\t\tBlock: aov\r\n\t\t\t\tBlock: covar\r\n\t\t\t\tBlock: fitcor\r\n\t\t\t\tBlock: totals\r\n\t\t\t\tBlock: fgrid\r\n\t\t\t\tBlock: test\r\n\t\t\t\tBlock: fouw\r\n\t\t\t\tBlock: prew\r\n\t\t\t\tBlock: aovmhw\r\n\t\t\t\tBlock: powspw\r\n\t\t\t\tBlock: aovtrw\r\n\t\t\t\tBlock: aovw\r\n\t\t\t\tBlock: ortprj\r\n\t\t\t\tBlock: ortint\r\nBuilding modules...\r\n\tBuilding module \"aov\"...\r\n\t\tConstructing F90 module support for \"aovconst\"...\r\n\t\t Variables: sp cp time pi2 ctmin minvar\r\n\t\tConstructing F90 module support for \"aovsub\"...\r\n\t\tCreating wrapper for Fortran function \"cracow\"(\"cracow\")...\r\n\t\t\tConstructing wrapper function \"aovsub.cracow\"...\r\n\t\t\t cracow = cracow(a,m)\r\n\t\t\tConstructing wrapper function \"aovsub.peak\"...\r\n\t\t\t xm,fm,dx = peak(fx,bacgnd,[n])\r\n\t\tCreating wrapper for Fortran subroutine \"givensacc\"(\"givensacc\")...\r\n\t\t\tConstructing wrapper function \"aovsub.givensacc\"...\r\n\t\t\t givensacc(a,r)\r\n\t\tCreating wrapper for Fortran function \"givens\"(\"givens\")...\r\n\t\t\tConstructing wrapper function \"aovsub.givens\"...\r\n\t\t\t givens,detnth = givens(r,idf)\r\n\t\tCreating wrapper for Fortran subroutine \"sortx_h\"(\"sortx_h\")...\r\n\t\t\tConstructing wrapper function \"aovsub.sortx_h\"...\r\n\t\t\t ind = sortx_h(dat)\r\n\t\tCreating wrapper for Fortran subroutine \"sortx_m_r4\"(\"sortx_m_r4\")...\r\n\t\t\tConstructing wrapper function \"aovsub.sortx_m_r4\"...\r\n\t\t\t ina = sortx_m_r4(dat)\r\n\t\tCreating wrapper for Fortran subroutine \"sortx_m_r8\"(\"sortx_m_r8\")...\r\n\t\t\tConstructing wrapper function \"aovsub.sortx_m_r8\"...\r\n\t\t\t ina = sortx_m_r8(dat)\r\n\t\tConstructing F90 module support for \"aov\"...\r\n\t\t\tConstructing wrapper function \"aov.covar\"...\r\n\t\t\t lav,lmi,lmx,cc,cmi,cmx,ilag = covar(t1,v1,e1,t2,v2,e2,nlag,eps,iscale,ifunct,[n1,n2])\r\n\t\t\tConstructing wrapper function \"aov.fitcor\"...\r\n\t\t\t cp,chi2 = fitcor(cn,co,[npar1,no])\r\n\t\t\tConstructing wrapper function \"aov.totals\"...\r\n\t\t\t totals(x,[n])\r\n\t\t\tConstructing wrapper function \"aov.fgrid\"...\r\n\t\t\t fstop,fstep,fr0 = fgrid(t,[nobs])\r\n\t\t\tConstructing wrapper function \"aov.test\"...\r\n\t\t\t t,f,er = test(no)\r\n\t\t\tConstructing wrapper function \"aov.fouw\"...\r\n\t\t\t frout,dfrout,valout,cof,dcof = fouw(t,valin,er,frin,bacgnd,nh2,[nobs])\r\n\t\t\tConstructing wrapper function \"aov.prew\"...\r\n\t\t\t frout,dfrout,valout = prew(t,valin,er,frin,bacgnd,nh2,[nobs])\r\n\t\t\tConstructing wrapper function \"aov.aovmhw\"...\r\n\t\t\t th,frmax = aovmhw(t,f,er,frs,nfr,[fr0,nh2,nobs,ncov])\r\n\t\t\tConstructing wrapper function \"aov.powspw\"...\r\n\t\t\t th,frmax = powspw(t,f,er,frs,nfr,[fr0,nh2,nobs,ncov])\r\n\t\t\tConstructing wrapper function \"aov.aovtrw\"...\r\n\t\t\t th,frmax = aovtrw(tin,fin,er,frs,nfr,[fr0,nh2,nobs,ncov])\r\n\t\t\tConstructing wrapper function \"aov.aovw\"...\r\n\t\t\t th,frmax = aovw(tin,fin,er,frs,nfr,[fr0,nh2,nobs,ncov])\r\n\t\tCreating wrapper for Fortran subroutine \"ortprj\"(\"ortprj\")...\r\n\t\t\tConstructing wrapper function \"aov.ortprj\"...\r\n\t\t\t a,c,per = ortprj(t,xrw,rw,frs,[fr0])\r\n\t\tCreating wrapper for Fortran subroutine \"ortint\"(\"ortint\")...\r\n\t\t\tConstructing wrapper function \"aov.ortint\"...\r\n\t\t\t x = ortint(t,fr,a,c)\r\n\tWrote C\/API module \"aov\" to file \"working\/src.linux-x86_64-3.10\/aovmodule.c\"\r\n\tFortran 90 wrappers are saved to \"working\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90\"\r\n  adding 'working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.c' to sources.\r\n  adding 'working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10' to include_dirs.\r\ncreating working\/src.linux-x86_64-3.10\/working\r\ncreating working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\r\ncopying \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/f2py\/src\/fortranobject.c -> working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\r\ncopying \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/f2py\/src\/fortranobject.h -> working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\r\n  adding 'working\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90' to sources.\r\nbuild_src: building npy-pkg config files\r\nrunning build_ext\r\ncustomize UnixCCompiler\r\nC compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-cc -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include -fPIC -O2 -isystem \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -fPIC\r\n\r\ncreating \/tmp\/tmpu6sqdbqp\/tmp\r\ncreating \/tmp\/tmpu6sqdbqp\/tmp\/tmpu6sqdbqp\r\ncompile options: '-MMD -MF \/tmp\/tmpu6sqdbqp\/file.c.d -c'\r\nx86_64-conda-linux-gnu-cc: \/tmp\/tmpu6sqdbqp\/file.c\r\ncustomize UnixCCompiler using build_ext\r\nget_default_fcompiler: matching types: '['gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag', 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 'pathf95', 'nagfor', 'fujitsu']'\r\ncustomize Gnu95FCompiler\r\nFound executable \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran\r\nFound executable \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-ld\r\nFound executable \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-ar\r\nFound executable \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-ranlib\r\ncustomize Gnu95FCompiler\r\ncustomize Gnu95FCompiler using build_ext\r\nbuilding 'aov' extension\r\ncompiling C sources\r\nC compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-cc -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include -fPIC -O2 -isystem \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -fPIC\r\n\r\ncreating working\/working\r\ncreating working\/working\/src.linux-x86_64-3.10\r\ncreating working\/working\/src.linux-x86_64-3.10\/working\r\ncreating working\/working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\r\ncompile options: '-DNPY_DISABLE_OPTIMIZATION=1 -Iworking\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10 -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include\/python3.10 -c'\r\nx86_64-conda-linux-gnu-cc: working\/src.linux-x86_64-3.10\/aovmodule.c\r\nx86_64-conda-linux-gnu-cc: working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.c\r\nIn file included from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/ndarraytypes.h:1969,\r\n                 from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/ndarrayobject.h:12,\r\n                 from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/arrayobject.h:4,\r\n                 from working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.h:13,\r\n                 from working\/src.linux-x86_64-3.10\/aovmodule.c:16:\r\n\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\r\n      |  ^~~~~~~\r\nIn file included from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/ndarraytypes.h:1969,\r\n                 from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/ndarrayobject.h:12,\r\n                 from \/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/arrayobject.h:4,\r\n                 from working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.h:13,\r\n                 from working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.c:2:\r\n\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include\/numpy\/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\r\n      |  ^~~~~~~\r\nworking\/src.linux-x86_64-3.10\/aovmodule.c:149:12: warning: 'f2py_size' defined but not used [-Wunused-function]\r\n  149 | static int f2py_size(PyArrayObject* var, ...)\r\n      |            ^~~~~~~~~\r\ncompiling Fortran 90 module sources\r\nFortran f77 compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\nFortran f90 compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\nFortran fix compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\ncompile options: '-Iworking\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10 -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include\/python3.10 -c'\r\nextra options: '-Jworking\/ -Iworking\/'\r\nx86_64-conda-linux-gnu-gfortran:f90: aovconst.f90\r\nx86_64-conda-linux-gnu-gfortran:f90: aovsub.f90\r\nx86_64-conda-linux-gnu-gfortran:f90: aov.f90\r\naov.f90:1126:26:\r\n\r\n 1126 |                per(ifr) = per(ifr) + Abs (sc) ** 2\/sn\r\n      |                          1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:992:22:\r\n\r\n  992 |                   sav=dph-floor(dph);ph(i)=sav\r\n      |                      1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:854:14:\r\n\r\n  854 |           sav=dph-floor(dph);ph(i)=sav\r\n      |              1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:766:12:\r\n\r\n  766 |     th(l) = abs(sum(Real((f-avf)*w)* &\r\n      |            1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:706:14:\r\n\r\n  706 |       th(l) = th(l) + Abs (sc) ** 2\/sn\r\n      |              1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:296:6:\r\n\r\n  296 |   del=t(ind(2:))-t(ind(:nobs1))\r\n      |      1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:554:9:\r\n\r\n  554 |       dx=0.5_SP*dx*dfr\r\n      |         1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:432:15:\r\n\r\n  432 |         e(n)=  cos(ph*n)\r\n      |               1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:433:15:\r\n\r\n  433 |         e(n+1)=sin(ph*n)\r\n      |               1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:438:12:\r\n\r\n  438 |       e(nx)=e(nx)*PI2*0.5_SP*(t(l)-t0)\r\n      |            1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:465:10:\r\n\r\n  465 |   cof(nx)=t0\r\n      |          1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:347:8:\r\n\r\n  347 |     omt=pi2*(dph-floor(dph))\r\n      |        1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:74:43:\r\n\r\n   74 |   Forall(i1=1:n1,i2=1:n2) dt((i1-1)*n2+i2)=t2(i2)-t1(i1)\r\n      |                                           1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:124:9:\r\n\r\n  124 |     lags=t2(ib2(:ict))-t1(ib1(:ict))   ! average & extreme intervals\r\n      |         1\r\nWarning: Possible change of value in conversion from REAL(8) to REAL(4) at (1) [-Wconversion]\r\naov.f90:738:31:\r\n\r\n  738 |   Real(SP)   :: w(nobs),avf,vrf,fm,dx,sw\r\n      |                               1\r\nWarning: Unused variable 'vrf' declared at (1) [-Wunused-variable]\r\ncompiling Fortran sources\r\nFortran f77 compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -ffixed-form -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\nFortran f90 compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\nFortran fix compiler: \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -ffixed-form -fno-second-underscore -Wall -g -fno-second-underscore -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem \/home\/rgoswami\/micromamba\/envs\/numpy-dev\/include -O3 -funroll-loops\r\ncompile options: '-Iworking\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10 -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/lib\/python3.10\/site-packages\/numpy\/core\/include -I\/home\/rgoswami\/Downloads\/f2pybug\/.pixi\/env\/include\/python3.10 -c'\r\nextra options: '-Jworking\/ -Iworking\/'\r\nx86_64-conda-linux-gnu-gfortran:f90: working\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90\r\nworking\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90:17:39:\r\n\r\n   17 |       subroutine f2pywrap_aovsub_cracow (cracowf2pywrap, a, m, f2py_a_d0&\r\n      |                                       1\r\n......\r\n   79 |       subroutine f2pywrap_aovsub_cracow (cracowf2pywrap, cracow, a, m, f&\r\n      |                                       2\r\nWarning: Rank mismatch in argument 'cracow' (0\/2) between (1) and (2)\r\nworking\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90:37:39:\r\n\r\n   37 |       subroutine f2pywrap_aovsub_givens (givensf2pywrap, r, idf, detnth,&\r\n      |                                       1\r\n......\r\n   96 |       subroutine f2pywrap_aovsub_givens (givensf2pywrap, givens, r, idf,&\r\n      |                                       2\r\nWarning: Rank mismatch in argument 'givens' (0\/2) between (1) and (2)\r\n\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/x86_64-conda-linux-gnu-gfortran -Wall -g -Wall -g -shared -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/lib -Wl,-rpath-link,\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/lib -L\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/lib working\/working\/src.linux-x86_64-3.10\/aovmodule.o working\/working\/src.linux-x86_64-3.10\/working\/src.linux-x86_64-3.10\/fortranobject.o working\/aovconst.o working\/aovsub.o working\/aov.o working\/working\/src.linux-x86_64-3.10\/aov-f2pywrappers2.o -L\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/..\/lib -L\/home\/rgoswami\/micromamba\/envs\/numpy-dev\/bin\/..\/lib\/gcc\/x86_64-conda-linux-gnu\/12.3.0\/..\/..\/..\/..\/x86_64-conda-linux-gnu\/lib\/..\/lib -lgfortran -o .\/aov.cpython-310-x86_64-linux-gnu.so\r\nfinished OK\r\n```\r\n\r\nI would be concerned about the `-Wconversion` warnings and the rank mismatch in particular. I can reproduce this locally, will look into it soon. At first glance it seems to mostly be localized in `aov.f90` and some of the routines are not being picked up:\r\n\r\n```diff\r\ndiff working\/src.linux-x86_64-3.10\/aov-f2pywrappers2.f90 broken\/aov-f2pywrappers2.f90\r\n2c2\r\n< !     This file is autogenerated with f2py (version:1.21.5)\r\n---\r\n> !     This file is autogenerated with f2py (version:2.0.0.dev0+git20231216.4b98384)\r\n17,133d16\r\n<       subroutine f2pywrap_aovsub_cracow (cracowf2pywrap, a, m, f2py_a_d0&\r\n<      &, f2py_a_d1)\r\n<       use aovsub, only : cracow\r\n<       integer m\r\n<       integer f2py_a_d0\r\n<       integer f2py_a_d1\r\n<       real(kind=4) a(f2py_a_d0,f2py_a_d1)\r\n<       real(kind=4) cracowf2pywrap\r\n<       cracowf2pywrap = cracow(a, m)\r\n<       end subroutine f2pywrap_aovsub_cracow\r\n<       subroutine f2pywrap_aovsub_givensacc (a, r, f2py_a_d0, f2py_r_d0, &\r\n<      &f2py_r_d1)\r\n<       use aovsub, only : givensacc\r\n<       integer f2py_a_d0\r\n<       integer f2py_r_d0\r\n<       integer f2py_r_d1\r\n<       real(kind=4) a(f2py_a_d0)\r\n<       real(kind=4) r(f2py_r_d0,f2py_r_d1)\r\n<       call givensacc(a, r)\r\n<       end subroutine f2pywrap_aovsub_givensacc\r\n<       subroutine f2pywrap_aovsub_givens (givensf2pywrap, r, idf, detnth,&\r\n<      & f2py_r_d0, f2py_r_d1)\r\n<       use aovsub, only : givens\r\n<       integer idf\r\n<       real(kind=4) detnth\r\n<       integer f2py_r_d0\r\n<       integer f2py_r_d1\r\n<       real(kind=4) r(f2py_r_d0,f2py_r_d1)\r\n<       real(kind=4) givensf2pywrap\r\n<       givensf2pywrap = givens(r, idf, detnth)\r\n<       end subroutine f2pywrap_aovsub_givens\r\n<       subroutine f2pywrap_aovsub_sortx_h (dat, ind, f2py_dat_d0, f2py_in&\r\n<      &d_d0)\r\n<       use aovsub, only : sortx_h\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ind_d0\r\n<       real(kind=4) dat(f2py_dat_d0)\r\n<       integer ind(f2py_ind_d0)\r\n<       call sortx_h(dat, ind)\r\n<       end subroutine f2pywrap_aovsub_sortx_h\r\n<       subroutine f2pywrap_aovsub_sortx_m_r4 (dat, ina, f2py_dat_d0, f2py&\r\n<      &_ina_d0)\r\n<       use aovsub, only : sortx_m_r4\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ina_d0\r\n<       real(kind=4) dat(f2py_dat_d0)\r\n<       integer ina(f2py_ina_d0)\r\n<       call sortx_m_r4(dat, ina)\r\n<       end subroutine f2pywrap_aovsub_sortx_m_r4\r\n<       subroutine f2pywrap_aovsub_sortx_m_r8 (dat, ina, f2py_dat_d0, f2py&\r\n<      &_ina_d0)\r\n<       use aovsub, only : sortx_m_r8\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ina_d0\r\n<       real(kind=8) dat(f2py_dat_d0)\r\n<       integer ina(f2py_ina_d0)\r\n<       call sortx_m_r8(dat, ina)\r\n<       end subroutine f2pywrap_aovsub_sortx_m_r8\r\n<       \r\n<       subroutine f2pyinitaovsub(f2pysetupfunc)\r\n<       use aovsub, only : peak\r\n<       interface \r\n<       subroutine f2pywrap_aovsub_cracow (cracowf2pywrap, cracow, a, m, f&\r\n<      &2py_a_d0, f2py_a_d1)\r\n<       real(kind=4) cracow\r\n<       integer m\r\n<       integer f2py_a_d0\r\n<       integer f2py_a_d1\r\n<       real(kind=4) a(f2py_a_d0,f2py_a_d1)\r\n<       real(kind=4) cracowf2pywrap\r\n<       end subroutine f2pywrap_aovsub_cracow \r\n<       subroutine f2pywrap_aovsub_givensacc (a, r, f2py_a_d0, f2py_r_d0, &\r\n<      &f2py_r_d1)\r\n<       integer f2py_a_d0\r\n<       integer f2py_r_d0\r\n<       integer f2py_r_d1\r\n<       real(kind=4) a(f2py_a_d0)\r\n<       real(kind=4) r(f2py_r_d0,f2py_r_d1)\r\n<       end subroutine f2pywrap_aovsub_givensacc \r\n<       subroutine f2pywrap_aovsub_givens (givensf2pywrap, givens, r, idf,&\r\n<      & detnth, f2py_r_d0, f2py_r_d1)\r\n<       real(kind=4) givens\r\n<       integer idf\r\n<       real(kind=4) detnth\r\n<       integer f2py_r_d0\r\n<       integer f2py_r_d1\r\n<       real(kind=4) r(f2py_r_d0,f2py_r_d1)\r\n<       real(kind=4) givensf2pywrap\r\n<       end subroutine f2pywrap_aovsub_givens \r\n<       subroutine f2pywrap_aovsub_sortx_h (dat, ind, f2py_dat_d0, f2py_in&\r\n<      &d_d0)\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ind_d0\r\n<       real(kind=4) dat(f2py_dat_d0)\r\n<       integer ind(f2py_ind_d0)\r\n<       end subroutine f2pywrap_aovsub_sortx_h \r\n<       subroutine f2pywrap_aovsub_sortx_m_r4 (dat, ina, f2py_dat_d0, f2py&\r\n<      &_ina_d0)\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ina_d0\r\n<       real(kind=4) dat(f2py_dat_d0)\r\n<       integer ina(f2py_ina_d0)\r\n<       end subroutine f2pywrap_aovsub_sortx_m_r4 \r\n<       subroutine f2pywrap_aovsub_sortx_m_r8 (dat, ina, f2py_dat_d0, f2py&\r\n<      &_ina_d0)\r\n<       integer f2py_dat_d0\r\n<       integer f2py_ina_d0\r\n<       real(kind=8) dat(f2py_dat_d0)\r\n<       integer ina(f2py_ina_d0)\r\n<       end subroutine f2pywrap_aovsub_sortx_m_r8\r\n<       end interface\r\n<       external f2pysetupfunc\r\n<       call f2pysetupfunc(f2pywrap_aovsub_cracow,peak,f2pywrap_aovsub_giv&\r\n<      &ensacc,f2pywrap_aovsub_givens,f2pywrap_aovsub_sortx_h,f2pywrap_aov&\r\n<      &sub_sortx_m_r4,f2pywrap_aovsub_sortx_m_r8)\r\n<       end subroutine f2pyinitaovsub\r\n< \r\n146,147c29,30\r\n<       complex(kind=8) a(1)\r\n<       complex(kind=8) c(1)\r\n---\r\n>       complex(kind=8) a(1 + )\r\n>       complex(kind=8) c(1 + )\r\n159,160c42,43\r\n<       complex(kind=8) a(1)\r\n<       complex(kind=8) c(1)\r\n---\r\n>       complex(kind=8) a(1 + )\r\n>       complex(kind=8) c(1 + )\r\n188,189c71,72\r\n<       complex(kind=8) a(1)\r\n<       complex(kind=8) c(1)\r\n---\r\n>       complex(kind=8) a(1 + )\r\n>       complex(kind=8) c(1 + )\r\n199,200c82,83\r\n<       complex(kind=8) a(1)\r\n<       complex(kind=8) c(1)\r\n---\r\n>       complex(kind=8) a(1 + )\r\n>       complex(kind=8) c(1 + )\r\n```\r\n\r\nAlong with the incorrect size determination for the arrays of course. However, it would appear that the previous behaviour `a(1)` instead of `a(1+)` is also likely incorrect.","Hi, my evaluation is empirical: numpy\/f2py 1.21.5 despite warnings finishes with 0 flag, hence text 'finished OK' in the last line, and under linux generates .so library which works correctly with python. If I understand,\r\n.f90 files are converted to objects by gfortran called by f2py behind the scene, and not f2py, so most warnings have little effect on the result f90 object. f2py generates C wrappers\r\nto each subroutine so main misunderstandings occure at the level of parameter declarations and porting them to C.\r\n\r\nThe conversion warnings  comcern of intended rounding to single precision in fortran.\r\n\r\nI thought, that array dimension was gone after f2py started compiling a(0:N\/2*2) correctly.\r\n\r\nHere I have two sorts of difficulty: As I have sein in ENH, I do not expect f2py to process more advanced constructs aka\r\ndefined types, overloading etc. However, it would be good to find in doc which fortran constructs (in declarations)\r\nf2py is already supposed to process correctly. This would help to adjust .f90 code to stisfy f2py requirements. I know f2py is reliable of f77 (LAPACK\/BLAS demonstrating), however forcing complete abandon of basic f90 improvements \r\nin declarations would be wrong policy.","array dimension problem was gone after f2py started compiling declaration real(kind=8) a(0:N\/2*2) correctly.","As this bug report already is very large, I did not include pyaov.py and aovgui.py files wchich are using the said .so library,\r\nif that was any help pyaov.py and calling it ex1.py example script may be downloaded from my web page just one line above the \"m\" tab you were using. However, if .\/build.sh would generate good .so library is already fine for me.\r\n\r\nRe. a(1): in ancient f77 dialect a(1) would be OK as then f77 simply passes as parameter pointer to the start of array 'a' and no else information is needed. The user has to take care to supply array length as other parameters.\r\nIn f95 such practice is discouraged and may cause compilation error, depending on flags. I do not use a(1) in my .f90 code."],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"MAINT, ENH: Try to speed up the `meson` backend","body":"At the moment, since the switch to `meson` as the default, all the compile tests had to be marked slow. At first, it was assumed that the fixture based tests were at fault (https:\/\/github.com\/numpy\/numpy\/pull\/25252). However, on further investigation, the problem is that that the `meson` is quite a bit slower in general:\r\n\r\n```bash\r\n\u276f hyperfine \"f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend distutils\" \"f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend meson\"\r\nBenchmark 1: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend distutils\r\n  Time (mean \u00b1 \u03c3):     380.0 ms \u00b1 179.0 ms    [User: 531.0 ms, System: 941.1 ms]\r\n  Range (min \u2026 max):   219.0 ms \u2026 656.2 ms    10 runs\r\n \r\nBenchmark 2: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend meson\r\n  Time (mean \u00b1 \u03c3):      1.512 s \u00b1  0.042 s    [User: 1.964 s, System: 2.480 s]\r\n  Range (min \u2026 max):    1.465 s \u2026  1.588 s    10 runs\r\n \r\nSummary\r\n  f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend distutils ran\r\n    3.98 \u00b1 1.88 times faster than f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend meson\r\nhyperfine    25.10s user 34.36s system 309% cpu 19.216 total\r\n```\r\n\r\nThis would be very nice to fix, but it is also kind of **low prio** right now (in light of the `meson` backend still sort of being brought up to speed \/ tests are running on CI, just maybe not as often as they could). ","comments":["The most likely culprit is the extended string substitution done for the `meson` backend, but I also wouldn't really be amenable to adding a whole new dependency just for faster template generation.","Also related is #25239."],"labels":["component: numpy.f2py"]},{"title":"BUG: Trace returns results for non-square matrices","body":"### Describe the issue:\r\n\r\nFrom a mathematical standpoint the trace is [only defined for square matrices ](https:\/\/en.wikipedia.org\/wiki\/Trace_(linear_algebra)) or more generally for matrices in \u2102<sup>n<\/sup>.\r\n\r\nHowever, the [numpy implementation](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/numpy\/core\/fromnumeric.py#L1700-L1761) returns the sum of entries where all indices are the same which for square matrices coincides with the trace but also returns wrong results disguised as trace if the matrix is not square. This can result in unwanted behavior as one could use the trace to check if the matrix is square, expecting an error if it is not.\r\n\r\nI can see that this implementation of the trace may have its use cases. But I would strongly suggest modifying the default function to return only the mathematically correct trace and extending the function with a flag which allows non-square matrices.\r\n\r\n```\r\nimport numpy as np\r\n\r\nA = np.array([[1, 2, 3],\r\n              [1, 2, 3]])\r\n\r\nprint(A.trace())\r\n```\r\nResult: `3`\r\nExpected Result: `\"AssertionError: Trace is not defined for non-square matrices\"`\r\n\r\nFurther, I am interested if anyone knows why this implementation was chosen and what is actual use cases are.\r\n\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nA = np.array([[1, 2, 3],\r\n              [1, 2, 3]])\r\n\r\nprint(A.trace())\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nNo Error\r\n```\r\n\r\n\r\n### Python and NumPy Versions:\r\n\r\nnumpy: 1.21.5\r\nsys: 3.9.13\r\n\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["I agree, the trace of a non-square matrix should not be defined. I don't think it is helpful for a scientific tool to allow this in any way, it gives legitimacy to an incorrect interpretation of the function. If there is a use for the way trace() handles non-square arrays, then I cannot think of it. ","I could work on this but I imagine, that this hurts backwards compatibility. ","I can definitely help you if you want. But do you know how one can get an idea if this change would be accepted by a majority? ","We should see what the maintainers say.","Changes like this should be [proposed on the mailing list](https:\/\/mail.python.org\/mailman\/listinfo\/numpy-discussion) to ensure that it gets attention.\r\n\r\nBut I doubt that we will introduce a new exception for mathematical purity.\r\n\r\n> This can result in unwanted behavior as one could use the trace to check if the matrix is square, expecting an error if it is not.\r\n\r\nSince the straightforward way to do this check is to examine the `.shape`, I don't think we will consider this as a use case for `trace()`.","Thank you very much, I will propose it.\r\n\r\nDon't get me wrong, I think the function can be very useful to compute specific diagonal sums. But the function does not do what the trace actually should do. \r\n\r\nFurther, I noticed that `np.array([1]).trace()` returns ` diag requires an array of at least two dimensions` while the trace of a scalar should return the scalar itself."],"labels":["00 - Bug"]},{"title":"MAINT: Add stand-alone libnpymath and remove dependency to numpy from npyrandom","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["The test failures look like they come from `test_cffi`, but I don't really understand it. Does `cffi` have a problem parsing `Py_ssize_t`?","There's something weird going on on my system locally as well, which I don't really understand. The tests fails when ran with spin, but succeed when ran with pytest under the tools directory.\r\n\r\n```python3\r\nnumpy on \ue0a0 remove-npyrandom-staticlib is \ud83d\udce6 v2.0.0.dev0 via \ud83d\udc0d pyenv mambaforge (venv) \r\n\u276f python -m spin test numpy\/random\/tests\/test_extending.py -- -v\r\nInvoking `build` prior to running tests:\r\n$ \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python vendored-meson\/meson\/meson.py compile -C build\r\nINFO: autodetecting backend as ninja\r\nINFO: calculating backend command to run: \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/ninja -C \/Users\/lysnikolaou\/repos\/python\/numpy\/build\r\nninja: Entering directory `\/Users\/lysnikolaou\/repos\/python\/numpy\/build'\r\n[1\/1] Generating numpy\/generate-version with a custom command\r\nSaving version to numpy\/version.py\r\n$ \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python vendored-meson\/meson\/meson.py install --only-changed -C build --destdir ..\/build-install\r\n$ export PYTHONPATH=\"\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\"\r\n$ \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python -P -c 'import numpy'\r\n$ export PYTHONPATH=\"\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\"\r\n$ cd \/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\r\n$ \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python -m pytest --rootdir=\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages -m 'not slow' numpy\/random\/tests\/test_extending.py -v\r\n========================================================== test session starts ===========================================================\r\nplatform darwin -- Python 3.11.3, pytest-7.4.0, pluggy-1.3.0 -- \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python\r\ncachedir: .pytest_cache\r\nhypothesis profile 'np.test() profile' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=[HealthCheck.data_too_large, HealthCheck.filter_too_much, HealthCheck.too_slow, HealthCheck.large_base_example, HealthCheck.function_scoped_fixture]\r\nrootdir: \/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\r\nconfigfile: ..\/..\/..\/..\/..\/pytest.ini\r\nplugins: hypothesis-6.81.1, cov-4.1.0, xdist-3.3.1\r\ncollected 3 items \/ 1 deselected \/ 2 selected                                                                                            \r\n\r\nnumpy\/random\/tests\/test_extending.py::test_numba SKIPPED (requires numba and cffi)                                                 [ 50%]\r\nnumpy\/random\/tests\/test_extending.py::test_cffi FAILED                                                                             [100%]\r\n\r\n================================================================ FAILURES ================================================================\r\n_______________________________________________________________ test_cffi ________________________________________________________________\r\n\r\n    @pytest.mark.skipif(cffi is None, reason=\"requires cffi\")\r\n    def test_cffi():\r\n>       from numpy.random._examples.cffi import extending  # noqa: F401\r\n\r\n\r\nnumpy\/random\/tests\/test_extending.py:118: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nnumpy\/random\/_examples\/cffi\/extending.py:31: in <module>\r\n    lib.random_standard_normal_fill(interface.bit_generator, n, vals_cffi)\r\n        __builtins__ = <builtins>\r\n        __cached__ = '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/random\/_examples\/cffi\/__pycache__\/extending.cpython-311.pyc'\r\n        __doc__    = '\\nUse cffi to access any of the underlying C functions from distributions.h\\n'\r\n        __file__   = '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/random\/_examples\/cffi\/extending.py'\r\n        __loader__ = <_frozen_importlib_external.SourceFileLoader object at 0x1079c9c30>\r\n        __name__   = 'numpy.random._examples.cffi.extending'\r\n        __package__ = 'numpy.random._examples.cffi'\r\n        __spec__   = ModuleSpec(name='numpy.random._examples.cffi.extending', loader=<_frozen_importlib_external.SourceFileLoader object at...ysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/random\/_examples\/cffi\/extending.py')\r\n        bit_gen    = <numpy.random._pcg64.PCG64 object at 0x10643b520>\r\n        cffi       = <module 'cffi' from '\/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/lib\/python3.11\/site-packages\/cffi\/__init__.py'>\r\n        ffi        = <cffi.api.FFI object at 0x1079c95f0>\r\n        inc_dir    = '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/_core\/include\/numpy'\r\n        interface  = interface(state_address=4400067976, state=<cdata 'void *' 0x10643b588>, next_uint64=<cdata 'uint64_t(*)(void *)' 0x107...void *)' 0x107963660>, next_double=<cdata 'double(*)(void *)' 0x107963690>, bit_generator=<cdata 'void *' 0x10643b540>)\r\n        lib        = <cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x105d03490>\r\n        n          = 100\r\n        np         = <module 'numpy' from '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/__init__.py'>\r\n        os         = <module 'os' from '\/Users\/lysnikolaou\/repos\/python\/cpython-versions\/3.11.3\/build\/lib\/python3.11\/os.py'>\r\n        parse_distributions_h = <function parse_distributions_h at 0x1079a6db0>\r\n        rng        = Generator(PCG64) at 0x1079EBD50\r\n        state      = {'bit_generator': 'PCG64', 'has_uint32': 0, 'state': {'inc': 14220707047729487342796260036649536101, 'state': 231538475895373809649456743026176147886}, 'uinteger': 0}\r\n        vals_cffi  = <cdata 'double[100]' owning 800 bytes>\r\n..\/..\/..\/..\/..\/venv\/lib\/python3.11\/site-packages\/cffi\/api.py:912: in __getattr__\r\n    make_accessor(name)\r\n        make_accessor = <function _make_ffi_library.<locals>.make_accessor at 0x107d31f40>\r\n        name       = 'random_standard_normal_fill'\r\n        self       = <cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x105d03490>\r\n..\/..\/..\/..\/..\/venv\/lib\/python3.11\/site-packages\/cffi\/api.py:908: in make_accessor\r\n    accessors[name](name)\r\n        FFILibrary = <class 'cffi.api._make_ffi_library.<locals>.FFILibrary'>\r\n        accessors  = {'random_beta': <function _make_ffi_library.<locals>.accessor_function at 0x107d31860>, 'random_binomial': <function _...0x107d31860>, 'random_binomial_inversion': <function _make_ffi_library.<locals>.accessor_function at 0x107d31860>, ...}\r\n        ffi        = <cffi.api.FFI object at 0x1079c95f0>\r\n        library    = <cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x105d03490>\r\n        name       = 'random_standard_normal_fill'\r\n        update_accessors = <function _make_ffi_library.<locals>.update_accessors at 0x107d31e90>\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nname = 'random_standard_normal_fill'\r\n\r\n    def accessor_function(name):\r\n        key = 'function ' + name\r\n        tp, _ = ffi._parser._declarations[key]\r\n        BType = ffi._get_cached_btype(tp)\r\n>       value = backendlib.load_function(BType, name)\r\nE       AttributeError: function\/symbol 'random_standard_normal_fill' not found in library '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/random\/_generator.cpython-311d-darwin.so': dlsym(0x80452540, random_standard_normal_fill): symbol not found\r\n\r\nBType      = <ctype 'void(*)(bitgen_t *, intptr_t, double *)'>\r\n_          = 0\r\nbackendlib = <clibrary '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-install\/usr\/lib\/python3.11\/site-packages\/numpy\/random\/_generator.cpython-311d-darwin.so'>\r\nffi        = <cffi.api.FFI object at 0x1079c95f0>\r\nkey        = 'function random_standard_normal_fill'\r\nlibrary    = <cffi.api._make_ffi_library.<locals>.FFILibrary object at 0x105d03490>\r\nname       = 'random_standard_normal_fill'\r\ntp         = <void(*)(bitgen_t *, intptr_t, double *)>\r\n\r\n..\/..\/..\/..\/..\/venv\/lib\/python3.11\/site-packages\/cffi\/api.py:838: AttributeError\r\n======================================================== short test summary info =========================================================\r\nFAILED numpy\/random\/tests\/test_extending.py::test_cffi - AttributeError: function\/symbol 'random_standard_normal_fill' not found in library '\/Users\/lysnikolaou\/repos\/python\/numpy\/build-insta...\r\n=============================================== 1 failed, 1 skipped, 1 deselected in 2.23s ===============================================\r\n\r\nnumpy on \ue0a0 remove-npyrandom-staticlib is \ud83d\udce6 v2.0.0.dev0 via \ud83d\udc0d pyenv mambaforge (venv) took 11s \r\n\u276f cd tools                                                      \r\n\r\nnumpy\/tools on \ue0a0 remove-npyrandom-staticlib via \ud83d\udc0d pyenv mambaforge (venv) \r\n\u276f pytest --pyargs numpy.random.tests.test_extending -v          \r\n========================================================== test session starts ===========================================================\r\nplatform darwin -- Python 3.11.3, pytest-7.4.0, pluggy-1.3.0 -- \/Users\/lysnikolaou\/repos\/python\/numpy\/venv\/bin\/python\r\ncachedir: .pytest_cache\r\nhypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('\/Users\/lysnikolaou\/repos\/python\/numpy\/tools\/.hypothesis\/examples')\r\nrootdir: \/Users\/lysnikolaou\/repos\/python\/numpy\r\nconfigfile: pytest.ini\r\nplugins: hypothesis-6.81.1, cov-4.1.0, xdist-3.3.1\r\ncollected 3 items                                                                                                                        \r\n\r\n..\/venv\/lib\/python3.11\/site-packages\/numpy\/random\/tests\/test_extending.py::test_cython PASSED                                      [ 33%]\r\n..\/venv\/lib\/python3.11\/site-packages\/numpy\/random\/tests\/test_extending.py::test_numba SKIPPED (requires numba and cffi)            [ 66%]\r\n..\/venv\/lib\/python3.11\/site-packages\/numpy\/random\/tests\/test_extending.py::test_cffi PASSED                                        [100%]\r\n\r\n===================================================== 2 passed, 1 skipped in 18.36s ======================================================\r\n```","This does look promising. It looks like there's still a bit of polishing to do, but it looks about right. Changing this code to regular C99-compliant code should make it more reusable. ","Just did a quick glance - what about the public half float API? Don't we need to add it to the NumPy C API as well?","> Don't we need to add it to the NumPy C API as well?\r\n\r\nDo you mean move it out of npymath and add it to numpy? ","Yes, otherwise downstream consumers of the half float API will need to add a new compile-time dependency. All the other stuff in npymath in principle they could migrate away from, as scipy has, but there's no other API for working with that dtype, and I think it belongs in NumPy with the rest of C API for working with numpy dtypes.","> Yes, otherwise downstream consumers of the half float API will need to add a new compile-time dependency. All the other stuff in npymath in principle they could migrate away from, as scipy has, but there's no other API for working with that dtype, and I think it belongs in NumPy with the rest of C API for working with numpy dtypes.\r\n\r\nWe chatted about this at the community meeting and I withdraw this. \r\n\r\nIEEE Half float is more widely supported in compilers than I realized, there's no need for downstream to have an API available in numpy. They can either use the support built in to their compiler or vendor the static library.","The `cffi` failure is due to symbol visibility; in `random\/_generator.so` the `random_standard_normal_fill` symbol and others like it are private (`t` output from `nm`) while when `_generator.so` is built by linking in the separate `libnpymath.a` they're public (`T` output from `nm`).\r\n\r\nHowever, it looks to me like the test and the docs at https:\/\/numpy.org\/devdocs\/reference\/random\/examples\/cffi.html are wrong. No one has any business digging into the symbols of `_generator` - nor should they even expect that private extension module to exist. The second piece of documentation mentions `_generator` too, but doesn't actually use it in the code: https:\/\/numpy.org\/devdocs\/reference\/random\/extending.html#cffi.\r\n\r\nI think the right change to make is to change the problematic example\/test, avoiding any mention of `_generator`, and make it use `rng.bit_generator.cffi` which is public. Probably in a separate PR that can be merged quickly, and then rebase this PR on top.\r\n\r\n@mattip do you agree?","This might need a release note, although it looks like this may not make it into 2.0.0.","I've been pushing changes to the libnpymath repo and this PR with fixes for the `f2py` issues and `s390x`. After the libnpymath PRs get merged, the only outstanding issue would be the one related to `test_cffi`. I'm trying to fix the test there, according to @rgommers's suggestion, but I know too little about `cffi` for it to go much faster.\r\n\r\nI'll certainly add a release note, as soon as CI passes.","@rgommers @mattip Regarding the `test_cffi` issue (which is the only one that remains), I'm not sure how exactly that test can be fixed. Would it make sense to follow the approach mentioned in the [numba + cffi example](https:\/\/numpy.org\/devdocs\/reference\/random\/examples\/numba_cffi.html#extending-via-numba-and-cffi). Otherwise I can't see how we get access to those symbols with just `rng.bit_generator.cffi`.","> Would it make sense to follow the approach mentioned in the [numba + cffi example](https:\/\/numpy.org\/devdocs\/reference\/random\/examples\/numba_cffi.html#extending-via-numba-and-cffi). Otherwise I can't see how we get access to those symbols with just `rng.bit_generator.cffi`.\r\n\r\nI think that that example is consistent with the approach here, since what it does is simply tell the user to vendor `distributions.c` and use symbols from there. There is no use of any private numpy libraries or extension modules, only `rng.bit_generator.cffi`. I think that is the only reasonable thing to do. Please feel free to remove or modify any test or example that makes use of anything that starts with an underscore.\r\n\r\nTo not bloat the size of this PR further, it's also fine with me to mark such tests as `xfail` or delete them, and file follow-up issues."],"labels":["03 - Maintenance"]},{"title":"ENH: granular dtype reprs, even when bitwise-identical","body":"I don't see an issue open for this at the moment, but the search for \"dtype repr\" returns a bunch of results. This came up in gh-25285 and over in SciPy at i.e., https:\/\/github.com\/scipy\/scipy\/pull\/19686.\r\n\r\nBecause different platforms\/archs can interpret i.e., the single-character typecodes \"differently,\" we can end up with stuff like this, where the reprs are deceptive of the bit layout \"meaning\" (true type), even though the bits are in the same places:\r\n\r\n```python\r\n# arm64 Mac example\r\n>>> np.array([0], dtype=\"G\").dtype\r\ndtype('complex128')\r\n>>> np.array([0], dtype=\"D\").dtype\r\ndtype('complex128')\r\n>>> np.array([0], dtype=\"G\").dtype == np.array([0], dtype=\"D\").dtype\r\nTrue\r\n>>> np.array([0], dtype=\"G\").tobytes() == np.array([0], dtype=\"D\").tobytes()\r\nTrue\r\n>>> np.issubdtype(np.array([0], dtype=\"G\").dtype, np.clongdouble)\r\nTrue\r\n>>> np.issubdtype(np.array([0], dtype=\"D\").dtype, np.clongdouble)\r\nFalse\r\n```\r\n\r\nPerhaps `dtype` reprs might eventually reflect the bit layout \"meaning\" (the true type, rather than \"some type\" that represents just the same bit width?). It sounds like Sebastian might have been \"ok\" with some approach to resolving this, though I may have misunderstood.\r\n\r\n","comments":["I think the bit size is the best representation in general.  It doesn't bother me much that it is losing a bit of info: for the majority of users that info is hopefully meaningless.\r\n\r\n`clongdouble` and `longdouble` are special though and I am happy if their dtypes (not just the scalar types) stop using the bit-size, because for them the bit-size is often misleading at best and also not portable.\r\n\r\nI suspect the repr is picked in a Python helper somewhere in `_internal.py`.","This type of changes would do it:\r\n```\r\ndiff --git a\/numpy\/_core\/_dtype.py b\/numpy\/_core\/_dtype.py\r\nindex 6acf01ba88..e7a3211509 100644\r\n--- a\/numpy\/_core\/_dtype.py\r\n+++ b\/numpy\/_core\/_dtype.py\r\n@@ -146,8 +146,17 @@ def _scalar_str(dtype, short):\r\n     elif np.issubdtype(dtype, np.number):\r\n         # Short repr with endianness, like '<f8'\r\n         if short or dtype.byteorder not in ('=', '|'):\r\n+            if dtype.type == np.longdouble:\r\n+                return f\"'{byteorder}g'\"\r\n+            elif dtype.type == np.clongdouble:\r\n+                return f\"'{byteorder}G'\"\r\n+\r\n             return \"'%s%c%d'\" % (byteorder, dtype.kind, dtype.itemsize)\r\n \r\n+        elif dtype.type == np.longdouble:\r\n+            return \"'longdouble'\"\r\n+        elif dtype.type == np.clongdouble:\r\n+            return \"'clongdouble'\"\r\n         # Longer repr, like 'float64'\r\n         else:\r\n             return \"'%s%d'\" % (_kind_name(dtype), 8*dtype.itemsize)\r\n```\r\nAlthough, I almost wonder if:\r\n```\r\nnp.dtypes.CLongDouble()\r\n```\r\nand `np.dtypes.LongDouble(byteorder=\">\")` wouldn't be nicer (would need to be implemented to actually work, but shouldn't be hard, I think).","I agree if we want nicer reprs we should make them use the class initialization syntax (`np.dtypes.CLongDouble()`)."],"labels":["01 - Enhancement"]},{"title":"ENH: new datatypes for efficient storage","body":"### Proposed new feature or change:\n\nBased on https:\/\/github.com\/numpy\/numpy\/pull\/25347#issuecomment-1848700866\r\n\r\nOf course, this concept is up for discussion, I'd like to have some feedback and therefore, I will also provide multiple options for various aspects.\r\n\r\nCurrently, Numpy does allow to store arbitrary data within arrays via the dtype object. However, it comes with a lot of overhead when writing contents to disk: contents need to be pickled, which consumes both a lot of space and CPU usage, since serialization is _actually_ necessary. Also, since the memory representation is different from the serialization, object array .npy files cannot be (efficiently) memory mapped.\r\n\r\nOn the other hand, there are Numpy datatypes (byte, int, float, double, ... and structs thereof) that can be written to disk very efficiently with barely any serialization necessary besides the creation of a header, which is just a few bytes long. Instead, memory is just dumped into a file without any processing which can happen at the speed of the interface. At the time of writing, modern consumer hardware (PCIE 5.0 SSDs with 4 lanes) exists that can read and write with more than 10 GBytes\/s. Not too many years ago this used to be the speed of RAM, so memory mapping becomes even more viable nowadays. On the other hand, e.g. large language models (LLMs) and video consume memory like never before with no end in sight, so there will be demand. Even when not using larger arrays then the main memory, memory mapping is convenient by eliminating load times between program runs.\r\n\r\nWith certain applications however come problems: e.g. both text (LLMs) as well as video can vary in length (ragged arrays), which is currently only supported via the object dtype. While text is being worked on https:\/\/github.com\/numpy\/numpy\/pull\/25347 there is no solution for serialization and there is also no solution for ragged arrays (like e.g. video) yet.\r\n\r\nThis issue suggests how to handle certain datatypes which are currently only available via the object interface in a way that they can be (de)serialized efficiently - in other words \"dumped\" from memory and memory mapped. It would also be interesting to preserve the ability to append efficiently to .npy files and also to not modify the file format specified in [NEP 1](https:\/\/numpy.org\/neps\/nep-0001-npy-format.html), but just add new dtypes.\r\n\r\nAppendability, although not explicitly mentioned in NEP 1 is quite a sought-after property, compare the discussion and the top answer to https:\/\/stackoverflow.com\/a\/30379177 This also shows that the .npy file format is popular not _despite_ but _because_ of its simplicity.\r\n\r\nI would like to suggest the following dtypes:\r\n\r\n1. Reference (alternatively: Enumeration, Reference, Pointer, Offset or Index)\r\nArray Indices would refer to the first axis (0 for C order, -1 for Fortran order), which would also determine the final shape of the array. For example, if the referred-to array has the shape (100, 20, 30) \"array of images\" and C order, then a Reference would be a (20,30) \"image\". If the referring array has the shape (120, 5, 10), then the final shape would be (120, 5, 10, 20, 30). I would not recommend to use byte offsets since this makes it more difficult to do certain modifications to the referred-to array: for video one could modify all images (e.g. using a different crop, i.e. different dimensions) without invalidating the References (not possible with byte offsets).\r\n2. Range (two references from 1. or one reference and one length)\r\nTo create ragged arrays, Ranges are required. This would require to modify the concept of a shape. Raggedness can happen in multiple dimensions, depending on how many levels of References\/Ranges are used. For the shapes above, using Ranges instead of References would result in the final array to have a shape like (120, 5, 10, *, 20, 30) with * indicating varying lengths.\r\n3. Strings (can be either zero delimited UNIX style strings, then it's a reference (see 1), otherwise a Range into a byte array)\r\n3.1. Zero delimited is more space efficient both with references and storage\r\n3.2. Ranges into byte arrays use more space, but certain operations can be faster, strings can contain zero (has advantages and disadvantages)\r\n3.3. One could theoretically also implement both String variants and let the user decide\r\n\r\nSo basically, all of those dtypes are uint64 numbers or tuples with two elements of uint64 numbers. They can point to different arrays that can be stored in different .npy files and\/or in memory, making these dtypes memory mappable. Since multiple regular .npy files are used, they can always be appended to. Doing many append operations on multiple files may stress the file system (fragmentation), but makes the implementation easier.\r\n\r\nAdditional .npy files can be organized in a hierarchical way (one .npy file and all other filenames derived from that) or with multiple files referring to each other like in Microsoft Excel, with the difference that references are not per-cell, but per column. Both options do not exclude each other. It would be possible to make filenames in dtypes just optional. If a filename is provided, it is option 2, otherwise option 1 and filenames are derived automatically.\r\n\r\n# Option 1: Organize .npy hierarchically with automatic filenames\r\nLet's say we save some \"main.npy\" which refers to other arrays. Then, all those other arrays can be saved as \"main.npy.heap1\", \"main.npy.heap2\", ... if the dtype from main.npy is struct and multiple struct fields are References, Ranges or Strings or just \"main.npy.heap\" for simple, non-struct dtypes. One could also use a different ending for Strings, like \"main.npy.strings\" and it would be a (regular .npy) byte array. Although the heap files do not end in .npy, they would be regular .npy files. When the file main.npy gets loaded, all files of all reference dtypes are loaded or memory mapped as well. Even without extra documentation, a user could quickly see that those files belong together, since they share the main .npy file's name. String dtypes would require such approach, if one did not want to embed strings into the .npy file format by introducing a new Numpy file format version and\/or sacrificing appendability.\r\n\r\n# Option 2: Excel-like cross referenced .npy files\r\nIf one array has a Reference, Range or String dtype that refers to another array, both could be saved in different files with the filenames being specified explicitly in the dtype description. This would be comparable to cell cross file references in Excel, just on a per-column basis, e.g. \"video.npy\" could have one Range datatype that refers to \"image.npy\" with \"image.npy\" containing all (uncompressed, maybe low res) images of all videos in their order. Another file \"video_events.npy\" could contain Ranges to parts of those video files where some events happen. So one could have multiple Reference\/Range sets into the same data. With option 1 this would require \"image.npy\" to be duplicated, which might or might not be convenient\/possible.\r\n\r\nThat's it for now. Any feedback, suggestions, questions?","comments":["I think that it's premature to make a super-general solution for all kinds of possible custom dtypes that don't exist yet. I suspect that for most of those custom dtypes, we would simply choose to not support them in the NPY format. For these new and complicated custom dtypes, the dtype implementation comes first and the serialization design comes after, and very likely, both outside of numpy, at least at first.\r\n\r\nI'm happy to entertain proposals to implement the serialization of `stringdtype` inside of the NPY format, though, since it is in front of us inside of `numpy` and is aimed at replacing existing uses of other dtypes rather than completely new use cases. I'd lean towards implementing a super-simple subset of the [Arrow IPC streaming format](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#ipc-streaming-format) that only considers UTF-8 string data. When we introduced Yet Another File Format, it was important that only reinvented the tiniest of wheels. The data block is either just the plain old bytes from memory in the easy cases, and in the one hard case of `dtype=object`, we punt to an existing standard. `stringdtype` is another hard case, and if it is at all possible to do so, I'd like to also punt to an existing standard. I think there is a very minimal subset of the Arrow IPC streaming format that we could implement to do this (though I haven't put fingers to keyboard to try it).\r\n\r\nIf you'd like to continue a discussion on the serialization of `stringdtype` within the NPY format, it might be best to start a fresh issue.","Okay, after one week we evidently have no further participants in this discussion.\r\n\r\n> I think that it's premature to make a super-general solution for all kinds of possible custom dtypes that don't exist yet. I suspect that for most of those custom dtypes, we would simply choose to not support them in the NPY format. For these new and complicated custom dtypes, the dtype implementation comes first and the serialization design comes after, and very likely, both outside of numpy, at least at first.\r\n\r\nAccording to [NEP 1](https:\/\/numpy.org\/neps\/nep-0001-npy-format.html), dtypes should be memory-mappable (maybe with the exception of object). So when talking about the \"serialization\" first before anything else, it's effectively a discussion about memory layout instead of (solely) file formats. Should be a totally valid start point for a discussion. Besides: if a dtype is not serializable efficiently, then why not just take object in the first place? Makes much less trouble.\r\n\r\nAlso, I'm a little bit surprised on how stringdtype can be implemented with the \"serialization\" (or memory-mapping) part unknown, unspecified and undiscussed. I mean probably there is a lot of work to do to introduce a new dtype to Numpy even without \"serialization\", but I see this part as quite critical because otherwise one could just use the object dtype for strings.\r\n\r\nI also don't see what should be so complicated and custom about the References and Ranges I suggested. That's literally just indexing, an operation that even the most inexperienced Numpy user does all the time. And I have even not suggested full-featured slicing, because that would actually be complicated (on a datatype memory\/file level already). Indexing with a single number as index is the most simple thing everybody does all the time in Numpy. Probably that's also how everybody stores ragged arrays right now (two of such indices or an index and a length), with some custom implementation of the above mentioned Ranges. I have done it and such a custom implementation is around 20 lines of Python code. Problem-specific of course, because ragged arrays do not exist in Numpy yet, but just a bunch of uint64 numbers in struct in a Numpy file.\r\n\r\nRagged arrays are also mentioned e.g. in [NEP 55](https:\/\/numpy.org\/neps\/nep-0055-string_dtype.html) as \"Since NumPy has no first-class support for ragged arrays\", so this issue can be seen as my suggestion on how to implement \"first-class support for ragged arrays\" in Numpy or what I would consider the most pragmatic approach to that.\r\n\r\nWho knows, maybe this is already implemented somewhere outside of Numpy. But all the other Numpy-like libraries are inspired by Numpy (by definition of course) and I'd like to see ragged arrays as a standard feature one day and made a suggestion on how this can look like in memory.\r\n\r\nI have not seen any other suggestions or a discussion on how to implement ragged arrays in Numpy yet and I hereby claim that it can't be done any simplier than how I suggested above. If there is no interest in such a feature or it's considered as too maintenance-intense, that's another thing, but no comments were made about that so far either.\r\n\r\n> \r\n> I'm happy to entertain proposals to implement the serialization of `stringdtype` inside of the NPY format, though, since it is in front of us inside of `numpy` and is aimed at replacing existing uses of other dtypes rather than completely new use cases. I'd lean towards implementing a super-simple subset of the [Arrow IPC streaming format](https:\/\/arrow.apache.org\/docs\/format\/Columnar.html#ipc-streaming-format) that only considers UTF-8 string data. When we introduced Yet Another File Format, it was important that only reinvented the tiniest of wheels. The data block is either just the plain old bytes from memory in the easy cases, and in the one hard case of `dtype=object`, we punt to an existing standard. `stringdtype` is another hard case, and if it is at all possible to do so, I'd like to also punt to an existing standard. I think there is a very minimal subset of the Arrow IPC streaming format that we could implement to do this (though I haven't put fingers to keyboard to try it).\r\n\r\nImplementing even \"super-simple subsets of the Arrow IPC streaming format\" seems much more of a Yet Another File Format to me than my two ideas on how to implement strings. My first suggestion to just have byte offsets to zero-delimited arrays is literally how (UTF8) strings are implemented in Unix or C, with the only difference that there is some start of region that needs to be added to make actual C pointers out of it. My suggestion was to put all the strings into one regular Numpy file (or memory region, since memory-mapping is a requirement by NEP 1) and have the array with the stringdtype contain offsets into that actual string-containing array. I don't even know how to implement strings in an even simplier way. No modifications to the Numpy file format necessary, just a new dtype.\r\n\r\nYes, stringdtype is less trivial than the currently implemented dtypes in Numpy (with exception of object of course) because it has variable length. But nevertheless, strings are a solved problem in computer engineering and it's just a matter of which solution to pick.\r\n\r\nWhat I could add to my suggestion above is how memory is handled once the user wants to replace a string with another. If the new string is shorter than the old one, one can keep the pointer, just replace the string and move the zero delimiter accordingly. If the new string is longer, it can be appended to the end of the Numpy file that contain the actual strings and the pointer in the stringdtype Numpy file can be modified accordingly. Therefore, the file (or memory region) with the actual strings is required to be appendable and needs to be appended to. To clean up, the user can call some function, e.g. \"vacuum\" like in relational databases (e.g. sqlite). This will construct the strings file from new and remove all unused strings. Such an approach would ensure a predictable runtime performance and is pretty common (databases).\r\n\r\nAlso, please note that having regular Unix\/C strings lurk around somewhere in memory allocated e.g. via one malloc call per string and (actually) serializing it when writing a .npy file is not compatible with memory mapping and thus would violate NEP 1. So I do not see that many options in actually implementing a NEP 1 compatible stringdtype, which I consider a good thing.\r\n\r\n> \r\n> If you'd like to continue a discussion on the serialization of `stringdtype` within the NPY format, it might be best to start a fresh issue.\r\n\r\nI have made two suggestions about how to do the memory layout or \"serialization\" (same, since memory mapped) and I have nothing to add. Somebody else can open an issue if required.","Let me just say that you should ignore NEP 1 in this regard.  It was written very long ago when there were simply no dtypes that contained references\/needed this type of storage besides `object`.  So whatever applies to `object` can be generalize to \"includes internal references\" and strings do that (unless you propose to unravel the core of NumPy).\r\n\r\nAlso note that we can implement alternative ways to store the strings in the future.  We could well store them as Python strings or custom Python objects.  But NEP 55 points out some downsides (GIL, speed, memory use), and I mostly will trust they apply.  Those downsides are unrelated to potential storage to disc.\r\n\r\nWe may eventually implement a storage with memory mapping support.  I could for example imagine only supporting that read-only.  This might be a great step, as it could be nice to have a `StringDType(mutable=False)` that could ensure compact, more efficient, and maybe also simpler storage and access (mutability is a major pain in a lot of ways).\r\nBut, I simply don't see this as an urgent discussion w.r.t. to merging `StringDType()`.  I see it as a distinct project that will hopefully be attacked eventually.\r\n\r\n---\r\n\r\nWould more ragged array support be nice?  Maybe, but it would also be a rewrite\/duplication of most of NumPy.  It requires a whole new array type, a new way to make structured dtypes, and probably a lot more I am forgetting...\r\n\r\nImplementing such may be very doable for a new (especially dataframe) library, but for numpy as an N-D library that needs to give backwards compatibility guarantees, it seems far more complicated to even have the basic infrastructure implemented, then the `StringDType()` implementation itself is right now.  And if anyone deep into this was able to say with confidence that it is doable, I might agree.  But I have talked to smarter people than me about it and nobody could yet offer an idea of how it might be workable.","> Let me just say that you should ignore NEP 1 in this regard. It was written very long ago when there were simply no dtypes that contained references\/needed this type of storage besides `object`. So whatever applies to `object` can be generalize to \"includes internal references\" and strings do that (unless you propose to unravel the core of NumPy).\r\n>\r\n\r\nReferences existed long before NEP 1: for example, [Harold Lawson](https:\/\/en.wikipedia.org\/wiki\/Harold_Lawson) invented the pointer in 1964, C was published in 1972, while NEP 1 was published in 2007, which I would not even consider as old. Sebastian, I can still remember very well how we did night shifts together on the physics exercise sheets in G\u00f6ttingen back then :smile:\r\n\r\nI think the question is rather in which light to see NEP 1, dependent or independent of whether it is old or not: is it foundational to Numpy or obsolete.\r\n1. Is the sort of NEP 1 simplicity the reason why Numpy is so popular and inspired so many libraries and may inspire in the future or \r\n2. is NEP 1 limiting Numpy's growth by e.g. making text impossible?\r\n\r\nThat's also the reason why I've postet [this StackOverflow link](https:\/\/stackoverflow.com\/a\/30379177) in my comment above. Have a look at it and what happened: somebody asks how to append to .npy. Somebody else replies \"Nah, use something standardized like HDF5 instead\". People complain about them having various issues (all rooted in the complexity of HDF5), I created the npy-append-array module and even the author of the original answer changed mind and now refers to either npy-append-array (or zarr). I think that NEP 1 level simplicity is very much underrated and not well understood until one sees it in action (or see what troubles complicated solutions bring).\r\n\r\nAlso, the question is what .npy should be. Should it be a data exchange format like at least a dozen other or should it be more like a raw, fully-memory-mapped (and appendable) format to work with between program runs (that only take seconds instead of minutes thanks to memory-mapping and appendability) and once everything is condensed and settled to be exported into some (already existing) data exchange format? I see a great opportunity here for Numpy to redefine the state-of-the-art once again instead of chasing a runaway train (data exchange formats).\r\n\r\nWith npy-append-array (which I would still like to see in Numpy one day, the code has Numpy grade quality I would say) I have demonstrated that .npy is very capable of being this sort of work format. With reference-like datatypes (like stringdtype) all within the same .npy file, efficient appending does not work anymore. Multiple files would be needed for that (one suggestion on how see [above](https:\/\/github.com\/numpy\/numpy\/issues\/25374#issue-2038139997)). Also, a single `sidecar_size` might not cover the needs for future data storage (like for the References I described above). If multiple files are to be considered so much of a hassle to users, one may enable `np.load` to also load .zip-files and have some sort of main.npy in that zip-file. This would allow arbitrary many array and string storages (or whatever may come in the future) in one single file. On the plus side, one could also enable compression that way. Every .exe file in Windows btw. is a zip file and contains the individual parts of the executable (e.g. - guess what - strings). With some extra option to `np.save` (e.g. `individual_files=True`) one could save individual files (e.g. in the hierarchy suggested above) for appending and writeable memory mapping instead. \r\n\r\n> Also note that we can implement alternative ways to store the strings in the future. We could well store them as Python strings or custom Python objects. But NEP 55 points out some downsides (GIL, speed, memory use), and I mostly will trust they apply. Those downsides are unrelated to potential storage to disc.\r\n> \r\n> We may eventually implement a storage with memory mapping support. I could for example imagine only supporting that read-only. This might be a great step, as it could be nice to have a `StringDType(mutable=False)` that could ensure compact, more efficient, and maybe also simpler storage and access (mutability is a major pain in a lot of ways). But, I simply don't see this as an urgent discussion w.r.t. to merging `StringDType()`. I see it as a distinct project that will hopefully be attacked eventually.\r\n\r\nAs described above, there is a simple way to implement mutability. Maybe not the efficient way (I think there might be books filled with theory about how to do that), but a simple pragmatic option which is (most likely) identical to Sebastian's immutable suggestion if no strings are modified. I think arrays must be appendable though to implement any sort of mutability (since texts can always grow).\r\n\r\n> \r\n> Would more ragged array support be nice? Maybe, but it would also be a rewrite\/duplication of most of NumPy. It requires a whole new array type, a new way to make structured dtypes, and probably a lot more I am forgetting...\r\n> \r\n> Implementing such may be very doable for a new (especially dataframe) library, but for numpy as an N-D library that needs to give backwards compatibility guarantees, it seems far more complicated to even have the basic infrastructure implemented, then the `StringDType()` implementation itself is right now. And if anyone deep into this was able to say with confidence that it is doable, I might agree. But I have talked to smarter people than me about it and nobody could yet offer an idea of how it might be workable.\r\n\r\nI think it would make sense (given there is enough interest in ragged arrays) to discuss what the simpliest solution could be first.\r\n\r\nMy suggestion from above for example can be narrowed down even further where references need to be growing and cover the entire data array so that also no holes with unused data are left and no elements overlap. The good thing is that in my suggestion, data types are always well-defined. For stringdtype, an example for this narrowing-down would (probably) be the immutable version of stringdtype Sebastian mentioned.\r\n\r\nOtherwise, from the algorithmic side, most can probably be solved by adding an outer loop to every function (which for ordinary arrays would just iterate over one element) to broadcast over all the linear pieces of the ragged array. Also, that would be perfectly backwards compatible. Certain methods could use specific structures for speedup, like a search method for strings could look for the string in the entirety of the string data and resolve the reference to the string after finding it.\r\n\r\nAre there some functions which are really hard cases where such an approach would not work?\r\n\r\nAnother aspect to this is that ragged arrays could replace Python loops by effectively moving it to C via Numpy. Could speed up a lot of user code out there. Users could restructure their programs around ragged arrays, just as they do now by adding extra dimensions to arrays. From this perspective, first-class ragged array support would also be useful to users that don't even use improvised ragged arrays yet.","NEP 1 documents the NPY format; that's all it does. When it talks about dtypes, it's really only talking about the dtypes that were present at the time it was written (it's old enough to drink in some jurisdictions). In no sense was it intended to be read as constraining or even _commenting on_ the design of new dtypes or dtype infrastructure. It's just talking about the NPY format. Whether or not new dtypes will be supported by the NPY format is entirely optional. It's not a requirement that the NPY support a new dtype or a new dtype be designed to be supported by the NPY format. numpy is allowed to evolve in 2024 in ways not conceived of in 2007.\r\n\r\nIf you want to discuss ragged array support, that's fine (though I'm not sensing much excitement on the dev team for it), but definitely ignore NEP 1. It has nothing to constrain or guide you on that subject. Source: me; I wrote it.","Merry Christmas everybody!\r\n\r\nThank you for clarifying. There were moments in our discussion where I thought there might be a second Robert Kern. And yes, in Germany NEP 1 would be old enough so you could drink a beer with it :smile: To me, NEP 1 will always be my favorite NEP and thus, I will always be a fan of your work. But let's leave NEP 1 out of the discussion for now, I think I can make my point without it as well.\r\n\r\nCurrently, NEP 55 suggests to introduce a `sidecar_size` and a new .npy version 4. I have once experimented with the `np.load` and `np.save` code myself (`format.py`) so I know that even if the format otherwise stayed the exact same, the mere introduction of a new .npy version number will be a breaking change to existing Numpy versions. It will also break appendability and the `npy-append-array` library I have created. Maybe not for all arrays, but for all arrays containing stringdtype. I think that this is not necessary and I have suggested alternative implementations, e.g. zip which according to\r\n\r\n``cat \/usr\/share\/mime\/subclasses | grep \"application\/zip\" | wc -l``\r\n\r\nis used in 56 popular file formats. It would make .npy future proof without even requiring to introduce a new version 4.0 and it would most likely require less code changes than introducing a .npy version 4, since zip is natively supported by Python. Only `np.load` and `np.save` would need to be adapted. On top, this would add compression (optionally), which many users would profit from.\r\n\r\nIf we make a breaking change anyway, could we maybe switch to a .zip-based format with some main .npy file inside? No matter what one wanted or does not want to do in the future, zip would provide a lot of flexibility, is a super-standard way to go and would most likely be the smaller code change over introducing a version 4.0 of .npy.\r\n\r\nAny opinions about this?","That would still be a new format, essentially comprising a new version of NPY. Files created in this format by numpy 2.x would not be readable by numpy 1.26. We would still go through all of the same processes, like incrementing the format version number, to handle that. All of the same mitigations (e.g. only using the newest format when necessitated by the presence of new dtypes) would also within the existing NPY format framework.\r\n\r\nUsing a ZIP format in this way is not an unreasonable approach for `stringdtype`, but I don't think it solves the problem you are claiming it solves. `stringdtype` is sufficiently different from our other dtypes that we will need what amounts to a new version of the NPY file format to support it in there, no matter what approach we take. That's fine. That's not a problem that we need to avoid.\r\n\r\nIf you want to continue a discussion about `stringdtype` and NPY per se, I do suggest closing this issue and opening a new one. The title and description of this issue is no longer relevant to that discussion."],"labels":["01 - Enhancement","57 - Close?"]},{"title":"BUG: error: 'llrint' has not been declared in '::'","body":"### Describe the issue:\n\nI tried to install numpy on OpenIndiana and the compilation failed.\n\n### Reproduce the code example:\n\n```python\n$ PATH=\/usr\/gcc\/13\/bin:$PATH pip install numpy\n```\n\n\n### Error message:\n\n```shell\n[79\/512] Compiling C++ object numpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p\/src_npysort_simd_qsort_16bit.dispatch.cpp.o\r\n      FAILED: numpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p\/src_npysort_simd_qsort_16bit.dispatch.cpp.o\r\n      ccache c++ -Inumpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p -Inumpy\/core -I..\/..\/numpy\/core -Inumpy\/core\/include -I..\/..\/numpy\/core\/include -I..\/..\/numpy\/core\/src\/common -I..\/..\/numpy\/core\/src\/multiarray -I..\/..\/numpy\/core\/src\/npyma\r\nth -I..\/..\/numpy\/core\/src\/umath -I\/usr\/include\/python3.9 -I\/tmp\/pip-install-ttcaxowf\/numpy_70bcd61bf3234fffaafb4b85dfa26f4a\/.mesonpy-huevykhk\/build\/meson_cpu -fdiagnostics-color=always -DNDEBUG -Wall -Winvalid-pch -std=c++17 -O3 -msse -msse2 -msse3 -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -O3 -DNPY_HAVE_SSE2 -DNPY_HAVE_SSE -DNPY_HAVE_SSE3 -DNPY_HAVE_SSSE3 -DNPY_HAVE_SSE41 -DNPY_HAVE_POPCNT -DNPY_HAVE_SSE42 -DNPY_HAVE_AVX -DNPY_HAVE_F16C -DNPY_HAVE_FMA3 -DNPY_HAVE_AVX2 -DNPY_HAVE_AVX512F -DNPY_HAVE_AVX512F_REDUCE -DNPY_HAVE_AVX512CD -DNPY_HAVE_AVX512_SKX -DNPY_HAVE_AVX512VL -DNPY_HAVE_AVX512BW -DNPY_HAVE_AVX512DQ -DNPY_HAVE_AVX512BW_MASK -DNPY_HAVE_AVX512DQ_MASK -DNPY_HAVE_AVX512_CLX -DNPY_HAVE_AVX512VNNI -DNPY_HAVE_AVX512_CNL -DNPY_HAVE_AVX512IFMA -DNPY_HAVE_AVX512VBMI -DNPY_HAVE_AVX512_ICL -DNPY_HAVE_AVX512VBMI2 -DNPY_HAVE_AVX512BITALG -DNPY_HAVE_AVX512VPOPCNTDQ -DNPY_HAVE_AVX512_SPR -DNPY_HAVE_AVX512FP16 -msse -msse2 -msse3 -mssse3 -msse4.1 -mpopcnt -msse4.2 -mavx -mf16c -mfma -mavx2 -mno-mmx -mavx512f -mavx512cd -mavx512vl -mavx512bw -mavx512dq -mavx512vnni -mavx512ifma -mavx512vbmi -mavx512vbmi2 -mavx512bitalg -mavx512vpopcntdq -mavx512fp16 -DNPY_MTARGETS_CURRENT=AVX512_SPR -MD -MQ numpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p\/src_npysort_simd_qsort_16bit.dispatch.cpp.o -MF numpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p\/src_npysort_simd_qsort_16bit.dispatch.cpp.o.d -o numpy\/core\/libsimd_qsort_16bit.dispatch.h_AVX512_SPR.a.p\/src_npysort_simd_qsort_16bit.dispatch.cpp.o -c ..\/..\/numpy\/core\/src\/npysort\/simd_qsort_16bit.dispatch.cpp\r\n      <command-line>: warning: \"__STDC_VERSION__\" redefined\r\n      <built-in>: note: this is the location of the previous definition\r\n      In file included from ..\/..\/numpy\/core\/src\/common\/npstd.hpp:13,\r\n                       from ..\/..\/numpy\/core\/src\/common\/common.hpp:9,\r\n                       from ..\/..\/numpy\/core\/src\/npysort\/simd_qsort.hpp:4,\r\n                       from ..\/..\/numpy\/core\/src\/npysort\/simd_qsort_16bit.dispatch.cpp:8:\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1950:11: error: 'llrint' has not been declared in '::'\r\n       1950 |   using ::llrint;\r\n            |           ^~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1951:11: error: 'llrintf' has not been declared in '::'\r\n       1951 |   using ::llrintf;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1952:11: error: 'llrintl' has not been declared in '::'\r\n       1952 |   using ::llrintl;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1954:11: error: 'llround' has not been declared in '::'\r\n       1954 |   using ::llround;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1955:11: error: 'llroundf' has not been declared in '::'\r\n       1955 |   using ::llroundf;\r\n            |           ^~~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1956:11: error: 'llroundl' has not been declared in '::'\r\n       1956 |   using ::llroundl;\r\n            |           ^~~~~~~~\n```\n\n\n### Python and NumPy Versions:\n\nPython 3.9.16\r\nNumPy 1.26.2\n\n### Runtime Environment:\n\n_No response_\n\n### Context for the issue:\n\n_No response_","comments":["I believe this is already fixed on the `main` branch, could you try that? GCC 13 is brand-new, so I'm not very surprised that the latest release on PyPI may have an issue with it on more niche platforms.","Hi I'm getting the same errors. I've just taken a clone of main and followed the instructions in INSTALL.rst and I get the same errors about llrint etc. not being declared, same as mtelka. I'm using gcc 10.5.0 on OpenIndiana.","Is this still around and what fixed it in main (if it is fixed)?  If there is a simple fix, it might be good to backport to 1.26?  Ping @seiko2plus since I think you might just know? ","`llrint` & co are standard functions since C++11. The problem here seems to be in a system header from OpenIndiana. It'd be nice for someone with access to OpenIndiana to investigate. \r\n\r\nIf this is blocking building numpy, I think you can disable the code that's failing here with the `-Ddisable-optimization` build flag. E.g.:\r\n```\r\npip install . -Csetup-args=-Ddisable-optimization=true\r\n```\r\n\r\nI've removed the 2.0.0 milestone, since this isn't blocking and not easily actionable for a maintainer.","Do you have any suggestion how to prove the issue is in system header files?\r\n\r\nI tried this:\r\n```\r\n#include <cmath>\r\n#include <stdio.h>\r\n\r\nint main(void)\r\n{\r\n        printf(\"%lld\\n\", llrint(1.2));\r\n        return 0;\r\n}\r\n```\r\nand it compiles using `\/usr\/gcc\/13\/bin\/g++ -Wall test.c` and runs properly.  But this is obviously far from proving anything.\r\n\r\nThank you.","> ... someone with access to OpenIndiana to investigate.\r\n\r\nFYI, OpenIndiana can be downloaded here: https:\/\/openindiana.org\/downloads\/","I tried to install with `-Csetup-args=-Ddisable-optimization=true` and it still failed, but for different file:\r\n```\r\n$ PATH=\/usr\/gcc\/13\/bin:$PATH pip install numpy -Csetup-args=-Ddisable-optimization=true\r\n\r\n...\r\n\r\n      [194\/311] Compiling C++ object numpy\/core\/_multiarray_umath.cpython-39.so.p\/src_npysort_timsort.cpp.o\r\n      FAILED: numpy\/core\/_multiarray_umath.cpython-39.so.p\/src_npysort_timsort.cpp.o\r\n      c++ -Inumpy\/core\/_multiarray_umath.cpython-39.so.p -Inumpy\/core -I..\/numpy\/core -Inumpy\/core\/include -I..\/numpy\/core\/include -I..\/numpy\/core\/src\/common -I..\/numpy\/core\/src\/multiarray -I..\/numpy\/core\/src\/npymath -I..\/numpy\/core\/src\/umath -I\/usr\/include\/python3.9 -I\/tmp\/pip-install-_yqxfzbh\/numpy_9972414f41c04398a51dc1b1cc7e797b\/.mesonpy-nmi7lpja\/meson_cpu -fvisibility=hidden -fvisibility-inlines-hidden -fdiagnostics-color=always -DNDEBUG -Wall -Winvalid-pch -std=c++17 -O3 -DNPY_DISABLE_OPTIMIZATION -fPIC -DNPY_INTERNAL_BUILD -DHAVE_NPY_CONFIG_H -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -D__STDC_VERSION__=0 -fno-exceptions -fno-rtti -MD -MQ numpy\/core\/_multiarray_umath.cpython-39.so.p\/src_npysort_timsort.cpp.o -MF numpy\/core\/_multiarray_umath.cpython-39.so.p\/src_npysort_timsort.cpp.o.d -o numpy\/core\/_multiarray_umath.cpython-39.so.p\/src_npysort_timsort.cpp.o -c ..\/numpy\/core\/src\/npysort\/timsort.cpp\r\n      <command-line>: warning: \"__STDC_VERSION__\" redefined\r\n      <built-in>: note: this is the location of the previous definition\r\n      In file included from \/usr\/gcc\/13\/include\/c++\/13.2.0\/math.h:36,\r\n                       from \/usr\/include\/python3.9\/pyport.h:205,\r\n                       from \/usr\/include\/python3.9\/Python.h:50,\r\n                       from ..\/numpy\/core\/src\/common\/npy_sort.h.src:5,\r\n                       from ..\/numpy\/core\/src\/npysort\/timsort.cpp:35:\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1950:11: error: 'llrint' has not been declared in '::'\r\n       1950 |   using ::llrint;\r\n            |           ^~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1951:11: error: 'llrintf' has not been declared in '::'\r\n       1951 |   using ::llrintf;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1952:11: error: 'llrintl' has not been declared in '::'\r\n       1952 |   using ::llrintl;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1954:11: error: 'llround' has not been declared in '::'\r\n       1954 |   using ::llround;\r\n            |           ^~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1955:11: error: 'llroundf' has not been declared in '::'\r\n       1955 |   using ::llroundf;\r\n            |           ^~~~~~~~\r\n      \/usr\/gcc\/13\/include\/c++\/13.2.0\/cmath:1956:11: error: 'llroundl' has not been declared in '::'\r\n       1956 |   using ::llroundl;\r\n            |           ^~~~~~~~\r\n      ninja: build stopped: subcommand failed.\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n```"],"labels":["00 - Bug"]},{"title":"DOC: Misleading docs for `np.fromfunction`","body":"### Issue with current documentation:\r\n\r\nI have ~2~ 1 things to note:\r\n\r\n- > Each parameter represents the coordinates of the array varying along a specific axis.\r\n\r\n  This leads the reader to assume that `i` and `j` will just be indices they can use which is not the case.\r\n- > ~then the parameters would be `array([[0, 0], [1, 1]])` and `array([[0, 1], [0, 1]])`~\r\n  \r\n  ~appears to have a typo as `[0,1]` is mentioned twice~ [see here](https:\/\/github.com\/numpy\/numpy\/issues\/25363#issuecomment-1853416556)\r\n\r\nThe function is defined [here](https:\/\/github.com\/numpy\/numpy\/blob\/v1.26.0\/numpy\/core\/numeric.py#L1776-L1845), rendered docs [here](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.fromfunction.html).\r\n\r\n### Idea or request for content:\r\n\r\nI came across [this](https:\/\/stackoverflow.com\/questions\/18702105\/parameters-to-numpys-fromfunction) SO question which describes the same issue I had. The [solution](https:\/\/stackoverflow.com\/a\/24900335\/12756474) also solved my problem.\r\n\r\nIn my opinion, explaining what `i` and `j` actually are should be both present and clear. In my case, the fact that I have used functions with similar names in different programming languages made me assume that this would work the same which is not true, I would assume this is true for a lot of people which is why it should be very obvious that this is not the case.\r\n\r\nThis seems to have been an issue for a while, I would be interested in making a PR for this as long as some discussion is had about how the docs could be changed :)","comments":["> appears to have a typo as [0,1] is mentioned twice\r\n\r\nThis is actually correct as is demonstrated in the examples section. The way to think about this is `np.fromfunction` is behaving like `f(x, y)` where `x` and `y` are arrays containing the indices of the x and y coordinates. By example:\r\n\r\n```python\r\n>>> np.fromfunction(lambda p1, p2: p1 + p2, shape=(2, 2))\r\narray([[0., 1.],\r\n       [1., 2.]])\r\n>>> np.array([[0, 0], [1, 1]]) + np.array([[0, 1], [0, 1]])\r\narray([[0, 1],\r\n       [1, 2]])\r\n```\r\n\r\n`(2, 2)` is perhaps not the best example shape, because the number of elements in each dimension is equal to the number of dimensions. Perhaps it's more clear with `(3, 3)`:\r\n\r\n```python\r\n>>> np.fromfunction(lambda x, y: x + y, shape=(3, 3))\r\narray([[0., 1., 2.],\r\n       [1., 2., 3.],\r\n       [2., 3., 4.]])\r\n# equivalent to:\r\n>>> np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]]) + np.array([[0, 1, 2], [0, 1, 2], [0, 1, 2]])\r\narray([[0, 1, 2],\r\n       [1, 2, 3],\r\n       [2, 3, 4]])\r\n```\r\n\r\n> This leads the reader to assume that i and j will just be indices they can use which is not the case.\r\n\r\nYeah IME this is a common misunderstanding and I don't disagree that `fromfunction` is often not what users actually want.\r\n\r\nxref #7622","Thanks for the correction! I crossed that part out in the issue text since it does not apply, I misread the example \ud83d\ude05\r\n\r\nAs for the rest of the docs, we could maybe mention as a note\/warning (highlight it in some way so its apparent) that the function will receive the coordinates as arrays and *not* numbers. We could also mention `vectorize` since that is from what I understand what anyone that has this issue to begin with needs to easily solve it.\r\n\r\nAlternatively, this information could potentially be conveyed through detailing the `callable` argument types as seen [here](https:\/\/docs.python.org\/3\/library\/typing.html#annotating-callable-objects), making it explicit that they are of type `numpy.ndarray` and not numeric. Even in this case I think a `vectorize` example should be mentioned.","Well, the [docs say that the `nin` argument is \"The number of input arguments\"](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.frompyfunc.html#numpy.frompyfunc)\r\n\r\nThis is what ChatGPT has to say about it:\r\n\r\n```\r\nYes, the term \"number of input arguments\" might be a bit abstract in this context. The shape parameter in np.fromfunction essentially determines the shape of the output array that you want to create, and it indirectly specifies the range of indices over which the provided function will be called.\r\n\r\nIn the context of np.fromfunction, the \"number of input arguments\" refers to the number of coordinates for which the function is applied. If shape is a tuple (n, m), then the function will be called with indices (i, j) for all i from 0 to n-1 and j from 0 to m-1. The function is called for each combination of these indices.\r\n\r\nSo, while the docs mention \"the number of input arguments,\" it might be clearer to think of shape as specifying the grid or dimensions over which the function will be evaluated. The resulting array will have the shape defined by shape, and the function will be applied to each combination of indices within that shape.\r\n```\r\n\r\nSo unless the number of something is synonymous to the shape of something in numpy parlance, I think this could be corrected in the docs so that the behavior can be gleaned off the documentation without requiring an external example of this use case. \r\n\r\nEven then why not explicitly mention that it will call the provided function with indices rather than numbers?\r\n\r\nWould that not be correct in the general case?\r\n\r\nHappy to learn. "],"labels":["04 - Documentation"]},{"title":"BUG: average() returns nan when an `inf` value is given a 0 weight","body":"### Describe the issue:\n\nWhen I try to compute the weighted mean of an array that includes `inf`, and that element is given a weight of 0, I'm getting a RuntimeWarning, and a nan result.\r\nThis isn't the case when the weight is just slightly larger than zero.\r\n\r\nDrilling this down, I reached np.multiply(), but I'm not familiar enough with python\/C to dig deeper..\n\n### Reproduce the code example:\n\n```python\nimport sys\r\n\r\nimport numpy as np\r\n\r\nprint(np.average([1, 1], weights=[1, 0]))\r\n# 1.0\r\nprint(np.average([np.inf, 1], weights=[1, 1]))\r\n# inf\r\nprint(np.average([np.inf, 1], weights=[1, 0]))\r\n# inf\r\nprint(np.average([np.inf, 1], weights=[0, 1]))\r\n# nan\r\n# RuntimeWarning: invalid value encountered in multiply avg = np.multiply(a, wgt,\r\n\r\neps = sys.float_info.epsilon\r\nprint(np.average([np.inf, 1], weights=[0+eps, 1]))\r\n# inf\n```\n\n\n### Error message:\n\n```shell\nC:\\Users\\REDACTED\\anaconda3\\envs\\REDACTED\\lib\\site-packages\\numpy\\lib\\function_base.py:550: RuntimeWarning: invalid value encountered in multiply\r\n  avg = np.multiply(a, wgt,\n```\n\n\n### Python and NumPy Versions:\n\n1.26.2\r\n3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n\n### Runtime Environment:\n\n[{'numpy_version': '1.26.2',\r\n  'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, '\r\n            '13:26:23) [MSC v.1916 64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='DESKTOP-FIBPK27', release='10', version='10.0.22631', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'filepath': 'C:\\\\Users\\\\micha\\\\miniconda3\\\\envs\\\\np_bug\\\\Library\\\\bin\\\\mkl_rt.2.dll',\r\n  'internal_api': 'mkl',\r\n  'num_threads': 8,\r\n  'prefix': 'mkl_rt',\r\n  'threading_layer': 'intel',\r\n  'user_api': 'blas',\r\n  'version': '2023.1-Product'}]\r\nNone\n\n### Context for the issue:\n\nI came across this trying to use scipy's `gmean()` on arrays that contain 0, with 0 weights.\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/19647\r\n\r\n","comments":["To add some more context, I believe the current behavior is inconsistent with an intuitive interpretation of a 0 weight as meaning 'omit this value entirely'.\r\n\r\n```python\r\nfrom numpy import average\r\n\r\nprint(average([0, 1], weights=[1,0]))\r\n# 0 - implying the 0 weight means `omit this value entirely`\r\n\r\nprint(average([1, 0], weights=[1,0]))\r\n# 1 - implying the 0 weight means `omit this value entirely`\r\n```\r\n\r\nTherefore, even if the element to be ignored is `inf`, a consistent behavior would ignore it, not return a NaN value.","IEEE 754 floats are defined such that `inf * 0` is `NaN`:\r\n\r\n```python\r\n>>> 0 * float(\u201cInf\u201d)\r\nnan\r\n```\r\n\r\n","I see in the SciPy issue that R\u2019s behavior is prior art for handling zero weights with special behavior. R\u2019s docs indicates that weighted averages have special handling for zero weight values - pretty neat. As far as I can tell R has always behaved that way.\r\n\r\nMaking this change in numpy needs to clear a high bar because doing so is a behavior change. Since it will produce different results in a calculation, the behavior change must be justified given that this will inevitably break user code that is relying on this behavior somehow.\r\n\r\nHere is a code search that finds a number of [usages in downstream code](https:\/\/github.com\/search?q=np.average+weights&type=code). One way to gauge the impact of such a change would be to patch numpy to have this behavior and then run e.g. the scikit-learn tests and see if there are failures. I wouldn\u2019t be surprised if there are.\r\n\r\nIt\u2019s not just whether or not changing numpy\u2019s behavior would make it easier for newcomers to learn numpy, we also have to gauge downstream impact and decide whether the code churn is worth the pain of dealing with an imperfection in numpy that\u2019s existed for a long time.","Maybe worth  adding a not-default option for R\u2019s behaviour?\r\n\r\nOn Sun, 10 Dec 2023 at 17:34, Nathan Goldbaum ***@***.***>\r\nwrote:\r\n\r\n> I see in the SciPy issue that R\u2019s behavior is prior art for handling zero\r\n> weights with special behavior. R\u2019s docs indicates that weighted averages\r\n> have special handling for zero weight values - pretty neat. As far as I can\r\n> tell R has always behaved that way.\r\n>\r\n> Making this change in numpy needs to clear a high bar because doing so is\r\n> a behavior change. Since it will produce different results in a\r\n> calculation, the behavior change must be justified given that this will\r\n> inevitably break user code that is relying on this behavior somehow.\r\n>\r\n> Here is a code search that finds a number of usages in downstream code\r\n> <https:\/\/github.com\/search?q=np.average+weights&type=code>. One way to\r\n> gauge the impact of such a change would be to patch numpy to have this\r\n> behavior and then run e.g. the scikit-learn tests and see if there are\r\n> failures. I wouldn\u2019t be surprised if there are.\r\n>\r\n> It\u2019s not just whether or not changing numpy\u2019s behavior would make it\r\n> easier for newcomers to learn numpy, we also have to gauge downstream\r\n> impact and decide whether the code churn is worth the pain of dealing with\r\n> an imperfection in numpy that\u2019s existed for a long time.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/numpy\/numpy\/issues\/25362#issuecomment-1849029307>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAAQQHDRMWWAG2HLSMPJB5TYIXXDHAVCNFSM6AAAAABAONZJD2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQNBZGAZDSMZQG4>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n","Another good option might be to add a `where` parameter to `np.average`, similar to the `where` parameter in `np.mean` and other array reduction functions, then you could get the desired behavior with\r\n```\r\nnp.average(x, weights=w, where=w!=0)\r\n```","> the current behavior is inconsistent with an intuitive interpretation of a 0 weight\r\n\r\nAgree with this. Using weights to remove outliers and other bad data is pretty common, also needed for robust statistics. I haven't run into this because `inf` is an unusual value for data. How did you get it?","> Another good option might be to add a `where` parameter to `np.average`, similar to the `where` parameter in `np.mean` and other array reduction functions, then you could get the desired behavior with\r\n> \r\n> ```\r\n> np.average(x, weights=w, where=w!=0)\r\n> ```\r\n\r\nThis seems to be a very good option because this circumvents the issues of redefining `0 * inf` and instead provides ways to pick valid data out of the dataset.","Nice!\r\n\r\nOn Sun, 10 Dec 2023 at 18:56, Jake Vanderplas ***@***.***>\r\nwrote:\r\n\r\n> Another good option might be to add a where parameter to np.average,\r\n> similar to the where parameter in np.mean and other array reduction\r\n> functions, then you could get the desired behavior with\r\n>\r\n> np.average(x, weights=w, where=w!=0)\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/numpy\/numpy\/issues\/25362#issuecomment-1849050449>, or\r\n> unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAAQQHC56WGBPOMHTT2DPETYIYAXTAVCNFSM6AAAAABAONZJD2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQNBZGA2TANBUHE>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>\r\n","Using `np.ma.MaskedArray`, one can use the mask (which could be `weight>0` obviously):\r\n```\r\nnp.ma.average(np.ma.MaskedArray([np.inf, 1], mask=[True, False]), weights=[0, 1])\r\n\/usr\/lib\/python3\/dist-packages\/numpy\/ma\/extras.py:649: RuntimeWarning: invalid value encountered in multiply\r\n  avg = np.multiply(a, wgt,\r\n1.0\r\n```\r\nStill get the warning, but the result now is correct.","> How did you get it?\r\n\r\nI encountered this using scipy.gmean(), and initially reported there https:\/\/github.com\/scipy\/scipy\/issues\/19647#issue-2028761984 \r\nSpecifically, I tried to omit a 0 value in a geometric mean calculation. ","The issue you are encountering is related to the handling of infinity in Numpy's  'average' function when combined with weights. when you use a weight of 0 for an element with an infinite value, it leads to a multiplication of infinity by 0, resulting in a 'nan' value and not a number value. \r\n\r\nTo avoid this issue , you can preprocess your data to handle the cases where weights are zero for elements with infinite values. one way is to replace those weights with a very small positive value\r\n'sys.float_info.epsilon'   - \r\n\r\n```\r\nimport sys\r\nimport numpy as np\r\ndef weighted_average(data, weights):\r\n    eps = sys.float_info.epsilon\r\n    weights = [w if w != 0 else eps for w in weights]\r\n    return np.average(data, weights=weights)\r\n\r\nprint(weighted_average([1, 1], weights=[1, 0]))\r\n# 1.0\r\n\r\nprint(weighted_average([np.inf, 1], weights=[1, 1]))\r\n# inf\r\n\r\nprint(weighted_average([np.inf, 1], weights=[1, 0]))\r\n# inf\r\n\r\nprint(weighted_average([np.inf, 1], weights=[0, 1]))\r\n# inf\r\n\r\nprint(weighted_average([np.inf, 1], weights=[0 + sys.float_info.epsilon, 1]))\r\n# inf\r\n```","@Harshwardhanpjadhav the comment you just posted in this thread was non-responsive to the reporter. Please do not paste random code snippets in issues like this, it is a form of spamming. I've deleted the comment."],"labels":["00 - Bug"]},{"title":"[MAINT] update all TypedDict imports [skip ci]","body":"addresses https:\/\/github.com\/numpy\/numpy\/issues\/25206 by updating where TypedDict is imported from. This is to address an error pydantic raises when numpy_typing.DTypeLike is used in pydantic v2 BaseModels.\r\n\r\nI'm skipping ci since this is my first PR and I'm not sure if I should skip some of the CI described here given that the change is minimal: https:\/\/numpy.org\/devdocs\/dev\/development_workflow.html#commands-to-skip-continuous-integration","comments":["For the issue in particular the `*.pyi` changes are completely redundant anyway as Pydanticis purely a runtime tool; not a static type checker. The only way to fix this issue would be via the runtime import of `from typing_extensions import TypedDict` (in combination with a try\/except block), reintroduction of which I'm admittedly lukewarm about. Especially considering, as was already noted by Ralf, this issue is due to a (overly?) strict typeddict requirement on Pydantics side rather than a numpy bug.\r\n\r\nNow, if we're going ahead with the runtime import from typing extensions then we could also really use a reintroduction of this test (xref https:\/\/github.com\/numpy\/numpy\/pull\/19525) in order to assure we don't accidentally make typing extensions a hard dependency.","Thanks for the feedback! \r\n\r\n> The only way to fix this issue would be via the runtime import of from typing_extensions import TypedDict (in combination with a try\/except block), reintroduction of which I'm admittedly lukewarm about. \r\n\r\nI'll leave it up to the numpy maintainers if this is something that is alright to do. I agree the requirement is overly strict. Unfortunately I don't think pydantic will adjust this behavior since they closed issues related to this pretty emphatically. I don't need to use npt.DtypeLike in pydantic BaseModels but it would be nice if this worked.\r\n\r\nhttps:\/\/github.com\/pydantic\/pydantic\/issues\/6860#issuecomment-1651005718\r\nhttps:\/\/github.com\/pydantic\/pydantic\/issues\/6645#issuecomment-1634282750\r\n"],"labels":["03 - Maintenance","Static typing"]},{"title":"DOC: GSOD-NumPy Community Review #3","body":"### Overview:\r\n\r\n<a href=\"https:\/\/drive.google.com\/file\/d\/1F1pLhpVNRuMnVdmZ50QmeDMCDT0rdTFo\/view?usp=sharing\"><img src=\"https:\/\/hackmd.io\/_uploads\/ryIHONKmp.png\" width=\"400\"><\/a>\r\nHi everyone! I am inviting the community to review the first 10 pages of the '**NumPy Contributor Comics**', which is part of the GSOD-NumPy 2023 project.'\r\n\r\n**The comic can be [viewed here, as a PDF](https:\/\/drive.google.com\/file\/d\/1F1pLhpVNRuMnVdmZ50QmeDMCDT0rdTFo\/view?usp=sharing)**. You can comment direct on the PDF viewer or on this Github issue.\r\n\r\nUpdate since [Community Review #2](https:\/\/github.com\/numpy\/numpy\/issues\/24382): Illustrated of first 10 pages, based on the script. This version of the comics was released at the PyData NYC 2023 conference. I gave a talk there- [here\u2019s the video of the talk](https:\/\/youtu.be\/Gv_Ea94wquM?si=drLkjdN_0A75Jshn&t=40).\r\n\r\nThese comics aim to show newcomers that NumPy is driven by  people who use it: in this case, scientists! These comics will show grad students finding a usability problem in NumPy,  working together to submit an issue, and talking to community members.\r\n\r\nFor more details, check out the [project roadmap](https:\/\/medium.com\/@marsbarlee\/gsod-numpy-contributor-comics-project-roadmap-521280503fbd).\r\n\r\nI will also host a **review session at the next NumPy Community Meeting on December 6**, if you would like to chat!\r\n\r\nThis thread will be open for the next two weeks and **close on December 20.**\r\n\r\n### For Review:\r\n\r\nFor review, you can comment on any part, such as re-wording a specific line or panel.\r\n\r\nNote: The comic has 10\/16 pages complete. The remaining 6 pages will illustrate the script. The [script](https:\/\/docs.google.com\/document\/d\/1fjLTDqSkcKMxo8oSTRThllBFpjo5rXS9\/edit) has been reviewed in [Community Review #2](https:\/\/github.com\/MarsBarLee\/gsod-numpy-2023\/blob\/main\/review-2\/numpy-community-2023-08-16.md). \r\n\r\nSome specific parts I intend to add or change in the future:\r\n\r\n**Adapting from PyData NYC 2023 edition to numpy.org version**\r\n- Currently there are references to PyData NYC (on front cover and back cover)\r\n- I plan on removing these references \r\n- Replace the QR code on a page to lead to the numpy.org comic version\r\n\r\n**Next 6 pages**\r\n- Last panel of the comic on Page 10 explains the relationship between maintainers and contributors\r\n- Expand on this relationship in following pages\r\n- A student will ask \u201cWhy doesn\u2019t the maintainer do everything?\u201d\r\n- Answered by a visual metaphor of multiple people in a campsite working together. The maintainer as the lead organizer, and each doing a different job\r\n\r\n**Credits**\r\n- Add a 'Based on a true story': links to real [Github issue](https:\/\/github.com\/numpy\/numpy\/issues\/23319) and [PR](https:\/\/github.com\/numpy\/numpy\/pull\/23357) the comic was based on","comments":[],"labels":["04 - Documentation"]},{"title":"ENH: Release crackfortran as a standalone tool for parsing Fortran.","body":"The `crackfortran` tool is developed to be a backend for the f2py utility. However, crackfortran could also be very useful on its own.\r\n\r\nThe use of crackfortran could be demonstrated by the following example:\r\n\r\n```\r\n\u25ba crackfortran symbol.f90 -show\r\nReading fortran codes...\r\n        Reading file 'symbol.f90' (format:free)\r\nPost-processing...\r\n        Block: symbol\r\nPost-processing (stage 2)...\r\n[{'args': ['x', 'y', 'symx', 'opt'],\r\n  'block': 'subroutine',\r\n  'body': [],\r\n  'common': {'pltdat': ['model', 'ploter'], 'symbls': ['nsym', 'symbl']},\r\n  'commonvars': ['model', 'ploter', 'nsym', 'symbl'],\r\n  'entry': {},\r\n  'externals': [],\r\n  'from': 'symbol.f90',\r\n  'implicit': None,\r\n  'interfaced': [],\r\n  'name': 'symbol',\r\n  'sortvars': ['model', 'nsym', 'ploter', 'symbl', 'opt', 'x', 'y', 'symx'],\r\n  'vars': {'model': {'attrspec': [], 'typespec': 'integer'},\r\n           'nsym': {'attrspec': [], 'typespec': 'integer'},\r\n           'opt': {'typespec': 'integer'},\r\n           'ploter': {'attrspec': [], 'typespec': 'integer'},\r\n           'symbl': {'attrspec': [],\r\n                     'dimension': ['20', '2'],\r\n                     'typespec': 'integer'},\r\n           'symx': {'attrspec': [], 'dimension': ['2'], 'typespec': 'integer'},\r\n           'x': {'typespec': 'real'},\r\n           'y': {'typespec': 'real'}}}]\r\n```\r\n\r\nThe tool presents the subroutine declaration statements as a JSON that can be consumed by other tools for further analysis.\r\n\r\nEveryone who is looking for the Fortran parser should first come across crackfortran, before wasting time on many sloppy half-working parsers on the Internet. In this regard, the importance of crackfortran is currently misrepresented by keeping it in the depth of numpy internals. Therefore, this patch adds changes required to expose crackfortran as an entry point, next to f2py3 and reveal it to the world.\r\n","comments":["This would be a good topic to raise on the numpy mailing list.","> This would be a good topic to raise on the numpy mailing list.\r\n\r\n@charris , could you please help with this?","You can subscribe to the mailing list at https:\/\/mail.python.org\/accounts\/signup\/?next=\/mailman3\/lists\/numpy-discussion.python.org\/. Then just send an email to numpy-discussion@python.org. You can also open a numpy issue with the suggestion.","@HaoZeke does this seem reasonable, or is (for example) LFortran already exposing a more maintainable and robust parser?","@rgommers , sincerely any rich compiler-level technology is irrelevant to this call. The beauty of crackfortran is in its simplicity. Crackfortran parses the source (actually, the interface part only, not the code!) unarmed, only by regex matching. This approach is more oriented on extracting analytics from the syntactically-correct source. Unlike that, compiler parser has to be precise about may possible error states to assist correction of malformed code. Furthermore, extracting a JSON usable by a non-compiler person from a yacc\/bison-generated AST would be another tough adventure. So my point is: these are two different worlds. There are cars and carts, and good cars are not obsoleting the joy of carting.","> @HaoZeke does this seem reasonable, or is (for example) LFortran already exposing a more maintainable and robust parser?\r\n\r\nLFortran has a more robust parser, but it is still less complete than that of `f2py` at the moment. It will take time. Perhaps more importantly, at the moment the development is such that a dependency on `lfortran` is quite heavy, and would include LLVM. This could also be fixed, but I am not sure when it would be feasible to split the parser development into a separate project. Perhaps @certik would be able to give more insight.\r\n\r\nWith regards to @dmikushin's eloquent comment:\r\n\r\n> ... So my point is: these are two different worlds. There are cars and carts, and good cars are not obsoleting the joy of carting.\r\n\r\nI actually tend to agree. @pearu has mentioned on several occasions that F2PY doesn't need to penalize the user for not constructing correct Fortran (also F2PY takes the `!f2py` directives and `usercode` which LFortran would need a plugin ecosystem to support).\r\n\r\nThe cleanest route to couple LFortran and F2PY in the near future would be to generate F2PY's `crackfortran` style input from LFortran's ASR which would then guarantee semantic correctness while keeping the backend from F2PY agnostic to changes. Since LFortran also can generate JSON representations, it would be a good place to start integrating the two in any case.\r\n\r\nSome other projects also use `crackfortran` (e.g. `sphinx-fortran`) so I guess it could be a good idea to expose it.\r\n\r\nHowever some more thoughts for @dmikushin:\r\n- Would need documentation\r\n- Use cases \/ examples\r\n- AFAIK we use recursive dictionaries, do they have a good representation in JSON?\r\n\r\nIn principle, letting users clean up `crackfortran`'s output on their own is a good idea, it could be paired with the hooks @pearu introduced in #19388."],"labels":["01 - Enhancement","component: numpy.f2py"]},{"title":"DOC: Subarray Iteration with nditer","body":"### Issue with current documentation:\n\nDear Numpy Team,\r\n\r\nI am trying to use numpy.nditer to iterate over subarrays of ndarrays. While working on this, I encountered some challenges and would appreciate your insights on the best practices for such iterations.\r\n\r\nI am referring to:\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.nditer.html\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/arrays.nditer.html\r\n\r\nHere are some observations and questions based on my experimentation:\r\n\r\nTo obtain subarrays on each iteration step, it is necessary to use the \"external_loop\" flag, otherwise, you receive element-wise values.\r\n\r\nFor column-wise iteration over a C-contiguous array, setting the iteration order to \"F\" (Fortran) was required to avoid a flattened array. The result needed transposition to return to a C-contiguous array.\r\n\r\nFor row-wise iteration over a C-contiguous array, I initially copied and transposed the input array, and again set the iteration order to \"F.\" The result required transposition to return to a C-contiguous array. Can the copy process be avoided?\r\n\r\nHandling multiple ndarrays in such iterations could potentially become complex, and I suspect there might be more straightforward ways to achieve the same results.\r\n\r\nI have included a code example to illustrate the challenges and approaches I've taken. It would be very helpful if you could share any best practices or recommendations for efficiently iterating over subarrays using numpy.nditer in Python and\/or Cython.\r\n\r\n```\r\nimport numpy as np\r\n\r\ndef iter_array(arr: np.ndarray,\r\n               axis: int | None = None,\r\n               res: np.ndarray | None = None) -> np.ndarray:\r\n    \"\"\"\r\n    Function iterates over subarrays of an ndarray based on the specified axis.\r\n\r\n    Parameters:\r\n    - arr (np.ndarray): The input array to be iterated over.\r\n    - axis (int, optional): The axis along which to iterate.\r\n        If None, the array is flattened.\r\n    - res (np.ndarray, optional): An array to store the result of the iteration.\r\n        If None, a new array is allocated.\r\n\r\n    Returns:\r\n    - np.ndarray: The result array after iteration.\r\n\r\n    Example:\r\n        arr = np.arange(6.).reshape(2, 3)\r\n        iter_array(arr, axis=None)\r\n        iter_array(arr, axis=0)\r\n        iter_array(arr, axis=1)\r\n    \"\"\"\r\n    # ***** Print some information *****\r\n    msg = 'iterate C_CONTIGUOUS array '\r\n    if axis == 0:\r\n        msg += ('column-wise' + '\\n'\r\n                + '(order=F, transpose output)')\r\n    elif axis == 1:\r\n        msg += ('row-wise' + '\\n'\r\n                + '(order=F, tanspose+copy input, transpose output)')\r\n    else:\r\n        msg += 'flattened' + '\\n' + '(order=k)'\r\n    print('=============================================')\r\n    print(msg)\r\n    print('=============================================')\r\n    print('Input array:', '\\n', arr, '\\n')\r\n\r\n    # ***** Iteration process *****\r\n    a = arr.T.copy() if axis == 1 else arr\r\n    order = 'k' if axis is None else 'F'\r\n    with np.nditer(\r\n        op=[a, res],\r\n        flags=['external_loop'],\r\n        op_flags=[['readonly'],                 # Input flags\r\n                  ['writeonly', 'allocate']],   # Output flags\r\n        order=order,\r\n    ) as it:\r\n        for it_in, it_out in it:\r\n            it_out[...] = it_in - np.mean(it_in)\r\n            print(f'iterator: {it_in} do some math => x-xmean = {it_out}')\r\n        res = it.operands[1] if axis is None else it.operands[1].T\r\n    print('\\nResult array:', '\\n', res, '\\n')\r\n    return res\r\n\r\n# =============================================\r\n# iterate C_CONTIGUOUS array flattened\r\n# (order=k)\r\n# =============================================\r\n# Input array:\r\n#  [[0. 1. 2.]\r\n#  [3. 4. 5.]]\r\n\r\n# iterator: [0. 1. 2. 3. 4. 5.] do some math => x-xmean = [-2.5 -1.5 -0.5  0.5  1.5  2.5]\r\n\r\n# Result array:\r\n#  [[-2.5 -1.5 -0.5]\r\n#  [ 0.5  1.5  2.5]]\r\n\r\n# =============================================\r\n# iterate C_CONTIGUOUS array column-wise\r\n# (order=F, transpose output)\r\n# =============================================\r\n# Input array:\r\n#  [[0. 1. 2.]\r\n#  [3. 4. 5.]]\r\n\r\n# iterator: [0. 3.] do some math => x-xmean = [-1.5  1.5]\r\n# iterator: [1. 4.] do some math => x-xmean = [-1.5  1.5]\r\n# iterator: [2. 5.] do some math => x-xmean = [-1.5  1.5]\r\n\r\n# Result array:\r\n#  [[-1.5  1.5]\r\n#  [-1.5  1.5]\r\n#  [-1.5  1.5]]\r\n\r\n# =============================================\r\n# iterate C_CONTIGUOUS array row-wise\r\n# (order=F, tanspose+copy input, transpose output)\r\n# =============================================\r\n# Input array:\r\n#  [[0. 1. 2.]\r\n#  [3. 4. 5.]]\r\n\r\n# iterator: [0. 1. 2.] do some math => x-xmean = [-1.  0.  1.]\r\n# iterator: [3. 4. 5.] do some math => x-xmean = [-1.  0.  1.]\r\n\r\n# Result array:\r\n#  [[-1.  0.  1.]\r\n#  [-1.  0.  1.]]\r\n```\r\n\n\n### Idea or request for content:\n\nIdea\/Request: Extending \"Iterating Over Arrays\" Documentation\r\nhttps:\/\/numpy.org\/doc\/stable\/reference\/arrays.nditer.html\r\n\r\nAdd a subsection on using nditer for iterating over subarrays.\r\nProvide guidelines and code examples of using nditer in different scenarios i.e operating on multiple arrays in different dimensions.\r\nExtend the section on array iteration to include performance considerations, highlighting potential bottlenecks and optimizations.\r\nHow does the performance of nditer compare to Python or C iterations.\r\nInclude examples of i.e. Cython memory views and buffer access for efficient array iteration, along with strategies for minimizing Python overhead.\r\n\r\nRegards Oyibo","comments":["`nditer` is really a very raw API and more useful to understand the C-part in many ways.  It isn't meant to do what you want to do.  One can remove axes to exclude them from iteration, but again this is going to be very raw\/unsafe in Python.\r\n\r\nThere is `nested_nditer` or so, which might help.  Overall, I would suggest to just use `np.ndindex()` or a direct iteration and using normal indexing instead of `nditer`.","Hello @seberg ,\r\n\r\nThank you very much for your quick response regarding `nditer`.\r\n\r\nI have a follow-up question to make sure I got it right. In your message, you mentioned that nditer is more of a raw API useful for understanding the C-part. I was wondering if this advice is specific to subarray iteration or if it extends to general array iteration in Python. Should nditer be avoided where other iteration methods are available in Python (and C)?\r\n\r\n`nested_nditer` does not exist. You probably mean `nested_iters` which returns a tuple of `nditer`.\r\n\r\nI observed the current introduction to \"Iterating over Arrays\" in the NumPy documentation:\r\n\r\n> The iterator object nditer, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion. This page introduces some basic ways to use the object for computations on arrays in Python, then concludes with how one can accelerate the inner loop in Cython. Since the Python exposure of nditer is a relatively straightforward mapping of the C array iterator API, these ideas will also provide help working with array iteration from C or C++.\r\n\r\nAs there are currently 242 results when searching for \"nditer\" on Stack Overflow, it might be beneficial to include an explanatory note about the purpose of nditer in Python, something like:\r\n\r\n> nditer is intended for use in C-level code, with its exposure at the Python level serving as a convenience. It allows users to test ideas in Python code before transitioning them to C.\r\n\r\nOn a related note is there a complete code example available how to define a custom loop function for generalized universal functions for the C-API `PyUFunc_FromFuncAndDataAndSignature` using Cython? I was exloring if nditer could be useful here.\r\n\r\nThanks again for your time and effort.","If you use a ufunc, you don't need an iterator.  I am not sure if we have good examples for that, maybe just C ones.  If you use an iterator, you don't use a ufunc...  The mechanism for subarray iteration is `op_axes` in the `NpyIter`\/`nditer`, but using that from Python will need some hacks, `nested_iters()` might allow you to have slightly less hacks, but it is always hacky, because it is always raw C memory access effectively (sure wrapped into arrays, but not safely so).\r\n\r\nThere are other things why it has lots of limits, because the Python API in NumPy have safety features that will make a \"ufunc\" written in pure Python with it, probably not work correctly e.g. with broadcasting.\r\n\r\nYou can write things in Cython, Cython 3 even has some decorator for it (no idea if it works and how well it works).  But whichever way you go, the Cython code will probably look a lot like C anyway for now.","Hello @seeberg,\r\n\r\nThank you once again for sharing your thoughts.\r\n\r\nI believe there might be a slight miscommunication in our discussion, and I'd like to clarify a bit.\r\n\r\nMy goal is not to create universal functions (ufuncs) but rather generalized universal functions (gufuncs) or functions with similar capabilities. Specifically, I aim to implement these in Cython to generate C-code.\r\n\r\nIn the NumPy documentation under \"Iterating over arrays,\" there's an example using nditer for efficient array iteration. I experimented with nditer in Python to understand how it works and how to tune the parameters. I checked the possibility of utilizing the iterator to iterate not only element-wise but also over entire columns and rows by adjusting the memory order.\r\n\r\nAs you pointed out already, nditer or NpyIter might not be the optimal choices for this use case.\r\n\r\nAdditionally, I'm uncertain whether using nditer in Cython provides access to the C-interface. This is something I could find out from the Cython team. Nevertheless, I could use NpyIter instead if it would be more suitable.\r\n\r\nTo avoid any confusion, let me clarify some definitions:\r\n\r\nUfuncs perform element-wise operations with scalar arguments.\r\n```\r\n@ufunc(signature=\"(),()->()\")\r\ndef foo(a: float, b: float) -> float:\r\n    return a + b\r\n```\r\n\r\nGufuncs extend ufuncs to support more complex operations and custom broadcasting rules. The core dimensions can be scalars or ndarrays.\r\n```\r\n@gufunc(signature=\"(k?),(m?,n)->(k?,m?)\")\r\ndef npv(rate: float, values: np.ndarray[np.float64]) -> float:\r\n    res = 0.0\r\n    for timestep in range(values.shape[0]):\r\n        res += values[timestep] \/ ((1.0 + rate) ** timestep)\r\n    return res\r\n```\r\nWhile Cython provides a decorator for generating ufuncs, there is currently no tool for generating custom gufuncs. Numba, on the other hand, offers a \"vectorize\" decorator for ufuncs and a \"guvectorize\" decorator for gufuncs. However, there are limitations, such as the inability to use these functions in other jitted functions and the lack of support for NEP 20 signatures like \"(k?),(m?,n)->(k?,m?)\". It does not seem to be possible to compile these functions ahead of time. Numba has to become a dependency to your package.\r\n\r\nI haven't found a tool to assist in generating custom gufuncs. This led me to consider using Cython and the NumPy C-Interface, but I faced problems in defining the first two arguments of PyUFunc_FromFuncAndDataAndSignature (PyUFuncGenericFunction *func, void *const *data, ...) correctly. Tutorials with comprehensive examples for this process are not easy to find.\r\n\r\nIn conclusion, the alternative approach is to write functions entirely in Cython or C and explicitly implement the iteration logic. However, even a simple function signature like \r\n`(k?),(m?,n)->(k?,m?)`\r\n generates multiple variations, such as \r\n```\r\n['(),(n)->()',\r\n '(),(m,n)->(m)',\r\n '(k),(n)->(k)',\r\n '(k),(m,n)->(k,m)']\r\n```\r\nYou can easily miss cases or mix something up. That is a disadvantage compared to an automated process. At the end you want to write less amount of code while ensuring safety and performance.","OK, so you want C\/Cython and not `nditer`, so you are using `nditer` in the way I think it is mostly useful (unless you know it already, there are some Python use-cases, but all of them come with caveats pretty much).\r\n\r\nFor GUFuncs, I dunno.  TBH, while I wouldn't mind having Cython docs, I also don't think it is unreasonable to only have C ones.  Now we probably don't have C ones either really (maybe the NEP has some info), but you can find some being generated in our tests for example.\r\nAnother thing is that we have some new API upcoming, so that makes me want to look into it even less.\r\n\r\nHaving problems defining the first two arguments seems just like two small Cython hacks to figure out?\r\nCython might be annoying around `const`s?! What is your *actual* problem, you still didn't really say.  Maybe just define everything with `void *` in Cython and cast to that when passing it...\r\n\r\nFor `NpyIter` I still do *not* think you want to use it, but we just added the `NpyIter` API into the cython `__init__.pyd` file, you could copy it out if you wish to use it in Cython.  But, it is a very C-like API, so you need to take a lot of care with cleanup, etc.\r\n\r\nThe important point there, as I said, is `op_axes` which selects all but the core dimensions, using that from Python gives you a single element I think, but that is a pointer to the start of the subarray you want.\r\nThat is what NumPy uses internally to implement gufuncs, although casts, etc. are more clunky for them (and could be improved).","Hello @seberg ,\r\n\r\nthank you very much.\r\nI have found some resources including the one you shared that should help me to get over the hill.\r\n\r\nhttps:\/\/scipy-lectures.org\/advanced\/advanced_numpy\/#generalized-ufuncs\r\nhttps:\/\/github.com\/xnd-project\/gufunctools\/blob\/master\/modules\/examples\/src\/example_gufuncs.c\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/_core\/src\/umath\/_umath_tests.c.src\r\nhttps:\/\/github.com\/WarrenWeckesser\/ufunclab\/tree\/main\r\n\r\n> Another thing is that we have some new API upcoming, so that makes me want to look into it even less.\r\n\r\nRegarding the upcoming API changes, I understand your perspective on waiting for the new API.\r\nI'll keep an eye out for the release of Numpy 2.0 in the coming weeks\/months.\r\n\r\nOnce again, thank you for your help."],"labels":["04 - Documentation"]},{"title":"API: Return type of isin() for scalar inputs","body":"### Describe the issue:\r\n\r\nI would have assumed that for a scalar *element* input like `np.isin(1, [1, 2, 3])`, I'll get a simple scalar bool result. However, the result is a 0d bool array.\r\n\r\nThis expectation is in analogy to e.g. `np.exp(0)`, which also returns a simple float for a float input.\r\n\r\nPreferable for consistency would be a change to returning simple bools for scalar input. But that may raise compatibility issues. I'm unclear whether you could go for this (maybe as part of the 2.0 transition?). Alternatively, this behaviour should at least be documented in https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.isin.html#numpy.isin\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nnp.isin(1, [1, 2, 3])\r\n# returns a 0d array(True)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Python and NumPy Versions:\r\n\r\n3.11\r\n1.26.2\r\n\r\n### Runtime Environment:\r\n\r\n_No response_\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["Does it really matter? This isn't quite an element-wise operation, so basing the expectation on `np.exp` seems a little arbitrary to me. The first function I tried to compare it to is `unique`, which also always returns an array. This behavior isn't fully consistent across NumPy unfortunately, but it's certainly not a bug I'd say. And in principle 0-D arrays are much nicer for both consistency and for static typing.","FWIW, https:\/\/github.com\/numpy\/numpy\/pull\/24214 I tried to define a path for returning the scalar in Python (keep current typical behavior) iff the input was scalar.\r\nThere is a subclass annoyance to figure out, due to (maybe broken) `__array_wrap__` behavior.\r\n\r\nSee gh-24897 for the generic discussion.\r\n\r\nOverall, I think I am happy with just changing this to be more ufunc like (maybe with at least preserving 0-D arrays, because I really think we can pull that one off, while I am not sure you can pull of removing scalars).\r\n\r\nThat makes NumPy's different parts more consistent and the scalar prefering part is dominant right now.  And most importantly, I don't see such a change really standing in the way of removing scalars if someone could figure it out and spend the cycles:  The big disruption is ufuncs anyway, not the few outliners that return arrays.\r\n\r\nIn other words: i think returning scalars (for scalar inputs!) is the consistent thing, until a possible future where it stops being that.  Other functions like `isclose` have explicit tests for it.","> This isn't quite an element-wise operation, so basing the expectation on `np.exp` seems a little arbitrary to me.\r\n\r\nThis is an element-wise operation wrt to the first argument *element*. It becomes more obvious if you hide the test elements in a partial:\r\n\r\n```\r\n>>> isin_test_elements = partial(np.isin, test_elements=[1, 2, 3])\r\n>>> isin_test_elements(1)\r\narray(True)\r\n>>> isin_test_ements([1, 5])\r\narray([True, False])\r\n```\r\ncompare that to\r\n```\r\n>>> np.exp(0)\r\n1.0\r\n>>> np.exp([0, 1])\r\narray([1.        , 2.71828183])\r\n```\r\n\r\n> This behavior isn't fully consistent across NumPy unfortunately, but it's certainly not a bug I'd say.\r\n\r\nI agree it's technically not a bug. It's an API inconsistency. The Github interface made me choose between *bug report* and *feature request*, between those two, I went with bug. I've now changed the title to `API: `.\r\n"],"labels":["54 - Needs decision"]},{"title":"Discuss the issue of Numpy CI testing for LSX instructions","body":"https:\/\/github.com\/numpy\/numpy\/pull\/25215#issue-2004051415\r\n\r\nDear @rgommers @seiko2plus, about LSX instruction CI testing problem, I did some work.\r\nFirst, ubuntu does not yet have a suitable toolchain, so ci (https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/linux_qemu.yml) is not applicable now.\r\nSecond\uff0c I have verified through [LoongArch sbuildQEMU on debian community](https:\/\/wiki.debian.org\/LoongArch\/sbuildQEMU) .  I have stored gcc and qemu that need to be upgraded( supporting simd), [Click here](https:\/\/github.com\/loongson-zn\/qemu-debian)  have gcc and qemu binary,  rootfs can running on  x86. If you want to do it yourself, you can refer to [README.md](https:\/\/github.com\/loongson-zn\/qemu-debian\/blob\/master\/README.md)\r\nIf both of you have other ideas, I would be happy to help.\r\n\r\nParticularize:\r\n[gcc master branch](https:\/\/gcc.gnu.org\/git\/gcc.git) and [qemu master branch](https:\/\/github.com\/qemu\/qemu.git)  already supports the extension vector instruction of LoongArch(LSX).\r\n","comments":["If you have x86_64 debian os(maybe other system is ok) , simple methods:\r\n```\r\n$ git clone https:\/\/github.com\/loongson-zn\/qemu-debian.git\r\n$ cd qemu-debian\r\n$ cp qemu\/qemu-loongarch64-static \/usr\/bin\/qemu-loongarch64-static\r\n$ update-binfmts --import  qemu\/qemu-loongarch64 \r\n$ cd chroots\/sid-loong64-sbuild\/\r\n$ mkdir {proc, sys}\r\n$ mount -t proc proc proc\r\n$ mount -t sysfs sys sys\r\n$ mount -t devtmpfs dev dev \r\n$ mount -t devpts devpts dev\/pts \r\n$ mount -t tmpfs shmfs dev\/shm\r\n$ chroot .\r\n``` \r\nexport PATH=$PATH:\/root\/.local\/bin\r\nthen, start `spin test` , if have command not find , please set PATH\r\n","> First, ubuntu does not yet have a suitable toolchain, so ci (https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/linux_qemu.yml) is not applicable now\r\n\r\nIn that case, feel free to create an independent workflow limited to LoongArch based on Debian.\r\n\r\n> Second\uff0c I have verified through [LoongArch sbuildQEMU on debian community](https:\/\/wiki.debian.org\/LoongArch\/sbuildQEMU) ..... If both of you have other ideas, I would be happy to help.\r\nIf you have x86_64 debian os(maybe other system is ok) , simple methods:....\r\n\r\nI don't know exactly what is required of us, but it seems that you have sufficient tools to create a workflow for LongArch. ","> In that case, feel free to create an independent workflow limited to LoongArch based on Debian.\r\n\r\nYes, but I don't know how to submit the PR related to numpy CI testing. ","> I don't know exactly what is required of us, but it seems that you have sufficient tools to create a workflow for LongArch.\r\n\r\nIn other words, It's me provide you with CI testing methods. #25215 ","To successfully integrate CI testing for LoongArch64, the following components are essential:\r\n\r\n- [ ] **Cross-Compiler Packages for x86_64**: There's a need for cross-compiler packages compatible with `ubuntu:22.04`, specifically `gcc-loongarch64-linux-gnu`, `g++-loongarch64-linux-gnu`, and `gfortran-loongarch64-linux-gnu`.\r\n\r\n- [ ] **Hosted Docker Container**: A Docker container should be hosted, containing a Linux repository that provides binutils, compilers compatible with the cross-compiler mentioned above, and main Python packages. Note the required packages within `numpy\/build_requirements.txt` and `pytest pytest-xdist hypothesis typing_extensions` are required to run without any issues.\r\n\r\n\r\n- [ ] **Maintenance and Suppor**t: Although I am willing to initiate the implementation, it is imperative that either you or another party commits to its maintenance. This includes overseeing regular updates and managing support for any external packages that may be incorporated in the future.","I enjoyed doing it","> I enjoyed doing it\r\n\r\nSounds great!"],"labels":["05 - Testing","component: SIMD"]},{"title":"BUG: Using a Numpy array to advance index a Array API array succeeds","body":"### Describe the issue:\r\n\r\nUsing a Numpy array to (advance) index a Array API array does not lead to an exception. If instead you use convert `idx` to a Array API array before `X_xp[idx,:]` an exception is raised informing you that advanced integer indexing is not part of the Array API.\r\n\r\nI think some kind of exception should be raised in the case of the Numpy array as well. At least it seems weird for it to succeed with a numpy array but fail with an Array API array.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nimport numpy.array_api as xp\r\n\r\nX = np.arange(100).reshape((10, 10))\r\nX_xp = xp.asarray(X)\r\n# a numpy array for indexing\r\nidx = np.arange(4)\r\n\r\nX_xp[idx,:]\r\n```\r\n\r\n### Runtime information:\r\n\r\n```\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:26:08) '\r\n            '[Clang 14.0.6 ]',\r\n  'uname': uname_result(system='Darwin', node='thead-mlt', release='23.0.0', version='Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:43 PDT 2023; root:xnu-10002.1.13~1\/RELEASE_ARM64_T6000', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP', 'ASIMDDP'],\r\n                      'not_found': ['ASIMDFHM']}}]\r\nNone\r\n```\r\n","comments":["cc @ogrisel ","Hi everyone,\r\n\r\nI encountered a similar issue while working with the numpy.array_api submodule. Specifically, when using a NumPy array for indexing an Array API array, I noticed that it doesn't raise an exception, unlike when using an Array API array directly.\r\n\r\n# output: C:\\Users\\akava\\Desktop\\OpenSource\\hello.py:2: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\r\n**import numpy.array_api as xp**\r\n  \r\n  \r\nIt seems unexpected that this behavior differs between NumPy arrays and Array API arrays. Perhaps raising an exception or having consistent behavior in both scenarios could improve usability and prevent potential confusion for users moving between NumPy and the Array API.\r\n\r\nWould it be possible to align the behavior between NumPy arrays and Array API arrays in this context?\r\n\r\nThanks!"],"labels":["00 - Bug","component: numpy.array_api"]},{"title":"ENH: Add templates for f2py c functions and handle more C types","body":"Closes #25229.\r\n\r\nThis PR has commits from https:\/\/github.com\/numpy\/numpy\/pull\/25226, we'll wait till it gets merged.","comments":["@Pranavchiku now that #25226 is in, could you rebase this and address the test failures?","@Pranavchiku, the problem is essentially in the way the `c_double` is being mapped. It is now incorrectly mapped to `float` somewhere instead of the correct `double`. Here are the changes from the older file (working) to the this one:\r\n\r\n```diff\r\n\u276f diff meson_works\/codditymodule.c meson_broken\/codditymodule.c\r\n2c2\r\n<  * This file is auto-generated with f2py (version:2.0.0.dev0+git20231224.a43c068).\r\n---\r\n>  * This file is auto-generated with f2py (version:2.0.0.dev0+git20231222.94d21b0).\r\n5c5\r\n<  * Generation date: Sun Dec 24 18:19:50 2023\r\n---\r\n>  * Generation date: Sun Dec 24 18:24:33 2023\r\n140a141,152\r\n> static int\r\n> float_from_pyobj(float* v, PyObject *obj, const char *errmess)\r\n> {\r\n>     double d=0.0;\r\n>     if (double_from_pyobj(&d,obj,errmess)) {\r\n>         *v = (float)d;\r\n>         return 1;\r\n>     }\r\n>     return 0;\r\n> }\r\n> \r\n> \r\n170c182\r\n<                            void (*f2py_func)(double*,double*,double*)) {\r\n---\r\n>                            void (*f2py_func)(float*,float*,float*)) {\r\n175c187\r\n<     double a = 0;\r\n---\r\n>     float a = 0;\r\n177c189\r\n<     double b = 0;\r\n---\r\n>     float b = 0;\r\n179c191\r\n<     double c = 0;\r\n---\r\n>     float c = 0;\r\n192c204\r\n<         f2py_success = double_from_pyobj(&a,a_capi,\"coddity.coddity.c_add() 1st argument (a) can't be converted to double\");\r\n---\r\n>         f2py_success = float_from_pyobj(&a,a_capi,\"coddity.coddity.c_add() 1st argument (a) can't be converted to float\");\r\n195c207\r\n<         f2py_success = double_from_pyobj(&b,b_capi,\"coddity.coddity.c_add() 2nd argument (b) can't be converted to double\");\r\n---\r\n>         f2py_success = float_from_pyobj(&b,b_capi,\"coddity.coddity.c_add() 2nd argument (b) can't be converted to float\");\r\n214c226\r\n<         capi_buildvalue = Py_BuildValue(\"d\",c);\r\n---\r\n>         capi_buildvalue = Py_BuildValue(\"f\",c);\r\n298c310\r\n<     s = PyUnicode_FromString(\"2.0.0.dev0+git20231224.a43c068\");\r\n---\r\n>     s = PyUnicode_FromString(\"2.0.0.dev0+git20231222.94d21b0\");\r\n302c314\r\n<         \"This module 'coddity' is auto-generated with f2py (version:2.0.0.dev0+git20231224.a43c068).\\nFunctions:\\n\"\r\n---\r\n>         \"This module 'coddity' is auto-generated with f2py (version:2.0.0.dev0+git20231222.94d21b0).\\nFunctions:\\n\"\r\n306c318\r\n<     s = PyUnicode_FromString(\"2.0.0.dev0+git20231224.a43c068\");\r\n---\r\n>     s = PyUnicode_FromString(\"2.0.0.dev0+git20231222.94d21b0\");\r\n```\r\n\r\nFor this test case (extracted from the `isoCtests.f90` file):\r\n\r\n```f90\r\n  module coddity\r\n    use iso_c_binding, only: c_double, c_int, c_int64_t\r\n    implicit none\r\n    contains\r\n      subroutine c_add(a, b, c) bind(c, name=\"c_add\")\r\n        real(c_double), intent(in) :: a, b\r\n        real(c_double), intent(out) :: c\r\n        c = a + b\r\n      end subroutine c_add\r\n  end module coddity\r\n```\r\n\r\nThe steps to reproduce this (and other similar errors) is:\r\n```bash\r\nmicromamba activate numpy-dev\r\nspin run $SHELL\r\ncd gh-25229 # or wherever the file is\r\nf2py -m coddity -c coddity.f90 --build-dir blah --backend meson\r\n# Modify the files after inspection\r\n```\r\n\r\nAnother thing to do is to [use meld](https:\/\/meldmerge.org\/) to visually diff whole folders at once, between working and non-working variants.","Thank you @HaoZeke, I'll apply code review by tonight. ","Tests pass locally for me."],"labels":["01 - Enhancement","component: numpy.f2py"]},{"title":"TST: Fixture based tests for ``f2py``","body":"Closes #25239. Closes #25447. Draft until it is actually faster than `main`...\r\n\r\nCurrently isn't much faster (+\/- 10s or so).\r\n\r\n**Reviewer Aids**\r\nThis PR touches every one of the 900 F2PY tests. It removes the older `unittest` class style test structure in favor of a (better to cache) fixture based method. In doing so, there is a little bit of overhead (cognitively) however, the end result should be more robust than the earlier approach (which involved building a module and replacing it within a class).\r\n\r\nHere is the basic concept, highlighting the changes from the class based design.\r\n- The module is now returned from a specification (a special fixture which is scoped to the module)\r\n  + This is similar to the older F2PyTest class attributes\r\n- The compilers are only checked once per session (slight speedup)\r\n  - As opposed to the earlier per-module check\r\n- Instead of `self.module` a `_mod` parameter is passed which is parameterized by the `spec` returning fixture\r\n\r\nThis is possibly best seen with an example, here is a basic one:\r\n\r\n```python\r\n# Older\r\nclass TestStringAssumedLength(util.F2PyTest):\r\n    sources = [util.getpath(\"tests\", \"src\", \"string\", \"gh24008.f\")]\r\n\r\n    def test_gh24008(self):\r\n        self.module.greet(\"joe\", \"bob\")\r\n# New\r\n@pytest.fixture(scope=\"module\")\r\ndef string_assumed_length_spec():\r\n    spec = util.F2PyModuleSpec(\r\n        test_class_name=\"TestStringAssumedLength\",\r\n        sources=[util.getpath(\"tests\", \"src\", \"string\", \"gh24008.f\")],\r\n    )\r\n    return spec\r\n\r\n\r\n@pytest.mark.parametrize(\"_mod\", [\"string_assumed_length_spec\"], indirect=True)\r\ndef test_gh24008(_mod):\r\n    _mod.greet(\"joe\", \"bob\")\r\n```\r\n\r\nPerhaps this looks a bit unweildy, but it does make parameterizations over specifications much easier:\r\n\r\n```python\r\n# Old, repeat the test logic in each class\r\n@pytest.mark.slow\r\nclass TestNewCharHandling(util.F2PyTest):\r\n    # from v1.24 onwards, gh-19388\r\n    sources = [\r\n        util.getpath(\"tests\", \"src\", \"string\", \"gh25286.pyf\"),\r\n        util.getpath(\"tests\", \"src\", \"string\", \"gh25286.f90\")\r\n    ]\r\n    module_name = \"_char_handling_test\"\r\n\r\n    def test_gh25286(self):\r\n        info = self.module.charint('T')\r\n        assert info == 2\r\n\r\n@pytest.mark.slow\r\nclass TestBCCharHandling(util.F2PyTest):\r\n    # SciPy style, \"incorrect\" bindings with a hook\r\n    sources = [\r\n        util.getpath(\"tests\", \"src\", \"string\", \"gh25286_bc.pyf\"),\r\n        util.getpath(\"tests\", \"src\", \"string\", \"gh25286.f90\")\r\n    ]\r\n    module_name = \"_char_handling_test\"\r\n\r\n    def test_gh25286(self):\r\n        info = self.module.charint('T')\r\n        assert info == 2\r\n\r\n# New approach, just two specs to parameterize over\r\n@pytest.fixture(scope=\"module\")\r\ndef string_newchar_spec():\r\n    spec = util.F2PyModuleSpec(\r\n        test_class_name=\"TestNewCharHandling\",\r\n        # from v1.24 onwards, gh-19388\r\n        sources=[\r\n            util.getpath(\"tests\", \"src\", \"string\", \"gh25286.pyf\"),\r\n            util.getpath(\"tests\", \"src\", \"string\", \"gh25286.f90\"),\r\n        ],\r\n        module_name=\"_char_handling_test\",\r\n    )\r\n    return spec\r\n\r\n@pytest.fixture(scope=\"module\")\r\ndef string_bc_char_spec():\r\n    spec = util.F2PyModuleSpec(\r\n        test_class_name=\"TestBCCharHandling\",\r\n        # SciPy style, \"incorrect\" bindings with a hook\r\n        sources=[\r\n            util.getpath(\"tests\", \"src\", \"string\", \"gh25286_bc.pyf\"),\r\n            util.getpath(\"tests\", \"src\", \"string\", \"gh25286.f90\"),\r\n        ],\r\n        module_name=\"_char_handling_test\",\r\n    )\r\n    return spec\r\n\r\n@pytest.mark.parametrize(\r\n    \"_mod\", [\"string_newchar_spec\", \"string_bc_char_spec\"], indirect=True\r\n)\r\ndef test_gh25286(_mod):\r\n    info = _mod.charint(\"T\")\r\n    assert info == 2\r\n```\r\n\r\nAlso better caching. In general it seems [more recommended](https:\/\/docs.pytest.org\/en\/7.1.x\/how-to\/unittest.html#pytest-features-in-unittest-testcase-subclasses) to use fixtures and `pytest` parameterization anyway.\r\n\r\n**Notable incompatibilities**\r\n- Test names now **must be unique** as varying parameters are not enough for `pytest` to distinguish them\r\n\r\nAgain, an example helps:\r\n\r\n```python\r\n# Pseudocode, would collect 2 tests\r\nclass TestBlah:\r\n   def test_input(self):\r\n     pass\r\nclass TestBlahTwo\r\n   def test_input(self):\r\n     pass\r\n# New, will only get 1 test!!\r\n@pytest.mark.parametrize(\"_mod\", [\"specA\"], indirect=True)\r\ndef test_input(_mod):\r\n  pass\r\n\r\n@pytest.mark.parametrize(\"_mod\", [\"specB\"], indirect=True)\r\ndef test_input(_mod):\r\n  _mod.callable()\r\n  pass\r\n```\r\n\r\nThe tests should just be renamed, e.g. `test_input_spec_a` and `test_input_spec_b`.\r\n\r\n**Checklist**\r\nMainly to make sure the right number of tests are collected (e.g. via `spin test -t numpy.f2py -m full -- --collect-only -k \"test_value_attrspec\"`).\r\n\r\n- [x] (2) `test_abstract_interface`\r\n- [x] (581) `test_array_from_pyobj`\r\n- [x] (2) `test_assumed_shape`\r\n- [x] (1) `test_block_docstring`\r\n- [x] (17) `test_callback`\r\n- [x] (49) `test_character`\r\n- [x] (2) `test_common`\r\n- [x] (50) `test_crackfortran`\r\n- [x] (6) `test_data`\r\n- [x] (5) `test_docs`\r\n- [x] (2) `test_f2cmap`\r\n- [x] (52) `test_f2py2e`\r\n- [x] (5) `test_isoc`\r\n- [x] (3) `test_kind`\r\n- [x] (2) `test_mixed`\r\n- [x] (1) `test_module_doc`\r\n- [x] (12) `test_parameter`\r\n- [x] (1) `test_pyf_src`\r\n- [x] (1) `test_quoted_character`\r\n- [x] (6) `test_regression`\r\n- [x] (15) `test_return_character`\r\n- [x] (16) `test_return_complex`\r\n- [x] (20) `test_return_integer`\r\n- [x] (18) `test_return_logical`\r\n- [x] (20) `test_return_real`\r\n- [x] (2) `test_semicolon_split`\r\n- [x] (3) `test_size`\r\n- [x] (6) `test_string`\r\n- [x] (11) `test_symbolic`\r\n- [x] (1) `test_value_attrspec`\r\n\r\n","comments":["This is necessary, but won't be enough since the slowdown is inherently present in the new backend:\r\n\r\n```bash\r\n\u276f hyperfine \"f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend distutils\" \"f2py --verbose -c _bufrlib.pyf -m _bufrlib -L$PWD -lbvers --backend meson\"\r\nBenchmark 1: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend distutils\r\n  Time (mean \u00b1 \u03c3):     380.0 ms \u00b1 179.0 ms    [User: 531.0 ms, System: 941.1 ms]\r\n  Range (min \u2026 max):   219.0 ms \u2026 656.2 ms    10 runs\r\n \r\nBenchmark 2: f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend meson\r\n  Time (mean \u00b1 \u03c3):      1.512 s \u00b1  0.042 s    [User: 1.964 s, System: 2.480 s]\r\n  Range (min \u2026 max):    1.465 s \u2026  1.588 s    10 runs\r\n \r\nSummary\r\n  f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend distutils ran\r\n    3.98 \u00b1 1.88 times faster than f2py --verbose -c _bufrlib.pyf -m _bufrlib -L\/blah\/playground\/numpy_bugs\/gh-25266 -lbvers --backend meson\r\nhyperfine    25.10s user 34.36s system 309% cpu 19.216 total\r\n```","The main source of the slowdown is the multiple invocations of the `meson` build. One approach would be to manually collate test snippets so only one module is compiled and shared (this is the approach in parts of `test_character`).","I'm going to solicitate reviews now, since it is quite a large refactor... Maybe @rgommers (reported the speed and parallelism issues) and @mattip and @melissawm. Any other reviews are also welcome!\r\n\r\nAlso as a (slightly new) pair of eyes, @Pranavchiku.","I am not sure what kind of a review you are looking for, and the PR is still in draft. Is the final outcome any faster?","> I am not sure what kind of a review you are looking for, and the PR is still in draft. Is the final outcome any faster?\r\n\r\nIn my local tests the speed is roughly similar (perhaps a little better after this PR). However, the real reason to go through with this exercise is to consistently use fixtures to manage the state better rather than injecting compiled modules into the class data and trying to cache them by hand. It is expected that this solves the problem in #25447 as well.\r\n\r\nSpeed issues are tracked in #25415 and will be independent of this, though I can collate the test code more easily here in the new style to make the tests run faster (at the end).\r\n\r\nI'll finish this this week before requesting a more comprehensive review.","Collating the fortran code into single modules works very well, e.g. for `test_string` it goes from around 13-14 seconds on `main` to around 7.8-8 seconds here.","Need to add to the documentation, should leave information on the testing design for posterity."],"labels":["05 - Testing","component: numpy.f2py"]},{"title":"ENH: Enable native half-precision scalar conversion operations on ARM","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":[],"labels":["01 - Enhancement"]},{"title":"TST: Use fixtures for `f2py` tests","body":"The issues with the CI test timings (https:\/\/github.com\/numpy\/numpy\/pull\/25111#issuecomment-1824873678, https:\/\/github.com\/numpy\/numpy\/issues\/25134#issuecomment-1824074863, etc.) for `f2py` are part of the older `unittest` style test framework. The compiler checks can (and should) be implemented as `pytest` fixtures of `session` scope.\r\n\r\nIn keeping with the discussion on https:\/\/github.com\/numpy\/numpy\/pull\/25111#issuecomment-1824873678.","comments":["#25447 highlights the need for this, might as well get to it (speed notwithstanding)."],"labels":["05 - Testing","component: numpy.f2py","component: CI"]},{"title":"ENH: Add templates for `f2py` c functions and handle more C types","body":"Consider the following (after the corrected handling of `ISO_C` in #25226):\r\n\r\n```python\r\n   Ignoring map {'integer':{'c_short':'short int'}}: 'short int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_long':'long int'}}: 'long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_long_long':'long long int'}}: 'long long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_signed_char':'signed char'}}: 'signed char' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_size_t':'size_t'}}: 'size_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int8_t':'int8_t'}}: 'int8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int16_t':'int16_t'}}: 'int16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int32_t':'int32_t'}}: 'int32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int64_t':'int64_t'}}: 'int64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least8_t':'int_least8_t'}}: 'int_least8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least16_t':'int_least16_t'}}: 'int_least16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least32_t':'int_least32_t'}}: 'int_least32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least64_t':'int_least64_t'}}: 'int_least64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast8_t':'int_fast8_t'}}: 'int_fast8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast16_t':'int_fast16_t'}}: 'int_fast16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast32_t':'int_fast32_t'}}: 'int_fast32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast64_t':'int_fast64_t'}}: 'int_fast64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_intmax_t':'intmax_t'}}: 'intmax_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_intptr_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_ptrdiff_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tMapping \"real(kind=c_float)\" to \"float\"\r\n\tMapping \"real(kind=c_double)\" to \"double\"\r\n\tIgnoring map {'real':{'c_long_double':'long double'}}: 'long double' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_float_complex':'float _Complex'}}: 'float _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_double_complex':'double _Complex'}}: 'double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_long_double_complex':'long double _Complex'}}: 'long double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'logical':{'c_bool':'_Bool'}}: '_Bool' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tMapping \"character(kind=c_char)\" to \"char\"\r\n```\r\n\r\nWhich can be seen by passing `verbose = True` to `process_f2cmap_dict` while mapping the `iso_c` values in `capi_maps.py`. To silence these, there are the (commented out) entries in `iso_c2py_map` in `_isocbind.py`.\r\n\r\nThe reason they are commented out is, otherwise, F2PY will internally construct and try to use \"construction\" functions like these:\r\n\r\n```C\r\nneeds['short_from_pyobj'] = ['int_from_pyobj']\r\ncfuncs['short_from_pyobj'] = \"\"\"\r\nstatic int\r\nshort_from_pyobj(short* v, PyObject *obj, const char *errmess) {\r\n    int i = 0;\r\n    if (int_from_pyobj(&i, obj, errmess)) {\r\n        *v = (short)i;\r\n        return 1;\r\n    }\r\n    return 0;\r\n}\r\n```\r\n\r\nWhich are defined in `cfuncs.py`. Now most of these (almost all) follow a pattern, they just cast and call `int_from_pyobj` or similar. However, for the `iso_c` (and really in general for the remaining C types) there should be similar functions. \r\n\r\n## Solution\r\n\r\nThis could be copy pasted, but really they should be generated on the fly out of templates like the `meson` backend and its `meson.build` backend. That would also finally allow for syntax highlighting, currently modifying the `cfuncs.py` C code is rather painful.\r\n\r\n\r\n### Addenum\r\n\r\nFor #25226 the mapping of types isn't really required. Here are the changes needed to `_isocbind.py` to reproduce the issue:\r\n\r\n```python\r\niso_c_binding_map = {\r\n    'integer': {\r\n        'c_int': 'int',\r\n        'c_short': 'short int',\r\n        'c_long': 'long int',\r\n        'c_long_long': 'long long int',\r\n        'c_signed_char': 'signed char',\r\n        'c_size_t': 'size_t',\r\n        'c_int8_t': 'int8_t',\r\n        'c_int16_t': 'int16_t',\r\n        'c_int32_t': 'int32_t',\r\n        'c_int64_t': 'int64_t',\r\n        'c_int_least8_t': 'int_least8_t',\r\n        'c_int_least16_t': 'int_least16_t',\r\n        'c_int_least32_t': 'int_least32_t',\r\n        'c_int_least64_t': 'int_least64_t',\r\n        'c_int_fast8_t': 'int_fast8_t',\r\n        'c_int_fast16_t': 'int_fast16_t',\r\n        'c_int_fast32_t': 'int_fast32_t',\r\n        'c_int_fast64_t': 'int_fast64_t',\r\n        'c_intmax_t': 'intmax_t',\r\n        'c_intptr_t': 'intptr_t',\r\n        'c_ptrdiff_t': 'intptr_t',\r\n    },\r\n    'real': {\r\n        'c_float': 'float',\r\n        'c_double': 'double',\r\n        'c_long_double': 'long double'\r\n    },\r\n    'complex': {\r\n        'c_float_complex': 'float _Complex',\r\n        'c_double_complex': 'double _Complex',\r\n        'c_long_double_complex': 'long double _Complex'\r\n    },\r\n    'logical': {\r\n        'c_bool': '_Bool'\r\n    },\r\n    'character': {\r\n        'c_char': 'char'\r\n    }\r\n}\r\n\r\n# TODO: At some point these should be included, but then they'd need special\r\n# handling in cfuncs.py e.g. needs[int64_t_from_pyobj] These are not very hard\r\n# to add, since they all derive from the base `int_from_pyobj`, e.g. the way\r\n# `short_from_pyobj` and others do\r\n\r\nisoc_c2pycode_map = {\r\n    'int': 'i',  # int\r\n    'short int': 'h',  # short int\r\n    'long': 'l',  # long int\r\n    'long long': 'q',  # long long int\r\n    'signed char': 'b',  # signed char\r\n    'size_t': 'I',  # size_t (approx unsigned int)\r\n    'int8_t': 'b',  # int8_t\r\n    'int16_t': 'h',  # int16_t\r\n    'int32_t': 'i',  # int32_t\r\n    'int64_t': 'q',  # int64_t\r\n    'int_least8_t': 'b',  # int_least8_t\r\n    'int_least16_t': 'h',  # int_least16_t\r\n    'int_least32_t': 'i',  # int_least32_t\r\n    'int_least64_t': 'q',  # int_least64_t\r\n    'int_fast8_t': 'b',  # int_fast8_t\r\n    'int_fast16_t': 'h',  # int_fast16_t\r\n    'int_fast32_t': 'i',  # int_fast32_t\r\n    'int_fast64_t': 'q',  # int_fast64_t\r\n    'intmax_t': 'q',  # intmax_t (approx long long)\r\n    'intptr_t': 'q',  # intptr_t (approx long long)\r\n    'ptrdiff_t': 'q',  # intptr_t (approx long long)\r\n    'float': 'f',  # float\r\n    'double': 'd',  # double\r\n    'long double': 'g',  # long double\r\n    'float _Complex': 'F',  # float  _Complex\r\n    'double _Complex': 'D',  # double  _Complex\r\n    'long double _Complex': 'D',  # very approximate complex\r\n    '_Bool': 'i',  #  Bool but not really\r\n    'char': 'c',   # char\r\n}\r\n\r\niso_c2py_map = {\r\n    'int': 'int',\r\n    'short int': 'int',                 # forced casting\r\n    'long': 'int',\r\n    'long long': 'long',\r\n    'signed char': 'int',           # forced casting\r\n    'size_t': 'int',                # approx Python int\r\n    'int8_t': 'int',                # forced casting\r\n    'int16_t': 'int',               # forced casting\r\n    'int32_t': 'int',\r\n    'int64_t': 'long',\r\n    'int_least8_t': 'int',          # forced casting\r\n    'int_least16_t': 'int',         # forced casting\r\n    'int_least32_t': 'int',\r\n    'int_least64_t': 'long',\r\n    'int_fast8_t': 'int',           # forced casting\r\n    'int_fast16_t': 'int',          # forced casting\r\n    'int_fast32_t': 'int',\r\n    'int_fast64_t': 'long',\r\n    'intmax_t': 'long',\r\n    'intptr_t': 'long',\r\n    'ptrdiff_t': 'long',\r\n    'float': 'float',\r\n    'double': 'float',              # forced casting\r\n    'long double': 'float',         # forced casting\r\n    'float _Complex': 'complex',     # forced casting\r\n    'double _Complex': 'complex',\r\n    'long double _Complex': 'complex', # forced casting\r\n    '_Bool': 'bool',\r\n    'char': 'bytes',                  # approx Python bytes\r\n}\r\n\r\n```","comments":["Note that essentially this is a bug, because the ignored types, if used, (they're not common) would throw an error in the generated C code \/ map to an incorrect type.","I am able to reproduce it after checking out #25226 and applying diff:\r\n\r\n```diff\r\ndiff --git a\/numpy\/f2py\/capi_maps.py b\/numpy\/f2py\/capi_maps.py\r\nindex fa477a5b9..c12657670 100644\r\n--- a\/numpy\/f2py\/capi_maps.py\r\n+++ b\/numpy\/f2py\/capi_maps.py\r\n@@ -129,7 +129,7 @@\r\n # Add ISO_C handling\r\n c2pycode_map.update(isoc_c2pycode_map)\r\n c2py_map.update(iso_c2py_map)\r\n-f2cmap_all, _ = process_f2cmap_dict(f2cmap_all, iso_c_binding_map, c2py_map)\r\n+f2cmap_all, _ = process_f2cmap_dict(f2cmap_all, iso_c_binding_map, c2py_map, True)\r\n # End ISO_C handling\r\n f2cmap_default = copy.deepcopy(f2cmap_all)\r\n ```\r\n\r\n```console\r\n% spin test numpy\/f2py\/tests\/test_isoc.py -v -- --pdb -s\r\nInvoking `build` prior to running tests:\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 vendored-meson\/meson\/meson.py compile -C build\r\nINFO: autodetecting backend as ninja\r\nINFO: calculating backend command to run: \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/ninja -C \/Users\/pranavchiku\/repos\/numpy\/build\r\nninja: Entering directory `\/Users\/pranavchiku\/repos\/numpy\/build'\r\n[1\/1] Generating numpy\/generate-version with a custom command\r\nSaving version to numpy\/version.py\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 vendored-meson\/meson\/meson.py install --only-changed -C build --destdir ..\/build-install\r\n$ export PYTHONPATH=\"\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\"\r\n$ export PYTHONPATH=\"\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\"\r\n$ cd \/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 -m pytest --rootdir=\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages -v -m 'not slow' numpy\/f2py\/tests\/test_isoc.py --pdb -s\r\n======================================== test session starts ========================================\r\nplatform darwin -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9\r\ncachedir: .pytest_cache\r\nhypothesis profile 'np.test() profile' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=[HealthCheck.data_too_large, HealthCheck.filter_too_much, HealthCheck.too_slow, HealthCheck.large_base_example, HealthCheck.function_scoped_fixture, HealthCheck.differing_executors]\r\nrootdir: \/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\r\nconfigfile: ..\/..\/..\/..\/..\/pytest.ini\r\nplugins: cov-4.1.0, xdist-3.5.0, hypothesis-6.90.0\r\ncollecting ... \tMapping \"integer(kind=c_int)\" to \"int\"\r\n\tIgnoring map {'integer':{'c_short':'short int'}}: 'short int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_long':'long int'}}: 'long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_long_long':'long long int'}}: 'long long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_signed_char':'signed char'}}: 'signed char' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_size_t':'size_t'}}: 'size_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int8_t':'int8_t'}}: 'int8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int16_t':'int16_t'}}: 'int16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int32_t':'int32_t'}}: 'int32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int64_t':'int64_t'}}: 'int64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least8_t':'int_least8_t'}}: 'int_least8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least16_t':'int_least16_t'}}: 'int_least16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least32_t':'int_least32_t'}}: 'int_least32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_least64_t':'int_least64_t'}}: 'int_least64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast8_t':'int_fast8_t'}}: 'int_fast8_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast16_t':'int_fast16_t'}}: 'int_fast16_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast32_t':'int_fast32_t'}}: 'int_fast32_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_int_fast64_t':'int_fast64_t'}}: 'int_fast64_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_intmax_t':'intmax_t'}}: 'intmax_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_intptr_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'integer':{'c_ptrdiff_t':'intptr_t'}}: 'intptr_t' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tMapping \"real(kind=c_float)\" to \"float\"\r\n\tMapping \"real(kind=c_double)\" to \"double\"\r\n\tIgnoring map {'real':{'c_long_double':'long double'}}: 'long double' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_float_complex':'float _Complex'}}: 'float _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_double_complex':'double _Complex'}}: 'double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'complex':{'c_long_double_complex':'long double _Complex'}}: 'long double _Complex' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tIgnoring map {'logical':{'c_bool':'_Bool'}}: '_Bool' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character']\r\n\tMapping \"character(kind=c_char)\" to \"char\"\r\ncollected 4 items                                                                                   \r\n\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_c_double PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_function PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_kinds PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::test_process_f2cmap_dict PASSED\r\n\r\n========================================= 4 passed in 1.81s =========================================\r\n```","Okay, after adding:\r\n```diff\r\ndiff --git a\/numpy\/f2py\/cfuncs.py b\/numpy\/f2py\/cfuncs.py\r\nindex 6c0dcb8a4..5b0289d95 100644\r\n--- a\/numpy\/f2py\/cfuncs.py\r\n+++ b\/numpy\/f2py\/cfuncs.py\r\n@@ -865,6 +865,19 @@\r\n }\r\n \"\"\"\r\n \r\n+needs['int64_t_from_pyobj'] = ['int_from_pyobj']\r\n+cfuncs['int64_t_from_pyobj'] = \"\"\"\r\n+static int\r\n+int64_t_from_pyobj(int64_t* v, PyObject *obj, const char *errmess) {\r\n+    int i = 0;\r\n+    if (int_from_pyobj(&i, obj, errmess)) {\r\n+        *v = (int64_t)i;\r\n+        return 1;\r\n+    }\r\n+    return 0;\r\n+}\r\n+\"\"\"\r\n+\r\n```\r\n\r\nWhen I run:\r\n\r\n```console\r\n% spin test numpy\/f2py\/tests\/test_isoc.py -v -- --pdb -s\r\n[...]\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_c_double PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_function PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_kinds Fatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00000001e282d300 (most recent call first):\r\n  File \"\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\/numpy\/f2py\/tests\/test_isoc.py\", line 23 in test_bindc_kinds\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/python.py\", line 194 in pytest_pyfunc_call\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/python.py\", line 1792 in runtest\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 169 in pytest_runtest_call\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 262 in <lambda>\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 341 in from_call\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 261 in call_runtest_hook\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 222 in call_and_report\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 133 in runtestprotocol\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/runner.py\", line 114 in pytest_runtest_protocol\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/main.py\", line 350 in pytest_runtestloop\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/main.py\", line 325 in _main\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/main.py\", line 271 in wrap_session\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/main.py\", line 318 in pytest_cmdline_main\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_callers.py\", line 77 in _multicall\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_manager.py\", line 115 in _hookexec\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pluggy\/_hooks.py\", line 493 in __call__\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/config\/__init__.py\", line 169 in main\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/_pytest\/config\/__init__.py\", line 192 in console_main\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/site-packages\/pytest\/__main__.py\", line 5 in <module>\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/runpy.py\", line 87 in _run_code\r\n  File \"\/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/lib\/python3.9\/runpy.py\", line 197 in _run_module_as_main\r\nzsh: segmentation fault  spin test numpy\/f2py\/tests\/test_isoc.py -v -- --pdb -s\r\n```","Interesting, @Pranavchiku, you'll need to debug it a bit more out of tree, you can use `f2py -c --backend meson --build-dir blah -m fadd` to get the meson skeleton and the files, modifying and debugging the `.c` file should help track this down.","This is weird, I am trying but found no reason of this happening. ","> This is weird, I am trying but found no reason of this happening.\r\n\r\nI've updated the underlying PR (for a forced casting approach), but also updated the issue since this is the correct approach. We can get a reproducer via a simple test subroutine:\r\n\r\n```fortran\r\nmodule coddity\r\n  use iso_c_binding, only: c_double, c_int, c_int64_t\r\n  implicit none\r\n  contains\r\n    subroutine c_add_int64(a, b, c) bind(c)\r\n      integer(c_int64_t), intent(in) :: a, b\r\n      integer(c_int64_t), intent(out) :: c\r\n      c = a + b\r\n    end subroutine c_add_int64\r\nend module coddity\r\n```\r\n\r\nWhich works with the forced casting in the PR, but can be used to reproduce the error seen in the tests:\r\n\r\n```bash\r\nf2py -c add.f90 -m fadd --backend meson --build-dir blah --debug-capi\r\ncd blah\r\nmeson compile -C bbdir\r\nipython -c \"from bbdir import fadd; print(fadd.coddity.c_add_int64(1,2))\"\r\n[1]    2625628 segmentation fault (core dumped)  ipython -c \"from bbdir import fadd; print(fadd.coddity.c_add_int64(1,2))\"\r\n```\r\n\r\nWhich is a good reproduction. We can get a closer look as well:\r\n\r\n```bash\r\ngdb --args python -c \"from bbdir import fadd; print(fadd.coddity.c_add_int64(1,2))\"\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\ndebug-capi:Python C\/API function fadd.c_add_int64(a,b)\r\ndebug-capi:int64_t a=:input,required,scalar\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\ndo_mkvalue (flags=<optimized out>, p_va=<optimized out>, \r\n    p_format=<optimized out>)\r\n    at \/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Python\/modsupport.c:481\r\n481\t\/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Python\/modsupport.c: No such file or directory.\r\n(gdb) bt\r\n#0  do_mkvalue (flags=<optimized out>, p_va=<optimized out>, \r\n    p_format=<optimized out>)\r\n    at \/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Python\/modsupport.c:481\r\n#1  va_build_value (format=<optimized out>, va=va@entry=0x7fffffffabc0, \r\n    flags=flags@entry=1)\r\n    at \/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Python\/modsupport.c:562\r\n#2  0x000055555568691e in _Py_BuildValue_SizeT (\r\n    format=format@entry=0x7ffff7733037 \"O\")\r\n    at \/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Python\/modsupport.c:530\r\n#3  0x00007ffff77305e1 in f2py_rout_fadd_coddity_c_add_int64 (\r\n    capi_self=<optimized out>, capi_args=<optimized out>, \r\n    capi_keywds=<optimized out>, \r\n    f2py_func=0x7ffff7730110 <coddity::c_add_int64>) at ..\/faddmodule.c:250\r\n#4  0x0000555555699ba6 in _PyObject_MakeTpCall (tstate=0x555555947f40, \r\n    callable=callable@entry=0x7ffff7b4c570, args=args@entry=0x7ffff7bb9580, \r\n    nargs=<optimized out>, keywords=<optimized out>, keywords@entry=0x0)\r\n    at \/home\/conda\/feedstock_root\/build_artifacts\/python-split_1606376621496\/work\/Objects\/call.c:191\r\n```\r\n\r\nSo it seems like the problem is with the building of output values at the moment. The relevant part of the `faddmodule.c` is:\r\n\r\n```C\r\n        CFUNCSMESS(\"Building return value.\\n\");\r\n        capi_buildvalue = Py_BuildValue(\"O\",c);\r\n```\r\n\r\nThe fix is to use `long_long`'s return type, that is:\r\n\r\n```C\r\n        CFUNCSMESS(\"Building return value.\\n\");\r\n        capi_buildvalue = Py_BuildValue(\"L\",c);\r\n```\r\n\r\nWith this change:\r\n\r\n```bash\r\npython -c \"from bbdir import fadd; print(fadd.coddity.c_add_int64(1,2))\"\r\ndebug-capi:Python C\/API function fadd.c_add_int64(a,b)\r\ndebug-capi:int64_t a=:input,required,scalar\r\n#vardebugshowvalue#\r\ndebug-capi:int64_t b=:input,required,scalar\r\n#vardebugshowvalue#\r\ndebug-capi:int64_t c=:output,hidden,scalar\r\n#vardebugshowvalue#\r\ndebug-capi:Fortran subroutine `c_add_int64(&a,&b,&c)'\r\n#vardebugshowvalue#\r\n#vardebugshowvalue#\r\n#vardebugshowvalue#\r\ndebug-capi:Building return value.\r\ndebug-capi:Python C\/API function fadd.c_add_int64: successful.\r\ndebug-capi:Freeing memory.\r\n3\r\n```\r\n\r\nSo the next task is to make sure `int64_t` maps to `L`.","Also, @Pranavchiku, I now believe the map should be to `long_long`, not `int`, see https:\/\/github.com\/numpy\/numpy\/pull\/25226. :)","Damn, nice. I'll try to make progress over it. Thank you for resolving the blocker :)","yes, I got it working with following diff:\r\n\r\n```diff\r\ndiff --git a\/numpy\/f2py\/auxfuncs.py b\/numpy\/f2py\/auxfuncs.py\r\nindex 13a1074b4..2565b036f 100644\r\n--- a\/numpy\/f2py\/auxfuncs.py\r\n+++ b\/numpy\/f2py\/auxfuncs.py\r\n@@ -974,9 +974,9 @@ def process_f2cmap_dict(f2cmap_all, new_map, c2py_map, verbose = False):\r\n                         \"\\tWarning: redefinition of {'%s':{'%s':'%s'->'%s'}}\\n\"\r\n                         % (k, k1, f2cmap_all[k][k1], v1)\r\n                     )\r\n-                f2cmap_all[k][k1] = v1\r\n+                f2cmap_all[k][k1] = c2py_map[v1]\r\n                 if verbose:\r\n-                    outmess('\\tMapping \"%s(kind=%s)\" to \"%s\"\\n' % (k, k1, v1))\r\n+                    outmess('\\tMapping \"%s(kind=%s)\" to \"%s\"\\n' % (k, k1, c2py_map[v1]))\r\n                 f2cmap_mapped.append(v1)\r\n             else:\r\n                 if verbose:\r\n```\r\n\r\nStatus of warnings:\r\n\r\n```console\r\ncollecting ... \tMapping \"integer(kind=c_int)\" to \"int\"\r\n\tMapping \"integer(kind=c_short)\" to \"int\"\r\n\tIgnoring map {'integer':{'c_long':'long int'}}: 'long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character', 'short int', 'long long', 'signed char', 'size_t', 'int8_t', 'int16_t', 'int32_t', 'int64_t', 'int_least8_t', 'int_least16_t', 'int_least32_t', 'int_least64_t', 'int_fast8_t', 'int_fast16_t', 'int_fast32_t', 'int_fast64_t', 'intmax_t', 'intptr_t', 'ptrdiff_t', 'long double', 'float _Complex', 'double _Complex', 'long double _Complex', '_Bool']\r\n\tIgnoring map {'integer':{'c_long_long':'long long int'}}: 'long long int' must be in ['double', 'float', 'long_double', 'char', 'signed_char', 'unsigned_char', 'short', 'unsigned_short', 'int', 'long', 'long_long', 'unsigned', 'complex_float', 'complex_double', 'complex_long_double', 'string', 'character', 'short int', 'long long', 'signed char', 'size_t', 'int8_t', 'int16_t', 'int32_t', 'int64_t', 'int_least8_t', 'int_least16_t', 'int_least32_t', 'int_least64_t', 'int_fast8_t', 'int_fast16_t', 'int_fast32_t', 'int_fast64_t', 'intmax_t', 'intptr_t', 'ptrdiff_t', 'long double', 'float _Complex', 'double _Complex', 'long double _Complex', '_Bool']\r\n\tMapping \"integer(kind=c_signed_char)\" to \"int\"\r\n\tMapping \"integer(kind=c_size_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_int_least8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_int_fast8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_intmax_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_intptr_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_ptrdiff_t)\" to \"long\"\r\n\tMapping \"real(kind=c_float)\" to \"float\"\r\n\tMapping \"real(kind=c_double)\" to \"float\"\r\n\tMapping \"real(kind=c_long_double)\" to \"float\"\r\n\tMapping \"complex(kind=c_float_complex)\" to \"complex\"\r\n\tMapping \"complex(kind=c_double_complex)\" to \"complex\"\r\n\tMapping \"complex(kind=c_long_double_complex)\" to \"complex\"\r\n\tMapping \"logical(kind=c_bool)\" to \"bool\"\r\n\tMapping \"character(kind=c_char)\" to \"bytes\"\r\ncollected 5 items \/ 1 deselected \/ 4 selected                                                         \r\n\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_function PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_kinds PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_add_arr PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::test_process_f2cmap_dict PASSED\r\n\r\n```","This is what I get\r\n\r\n```console\r\n% spin test numpy\/f2py\/tests\/test_isoc.py -v -- --pdb -s\r\nInvoking `build` prior to running tests:\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 vendored-meson\/meson\/meson.py compile -C build\r\nINFO: autodetecting backend as ninja\r\nINFO: calculating backend command to run: \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/ninja -C \/Users\/pranavchiku\/repos\/numpy\/build\r\nninja: Entering directory `\/Users\/pranavchiku\/repos\/numpy\/build'\r\n[1\/1] Generating numpy\/generate-version with a custom command\r\nSaving version to numpy\/version.py\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 vendored-meson\/meson\/meson.py install --only-changed -C build --destdir ..\/build-install\r\n$ export PYTHONPATH=\"\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\"\r\n$ export PYTHONPATH=\"\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\"\r\n$ cd \/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\r\n$ \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9 -m pytest --rootdir=\/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages -v -m 'not slow' numpy\/f2py\/tests\/test_isoc.py --pdb -s\r\n========================================= test session starts =========================================\r\nplatform darwin -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- \/Users\/pranavchiku\/mambaforge\/envs\/numpy-dev\/bin\/python3.9\r\ncachedir: .pytest_cache\r\nhypothesis profile 'np.test() profile' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=[HealthCheck.data_too_large, HealthCheck.filter_too_much, HealthCheck.too_slow, HealthCheck.large_base_example, HealthCheck.function_scoped_fixture, HealthCheck.differing_executors]\r\nrootdir: \/Users\/pranavchiku\/repos\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\r\nconfigfile: ..\/..\/..\/..\/..\/pytest.ini\r\nplugins: cov-4.1.0, xdist-3.5.0, hypothesis-6.90.0\r\ncollecting ... \tMapping \"integer(kind=c_int)\" to \"int\"\r\n\tMapping \"integer(kind=c_short)\" to \"int\"\r\n\tMapping \"integer(kind=c_long)\" to \"int\"\r\n\tMapping \"integer(kind=c_long_long)\" to \"long\"\r\n\tMapping \"integer(kind=c_signed_char)\" to \"int\"\r\n\tMapping \"integer(kind=c_size_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_int_least8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_least64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_int_fast8_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast16_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast32_t)\" to \"int\"\r\n\tMapping \"integer(kind=c_int_fast64_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_intmax_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_intptr_t)\" to \"long\"\r\n\tMapping \"integer(kind=c_ptrdiff_t)\" to \"long\"\r\n\tMapping \"real(kind=c_float)\" to \"float\"\r\n\tMapping \"real(kind=c_double)\" to \"float\"\r\n\tMapping \"real(kind=c_long_double)\" to \"float\"\r\n\tMapping \"complex(kind=c_float_complex)\" to \"complex\"\r\n\tMapping \"complex(kind=c_double_complex)\" to \"complex\"\r\n\tMapping \"complex(kind=c_long_double_complex)\" to \"complex\"\r\n\tMapping \"logical(kind=c_bool)\" to \"bool\"\r\n\tMapping \"character(kind=c_char)\" to \"bytes\"\r\ncollected 5 items \/ 1 deselected \/ 4 selected                                                         \r\n\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_function PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_kinds PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::TestISOC::test_bindc_add_arr PASSED\r\nnumpy\/f2py\/tests\/test_isoc.py::test_process_f2cmap_dict PASSED\r\n\r\n=================================== 4 passed, 1 deselected in 2.09s ===================================\r\n```\r\n\r\nWith diff: https:\/\/gist.github.com\/Pranavchiku\/8685118f44a09d5450a153afed879e6b","Excellent work @Pranavchiku, one thing stood out in the diff though (perhaps a draft PR would be easier to discuss them):\r\n- You might need additions to `cfuncs` for the remaining types mapped (currently its just `int64_t`)","> You might need additions to cfuncs for the remaining types mapped (currently its just int64_t)\r\n\r\nI was actually wondering that but, the ignore warnings are not there now so I thought it will be fine. I opened a draft PR, let's continue discussion there.","Awesome, might be worthwhile to parameterize tests over the new c types :)\n\n\nOn 11\/27\/23 5:21 PM, Pranav ***@***.***> wrote:\n>     You might need additions to cfuncs for the remaining types mapped\n>     (currently its just int64_t)\n> \n> I was actually wondering that but, the ignore warnings are not there now \n> so I thought it will be fine. I opened a draft PR, let's continue \n> discussion there.\n> \n> \u2014\n> Reply to this email directly, view it on GitHub \n> <https:\/\/github.com\/numpy\/numpy\/issues\/25229#issuecomment-1828288720>, \n> or unsubscribe \n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABBCUT3W422BLIGN4H46ERDYGTDYZAVCNFSM6AAAAAA7W4CDG2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQMRYGI4DQNZSGA>.\n> You are receiving this because you authored the thread.Message ID: \n> ***@***.***>\n> \n> \n"],"labels":["00 - Bug","01 - Enhancement","component: numpy.f2py"]},{"title":"TYPING: false positives in string comparison between string arrays or literals","body":"### Describe the issue:\n\nTyping thinks `np.array(\"\")` cannot be compared to `\"\"`\n\n### Reproduce the code example:\n\n```python\nfrom typing import reveal_type\r\n\r\nimport numpy as np\r\nimport numpy.typing as npt\r\n\r\nX: npt.NDArray[np.str_] = np.array(\"\")\r\nreveal_type(X)\r\nprint(X > \"\")\r\nprint(X > X)\n```\n\n\n### Error message:\n\n```shell\n(.venv) C:\\Build\\project>pyright bug.py\r\nC:\\Build\\project\\bug.py\r\n  C:\\Build\\project\\bug.py:7:13 - information: Type of \"X\" is \"ndarray[Any, dtype[str_]]\"\r\n  C:\\Build\\project\\bug.py:8:7 - error: Operator \">\" not supported for types \"NDArray[Any]\" and \"Literal['']\" (reportGeneralTypeIssues)\r\n  C:\\Build\\project\\bug.py:9:7 - error: Operator \">\" not supported for types \"NDArray[Any]\" and \"NDArray[Any]\" (reportGeneralTypeIssues)\r\n2 errors, 0 warnings, 1 information\r\n\r\n(.venv) C:\\Build\\project>mypy bug.py\r\nbug.py:7: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[numpy.str_]]\"\r\nbug.py:8: error: Unsupported operand types for > (\"ndarray[Any, dtype[str_]]\" and \"str\")  [operator]\r\nbug.py:8: note: Following member(s) of \"str\" have conflicts:\r\nbug.py:8: note:     Expected:\r\nbug.py:8: note:         def __contains__(self, object, \/) -> bool\r\nbug.py:8: note:     Got:\r\nbug.py:8: note:         def __contains__(self, str, \/) -> bool\r\nbug.py:8: note:     Expected:\r\nbug.py:8: note:         def __getitem__(self, int, \/) -> _SupportsArray[dtype[object_]] | _NestedSequence[_SupportsArray[dtype[object_]]]\r\nbug.py:8: note:     Got:\r\nbug.py:8: note:         def __getitem__(self, SupportsIndex | slice, \/) -> str\r\nbug.py:8: note:     <2 more conflict(s) not shown>\r\nbug.py:8: note:     Expected:\r\nbug.py:8: note:         def __contains__(self, object, \/) -> bool\r\nbug.py:8: note:     Got:\r\nbug.py:8: note:         def __contains__(self, str, \/) -> bool\r\nbug.py:8: note:     Expected:\r\nbug.py:8: note:         def __getitem__(self, int, \/) -> _SupportsArray[dtype[object_]] | _NestedSequence[_SupportsArray[dtype[object_]]]\r\nbug.py:8: note:     Got:\r\nbug.py:8: note:         def __getitem__(self, SupportsIndex | slice, \/) -> str\r\nbug.py:8: note: See https:\/\/mypy.rtfd.io\/en\/stable\/_refs.html#code-operator for more info\r\nbug.py:9: error: Unsupported operand types for > (\"ndarray[Any, dtype[str_]]\" and \"ndarray[Any, dtype[str_]]\")  [operator]\r\nFound 2 errors in 1 file (checked 1 source file)\n```\n\n\n### Runtime information:\n\n1.26.2\r\n3.12.0 (tags\/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n\n### Context for the issue:\n\n_No response_","comments":[],"labels":["00 - Bug","Static typing"]},{"title":"ENH: Add LSX optimization for LoongArch","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\nAdd LSX optimization for LoongArch.\r\nFirst, Maximum performance improvement of 42.3x (python3.9 runtest.py --bench);\r\nSecond, I have completed the functional testing on V1.25.2 \\ V1.26.2 and main branch.\r\nV1.25.2 : 'python3.9 runtest.py -v  -m full',  python3.9 and  gcc 8.3  `35023 passed, 443 skipped, 31 xfailed, 5 xpassed`;  \r\n'spin test -v ' `33801 passed, 395 skipped, 1303 deselected, 30 xfailed, 4 xpassed `\r\n\r\nV1.26.2 :  'python3.9 runtest.py -v  -m full',  python3.9 and gcc 8.3 `35142 passed, 357 skipped, 30 xfailed, 4 xpassed`\r\n'spin test -v ' `34020 passed, 176 skipped, 1303 deselected, 30 xfailed, 4 xpassed `\r\n\r\nmain branch: 'spin test -v' ,  python3.9 and gcc 10.3 `39359 passed, 177 skipped, 1303 deselected, 34 xfailed, 6 xpassed in 450.49s `\r\n\r\n","comments":["@loongson-zn thank you for this PR. At first sight, the test\/benchmark results look good and the code is clean. My first question is regarding testing, since we don't have access to this hardware. We test other architectures for which we don't have regular hosted CI available through QEMU (see https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/linux_qemu.yml). Is there QEMU support for loongarch, and if so can you add a CI job in that `linux_qemu.yml` file?","> @loongson-zn thank you for this PR. At first sight, the test\/benchmark results look good and the code is clean. My first question is regarding testing, since we don't have access to this hardware. We test other architectures for which we don't have regular hosted CI available through QEMU (see https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/linux_qemu.yml). Is there QEMU support for loongarch, and if so can you add a CI job in that `linux_qemu.yml` file?\r\n\r\nLoongArch is promoting the Debian community, and ubuntu does not yet have a suitable toolchain, so ci is not applicable( ex: apt install gcc-xx). I am verifying with QEMU, but I have encountered some trouble. Please give me some time to debug.","@rgommers  I have  [x86_64 crosscompile toolchain](https:\/\/github.com\/loongson\/build-tools\/releases\/download\/2023.08.08\/x86_64-cross-tools-loongarch64-gcc-libc.tar.xz) and  enabling native execution [via binfmt](https:\/\/github.com\/loongson\/build-tools\/releases\/download\/2023.08.08\/qemu-loongarch64), but  I encountered issues related to Python. Perhaps you can provide me with some help\r\n","> but I encountered issues related to Python. Perhaps you can provide me with some help\r\n\r\nI can give you a hand, what kind of issues are you facing? maybe it would be better to open a separate pull request that adds a CI test for LoongArch.","> > but I encountered issues related to Python. Perhaps you can provide me with some help\r\n> \r\n> I can give you a hand, what kind of issues are you facing? maybe it would be better to open a separate pull request that adds a CI test for LoongArch.\r\n\r\nThanks @seiko2plus, I will try my best to provide a CI testing environment within a week and start a new issue."],"labels":["01 - Enhancement","component: SIMD"]},{"title":"Pydantic v2 and numpy.typing conflict when using npt.DtypeLike in pydantic.BaseModel with python <=3.11 ","body":"### Describe the issue:\r\n\r\nWhen I try to use npt.DTypeLike in a pydantic BaseModel, I get an error from pydantic that won't be fixed by pydantic\r\n\r\n```\r\nPlease use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\r\n\r\nFor further information visit https:\/\/errors.pydantic.dev\/2.5\/u\/typed-dict-version\r\n  File \"\/opt\/workspace\/src\/sedonaai\/tasks\/ml_model_schemas.py\", line 46, in <module>\r\n    class TensorInfo(BaseModel):\r\npydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\r\n\r\nFor further information visit https:\/\/errors.pydantic.dev\/2.5\/u\/typed-dict-version\r\n```\r\nthis issue was first reported here:\r\nhttps:\/\/github.com\/pydantic\/pydantic\/issues\/6645\r\n\r\nand this is where TypedDict is used from the typing module: https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/_typing\/_dtype_like.py#L8\r\n\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nfrom pydantic import BaseModel, Field, FilePath, field_validator\r\nfrom typing import List, Tuple, Dict, Optional, Literal, Union\r\nimport numpy.typing as npt\r\n\r\nclass TensorInfo(BaseModel):\r\n    dtype: npt.DTypeLike = Field(...)\r\n    shape: Union[Tuple[int, ...], List[int]] = Field(...)\r\n\r\n    class Config:\r\n        arbitrary_types_allowed = True\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nPlease use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\r\n\r\nFor further information visit https:\/\/errors.pydantic.dev\/2.5\/u\/typed-dict-version\r\n  File \"\/opt\/workspace\/src\/sedonaai\/tasks\/ml_model_schemas.py\", line 46, in <module>\r\n    class TensorInfo(BaseModel):\r\npydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\r\n\r\nFor further information visit https:\/\/errors.pydantic.dev\/2.5\/u\/typed-dict-version\r\n```\r\n\r\n\r\n\r\n### Runtime information:\r\n\r\n```\r\n1.26.2\r\n3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\n```\r\n\r\n```\r\n[{'numpy_version': '1.26.2',\r\n  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',\r\n  'uname': uname_result(system='Linux', node='0591b98d73a9', release='6.2.0-36-generic', version='#37~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  9 15:34:04 UTC 2', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\n\r\n```\r\n\r\n### Context for the issue:\r\n\r\nI'm trying to define pydantic models for a metadata standard for computer vision models that operate on satellite images. validating numpy types would be very useful so that users of the standard can easily check if their model inputs match the correct numpy type and dimension shape. Supporting this functionality with numpy.typing is desirable across python versions and so I don't need to reinvent the wheel on how to serialize and deserialize numpy types. ","comments":["Thanks for the report @rbavery. We're already relying on `typing_extensions`, so I think we can make this change in principle.\r\n\r\nIt can't really be considered a numpy bug of course, all we're doing is `from typing import TypedDict`. It's quite aggressive from Pydantic to raise an error for that.","I put up a PR to address the imports, am I missing anything besides changing the imports?\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/pull\/25352"],"labels":["Static typing"]},{"title":"DOC: Explain distribution for `f2py` with `meson-python`","body":"Some older issues (e.g. #19441, https:\/\/github.com\/numpy\/numpy\/issues\/22211#issuecomment-1724289771) and parts of the documentation very briefly cover distribution of extensions with F2PY sources. It seems (anecdotally) that telling people to go look at SciPy hasn't been super helpful so, a proper documentation section would be nice.","comments":["I'm commenting here but happy to open a new issue\/PR, depending on how the decision is to go forward. \r\n\r\nWhen building Fortran extension modules, one might only want to use `f2py` to generate the C sources (see also https:\/\/github.com\/numpy\/numpy\/issues\/22211#issuecomment-1724289771) but do the compiling with the build system. In addition, custom `*.pyf` signature files can be used to only expose the routines required. Consider the following Fortran file defining two subroutines `ADD` and `HELLO`, of which we only want to expose `ADD` (see the *.pyf):\r\n\r\n```fortran\r\nC    FILE ADD.F\r\nC\r\n      SUBROUTINE ADD(A,B,C)\r\nC\r\nCF2PY INTENT(OUT) :: C\r\nCF2PY INTENT(IN) :: A\r\nCF2PY INTENT(IN) :: B\r\nCF2PY DOUBLE PRECISION :: A\r\nCF2PY DOUBLE PRECISION :: B\r\nCF2PY DOUBLE PRECISION :: C\r\n      DOUBLE PRECISION A\r\n      DOUBLE PRECISION B\r\n      DOUBLE PRECISION C\r\n      C = A + B\r\n      END\r\nC\r\n      SUBROUTINE HELLO()\r\nC\r\n      PRINT*,\"Hello from fortran\"\r\n      END\r\n```\r\n```fortran\r\n!  File add.pyf\r\n!    -*- f90 -*-\r\n! Note: the context of this file is case sensitive.\r\n\r\npython module add ! in \r\n    interface  ! in :add\r\n        subroutine add(a,b,c) ! in :add:add.f\r\n            double precision intent(in) :: a\r\n            double precision intent(in) :: b\r\n            double precision intent(out) :: c\r\n        end subroutine add\r\n    end interface \r\nend python module add\r\n\r\n! This file was auto-generated with f2py (version:1.26.1).\r\n! See:\r\n! https:\/\/web.archive.org\/web\/20140822061353\/http:\/\/cens.ioc.ee\/projects\/f2py2e\r\n```\r\nGenerating the C sources as follows works as expected:\r\n```bash\r\nf2py add.pyf\r\n```\r\nHowever, generating the C sources as follows:\r\n```bash\r\nf2py -m add add.pyf add.f --lower\r\n```\r\nalso adds the interface to `hello()` to `addmodule.c`.\r\n\r\nI see this being fixed **one of two ways**:\r\n- properly explain this usage and the caveats in the [documentation](https:\/\/numpy.org\/doc\/stable\/f2py\/usage.html#extension-module-construction) \r\n- check for the presence of *.pyf file(s) and deal with this transparently, similar to `f2py -c`.\r\nSee here in the `run_compile()` method:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/7a9ab6115bb6a9bf952e5ededec7210bd679ec1e\/numpy\/f2py\/f2py2e.py#L682-L688\r\nThe check would have to be moved out of the `run_compile()` method into a separate method, and called from `main()` appropriately:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/7a9ab6115bb6a9bf952e5ededec7210bd679ec1e\/numpy\/f2py\/f2py2e.py#L737-L740\r\n\r\nI would appreciate comments on how to deal with this and can submit a PR for either scenario."],"labels":["04 - Documentation","component: numpy.f2py"]},{"title":"index assignment using masked boolean array unexpected behaviour","body":"### Describe the issue:\n\nassigning using a masked boolean array as index does not work as expected, as it may overwrite target elems for masked index elems.. \r\n\r\nI know we can use 'filled', but you need to be aware of the issue and this should not be necessary.\r\n\r\nif backward compatibility is a concern, perhaps this could be fixed for numpy 2?\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nimport numpy.ma as ma\r\n\r\narr = np.ones((2,2))\r\nmask = np.array([[False, False], [False, True]])\r\n\r\nmarr = ma.MaskedArray(arr, mask, dtype=bool)\r\n\r\nprint(marr)\r\narr[marr] = 2\r\n\r\nprint(arr) # all 2's but there should be one 1\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n>>> import sys, numpy; print(numpy.__version__); print(sys.version) \r\n1.24.2\r\n3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]\r\n>>> print(numpy.show_runtime())\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libopenblasp-r0.3.21.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21'}]\r\nNone\r\n>>> \r\n\n\n### Context for the issue:\n\nthis now causes subtle issues without any warning. ","comments":["Can I work on this issue?","For a plain `ndarray` the mask gets ignored - `ndarray` does not know anything about `MaskedArray`.\r\n\r\nIn principle, I think for `MaskedArray` itself, the mask should not be ignored (but it is):\r\n```\r\narr = np.ma.ones((2,2))\r\nmask = np.ma.MaskedArray(np.ones((2,2), dtype=bool), mask=[[False, False], [False, True]])\r\narr[mask] = 2\r\narr\r\nmasked_array(\r\n  data=[[2., 2.],\r\n        [2., 2.]],\r\n  mask=False,\r\n  fill_value=1e+20)\r\n```\r\nIn this example, it might be more logical for the mask to be or'd with the mask of the boolean array.\r\n\r\nBut for `ndarray`, I don't really see how one can change the current logic (except by error'ing).","Okay","> ### Describe the issue:\r\n> \r\n> assigning using a masked boolean array as index does not work as expected, as it may overwrite target elems for masked index elems..\r\n> \r\n> I know we can use 'filled', but you need to be aware of the issue and this should not be necessary.\r\n> \r\n> if backward compatibility is a concern, perhaps this could be fixed for numpy 2?\r\n> ### Reproduce the code example:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> import numpy.ma as ma\r\n> \r\n> arr = np.ones((2,2))\r\n> mask = np.array([[False, False], [False, True]])\r\n> \r\n> marr = ma.MaskedArray(arr, mask, dtype=bool)\r\n> \r\n> print(marr)\r\n> arr[marr] = 2\r\n> \r\n> print(arr) # all 2's but there should be one 1\r\n> ```\r\n> \r\n> ### Error message:\r\n> \r\n> _No response_\r\n> ### Runtime information:\r\n> \r\n> > > > import sys, numpy; print(numpy.**version**); print(sys.version)\r\n> > > > 1.24.2\r\n> > > > 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]\r\n> > > > print(numpy.show_runtime())\r\n> > > > [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n> > > > 'found': ['SSSE3',\r\n> > > > 'SSE41',\r\n> > > > 'POPCNT',\r\n> > > > 'SSE42',\r\n> > > > 'AVX',\r\n> > > > 'F16C',\r\n> > > > 'FMA3',\r\n> > > > 'AVX2'],\r\n> > > > 'not_found': ['AVX512F',\r\n> > > > 'AVX512CD',\r\n> > > > 'AVX512_SKX',\r\n> > > > 'AVX512_CLX',\r\n> > > > 'AVX512_CNL',\r\n> > > > 'AVX512_ICL']}},\r\n> > > > {'architecture': 'Haswell',\r\n> > > > 'filepath': '\/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libopenblasp-r0.3.21.so',\r\n> > > > 'internal_api': 'openblas',\r\n> > > > 'num_threads': 12,\r\n> > > > 'prefix': 'libopenblas',\r\n> > > > 'threading_layer': 'pthreads',\r\n> > > > 'user_api': 'blas',\r\n> > > > 'version': '0.3.21'}]\r\n> > > > None\r\n> \r\n> ### Context for the issue:\r\n> \r\n> this now causes subtle issues without any warning.\r\n\r\nI tried above code in Numpy 1.26.3 . As you wrote  \"print(arr) # all 2's but there should be one 1\" is reproducable."],"labels":["00 - Bug"]},{"title":"OpenBSD\/amd64 7.4 undefined symbols during the import","body":"### Steps to reproduce:\n\nin the python's repl\r\nimport numpy\n\n### Error message:\n\n```shell\nPython 3.10.13 (main, Oct  5 2023, 16:21:31) [Clang 13.0.0 ] on openbsd7\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sswap_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'scopy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'izamax_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zaxpy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'isamax_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'snrm2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssyr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sger_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsyr2k_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zdscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zgerc_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zgeru_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dgemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dcopy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztrsm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zgemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zcopy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zswap_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zgemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ccopy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dswap_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ddot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'daxpy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'idamax_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctrsm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cherk_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cgemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtbsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dgemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dger_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sgemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'saxpy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztpsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'csscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cher_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'drot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cscal_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cswap_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chpr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'icamax_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztrmm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zher_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhpr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zdotc_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sdot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctrmm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sspmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zdrot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'caxpy_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctbsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztrmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cdotc_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cgemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cgerc_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctrmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'strmm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cher2k_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dzasum_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cher2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dznrm2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zdotu_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cgeru_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtrmm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsyrk_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'strsm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dnrm2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sgemm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'srot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtrmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsyr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssyr2k_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssyrk_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctpsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dspmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'stpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'stpsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssymv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sspr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtrsm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'scasum_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtrsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'scnrm2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dasum_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sspr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zher2k_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsymv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtpsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zgbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssyr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsyr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'strmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhpr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztbsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'csrot_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhemv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ztrsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ctrsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cdotu_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsymm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chpmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dspr_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'ssymm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'srotm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sasum_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zherk_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'chpr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'drotm_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zher2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dspr2_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dgbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'zhbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'stbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'stbsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'strsv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dtbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'dsbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'cgbmv_'\r\npython:\/usr\/local\/lib\/liblapack.so.7.1: undefined symbol 'sgbmv_'\r\npython:\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/_umath_linalg.cpython-310.so: undefined symbol 'scopy_'\r\npython:\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/_umath_linalg.cpython-310.so: undefined symbol 'dcopy_'\r\npython:\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/_umath_linalg.cpython-310.so: undefined symbol 'ccopy_'\r\npython:\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/_umath_linalg.cpython-310.so: undefined symbol 'zcopy_'\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/__init__.py\", line 149, in <module>\r\n    from . import lib\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/lib\/__init__.py\", line 23, in <module>\r\n    from . import index_tricks\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/lib\/index_tricks.py\", line 12, in <module>\r\n    import numpy.matrixlib as matrixlib\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/matrixlib\/__init__.py\", line 4, in <module>\r\n    from . import defmatrix\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/matrixlib\/defmatrix.py\", line 12, in <module>\r\n    from numpy.linalg import matrix_power\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/__init__.py\", line 73, in <module>\r\n    from . import linalg\r\n  File \"\/home\/david\/.local\/lib\/python3.10\/site-packages\/numpy\/linalg\/linalg.py\", line 35, in <module>\r\n    from numpy.linalg import _umath_linalg\r\nImportError: Cannot load specified object\n```\n\n\n### Additional information:\n\n_No response_","comments":["How did you install NumPy and the liblapack there?  Which NumPy version is this?\r\n\r\nThere is something wrong with your linking and liblapack, although if you build from scratch numpy should maybe have build a bad fallback version instead of this happening.","@seberg \r\nI use the pkg_add lapack and pip install numpy.\r\nThe numpy's version is 1.26.2.\r\n\r\nAnd if I remove the lapack, it seems works. ","I'm not sure we can do much about this unless we see a full build log. We have a FreeBSD CI job, which should cover the basics here - so the problem may be OpenBSD-specific."],"labels":["32 - Installation"]},{"title":"MAINT: Unify `run_main()` and `run_compile()`","body":"Currently `f2py` has two separate paths within `f2py2e` which do \"almost the same thing\". `run_main` is supposed to do everything but `-c` and `run_compile()` is supposed to compile to a module. There is a lot of technical debt being carried here, `run_compile()` shouldn't be doing anything different from `run_main` in the first place. This also leads to subtle bugs since the results of `-c` and regular `f2py` runs are often different. It also makes it hard to test the CLI well (at the very least, doubles the number of possible tests).","comments":["#24552 is related, but only tangentially. Bug for bug compatibility is great, and new features are great too but this is a matter of testing. For example #25114 is difficult to test because technically the behavior should be the same with or without `-c` but it really only shows up as a bug with `-c` (to be fair it is still a bug without it but unlikely to happen in practice).","This would for example, allow better understanding \/ isolation of #2547 and #13553.","I guess this comment https:\/\/github.com\/numpy\/numpy\/issues\/25199#issuecomment-1829544967 is pertinent here as well. I have been looking at that part of the code in detail in the context of #25263. I am happy to contribute to this if we have general agreement on how to proceed, but I also found the distinction between `run_main()` and `run_compile()` clunky, especially since the latter calls the former.","> I guess this comment [#25199 (comment)](https:\/\/github.com\/numpy\/numpy\/issues\/25199#issuecomment-1829544967) is pertinent here as well. I have been looking at that part of the code in detail in the context of #25263. I am happy to contribute to this if we have general agreement on how to proceed, but I also found the distinction between `run_main()` and `run_compile()` clunky, especially since the latter calls the former.\r\n\r\n@jmrohwer, I would really appreciate help streamlining this file. The problem is that there's global state populated in `f2py2e` which makes it rather hit and miss to refactor easily.\r\n\r\nNote that (problematically) `run_main` is only called by `run_compile` for the `meson` backend, the `distutils` backend used to handle `f2py` specific build files internally. For the `meson` backend, a choice was made to make sure that the same code path is taken, which is still a source of bugs.\r\n\r\nHowever, the goal would be to align them logically, that is:\r\n\r\n- `run_compile` shouldn't do anything `run_main` doesn't basically\r\n\r\nI think for starters that would involve reducing `run_compile` to just calling `build_backend` or even removing it completely, the \"right way\" would be to parse all the arguments using `argparse` and then dispatch from there instead of the `scaninputline` stuff which is a legacy holdover from pre-`argparse`.\r\n\r\nWhat were your thoughts about this?","@HaoZeke Thanks for getting back to me, I'm keen to be involved. I certainly don't have the bandwidth to refactor the whole CLI in terms of `argparse` and as I understood it the decision in #24552 was that this be done incrementally with any new command-line arguments being moved to `argparse`.\r\n\r\nMy main gripe with the current CLI is inconsistent behaviour (see also https:\/\/github.com\/numpy\/numpy\/issues\/25199#issuecomment-1829544967), specifically when using a custom edited *.pyf file to only expose some of the Fortran routines to Python. Consider `mymodule.f` that defines two subroutines `f1()` and `f2()`, and `mymodule.pyf` that only exposes the interface to `f1` because we don't want to expose `f2` in Python. (The original `mymodule.pyf` could have been generated with `f2py -h` but custom edited afterwards according to needs).\r\n\r\n1. `f2py -m mymodule mymodule.pyf mymodule.f` will generate wrappers and C-code for `f1` AND `f2` (this was counter-intuitive to me).\r\n2. In contrast `f2py mymodule.pyf` will generate wrappers and C-code for `f1` only.\r\n3. Again, in contrast (and contrary to 1. above), `f2py -c mymodule.pyf mymodule.f` will create wrappers and compile the module **exposing `f1` only**!\r\n\r\nA user may want to use `f2py` to generate C-code and wrappers only, and compile using their own build system (this is how I stumbled upon these inconsistencies). \r\n\r\nA further point is that I see in the code that in `run_compile` the call to `run_main` is **only done for the meson backend**, not for distutils. I am not sure why this is and have not looked at the distutils backend in detail.  How long will the distutils backend be supported and would any changes implemented here also have to be cross-checked against the distutils backend?\r\n\r\nBefore proceeding, we would need to agree and define exactly what is the **expected behaviour** for\r\n- `f2py -m mymodule mymodule.pyf mymodule.f`\r\n- `f2py -c mymodule.pyf mymodule.f`\r\n\r\nin a situation as described.\r\n\r\n**EDIT:** I see the behaviour has changed again after https:\/\/github.com\/numpy\/numpy\/pull\/25267. The behaviour described above was before this commit.\r\n\r\n- `f2py mymodule.pyf mymodule.f` now produces two C files: `mymodulemodule.c` (wraps only `f1`) and `untitledmodule.c` (wraps `f1` and `f2`).\r\n- `f2py mymodule.pyf` produces the same two files, with `mymodulemodule.c` being the same as above but `untitledmodule.c` being smaller and containing no wrappers for either `f1` or `f2`\r\n- `f2py -m mymodule mymodule.pyf mymodule.f` produces `mymodulemodule.c` that wraps both `f1` and `f2`\r\n- `f2py -c mymodule.pyf mymodule.f` compiles correctly but again produces two files during the build process (`mymodulemodule.c` and `untitledmodule.c`, seems the latter is unused?)\r\n\r\nThis is confusing to say the least!","> @HaoZeke Thanks for getting back to me, I'm keen to be involved. I certainly don't have the bandwidth to refactor the whole CLI in terms of `argparse` and as I understood it the decision in #24552 was that this be done incrementally with any new command-line arguments being moved to `argparse`.\r\n\r\nYup, that was the guideline for new flags (e.g. `meson` ones) but this has to do with different intermediates generated by `distutils` as a backend (which doesn't call `run_main`) and `meson` as a backend (which does)..\r\n\r\n> My main gripe with the current CLI is inconsistent behaviour (see also [#25199 (comment)](https:\/\/github.com\/numpy\/numpy\/issues\/25199#issuecomment-1829544967)), specifically when using a custom edited *.pyf file to only expose some of the Fortran routines to Python. Consider `mymodule.f` that defines two subroutines `f1()` and `f2()`, and `mymodule.pyf` that only exposes the interface to `f1` because we don't want to expose `f2` in Python. (The original `mymodule.pyf` could have been generated with `f2py -h` but custom edited afterwards according to needs).\r\n> \r\n>     1. `f2py -m mymodule mymodule.pyf mymodule.f` will generate wrappers and C-code for `f1` AND `f2` (this was counter-intuitive to me).\r\n> \r\n>     2. In contrast `f2py mymodule.pyf` will generate wrappers and C-code for `f1` only.\r\n> \r\n>     3. Again, in contrast (and contrary to 1. above), `f2py -c mymodule.pyf mymodule.f` will create wrappers and compile the module **exposing `f1` only**!\r\n> \r\n> \r\n> A user may want to use `f2py` to generate C-code and wrappers only, and compile using their own build system (this is how I stumbled upon these inconsistencies).\r\n\r\nThis is also what the `meson` backend tries to do (which is why it calls `run_main`).\r\n\r\n\r\n> \r\n> A further point is that I see in the code that in `run_compile` the call to `run_main` is **only done for the meson backend**, not for distutils. I am not sure why this is and have not looked at the distutils backend in detail. How long will the distutils backend be supported and would any changes implemented here also have to be cross-checked against the distutils backend?\r\n\r\n\r\nIt is difficult to say, technically until the minimum Python version supported in `numpy` becomes `3.12` we'd need to support it, but it is a major hindrance. `distutils` does quite a bit of special handling for `-c` which even includes different wrappers being generated even back to NumPy `1.19` as seen in https:\/\/github.com\/numpy\/numpy\/pull\/25287#issuecomment-1835986872.\r\n \r\n> Before proceeding, we would need to agree and define exactly what is the **expected behaviour** for\r\n> \r\n>     * `f2py -m mymodule mymodule.pyf mymodule.f`\r\n> \r\n>     * `f2py -c mymodule.pyf mymodule.f`\r\n> \r\n> \r\n> in a situation as described.\r\n> \r\n> **EDIT:** I see the behaviour has changed again after #25267. The behaviour described above was before this commit.\r\n> \r\n>     * `f2py mymodule.pyf mymodule.f` now produces two C files: `mymodulemodule.c` (wraps only `f1`) and `untitledmodule.c` (wraps `f1` and `f2`).\r\n> \r\n>     * `f2py mymodule.pyf` produces the same two files, with `mymodulemodule.c` being the same as above but `untitledmodule.c` being smaller and containing no wrappers for either `f1` or `f2`\r\n> \r\n>     * `f2py -m mymodule mymodule.pyf mymodule.f` produces `mymodulemodule.c` that wraps both `f1` and `f2`\r\n> \r\n>     * `f2py -c mymodule.pyf mymodule.f` compiles correctly but again produces two files during the build process (`mymodulemodule.c` and `untitledmodule.c`, seems the latter is unused?)\r\n> \r\n> \r\n> This is confusing to say the least!\r\n\r\nThe intent in #25267 was to ensure `untitledmodule.c` is never made (I can't reproduce that locally), but something was missing there anyway, working on it over in #25287.\r\n\r\nI think for:\r\n* `f2py -m mymodule mymodule.pyf mymodule.f`\r\n\r\nThis should ignore `mymodule` unless it is the module name defined in the `.pyf` file\r\n\r\n* `f2py -c mymodule.pyf mymodule.f`\r\n\r\nThis should behave the same as the previous, i.e. take the `.pyf` file as the reference.\r\n\r\nHowever, unfortunately something in #25267 broke `scipy` (quite badly) and this shouldn't be happening. Once that's done I assume much of these problems will fine.","At this point I feel like there's little else to do but bite the bullet and ensure `run_main` and `run_compile` generate the same intermediate files, even if it takes a lot of effort. As soon as I get a pragmatic fix to `scipy` into `main` I'll start with it.","Another data point:\r\nThe root cause of this is that `-m` and `-c` produce different module wrapper files #25179, and the `meson` backend relies on them being the same. When `distutils` was the backend, the difference between the working `-c` and the non-working `-m` is, for `v1.19.5`:\r\n\r\n```diff\r\n5c5\r\n<  * Generation date: Fri Dec  1 11:42:08 2023\r\n---\r\n>  * Generation date: Fri Dec  1 11:38:27 2023\r\n23c23\r\n< typedef signed char signed_char;\r\n---\r\n> \/*need_typedefs*\/\r\n26c26\r\n< typedef double(*cb_fun_in___user__routines_typedef)(int *);\r\n---\r\n> typedef double(*cb_fun_in_foo__user__routines_typedef)(int *);\r\n47,53d46\r\n< #define GETSCALARFROMPYTUPLE(tuple,index,var,ctype,mess) {\\\r\n<         if ((capi_tmp = PyTuple_GetItem((tuple),(index)))==NULL) goto capi_fail;\\\r\n<         if (!(ctype ## _from_pyobj((var),capi_tmp,mess)))\\\r\n<             goto capi_fail;\\\r\n<     }\r\n< \r\n< #define pyobj_from_int1(v) (PyInt_FromLong(v))\r\n107,145d99\r\n< static int double_from_pyobj(double* v,PyObject *obj,const char *errmess) {\r\n<     PyObject* tmp = NULL;\r\n<     if (PyFloat_Check(obj)) {\r\n< #ifdef __sgi\r\n<         *v = PyFloat_AsDouble(obj);\r\n< #else\r\n<         *v = PyFloat_AS_DOUBLE(obj);\r\n< #endif\r\n<         return 1;\r\n<     }\r\n<     tmp = PyNumber_Float(obj);\r\n<     if (tmp) {\r\n< #ifdef __sgi\r\n<         *v = PyFloat_AsDouble(tmp);\r\n< #else\r\n<         *v = PyFloat_AS_DOUBLE(tmp);\r\n< #endif\r\n<         Py_DECREF(tmp);\r\n<         return 1;\r\n<     }\r\n<     if (PyComplex_Check(obj))\r\n<         tmp = PyObject_GetAttrString(obj,\"real\");\r\n<     else if (PyString_Check(obj) || PyUnicode_Check(obj))\r\n<         \/*pass*\/;\r\n<     else if (PySequence_Check(obj))\r\n<         tmp = PySequence_GetItem(obj,0);\r\n<     if (tmp) {\r\n<         PyErr_Clear();\r\n<         if (double_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}\r\n<         Py_DECREF(tmp);\r\n<     }\r\n<     {\r\n<         PyObject* err = PyErr_Occurred();\r\n<         if (err==NULL) err = callback2_error;\r\n<         PyErr_SetString(err,errmess);\r\n<     }\r\n<     return 0;\r\n< }\r\n< \r\n262c216\r\n< extern void F_FUNC(foo,FOO)(cb_fun_in___user__routines_typedef,double*);\r\n---\r\n> extern void F_FUNC(foo,FOO)(cb_fun_in_foo__user__routines_typedef,double*);\r\n270,277c224,231\r\n< \/************************* cb_fun_in___user__routines *************************\/\r\n< PyObject *cb_fun_in___user__routines_capi = NULL;\/*was Py_None*\/\r\n< PyTupleObject *cb_fun_in___user__routines_args_capi = NULL;\r\n< int cb_fun_in___user__routines_nofargs = 0;\r\n< jmp_buf cb_fun_in___user__routines_jmpbuf;\r\n< \/*typedef double(*cb_fun_in___user__routines_typedef)(int *);*\/\r\n< static double cb_fun_in___user__routines (int *i_cb_capi) {\r\n<   PyTupleObject *capi_arglist = cb_fun_in___user__routines_args_capi;\r\n---\r\n> \/*********************** cb_fun_in_foo__user__routines ***********************\/\r\n> PyObject *cb_fun_in_foo__user__routines_capi = NULL;\/*was Py_None*\/\r\n> PyTupleObject *cb_fun_in_foo__user__routines_args_capi = NULL;\r\n> int cb_fun_in_foo__user__routines_nofargs = 0;\r\n> jmp_buf cb_fun_in_foo__user__routines_jmpbuf;\r\n> \/*typedef double(*cb_fun_in_foo__user__routines_typedef)(int *);*\/\r\n> static double cb_fun_in_foo__user__routines (int *i_cb_capi) {\r\n>   PyTupleObject *capi_arglist = cb_fun_in_foo__user__routines_args_capi;\r\n289,291c243,245\r\n<   CFUNCSMESS(\"cb:Call-back function cb_fun_in___user__routines (maxnofargs=1(-0))\\n\");\r\n<   CFUNCSMESSPY(\"cb:cb_fun_in___user__routines_capi=\",cb_fun_in___user__routines_capi);\r\n<   if (cb_fun_in___user__routines_capi==NULL) {\r\n---\r\n>   CFUNCSMESS(\"cb:Call-back function cb_fun_in_foo__user__routines (maxnofargs=1(-0))\\n\");\r\n>   CFUNCSMESSPY(\"cb:cb_fun_in_foo__user__routines_capi=\",cb_fun_in_foo__user__routines_capi);\r\n>   if (cb_fun_in_foo__user__routines_capi==NULL) {\r\n293c247\r\n<     cb_fun_in___user__routines_capi = PyObject_GetAttrString(callback2_module,\"fun\");\r\n---\r\n>     cb_fun_in_foo__user__routines_capi = PyObject_GetAttrString(callback2_module,\"fun\");\r\n295c249\r\n<   if (cb_fun_in___user__routines_capi==NULL) {\r\n---\r\n>   if (cb_fun_in_foo__user__routines_capi==NULL) {\r\n299,302c253,256\r\n<   if (F2PyCapsule_Check(cb_fun_in___user__routines_capi)) {\r\n<   cb_fun_in___user__routines_typedef cb_fun_in___user__routines_cptr;\r\n<   cb_fun_in___user__routines_cptr = F2PyCapsule_AsVoidPtr(cb_fun_in___user__routines_capi);\r\n<   return_value=(*cb_fun_in___user__routines_cptr)(i_cb_capi);\r\n---\r\n>   if (F2PyCapsule_Check(cb_fun_in_foo__user__routines_capi)) {\r\n>   cb_fun_in_foo__user__routines_typedef cb_fun_in_foo__user__routines_cptr;\r\n>   cb_fun_in_foo__user__routines_cptr = F2PyCapsule_AsVoidPtr(cb_fun_in_foo__user__routines_capi);\r\n>   return_value=(*cb_fun_in_foo__user__routines_cptr)(i_cb_capi);\r\n332c286\r\n<   if (cb_fun_in___user__routines_nofargs>capi_i)\r\n---\r\n>   if (cb_fun_in_foo__user__routines_nofargs>capi_i)\r\n346c300\r\n<   capi_return = PyObject_CallObject(cb_fun_in___user__routines_capi,(PyObject *)capi_arglist_list);\r\n---\r\n>   capi_return = PyObject_CallObject(cb_fun_in_foo__user__routines_capi,(PyObject *)capi_arglist_list);\r\n350c304\r\n<   capi_return = PyObject_CallObject(cb_fun_in___user__routines_capi,(PyObject *)capi_arglist);\r\n---\r\n>   capi_return = PyObject_CallObject(cb_fun_in_foo__user__routines_capi,(PyObject *)capi_arglist);\r\n371,372c325,326\r\n<     GETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,double,\"double_from_pyobj failed in converting return_value of call-back function cb_fun_in___user__routines to C double\\n\");\r\n<   CFUNCSMESS(\"cb:cb_fun_in___user__routines:successful\\n\");\r\n---\r\n>     GETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,double,\"double_from_pyobj failed in converting return_value of call-back function cb_fun_in_foo__user__routines to C double\\n\");\r\n>   CFUNCSMESS(\"cb:cb_fun_in_foo__user__routines:successful\\n\");\r\n379c333\r\n<   fprintf(stderr,\"Call-back cb_fun_in___user__routines failed.\\n\");\r\n---\r\n>   fprintf(stderr,\"Call-back cb_fun_in_foo__user__routines failed.\\n\");\r\n383c337\r\n<     longjmp(cb_fun_in___user__routines_jmpbuf,-1);\r\n---\r\n>     longjmp(cb_fun_in_foo__user__routines_jmpbuf,-1);\r\n388c342\r\n< \/********************* end of cb_fun_in___user__routines *********************\/\r\n---\r\n> \/******************** end of cb_fun_in_foo__user__routines ********************\/\r\n395c349\r\n< r = foo(f,[f_extra_args])\\n\\nWrapper for ``foo``.\\\r\n---\r\n> r = foo(fun,[fun_extra_args])\\n\\nWrapper for ``foo``.\\\r\n397c351\r\n< \"f : call-back function => fun\\n\"\r\n---\r\n> \"fun : call-back function\\n\"\r\n399c353\r\n< \"f_extra_args : input tuple, optional\\n    Default: ()\\n\"\r\n---\r\n> \"fun_extra_args : input tuple, optional\\n    Default: ()\\n\"\r\n408c362\r\n< \/* extern void F_FUNC(foo,FOO)(cb_fun_in___user__routines_typedef,double*); *\/\r\n---\r\n> \/* extern void F_FUNC(foo,FOO)(cb_fun_in_foo__user__routines_typedef,double*); *\/\r\n412c366\r\n<                            void (*f2py_func)(cb_fun_in___user__routines_typedef,double*)) {\r\n---\r\n>                            void (*f2py_func)(cb_fun_in_foo__user__routines_typedef,double*)) {\r\n417,421c371,375\r\n<   PyObject *f_capi = Py_None;\r\n<   PyTupleObject *f_xa_capi = NULL;\r\n<   PyTupleObject *f_args_capi = NULL;\r\n<   int f_nofargs_capi = 0;\r\n<   cb_fun_in___user__routines_typedef f_cptr;\r\n---\r\n>   PyObject *fun_capi = Py_None;\r\n>   PyTupleObject *fun_xa_capi = NULL;\r\n>   PyTupleObject *fun_args_capi = NULL;\r\n>   int fun_nofargs_capi = 0;\r\n>   cb_fun_in_foo__user__routines_typedef fun_cptr;\r\n423c377\r\n<   static char *capi_kwlist[] = {\"f\",\"f_extra_args\",NULL};\r\n---\r\n>   static char *capi_kwlist[] = {\"fun\",\"fun_extra_args\",NULL};\r\n431c385\r\n<     capi_kwlist,&f_capi,&PyTuple_Type,&f_xa_capi))\r\n---\r\n>     capi_kwlist,&fun_capi,&PyTuple_Type,&fun_xa_capi))\r\n434,436c388,390\r\n<   \/* Processing variable f *\/\r\n< if(F2PyCapsule_Check(f_capi)) {\r\n<   f_cptr = F2PyCapsule_AsVoidPtr(f_capi);\r\n---\r\n>   \/* Processing variable fun *\/\r\n> if(F2PyCapsule_Check(fun_capi)) {\r\n>   fun_cptr = F2PyCapsule_AsVoidPtr(fun_capi);\r\n438c392\r\n<   f_cptr = cb_fun_in___user__routines;\r\n---\r\n>   fun_cptr = cb_fun_in_foo__user__routines;\r\n441,447c395,401\r\n<   f_nofargs_capi = cb_fun_in___user__routines_nofargs;\r\n<   if (create_cb_arglist(f_capi,f_xa_capi,1,0,&cb_fun_in___user__routines_nofargs,&f_args_capi,\"failed in processing argument list for call-back f.\")) {\r\n<     jmp_buf f_jmpbuf;\r\n<     CFUNCSMESS(\"Saving jmpbuf for `f`.\\n\");\r\n<     SWAP(f_capi,cb_fun_in___user__routines_capi,PyObject);\r\n<     SWAP(f_args_capi,cb_fun_in___user__routines_args_capi,PyTupleObject);\r\n<     memcpy(&f_jmpbuf,&cb_fun_in___user__routines_jmpbuf,sizeof(jmp_buf));\r\n---\r\n>   fun_nofargs_capi = cb_fun_in_foo__user__routines_nofargs;\r\n>   if (create_cb_arglist(fun_capi,fun_xa_capi,1,0,&cb_fun_in_foo__user__routines_nofargs,&fun_args_capi,\"failed in processing argument list for call-back fun.\")) {\r\n>     jmp_buf fun_jmpbuf;\r\n>     CFUNCSMESS(\"Saving jmpbuf for `fun`.\\n\");\r\n>     SWAP(fun_capi,cb_fun_in_foo__user__routines_capi,PyObject);\r\n>     SWAP(fun_args_capi,cb_fun_in_foo__user__routines_args_capi,PyTupleObject);\r\n>     memcpy(&fun_jmpbuf,&cb_fun_in_foo__user__routines_jmpbuf,sizeof(jmp_buf));\r\n454c408\r\n<     if ((setjmp(cb_fun_in___user__routines_jmpbuf))) {\r\n---\r\n>     if ((setjmp(cb_fun_in_foo__user__routines_jmpbuf))) {\r\n457c411\r\n<         (*f2py_func)(f_cptr,&r);\r\n---\r\n>         (*f2py_func)(fun_cptr,&r);\r\n475,480c429,434\r\n<     CFUNCSMESS(\"Restoring jmpbuf for `f`.\\n\");\r\n<     cb_fun_in___user__routines_capi = f_capi;\r\n<     Py_DECREF(cb_fun_in___user__routines_args_capi);\r\n<     cb_fun_in___user__routines_args_capi = f_args_capi;\r\n<     cb_fun_in___user__routines_nofargs = f_nofargs_capi;\r\n<     memcpy(&cb_fun_in___user__routines_jmpbuf,&f_jmpbuf,sizeof(jmp_buf));\r\n---\r\n>     CFUNCSMESS(\"Restoring jmpbuf for `fun`.\\n\");\r\n>     cb_fun_in_foo__user__routines_capi = fun_capi;\r\n>     Py_DECREF(cb_fun_in_foo__user__routines_args_capi);\r\n>     cb_fun_in_foo__user__routines_args_capi = fun_args_capi;\r\n>     cb_fun_in_foo__user__routines_nofargs = fun_nofargs_capi;\r\n>     memcpy(&cb_fun_in_foo__user__routines_jmpbuf,&fun_jmpbuf,sizeof(jmp_buf));\r\n482c436\r\n<   \/* End of cleaning variable f *\/\r\n---\r\n>   \/* End of cleaning variable fun *\/\r\n548c502\r\n< \"  r = foo(f,f_extra_args=())\\n\"\r\n---\r\n> \"  r = foo(fun,fun_extra_args=())\\n\"\r\n```\r\n\r\n\r\nSo it will likely not work with `meson` for a bit. There should be a pragmatic enough fix to get `scipy` working again though."],"labels":["component: numpy.f2py"]},{"title":"ENH: Added ``fill`` option to ``np.atleast_2d`` and ``np.atleast_3d``","body":"- [x] _Ready for review_\r\n\r\nAdded optional keyword argument `fill` to `np.atleast_2d` and `np.atleast_3d`, to specify where the missing dimension(s) should be added.\r\n\r\n- Default values are consistent with current behaviours,\r\n- I didn't add it to `np.atleast_1d` for obvious reasons, but if signature consistency is an issue, it can be done,\r\n- Implementation: I parse the non-default behaviours first, so that illegal values are mapped to default behaviour.\r\n\r\nThis can be useful when working with Fortan convention for example, or for locally-specifi reasons.\r\n\r\n","comments":["Can someone help me fix the failed build \/ give some feedback?\r\nThanks!","Up"],"labels":["01 - Enhancement"]},{"title":"BUG: f2py undefined symbol for MPI application in HPE-Cray cluster with gfortran compiler (or \"ftn\" wrapper)","body":"### Describe the issue:\r\n\r\nI have a fortran subroutine that makes use of MPI:\r\n```\r\nsubroutine sayhello\r\nuse mpi\r\nimplicit none\r\n\r\n\r\n    integer :: comm, rank, size, ierr, namelength\r\n    character(len=15) :: processorname\r\n\r\ncall MPI_INIT(ierr)\r\ncall MPI_Comm_size(MPI_COMM_WORLD, size, ierr)\r\ncall MPI_Comm_rank(MPI_COMM_WORLD, rank, ierr)\r\ncall MPI_GET_PROCESSOR_NAME(processorName, namelength, ierr)\r\nprint *, 'Hello, World! I am process ',rank,' of ',size,'.'\r\n\r\nend subroutine sayhello\r\n```\r\n\r\nThen, I create the library with f2py:\r\n```\r\n$ module load PrgEnv-gnu\/8.3.3 python\/3.10.10 py-numpy\/1.23.4 py-mpi4py\/3.1.4-py3.10.10\r\n$ LDSHARED=ftn F77=ftn F90=ftn f2py --f90exec=ftn --verbose -c helloworld.f90 -m helloworld\r\n```\r\n\r\nThis creates the library without errors:\r\n`helloworld.cpython-310-x86_64-linux-gnu.so`\r\n\r\nBut when loading the library, I get an error related to MPI libraries:\r\n```\r\n$ python -c \"import helloworld\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: \/software\/projects\/bq2\/espinosa\/f2py\/f2py_test\/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_\r\n```\r\n\r\nWhen reviewing the output of the compilation, I can see that the \"ftn\" wrapper (which provides all the \"options machinery\" for compiling MPI code in the HPE-Cray) is used for compilation, but not for linking (partial output below):\r\n```\r\nINFO: customize Gnu95FCompiler\r\nDEBUG: find_executable('gfortran')\r\nINFO: Found executable \/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran\r\nINFO: customize Gnu95FCompiler using build_ext\r\n********************************************************************************\r\n<class 'numpy.distutils.fcompiler.gnu.Gnu95FCompiler'>\r\nversion_cmd     = ['\/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran', '-dumpversion']\r\ncompiler_f77    = ['ftn', '-Wall', '-g', '-ffixed-form', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']\r\ncompiler_f90    = ['ftn', '-Wall', '-g', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']\r\ncompiler_fix    = ['ftn', '-Wall', '-g', '-ffixed-form', '-fno-second-underscore', '-Wall', '-g', '-fno-second-underscore', '-fPIC', '-O3', '-funroll-loops']\r\nlinker_so       = ['\/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran', '-Wall', '-g', '-Wall', '-g', '-shared']\r\narchiver        = ['\/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran', '-cr']\r\nranlib          = ['\/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran']\r\nlinker_exe      = ['\/opt\/cray\/pe\/gcc\/12.2.0\/bin\/gfortran', '-Wall', '-Wall']\r\nversion         = LooseVersion ('12.2.0')\r\nlibraries       = ['gfortran']\r\nlibrary_dirs    = ['\/opt\/cray\/pe\/gcc\/12.2.0\/snos\/lib\/gcc\/x86_64-suse-linux\/12.2.0\/..\/..\/..\/..\/lib64', '\/opt\/cray\/pe\/gcc\/12.2.0\/snos\/lib\/gcc\/x86_64-suse-linux\/12.2.0\/..\/..\/..\/..\/lib64', '\/software\/setonix\/2023.08\/software\/linux-sles15-zen3\/gcc-12.2.0\/python-3.10.10-bk4mjnuv6ufkvy3gb5h62l65dgv6zost\/lib']\r\nobject_switch   = '-o '\r\ncompile_switch  = '-c'\r\ninclude_dirs    = ['\/software\/setonix\/2023.08\/software\/linux-sles15-zen3\/gcc-12.2.0\/python-3.10.10-bk4mjnuv6ufkvy3gb5h62l65dgv6zost\/include\/python3.10']\r\n```\r\nThe `linker_so` is calling `gfortran` directly, so it is not using all the libraries and paths that the \"ftn\" wrapper calls.\r\n\r\nIssue is very similar (if not exactly the same) as that in:\r\n#16481 (f2py undefined symbol with PGI fortran compiler and MPI calls)\r\nbut now when using the gfortran compiler (or \"ftn\" wrapper) in HPE-Cray EX cluster. On that similar issue, the explicit use of `<F90>` in the settings for \"linker_so\" made the trick. So I went into `gnu.py` file and noticed that the mentioned fix does not apply here, as `<F90>` setting is already there:\r\n```\r\npossible_executables = ['gfortran', 'f95']\r\n    executables = {\r\n        'version_cmd'  : [\"<F90>\", \"-dumpversion\"],\r\n        'compiler_f77' : [None, \"-Wall\", \"-g\", \"-ffixed-form\",\r\n                          \"-fno-second-underscore\"],\r\n        'compiler_f90' : [None, \"-Wall\", \"-g\",\r\n                          \"-fno-second-underscore\"],\r\n        'compiler_fix' : [None, \"-Wall\",  \"-g\",\"-ffixed-form\",\r\n                          \"-fno-second-underscore\"],\r\n        'linker_so'    : [\"<F90>\", \"-Wall\", \"-g\",],\r\n        'archiver'     : [\"ar\", \"-cr\"],\r\n        'ranlib'       : [\"ranlib\"],\r\n        'linker_exe'   : [None, \"-Wall\"]\r\n    }\r\n```\r\n\r\nThen, I modified the `gnu.py` file and add all the paths and libraries that the `ftn` wrapper would have called in the linking step:\r\n```\r\n'linker_so'    : [\"<F90>\", \"-Wall\", \"-g\",\r\n                          \"-I\/opt\/cray\/pe\/mpich\/8.1.25\/ofi\/gnu\/9.1\/include\",\r\n                          \"-I\/opt\/cray\/pe\/libsci\/23.02.1.1\/GNU\/9.1\/x86_64\/include\",\r\n                          \"-I\/opt\/cray\/pe\/dsmml\/0.2.2\/dsmml\/\/include\",\r\n                          \"-I\/opt\/cray\/xpmem\/2.5.2-2.4_3.47__gd0f7936.shasta\/include\",\r\n                          \"-L\/opt\/cray\/pe\/mpich\/8.1.25\/ofi\/gnu\/9.1\/lib\",\r\n                          \"-L\/opt\/cray\/pe\/libsci\/23.02.1.1\/GNU\/9.1\/x86_64\/lib\",\r\n                          \"-L\/opt\/cray\/pe\/dsmml\/0.2.2\/dsmml\/\/lib\",\r\n                          \"-L\/opt\/cray\/xpmem\/2.5.2-2.4_3.47__gd0f7936.shasta\/lib64\",\r\n                          \"-ldl\",\r\n                          \"-Wl,--as-needed,-lsci_gnu_82_mpi,--no-as-needed\",\r\n                          \"-Wl,--as-needed,-lsci_gnu_82,--no-as-needed\",\r\n                          \"-Wl,--as-needed,-lmpifort_gnu_91,--no-as-needed\",\r\n                          \"-Wl,--as-needed,-lmpi_gnu_91,--no-as-needed\",\r\n                          \"-Wl,--as-needed,-ldsmml,--no-as-needed\",\r\n                          \"-lxpmem\"],\r\n```\r\n\r\nWith that change, the creation of the library again finished without errors, but when calling the library in python the same error happens:\r\n```\r\n$ python -c \"import helloworld\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: \/software\/projects\/bq2\/espinosa\/f2py\/f2py_test\/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_\r\n```\r\nSo, it seems that even if the `ftn` wrapper was used in the linking step, the same error may persists. What would be the error source then?\r\n\r\n(Using a ctypes approach works for the same kind of MPI test function, so I'm wondering what could be going wrong with the f2py approach?)\r\n\r\n### Reproduce the code example:\r\n\r\nAll explained in the Description section above (including code snippets)\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n$ python -c \"import helloworld\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: \/software\/projects\/bq2\/espinosa\/f2py\/f2py_test\/helloworld.cpython-310-x86_64-linux-gnu.so: undefined symbol: mpi_comm_rank_\r\n```\r\n\r\n### Runtime information:\r\n\r\n```\r\n$ python -c \"import sys, numpy; print(numpy.__version__); print(sys.version)\"\r\n1.23.4\r\n3.10.10 (main, Aug 25 2023, 00:41:58) [GCC 12.2.0 20220819 (HPE)]\r\n```\r\n\r\n### Context for the issue:\r\n\r\nUsers already count with postprocessing sets of scripts and functions that make use of f2py with fortran-MPI code. This error is affecting their workflows! Currently they are moving their data to an external cluster to proceed with their postprocessing which is, definitively, not ideal!","comments":["Thanks for the report @AlexisEspinosaGayosso \r\n\r\n> When reviewing the output of the compilation, I can see that the \"ftn\" wrapper (which provides all the \"options machinery\" for compiling MPI code in the HPE-Cray) is used for compilation, but not for linking (partial output below):\r\n> [...]\r\n> So, it seems that even if the `ftn` wrapper was used in the linking step, the same error may persists. What would be the error source then?\r\n\r\nThat is hard to say without being able to reproduce the issue. My first recommendation is to see if things work for you with numpy `1.26.2` or the `main` branch, because we switched from using `distutils` to using `meson` for numpy itself and in f2py. "],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"BUG: Type hints for dtype related functions regressed","body":"### Describe the issue:\n\nWhen upgrading from 1.24.3 to 1.26, mypy is now failing to understand the type hints from dtype related functions.\r\nRunning with mypy 1.6.0\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nA = np.random.rand(2, 3)\r\nB = A.astype(np.float32)\r\nreveal_type(A)\r\nreveal_type(B)\n```\n\n\n### Error message:\n\n```shell\n1.24.3 output:\r\n\r\ntest.py:5:13: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[numpy.floating[numpy._typing._64Bit]]]\"\r\ntest:6:13: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[numpy.floating[numpy._typing._32Bit]]]\"\r\n```\r\n\r\n1.26.0 output:\r\n```\r\ntest.py:4:5: error: Need type annotation for \"B\"  [var-annotated]\r\ntest.py:5:13: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[numpy.floating[numpy._typing._64Bit]]]\"\r\ntest.py:6:13: note: Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n```\n```\n\n\n### Runtime information:\n\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.26.0\r\n3.11.4+ (heads\/3.11:f207b346d4, Jun 11 2023, 14:33:13) [GCC 12.3.1 20230508 (Red Hat 12.3.1-1)]\r\n>>> print(numpy.show_runtime())\r\n[{'numpy_version': '1.26.0',\r\n  'python': '3.11.4+ (heads\/3.11:f207b346d4, Jun 11 2023, 14:33:13) [GCC '\r\n            '12.3.1 20230508 (Red Hat 12.3.1-1)]',\r\n  'uname': uname_result(system='Linux', node='albandes-fedora-K2202N0104138', release='6.5.6-100.fc37.x86_64', version='#1 SMP PREEMPT_DYNAMIC Fri Oct  6 19:01:16 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/albandes\/local\/pytorch\/3.11_debug_source_env\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 20,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n\n\n### Context for the issue:\n\nThis is making our lint fail when upgrading to latest numpy in PyTorch.","comments":["@albanD I think we could close this issue - in the 2.0.0b1 release revealed type for `B` is:\r\n```\r\nnumpy.ndarray[Any, numpy.dtype[numpy.floating[numpy._typing._32Bit]]]\r\n```\r\n\r\nAlso, there are reveal tests for `astype`: https:\/\/github.com\/numpy\/numpy\/blob\/56dab5053eedbd0a34b8c5aa8bf66c46a8b15cb5\/numpy\/typing\/tests\/data\/reveal\/ndarray_conversion.pyi#L39C44-L39C51"],"labels":["00 - Bug","Static typing"]},{"title":"CI, TST: have a f2py-specific CI run and properly skip f2py tests elsewhere","body":"From #25073 where [@HaoZeke pointed out](https:\/\/github.com\/numpy\/numpy\/pull\/25073#issuecomment-1807161922) CI is currently not properly testing f2py on windows. \r\n\r\n> I believe the solution is to have a new CI job, where (some) f2py tests should be tested the way users would use them, i.e. by generating the meson build directory with the skeleton template and then running meson within with the right environment variables.\r\n\r\n","comments":["+1 for this idea. I'd like to treat `f2py` compile tests similarly to Mypy tests - a couple of dedicated jobs, and disabled everywhere else. Proposed config:\r\n\r\n- 1 Windows job, MSVC\/gfortran on py311 (for now, could switch to Flang or LFortran at some point)\r\n- 2 Linux jobs, gcc\/gfortran, for lowest and highest supported Python versions\r\n- 1 macOS job, clang\/gfortran\r\n\r\nThis is the output of `spin test -- numpy\/f2py\/ --durations=25` right now:\r\n\r\n```\r\n========================================================================== slowest 25 durations ==========================================================================\r\n4.19s call     numpy\/f2py\/tests\/test_compile_function.py::test_f2py_init_compile[]\r\n4.14s call     numpy\/f2py\/tests\/test_compile_function.py::test_f2py_init_compile[--noopt --debug]\r\n4.02s call     numpy\/f2py\/tests\/test_compile_function.py::test_f2py_init_compile[extra_args0]\r\n3.38s setup    numpy\/f2py\/tests\/test_kind.py::TestKind::test_int\r\n2.92s setup    numpy\/f2py\/tests\/test_mixed.py::TestMixed::test_all\r\n2.53s setup    numpy\/f2py\/tests\/test_abstract_interface.py::TestAbstractInterface::test_abstract_interface\r\n2.43s setup    numpy\/f2py\/tests\/test_module_doc.py::TestModuleDocString::test_module_docstring\r\n2.34s setup    numpy\/f2py\/tests\/test_callback.py::TestF77CallbackPythonTLS::test_all[t]\r\n2.33s setup    numpy\/f2py\/tests\/test_isoc.py::TestISOC::test_c_double\r\n2.33s setup    numpy\/f2py\/tests\/test_return_real.py::TestFReturnReal::test_all_f77[t0]\r\n2.29s setup    numpy\/f2py\/tests\/test_common.py::TestCommonWithUse::test_common_gh19161\r\n2.27s setup    numpy\/f2py\/tests\/test_return_integer.py::TestFReturnInteger::test_all_f77[t0]\r\n2.24s setup    numpy\/f2py\/tests\/test_character.py::TestStringOptionalInOut::test_gh24662\r\n2.24s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestUnicodeComment::test_encoding_comment\r\n2.24s setup    numpy\/f2py\/tests\/test_character.py::TestCharacterString::test_input[1]\r\n2.23s setup    numpy\/f2py\/tests\/test_quoted_character.py::TestQuotedCharacter::test_quoted_character\r\n2.21s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestDimSpec::test_array_size[n]\r\n2.21s setup    numpy\/f2py\/tests\/test_callback.py::TestF77Callback::test_all[t]\r\n2.21s setup    numpy\/f2py\/tests\/test_return_complex.py::TestFReturnComplex::test_all_f77[t0]\r\n2.20s setup    numpy\/f2py\/tests\/test_callback.py::TestGH18335::test_gh18335\r\n2.20s setup    numpy\/f2py\/tests\/test_f2cmap.py::TestF2Cmap::test_long_long_map\r\n2.19s setup    numpy\/f2py\/tests\/test_string.py::TestFixedString::test_intent_in\r\n2.18s setup    numpy\/f2py\/tests\/test_character.py::TestMiscCharacter::test_gh18684\r\n2.18s setup    numpy\/f2py\/tests\/test_data.py::TestDataF77::test_data_stmts\r\n2.18s setup    numpy\/f2py\/tests\/test_crackfortran.py::TestNoSpace::test_module\r\n================================================== 762 passed, 9 skipped, 36 deselected, 3 xfailed in 114.32s (0:01:54)\r\n```\r\n\r\nThat should all be disabled in the default test runs, it's way too expensive. _EDIT: the above is with gh-25111, it's better on `main` - but still taking almost a minute on macOS arm64._","Hi Everyone,\r\n\r\nGreetings!\r\nI hope you all are doing well. Is this issue still unresolved I am interested in working on this issue.\r\n\r\n\r\nThanks and Regards","Hi @ojuschugh1, yes this still needs doing and would be helpful. I think it can be done without crossing paths with @HaoZeke's gh-25252, so should be fine to work on now.\r\n\r\nI'd suggest:\r\n1. looking at how we test with MyPy (e.g., the [mypy.yml](https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/mypy.yml) workflow file and the `RUN_MYPY` environment variable),\r\n2. Iterating on this on your own fork, so you're running only the new CI jobs, and only opening a PR to this repo once the new jobs work","Hi @rgommers ,\r\n\r\nThanks for the prompt response. Sure, I would be happy to work on this :D.\r\n\r\nThanks and Regards\r\n","> Hi @rgommers ,\r\n> \r\n> Thanks for the prompt response. Sure, I would be happy to work on this :D.\r\n> \r\n> Thanks and Regards\r\n\r\nThanks for taking a look at this, let me know if you'd like to setup a meet to go over parts of the CI and what would be needed.","Hi @HaoZeke ,\r\n\r\nThank you for your support. Sure, I will definitely contact you if I face any trouble.\r\n\r\nThanks again for this nice gesture.\r\n\r\nThanks and Regards,\r\nOjus Chugh","Hello @HaoZeke ,\r\nGreetings for the day. I hope you are doing great. I apologize for the delay. I wanted to work on this issue - [https:\/\/github.com\/numpy\/numpy\/issues\/25134]. I have forked the NumPy repo. Could you kindly help me set this up locally on my PC tomorrow if you have some time? Thanks for the help. I look forward to hearing from you soon. :)\r\nThanks and Regards.","Hi Ojus, I will be unavailable until Friday, but I would recommend going over the documentation, and send me a mail to schedule a meeting on Friday if there are still problems (or perhaps ping me on the NumPy slack?)\n\n\nOn 3\/17\/24 6:38 PM, Ojus Chugh ***@***.***> wrote:\n> Hello @HaoZeke <https:\/\/github.com\/HaoZeke> ,\n> Greetings for the day. I hope you are doing great. I apologize for the \n> delay. I wanted to work on this issue - \n> [https:\/\/github.com\/\/issues\/25134 \n> <https:\/\/github.com\/numpy\/numpy\/issues\/25134>]. I have forked the NumPy \n> repo. Could you kindly help me set this up locally on my PC tomorrow if \n> you have some time? Thanks for the help. I look forward to hearing from \n> you soon. :)\n> Thanks and Regards.\n> \n> \u2014\n> Reply to this email directly, view it on GitHub \n> <https:\/\/github.com\/numpy\/numpy\/issues\/25134#issuecomment-2002573057>, \n> or unsubscribe \n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABBCUTYULLIGE3P7EVQOTUTYYXPJNAVCNFSM6AAAAAA7IT33VSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMBSGU3TGMBVG4>.\n> You are receiving this because you were mentioned.Message ID: \n> ***@***.***>\n> \n> \n","Thanks @HaoZeke for your prompt response. I will certainly try to follow the documentation and set up the system. Let's plan to meet on Friday at 10 pm  IST[ if that's okay with you ] to discuss this bug further and address any doubts I might have encountered.\r\n\r\nI've also messaged you on Slack. We can discuss further details there as well.\r\n\r\nThanks and regards"],"labels":["05 - Testing","component: numpy.f2py"]},{"title":"ENH: Use new `spin` commands for CI","body":"### Changes\r\n- use `spin lint` for CI\r\n- use `spin notes` for CI\r\n\r\n### Testing\r\n\r\n- Added a tmp [commit](https:\/\/github.com\/numpy\/numpy\/pull\/25103\/commits\/17ee666095a630605821962ffd5b73c260b0b64e) with linter errors: [failure](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/6859950172\/job\/18653011873?pr=25103#step:5:1)\r\n- Release notes: [CI run](https:\/\/app.circleci.com\/pipelines\/github\/numpy\/numpy\/23103\/workflows\/c5c5090b-11eb-43a1-9cb2-97dca594ff3d\/jobs\/35978?invite=true#step-107-4)\r\n\r\n### Notes\r\n- related\r\n  - #24080\r\n  - https:\/\/github.com\/numpy\/numpy\/pull\/24983\r\n  - https:\/\/github.com\/numpy\/numpy\/pull\/25017","comments":["> Would it make sense to merge linter_requirements.txt with build_requirements.txt now? Since lint checks should technically be part of the development workflow?\r\n\r\nI would personally prefer they remain separate, but am willing to be convinced that they can be merged.","+1 for keeping separate, `build_requirements` is for building and is used a fair amount. linting is only done in 1-2 jobs. no need to install unnecessary linting tools in regular CI jobs.","Thanks for the inputs folks, I have modified the requirements to include spin.\r\n\r\nTesting:\r\n- Added a tmp [commit](https:\/\/github.com\/numpy\/numpy\/pull\/25103\/commits\/17ee666095a630605821962ffd5b73c260b0b64e) with linter errors: [failure](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/6859950172\/job\/18653011873?pr=25103#step:5:1)\r\n- Release notes: [CI run](https:\/\/app.circleci.com\/pipelines\/github\/numpy\/numpy\/23103\/workflows\/c5c5090b-11eb-43a1-9cb2-97dca594ff3d\/jobs\/35978?invite=true#step-107-4)"],"labels":["01 - Enhancement","16 - Development"]},{"title":"BUG: `np.split(a_list, an_int)` raises an exception","body":"Add: This is the first time I report a bug of NumPy. I don't know why the title has \"np.split(a_list, an_int) raises an exception\", because I set a different title before.\r\n\r\n### Describe the issue:\r\n\r\nThere are two groups of issues.\r\n1. np.fill_diagonal, np.diag, and np.diagonal can cause unexpected value assigning when flipping diagonals. \r\n\r\nAn important underlying behavior about the underlying code of np.fill_diagonal should be as I interpret in the example below:\r\nIn a wrong flipping of the diagonal of the array:\r\n[[0, 1]\r\n [2, 3]],\r\nthe underlying code of numpy first assigns the value 3 to the top-left corner of the array. Then the underlying code treats the top-left corner has the value 3 rather than the original value 0 and assign 3 to the bottom-right of the array.\r\n\r\nBut if use the subscript of array instead of np.fill_diagonal, the behavior looks to be different, as in the last example in the first code group below.\r\n\r\n2. The document of np.array_split doesn't say that the np.split doesn't allows lists, whereas np.array_split allows lists and has weird behavior on some lists.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n# the first code group\r\nimport numpy as np\r\nnp.__version__\r\na = np.arange(4).reshape(2, 2)\r\narr = a.copy()\r\narr\r\nnp.fill_diagonal(arr, np.flip(np.diag(arr.copy()))) # a correct flip\r\narr\r\narr = a.copy()\r\narr\r\nnp.fill_diagonal(arr, np.flip(np.diag(arr))) # has a bug\r\narr\r\n# If use np.diagonal instead of np.diag in the above, the result will be the same.\r\narr = a.copy()\r\narr\r\nnp.fill_diagonal(arr, np.flip(np.diagonal(arr)))\r\narr\r\n# Another example on a 5 by 5 matrix\r\na = np.diag([1,2,3,4,5])\r\narr = np.fliplr(a.copy())\r\n# a correct assigning similar to a way of assigning anti-diagonal by np.fliplr that is showed near the end of the document of np.fill_diagonal\r\nnp.fill_diagonal(np.fliplr(arr), np.diag(np.flip(np.fliplr(arr.copy()))))\r\narr\r\narr = np.fliplr(a.copy())\r\narr\r\nnp.fill_diagonal(np.fliplr(arr), np.diag(np.flip(np.fliplr(arr)))) # has a bug\r\narr\r\narr = np.fliplr(a.copy())\r\nnp.fill_diagonal(np.fliplr(arr), np.diagonal(np.flip(np.fliplr(arr)))) # has a bug\r\narr\r\n# A similar bug on Assigning\r\narr = np.fliplr(a.copy())\r\nidx = np.arange(arr.shape[0])\r\nidx\r\narr[-idx-1, idx] = np.diag(np.fliplr(arr.copy())) # a correct flip\r\narr\r\narr = np.fliplr(a.copy())\r\narr\r\narr[-idx-1, idx] = np.diag(np.fliplr(arr)) # bug\r\narr\r\narr = np.fliplr(a.copy())\r\narr[-idx-1, idx] = np.diagonal(np.fliplr(arr)) # bug\r\narr\r\n\r\n# the second code group\r\nList = [range(4), range(4)]\r\nnp.array_split(List, 2, axis=1)\r\nList = [[0,1,2,3], [0,1,2,3]]\r\nnp.array_split(List, 2, axis=1)\r\nnp.split(List, 2, axis=1) # as the document of np.split says, the first positional argument should be ndarray, so this code will cause error, but the error message doesn't tell users to use ndarray\r\nimport sys, numpy; print(numpy.__version__); print(sys.version)\r\nprint(numpy.show_runtime())\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"D:\\Lib\\site-packages\\numpy\\lib\\shape_base.py\", line 859, in split\r\n    len(indices_or_sections)\r\nTypeError: object of type 'int' has no len()\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#52>\", line 1, in <module>\r\n    np.split(List, 2, axis=1) # as the document of np.split says, the first positional argument should be ndarray, so this code will cause error, but the error message doesn't tell users to use ndarray\r\n  File \"D:\\Lib\\site-packages\\numpy\\lib\\shape_base.py\", line 862, in split\r\n    N = ary.shape[axis]\r\nAttributeError: 'list' object has no attribute 'shape'\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.25.2\r\n3.11.5 (tags\/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.11.5 (tags\/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 '\r\n            '64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='LAPTOP-G4EA37VA', release='10', version='10.0.22621', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Zen',\r\n  'filepath': 'D:\\\\Lib\\\\site-packages\\\\numpy\\\\.libs\\\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'}]\r\nNone\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["> In a wrong flipping of the diagonal of the array:\r\n\r\nThis bug report is invalid. You are using aliased memory here, the second input to your `fill_diagonal` call contains a view on the first array:\r\n```python\r\n>>> a = np.arange(4).reshape(2, 2)\r\n>>> ix = np.flip(np.diagonal(a))\r\n>>> ix\r\narray([3, 0])\r\n>>> ix.base\r\narray([0, 1, 2, 3])\r\n>>> np.fill_diagonal(a, ix)\r\n>>> a\r\narray([[3, 1],\r\n       [2, 3]])\r\n```\r\n\r\nThat clearly will not give the answer you want - just don't do that.\r\n\r\n* * *\r\n\r\nThe `np.split` one can be reduced down to:\r\n```python\r\n>>> np.split([1, 2, 3], [1])\r\n[array([1]), array([2, 3])]\r\n>>> np.split([1, 2, 3], 1)\r\n...\r\nTypeError: object of type 'int' has no len()\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n  File ~\/code\/numpy\/build-install\/usr\/lib\/python3.9\/site-packages\/numpy\/lib\/_shape_base_impl.py:878 in split\r\n    N = ary.shape[axis]\r\nAttributeError: 'list' object has no attribute 'shape'\r\n```\r\n\r\nThat could use a fix indeed, the code is not handling array-likes well.","> This bug report is invalid.\r\n\r\nI would say it is very much a valid bug.  `arr.flat` assignments don't protect users from aliased memory (yes aliased memory is a bit tricky in either case that is true).\r\n\r\nBut, almost all of NumPy *does* protect from it.  And any place like this, which doesn't should be fixed in principle.","> But, almost all of NumPy _does_ protect from it. And any place like this, which doesn't should be fixed in principle.\r\n\r\nIn general I don't think that is possible to do reliably? Is it a check only for `.base`, or something else?","Pauli implemented a reliable solution many years ago, it is exposed in Python as `np.may_share_memory` (and `np.shares_memory`, IIRC).  Of course you are right.  In practice you do have to give up at some point and assume the arrays may have overlapping memory.\r\n\r\nWe use this in ufuncs and I think at least most indexing paths.  So at least for that part, I think it would be a good improvement (and maybe a small bug, because we do overlap detection in most places)."],"labels":["00 - Bug"]},{"title":"ENH: Added `spin refguide-check`","body":"### Changes\r\n- Added `spin refguide-check`\r\n\r\n### Testing\r\nNot done yet\r\n\r\n### Notes\r\n- Raising the first revision to get some discussions going\r\n- Related: #24080","comments":[],"labels":["01 - Enhancement","16 - Development","04 - Documentation"]},{"title":"BUG: np.histogram raise IndexError","body":"### Describe the issue:\n\nCalling np.histogram(data, bins=10000) raise IndexError: index 10008 is out of bounds for axis 0 with size 10001. The data is attached here.\r\n[data.zip](https:\/\/github.com\/numpy\/numpy\/files\/13235543\/data.zip)\r\n\n\n### Reproduce the code example:\n\n```python\n# unzip the data.zip and get the data.pkl first.\r\n\r\nimport numpy as np\r\nimport pickle\r\n\r\nwith open('data.pkl', 'rb') as fp:\r\n    data = pickle.load(fp)\r\n\r\nnp.histogram(data, bins=10000)\n```\n\n\n### Error message:\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/xxxx\/lib\/python3.10\/site-packages\/numpy\/lib\/histograms.py\", line 844, in histogram\r\n    decrement = tmp_a < bin_edges[indices]\r\nIndexError: index 10008 is out of bounds for axis 0 with size 10001\n```\n\n\n### Runtime information:\n\n`python -c \"import sys, numpy; print(numpy.__version__); print(sys.version)\"` gives:\r\n```\r\n1.24.3\r\n3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\r\n```\r\n\r\n`python -c \"import numpy; print(numpy.show_runtime())\"` gives\r\n```\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n```\n\n### Context for the issue:\n\n_No response_","comments":["I'm not going to open an [untrusted pickle file](https:\/\/huggingface.co\/docs\/hub\/security-pickle#why-is-it-dangerous), but we could probably help you more if you told us the min and max values in the `data` array. If I had to guess, some kind of floating point round-off issue is happening.","Sure, the min and max are:\r\n```\r\n>>> data.min()\r\n0.00391\r\n>>> data.max()\r\n0.5845\r\n>>> data.mean()\r\n0.01232\r\n>>> data.size\r\n1355150\r\n```\r\n\r\nI find version 1.25 and version 1.26 work well, maybe I need to upgrade it."],"labels":["00 - Bug"]},{"title":"TASK,TYP: Review `np.long` related (typing) overloads and tests","body":"### Proposed new feature or change:\r\n\r\nxref https:\/\/github.com\/numpy\/numpy\/pull\/25039 and https:\/\/github.com\/numpy\/numpy\/pull\/24224\r\n\r\nAforementioned PRs have exposed a number of cases wherein the `np.long` re-introduction and shift to a new default int size of 64 bit have resulted in a number of failures in the typing test suite. These cases should be further reviewed and addressed appropriately.\r\n\r\nIn particular this concerns the:\r\n* [ ] np.ctypeslibs reveal tests\r\n* [ ] np.random reveal tests\r\n","comments":["I think one reason why I got stuck multiple times is that it is easy to forget a definition, and that seems to translate to `Any`.  I would guess mypy gives at least a warning for that, maybe even an error?\r\nCould we make sure we notice undefined variables in the stubs when running tests?\r\n\r\nEDIT: as of writing gh-24224 may have left `_NBitInt` and the way the aliases work in a (presumably OK), but not ideal state.","@BvB93 I think we want to keep the 2.0.0 milestone for this, but it's probably not blocking for the release branch creation and `2.0.0rc1`, right?","Correct, I'll see if I can make some time next week to review the issue but this is by no means a blocker."],"labels":["Static typing"]},{"title":"BLD: Check Intel compiler when checking complex types in build","body":"Fixes #25034.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Rebased to fix the accidental submodule merge (and squash commits).  Since it's a one line change and nobody commented on it, will assume this is a step in the right direction.\r\n\r\nThanks @lysnikolaou.","I'm not sure about this, but I think I can recall `clang-cl` using the MSVC types. Should I look into it?\r\n\r\nEDIT: Yes, `clang-cl` CI fails due to this.","Argg, sorry... Interesting, but that makes me think that more that switching to C11 requires also changing this here in gh-25072.","I added a few more changes, especially the one needed for fast math from [this comment in the issue](https:\/\/github.com\/numpy\/numpy\/issues\/25034#issuecomment-1791502310) and adding a few more `std=` flags. That together with [the workaround for `-utf-8`](https:\/\/github.com\/mesonbuild\/meson\/issues\/12462#issuecomment-1793193550) gets me to a pretty clean build with one warning that repeats whenever compiling with cpp and one link-time error:\r\n```\r\nd:\\Intel\\oneAPI\\compiler\\latest\\windows\\bin-llvm\\..\\compiler\\include\\complex.h(37,14): warning: \"The \/Qstd=c99 compilation option is required to enable C99 support for C programs\" [-W#warnings]\r\n            #warning \"The \/Qstd=c99 compilation option is required to enable C99 support for C programs\"\r\n             ^\r\n1 warning generated.\r\n[421\/496] Linking target numpy\/_core\/_multiarray_umath.cp310-win_amd64.pyd\r\nFAILED: numpy\/_core\/_multiarray_umath.cp310-win_amd64.pyd numpy\/_core\/_multiarray_umath.cp310-win_amd64.pdb\r\n\"xilink.exe\" @numpy\/_core\/_multiarray_umath.cp310-win_amd64.pyd.rsp\r\nlibmmd.lib(libmmd.dll) : error LNK2005: ldexpf already defined in meson-generated_arraytypes.c.obj\r\n   Creating library numpy\\_core\\_multiarray_umath.cp310-win_amd64.lib and object numpy\\_core\\_multiarray_umath.cp310-win_amd64.exp\r\nnumpy\\_core\\_multiarray_umath.cp310-win_amd64.pyd : fatal error LNK1169: one or more multiply defined symbols found\r\n```","Hmm. it seems `clang-cl` needs the `msvc` complex definitions, but the intel clang-based compilers need the other ones.","> Hmm. it seems `clang-cl` needs the `msvc` complex definitions, but the intel clang-based compilers need the other ones.\r\n\r\nYup, that's right. I think `if cc.get_argument_syntax() == 'msvc' and not cc.get_id().startswith('intel')` should work in the `meson.build` file, right?","This is what we want, right?\r\n\r\n`get_id()` | result\r\n---|---\r\n`msvc` | True\r\n`clang-cl` | True\r\n`intel-llvm-cl` | False\r\n\r\nSo  `cc.get_id() == 'msvc' or cc.get_id() == 'clang-cl'","CI is passing","I don't understand the Intel\/fastmath related change well enough to comment on it, but the rest looks good!","I only added `__INTEL_LLVM_COMPILER` next to `__INTEL_COMPILER` where it was required for compilation. \r\n\r\nThis PR is still incomplete: linking fails. To recap (from [a comment in the issue](https:\/\/github.com\/numpy\/numpy\/issues\/25034#issuecomment-1790830697)), to test this PR one needs to\r\n- install the Intel oneAPI windows compiler from the [intel download site](https:\/\/www.intel.com\/content\/www\/us\/en\/developer\/tools\/oneapi\/base-toolkit-download.html?operatingsystem=window&distributions=online)\r\n- Set some environment variables:\r\n  ```\r\n  <install path>\\setvars.bat intel64\r\n  set CC=icx.exe\r\n  set CXX=icx.exe\r\n  ```\r\n- install packages for the build (standard stuff)\r\n  ```\r\n  pip install scipy-openblas64 -r build_requirements.txt -r test_requirements.txt\r\n  REM use scipy-openblas wheels to provide OpenBLAS\r\n  pip install scipy-openblas64\r\n  set PKG_CONFIG_PATH=<path>\\.openblas\r\n  ```\r\n\r\n- try to build NumPy with `pip install . --no-build-isolation`\r\n\r\nIf we decide this is important, there is an [example github repo](https:\/\/github.com\/oneapi-src\/oneapi-ci\/tree\/master#intel-oneapi-ci-samples) with a workflow that downloads and caches the compiler.","> If we decide this is important, there is an [example github repo](https:\/\/github.com\/oneapi-src\/oneapi-ci\/tree\/master#intel-oneapi-ci-samples) with a workflow that downloads and caches the compiler.\r\n\r\nI tried this before in https:\/\/github.com\/scipy\/scipy\/pull\/16957. It's a real pain to work with. I'm hoping the Intel compilers before available in conda-forge (we had some discussions with Intel engineers around that), then it'll be much easier. I wouldn't bother now.","Needs rebase.","Be good to finish this.","@rgommers @mattip I rebased this PR, so that it's easier to be reviewed\/merged, in case we want to do so before 2.0.","There is so much already in 2.0 I didn't want to add more. Also\r\n> This PR is still incomplete: linking fails.\r\n\r\nand I have not gotten it to work yet","Got it. I only wanted to remind you all of it, just in case it was missed."],"labels":["36 - Build"]},{"title":"`npy_cdouble` related error when compiling 1.26.1 with Intel compilers on Windows","body":"_Transferred from https:\/\/github.com\/numpy\/numpy\/issues\/22711#issuecomment-1785115279 (bug report by @ashish-2022)._:\r\n\r\nI get this error when compiling numpy 1.26.1 with intel MKL on Windows (Python 3.11.6). is there any extra environment variables I need to set for this?\r\n\r\n```\r\n [7\/489] Compiling C object numpy\/core\/libnpymath.a.p\/meson-generated_ieee754.c.obj\r\n      FAILED: numpy\/core\/libnpymath.a.p\/meson-generated_ieee754.c.obj\r\n      \"icl\" \"-Inumpy\\core\\libnpymath.a.p\" \"-Inumpy\\core\" \"-I..\\..\\numpy\\core\" \"-Inumpy\\core\\include\" \"-I..\\..\\numpy\\core\\include\" \"-I..\\..\\numpy\\core\\src\\npymath\" \"-I..\\..\\numpy\\core\\src\\common\" \"-IC:\\DE-Python\\Include\" \"-ID:\\Script_VMC\\numpy-1.26.1\\.mesonpy-okitud_x\\build\\meson_cpu\" \"-DNDEBUG\" \"\/MD\" \"\/nologo\" \"\/showIncludes\" \"\/utf-8\" \"\/W2\" \"\/Qstd:c99\" \"\/O3\" \"\/arch:SSE3\" \"-DNPY_HAVE_SSE2\" \"-DNPY_HAVE_SSE\" \"-DNPY_HAVE_SSE3\" \"-DMS_WIN64=\" \"\/Fdnumpy\\core\\libnpymath.a.p\\meson-generated_ieee754.c.pdb\" \/Fonumpy\/core\/libnpymath.a.p\/meson-generated_ieee754.c.obj \"\/c\" numpy\/core\/libnpymath.a.p\/ieee754.c\r\n      icl: remark #10441: The Intel(R) C++ Compiler Classic (ICC) is deprecated and will be removed from product release in the second half of 2023. The Intel(R) oneAPI DPC++\/C++ Compiler (ICX) is the recommended compiler moving forward. Please transition to use this compiler. Use '\/Qdiag-disable:10441' to disable this message.\r\n      ..\/..\/numpy\/core\/include\/numpy\/npy_common.h(388): catastrophic error: #error directive: npy_cdouble definition is not compatible with C99 complex definition !         Please contact NumPy maintainers and give detailed information about your         compiler and platform\r\n        #error npy_cdouble definition is not compatible with C99 complex definition ! \\\r\n         ^\r\n\r\n      compilation aborted for numpy\/core\/libnpymath.a.p\/ieee754.c (code 4)\r\n```","comments":["The relevant code is: https:\/\/github.com\/numpy\/numpy\/blob\/6207a5203983e720cd36f49a39ba8ea3433c3231\/numpy\/core\/include\/numpy\/npy_common.h#L387-L392\r\n\r\n@ashish-2022 would you mind providing a full build log and the build command you used? Also, this code has changed a lot in the `main` branch, so if you could check if you are getting the same error there, that would be helpful.\r\n","> The relevant code is:\r\n> \r\n> https:\/\/github.com\/numpy\/numpy\/blob\/6207a5203983e720cd36f49a39ba8ea3433c3231\/numpy\/core\/include\/numpy\/npy_common.h#L387-L392\r\n> \r\n> @ashish-2022 would you mind providing a full build log and the build command you used? Also, this code has changed a lot in the `main` branch, so if you could check if you are getting the same error there, that would be helpful.\r\n\r\nOk I'll try to build from latest code on GitHUB","Complete build log(code taken from https:\/\/files.pythonhosted.org\/packages\/78\/23\/f78fd8311e0f710fe1d065d50b92ce0057fe877b8ed7fd41b28ad6865bfc\/numpy-1.26.1.tar.gz)\r\n\r\nLog Attached.\r\n[numpy_1_26_1_build_log.txt](https:\/\/github.com\/numpy\/numpy\/files\/13206269\/numpy_1_26_1_build_log.txt)\r\n\r\nCommand used to build:\r\n`python -m pip install . `\r\n\r\n","Code from **main** branch exited much earlier on my setup at following check:\r\n\r\n```\r\nHas header \"complex.h\" : YES\r\n      Checking for type \"_Fcomplex\" : NO\r\n\r\n      ..\\numpy\\_core\\meson.build:143:4: ERROR: Problem encountered: \"complex.h\" header does not include complex type _Fcomplex\r\n\r\n      A full log can be found at D:\\Test_Numpy\\numpy\\.mesonpy-gt7tmyk6\\meson-logs\\meson-log.txt\r\n      [end of output]\r\n```\r\nFull log here:\r\n[build_log_main.txt](https:\/\/github.com\/numpy\/numpy\/files\/13206669\/build_log_main.txt)\r\n","Hmm, I assume this happens due to https:\/\/github.com\/numpy\/numpy\/blob\/cdfbdf428d9df9c7119cecae323512a4cd3f57b7\/numpy\/_core\/meson.build#L127 We should probably special-case Intel there, right?","> Hmm, I assume this happens due to\r\n> \r\n> https:\/\/github.com\/numpy\/numpy\/blob\/cdfbdf428d9df9c7119cecae323512a4cd3f57b7\/numpy\/_core\/meson.build#L127\r\n> \r\n> We should probably special-case Intel there, right?\r\n\r\nThis can be achieved using `compiler.get_id()`, this is the list: https:\/\/mesonbuild.com\/Reference-tables.html#compiler-ids","@ashish-2022 Can you check whether you can build numpy with #25044?","Hi @lysnikolaou, I had tried just putting that condition in meson.build but we will need more changes related to intel compiler complex types.\r\n\r\n```\r\n[9\/491] Compiling C++ object numpy\/_core\/libnpymath.a.p\/src_npymath_halffloat.cpp.obj\r\n      FAILED: numpy\/_core\/libnpymath.a.p\/src_npymath_halffloat.cpp.obj\r\n      \"icl\" \"-Inumpy\\_core\\libnpymath.a.p\" \"-Inumpy\\_core\" \"-I..\\numpy\\_core\" \"-Inumpy\\_core\\include\" \"-I..\\numpy\\_core\\include\" \"-I..\\numpy\\_core\\src\\npymath\" \"-I..\\numpy\\_core\\src\\common\" \"-IC:\\DE-Python\\Include\" \"-ID:\\Test_Numpy\\numpy\\.mesonpy-q6_9ehof\\meson_cpu\" \"-DNDEBUG\" \"\/MD\" \"\/nologo\" \"\/showIncludes\" \"\/utf-8\" \"\/W2\" \"\/EHsc\" \"\/std:c++17\" \"\/permissive-\" \"\/O3\" \"\/arch:SSE3\" \"-DNPY_HAVE_SSE2\" \"-DNPY_HAVE_SSE\" \"-DNPY_HAVE_SSE3\" \"-DMS_WIN64=\" \"\/Fdnumpy\\_core\\libnpymath.a.p\\src_npymath_halffloat.cpp.pdb\" \/Fonumpy\/_core\/libnpymath.a.p\/src_npymath_halffloat.cpp.obj \"\/c\" ..\/numpy\/_core\/src\/npymath\/halffloat.cpp\r\n      icl: remark #10441: The Intel(R) C++ Compiler Classic (ICC) is deprecated and will be removed from product release in the second half of 2023. The Intel(R) oneAPI DPC++\/C++ Compiler (ICX) is the recommended compiler moving forward. Please transition to use this compiler. Use '\/Qdiag-disable:10441' to disable this message.\r\n      C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\windows\\compiler\\include\\complex.h(37): warning #1224: #warning directive: \"The \/Qstd=c99 compilation option is required to enable C99 support for C programs\"\r\n                    #warning \"The \/Qstd=c99 compilation option is required to enable C99 support for C programs\"\r\n                     ^\r\n\r\n      ..\/numpy\/_core\/include\/numpy\/npy_common.h(397): error: expected a \";\"\r\n        typedef double _Complex npy_cdouble;\r\n```\r\nSee full log here: \r\n[intel_icl_log.txt](https:\/\/github.com\/numpy\/numpy\/files\/13219804\/intel_icl_log.txt)\r\n\r\nThen I also included \"\/Qstd=c99\" flag in compilation but there was no use. need to check with some intel expert @rgommers ","Unfortunately I don't have much experience with Intel and also no Windows machine to reproduce this issue at the moment, so I'll leave this to someone else.","From commits history following members can assign this to right person: @mattip, @chaoyu3","As the intel compiler is proprietary, we would need someone with a proper license to work through this. I do not have such a license.","It looks like at least the old intel compiler doesn't like `_Complex` in C++ (on windows?).  My suspicion would be that needs to be tweaked, half of the branching necessary is already there.  Can't we rely on a standard form in C++ mode in all cases?\r\n\r\nIn either case, it needs someone with the compiler to try out what works well here.","@mattip that is no longer the case. Well, it's proprietary still, but freely available - you can simple download and install it, no license required. ","The installers are quite cumbersome indeed. You can deselect most components though, so it shouldn't take that much space. That said, this is still a bit of a time sink, and this isn't super urgent probably. I'm planning to get a Windows laptop at the end of the month to be able to deal with this kind of thing more easily. So I can circle back to it then - before 2.0 at least.","Somehow the installer works differently via a remote desktop, it seems to work better with direct access. ","The latest oneAPI compiler suite I installed came with `icx.exe` and not `icl.exe`. I needed to do this to build:\r\n```\r\n<install path>\\setvars.bat intel64\r\nset CC=icx.exe\r\nset CXX=icx.exe\r\npip install scipy-openblas64 -r build_requirements.txt -r test_requirements.txt\r\n\r\nREM use scipy-openblas wheels to provide OpenBLAS\r\npip install scipy-openblas64\r\nset PKG_CONFIG_PATH=<path>\\.openblas\r\n\r\npip install . --no-build-isolation\r\n```\r\n\r\nOn HEAD I get the same error as in [the comment above](https:\/\/github.com\/numpy\/numpy\/issues\/25034#issuecomment-1785428596) in the compiler check stage. I am starting to work through the needed changes. Many of the warnings come from `icx.exe` being a clang-based c++ compiler which is used to compile c code. ","Hi @mattip, This is how I was building earlier version's of numpy 1.24.3 with intel MKL:\r\n\r\n```\r\n:: ------------------------------------------------------------------------------\r\n:: Compile and install numpy\r\n:: ------------------------------------------------------------------------------\r\n:: Download numpy\r\npowershell Invoke-WebRequest -Uri 'https:\/\/files.pythonhosted.org\/packages\/2c\/d4\/590ae7df5044465cc9fa2db152ae12468694d62d952b1528ecff328ef7fc\/numpy-1.24.3.tar.gz' -OutFile 'numpy-1.24.3.tar.gz'\r\ntar -xvzf numpy-1.24.3.tar.gz\r\nset NUMPY_DIR=\"numpy-1.24.3\"\r\n:: Create site.cfg\r\necho [mkl] > %NUMPY_DIR%\/site.cfg\r\necho include_dirs = C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\include >> %NUMPY_DIR%\/site.cfg\r\necho library_dirs = C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\lib\\intel64 >>%NUMPY_DIR%\/site.cfg\r\necho libraries = mkl_rt >> %NUMPY_DIR%\/site.cfg\r\n:: Start build\r\npython -m pip install --verbose %NUMPY_DIR%\/\r\n:: Copy intel MKL libs for numpy\r\nmkdir \"%PYTHONHOME%\\Lib\\site-packages\\numpy\\.libs\"\r\ncopy \"C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\redist\\intel64\\*.dll\" %PYTHONHOME%\\Lib\\site-packages\\numpy\\.libs\r\nrobocopy \"C:\\Program Files (x86)\\Intel\\oneAPI\\mkl\\latest\\redist\\intel64\\1033\" %PYTHONHOME%\\Lib\\site-packages\\numpy\\.libs\\1033\r\n```\r\nPlease note it required to link to intel MKL libs from site-packages\/numpy\/.libs\/\r\n\r\nThere was some code I had seen long back for this (dont remember where it is, may be in one of __init__.py)","Right, but we no longer use `site.cfg`","There are many warnings from CPython about a missing declaration of `struct timeval`. This was fixed for CPython 3.12, so I would recommend using that for compilation with the oneAPI compiler (or clang-cl for that matter). See python\/cpython#82863 and python\/cpython#77532","There are also many warnings about \"warning: explicit comparison with NaN in fast floating point mode\". It seems meson should use at least `-fhonor-infinities -fhonor-nans` according to [this issue in another repo](https:\/\/github.com\/mfem\/mfem\/issues\/3655)","We should be disabling `-ffast-math` for that on Intel compilers. It's missing in NumPy; I did do it for SciPy:\r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/blob\/ae026f72fbc3b6c60a5d5b9d2e392b5c1919fd44\/meson.build#L91-L117","OK. I will add that to #25044"],"labels":["32 - Installation"]},{"title":"ENH: Improved support for typing with __array_function__ usage","body":"### Proposed new feature or change:\n\nI'm not sure if this is a bug (it is intended to work already) or a feature request (it isn't expected to work). For context I made an issue with the xarray library and it seemed like a good idea to move that discussion here.\r\n\r\nhttps:\/\/github.com\/pydata\/xarray\/issues\/8388\r\n\r\nBottom line is I would like mypy to understand the typing annotations and \"generic\"-ness of the below:\r\n\r\n```python\r\nimport xarray as xr\r\nimport numpy as np\r\n\r\ndef compute_relative_azimuth(sat_azi: xr.DataArray, sun_azi: xr.DataArray) -> xr.DataArray:\r\n    abs_diff = np.absolute(sun_azi - sat_azi)\r\n    ssadiff = np.minimum(abs_diff, 360 - abs_diff)\r\n    return ssadiff\r\n\r\n```\r\n\r\n```bash\r\n$ mypy .\/xarray_mypy.py\r\nxarray_mypy.py:7: error: Incompatible return value type (got \"ndarray[Any, dtype[Any]]\", expected \"DataArray\")  [return-value]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\nI'm wondering if this would work if this below overload definition was swapped with the `_SupportsArrayUFunc` overload below it:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/eae0b8bc7cebda2c0e32c0552bda21ff64cf990f\/numpy\/_typing\/_ufunc.pyi#L95-L107","comments":[],"labels":["Static typing"]},{"title":"BUG: np.random.Generator(MyBitGenerator).random() segfaults every time","body":"### Describe the issue:\n\nI was trying to implement my own random generator but encountered a crash when trying to use it with a Generator. Mine is based on CFFI so that was a suspect but the simplified test case below crashes any machine where I try it even though it doesn't do anything dangerous.\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nclass FakeRandom(np.random.BitGenerator):\r\n    def __init__(self, seed=None):\r\n        super().__init__(seed)\r\n\r\n    def random_raw(self, size=None):\r\n        return 0\r\n\r\n    def spawn(self, n):\r\n        raise NotImplementedError\r\n\r\nbg = FakeRandom()\r\ngen = np.random.Generator(bg)\r\nprint(gen)    # Prints Generator(FakeRandom)\r\ngen.random()  # SIGSEGV\n```\n\n\n### Error message:\n\n```shell\nTraceback:\r\n\r\n#0  0x0000000000000000 in ?? ()\r\n#1  0x00007fffa5fa6867 in random_standard_uniform_fill ()\r\n   from \/home\/user\/.local\/share\/hatch\/env\/virtual\/randquik\/45ZJMGs_\/randquik\/lib\/python3.10\/site-packages\/numpy\/random\/_generator.cpython-310-x86_64-linux-gnu.so\r\n#2  0x00007fffa61d08d8 in __pyx_f_5numpy_6random_7_common_double_fill ()\r\n   from \/home\/user\/.local\/share\/hatch\/env\/virtual\/randquik\/45ZJMGs_\/randquik\/lib\/python3.10\/site-packages\/numpy\/random\/_common.cpython-310-x86_64-linux-gnu.so\r\n#3  0x00007fffa5f805f7 in __pyx_pw_5numpy_6random_10_generator_9Generator_15random ()\r\n   from \/home\/user\/.local\/share\/hatch\/env\/virtual\/randquik\/45ZJMGs_\/randquik\/lib\/python3.10\/site-packages\/numpy\/random\/_generator.cpython-310-x86_64-linux-gnu.so\r\n#4  0x00005555556d4dde in ?? ()\r\n#5  0x000055555569cf52 in _PyEval_EvalFrameDefault ()\r\n```\n```\n\n\n### Runtime information:\n\nPython 3.10.12, WSL2, Numpy 1.26.1 - SIGSEGV\r\nPython 3.11.0, Windows, Numpy 1.24.1 - just terminates with no error message\r\nPython 3.11.2, Linux, Numpy 1.25.2 - SIGSEGV\r\n\n\n### Context for the issue:\n\nMy random generator is of better quality and runs faster than MT or PCG64 (which are also very fast in Numpy implementation).\r\n","comments":["We don't expect a pure Python subclass of `np.random.BitGenerator` to work (the `bitgen_t* _bitgen` structure needs to be filled in at the C level), so the crash is probably unrelated to your CFFI implementation crash. We can't do anything to help with this amount of information.","Okay, so BitGenerators can only be implemented against the C API? In that case I need to take another approach. Didn't see that mentioned on the docs.\r\n\r\nAlso, it still shouldn't randomly segfault with Python code, need to check those NULL pointers...\r\n","Other than the simplified test case above, if anyone is interested on the context, this was my approach today: https:\/\/github.com\/LeoVasanko\/RandQuik\/blob\/main\/randquik\/nprand.py","https:\/\/numpy.org\/doc\/1.26\/reference\/random\/extending.html#new-bit-generators","There should be a simple bufix, though: `BitGenerator` should have the `@cython.final` decorator to prevent subclassing in Python.","A pretty steep learning curve given I hadn't used Cython before this day, but I got it working. https:\/\/github.com\/LeoVasanko\/RandQuik\/blob\/main\/src\/nprand.pyx","Maybe worth adding to the [tutorials](https:\/\/numpy.org\/numpy-tutorials\/)?","I am only able to reach slightly faster performance than PCG64 because the API is so slow. Asking for 64 bits at a time - releasing GIL and all that for it - is a problem. Much more time is spent on function calls than on actual generation. If the bit generation API is ever going to be developed, I would suggest ditching the different data types produced and instead letting a bit generator simply fill a buffer of bytes (of large size when a large amount of randomness is requested).\r\n\r\nIdeally a buffer would be supplied by caller, avoiding memcpy for large amounts of data.\r\n\r\nHaving to produce uint32 and double as well in bit generator seems odd. Distributions should take care of that instead.\r\n\r\nGranted, some generators that use modulo other than 2**8n have a disadvantage with a byte-based API. But then, I would argue all the legacy generators can be replaced with ChaCha which has higher performance and better random numbers.\r\n","> I am only able to reach slightly faster performance than PCG64 because the API is so slow.\r\n\r\n[SFC64 reaches noticeably (but not stunningly) faster performance than PCG64](https:\/\/numpy.org\/doc\/1.26\/reference\/random\/performance.html), even with the same API overheads.\r\n\r\n> Asking for 64 bits at a time - releasing GIL and all that for it - is a problem.\r\n\r\nI'm not sure what you mean. We generally [release the GIL once](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/random\/_bounded_integers.pyx.in#L365-L366) and call the `next_uint64` and other `bitgen_t` functions many times when we fill large arrays.\r\n\r\n> If the bit generation API is ever going to be developed, I would suggest ditching the different data types produced and instead letting a bit generator simply fill a buffer of bytes (of large size when a large amount of randomness is requested).\r\n\r\nThe problem is that filling a buffer with bytes is a very rare use case for `np.random`. That's not a case we care to optimize for. While there are some bulk operations that could be written in the form of \"fill any array of bulk bytes, then do operations on that array\", most of the interesting things in the `Generator` API can't be written that way (or would be too awkward to code that way for the performance gain we would get out of it). The C function pointer call overhead of each `next_uint64` is not insignificant, but in most codepaths is one of quite a few function calls already.\r\n\r\n> Having to produce uint32 and double as well in bit generator seems odd. Distributions should take care of that instead.\r\n\r\nDifferent PRNGs have different native outputs. `MT19937` outputs `uint32` words while `PCG64` and the other newer `BitGenerator`s natively output `uint64`. While most implementations should reuse [the common utility function](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/random\/_common.pxd#L68-L69) for converting `uint64` draws to `double`s, there are a handful of PRNGs out there that natively output float-point numbers or a prime-sized integer range. Because each of these 3 functions are easy enough to implement from the whichever is native (and also because new `BitGenerator`s are rare), we did decide to have `BitGenerator`s implement all 3 in whatever way is the most well-adapted to the algorithm's native output. If it were harder to do or the task more common, we might have chosen otherwise.","> [SFC64 reaches noticeably (but not stunningly) faster performance than PCG64](https:\/\/numpy.org\/doc\/1.26\/reference\/random\/performance.html), even with the same API overheads.\r\n\r\nA quick set of benchmarks, using `bitgen.random_raw(size=134217728)` to generate 1 GiB of randomness. A no-op of `return 4;` takes 230ms, SFC64 takes 290ms, PCG64 450ms and my implementation 435ms. As opposed to 135ms generating into a Python buffer without Numpy.Random, avoiding internal buffer & memcpy too.\r\n\r\nFor perspective, `Generator(any of them).normal(size=134217728)` takes 1.1 seconds.\r\n\r\nI guess the overhead is quite reasonable for this usage, and SFC64 indeed is extremely fast.\r\n"],"labels":["00 - Bug"]},{"title":"API: Allow weak promotion (NEP 50) as the only option","body":"This is a start of a cleanup, removing only the most obvious unused code chunks\/paths.  The main reasoning for doing this, is that it allows replacing further legacy code paths.\r\nIt would also make some experiments such as not converting incoming scalars to an array right away, feasible.\r\nSome such experiments\/optimizations are not possible as long as we need full legacy path support.\r\n\r\nI would like to go with this because it is a big cleanup and allows further cleanups.  It *does* however mean that one has to rely on numpy 1.26 if you want the \"warning\" option to get a sense of how much breakage there is or track down a change (and in some additional niche cases the warnings may be off with the final result due to fixes not backported).\r\n\r\n----\r\n\r\n(There is also a lot more related code, that is still relevant for niche public API Functions.)","comments":[],"labels":["30 - API"]},{"title":"BUG: Eigenvectors generated via linalg.eigh are not orthonormal in v1.26.0","body":"### Describe the issue:\r\n\r\nHi, \r\n\r\nWhile computing the eigenvectors of Hermitian matrices using numpy.linalg.eigh the eigenvectors are not coming out to be orthonormal. The eigenvectors are orthonormal using the scipy.linalg.eigh routine.\r\n\r\nA jupyter notebook with a simple example is attached.\r\n\r\n[Compare_diag.pdf](https:\/\/github.com\/numpy\/numpy\/files\/13173924\/Compare_diag.pdf)\r\n\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy as sp\r\n\r\ndef create_random_hermitian_mat(dim):\r\n    a = np.random.random((dim,dim)) + 1j*np.random.random((dim,dim))\r\n    b = np.conj(a).T + a\r\n    return b\r\n\r\na = create_random_hermitian_mat(4)\r\nu, v = np.linalg.eigh(a)\r\nprint(\"Numpy Results = \\n\", np.matmul(np.conj(v).T, v))\r\nu_sp, v_sp = sp.linalg.eigh(a)\r\nprint(\"\\n Scipy results = \\n\", np.matmul(np.conj(v_sp).T, v_sp))\r\n```\r\n\r\nOutput:\r\n```\r\nNumpy Results = \r\n[[ 2.   +0.j    0.   +0.j    -0.   -0.j    1.586+1.586j]\r\n[ 0.   +0.j    2.   +0.j    0.   +0.j    -0.651-0.651j]\r\n[-0.   +0.j    0.   -0.j    2.   +0.j    -1.027-1.027j]\r\n[1.586-1.586j -0.651+0.651j -1.027+1.027j  5.   +0.j   ]]\r\n\r\nScipy Results =\r\n[[ 1.+0.j -0.-0.j -0.-0.j  0.-0.j]\r\n[-0.+0.j  1.+0.j  0.+0.j -0.+0.j]\r\n[-0.+0.j  0.-0.j  1.+0.j  0.+0.j]\r\n[ 0.+0.j -0.-0.j  0.-0.j  1.+0.j]]\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n\r\nNumpy version. \r\n```\r\n'1.26.0'\r\n```\r\n\r\nNumpy configuration\r\n```\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/include\r\n    lib directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\r\n    name: blas\r\n    pc file directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/include\r\n    lib directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\r\n    name: lapack\r\n    pc file directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\/pkgconfig\r\n    version: 3.9.0\r\nCompilers:\r\n  c:\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  c++:\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 3.0.2\r\nMachine Information:\r\n  build:\r\n    cpu: aarch64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: arm64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/bin\/python\r\n  version: '3.11'\r\nSIMD Extensions:\r\n  baseline:\r\n  - NEON\r\n  - NEON_FP16\r\n  - NEON_VFPV4\r\n  - ASIMD\r\n  found:\r\n  - ASIMDHP\r\n  not found:\r\n  - ASIMDFHM\r\n\r\n```\r\n\r\nScipy Version\r\n```\r\n'1.11.3'\r\n```\r\n\r\n\r\nScipy Configuration\r\n```\r\nBuild Dependencies:\r\n  blas:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/include\r\n    lib directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\r\n    name: blas\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  lapack:\r\n    detection method: pkgconfig\r\n    found: true\r\n    include directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/include\r\n    lib directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\r\n    name: lapack\r\n    openblas configuration: unknown\r\n    pc file directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/lib\/pkgconfig\r\n    version: 3.9.0\r\n  pybind11:\r\n    detection method: pkgconfig\r\n    include directory: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/include\r\n    name: pybind11\r\n    version: 2.11.1\r\nCompilers:\r\n  c:\r\n    commands: arm64-apple-darwin20.0.0-clang\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  c++:\r\n    commands: arm64-apple-darwin20.0.0-clang++\r\n    linker: ld64\r\n    name: clang\r\n    version: 15.0.7\r\n  cython:\r\n    commands: cython\r\n    linker: cython\r\n    name: cython\r\n    version: 0.29.36\r\n  fortran:\r\n    commands: \/Users\/runner\/miniforge3\/conda-bld\/scipy-split_1696467662374\/_build_env\/bin\/arm64-apple-darwin20.0.0-gfortran\r\n    linker: ld64\r\n    name: gcc\r\n    version: 12.3.0\r\n  pythran:\r\n    include directory: ..\/..\/_build_env\/venv\/lib\/python3.11\/site-packages\/pythran\r\n    version: 0.14.0\r\nMachine Information:\r\n  build:\r\n    cpu: x86_64\r\n    endian: little\r\n    family: x86_64\r\n    system: darwin\r\n  cross-compiled: true\r\n  host:\r\n    cpu: arm64\r\n    endian: little\r\n    family: aarch64\r\n    system: darwin\r\nPython Information:\r\n  path: \/Users\/shinjan\/Programs\/miniforge3\/envs\/scicomp\/bin\/python\r\n  version: '3.11'\r\n\r\n```\r\n\r\n\r\n### Context for the issue:\r\n\r\nErroneous eigenvectors from numpy.linalg.eigh ","comments":["How did you install numpy and what does `np.show_runtime()` say?  `show_config()` is not really useful.\r\nOr just `threadpoolctl.threadpool_info()` after importing everything (you must install threadpoolctl in both cases to get anything useful).","Output for `np.show_runtime()`\r\n```\r\n[{'numpy_version': '1.26.0',\r\n  'python': '3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:37:07) '\r\n            '[Clang 15.0.7 ]',\r\n  'uname': uname_result(system='Darwin', node='Shinjans-MacBook-Air.local', release='23.0.0', version='Darwin Kernel Version 23.0.0: Fri Sep 15 14:42:57 PDT 2023; root:xnu-10002.1.13~1\/RELEASE_ARM64_T8112', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP'],\r\n                      'not_found': ['ASIMDFHM']}}]\r\nNone\r\n```\r\n\r\nOutput for `threadpoolctl.threadpool_info()`\r\n```\r\n[]\r\n```\r\n\r\nI installed the `numpy` library via `conda` using:\r\n```\r\nconda create -n qtm python=3.11 numpy \"libblas=*=*accelerate\"\r\n```\r\nand the `scipy` library using:\r\n```\r\nconda install -c conda-forge scipy \r\n```\r\n\r\nI wanted to use the vecLib framework for Apple, which is supposed to be optimum for Apple Silicon. ","Did you import numpy and scipy.linalg before the `threadpoolctl.thradpool_info()`?  Might be that it doesn't show anything for accelerate, so you would have to inspect tihngs closer.  In that case it seems like this might be an accelerate issue.","I cannot reproduce this on arm64 macOS 14, with Accelerate built from source I get:\r\n```\r\narray([[ 1.00000000e+00+0.00000000e+00j, -3.33066907e-16-6.93889390e-17j,\r\n         2.77555756e-16+2.77555756e-17j,  1.04083409e-16+4.16333634e-17j],\r\n       [-3.33066907e-16+6.93889390e-17j,  1.00000000e+00+0.00000000e+00j,\r\n        -1.31838984e-16+2.08166817e-16j, -1.42247325e-16+1.90819582e-16j],\r\n       [ 2.77555756e-16-2.77555756e-17j, -1.31838984e-16-2.08166817e-16j,\r\n         1.00000000e+00+0.00000000e+00j,  1.04083409e-16+1.52655666e-16j],\r\n       [ 1.04083409e-16-4.16333634e-17j, -1.42247325e-16-1.90819582e-16j,\r\n         1.04083409e-16-1.52655666e-16j,  1.00000000e+00+0.00000000e+00j]])\r\n```\r\n\r\nSurely this is also tested in our test suite.\r\n\r\n`threadpoolctl` indeed doesn't recognize new Accelerate, I'll follow up on that. EDIT: see https:\/\/github.com\/joblib\/threadpoolctl\/issues\/135\r\n\r\n> `conda create -n qtm python=3.11 numpy \"libblas=*=*accelerate\"`\r\n\r\nThis is not the new Accelerate, but only old Accelerate BLAS plus Netlib LAPACK routines. That config doesn't get tested even in conda-forge CI, so I would avoid using it.","> This is not the new Accelerate, but only old Accelerate BLAS plus Netlib LAPACK routines. That config doesn't get tested even in conda-forge CI, so I would avoid using it.\r\n\r\nHow do I get the latest Accelerate via conda-forge? \r\n\r\n> Surely this is also tested in our test suite. \r\n\r\nThe test suite is also reporting errors. Should I attach the entire error report here?","You can't - it's an open feature request for conda-forge, I linked to it above. The only way to get new Accelerate right now is to build from source.\r\n\r\n> The test suite is also reporting errors. Should I attach the entire error report here?\r\n\r\nMay be useful for future reference. It's unlike we can fix things though."],"labels":["00 - Bug"]},{"title":"BUG: np.cross is annotated as \"NoReturn\", despite returning an array. This results in IDE errors (amongst other issues).","body":"### Describe the issue:\n\nThe np.cross override has a NoReturn return annotation, see [here](https:\/\/github.com\/numpy\/numpy\/blob\/382eedf3891d474196677e65f3aae066afb32c5f\/numpy\/_core\/numeric.pyi#L506).\r\n\r\nThis results in the following bug in PyCharm, which I have also submitted: https:\/\/youtrack.jetbrains.com\/issue\/PY-63674\n\n### Reproduce the code example:\n\n```python\ncentre_pos_abs_mm_xyz = centre_pos_rel_mm - pos_bot_slice_top_left_xyz + offset_xyz_mm\r\ncentre_pos_abs_mm_zyx = tuple(np.flip(centre_pos_abs_mm_xyz))\r\n\r\n# ImageOrientationPatient -> Normal & Rotation\r\nnormal_xyz = np.cross(dir_right, dir_down)\r\nnormal_zyx = tuple(np.flip(normal_xyz)\n```\n\n\n### Error message:\n\n```shell\nThe IDE thinks the line with \"np.cross\" is unreachable (please see screenshots in the JetBrain bug tracking link above).\n```\n\n\n### Runtime information:\n\n1.23.3\r\n3.9.12 (main, Apr  5 2022, 06:56:58)\r\n[GCC 7.5.0]\n\n### Context for the issue:\n\nI cannot do any code refactoring on lines that follow from np.cross, linting fails etc.","comments":["The `NoReturn` overload is correct, as `np.cross` does not support operations between two boolean arrays which is reflected here by the second overload. We've had similar issues involving pyright before, hence the addition of the first overload involving `_ArrayLikeUnknown` as a safety net. And yet... pycharm ignores it anyway? And somehow ends up settling for a random overload that has `NoReturn` and treats that as the \"correct\" one here? I'm struggling a bit to see the logic here and would very much consider this a pycharm bug."],"labels":["00 - Bug","Static typing"]},{"title":"Linting Package Replacement: Pycodestyle to Ruff","body":"### Proposed new feature or change:\n\nThere is a newer package that has been implemented by other major packages such as FastAPI and Llama index which has overwhelming performance gains. I would like to make the change to Numpy if that's okay?\r\n\r\nMore info:\r\nhttps:\/\/docs.astral.sh\/ruff\/","comments":["Hi @Luca-Blight, thanks for proposing to work on this. If it's a simple swap of tools, then yes please go for it - `ruff` is certainly an upgrade. If it requires significant code changes throughout the code base (e.g. because `ruff` doesn't run on the same diff as right now), then it'd be better to avoid this until after the 2.0 release, given that we have a lot more in flight than during an average release cycle.\r\n\r\n","@rgommers  got it I will keep that in mind.  Thanks for your response.","I did a little experiment and ran `pycodestyle` without the `diff` option and the results are not too good.\r\n\r\n```\r\n7       E101 indentation contains mixed spaces and tabs\r\n13      E111 indentation is not a multiple of 4\r\n9       E114 indentation is not a multiple of 4 (comment)\r\n9       E116 unexpected indentation (comment)\r\n11      E117 over-indented (comment)\r\n58      E124 closing bracket does not match visual indentation\r\n3       E129 visually indented line with same indent as next logical line\r\n5       E131 continuation line unaligned for hanging indent\r\n334     E201 whitespace after '['\r\n57      E202 whitespace before ']'\r\n301     E203 whitespace before ':'\r\n30      E211 whitespace before '('\r\n200     E221 multiple spaces before operator\r\n11      E222 multiple spaces after operator\r\n187     E225 missing whitespace around operator\r\n2       E227 missing whitespace around bitwise or shift operator\r\n8       E228 missing whitespace around modulo operator\r\n1163    E231 missing whitespace after ','\r\n90      E261 at least two spaces before inline comment\r\n13      E262 inline comment should start with '# '\r\n10      E271 multiple spaces after keyword\r\n13      E272 multiple spaces before keyword\r\n98      E301 expected 1 blank line, found 0\r\n83      E303 too many blank lines (2)\r\n2       E304 blank lines found after function decorator\r\n137     E305 expected 2 blank lines after class or function definition, found 1\r\n52      E306 expected 1 blank line before a nested definition, found 0\r\n6       E401 multiple imports on one line\r\n2308    E501 line too long (90 > 79 characters)\r\n53      E502 the backslash is redundant between brackets\r\n73      E701 multiple statements on one line (colon)\r\n6       E702 multiple statements on one line (semicolon)\r\n1       E703 statement ends with a semicolon\r\n21      E711 comparison to None should be 'if cond is None:'\r\n14      E713 test for membership should be 'not in'\r\n1       E714 test for object identity should be 'is not'\r\n2       E722 do not use bare 'except'\r\n1       E743 ambiguous function definition 'I'\r\n6       W191 indentation contains tabs\r\n3       W292 no newline at end of file\r\n107     W293 blank line contains whitespace (I enabled this rule, it was ignored before)\r\n```\r\n\r\nIt would be a good amount of effort to solve all of these, but I think we should do it. A good plan would be to first replace `pycodestyle` with `ruff`, which includes `pycodestyle` rules amongst others, fix all errors\/warnings (ruff can probably autofix quite a few of these), and, finally, change the CI to lint the whole code base on every run, instead of just the diff. Ruff is fast, so this shouldn't be a problem.\r\n\r\n@Luca-Blight Are you still planning to work on this?","> A good plan would be to first replace pycodestyle with ruff, which includes pycodestyle rules amongst others, fix all errors\/warnings (ruff can probably autofix quite a few of these)\r\n\r\nThat's 2004 errors, with 789 \"auto-fixable\" (not sure how safe the `F401` fixes would be). We ignore `E741` in SciPy, so you can remove 82 from that. Doable? But certainly a bit of work.\r\n\r\n```\r\n> ruff check . --exclude vendored-meson --exclude numpy\/typing\/tests\/data --exclude numpy\/typing\/_char_codes.py --exclude numpy\/__config__.py --exclude numpy\/f2py --statistics\r\n\r\n725\tF401\t[*] `.._utils._inspect.formatargspec` imported but unused\r\n348\tF405\t[ ] `ALLOW_THREADS` may be undefined, or defined from star imports\r\n219\tF811\t[*] Redefinition of unused `Sequence` from line 1\r\n133\tF821\t[ ] Undefined name `Bool`\r\n 92\tE402\t[ ] Module level import not at top of file\r\n 82\tE741\t[ ] Ambiguous variable name: `I`\r\n 75\tF841\t[*] Local variable `A` is assigned to but never used\r\n 73\tE701\t[ ] Multiple statements on one line (colon)\r\n 54\tE712\t[*] Comparison to `False` should be `cond is False` or `if not cond:`\r\n 51\tE731\t[*] Do not assign a `lambda` expression, use a `def`\r\n 46\tE721\t[ ] Do not compare types, use `isinstance()`\r\n 27\tF403\t[ ] `from ._asarray import *` used; unable to detect undefined names\r\n 21\tE711\t[*] Comparison to `None` should be `cond is None`\r\n 17\tE713\t[*] Test for membership should be `not in`\r\n 14\tF541\t[*] f-string without any placeholders\r\n 12\tE714\t[*] Test for object identity should be `is not`\r\n  6\tE702\t[ ] Multiple statements on one line (semicolon)\r\n  5\tE401\t[*] Multiple imports on one line\r\n  2\tE743\t[ ] Ambiguous function name: `I`\r\n  1\tE703\t[*] Statement ends with an unnecessary semicolon\r\n  1\tE722\t[ ] Do not use bare `except`\r\n```\r\n"],"labels":["16 - Development"]},{"title":"ENH: Added ``coverage``, ``gcov`` and ``generate-lcov-html`` flags in ``spin test``","body":"### Changes\r\n- Added `coverage` flag in `test`\r\n\r\n### Example\r\n```\r\n$ spin test --coverage -- -v -k numpy\/tests\/test_numpy_config.py \r\nRemoving `\/workspaces\/numpy\/build\/coverage`\r\nInvoking `build` prior to running tests:\r\n\r\n.\r\n.\r\n.\r\n\r\n------------------------------------------------------------------------------\r\nTOTAL                                                     109351  13626    88%\r\nCoverage HTML written to dir \/workspaces\/numpy\/build\/coverage\r\n```\r\n\r\nIn Spin, after merging of https:\/\/github.com\/scientific-python\/spin\/pull\/146:\r\n\r\n![image](https:\/\/github.com\/scientific-python\/spin\/assets\/20969920\/1ed0887f-3875-4846-a9ae-47ad5aab39c4)\r\n\r\n1. `spin test --coverage --gcov`\r\n2. `spin test --generate-lcov-html`\r\n```\r\n~\/os\/numpy (bld_24080_coverage) \u00bb spin test --generate-lcov-html                                                                                                            ganesh@ganesh-MS-7B86\r\nDeleting old HTML coverage report...\r\nGenerating HTML coverage report...\r\n$ ninja coverage-html -C build\r\nCoverage report generated successfully and written to \/home\/ganesh\/os\/numpy\/build\/meson-logs\/coveragereport\/index.html\r\n```\r\n\r\n---\r\n\r\nOther error handling\r\n1. `spin build`\r\n2. `spin test --generate-lcov-html`\r\n```\r\n~\/os\/numpy (bld_24080_coverage) \u00bb spin test --generate-lcov-html                                                                                                            ganesh@ganesh-MS-7B86\r\nError: GCOV files missing... Cannot generate coverage reports. Coverage reports can be generated by `spin test --coverage --gcov`\r\n\r\n```\r\n\r\n---\r\n\r\n\r\n### Notes\r\nrelated #24080, https:\/\/github.com\/scientific-python\/spin\/pull\/100\r\ndepends on https:\/\/github.com\/scientific-python\/spin\/pull\/146","comments":["Hi @ganesh-k13, any reason to not simply let the coverage flag get passed through to the built-in command?","Ah yes, that's possible. But I was thinking from `runtest.py` perspective and migrate all it's functionalities. ","Hey @ganesh-k13 do you have any followup for this? I'd like to bring it in and also add support for generating C coverage via `gcov`.","Hey @ngoldbaum , I'd be happy to add `gcov` integration to spin if there is a need for it. I'll get working on it tonight IST. Thanks for following up on this PR.","That'd be really useful. It would be nice to be able to generate a coverage report locally for both C and python.","CI builds will fail till https:\/\/github.com\/scientific-python\/spin\/pull\/146 is merged and released, I'll move it ready for review so it does not go stale.","> Hi @ganesh-k13, any reason to not simply let the coverage flag get passed through to the built-in command?\r\n\r\nOhh I understood this statement now @stefanv . I have made the changes to reflect that via \r\nc805f94."],"labels":["01 - Enhancement"]},{"title":"ENH: Create obvious way of getting a single 1-D list with all array elements","body":"### Proposed new feature or change:\r\n\r\n@rgommers This has been discussed several times:\r\n- https:\/\/github.com\/numpy\/numpy\/issues\/16243\r\n- https:\/\/github.com\/pytorch\/pytorch\/issues\/52262\r\n\r\nI propose to review it once again to potentially introduce a breaking change or some solution for this in NumPy 2.0 or Array API. It's most of time inconvenient to have the return type change as a function of numel() - usually we do some array processing after and having conditions complicates the consuming code.\r\n\r\nMaybe a new argument `force=True` argument can be introduced with existing behavior `force=False` as default, and `atleast1d().tolist()` behavior if `force=True` is passed. OR maybe a new method `aslist()` can be introduced and `tolist()` be deprecated\/produce-warnings and removed in a few years.","comments":["I browsed gh-16243 and I don't find the reasoning for not returning a list compelling:\r\n\r\n> They need to be different because of the intent that `array(x.tolist()) == x` (note there are exceptions to this intent already, in cases like `np.zeros((0, 2))` where it is impossible).\r\n\r\nIt seems pretty obvious to me that a method named `tolist` _must_ return a list. It's not really a problem if a 0-D array and 1-D array with 1 element return the same thing - which they don't right now:\r\n```python\r\n>>> np.array([42]).tolist()\r\n[42]\r\n>>> np.array(42).tolist()\r\n42\r\n>>> np.int64(42).tolist()\r\n42\r\n```\r\n\r\nThere are also several TODO's in `numpy\/__init__.pyi`, because the current behavior is a problem for static typing. And we decided to, in principle, fix up that type of non-type-stable behavior in the discussions for 2.0.\r\n\r\nThis wasn't really on my radar, but I think it'd be a reasonable change to make for 2.0 (always returning a list that is, not `force=True`), that won't have a significant backwards compatibility impact.\r\n\r\nI wasn't involved in the previous decision though, and I may be missing something. So pinging @eric-wieser and @seberg for input here.\r\n\r\nAlso, we are pretty short on time for all the plans\/wishes for 2.0. If the outcome is that we want to do this, would you be willing to implement it @vadimkantorov?","Yes, it is mild misnomer, but no, I am absolutely not in favor of changing this.\r\n> It's not really a problem if a 0-D array and 1-D array with 1 element return the same thing\r\n\r\nAnd what about N-D arrays?  I suspect this request comes from users mixing 1-D or 0-D arrays for whatever reason, if they were having 2-D arrays in there we would have another round of discussion and frustration.\r\nSo changing this makes one use case (mixed 1-D and 0-D arrays) clearer, while breaking an unknown amount of use-cases where dimensionality matters.\r\n\r\nI would say a `tolits()` that implicitly ravels makes sense and probably would be the better function to begin with, but fuzzying a correct dimensionality generalization isn't helpful IMO.\r\n\r\nYou can use `arr.ravel().tolist()` or `list(arr.flat)`, or... (the `.tolist()` path might be slightly faster).","> And what about N-D arrays? I suspect this request comes from users mixing 1-D or 0-D arrays for whatever reason, if they were having 2-D arrays in there we would have another round of discussion and frustration.\r\n\r\nWhy? That returns a list just fine, nothing changes:\r\n\r\n```python\r\n>>> x = np.ones((3, 2))\r\n>>> l = x.tolist()\r\n>>> type(l)\r\n<class 'list'>\r\n>>> l\r\n[[1.0, 1.0], [1.0, 1.0], [1.0, 1.0]]\r\n```","The point is that we return a nested list of depth `N`, the reason that `N=0` is special there, is because the user has only 1-D arrays otherwise, I am sure.  Not because 0 is really all that special.","I'm not at python right now, but I think our `tolist` is consistent with `memoryview.tolist` on 0D arrays (and I definitely still think that `array(1).tolist()` should *not* be `[1]`!)","Okay, I interpreted the request as `type(x.tolist()` should be `list`, nothing beyond that. @vadimkantorov can you clarify? What will you do with it, and do you use >=2-D arrays?","The simple reason is, that returning a list breaks consistency quite badly, and that is bound to bite someone else. Most likely worse than then the mild convenience added for the original person here.\r\n\r\nThere *is* a simple solution IMO, and I have already said that, add one or both of:\r\n* `arr.flat.tolist()`\r\n* `arr.tolist(raveled=True)`\r\n\r\n(It is the job of the user to know the want to ravel higher dimensional arrays then, but OK.)","Also it seems like this was trigger of the current iteration: https:\/\/github.com\/pytorch\/pytorch\/pull\/104908#discussion_r1258935569 where you are dealing with N dimensions anyway and already must flatten.  All additionally added checks are unnecessary (unless torch gets `flatten()` wrong)."],"labels":["01 - Enhancement","62 - Python API"]},{"title":"ENH: expose numpy.dtype.flags bitmasks in numpy.dtypes","body":"### Proposed new feature or change:\r\n\r\ncurrently the [numpy.dtype.flags](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.dtype.flags.html) bitmasks are exposed in numpy._core.multiarray\r\n\r\nthe flag numpy._core.multiarray.LIST_PICKLE is [used by distributed](https:\/\/github.com\/dask\/distributed\/blob\/7ea3bff22897f46b0cea340ec47c67c407e5b12a\/distributed\/protocol\/numpy.py#L25) to determine how best to pickle an ndarray so it would be great if there were a public namespace to access these flags, eg `numpy.dtypes.LIST_PICKLE`","comments":["We had discussed that a bit on slack.  A neat way would also be to make the flags return an `enum.IntFlags`.  The class of which might fit into `np.dtypes` (better repr and reduces the spam in the module)","Another way to solve this without exposing the flags in python would be to add a member hanging off the dtype object that indicates if it's supposed to be pickled for serialization (that internally would do the check against the flags).","It is pretty niche, but the property hanging off the dtype object is also pretty easy to implement - just add a new function that does the flag check and add an entry for it in the `arraydescr_getsets` array in `numpy\/_core\/src\/multiarray\/descriptor.c`.\r\n\r\nExposing all the flags to python would also work, and I guess we already expose the flags too so exposing the whole enum probably makes sense, just wanted to comment that the property is also not very hard to add.","Yes, a proprety is fine, I would like to not spoil the properties too much with things users will never really need, but overall it is not a big issue."],"labels":["01 - Enhancement","Numpy 2.0 API Changes"]},{"title":"ENH: Add lower\/upper ufuncs for unicode and bytes dtypes","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["I'm converting this to a draft, since `Py_UNICODE_TOUPPER`\/`LOWER` are deprecated (and buggy), but there's currently no other way to do this with the C API. I'll wait to hear from the team in https:\/\/github.com\/python\/cpython\/issues\/76535."],"labels":["01 - Enhancement"]},{"title":"WIP, BLD: PYTHONSAFEPATH support","body":"* Fixes #24907\r\n\r\n* marked as WIP (and CI skipped) for now because this is ugly and we could also just decide not to fix it at all, and\/or to error out when that env var is set with a clear error message\r\n \r\n* we can avoid `PYTHONSAFEPATH` causing our build to choke on the fact that the NumPy build requires Python code to execute an \"unsafe\" import from its own build dir\r\n\r\n* here, the chosen approach is to perform the import \"manually,\" but we could also decide to refuse to build if that environment variable is set for example, depending on our view on the matter\r\n\r\n[ci skip] [skip cirrus]","comments":["Maybe the fix is rather embrace the module nature, there already is an `__init__.py`!  I.e. change the calling to:\r\n```\r\npython -m code_generators.generate_numpy_api ...\r\n```\r\nAnd fix the imports to use `from . import genapi`.  I bet that will also fix that crazy `get_annotations()` function need in `numpy_api.py`."],"labels":["25 - WIP","36 - Build"]},{"title":"ENH: Drop requirement of stack arrays to be a sequence","body":"### Proposed new feature or change:\n\nCurrently _stack_dispatcher enforces that arrays argument of np.stack is a sequence with __get_item__.\r\nimho this restriction is not required, since first line of np.stack is \r\n`arrays = [asanyarray(arr) for arr in arrays]`\r\n\r\nthanks","comments":["Possible, but it was deprecated and removed for a reason:\r\n```\r\nThe `__array_function__` API currently will exhaust iterators so we\r\ncannot accept iterables reasonably. [...]\r\n\r\nFuture changes could allow this again, although it is not a useful API\r\nanyway, since we have to materialize the iterable in any case.\r\n```\r\n\r\nSo you would have to find a solution for not exhausting it twice."],"labels":["01 - Enhancement"]},{"title":"DOC: license text clogs pip show command","body":"### Issue with current documentation:\n\nNot sure if that's documentation, but it kind of is.\r\nSo when you call `pip show numpy`, you are supposed to see a brief overview of what the package is about, right?\r\n\r\nHere's how it was for 1.23.0 (and still is for many other packages):\r\n```\r\npip show numpy\r\nName: numpy\r\nVersion: 1.23.0\r\nSummary: NumPy is the fundamental package for array computing with Python.\r\nHome-page: https:\/\/www.numpy.org\r\nAuthor: Travis E. Oliphant et al.\r\nAuthor-email:\r\nLicense: BSD\r\nLocation: c:\\anaconda3\\envs\\converterenv\\lib\\site-packages\r\nRequires:\r\nRequired-by: contourpy, coremltools, h5py, imageio, Keras-Preprocessing, matplotlib, onnx, onnxruntime, onnxruntime-gpu, opencv-python, opt-einsum, pandas, PyWavelets, scikit-image\r\n, scikit-learn, scipy, tensorboard, tensorflow, tensorflow-hub, tensorflow-model-optimization, tf2onnx, tifffile, torchvision\r\n```\r\n\r\nWell from some point in time (I did not bother to check all the way down, but it works in 1.26.0) the WHOLE license is included, for some reason. Here's the snipped of what it looks like:\r\n```\r\nName: numpy\r\nVersion: 1.26.0\r\nSummary: Fundamental package for array computing in Python\r\nHome-page: https:\/\/numpy.org\r\nAuthor: Travis E. Oliphant et al.\r\nAuthor-email:\r\nLicense: Copyright (c) 2005-2023, NumPy Developers.\r\nAll rights reserved.\r\n\r\nRedistribution and use in source and binary forms, with or without\r\nmodification, are permitted provided that the following conditions are\r\nmet:\r\n\/~a thousand lines of the actual license go here~\/\r\nLocation: C:\\Anaconda3\\envs\\Py11\\Lib\\site-packages\r\nRequires:\r\nRequired-by: coremltools, h5py, ml-dtypes, onnx, onnxruntime-gpu, opencv-python, opt-einsum, scikit-learn, scipy, tensorboard, tensorflow-hub, tensorflow-intel, tf2onnx\r\n```\r\n\r\nSo this actually clogs the terminal output quite a lot.\r\nI'm pretty sure that this line causes the issue: https:\/\/github.com\/numpy\/numpy\/blob\/67539a40cb13bad56a650809bf10a49e905a250d\/pyproject.toml#L22\r\n\r\nBut that has to be verified.\n\n### Idea or request for content:\n\nI would be glad to see a brief summary of the License instead of the actual text. Similar to how it was before.","comments":["Thanks @DLumi. The request makes perfect sense, but we're waiting for [PEP 639](https:\/\/peps.python.org\/pep-0639\/) to finally be finished and supported by PyPI, so we can show the SPDX license expression. Right now the support for non-`distutils` build backends is insufficient in this regard; PEP 621 wasn't implemented by PyPI.","I guess, that makes sense then.\r\nBut this answer does not make it less sad to use, though. Until the PEP 639 is implemented by PyPI that is. \r\nAnyway, thanks for clarification."],"labels":["04 - Documentation"]},{"title":"BUG: failed to build with `PYTHONSAFEPATH=1` environment variable","body":"### Describe the issue:\r\n\r\nFailed to build numpy 1.26.0 or main branch with `PYTHONSAFEPATH=1` environment variable.\r\n\r\nPlease refer to: https:\/\/docs.python.org\/3\/using\/cmdline.html#envvar-PYTHONSAFEPATH\r\n\r\n### Reproduce the code example:\r\n\r\n```shell\r\nexport PYTHONSAFEPATH=1\r\npip wheel --no-binary numpy numpy==1.26.0\r\n# or main branch\r\npip wheel --no-binary numpy git+https:\/\/github.com\/numpy\/numpy\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/.mesonpy-2dh000d4\/build\/..\/..\/numpy\/core\/code_generators\/generate_ufunc_api.py\", line 4, in <module>\r\n    import genapi\r\nModuleNotFoundError: No module named 'genapi'\r\n```\r\n\r\n\r\n<details>\r\n\r\n<summary>full meson log<\/summary>\r\n\r\n```shell\r\nThe Meson build system\r\nVersion: 1.2.99\r\nSource dir: \/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\r\nBuild dir: \/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/.mesonpy-2dh000d4\/build\r\nBuild type: native build\r\nProject name: NumPy\r\nProject version: 1.26.0\r\nC compiler for the host machine: cc (gcc 13.2.1 \"cc (GCC) 13.2.1 20230801\")\r\nC linker for the host machine: cc ld.bfd 2.41.0\r\nC++ compiler for the host machine: c++ (gcc 13.2.1 \"c++ (GCC) 13.2.1 20230801\")\r\nC++ linker for the host machine: c++ ld.bfd 2.41.0\r\nCython compiler for the host machine: cython (cython 3.0.3)\r\nHost machine cpu family: x86_64\r\nHost machine cpu: x86_64\r\nProgram python found: YES (\/usr\/bin\/python)\r\nFound pkg-config: \/usr\/bin\/pkg-config (1.8.1)\r\nRun-time dependency python found: YES 3.11\r\nHas header \"Python.h\" with dependency python-3.11: YES\r\nCompiler for C supports arguments -fno-strict-aliasing: YES\r\nTest features \"SSE SSE2 SSE3\" : Supported\r\nTest features \"SSSE3\" : Supported\r\nTest features \"SSE41\" : Supported\r\nTest features \"POPCNT\" : Supported\r\nTest features \"SSE42\" : Supported\r\nTest features \"AVX\" : Supported\r\nTest features \"F16C\" : Supported\r\nTest features \"FMA3\" : Supported\r\nTest features \"AVX2\" : Supported\r\nTest features \"AVX512F\" : Supported\r\nTest features \"AVX512CD\" : Supported\r\nTest features \"AVX512_KNL\" : Supported\r\nTest features \"AVX512_KNM\" : Supported\r\nTest features \"AVX512_SKX\" : Supported\r\nTest features \"AVX512_CLX\" : Supported\r\nTest features \"AVX512_CNL\" : Supported\r\nTest features \"AVX512_ICL\" : Supported\r\nTest features \"AVX512_SPR\" : Supported\r\nConfiguring npy_cpu_dispatch_config.h using configuration\r\nMessage:\r\nCPU Optimization Options\r\n  baseline:\r\n    Requested : min\r\n    Enabled   : SSE SSE2 SSE3\r\n  dispatch:\r\n    Requested : max -xop -fma4\r\n    Enabled   : SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_KNL AVX512_KNM AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL AVX512_SPR\r\n\r\nLibrary m found: YES\r\nRun-time dependency openblas found: YES\r\nChecking if \"CBLAS\" with dependency openblas: links: YES\r\nDependency openblas found: YES unknown (cached)\r\nProgram _build_utils\/process_src_template.py found: YES (\/usr\/bin\/python \/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/numpy\/_build_utils\/process_src_template.py)\r\nProgram _build_utils\/tempita.py found: YES (\/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/numpy\/_build_utils\/tempita.py)\r\nConfiguring __config__.py using configuration\r\nChecking for size of \"short\" : 2\r\nChecking for size of \"int\" : 4\r\nChecking for size of \"long\" : 8\r\nChecking for size of \"long long\" : 8\r\nChecking for size of \"float\" : 4\r\nChecking for size of \"double\" : 8\r\nChecking for size of \"long double\" : 16\r\nChecking for size of \"off_t\" : 8\r\nChecking for size of \"Py_intptr_t\" with dependency python-3.11: 8\r\nChecking for size of \"PY_LONG_LONG\" with dependency python-3.11: 8\r\nHas header \"complex.h\" : YES\r\nChecking for type \"complex float\" : YES\r\nChecking for size of \"struct {float __x; float __y;}\" : 8\r\nChecking for type \"complex double\" : YES\r\nChecking for size of \"struct {double __x; double __y;}\" : 16\r\nChecking for type \"complex long double\" : YES\r\nChecking for size of \"struct {long double __x; long double __y;}\" : 32\r\nChecking for function \"sin\" : YES\r\nChecking for function \"cos\" : YES\r\nChecking for function \"tan\" : YES\r\nChecking for function \"sinh\" : YES\r\nChecking for function \"cosh\" : YES\r\nChecking for function \"tanh\" : YES\r\nChecking for function \"fabs\" : YES\r\nChecking for function \"floor\" : YES\r\nChecking for function \"ceil\" : YES\r\nChecking for function \"sqrt\" : YES\r\nChecking for function \"log10\" : YES\r\nChecking for function \"log\" : YES\r\nChecking for function \"exp\" : YES\r\nChecking for function \"asin\" : YES\r\nChecking for function \"acos\" : YES\r\nChecking for function \"atan\" : YES\r\nChecking for function \"fmod\" : YES\r\nChecking for function \"modf\" : YES\r\nChecking for function \"frexp\" : YES\r\nChecking for function \"ldexp\" : YES\r\nChecking for function \"expm1\" : YES\r\nChecking for function \"log1p\" : YES\r\nChecking for function \"acosh\" : YES\r\nChecking for function \"asinh\" : YES\r\nChecking for function \"atanh\" : YES\r\nChecking for function \"rint\" : YES\r\nChecking for function \"trunc\" : YES\r\nChecking for function \"exp2\" : YES\r\nChecking for function \"copysign\" : YES\r\nChecking for function \"nextafter\" : YES\r\nChecking for function \"strtoll\" : YES\r\nChecking for function \"strtoull\" : YES\r\nChecking for function \"cbrt\" : YES\r\nChecking for function \"log2\" : YES\r\nChecking for function \"pow\" : YES\r\nChecking for function \"hypot\" : YES\r\nChecking for function \"atan2\" : YES\r\nChecking for function \"csin\" : YES\r\nChecking for function \"csinh\" : YES\r\nChecking for function \"ccos\" : YES\r\nChecking for function \"ccosh\" : YES\r\nChecking for function \"ctan\" : YES\r\nChecking for function \"ctanh\" : YES\r\nChecking for function \"creal\" : YES\r\nChecking for function \"cimag\" : YES\r\nChecking for function \"conj\" : YES\r\nChecking for function \"cabs\" : YES\r\nChecking for function \"cabsf\" : YES\r\nChecking for function \"cabsl\" : YES\r\nChecking for function \"cacos\" : YES\r\nChecking for function \"cacosf\" : YES\r\nChecking for function \"cacosl\" : YES\r\nChecking for function \"cacosh\" : YES\r\nChecking for function \"cacoshf\" : YES\r\nChecking for function \"cacoshl\" : YES\r\nChecking for function \"carg\" : YES\r\nChecking for function \"cargf\" : YES\r\nChecking for function \"cargl\" : YES\r\nChecking for function \"casin\" : YES\r\nChecking for function \"casinf\" : YES\r\nChecking for function \"casinl\" : YES\r\nChecking for function \"casinh\" : YES\r\nChecking for function \"casinhf\" : YES\r\nChecking for function \"casinhl\" : YES\r\nChecking for function \"catan\" : YES\r\nChecking for function \"catanf\" : YES\r\nChecking for function \"catanl\" : YES\r\nChecking for function \"catanh\" : YES\r\nChecking for function \"catanhf\" : YES\r\nChecking for function \"catanhl\" : YES\r\nChecking for function \"cexp\" : YES\r\nChecking for function \"cexpf\" : YES\r\nChecking for function \"cexpl\" : YES\r\nChecking for function \"clog\" : YES\r\nChecking for function \"clogf\" : YES\r\nChecking for function \"clogl\" : YES\r\nChecking for function \"cpow\" : YES\r\nChecking for function \"cpowf\" : YES\r\nChecking for function \"cpowl\" : YES\r\nChecking for function \"csqrt\" : YES\r\nChecking for function \"csqrtf\" : YES\r\nChecking for function \"csqrtl\" : YES\r\nChecking for function \"csin\" : YES (cached)\r\nChecking for function \"csinf\" : YES\r\nChecking for function \"csinl\" : YES\r\nChecking for function \"csinh\" : YES (cached)\r\nChecking for function \"csinhf\" : YES\r\nChecking for function \"csinhl\" : YES\r\nChecking for function \"ccos\" : YES (cached)\r\nChecking for function \"ccosf\" : YES\r\nChecking for function \"ccosl\" : YES\r\nChecking for function \"ccosh\" : YES (cached)\r\nChecking for function \"ccoshf\" : YES\r\nChecking for function \"ccoshl\" : YES\r\nChecking for function \"ctan\" : YES (cached)\r\nChecking for function \"ctanf\" : YES\r\nChecking for function \"ctanl\" : YES\r\nChecking for function \"ctanh\" : YES (cached)\r\nChecking for function \"ctanhf\" : YES\r\nChecking for function \"ctanhl\" : YES\r\nChecking for function \"isfinite\" : YES\r\nHeader \"Python.h\" has symbol \"isfinite\" with dependency python-3.11: YES\r\nChecking for function \"isinf\" : YES\r\nHeader \"Python.h\" has symbol \"isinf\" with dependency python-3.11: YES\r\nChecking for function \"isnan\" : YES\r\nHeader \"Python.h\" has symbol \"isnan\" with dependency python-3.11: YES\r\nChecking for function \"signbit\" : YES\r\nHeader \"Python.h\" has symbol \"signbit\" with dependency python-3.11: YES\r\nChecking for function \"fallocate\" : YES\r\nHeader \"Python.h\" has symbol \"HAVE_FTELLO\" with dependency python-3.11: YES\r\nHeader \"Python.h\" has symbol \"HAVE_FSEEKO\" with dependency python-3.11: YES\r\nChecking for function \"strtold_l\" : NO\r\nChecking for function \"strtold_l\" : NO\r\nChecking for function \"backtrace\" : YES\r\nChecking for function \"madvise\" : YES\r\nHas header \"features.h\" : YES\r\nHas header \"xlocale.h\" : NO\r\nHas header \"dlfcn.h\" : YES\r\nHas header \"execinfo.h\" : YES\r\nHas header \"libunwind.h\" : YES\r\nHas header \"sys\/mman.h\" : YES\r\nCompiler for C supports arguments -O3: YES\r\nHas header \"endian.h\" : YES\r\nHas header \"sys\/endian.h\" : NO\r\nHeader \"inttypes.h\" has symbol \"PRIdPTR\" : YES\r\nCompiler for C supports function attribute visibility:hidden: YES\r\nConfiguring config.h using configuration\r\nConfiguring _numpyconfig.h using configuration\r\nConfiguring npymath.ini using configuration\r\nConfiguring mlib.ini using configuration\r\nGenerating multi-targets for \"_umath_tests.dispatch.h\"\r\n  Enabled targets: AVX2, SSE41, baseline\r\nGenerating multi-targets for \"argfunc.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX2, SSE42, baseline\r\nGenerating multi-targets for \"simd_qsort.dispatch.h\"\r\n  Enabled targets: AVX512_SKX\r\nGenerating multi-targets for \"simd_qsort_16bit.dispatch.h\"\r\n  Enabled targets: AVX512_SPR, AVX512_ICL\r\nGenerating multi-targets for \"loops_arithm_fp.dispatch.h\"\r\n  Enabled targets: FMA3__AVX2, baseline\r\nGenerating multi-targets for \"loops_arithmetic.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX512F, AVX2, SSE41, baseline\r\nGenerating multi-targets for \"loops_comparison.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX512F, AVX2, SSE42, baseline\r\nGenerating multi-targets for \"loops_exponent_log.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX512F, FMA3__AVX2, baseline\r\nGenerating multi-targets for \"loops_hyperbolic.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, FMA3__AVX2, baseline\r\nGenerating multi-targets for \"loops_logical.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX2, baseline\r\nGenerating multi-targets for \"loops_minmax.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX2, baseline\r\nGenerating multi-targets for \"loops_modulo.dispatch.h\"\r\n  Enabled targets: baseline\r\nGenerating multi-targets for \"loops_trigonometric.dispatch.h\"\r\n  Enabled targets: AVX512F, FMA3__AVX2, baseline\r\nGenerating multi-targets for \"loops_umath_fp.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, baseline\r\nGenerating multi-targets for \"loops_unary.dispatch.h\"\r\n  Enabled targets: AVX512_SKX, AVX2, baseline\r\nGenerating multi-targets for \"loops_unary_fp.dispatch.h\"\r\n  Enabled targets: SSE41, baseline\r\nGenerating multi-targets for \"loops_unary_fp_le.dispatch.h\"\r\n  Enabled targets: SSE41, baseline\r\nGenerating multi-targets for \"loops_unary_complex.dispatch.h\"\r\n  Enabled targets: AVX512F, FMA3__AVX2, baseline\r\nGenerating multi-targets for \"loops_autovec.dispatch.h\"\r\n  Enabled targets: AVX2, baseline\r\nGenerating multi-targets for \"_simd.dispatch.h\"\r\n  Enabled targets: SSE42, AVX2, FMA3, FMA3__AVX2, AVX512F, AVX512_SKX, baseline\r\nBuild targets in project: 101\r\n\r\nNumPy 1.26.0\r\n\r\n  User defined options\r\n    Native files: \/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/.mesonpy-2dh000d4\/build\/meson-python-native-file.ini\r\n    buildtype   : release\r\n    b_ndebug    : if-release\r\n    b_vscrt     : md\r\n\r\nFound ninja-1.11.1 at \/usr\/bin\/ninja\r\n+ \/usr\/bin\/ninja\r\n[1\/498] Generating numpy\/core\/npy_math_internal.h with a custom command\r\n[2\/498] Generating numpy\/__init__.pxd with a custom command\r\n[3\/498] Generating 'numpy\/core\/libnpymath.a.p\/ieee754.c'\r\n[4\/498] Generating numpy\/__init__.py with a custom command\r\n[5\/498] Generating numpy\/__init__.cython-30.pxd with a custom command\r\n[6\/498] Generating 'numpy\/core\/libnpymath.a.p\/npy_math_complex.c'\r\n[7\/498] Generating numpy\/core\/_umath_doc_generated with a custom command\r\n[8\/498] Generating numpy\/core\/__umath_generated with a custom command\r\n[9\/498] Generating numpy\/core\/__ufunc_api with a custom command\r\nFAILED: numpy\/core\/__ufunc_api.c numpy\/core\/__ufunc_api.h\r\n\/usr\/bin\/python ..\/..\/numpy\/core\/code_generators\/generate_ufunc_api.py -o numpy\/core\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/.mesonpy-2dh000d4\/build\/..\/..\/numpy\/core\/code_generators\/generate_ufunc_api.py\", line 4, in <module>\r\n    import genapi\r\nModuleNotFoundError: No module named 'genapi'\r\n[10\/498] Generating numpy\/core\/__multiarray_api with a custom command\r\nFAILED: numpy\/core\/__multiarray_api.c numpy\/core\/__multiarray_api.h\r\n\/usr\/bin\/python ..\/..\/numpy\/core\/code_generators\/generate_numpy_api.py -o numpy\/core --ignore numpy\/core\/__umath_generated.c\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/pip-wheel-vdqy4dbr\/numpy_1f47512b2a0a4ad3899813685753144f\/.mesonpy-2dh000d4\/build\/..\/..\/numpy\/core\/code_generators\/generate_numpy_api.py\", line 5, in <module>\r\n    import genapi\r\nModuleNotFoundError: No module named 'genapi'\r\n[11\/498] Compiling C object numpy\/core\/libnpymath.a.p\/meson-generated_ieee754.c.o\r\n[12\/498] Compiling C object numpy\/core\/libnpymath.a.p\/src_npymath_npy_math.c.o\r\n[13\/498] Compiling C object numpy\/core\/libnpymath.a.p\/meson-generated_npy_math_complex.c.o\r\n[14\/498] Compiling C++ object numpy\/core\/libnpymath.a.p\/src_npymath_halffloat.cpp.o\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n<\/details>\r\n\r\n### Runtime information:\r\n\r\n```\r\n>>> print(sys.version)\r\n3.11.5 (main, Sep  2 2023, 14:16:33) [GCC 13.2.1 20230801]\r\n```\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["I added an example shim that fixes the issue in the cross-linked PR, but I wouldn't be surprised if the NumPy core team prefers not to fix it at all, or if they decided to just error out with a clear error message in this scenario.","Hmm never seen that option before, seems like a good way to break a bunch of stuff. One could of course manually amend `sys.path` as done in @tylerjereddy's linked PR, but that kinda defeats the purpose perhaps. The \"modern way\" would be to import the needed module through `importlib` APIs, but that's very verbose and clumsy.\r\n\r\n@GalaxySnail is there a reason you want or need to use that `PYTHONSAFEPATH` option?","> is there a reason you want or need to use that `PYTHONSAFEPATH` option?\r\n\r\nIt's because the current directory or the parent directory of the script in `sys.path` is a vulnerability, which can lead to arbitrary code execution. For details: [1] [Never Run \u2018python\u2019 In Your Downloads Folder](https:\/\/blog.glyph.im\/2020\/08\/never-run-python-in-your-downloads-folder.html), [2] https:\/\/github.com\/python\/cpython\/issues\/57684\r\n\r\n>  The \"modern way\" would be to import the needed module through `importlib` APIs, but that's very verbose and clumsy.\r\n\r\nSince [`code_generators`](https:\/\/github.com\/numpy\/numpy\/tree\/d885b0b546cbe2ab8401622c8d4db984ec5ea31b\/numpy\/_core\/code_generators) is already a package, relative import may be helpful.","BTW, some projects are using `-I` option ([isolated mode](https:\/\/docs.python.org\/3\/using\/cmdline.html#cmdoption-I)) to run commands in CI, which implies `-P` option."],"labels":["00 - Bug"]},{"title":"BUG: `genfromtxt` silently returns nan whenever reading complex values","body":"### Describe the issue:\r\n\r\nWhenever reading complex numbers using `genfromtxt`, numpy silently returns `nan`.\r\n\r\nI've seen similar issues discussed in other places (e.g. https:\/\/github.com\/numpy\/numpy\/issues\/7895),\r\nbut I haven't found any issue for this specific case.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndata = np.array([0 + 0j, 1 + 1j])\r\nnp.savetxt(outfile := \".\/array_in_file.txt\", data)\r\nloaded_data = np.genfromtxt(outfile)\r\n\r\nprint(data)\r\nprint(loaded_data)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n[0.+0.j 1.+1.j]\r\n[nan nan]\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.26.0\r\n3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\r\n\r\n>>> print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.26.0',\r\n  'python': '3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]',\r\n  'uname': uname_result(system='Linux', node='pop-os', release='6.2.6-76060206-generic', version='#202303130630~1683753207~22.04~77c1465 SMP PREEMPT_DYNAMIC Wed M', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n\r\n\r\n### Context for the issue:\r\n\r\nThe current behavior is to silently parse a \"numerical value\" as a `nan`.\r\nI understand this is the default case for other non-numerical values such as strings,\r\nbut I would have expected complex numbers to behave more closely to numbers.\r\n\r\nI'm unaware of the complexity of this function, but would it be useful to add a minor\r\nwarning for the case of parsing complex numbers as `nan`?","comments":["For me it's not a bug as numpy should not guess if it's a string or a complex number.\r\n\r\nIt works if you specify the data type with **dtype='cfloat'**\r\n\r\n```\r\nimport numpy as np\r\n\r\ndata = np.array([0 + 0j, 1 + 1j])\r\nnp.savetxt(outfile := \".\/array_in_file.txt\", data)\r\nloaded_data = np.genfromtxt(outfile,dtype='cfloat')\r\n\r\nprint(data)\r\nprint(loaded_data)\r\n```\r\n\r\nreturns:\r\n```\r\n[0.+0.j 1.+1.j]\r\n[0.+0.j 1.+1.j]\r\n```","> For me it's not a bug as numpy should not guess if it's a string or a complex number.\r\n\r\nI agree, ~though in this case the dtype-determining logic lands on float which is arguably also not correct. Perhaps a better solution would be to raise, but I'm not sure how difficult that is without breaking the rest of the dtype-guessing logic.~\r\n\r\nEdit: the stricken text was based on `dtype=None` being the default, which is not true.","Hello, I think I found the issue. The `dtype` parameter is set to `float` by default: https:\/\/github.com\/numpy\/numpy\/blob\/18c6157f5da5736575b6d8d492aacca3b5d69551\/numpy\/lib\/_npyio_impl.py#L1713\r\n\r\nWhen I call the above code with `dtype=None`, `genfromtxt` correctly determines the complex type. The type detection also works with float. So, I see no reason why `float` is the default here. Additionally, this is also documented: \"If None, the dtypes will be determined by the contents of column, individually.\"\r\n\r\nI propose to change the default of the parameter to `None`. What do you think?\r\nIt would be very nice if I could provide a fix.","Ok - my comment above was based on an incorrect assumption that the default dtype for `genfromtext` was `dtype=None`. As pointed out in #25301, that is not the case. Given this, I don't think the issue raised in the OP is a bug --- the default `dtype` is `float` so genfromtext is doing exactly what is requested: trying to interpreted the input tokens as floats. It is the `loose=True` (also the default) that results in the NaN's. This is also the correct behavior.\r\n\r\nI'm inclined to close this issue. We can open a separate one about changing the default dtype parameter to `None`, but AFAICT this is not a bug."],"labels":["00 - Bug","57 - Close?"]},{"title":"ENH: Check for increasing monotonicity in interp","body":"### Proposed new feature or change:\n\nWhile the documentation does state that the `xp` to `np.interp` must be monotonically increasing and that if they are decreasing the results are meaningless, the user is not warned if a monotonically decreasing series is provided. It would be trivial to check this and at least warn the user. The failure mode often produces reasonable but erroneous results so it often results in bugs. I have hit this several times while using `np.interp`, especially as it's MATLAB equivalent of `interp1` does not require it to be monotonically decreasing. \r\n\r\nAn even better solution would be to detect if `xp` is monotonically decreasing and then just flip it and do the interpolation anyway, removing this unnecessary restriction. ","comments":[],"labels":["01 - Enhancement"]},{"title":"ENH: No longer auto-convert array scalars to numpy scalars in ufuncs (and elsewhere?)","body":"### Proposed new feature or change:\r\n\r\nThis may come too late, but numpy 2.0 may be a good opportunity to remove an irritating wart, that `ufunc(array_scalar_arguments)` returns a numpy scalar rather than an array scalar. There is a very long-standing PR (#14489) that introduces an option to avoid this (using `out=...`), but perhaps it is time to just ditch the conversion to scalar? \r\n\r\nSee gh-13105 for discussion. As a specific example of why it is unhandy, it mentions:\r\n> #13100 raises a case where `np.fix` resorts to calling `np.asanyarray(np.ceil(x, out=out))` in order to ensure that the result is an array. Unfortunately, this has a number of draw-backs:\r\n> \r\n> * It discards duck-types, like `dask` arrays\r\n> * It changes the dtype of 0d object arrays containing array-likes\r\n\r\nThe [first comment](https:\/\/github.com\/numpy\/numpy\/issues\/13105#issuecomment-471382946) on the issue by @rgommers suggests just removing the cast to scalar. To me, this still makes the most sense.\r\n\r\nEDIT: the simplest implementation of this is to not use `PyArray_Return` in ufuncs, or perhaps adjust `PyArray_Return` to remove the try of converting to scalars altogether. ","comments":["**Reposting from gh-14489, not everything aged perfectly:**\r\n\r\nAn alternative thing that would be a big change, but not as big as removing scalars one, would be this: https:\/\/github.com\/seberg\/numpy\/pull\/new\/try-0d-preservation that I tried to explore a bit today.\r\n\r\nThe proposal would be to:\r\n* Ensure that 0-d arrays are preserved in ufuncs, `np.add(0d_array, 2)` will return a 0d array.  (And all other functions, this only has ufuncs right now!)\r\n* Reductions would return scalars if `axis=None`.\r\n* We may or may not need a (internal) helper `arr, wrap = asarray_and_wrap(obj)` that returns a function to do the \"decaying\".\r\n\r\n(The tail of that is not super trivial, quite a few test failures, scipy also relies on scalars in a few places for example, but it looks harmless).  But run that branch with:\r\n```\r\nNPY_NUMPY_2_BEHAVIOR=1\r\n```\r\nto try.  A fun little caveat: `np.matmul(vector, vector)` should be a scalar so is a bit hackish still using the old behavior due to that.  \r\n\r\n---\r\n\r\nI never truly believed that we can pull of *removing* scalars or always converting them to arrays in ufuncs.  But maybe I am just too far on the side of \"change as little as possible\".\r\n\r\nThe alternative is what I say above.  That means making basically scalars something like NumPy duck-types with an implicit `__array_wrap__` that unwraps them, but they have a lower array-priority (one way to make a lot of our Python code be able to retain scalars when they should).\r\n\r\nIn general, am happy with any such thoughts, but really need to focus a bit more on NEP 50 (the overlap isn't that large).\r\nThe note above with `arr, wrap = asarray_and_wrap(obj)` has some overlap, because NEP 50 has the some use in getting the result-type in a python scalar aware way more conveniently.\r\n\r\n---\r\n\r\nIn the end, NumPy 2.0 clearly will slip a while.  But one thing I want to note is that while I would love this to be pushed, I also still think that NEP 50 and a few C-API cleanups should be the main must-have blockers.\r\n(Not that I am against more, but if it turns out they come with tricky downstream fallout, that would make me ask the question:  Isn't it better to do what we must do now, and just wait a few years, as painful as it is?) "],"labels":["Numpy 2.0 API Changes"]},{"title":"DOC: SIMD page refers to outdated Build Report","body":"The [build report](https:\/\/numpy.org\/devdocs\/reference\/simd\/build-options.html#build-report) section should be updated with the more recent meson changes. The `log_example.txt` is no longer relevant.","comments":["@mattip which meson changes to add, the vendored one? And, please assign me this issue after elaborating!","I guess it would be sufficient to build NumPy and then update the section with the results. We don't assign issues. When you submit a pull request, reference this issue like `closes #24865` which will both link the two and close this automatically when that PR is merged."],"labels":["04 - Documentation","component: SIMD"]},{"title":"Failing test with PyPy 3,.9-v7.3.13 on Windows.","body":"3,9-v7.3.13 was released Sept 29 and seems to have made it into azure. \r\n\r\nSee https:\/\/dev.azure.com\/numpy\/numpy\/_build\/results?buildId=32240&view=logs&jobId=95a72139-892c-5f80-4f09-2f9a3201b7ef for the error.\r\n\r\nThe error message is:\r\n```\r\n>                   assert_equal(arr_method(obj), NotImplemented, err_msg)\r\nE                   TypeError: operand 'MyType' does not support ufuncs (__array_ufunc__=None)\r\n```\r\nLooks like PyPy is failing to return `NotImplemented`.","comments":["If possible please leave this open until I find the root cause. ","This looks like one of those things that might be compiler related.","I think it is a true failure in the 7.3.13 version. The other PyPy build (on linux) [pins to the 7.3.12 version](https:\/\/github.com\/numpy\/numpy\/blob\/0f40795103458c0071e751c6d21fdb65bd021c2c\/.github\/workflows\/linux.yml#L75)"],"labels":["24 - PyPy"]},{"title":"ENH: dtype API needs an API slot for a single-element copy","body":"Currently, users will see seg faults if a user-defined dtype is passed somewhere where `copyswap` is used internally to copy array elements.\r\n\r\nIf we require that user-defined dtypes implement a single-element copy operation we will be able to implement a default copyswap implementation for user-defined dtypes which can be used where copyswap is used internally in NumPy.","comments":["There are two slightly different use-cases here.  In most cases, where we need a single element copy, we actually need to copy many, which means preparation is useful\/necessary.  But in some we may not need it.\r\n\r\nThe question is how we deal with allowing preparation (noting that for the majority of dtypes the preparation would be at best specializing for short copies or alignedness and it would be good if NumPy just provides that copy function for those without references at least.).","We should do this, but probably not a disaster if it's not in 2.0, I think."],"labels":["component: numpy.dtype"]},{"title":"ENH, BLD: use wheels to get BLAS implementations","body":"In PR #24839, we began using scipy-openblas wheels in the CI build\/test cycles. In order to use these in wheel building as well, we need to declare a dependency, so that `pip install numpy` in all its variations (build a wheel, build an sdist, install a wheel, install from an sdist) will\r\n- install `scipy-openblas` befor compiling\r\n- ship a modified `_distributor_init.py` (or add a `_distributor_init_local.py`) to import `scipy-openblas` before `numpy`\r\n- find the `scipy-openblas.pc` file when building\r\n- declare a dependency in the metadata of the numpy wheel\/sdist\r\n\r\nThis is tricky since (take from the short discussion starting in [this comment](https:\/\/github.com\/numpy\/numpy\/pull\/24749#discussion_r1332844224)\r\n- There are two variants: scipy-openblas32 and scipy-openblas64. Which to use as the canonical dependency? It seems at least for NumPy this is easy: always use the ILP64 package.\r\n- How to allow changing that choice for a different variant (if not the 32\/64 problem, then say MKL, or Accelerate, or None for platforms where OpenBLAS is not appropriate)?\r\n\r\nWe cannot currently have different metadata in the sdist than we have in the wheel on PyPI. There are some ideas in the draft [PEP 725](https:\/\/peps.python.org\/pep-0725\/). Or perhaps we could have a meta-package `scipy-openblasX`?","comments":["> always use the ILP64 package\r\n\r\nExcept on 32 bit platforms.","> Or perhaps we could have a meta-package scipy-openblasX?\r\n\r\nFrom a naming point of view, `scipy-openblasX` should probably be named `scipy-blas`. Ideally, it could implement a similar mechanism as the \"alternatives\" for BLAS in Debian\/Ubuntu or the virtual blas package of conda-forge:\r\n\r\n- https:\/\/github.com\/conda-forge\/blas-feedstock#about-blas\r\n- https:\/\/conda-forge.org\/docs\/maintainer\/knowledge_base.html#switching-blas-implementation\r\n\r\n>> always use the ILP64 package\r\n\r\n> Except on 32 bit platforms.\r\n\r\nIn particular for WASM \/ Pyodide-based deployments.","@ogrisel that's an interesting idea. It could be added on top later - given that name, I think it'd be an extra level of abstraction, that would only be needed if one would want to make OpenBLAS runtime swappable with another library. I think at that point we should build against FlexiBLAS though, which is already designed to do this and \"forward\" calls to routines to the relevant implementation.\r\n\r\nFor now I think I'd be happy with a plain `scipy-openblas` that would only serve to get around the \"need to declare a dependency\" issue. \r\n\r\n> In particular for WASM \/ Pyodide-based deployments.\r\n\r\nThose don't live on PyPI though - at least not yet. Right now, we have no 32-bit BLAS on PyPI at all; we still have `win32` wheels but they're built without BLAS support.","I did not know about flexiblas but it sounds promising. It might also allow to switch between scipy-openblas-pthreads and scipy-openblas-openmp for instance.","Yes, FlexiBLAS indeed seems quite nice. The two potential issues with it at the moment:\r\n- It doesn't support the `64_` symbol suffix for OpenBLAS, which we're using in our OpenBLAS builds\r\n- `threadpoolctl` may not be able to support it, because using its introspection API makes the GPL apply (see https:\/\/github.com\/scipy\/scipy\/issues\/17362)","> threadpoolctl may not be able to support it, because using its introspection API makes the GPL apply (see https:\/\/github.com\/scipy\/scipy\/issues\/17362)\r\n\r\nThis should not be a blocker (if the owners consider switching the license to LGPL as discussed in the scipy issue). If numpy\/scipy want to rely on FlexiBLAS we will add support for FlexiBLAS to threadpoolctl. I opened an issue to not forget about this.\r\n\r\n- https:\/\/github.com\/joblib\/threadpoolctl\/issues\/154\r\n\r\nFuthermore the latest version of threadpoolctl is pluggable:\r\n\r\n- https:\/\/github.com\/joblib\/threadpoolctl#writing-a-custom-library-controller\r\n\r\nso numpy or scipy can already start prototyping without having to wait for an official release of threadpoolctl with FlexiBLAS support.\r\n\r\n\r\nFor `64_` symbol suffix I have no idea about the complexity about handling it. What are the pros and cons of the `64_` symbols compared to the standard BLAS symbols?","Thanks for the update @ogrisel. I'll note that NumPy now has the option of building against FlexiBLAS, and will try to find it as one of the supported libraries. SciPy doesn't support it yet. It'll be a while before we can consider using it in wheels, due to the suffix issue below.\r\n \r\n> For `64_` symbol suffix I have no idea about the complexity about handling it. What are the pros and cons of the `64_` symbols compared to the standard BLAS symbols?\r\n\r\nOh we definitely want symbol suffixes, because in a stack with numpy and scipy we use a mix of LP64 and ILP64 APIs. There's no good control over that with wheels, and without suffixes it's too easy to get symbol clashes."],"labels":["01 - Enhancement","14 - Release","component: distribution"]},{"title":"NEP 50 adoption related improvements\/followups","body":"**Please don't hesitate to edit this issue to expand\/clarify\/add\/remove items**\r\n\r\nAdopting NEP 50 is the most important part of a NumPy 2.0 release opinion because it consists of larger changes that we cannot possibly via a warning\/deprecation path.  IMO it is also by far the most important part of the idea of being as compatible as possible with the array api.\r\n\r\ngh-23912 contains the maybe largest piece to try and adopt NEP 50 by default.  In the PR, I listed the main things to be addressed\/thought about.  But an issue is better for tracking.\r\n\r\nNote that I don't think I can find solutions and push on these all by myself.  So help is needed.  I also think that these issues have too many moving parts and it is probably impossible to wait for having a fix for all (or even most!) before we push on with removing the \"legacy\" mode!\r\nBecause of that, I really need others to chime in.\r\n\r\n* [ ] `np.can_cast(123, \"int8\")` currently assumes the default integer for the `123` which results in an unexpected result.\r\n  * This is actually a tricky issue!  Although we could special-case `can_cast`.  (I am not sure if `can_cast` fixing is vital or not.\r\n  * [ ] A similar thing happens for `np.add(1., 5.3, dtype=np.intp)` with cast checking (I do *not* consider this a big issue in practice.)\r\n  * The larger fix, may require serious thoughts :(, such as implementing either:\r\n    * Adding the notion of cast-safety to scalar assignment (`setitem`).\r\n    * Adding abstract dtype instances for the abstract DTypes representing Python integers (mainly), which actually know the value and could define a cast safety.  (But this adds a fair bit of complexity for a very specific issue!)\r\n* [ ] Vet as many functions as possible to see if they need to adapt to NEP 50 behavior.  **Anyone can start looking into these!**.  New infrastructure may help, but even `type(x) in (int, float, complex)` is probably an ok start in many cases:\r\n  * The PR has to do some dance to make `percentile` and `quantile` work as expected.\r\n  * Other examples which use `asanyarray()` but should behave ufunc-like w.r.t. to weak promotion:\r\n    * [ ] `isclose()` (requires `asanyarray()` for indexing)\r\n* [x] One big inconvenience is that NEP 50 says that we cast all integers to the other integer type in binary ufuncs, or the default integer in single input one (or if all integers).\r\n  * The main issue\/inconvenience is that `uint8 > -1` currently **does not** work.\r\n  * I suspect fixing this is just a lot harder if you have to juggle more of the compatibility layer.\r\n  * One fix could be a family of comparison functions when python integers are involved (unfortunately, that is a lot of functions!)\r\n* [ ] It would be nice to replace many or even all of the current type resolvers\r\n  * Would help with reducing complexity a lot. \r\n  * Requires the iteration to disable `legacy` mode\r\n* [ ] As noted in gh-24712, some functions should work for integer inputs on the grounds that the result is boolean and trivial (e.g. also if we would cast to float).  The initial PR ensures these work, but does so in a way which is not ideal.\r\n\r\nThe main points above are that e.g. `can_cast` logic may require some thoughts, which makes this quite tough.  Which also means that I cannot address all of these at once, there is just too many moving parts without pushing forwards with the main changes and others helping out, e.g. with vetting python functions to see what patterns will help us align them with NEP 50.\r\n\r\nFurther issues, which are not as high priority and only indirectly related are that we will want to change some other things such as disabling promotion of strings with numbers.","comments":["The PR now includes an approach for dealing with out-of-bound integer comparisons in a completely generic way: https:\/\/github.com\/numpy\/numpy\/pull\/23912#issuecomment-1759604341\r\n\r\nThis adds a fair bit of complexity, but recovers everything that used to work and makes it much much faster as any slow casts (maybe even to object) paths can now be avoided.","Just a note from the review of gh-23912 so we don't forget: we need better user-facing docs explaining the NEP 50 semantics. That came up there where some discussion about the semantics was added to `basics.creation.rst`, but we really need a whole section elsewhere describing the rules that can be linked to from the there instead of a single example. The text could probably be adapted from NEP 50.","@seberg would you mind updating this issue? I think the window for 2.0.0 is almost closed, so it may be worth opening a separate issue for anything that still needs doing before RC1.","I will push the milestone, at this point, I think the question is more about what is reported than what is plausible to do.  I can't do a big audit of Python functions right now for 100% correct behavior, so I think those are basically bug-fixes then (whether we wait for 2.1 or have them in 2.0.x)."],"labels":["62 - Python API"]},{"title":"ENH: optimize timsort by using binary insertion sort","body":"The current sorting algorithm in numpy's source code uses a standard insertion sort, which involves N^2 comparisons, leading to performance bottlenecks in cases where element comparisons are expensive, such as string sorts and generic sorting operations.\r\n\r\nThis commit replaces the standard insertion sort with a binary insertion sort, which only needs N*log(N) comparisons.\r\n\r\nAdditionally, this change aligns with the initial intention mentioned in the source code comments, which suggested the use of binary insertion sort to boost performance.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Thank you for conducting the tests and providing feedback. Based on the data, it appears that we haven't achieved a significant speedup.\r\n\r\nPerhaps I should consider closing this PR for now, reassessing the changes, and conducting further testing.\r\n\r\nFurthermore, there's something that's puzzling me. I've only made modifications to the timsort code, but as indicated in the data above, some tests with 'kind='heap'' or 'quick'' exhibit significant speed fluctuations, either faster or slower. I encountered similar behavior during local testing, and even after multiple tests, the results consistently showed these speed variations.\r\n\r\nCould these variations be attributed to hardware characteristics or OS scheduling, causing discrepancies in the results?","Maybe wait for one of the more performance-oriented core devs to take a look before you close it. I'm pretty sure NumPy now has a performance team that meets every few weeks as well, if you're keen on that. I believe they focus a lot on SIMD stuff, but anyway in case that is of interest to you.","Straight insertion sort is pretty good for merging small, almost ordered lists. In that case, the simplicity of the inner loop makes a difference. IIRC, the cutoff here is around 15-20 elements. I originally choose that number by timing how long sorting took for different values. It could be that an adjustment could be made these days. In any case, getting stable timing can be difficult, especially for small effects. You want to make sure that your OS isn't being power efficient and that there is not a lot of other stuff running. Cache usage can also make an enormous difference, operations can run 6x-10x faster if everything is in cache. When I wrote the original sorting code back around 2003, CPUs and Linux were a lot simpler."],"labels":["01 - Enhancement","component: numpy._core"]},{"title":"BUG: Occasional failure to sort complex numbers by absolute value on M2 macs","body":"### Describe the issue:\n\nConsider the complex array in the attached npy file (bad.npy in bad.zip), sort it by absolute value, and assert that the absolute value is increasing.  The assertion will occasionally fail (around 1-5 times per 100 iterations).\r\n[bad.zip](https:\/\/github.com\/numpy\/numpy\/files\/12781432\/bad.zip)\r\nIt is also possible to see the failure by running the loop from within an ipython shell, but it is much rarer (once per 1000 to 10000 iterations).\r\n\r\nThis occurs on a conda python + pip-installed numpy (a mix which may or may not be a good idea, I know); AFAICT this happens with numpy 1.25-1.26 but not on numpy 1.24.\n\n### Reproduce the code example:\n\n```python\nfor i in $(seq 0 99); do\r\n    echo \"$i\"\r\n    python -c 'import numpy as np; nz = np.load(\"\/tmp\/bad.npy\"); z = nz[np.argsort(abs(nz))]; assert np.diff(abs(z)).min() >= 0'\r\ndone\n```\n\n\n### Error message:\n\n```shell\nAssertionError\n```\n\n\n### Runtime information:\n\n1.26.0\r\n3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:58:31) [Clang 14.0.6 ]\r\n\r\n[{'numpy_version': '1.26.0',\r\n  'python': '3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:58:31) '\r\n            '[Clang 14.0.6 ]',\r\n  'uname': uname_result(system='Darwin', node='ICR-XYN62WVT43', release='22.6.0', version='Darwin Kernel Version 22.6.0: Wed Jul  5 22:17:35 PDT 2023; root:xnu-8796.141.3~6\/RELEASE_ARM64_T8112', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP'],\r\n                      'not_found': ['ASIMDFHM']}},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/Users\/antony\/.local\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/Users\/antony\/.local\/lib\/python3.11\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21.dev'},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/opt\/homebrew\/Cellar\/openblas\/0.3.24\/lib\/libopenblasp-r0.3.24.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'openmp',\r\n  'user_api': 'blas',\r\n  'version': '0.3.24'},\r\n {'filepath': '\/opt\/homebrew\/Cellar\/gcc\/13.2.0\/lib\/gcc\/current\/libgomp.1.dylib',\r\n  'internal_api': 'openmp',\r\n  'num_threads': 8,\r\n  'prefix': 'libgomp',\r\n  'user_api': 'openmp',\r\n  'version': None}]\n\n### Context for the issue:\n\n_No response_","comments":["I wonder if there is some edge case in the SIMD implementation of `abs` for armv8","I bisected locally on M2 max architecture:\r\n\r\n```\r\n4ec0182597695ba9a63bf4435882117bdc79b872 is the first bad commit\r\ncommit 4ec0182597695ba9a63bf4435882117bdc79b872\r\nAuthor: Sayed Adel <seiko@imavr.com>\r\nDate:   Fri Aug 11 22:27:02 2023 +0400\r\n\r\n    BLD, SIMD: The meson CPU dispatcher implementation  (#23096)\r\n    \r\n    Almost gives the same functionality as Distutils\/CCompiler Opt,\r\n    with a few changes to the way we specify the targets. Also, it\r\n    abandons the idea of wrapping the dispatchable sources, instead it\r\n    counts on static libraries to enable different paths and flags.\r\n\r\n .github\/meson_actions\/action.yml            |  29 ++\r\n .github\/workflows\/build_test.yml            |  20 +-\r\n MANIFEST.in                                 |   2 +\r\n build_requirements.txt                      |   4 +-\r\n doc\/source\/user\/quickstart.rst              |   2 +-\r\n meson.build                                 |   3 +-\r\n meson_cpu\/arm\/meson.build                   |  58 ++++\r\n meson_cpu\/main_config.h.in                  | 351 ++++++++++++++++++++++\r\n meson_cpu\/meson.build                       | 307 +++++++++++++++++++\r\n meson_cpu\/ppc64\/meson.build                 |  38 +++\r\n meson_cpu\/s390x\/meson.build                 |  18 ++\r\n meson_cpu\/x86\/meson.build                   | 227 ++++++++++++++\r\n meson_options.txt                           |  24 +-\r\n numpy\/core\/meson.build                      | 443 +++++++++++++++++++++-------\r\n numpy\/core\/src\/_simd\/_simd.c                |  10 +-\r\n numpy\/core\/src\/_simd\/_simd.dispatch.c.src   |   4 +-\r\n numpy\/core\/src\/common\/npy_cpu_dispatch.h    |   3 +-\r\n numpy\/core\/src\/common\/simd\/sse\/arithmetic.h |   4 +-\r\n 18 files changed, 1421 insertions(+), 126 deletions(-)\r\n create mode 100644 .github\/meson_actions\/action.yml\r\n create mode 100644 meson_cpu\/arm\/meson.build\r\n create mode 100644 meson_cpu\/main_config.h.in\r\n create mode 100644 meson_cpu\/meson.build\r\n create mode 100644 meson_cpu\/ppc64\/meson.build\r\n create mode 100644 meson_cpu\/s390x\/meson.build\r\n create mode 100644 meson_cpu\/x86\/meson.build\r\n```","@tylerjereddy can you run the following code to see if you get an assertion error? I'm noticing argsort giving incorrect results running it (granted, I'm using an Ice Lake (AVX-512) machine and not a macOS machine) :\r\n```python\r\nimport numpy as np\r\n\r\nexpected_array = np.array([0, 1, 5, 7, 3, 6, 2, 4])\r\nactual_array = np.argsort(np.array([1., 1., 8., 5., 8., 1., 5., 1.]))\r\nnp.testing.assert_equal(actual_array, expected_array)\r\n\r\n\"\"\"\r\nPotential failure message:\r\nAssertionError:\r\nArrays are not equal\r\n\r\nMismatched elements: 4 \/ 8 (50%)\r\nMax absolute difference: 3\r\nMax relative difference: 1.\r\n x: array([0, 1, 7, 5, 6, 3, 2, 4])\r\n y: array([0, 1, 5, 7, 3, 6, 2, 4])\r\n\"\"\"\r\n```\r\n\r\nFrom the looks of it, argsort might need to be tweaked a bit to support the not-so-widely-adopted architectures.","It passes for me on `13th Gen Intel(R) Core(TM) i9-13900K` at hash 0be4154648 and also on my M2 Max laptop (ARM Mac) at the same hash.\r\n\r\nI can, however, indeed reproduce your error if I use bleeding edge Sapphire Rapids hardware on an HPC node.\r\n\r\nI've been reading through one of Daniel Kusswurm's SIMD\/assembly books in my free time, but not yet ready to tackle this kind of stuff just yet. The problem disappears if I switch to half precision, which I believe tends to have narrower SIMD support, so perhaps SIMD stuff ya.","@tylerjereddy thanks for checking! I also see that argsort implementation is going to change again from the following PR that was merged last week: https:\/\/github.com\/numpy\/numpy\/pull\/25045. I'm going to try and build it and see if I can still reproduce this on my end.","> @tylerjereddy can you run the following code to see if you get an assertion error? I'm noticing argsort giving incorrect results running it (granted, I'm using an Ice Lake (AVX-512) machine and not a macOS machine) :\r\n\r\n`np.argsort` is by default unstable and when the array has duplicate entries there is no unique correct output to argsort which is why your assertion fails. You either need to add ` kind='stable'` argument to argsort or adjust your test to handle duplicate entires. \r\n\r\n","Ah, ok, I suppose that makes sense, thanks.","> The problem disappears if I switch to half precision,\r\n\r\nThats because `np.argsort` uses SIMD only for 32-bit and 64-bit dtypes. 16-bit argsort behavior hasn't changed, yet :)","@r-devulap thank you for the details! Good to know :)"],"labels":["00 - Bug","component: SIMD"]},{"title":"BUG: distutils.ccompiler_opt over-eager to detect cc_has_debug","body":"### Describe the issue:\n\n`numpy.distutils.ccompiler_opt` uses a list of regexs to determine features about the compile:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/5ca26b1b9df5e8c89923a2bb723a5ab95b6fe09b\/numpy\/distutils\/ccompiler_opt.py#L993\r\n\r\nHowever the `cc_has_debug` regex simply checks that the complete compile command _contains_ 'od', as it's a case-insensitive comparison.\r\n\r\nIt's very common for distributions to pass interesting compile flags such as `-fdebug-prefix-map` which will contain the build path, i.e. an arbitrary build.  This detection is therefore over-eager to detect a debug build.\r\n\r\nConcrete example: we pass `-fdebug-prefix-map` to relocate source paths in the binaries. If we build in a directory containing \"od\" then different compiler flags (specifically, an extra `-O3`) are used compared to a directory that does not contain \"od\".\n\n### Reproduce the code example:\n\n```python\nBuild with compiler flags containing 'od'. Build again with flags not containing 'od'. Binaries will be different.\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\nNumPy 1.26.\n\n### Context for the issue:\n\n_No response_","comments":["The `numpy.distutils` module is deprecated. We are not likely to invest effort in additional developement work. Does the problem show up in the meson builds?","I have not yet tried meson builds.  Is there a plan to remove all of the distutils pieces that are left, or are they being kept for compatibility with something?","`numpy.distutils` may be used by third-party builds, so we don't want to remove it quite yet. We left `setup.py` in 1.26 since it was felt removing it with no overlap with the meson build would be too disruptive, but it will not be part of the upcoming 2.0 release. Please try the meson build (`pip install .`) and report any problems or missing functionality."],"labels":["00 - Bug"]},{"title":"BUG: array2string's parameter \"prefix\" and \"suffix\" do not work","body":"### Describe the issue:\n\nWith passing \"prefix\" and \"suffix\" to `np.array2string` calling, the string returned by `np.array2string` does not match what it was expected to be: the prefix and suffix did not change.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\na = np.arange(9).reshape(3, 3)\r\nprint(a)\r\ns = np.array2string(a, separator=', ', prefix='{', suffix='}')\r\nprint(s)\n```\n\n\n### Error message:\n\n```shell\nNo Error exported, but just the result is not as expected\n```\n\n\n### Runtime information:\n\n[{'numpy_version': '1.26.0',\r\n  'python': '3.10.10 | packaged by Anaconda, Inc. | (main, Mar 21 2023, '\r\n            '18:39:17) [MSC v.1916 64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='DESKTOP-1HQPULJ', release='10', version='10.0.22621', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'filepath': 'C:\\\\Users\\\\for13to1\\\\.conda\\\\envs\\\\py3a\\\\Library\\\\bin\\\\mkl_rt.2.dll',\r\n  'internal_api': 'mkl',\r\n  'num_threads': 8,\r\n  'prefix': 'mkl_rt',\r\n  'threading_layer': 'intel',\r\n  'user_api': 'blas',\r\n  'version': '2023.1-Product'}]\n\n### Context for the issue:\n\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/115892874\/16332162-4d2c-4631-ae9c-bcaaa0115ba1)\r\n\r\nthe output of `print(a)` is:\r\n```python\r\n[[0 1 2]\r\n [3 4 5]\r\n [6 7 8]]\r\n```\r\n\r\nand the output of `print(s)` is:\r\n```python\r\n[[0, 1, 2],\r\n  [3, 4, 5],\r\n  [6, 7, 8]]\r\n```\r\n\r\nthe latter one output should be like this (as I specified the `prefix` be `{`, the `suffix` be `}`:\r\n```python\r\n{{0, 1, 2},\r\n  {3, 4, 5},\r\n  {6, 7, 8}}\r\n```\r\n\r\nor at least, it is morre reasonable be like this or somthing (obviously, it is also somehow weird):\r\n```python\r\n{[0, 1, 2],\r\n  [3, 4, 5],\r\n  [6, 7, 8]}\r\n```","comments":["Sorry to break it to you, but `prefix` and `suffix` are not for that. The [documentation](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.array2string.html) says that:\r\n\r\n> The length of the prefix and suffix strings are used to respectively align and wrap the output. An array is typically printed as: `prefix + array2string(a) + suffix`\r\n\r\n> It should be noted that the content of prefix and suffix strings are not included in the output.\r\n\r\n\r\n\r\n","> Sorry to break it to you, but `prefix` and `suffix` are not for that. The [documentation](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.array2string.html) says that:\r\n> \r\n> > The length of the prefix and suffix strings are used to respectively align and wrap the output. An array is typically printed as: `prefix + array2string(a) + suffix`\r\n> \r\n> > It should be noted that the content of prefix and suffix strings are not included in the output.\r\n\r\nThanks for reminding that, I got to notice that there is a saying `It should be noted that the content of prefix and suffix strings are not included in the output.`\r\n\r\nBut, it is still somehow confusing. So far as `prefix` and `suffix` can only function like specifing the padding size, it should be `int` type than `str` type? And `the content of prefix and suffix strings are not included in the output` is somehow wasting, as what is needed is only the \"padding size\"?"],"labels":["00 - Bug"]},{"title":"BUG: Importing embedded NumPy hangs on Windows","body":"### Describe the issue:\n\nHello everyone!\r\n\r\nI am working on a project in which usage of Numpy in a scenario with embedded subinterpreters is required. While I know this is not supported due to GIL shenanigans, I would like to at the very least be able to import numpy (and thus, reach the warning on subinterpreter usage).\r\n\r\nOn Linux, I am able to import numpy just fine (as long as I only do in a single subinterpreter).\r\nOn Windows, on the other hand, numpy freezes as soon as `getlimits.py` tries to use the `np.longdouble` data type. While I do not currently have debug information, I guess `_multiarray_umath.pyd` is trying to release the GIL, and thus deadlocks.\r\n\r\nMy main questions are:\r\n* What does Windows do differently from Linux re. `np.longdouble`?\r\n* What does `np.longdouble` do differently from other data types such as `float64` and so on?\r\n* Are there any known workarounds (apart from aliasing `longdouble` to `double` in `__init__`)?\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\nNumpy 1.22.2\r\nWindows Server 2016 x64\r\nPython 3.10.4\n\n### Context for the issue:\n\n_No response_","comments":["I doubt it is a GIL problem. We have seen that exact problem before, for instance [here is a work-around](https:\/\/github.com\/numpy\/numpy\/issues\/17747#issuecomment-932739490) for windows and floating point error traps, and [here](https:\/\/github.com\/numpy\/numpy\/issues\/20895) is an issue where OpenCV\/gevent change the floating point math registers.","Hi @mattip ,\r\n\r\nNone of the proposed workarounds seem to help. Here's the configuration as output by `np.show_config()`:\r\n```\r\nopenblas64__info:\r\n    library_dirs = ['[...]\\\\numpy\\\\build\\\\openblas64__info']\r\n    libraries = ['openblas64__info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\r\nblas_ilp64_opt_info:\r\n    library_dirs = ['[...]\\\\numpy\\\\build\\\\openblas64__info']\r\n    libraries = ['openblas64__info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\r\nopenblas64__lapack_info:\r\n    library_dirs = ['[...]\\\\numpy\\\\build\\\\openblas64__lapack_info']\r\n    libraries = ['openblas64__lapack_info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\r\nlapack_ilp64_opt_info:\r\n    library_dirs = ['[...]\\\\numpy\\\\build\\\\openblas64__lapack_info']\r\n    libraries = ['openblas64__lapack_info']\r\n    language = f77\r\n    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\r\nSupported SIMD extensions in this NumPy install:\r\n    baseline = SSE,SSE2,SSE3\r\n    found = SSSE3,SSE41,POPCNT,SSE42,AVX,F16C,FMA3,AVX2\r\n    not found = AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL\r\n```\r\n\r\nAs far as I can see from an interactive console, both `np.longdouble` and `np.longfloat` alias to `np.float64`. But float64 works, while the others do not...","The problem is probably external to numpy itself and has more to do with what cpu are you running on or what other packages do you have installed. You might also want to try a more modern NumPy, 1.22 is a bit old.","@car-bianoc, I agree with Matti that the most likely problem is floating point traps kicking in.  The question is in *what* you embed the Python interpreter and how that is compiled.\r\n\r\nThe work-around isn't some python stuff or even NumPy related (e.g. try executing `float(\"inf\") - float(\"inf\")` it likely causes the same type of issue).\r\n\r\nWhile I am surprised about hangs, maybe a hang is to be expected due to some other reason.  The point is, some toolchains (Delphi, Fortran, ...) trap on FPE errors, which is incompatible with Python in general and NumPy more so.\r\n\r\nFor details\/deeper workaround, please see this discussion: https:\/\/github.com\/numpy\/numpy\/issues\/20504 I don't want to try to dig out the exact incantations that probably help.","Upgrading to Numpy 1.26.0 did not solve the issue.\r\n\r\nThis is the backtrace when building Python with debug info:\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/18487753\/3e5b9701-5b4b-4c2b-91a6-6e9d86a46d83)\r\n\r\nAt least judging from this, it indeed looks like a GIL issue... which makes it even weirder that it's only happening when using `np.longdouble`.\r\n\r\nEDIT: Finally managed to build `_multiarray_umath` with debug information:\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/18487753\/78aabb78-1289-4b04-9cd9-631ca68a8986)\r\n\r\nIt's indeed a `np.longdouble` problem. Do you have any experience with longdouble-related issues?","The place it's getting stuck is a few stack frames up, inside `NumPyOS_ascii_strtod_plain`. It's trying to acquire the GIL in the `NPY_ALLOW_C_API` macro, but something else is still holding the GIL so it deadlocks there. To debug further you'd need to find the thread that is holding the GIL and see what it's doing that is preventing it from releasing the GIL."],"labels":["00 - Bug"]},{"title":"BUG: Windows7 Python 32-bit 3.8 after plain `pip install numpy` will import crash.","body":"### Describe the issue:\n\nIn Windows 7,\r\nDownload 3.8.10 32-bit Python.\r\npip install numpy\r\n\r\nThen in python:\r\n`import numpy`\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/3302478\/4ed81766-beba-4ae7-b49d-5eddbb76d054)\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy\n```\n\n\n### Error message:\n\n```shell\nProblem signature:\r\n  Problem Event Name:\tAPPCRASH\r\n  Application Name:\tpython.exe\r\n  Application Version:\t3.8.10150.1013\r\n  Application Timestamp:\t608fe058\r\n  Fault Module Name:\t_multiarray_umath.cp38-win32.pyd\r\n  Fault Module Version:\t0.0.0.0\r\n  Fault Module Timestamp:\t6498fabf\r\n  Exception Code:\tc000001d\r\n  Exception Offset:\t000269c9\r\n  OS Version:\t6.1.7601.2.1.0.256.1\r\n  Locale ID:\t1033\r\n  Additional Information 1:\t0a9e\r\n  Additional Information 2:\t0a9e372d3b4ad19135b953a78882e789\r\n  Additional Information 3:\t0a9e\r\n  Additional Information 4:\t0a9e372d3b4ad19135b953a78882e789\r\n\r\nRead our privacy statement online:\r\n  http:\/\/go.microsoft.com\/fwlink\/?linkid=104288&clcid=0x0409\r\n\r\nIf the online privacy statement is not available, please read our privacy statement offline:\r\n  C:\\Windows\\system32\\en-US\\erofflps.txt\n```\n\n\n### Runtime information:\n\nIt installs 1.24.5 but the max compatibility is 1.23.5\n\n### Context for the issue:\n\nGeneric `import numpy` installs an incompatible version.","comments":["Hmm, `1.24.4` has a wheel and that passed CI before uploading. I'm not sure what the problem would be - likely specific to Windows 7 or an old CPU.\r\n\r\nI don't think we'll revisit this anymore, unless there are many more folks with problems. If it's a single wheel, we may be able to remove it perhaps. Does `1.24.3` work for you on that same machine, or `1.24.4` with Python 3.9?","1.24.0 through 1.24.5 all instantly crash on import. 1.23.x work flawlessly.","Windows 7 does not allow python 3.9. py3.8 is the last supported for Windows 7.\r\n\r\nOther arrangements, 64bit python. Windows 10 running 32-bit python 3.8. All work correctly. It's only 32 bit, python 3.8, windows 7.","Okay, thanks for the added info @tatarize. If someone wants to dig into the root cause, I think a fix is very welcome. Until then, I think the workaround is to use numpy 1.23.5 and avoid 1.24.x. ","https:\/\/github.com\/numpy\/numpy\/issues\/23324\r\nDuplicates. Just pin the information somewhere for win7 users."],"labels":["00 - Bug","32 - Installation"]},{"title":"how to fix compile numpy 1.26.0 in dockerfire for cpu x86 (pentium-m)","body":"### Steps to reproduce:\n\nHi Guy, \r\n     Please help to step parameter  for compile my cpu detail below. Compile is success. Can not support sse3.\r\n\r\nENV CFLAGS=\"-march=native -msse2 -std=c99 -UNDEBUG -mtune=generic\"\r\n\r\nRUN pip3 install --user --no-cache-dir 'numpy@git+https:\/\/github.com\/numpy\/numpy\/'\r\n\r\ngrep -m1 -A3 \"vendor_id\" \/proc\/cpuinfo\r\nvendor_id       : GenuineIntel\r\ncpu family      : 6\r\nmodel           : 13\r\nmodel name      : Intel(R) Celeron(R) M processor         1.50GHz\r\n\r\ngcc -v -E -x c \/dev\/null -o \/dev\/null -march=native 2>&1 | grep \/cc1 | grep mtune\r\n \/usr\/lib\/gcc\/i686-linux-gnu\/7\/cc1 -E -quiet -v -imultiarch i386-linux-gnu \/dev\/null -o \/dev\/null -march=pentium-m -mmmx -mno-3dnow -msse -msse2 -mno-sse3 -mno-ssse3 -mno-sse4a -mno-cx16 -mno-sahf -mno-movbe -mno-aes -mno-sha -mno-pclmul -mno-popcnt -mno-abm -mno-lwp -mno-fma -mno-fma4 -mno-xop -mno-bmi -mno-sgx -mno-bmi2 -mno-tbm -mno-avx -mno-avx2 -mno-sse4.2 -mno-sse4.1 -mno-lzcnt -mno-rtm -mno-hle -mno-rdrnd -mno-f16c -mno-fsgsbase -mno-rdseed -mno-prfchw -mno-adx -mfxsr -mno-xsave -mno-xsaveopt -mno-avx512f -mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mno-clflushopt -mno-xsavec -mno-xsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mno-avx5124fmaps -mno-avx5124vnniw -mno-clwb -mno-mwaitx -mno-clzero -mno-pku -mno-rdpid --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=1024 -mtune=generic -fstack-protector-strong -Wformat -Wformat-security\n\n### Error message:\n\n```shell\nRuntimeError: NumPy was built with baseline optimizations:\r\n(SSE SSE2 SSE3 SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2) but your machine doesn't support:\r\n(SSE3).\n```\n\n\n### Additional information:\n\n_No response_","comments":["I think we require sse3 these days for HEAD. You can use older versions. I may be wrong, you might want to try setting minimum cpu features. I see the [reference page](https:\/\/numpy.org\/devdocs\/reference\/simd\/build-options.html) has not been updated for the new meson build system and we no longer support the `setup.py` workflow, so maybe try those instructions with the 1.26 sdist.","Is it possible that the CPU detection for x86 is incorrectly requiring sse3? I don't see any discussion about requiring SSE3 on x86 on github. It looks like numpy.distutils still only requires sse2 for x86:\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/eabefa4e041617d4fcd867a147aef15189f1de56\/numpy\/distutils\/ccompiler_opt.py#L230-L231\r\n\r\nJust raising that in case this was not an intentional choice.","I can't remember if this was intentional or not. Probably yes, to keep things simpler - there's no difference between `x86` and `x86_64` anymore, while in `numpy.distutils` there was. @seiko2plus may be able to confirm.","> ENV CFLAGS=\"-march=native -msse2 -std=c99 -UNDEBUG -mtune=generic\"\r\n\r\nCould you try omitting `-march=native` from your CFLAGS? It seems the flag for somehow misreports the CPU features that the build machine supports. If removing it doesn't resolve the issue, please share the build log so we can further investigate the problem."],"labels":["32 - Installation"]},{"title":"ENH: Sorting single-character arrays (S1 \/ U1) using integer sorting","body":"### Proposed new feature or change:\r\n\r\nSort single character (S1 or U1) arrays using an underlying integer representation to improve speed.\r\n\r\n### Background: \r\n\r\nWith a np.array of characters (U1\/S1), np.unique is much faster when doing np.view as int -> np.unique -> np.view as U1\/S1 for arrays of decent size. I would not have expected this since np.unique \"knows\" what's coming in as U1\/S1 and could handle the view-stuff internally. I've played with this a number of ways (e.g. S1 vs U1; int32 vs int64; return_counts = True vs False; 100, 1000, or 10k elements) and seem to notice the same pattern. A short illustration below with U1, int32, return_counts = False, 10 vs 10k.\r\n\r\nI posted this on the numpy listserv and got feedback from @ngoldbaum as follows (excerpt):\r\n\r\n> Looking at a py-spy profile of a slightly modified version of the code you\r\nshared, it seems the difference comes down to NumPy's sorting\r\nimplementation simply being faster for ints than unicode strings. In\r\nparticular, it looks like string_quicksort_<npy::unicode_tag, char> is two\r\nor three times slower than quicksort_<npy::int_tag, int> when passed the\r\nsame data.\r\n>\r\n> We could probably add a special case in the sorting code to improve\r\nperformance for sorting single-character arrays. I have no idea if that\r\nwould be complicated or would make the code difficult to deal with. I'll\r\nalso note that string sorting is a more general problem than integer\r\nsorting, since a generic string sort can't assume that it is handed\r\nsingle-character strings.\r\n\r\nhttps:\/\/mail.python.org\/archives\/list\/numpy-discussion@python.org\/thread\/VF43XVYRFWVUVCMOFPEEI73EVUJ6MRVI\/\r\n\r\nI am posting it here in case it might be worth considering the enhancement.\r\n\r\n```python\r\nimport numpy as np\r\n\r\ncharlist_10 = np.array(list('ASDFGHJKLZ'), dtype='U1')\r\ncharlist_10k = np.array(list('ASDFGHJKLZ' * 1000), dtype='U1')\r\n\r\ndef unique_basic(x):\r\n    return np.unique(x)\r\n\r\ndef unique_view(x):\r\n    return np.unique(x.view(np.int32)).view(x.dtype)\r\n\r\nIn [27]: %timeit unique_basic(charlist_10)\r\n2.17 \u00b5s \u00b1 40.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\nIn [28]: %timeit unique_view(charlist_10)\r\n2.53 \u00b5s \u00b1 38.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\nIn [29]: %timeit unique_basic(charlist_10k)\r\n204 \u00b5s \u00b1 4.61 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [30]: %timeit unique_view(charlist_10k)\r\n66.7 \u00b5s \u00b1 2.91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n```","comments":[],"labels":["01 - Enhancement"]},{"title":"BUG: `jumped` bit generators `.seed_seq` is not meaningful","body":"### Describe the issue:\r\n\r\nIm using the bit generators\/Generator classes in parallel using mpi4py.\r\nI instantiate PCG64DXSM with a fixed seed on multiple ranks, I then jump each state by its MPI rank.\r\nI would expect the entropy of each jumped state to be deterministic if I use the same initial seed on each rank.\r\n\r\nMy little test function below shows this even just on a single core with 3 different instances.  If I run this method twice, the entropies after jumping are not consistent. The random_raw() values, however, ARE the same, which is confusing me even more.\r\n\r\nCould someone please shed some light on this behaviour?  Is this expected?\r\n\r\nCheers,\r\n\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\ndef test_jump_seed(generator, seed):\r\n    print('fixed seed: {}'.format(seed))\r\n    bg = generator(seed = seed)\r\n    s_check = bg.seed_seq.entropy\r\n    print(\"Initialized entropy equals fixed seed: {}\".format(s_check == seed))\r\n    bg1 = bg.jumped(1)\r\n    bg2 = bg.jumped(2)\r\n    bg = bg.jumped(0)\r\n    print('ent 0: {}\\nent 1: {}\\nent 2: {}'.format(bg.seed_seq.entropy, bg1.seed_seq.entropy, bg2.seed_seq.entropy))\r\n    \r\n    print('random draw from 0: {}'.format((bg.random_raw(), bg.random_raw(), bg.random_raw())))\r\n    print('random draw from 1: {}'.format((bg1.random_raw(), bg1.random_raw(), bg1.random_raw())))\r\n    print('random draw from 2: {}'.format((bg2.random_raw(), bg2.random_raw(), bg2.random_raw())))    \r\n\r\n\r\nfrom numpy.random import PCG64DXSM\r\ntest_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)\r\ntest_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)\r\n\r\nThis results in the following\r\n\r\nNot an error but here the output showing the behaviour.\r\n\r\nfirst time\r\ntest_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)\r\nfixed seed: 185147845955919253731470418070286414128\r\nInitialized entropy equals fixed seed: True\r\nent 0: 8818521033502638969998011739757375928\r\nent 1: 276586473706694309202910464541218565095\r\nent 2: 6685832468418544397451491647484482421\r\nrandom draw from 0: (16533588765141592909, 4218189437431980598, 12363468412188683268)\r\nrandom draw from 1: (7254871951857873707, 7869898639939376462, 7791142928462258301)\r\nrandom draw from 2: (13533774799967122937, 1569031215448962883, 10916170404904575733)\r\n\r\nsecond time\r\ntest_jump_seed(PCG64DXSM, 185147845955919253731470418070286414128)\r\nfixed seed: 185147845955919253731470418070286414128\r\nInitialized entropy equals fixed seed: True\r\nent 0: 66283374068340491752624244213781575907\r\nent 1: 87969716045228665193716301454196756402\r\nent 2: 299128717460029393984061490474242083461\r\nrandom draw from 0: (16533588765141592909, 4218189437431980598, 12363468412188683268)\r\nrandom draw from 1: (7254871951857873707, 7869898639939376462, 7791142928462258301)\r\nrandom draw from 2: (13533774799967122937, 1569031215448962883, 10916170404904575733)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\n1.26.0\r\n3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["The `seed_seq` attribute on a jumped generator doesn't make much sense.  Jumping and seed sequence are two completely unrelated ways of spawning new generators.\r\n\r\nI might suggest that you just use `.spawn()` if you need multiple ones (on new numpy versions this is available on the (bit)generator directly.).\r\nBut if you prefer\/want jumping, then don't use `seed_seq` on the the jumped.  `SeedSequence` is how the generator is first *initialized*, but jumping just advances the original one so it's state has nothing to do with the `SeedSequence`.\r\n\r\nThere is a point that probably `seed_seq` should be `None` on a jumped generator.  Maybe the random specialists have more hints\/tips.","`.seed_seq` is not the state of the `BitGenerator`, which is in `.state`. When you derive a `BitGenerator` from another via `.jumped()`, you are only working with the `.state`.\r\n\r\nWhen you don't create a `BitGenerator` from a seed value, its `.seed_seq` is created fresh so that each `BitGenerator` has its own unique one. It probably could\/should be `None` and deal with its absence later, but that's mildly break-y.","Okay great I see!  Thanks for explaining!\r\n"],"labels":["00 - Bug","component: numpy.random"]},{"title":"ENH: An enhancement to searchsorted when both arrays are sorted, and one array is significantly smaller than the other","body":"### Proposed new feature or change:\r\n\r\nWhen both arrays are sorted and the \"searched\" array is smaller than the \"searching\" array, then it might prudent to reverse the order of the search, and then update the original array. An example implementation in python that doesn't take the \"side' parameter into account (simple to remedy that):\r\n\r\n```cython\r\nimport cython\r\nimport numpy as np\r\n\r\n\r\n@cython.boundscheck(False)  # turn off bounds-checking for entire function\r\n@cython.wraparound(False)  # turn off negative index wrapping for entire function\r\ndef sortedsearchsorted(a: cython.numeric[:], v: cython.numeric[:]) -> cython.longlong[:]:\r\n    la = len(a)\r\n    lv = len(v)\r\n    if la > lv:\r\n        return np.searchsorted(a, v)\r\n\r\n    inverse: cython.longlong[:] = np.searchsorted(v, a) \r\n    result: cython.longlong[:] = np.empty(len(v), np.int64)    \r\n    \r\n    i: cython.Py_ssize_t = 0\r\n    prev: cython.longlong = 0\r\n    for i in range(la):\r\n        result[prev:inverse[i] + 1] = i\r\n        prev = inverse[i] + 1\r\n        \r\n    result[prev:] = la\r\n    return np.asarray(result)\r\n```\r\n\r\nWhen the size of `a` is significantly larger than that of `v` you see clear results for that method. These will undoubtedly will be a lot more significant if written in C.  This will require an additional parameter `both_sorted` in the function's signature.","comments":[],"labels":["01 - Enhancement"]},{"title":"ENH: Please support subinterpreters","body":"### Proposed new feature or change:\n\nVersion 1.25.1, Python 3.12.re02\r\nAfter enabling interpreters in Python C Api:\r\n\r\nPyInterpreterConfig config = {\r\n    .check_multi_interp_extensions = 1,\r\n    .gil = PyInterpreterConfig_OWN_GIL,\r\n};\r\nPyThreadState *tstate = NULL;\r\nPyStatus status = Py_NewInterpreterFromConfig(&tstate, &config);\r\nif (PyStatus_Exception(status)) {\r\n    return -1;\r\n}\r\n\r\nImport numpy throws an exception:\r\nmodule numpy.core._multiarray._umath does not support loading in subinterpreters\r\n","comments":["[PEP 554](https:\/\/peps.python.org\/pep-0554\/#help-for-extension-module-maintainers) states:\r\n> To mitigate that impact and accelerate compatibility, we will do the following:\r\n> - be clear that extension modules are not required to support use in multiple interpreters\r\n> - raise ImportError when an incompatible module is imported in a subinterpreter\r\n> - provide resources (e.g. docs) to help maintainers reach compatibility\r\n> - reach out to the maintainers of Cython and of the most used extension modules (on PyPI) to get feedback and possibly provide assistance\r\n\r\nThe PEP also links to [Isolating Extensions](https:\/\/docs.python.org\/3\/howto\/isolating-extensions.html) which has a lot of theory, but does not clearly state how to migrate a large existing c-extension library like NumPy to support subinterpreters. I think we would need to:\r\n- move to HeapTypes\r\n- move all static state into module state\r\n- carefully analyze code for possible shared state.\r\n\r\nI am a bit unclear whether subinterpreters share a single GIL, if not we would also have to carefully examine the code for possible race conditions.\r\n\r\nThis is a lot of work, and may have performance implications. What is your use case for subinterpreters? Do you think you could help with the effort or find funding for this effort?","Relevant mailing list threads and issues:\r\n- [Support for Multiple Interpreters (Subinterpreters) in numpy](https:\/\/mail.python.org\/archives\/list\/numpy-discussion@python.org\/thread\/A3CWGUGK23D33ECXCX6C7JN6OK273KAJ\/#QXRHJS2HZEYWZVDJZQ5NUBOUEENUXQZ3) (Aug 2022)\r\n- [Dealing with static local variables in Numpy](https:\/\/mail.python.org\/archives\/list\/numpy-discussion@python.org\/thread\/CK57VNFCZDGQ2ZFCTXKQYE2FUBRYNTIT\/#CK57VNFCZDGQ2ZFCTXKQYE2FUBRYNTIT) (Aug 2023)\r\n- [Issues labelled with `Embedded`](https:\/\/github.com\/numpy\/numpy\/issues?q=is%3Aopen+is%3Aissue+label%3AEmbedded)\r\n","> This is a lot of work, and may have performance implications. What is your use case for subinterpreters? Do you think you could help with the effort or find funding for this effort?\r\n\r\nI suspect the vast majority of changes to be relatively easy, but there is still the same problem that we need someone to explicitly dedicate time on this, and I doubt it will be one of the current core devs.\r\nWe even added a warning a long time back saying exactly that, but it seems CPython changes to make `subinterpreter` support better in the long-run now enforces an error rather than a warning. ","> [PEP 554](https:\/\/peps.python.org\/pep-0554\/#help-for-extension-module-maintainers) states: \u2026\r\n\r\nFWIW the recent CPython changes should be from [PEP 684 \u201cPer-Interpreter GIL\u201d](https:\/\/peps.python.org\/pep-0684\/); PEP 554, for the Python API and subinterpreter management features, is still in draft status.\r\n","There's a very strong use case for subinterpreters since PEP 684 for parallel processing that I expect would be useful to a lot of numpy client code: using subinterpreters in separate threads will enable shared memory (at least in a read-only case) with significantly less hassle than multiprocessing.","I\u2019m also very excited about the potential opportunities of using subinterpreters with numpy, and agree with what @mdekstrand said. In particular, the latest draft of PEP 734 discusses sharing data via the buffer protocol (and already implemented in the private interpreters module since 3.13a1). Since ndarrays can export their buffer or be created from one without copies, this could be a very nice pattern:\r\n\r\n* pickle your array with protocol 5 to get some serialized metadata plus the memoryview,\r\n* pass that view to a bunch of interpreters (which is basically instant) as well as the small metadata,\r\n* and unpickle: now all of them are sharing the data in each of their arrays\r\n* And if you don\u2019t want to worry about data races, seems like np can handle that by setting the readonly flag.\r\n\r\nYou get concurrency with performant, opt-in data sharing, without the hassles of managing subprocesses and using multiprocessing.shared_memory where you have to create a shared buffer of fixed size ahead of time and only create arrays using that. With interpreters you can take any random array you got and easily share it."],"labels":["01 - Enhancement"]},{"title":"ENH: [typing] `NDarray[plt.Axes]` not possible - Type argument of \"NDArray\" must be a subtype of \"generic\"","body":"### Proposed new feature or change:\n\nFor `numpy.typing.NDArray`, the argument needs to be a subtype of `numpy.generic`. However, this leads to problem since we can no longer specify concrete types for `object`-arrays, for example, `mypy` raises `[type-var]` for this code:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom numpy.typing import NDArray\r\n\r\naxes: NDArray[plt.Axes]  # \u2718 mypy: raises type-var\r\nfig: plt.Figure\r\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\r\n\r\nfor ax in axes.flat:\r\n    reveal_type(axes)  # Axes if the previous error is ignored, else ndarray\r\n    ax.set_title(\"A subplot\")\r\n```\r\n\r\nFor `NDArray` of dtype `np.object_`, it should somehow wrap the given type. One approach could be to make `np.object_` generic and change methods overloads so that for object-type, it generally returns the wrapped type.","comments":["In type hinting Matplotlib, I stopped short of specifying the type in `subplots` (`Figure.subplots` only says `np.ndarray`, `pyplot.subplots` actually just does `Any`)\r\n\r\nIn most cases adding that specificity was just too much to do all at once so I left it for follow up. I don't specifically recall trying to specify here, but it is possible I did and ran into this then. ","> For `NDArray` of dtype `np.object_`, it should somehow wrap the given type. One approach could be to make `np.object_` generic and change methods overloads so that for object-type, it generally returns the wrapped type.\r\n\r\nIt's a feature on the wish list, but yeah this will in all likelihood be the most feasible and sensible approach here. I do foresee one limitation though that I sincerely doubt we'd be to handle satisfactory: type promotions between arrays with different object dtypes (_e.g._ an object array wrapping `builtins.int` and one wrapping `builtins.float` for example). Still, this shouldn't really be an issue as long as you're only mixing `np.object_[plt.Axes]` object arrays with each other for example.","Is there any workaround for this atm? I also run in this problem when trying to do `NDArray[T]` where `T` is a `TypeVar` bound to a custom Protocol type, which is very annoying..."],"labels":["01 - Enhancement","23 - Wish List","Static typing"]},{"title":"BUG: `einsum` with int8\/uint8 input is buggy","body":"These tests are failing on macOS x86-64 and with `clang-cl` on Windows after the move to Meson:\r\n```\r\nFAILED core\/tests\/test_einsum.py::TestEinsum::test_einsum_sums_int8 - AssertionError:\r\nFAILED core\/tests\/test_einsum.py::TestEinsum::test_einsum_sums_uint8 - AssertionError:\r\n```\r\n\r\nIt passes with Clang on macOS arm64, and with MSVC on Windows. It's also passing with and without SIMD optimizations. So it's very likely to be a Clang + x86-64 + exact compile flags used bug in the C code somewhere. This needs investigation. ","comments":[],"labels":["00 - Bug","component: numpy._core"]},{"title":"ENH: Release wheels for PyPy3.10","body":"### Proposed new feature or change:\n\nThe most recent PyPy release (7.3.12 from 2023-06-16) supports both 3.9 and 3.10:\r\n\r\n* https:\/\/www.pypy.org\/posts\/2023\/06\/pypy-v7312-release.html\r\n\r\nNumPy 1.26.0 has wheels for PyPy3.9 but not for PyPy3.10:\r\n\r\n* numpy-1.26.0-pp39-pypy39_pp73-win_amd64.whl\r\n* numpy-1.26.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n* numpy-1.26.0-pp39-pypy39_pp73-macosx_10_9_x86_64.whl\r\n* https:\/\/pypi.org\/project\/numpy\/1.26.0\/#files\r\n\r\nAre there any plans to release wheels for PyPy3.10?\r\n\r\nI see [SPEC 0](https:\/\/scientific-python.org\/specs\/spec-0000\/) recommends dropping Python 3.9 on 5th Oct 2023, so this could help towards that.\r\n\r\nA quick test (https:\/\/github.com\/hugovk\/numpy\/commit\/6d4526c7cba41ef97158cc05e2b1472323760e4c) shows [tests pass](https:\/\/github.com\/hugovk\/numpy\/actions\/runs\/6213979573\/job\/16865560410) and [wheels build](https:\/\/github.com\/hugovk\/numpy\/actions\/runs\/6213979572) for PyPy3.10.\r\n\r\nThank you!","comments":["Thanks @hugovk. I think we probably do want PyPy 3.10 wheels for the next release, it's a matter of updating the wheel build CI jobs. We probably should drop 3.9 at that point indeed, we've never supported two PyPy releases at the same time AFAIK.","It would be nice to release macos-arm64 wheels for PyPy. "],"labels":["14 - Release"]},{"title":"Query: isnan\/isinf and NEP50","body":"I've labelled this a question for now because NEP 50 is tricky. I placed a small reproducer with inline comments below, and I'm curious what the desired \"migration\" approach should be here when NEP 50\/weak promotion is activated? \r\n\r\n```python\r\n# on NumPy main at time of writing (03ccab7d46f8)\r\n\r\nimport numpy as np\r\n\r\nii = np.iinfo(int).max + 1\r\n# this is a simplified reproducer for NEP50-activated\r\n# https:\/\/github.com\/scipy\/scipy\/issues\/19239#issuecomment-1719764401\r\n# which hits isnan() through assert_equal() on an int\r\n# that is too big for int64, but does fit in uint64\r\n\r\nres = np.isnan(ii) # False\r\nnp._set_promotion_state(\"weak\")\r\nres = np.isnan(ii) # OverflowError: Python int too large to convert to C long\r\n```\r\n\r\nThe patch below \"fixes\" the problem, and I was just going to do something like that in SciPy, but I'm not sure I actually understand or agree which before\/after behavior even really make sense here in the context of determining whether an integer is a `NaN` or not? I think I prefer the original behavior from a purist standpoint perhaps. At some potential performance cost, couldn't one short-circuit to say that no integer can be NaN at a fundamental\/IEEE level?\r\n\r\n```diff\r\n--- a\/test.py\r\n+++ b\/test.py\r\n@@ -10,4 +10,4 @@ ii = np.iinfo(int).max + 1\r\n \r\n res = np.isnan(ii) # False\r\n np._set_promotion_state(\"weak\")\r\n-res = np.isnan(ii) # OverflowError: Python int too large to convert to C long\r\n+res = np.isnan(np.uint64(ii))\r\n```\r\n\r\n`np.isinf()` also changes behavior in this way with NEP50 it seems, though I haven't thought about that one in detail.\r\n\r\nIf I had to guess what was going on, perhaps it is related to this section of the NEP: https:\/\/numpy.org\/neps\/nep-0050-scalar-promotion.html#proposed-weak-promotion\r\n\r\n> ... Python int, float, and complex are not assigned one of the typical dtypes, such as float64 or int64. Rather, they are assigned a special abstract DType ...\r\n\r\n> At no time is the value used to decide the result of this promotion. The value is only considered when it is converted to the new dtype; this may raise an error.\r\n\r\nI guess for the last point, if this is what is happening, the argument I have in my mind is that we shouldn't even do an initial cast prior to the internal loop or whatever, since the error message tells us the control flow already knows we have an integer, but I look forward to Sebastian telling me why I'm wrong :) ","comments":["Hmmmm, this shouldn't be too hard to fix.  Some functions like `isinf` and `isnan` are defined for any integer input and just return the same value always (ignoring input).\r\n\r\nFor those functions, we could add an explicit loop that is used for any integer input (including Python integers), without any cast (accept them as `object` dtype, and ignore the value).","In the \"switch to NEP 50\" PR, there is actually already a work-around in place for this.  The path is to simply not do anything for unary ufuncs.  (That is not quite correct, as it allows some ufuncs to go to `object`.)\r\n\r\nSo the issue might be more seen as:  Follow-up to remove replace the hack with a slightly cleaner solution (that e.g. promotes to use a `float64` loop for ufuncs that are float only.).","I had looked a bit recently and it was a bit annoying to do with the support for the legacy path kept, but I guess we will need a solution for this one.\r\n\r\nThe solution is probably to add a custom *legacy* type resolution functions for all of these.  Although a promoter matching just Python integers may also work (the warnings won't work, but on this occasion that is probably OK).","I might get around to it, but seems like this can be bumped probably (especially to after ripping out the old paths)"],"labels":["33 - Question"]},{"title":"DEP: Deprecate registering dtype names with np.sctypeDict?","body":"On import, [`ml_dtypes`](https:\/\/github.com\/jax-ml\/ml_dtypes) adds new entries to `np.sctypeDict` so that e.g. `np.dtype(\u201cint4\u201d)` returns an int4 dtype defined outside NumPy.\r\n\r\nSince jax currently documents this behavior to users and relies on it internally, I don\u2019t think we can reasonably break it without a deprecation story and a migration story.\n\nFor deprecating it, we would keep a list of all the strings that NumPy accepts out of the box and if any other string is passed in and somehow we get back a valid dtype, we should raise a deprecation warning. I don\u2019t know if there are other ways of injecting a string dtype name into NumPy\u2019s internals without manipulating `sctypeDict` so this will catch any other shenanigans.\r\n\r\nWe should probably also deprecate `np.sctypeDict` too?\r\n\r\nIn a few releases after adding the deprecation, we could make it so `np.dtype` can only return dtype instances with a mapping defined out of the box in NumPy or via some as-yet unwritten mechanism to associate string names with dtypes, probably with some kind of support for namespacing.\r\n\r\nAs far as I know jax is the only downstream library that injects dtype names into the `np.dtype(\"dtype_name\")` mechanism.\n\nThe deprecation should not be added until we have a clear migration story for the jax library and any possible impacts on jax users are considered.\r\n\r\nxref https:\/\/github.com\/numpy\/numpy\/pull\/24376#issuecomment-1717908075 and the discussion that follows for context","comments":["Thanks for the summary @ngoldbaum!\r\n\r\nI think it would be useful to get the actual requirements and constraints here clear first, before thinking about potential solutions to make string names for external dtypes work.\r\n\r\n> Since jax currently documents this behavior to users and relies on it internally, I don\u2019t think we can reasonably break it without a deprecation story and a migration story.\r\n\r\nIsn't this completely untested\/documented and just happpened to work by relying on numpy-internal implementation details? If we did document or recommended it, that would make a difference here. But my current impression is that this could have broken at any time, _and_ it's bad practice for a package to modify global state in a way that can break other packages.\r\n\r\n> `np.dtype(\u201cint4\u201d)` returns an int4 dtype defined outside NumPy.\r\n\r\nI don't see a real reason to support this, at least if the only rationale is to save a few characters (since `np.dtype(ml_dtypes.int4)` works fine apparently). Is there a better reason?\r\n\r\nAlso, how often do you actually need this dtype instance? Idiomatic code in numpy uses `func(x, dtype=np.int8)` and not `func(x, dtype=np.dtype('int8'))`.","@rgommers Well, the entire numpy type extension API is more or less undocumented, so to a certain extent that's true of anything related to user-defined types. I know there are plans to change that (NEP 42) but they aren't ready yet.\r\n\r\nThis extension has existed in various forms since 2017 (originally as part of TensorFlow), and it looks like we started adding entries to `np.typeDict` (later `np.sctypeDict`) in 2021 (https:\/\/github.com\/tensorflow\/tensorflow\/commit\/16671ca46eda906e822f61a3979f1f68d8bf603c). The extension was later moved into its own package (`ml_dtypes`) so it could be shared between projects without requiring a TF dependency.\r\n\r\nThe issue is not so much that *we* write this, it's that our *users* who may write this or expect to write this. I don't think it's that prevalent, but some users definitely do it.\r\n\r\nAnd it strikes me as something that's reasonable to expect: we're trying to add additional NumPy integer types (`int4`, `uint4`) and floating point types (`bfloat16`, `float8_e4m3b11fnuz`, `float8_e4m3fn`, `float8_e4m3fnuz`, `float8_e5m2`, `float8_e5m2fnuz`) that to the extent possible are supposed to look and feel exactly like the builtin int and float types.\r\n\r\nThere are other instances where to do this right may require updating global state, for instance users expect to write things like `np.finfo(bfloat16)` (although one we don't do implement that one in practice for reasons not worth discussing right here). In that case, we just provide `ml_dtypes.finfo` and `ml_dtypes.iinfo` because NumPy wasn't sufficiently extensible.\r\n\r\nThat said, I suspect this problem is less pressing than it once was: now that `ml_dtypes` is a self-contained package, one can easily name the type directly (`ml_dtypes.bfloat16`). So if you want to disallow doing this, we can adapt.","> And it strikes me as something that's reasonable to expect ....  are supposed to look and feel exactly like the builtin int and float types.\r\n\r\nSure, but part of the point of all the cleanups in 2.0 is to reduce the many ways of doing the same thing, and give users some better guidance of doing things. And this to me does not look like recommended usage. E.g.:\r\n\r\n```python\r\n>>> x = np.ones(2, dtype=np.int8)  # the canonical way\r\n>>> x = np.ones(2, dtype='int8')\r\n>>> x = np.ones(2, dtype='i1')\r\n>>> x = np.ones(2, dtype=np.dtype('int8'))\r\n>>> x = np.ones(2, dtype=np.byte)\r\n>>> ... # etc.\r\n```\r\n\r\nThis is quite a mess. I believe the recommended way to write code for `ml_types` users should be `np.ones(2, dtype=ml_types.int4)`, analogous to the numpy canonical way.\r\n\r\n> The issue is not so much that we write this, it's that our users who may write this or expect to write this. I don't think it's that prevalent, but some users definitely do it.\r\n\r\nSure. We can think about ways to keep this working. However before we arrive at \"we cannot touch this at all for 2.0\", let's first figure out how this is supposed to look in the future. \r\n\r\nAnd only after that how we move forward in a way that isn't too disruptive for JAX. I'll note that you have to do a new release for 2.0 support anyway, so if we give you any other way (perhaps temporary\/private) to keep `np.dtype('int4')` working if you cannot deprecate it fast enough, that should be fine.\r\n","I think one challenge here is that JAX\/ml_dtypes cannot deprecate `np.dtype('bfloat16')` from the JAX side \u2013 that's a numpy API, and we cannot make it raise any sort of deprecation warning (except maybe by some sort of monkey patching, but I wouldn't consider that a viable solution). The only knob we have is to break users by no longer registering the type name.\r\n\r\nWe could certainly update all instances in our own code and other code that we have control over, but any more gradual runtime deprecation behavior would have to come from NumPy.\r\n\r\n> Isn't this completely untested\/documented and just happpened to work by relying on numpy-internal implementation details?\r\n\r\nNo, it's explicitly documented. e.g. in the README at https:\/\/github.com\/jax-ml\/ml_dtypes:\r\n\r\n> Importing `ml_dtypes` also registers the data types with numpy, so that they may be referred to by their string name:\r\n> ```python\r\n> >>> np.dtype('bfloat16')\r\n> dtype(bfloat16)\r\n> >>> np.dtype('float8_e5m2')\r\n> dtype(float8_e5m2)\r\n> ```\r\n\r\nThat's probably my fault \u2013 I had assumed this sort of registration was intentionally supported by NumPy, and so I advertised it as so.\r\n\r\n","> I think one challenge here is that JAX\/ml_dtypes cannot deprecate `np.dtype('bfloat16')` from the JAX side \u2013 that's a numpy API, and we cannot make it raise any sort of deprecation warning\r\n\r\nCan't you simply insert `{'bfloat16': ml_types._bfloat16)}` into `sctypeDict`, where `_bfloat16` raises a deprecation warning on access and then returns your public `ml_types.bfloat16`?\r\n\r\n> No, it's explicitly documented. e.g. in the README at https:\/\/github.com\/jax-ml\/ml_dtypes:\r\n\r\nI meant in the numpy docs. \r\n\r\n> That's probably my fault \u2013 I had assumed this sort of registration was intentionally supported by NumPy, and so I advertised it as so.\r\n\r\nNo worries at all. It's not impossible that this was in some numpy tutorial or docs. And yes, NumPy historically had neither docs for this kind of thing nor any sort of reasonable public\/private split. So who knows if this dict was ever meant to be public for reading from (let alone for writing into).","> Can't you simply insert `{'bfloat16': ml_types._bfloat16)}` into sctypeDict, where _bfloat16 raises a deprecation warning on access and then returns your public ml_types.bfloat16?\r\n\r\nI don't totally follow: say we create a shadow `_bfloat16` scalar type that looks just like the real `ml_dtypes.bfloat16` scalar type. Where exactly would we raise the deprecation warning? I suppose we'd probably want the equivalent of `np.dtype(ml_dtypes._bfloat16)` to result in a warning, but I'm unclear on what methods of the shadow `_bfloat16` object would be called in this case.","I haven't done this with the legacy API, but could you add a `tp_new` implementation to the dtype class and create the warning when a dtype instance is instantiated? It looks like the `ml_dtypes` types don't have `tp_new` implementations, but I think you could add one to the struct where it is filled in e.g. [here](https:\/\/github.com\/jax-ml\/ml_dtypes\/blob\/2ca30a2b3c0744625ae3d6988f5596740080bbd0\/ml_dtypes\/_src\/int4_numpy.h#L376) for int4.","Does accessing the dtype singleton involve instantiating the scalar type?","OK, so I traced it through: I think `np.dtype(typ)` will eventually end up here if `typ` is not an actual scalar type: https:\/\/github.com\/numpy\/numpy\/blob\/f6bf183abbb61507647aaf40b48c28525fc18e48\/numpy\/core\/src\/multiarray\/descriptor.c#L1572\r\n\r\nSo as long as `sctypeDict['bfloat16']` doesn't have to actually contain a scalar type, we could accomplish this deprecation warning this way:\r\n```python\r\nimport warnings\r\nimport ml_dtypes\r\nimport numpy as np\r\n\r\nclass _deprecated_bfloat16:\r\n  @classmethod\r\n  @property\r\n  def dtype(self):\r\n    warnings.warn(\"np.dtype('bfloat16') is deprecated. Use np.dtype(ml_dtypes.bfloat16) instead.\")\r\n    return np.dtype(ml_dtypes.bfloat16)\r\n\r\nnp.sctypeDict['bfloat16'] = _deprecated_bfloat16\r\n\r\nprint(np.dtype('bfloat16'))\r\n# UserWarning: np.dtype('bfloat16') is deprecated. Use np.dtype(ml_dtypes.bfloat16) instead.\r\n# dtype(bfloat16)\r\n```\r\nIt would be nice to make `_deprecated_bfloat16` a subclass of the true `bfloat16` to cover other use-cases of `sctypeDict['bfloat16']`, but in that case `np.dtype('bfloat16')` bottoms-out in this condition: https:\/\/github.com\/numpy\/numpy\/blob\/f6bf183abbb61507647aaf40b48c28525fc18e48\/numpy\/core\/src\/multiarray\/descriptor.c#L1536-L1538\r\n\r\nI don't see any good overloading route from the Python side within `PyArray_DescrFromTypeObject`, since it's only accessing C-level attributes.\r\n\r\nLet me know if you have better ideas!","I agree that the deprecation should be spit out by NumPy, happy to nudge you towards doing it, but hacking it in `ml_dtypes` seems just harder\/confusing.\r\nWe fall back to looking up in the scalar type dict at some point for `np.dtype()` construction and that should be a good place to do that.  It could probably be also be done by making `sctypeDict` a dict-like object or dict subclass, but I doubt it's better.\r\n(`np.dtype()` construction is very messy, but it probably doesn't even matter for this purpose.)\r\n\r\nThe only thing that would make me pause is if you\/users have a large want to keep it working.  In that case we should add a new way to do a proper registration with a function.  (and I might be fine to just make the old one fail.)","> The only thing that would make me pause is if you\/users have a large want to keep it working. \r\n\r\nI think this would be ideal \u2013 after all the internal dtype system already has a notion of dtype string name, and does string-to-dtype lookups at the C level, even for user-registered dtypes, without any references to `sctypeDict`. Supporting that at the Python level as well doesn't seem like too much of a stretch.","I don't _really_ like it, because you cannot control name clashes well and the next thing will be someone asking if we can allow `np.dtype(\"mydtype[days]\")` (parameters).  But, I can live with it with those caveats because unfortunately that is a typical pattern (and maybe convenient, although I am not sure it is meaningfully convenient).\r\n\r\nBut as I said, I would be fine with hiding a registration function somewhere (which could internally just insert into the `sctypeDict`."],"labels":["component: numpy.dtype"]},{"title":"BUG: Possible undefined behavior for numpy.void.byteswap()","body":"### Describe the issue:\r\n\r\nHi everyone,\r\n\r\nI am loading data from a binary file. The file has a header and I read it with:\r\n\r\n```python\r\nheader_type = np.dtype([('time', '>f8'), ('n', '>i4'), ('dims', '>i4'),\r\n                       ('n_gas', '>i4'), ('n_dark', '>i4'), ('n_star', '>i4'), ('pad', '>i4')])\r\nwith open('test.tipsy', 'rb') as binary_in:\r\n\theader = np.fromfile(header = np.fromfile(binary_in, count=1, dtype=header_type)[0]\r\n```\r\nNote, how I extract the single element with `[0]` instead of using the array as is. header has the type `numpy.void`.\r\n\r\nI then try to swap its byte order. This normally works on `numpy.ndarray` but seems to do something completely random on `numpy.void'\r\n\r\n```python\r\nprint(header.tobytes().hex())\r\nres = header.byteswap(inplace=False)\r\nprint(res.tobytes().hex())\r\n```\r\n\r\noutput of several runs\r\n```shell\r\n3f947ae147ae147b000400000000000300000000000400000000000000000000\r\n70cc49706c7f0000a0a33c491c5600000200000000000000040000006c7f0000\r\n\r\n3f947ae147ae147b000400000000000300000000000400000000000000000000\r\n50fbdfea127f0000a0b330d9e3550000020000000000000004000000127f0000\r\n\r\n3f947ae147ae147b000400000000000300000000000400000000000000000000\r\n107eca8d477f0000a05382ffa0550000020000000000000004000000477f0000\r\n```\r\nAs you can see, while the header before `byteswap()` stays the same it's output seems to be completely random.\r\n\r\nI don't really know why this method even exists on `numpy.void`. I couldn't find it when searching in the docs. Should this work? Or should the method just not be present?\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nheader_type = header_type = np.dtype([('time', '>f8'), ('n', '>i4'), ('dims', '>i4'), ('n_gas', '>i4'), ('n_dark', '>i4'), ('n_star', '>i4'), ('pad', '>i4')])\r\nheader = np.array([(31.2, 12, 1, 4, 4, 4, 0)], dtype=header_type)[0]\r\nprint(f\"{type(header) = }\")\r\nprint(header.tobytes().hex())\r\nheader = header.byteswap(inplace=False)\r\nprint(header.tobytes().hex())\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\ntype(header) = <class 'numpy.void'>\r\n403f3333333333330000000c0000000100000004000000040000000400000000\r\n909719af607f0000a043b0e6cf55000002000000000000000085cd3a82fd0100\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.25.2\r\n3.9.5 (default, Jun 23 2021, 13:33:38) \r\n[GCC 10.2.1 20210110]\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.9.5 (default, Jun 23 2021, 13:33:38) \\n[GCC 10.2.1 20210110]',\r\n  'uname': uname_result(system='Linux', node='phoenix', release='6.1.0-10-amd64', version='#1 SMP PREEMPT_DYNAMIC Debian 6.1.38-1 (2023-07-14)', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\n\r\n### Context for the issue:\r\n\r\nIt just took me several hours to track down why the header wouldn't be written correctly in the little endian format. Now I can work around it but it would be nice if it just didn't happen.","comments":["Yeah my main, or first, point of concern is that should numpy.void types support byteswap()? Given its representation of potentially diverse data types, the question of byte swapping becomes non-trivial.","Please update\/repost this with an example that is executable.  Otherwise, I will assume you just didn't flush the file writing, but I don't know.","> Please update\/repost this with an example that is executable. Otherwise, I will assume you just didn't flush the file writing, but I don't know.\r\n\r\n@seberg What do you mean? I included a minimal example that works in the \"Reproduce the code example:\" section. I don't need to write to a file there for the error to show.","> Yeah my main, or first, point of concern is that should numpy.void types support byteswap()? Given its representation of potentially diverse data types, the question of byte swapping becomes non-trivial.\r\n\r\nFor an array with a single numpy.void element the operation works and is seemingly well defined. It just swappes the bytes of all contained datatypes. I.e. if the datatype is comprised of a (\">f4\", \"<i8\") it just swappes both inner datatypes as documented (\"<f4\", \">i8\"). In this regard it seems to follow the documentation of [ndarray.newbyteorder()](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.dtype.newbyteorder.html#numpy-dtype-newbyteorder) \r\n\r\n> Notes\r\n> \r\n> Changes are also made in all fields and sub-arrays of the data type.\r\n\r\nWhich makes sense since `ndarray.newbyteorder().byteswap()` is documented to give an array with the same semantics but different byteorder.","Ah, right.  To be fair to me: the first part of the issue has a small code-block with at least 2 syntax errors and a distracting, unrelated, file loading.\r\n\r\nSo to diagnose the problem:  If you convert it to an array (e.g. use `[0, ...]` to index) everything is fine, the problem only occurs with byteswapping the void scalar.\r\n\r\nSo, yeah, there is probably a bug in the void byteswap, or how it falls back to an array byteswap.","> Ah, right. To be fair to me: the first part of the issue has a small code-block with at least 2 syntax errors and a distracting, unrelated, file loading.\r\n\r\nYeah sorry for that. I thought giving more context would be better but I can see how that can distract from the correct error reproduction.","> Ah, right.\r\n> So to diagnose the problem: If you convert it to an array (e.g. use `[0, ...]` to index) everything is fine, the problem only occurs with byteswapping the void scalar.\r\n> \r\n> So, yeah, there is probably a bug in the void byteswap, or how it falls back to an array byteswap.\r\n\r\nYes exactly.\r\n","@seberg should I try to come up with a fix?","Sure, if you have some time and want to dive into the code!  It will be somewhat obfuscated in C code, I think in `scalars.c.csr`.  It might be \"gentype\" or void specifically.","Thanks for the hint. I'll try and get back with a PR if I succeed or a message here if I fail. If I haven't reacted to it by Friday you can consider it as failed^^.","Yeah @SebastianJL if you post developments on here, I am more than happy to lend a hand with it too. ","I'm trying to find my way around the project.\r\n\r\n```c\r\n\/\/ In arrayscalars.h\r\n\r\ntypedef struct {\r\n        PyObject_VAR_HEAD\r\n        char *obval;\r\n        PyArray_Descr *descr;\r\n        int flags;\r\n        PyObject *base;\r\n    #if NPY_FEATURE_VERSION >= NPY_1_20_API_VERSION\r\n        void *_buffer_info;  \/* private buffer info, tagged to allow warning *\/\r\n    #endif\r\n} PyVoidScalarObject;\r\n```\r\nIs this the definition of `numpy.void` in the c layer?","Yes and right its `arrayscalars.c.src`.  You don't need the struct layout though, the important thing is the type definition and the methods defined on them, which is in the C-file and has little to do with the actual object struct.","Yes I was just sanity checking if I was on the right track. \r\nI set up the project in PyCharm so it can't index the c code. So I've been limited to string search. I might switch to Clion because I think it should be able index c and python in the same project.","Guys I'm sorry but I need some help.\r\nI setup a conda env with \r\n```\r\nconda env create -f environment.yml\r\n```\r\nand activated it with\r\n```\r\nconda activate numpy-dev\r\n```\r\nI am able to build numpy with \r\n```\r\nspin build\r\n```\r\n\r\nBut no matter what I change in the numpy source code (e.g. a random print statement), I cannot seem to make it appear when I import and run numpy functions.\r\nDoes `spin build` install numpy in the `numpy-dev` environment? If not how do I access the freshly built version of numpy?\r\n\r\nFor example I added a print statement here\r\n```python\r\n# numpy\/numpy\/ma\/core.py\r\n\r\ndef array(data, dtype=None, copy=False, order=None,\r\n          mask=nomask, fill_value=None, keep_mask=True,\r\n          hard_mask=False, shrink=True, subok=True, ndmin=0):\r\n    \"\"\"\r\n    Shortcut to MaskedArray.\r\n\r\n    The options are in a different order for convenience and backwards\r\n    compatibility.\r\n\r\n    \"\"\"\r\n    print(\"helloooo\")\r\n    return MaskedArray(data, mask=mask, dtype=dtype, copy=copy,\r\n                       subok=subok, keep_mask=keep_mask,\r\n                       hard_mask=hard_mask, fill_value=fill_value,\r\n                       ndmin=ndmin, shrink=shrink, order=order)\r\narray.__doc__ = masked_array.__doc__\r\n```\r\nand tried to run a python script from within the environment\r\n```python\r\nimport numpy as np\r\n\r\na = np.array([1,2,3])\r\n```\r\nI expected \r\n```\r\nhelloooo\r\n```\r\nto be printed.","I just realized, that that's not the `np.array()` function but `np.ma.core.array()`. Still doesn't work, but I found\r\n```\r\nspin ipython\r\n```\r\nThat drops me into an ipython sesssion that has the locally built numpy installed.\r\n\r\nIs there another way to do this?\r\nI'd like to write some scratch file for testing before I write an actual test in numpy.","Ok, another update.\r\nI was able to do it with \r\n```\r\npip install -e . --no-build-isolation\r\n```\r\nwhich apparently installs it in the conda env if the env is activated. I did not know that conda allows for pip installs. ","> Yes and right its `arrayscalars.c.src`. You don't need the struct layout though, the important thing is the type definition and the methods defined on them, which is in the C-file and has little to do with the actual object struct.\r\n\r\nThere doesn't seem to be a `arrayscalars.c.src`. Also greping for `PyVoidScalarType`, which I expected to be the type definition based on [this documentation](https:\/\/docs.python.org\/3\/extending\/newtypes_tutorial.html) didn't give any results.","Ok, I'm a bit further now. Important stuff, e.g. the byteswap() implementation seems to be in `scalartypes.c.src`","Question: Does it make sense that `numpy.float64` can be byteswapped inplace and `numpy.void` can't with the error message:\r\n```\r\nValueError: cannot byteswap a scalar in-place\r\n```","No it doesn't make sense that `np.float64` can be byteswapped inplace (if it can).  Voids scalars are sometimes mutable, but they shouldn't be, so that makes sense."],"labels":["00 - Bug"]},{"title":"BUG: using apply_along_axis on array of strings of varying length truncates string lengths to length of first string","body":"### Describe the issue:\n\nUsing \"apply_along_axis(...)\" on an array of strings has unexpected results when the strings are of varying lengths. All strings are truncated at the length of the first string of the input array. \"apply_along_axis(...)\" in this case is supposed to be similar to (but faster than) an in-line for loop, but the behaviour is not the same.\r\n \r\n```\r\nCorrect output: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19']\r\nWrong output: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1']\r\n``` \r\n\r\n\r\nIn the input array \"numbers\", the shortest string has a length of 1 and the longest string has a length of 2. In the incorrect output array \"wrong_joined_numbers\", all strings have a length of 1. In the output array \"correct_joined_numbers\", all of the entries are of the correct length and value.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nnumbers = np.arange(20).astype(str)[:, np.newaxis]\r\ncorrect_joined_numbers = np.array([''.join(ii) for ii in numbers])\r\nwrong_joined_numbers = np.apply_along_axis(''.join, arr=numbers, axis=1)\r\nprint(f'Correct output: {correct_joined_numbers}')\r\nprint(f'Wrong output: {wrong_joined_numbers}')\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n1.25.2\r\n3.11.1 (tags\/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.11.1 (tags\/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 '\r\n            '64 bit (AMD64)]'\n\n### Context for the issue:\n\n_No response_","comments":["Looking at the implementation of `apply_along_axis`, it does use the dtype of the first result element to get the dtype of the whole result array. This is a poor default for fixed-width strings.\r\n\r\nA workaround that you can use right now is to make the function you're applying a wrapper for `join` that takes a `dtype` argument:\r\n\r\n```\r\nimport numpy as np\r\n\r\n\r\ndef myjoin(item, dtype=None):\r\n    return np.str_(\"\".join(item)).astype(dtype)\r\n\r\n\r\nnumbers = np.arange(20).astype(str)[:, np.newaxis]\r\njoined_numbers = np.apply_along_axis(\r\n    myjoin, arr=numbers, axis=1, dtype=numbers.dtype\r\n)\r\n\r\n```\r\n\r\nI'm currently working on improving support for string operations in NumPy by adding a variable-width string dtype, which will avoid this issue entirely by not needing to care about the width of the string in the dtype. https:\/\/numpy.org\/neps\/nep-0055-string_dtype.html\r\n\r\nI'm not sure offhand if there's an easy way to improve `apply_along_axis` so the user can override the result `dtype`, that may break existing usages like the wrapper workaround above that already accept a `dtype` argument."],"labels":["00 - Bug"]},{"title":"ENH: Skew symmetric\/matrix cross product function","body":"### Proposed new feature or change:\n\nDynamicists frequently use the matrix representation of the cross product (https:\/\/en.m.wikipedia.org\/wiki\/Cross_product#Conversion_to_matrix_multiplication), and the lack of a skew symmetric operator in NumPy is a frequent source of annoyance.\r\n\r\nI propose either adding a skew function to linalg that returns the skew symmetric matrix of a vector `np.cross(a, np.identity(a.shape[0]) * -1)` (from https:\/\/stackoverflow.com\/questions\/66707295\/numpy-cross-product-matrix-function), or more radically, making the second argument `b` of np.cross optional and have `np.cross(a)` return the matrix representation of [a x]. For the latter, would need to consider the interaction with other kwargs and different input shapes. ","comments":["This seems a bit too specialized for NumPy.  The SciPy module [`scipy.linalg.special_matrices`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/linalg.html#special-matrices) might be a better fit.","Some very quick comments.\r\n- I would not consider `np.skew` more specialized than, say, `np.kron`.\r\n- `np.cross(a, np.identity(a.shape[0]) * -1)` works only in $R^3$, but `np.skew(a)` should\/could be generalized in $R^n$.\r\n- `np.cross(a)` is not a good idea, in my opinion, because it would be very difficult to generalize for $n \\neq 3$.\r\n- what about implementing the Levi-Civita symbol in `np.einsum`?","As far as I know, the operator can\u2019t be meaningfully generalized into n dimensions as it is only well defined for dimensions the size of an upper triangle\/those where a cross product exists (i.e. 1, 3, 7, 15, etc.). Please correct me if there\u2019s a good way of generalizing to other dimensions (maybe L-C does?  couldn\u2019t find that one way or another). As a counterargument, I\u2019d say that like np.cross, this is probably only commonly useful in n=3 and generalizing might not be worth it in this case; perhaps there are uses for other dimensions that I\u2019m not familiar with. \r\n\r\nAs for calling it `skew`, that seems to follow a convention that exists elsewhere and is probably the best name; however, to play devils advocate, there are many other ways of generating a skew symmetric matrix from a vector - it\u2019s just that few of them have any common use other than this one. \r\n\r\nI have no experience with einsum but read about the Levi-Civita symbol when researching this; what would the notation look like in einsum?","> As far as I know, the operator can\u2019t be meaningfully generalized into n dimensions as it is only well defined for dimensions the size of an upper triangle\/those where a cross product exists (i.e. 1, 3, 7, 15, etc.).\r\n\r\nYes, `skew(a)` should only be defined when the last dimension of `a` has length $\\frac12 n(n-1)$, so $1,3,6,10,15,\\ldots$\r\nIn other terms for $R^n$ the expression `skew(a) @ b` is meaningful only if $a$ is a is bivector ($\\frac12 n(n-1)$ components) and $b$ a vector ($n$ components).\r\n\r\n> I have no experience with einsum but read about the Levi-Civita symbol when researching this; what would the notation look like in einsum?\r\n\r\nI've not worked out this point, but I suspect that `np.skew(a) @ b` could be written efficently using `np.einsum` and some smart trick... but probably this path is really too specialized, with only marginal improvements.","> As far as I know, the operator can\u2019t be meaningfully generalized into n dimensions\r\n\r\nIt can be. An ordered list of `n - 1` vectors defines a parallelepiped  with oriented volume V. pick an oriented vector at right angles to the subspace determined by those vectors and give it a length equal to V. You need to pick an orientation for this to work. Related to the Hodge star of `n - 1` forms :)  Agree that it isn't a binary operator.","@miccoli My subconscious really wanted the 7d cross product matrix to just be a skew symmetric matrix constructed with 7 elements in the upper triangle, despite that clearly not being possible. \r\n\r\nIt seems like there are two (potentially linked) products that could come from this discussion:\r\n- A matrix cross function `np.cross(a)`\r\n- A generalized $\\frac{1}{2}n(n-1)$ (or even $n$) dimensional `np.skew(a)` function\r\n\r\nwhere the former is happens to be a special case of the latter for `a` with a last dimension shape 3, which seems appropriate given that `np.cross` is already (arbitrarily? pragmatically?) limited to 3 dimensional crosses. I know the former is practically useful for me and others based on conversations with colleagues and the existence of multiple stack overflow threads; is the latter actually a feature desired by anyone?","> I know the former is practically useful for me and others based on conversations with colleagues and the existence of multiple stack overflow threads; is the latter actually a feature desired by anyone?\r\n\r\nFair observation: for sure most applications would be for $n=3$, while $n\\neq 3$ would be in the realm of the \u201ctoo specialized\u201d cases ... much better to avoid a generalization that leads to consistent but rarely used cases.","> This seems a bit too specialized for NumPy. The SciPy module [`scipy.linalg.special_matrices`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/linalg.html#special-matrices) might be a better fit.\r\n\r\nI agree with @WarrenWeckesser here. The current discussion seems useful and interesting, so by all means continue it - but aiming for inclusion in SciPy seems better. We don't really want to expand `numpy.linalg`, SciPy contains a superset.","> > This seems a bit too specialized for NumPy. The SciPy module [`scipy.linalg.special_matrices`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/linalg.html#special-matrices) might be a better fit.\r\n\r\nSo maybe `a = scipy.linalg._special_matrices.cross(c)` with `c` of shape `(N*(N-1)\/2,)` and `a` `(N,N)`?\r\nBTW `scipy.linalg.cross` is free...\r\n\r\nMy only doubt here is that while numpy allows for *stacking* matrices\/vectors, `scipy.linalg._special_matrices` typically doesn't (the first statement is almost always `c = np.asarray(c).ravel()`).\r\n\r\nSince writing the desired function in pure python is not that difficult, I was assuming that inclusion in numpy was also in order to have a fast and efficient implementation.","Support for stacking is desirable for `scipy.linalg` as well. That most functions don't support that now is a combination of history and lack of bandwidth to implement it. I think you can keep stacking support for a new scipy function.","Seems like the consensus is leaning towards scipy. Will look into opening a PR over there when I get a chance. "],"labels":["01 - Enhancement","component: numpy.linalg"]},{"title":"DOC: what does the first argument to `np.typing.NDArray` mean?","body":"### Issue with current documentation:\n\nTHe documentation of [`numpy.typing.NDArray`](https:\/\/numpy.org\/devdocs\/reference\/typing.html#numpy.typing.NDArray) provides information that it could accept two arguments. Unspecified first and dtype. \r\n\r\nIt is unclear the purpose of first field. It could be used to provide information about dimensionality or array size, but there is no information about this in the documentation. Or may be reserved for future usage. \n\n### Idea or request for content:\n\nClear what is the purpose of this field to. If annotation `data: npt.NDArray[(3, 3), np.float]`  for 3D transformation is a good idea\/proper way to think about it?","comments":["The first parameter of `np.ndarray` (`npt.NDArray` only has a single parameter: the datatypes' underlying `np.generic` type) was introduced as a placeholder slot for a future shape-type, as at the time we were concerned that going from a dtype- to a dtype- & shape-generic type would be a backwards compatibility problem due to the addition of yet another parameter slot. With [PEP 646](https:\/\/peps.python.org\/pep-0646\/) now accepted it's questionable to what extent our concerns turned out to be valid, but at the time this was simply an unknown and the more cautious approach was preferred.\r\n\r\nIn any case, `npt.NDArray` was specifically introduced as a more compact _size- & shape-agnostic_ type alias for `np.ndarray`, and without implementation of PEP 646 in the likes of mypy and the ability to specify default typevar values ([PEP 696](https:\/\/peps.python.org\/pep-0696\/)) this won't change in all likelihood."],"labels":["04 - Documentation","Static typing"]},{"title":"Consider populating `optional-dependencies` in pyproject.toml","body":"This follows up on this comment from @skirpichev: https:\/\/github.com\/numpy\/numpy\/issues\/23808#issuecomment-1711046374 about `f2py -c` needing `meson` and `ninja` for Python 3.12 (or with `--backend=meson` on older Python versions).\r\n\r\nRight now we do not use `optional-dependencies` at all. It is also the future replacement for `test_requirements.txt` & co. Here is an example of how it can be used: https:\/\/github.com\/scipy\/scipy\/blob\/9f7549abcc61f5dc72ca000cef7bc43066c74527\/pyproject.toml#L74-L109\r\n\r\nIn numpy it could have an extra `f2py` section, and that would allow installing the needed dependencies with `pip install numpy[f2py]`. Of course you have to know that that's possible, and at that point you can equally well do `pip install meson ninja` , so this isn't all that urgent. An extra complication is that `ninja` is a system dependency, and using it from PyPI is a bit hacky. That's for example why `meson` and `meson-python` do not have a direct dependency on `ninja`; meson-python first checks if it's installed on the system and dynamically adds it in non-isolated installs if it's missing.\r\n\r\nI'd say we don't want to do this for 1.26.0, but we can consider populating the optional dependencies table for 2.0.","comments":["I'm bumping this to the next release cycle, because I don't want to touch optional dependencies right before creating a release branch, and this isn't blocking\/important for 2.0"],"labels":["03 - Maintenance"]},{"title":"MAINT: Improve performance of common_type and use result_type internally","body":"Cleanup implementation of `np.common_type` using `np.result_type` and improve performance.\r\n\r\n* The performance of `np.common_type(x, x)` with `x=np.array([1.])` goes from 1.6952696400003333 (main) to 1.1668818179987284 (this PR).\r\n* See additional discussion at #24656\r\n* There is a slight behaviour change in the sense that \r\n```\r\ni16=np.array(2, dtype=np.int16)\r\nf32=np.array(2, dtype=np.float32)\r\nnp.common_type(i16, f32)\r\n```\r\nnow returns `float32` instead of `float64`.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Using `result_type` should be good, let's give this a shot, thanks!","Ohh, now I see that the reason I must have hesitated is the behavior change.  Which may be fine, but would maybe need a shout-out.\r\nHow about:\r\n```\r\ndt = result_type(dtypes)\r\nif not inexact:\r\n   dt = result_type(np.float64, dt)\r\n   if not inexact:\r\n       raise\r\n```\r\nas logic, so that we can avoid that one change?  I am OK to not worry about `timedelta`.  Since float64 is the largest float necessary for any integer promotion, I think we can gamble on that being good.\r\n\r\n(I hate this function, but not sure I want to change it...)","> Ohh, now I see that the reason I must have hesitated is the behavior change. Which may be fine, but would maybe need a shout-out. How about:\r\n> \r\n> ```\r\n> dt = result_type(dtypes)\r\n> if not inexact:\r\n>    dt = result_type(np.float64, dt)\r\n>    if not inexact:\r\n>        raise\r\n> ```\r\n> \r\n> as logic, so that we can avoid that one change? I am OK to not worry about `timedelta`. Since float64 is the largest float necessary for any integer promotion, I think we can gamble on that being good.\r\n> \r\n> (I hate this function, but not sure I want to change it...)\r\n\r\nWith your suggestion we still have the behaviour change right? The result of `np.result_type(np.int16, np.float32)` is `dtype('float32')`, which is inexact so the part `result_type(np.float64, dt)` is never reached.\r\n\r\nI see now that the documentation of `common_type` explicitly states \"If one of the inputs is an integer array, the minimum precision type that is returned is a 64-bit floating point dtype.\". So if we allow the behaviour change, we should update the documentation and make a clear release note. \r\n\r\nI think that if we want to enforce that a single integer input type enforces the output is 64-bit float (at minimum) we have no alternative than to check each argument for being integer. In that case we fall back to https:\/\/github.com\/numpy\/numpy\/pull\/24467 or some combination that I suspect will be slower than the current implementation."],"labels":["03 - Maintenance"]},{"title":"BUG: array_api `stream` has wrong type","body":"### Describe the issue:\r\n\r\nAccording to the spec the `stream` parameter for `__dlpack__` and `to_device` should be `Optional[Union[int, Any]]`. The current implementation just uses `None`.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nfrom typing import Any, Protocol, Self\r\n\r\nfrom numpy import array_api as xp\r\n\r\nPyCapsule = Any\r\n\r\nclass Array(Protocol):\r\n    def __dlpack__(*, stream: int | Any | None = None) -> PyCapsule:\r\n        ...\r\n\r\n\r\ndef identity(array: Array) -> Array:\r\n    return array\r\n\r\nidentity(xp.asarray([1]))  # Error.\r\n\r\nclass Array2(Protocol):\r\n    def to_device(\r\n        self,\r\n        device: Any,\r\n        \/,\r\n        *,\r\n        stream: int | Any | None = None,\r\n    ) -> Self:\r\n        ...\r\n\r\ndef identity2(array: Array2) -> Array2:\r\n    return array\r\n\r\nidentity2(xp.asarray([1]))  # Error\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\n[Pyright] Argument of type \"Array\" cannot be assigned to parameter \"array\" of type \"Array\" in function \"identity\"\r\n\u00a0\u00a0\"Array\" is incompatible with protocol \"Array\"\r\n\u00a0\u00a0\u00a0\u00a0\"__dlpack__\" is an incompatible type\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type \"(*, stream: None = None) -> PyCapsule\" cannot be assigned to type \"(*, stream: int | Any | None = None) -> PyCapsule\"\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Keyword parameter \"stream\" of type \"int | Any | None\" cannot be assigned to type \"None\"\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type \"int | Any | None\" cannot be assigned to type \"None\"\r\n\r\n[Pyright] Argument of type \"Array\" cannot be assigned to parameter \"array\" of type \"Array2\" in function \"identity2\"\r\n\u00a0\u00a0\"Array\" is incompatible with protocol \"Array2\"\r\n\u00a0\u00a0\u00a0\u00a0\"to_device\" is an incompatible type\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type \"(device: Device, \/, stream: None = None) -> Array\" cannot be assigned to type \"(device: Any, \/, *, stream: int | Any | None = None) -> Array\"\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Keyword parameter \"stream\" of type \"int | Any | None\" cannot be assigned to type \"None\"\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Type \"int | Any | None\" cannot be assigned to type \"None\"\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.25.1\r\n3.11.4 (main, Jun  6 2023, 22:16:46) [GCC 12.3.0]\r\n\r\n### Context for the issue:\r\n\r\nLoosely related to #24641 \r\n\r\n`__dlpack__`: https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.array.__dlpack__.html\r\n`to_device`: https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.array.to_device.html","comments":["The `__dlpack__` [docs](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.array.__dlpack__.html) in the link you provided above do also clearly note that \"Support for a stream value other than `None` is optional and implementation-dependent.\", a case which is very much applicable to numpy.\r\n\r\nEDIT: Arguably would be more accurate to type it upstream as a free parameter, _e.g._ a union between `None` and some type variable, rather than a fixed type as is the case now.","> The `__dlpack__` [docs](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.array.__dlpack__.html) in the link you provided above do also clearly note that \"Support for a stream value other than `None` is optional and implementation-dependent.\", a case which is very much applicable to numpy.\r\n> \r\n> EDIT: Arguably would be more accurate to type it upstream as a free parameter, _e.g._ a union between `None` and some type variable, rather than a fixed type as is the case now.\r\n\r\nThe note only talks about the implementation for a `value` other than `None`, not about the type. From what I understand the concrete implementation that handles the non-None case doesn't have to exist, but the type signature should still match. So I'm not saying that there should be an implementation for the non-None case, just that the signature should be annotated.\r\n\r\n`None` vs `int | Any |None` is problematic since a Protocol that satisfies the standard API can't satisfy numpy.array_api arrays. I believe that this is the reason why the note you mentioned only talks about concrete values, not about types.","> The note only talks about the implementation for a value other than None, not about the type. \r\n\r\nThat's in practice purely academic difference? The only member of the `None` type is the literal `None`. Never the less, the fact remains that the signature in the array API docs is a simplification (xref https:\/\/github.com\/numpy\/numpy\/pull\/19083#discussion_r638069860), one that doesn't fully capture the optional nature of `int`\/`Any` in favor of providing a simpler type signature (hence the note). ","I wouldn't call this an academic difference since `(int | None) != None` and while this doesn't get flagged during runtime it gets flagged by static type checkers and potentially leads to hard errors when using runtime type checkers.\r\n\r\nIt seems to me that the issue here is with the API spec though. It's not clear whether the `Optional` means the actual `typing.Optional` (which would be `int | None`) or something like an informal \"you can decide whether you type this parameter int or None\".\r\n\r\nI'd argue that the following are quite different:\r\n\r\n```python\r\ndef f(x: int | None = None) -> Any:\r\n    # Probably the variant that should generally be preferred.\r\n    if x is not None:\r\n        raise NotImplementedError('We don't support this.')\r\n    return do_something(x)\r\n```\r\n```python\r\ndef g(x: int | None = None) -> Any:\r\n    # Not ideal, but valid. Spec doesn't require handling the not-None case.\r\n    return do_something(x)  # Undefined behavior, but fine per spec.\r\n```\r\n```python\r\ndef h(x: None = None) -> Any:  # Wrong. Doesn't match type signature.\r\n    return do_something(x)\r\n```\r\n\r\nI would assume that the spec means the optional in the sense of `f` and `g`, but not in the sense of `h`.\r\n\r\nIf `h` is valid it might make sense to either change this in the spec or to make the terminology more precise. AFAIU it would mean that we can't have generic protocols to check a library for it's array API compatibility.\r\n\r\ncc @rgommers ","All these signatures use positional arguments, so this doesn't make sense to me. It's a keyword argument with a default of `None`, and whether it's unused or an explicit `stream=None` is passed in is expected to not make a difference - you get the default behavior.","> All these signatures use positional arguments, so this doesn't make sense to me.\r\n\r\nMy bad those were meant to be kwargs. I've updated my previous comment.","Okay, after some more thought:\r\n- I agree that `f(x)` is preferred, and that we should change the annotation in `numpy.array_api`\r\n- `g(x)` is indeed valid; handling unexpected input values by a library is good practice, but not required by the standard\r\n- `h(x)` indeed seems wrong\r\n\r\n> AFAIU it would mean that we can't have generic protocols to check a library for it's array API compatibility.\r\n\r\nIt is a goal to have the whole standard be type-safe\/stable\/checkable. \r\n\r\n> It's not clear whether the `Optional` means the actual `typing.Optional` (which would be `int | None`) or something like an informal \"you can decide whether you type this parameter int or None\".\r\n\r\nThat should be `typing.Optional`. In general, the aim is to both have valid and complete type annotations, and make them easy to understand as documentation. Sometimes those things conflict a little, but this doesn't seem to be one of those cases."],"labels":["33 - Question","Static typing","component: numpy.array_api"]},{"title":"BUG: `np.linalg.eigh(complex)` give wrong eigenvectors with accelerate and netlib blas","body":"### Describe the issue:\n\nmacos-arm (apple silicon M2) only, not reproducible on `ubuntu-22.04,AMD-R7`\r\n\r\ncall `np.linalg.eigh` on a hermitian complex matrix, the eigenvectors are wrong with accelerate and netlib blas, but correct with openblas.\r\n\r\n| env | blas | eigh |\r\n| :-: | :-: | :-: |\r\n| `env00` | accelerate | wrong |\r\n| `env01` | netlib | wrong |\r\n| `env02` | openblas | correct |\r\n\r\n```bash\r\nconda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" numpy\r\n\r\nconda create -y -n env01 -c conda-forge \"libblas=*=*netlib\" numpy\r\n\r\n# default if no blas specified\r\nconda create -y -n env02 -c conda-forge \"libblas=*=*openblas\" numpy\r\n```\r\n\r\nseems related issue: numpy\/numpy#21950\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nnp0 = np.array([[0,1j],[-1j,0]])\r\nEVL,EVC = np.linalg.eigh(np0)\r\nprint(EVC)\r\nprint(EVC.T.conj() @ EVC)\n```\n\n\n### Error message:\n\n```shell\n# wrong (accelerate,netlib), the first EVC is correct but not normalized, the second EVC is wrong in direction\r\n# [[-0.70710678-0.70710678j  0.70710678+1.20710678j]\r\n#  [ 0.70710678-0.70710678j  0.5       -0.70710678j]]\r\n# [[ 2.        +0.j  -0.5       -0.5j]\r\n#  [-0.5       +0.5j  2.70710678+0.j ]]\r\n\r\n# correct (openblas)\r\n# [[-0.70710678+0.j          0.70710678+0.j        ]\r\n#  [ 0.        -0.70710678j  0.        -0.70710678j]]\r\n# [[1.00000000e+00+0.j 2.23711432e-17+0.j]\r\n#  [2.23711432e-17+0.j 1.00000000e+00+0.j]]\n```\n\n\n### Runtime information:\n\n```Python\r\nimport sys, numpy\r\nprint(numpy.__version__); print(sys.version)\r\n## same for all: env00 env01 env02\r\n# 1.25.2\r\n# 3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:33:12) [Clang 15.0.7 ]\r\n```\n\n### Context for the issue:\n\n_No response_","comments":["Solved in 1.26.\r\n\r\n```bash\r\nconda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" pip\r\nconda activate env00\r\npip install numpy\r\n\r\nconda create -y -n env01 -c conda-forge \"libblas=*=*netlib\" pip\r\nconda activate env01\r\npip install numpy\r\n```\r\n\r\n20231021 updated: change `pip install numpy=1.26rc1` to `pip install numpy` (the latest `numpy` will be installed, no need to specify it)","I reproduced this bug on M2 Mac using numpy version 1.26.0, and it seems that it has not been resolved or is there something wrong with my installation method?\r\n\r\n`conda env create -f environment.yml`\r\n\r\n`environment.yml` content:\r\n\r\n```yml\r\nname: foundation_accel\r\nchannels:\r\n  - conda-forge\r\ndependencies:\r\n  - python=3.11\r\n  - libblas=*=*accelerate\r\n  - nc-time-axis\r\n  - xarray\r\n  - matplotlib\r\n  - bottleneck\r\n  - dask\r\n  - seaborn\r\n  - netcdf4\r\n  - scipy\r\n  - jupyterlab\r\n  - ipympl\r\n  - mpl-probscale\r\n  - pytables\r\n  - scikit-image\r\n  - scikit-learn\r\n  - plotly\r\n  - vega_datasets\r\n  - altair\r\n  - tqdm\r\n  - palettable\r\n  - scienceplots\r\n  - joypy\r\n  - openpyxl\r\nprefix: \/Users\/fff8e7\/anaconda3\/envs\/foundation_accel\r\n```\r\n\r\nnumpy version: 1.26.0 with accelerate blas\r\n\r\nThere is no problem running the example code in another conda venv with openblas implemented 1.26.0.\r\n\r\n\r\n","> Solved in 1.26.\r\n> \r\n> ```shell\r\n> conda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" pip\r\n> conda activate env00\r\n> pip install numpy\r\n> \r\n> conda create -y -n env01 -c conda-forge \"libblas=*=*netlib\" pip\r\n> conda activate env01\r\n> pip install numpy\r\n> ```\r\n> \r\n> 20231021 updated: change `pip install numpy=1.26rc1` to `pip install numpy` (the latest `numpy` will be installed, no need to specify it)\r\n\r\nPlease try the commands above to create environment (`env00` or `env01`). I guess you install `numpy` from `conda-forge` channel like `env02` below\r\n\r\n```bash\r\nconda create -y -n env02 -c conda-forge \"libblas=*=*accelerate\" pip numpy\r\n```\r\n\r\nIn this case, i do reproduce this bug. I think this is a bug of `conda-forge` side.","> > Solved in 1.26.\r\n> > ```\r\n> > conda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" pip\r\n> > conda activate env00\r\n> > pip install numpy\r\n> > \r\n> > conda create -y -n env01 -c conda-forge \"libblas=*=*netlib\" pip\r\n> > conda activate env01\r\n> > pip install numpy\r\n> > ```\r\n> > \r\n> > \r\n> > 20231021 updated: change `pip install numpy=1.26rc1` to `pip install numpy` (the latest `numpy` will be installed, no need to specify it)\r\n> \r\n> Please try the commands above to create environment (`env00` or `env01`). I guess you install `numpy` from `conda-forge` channel like `env02` below\r\n> \r\n> ```shell\r\n> conda create -y -n env02 -c conda-forge \"libblas=*=*accelerate\" pip numpy\r\n> ```\r\n> \r\n> In this case, i do reproduce this bug. I think this is a bug of `conda-forge` side.\r\n\r\nAfter I installed numpy using pip, there was no problem. Now it seems that there is a problem with numpy provided by `conda-forge`. Thank you for your reply!","There again,\r\n\r\n> Solved in 1.26.\r\n> \r\n> ```shell\r\n> conda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" pip\r\n> conda activate env00\r\n> pip install numpy\r\n> \r\n> conda create -y -n env01 -c conda-forge \"libblas=*=*netlib\" pip\r\n> conda activate env01\r\n> pip install numpy\r\n> ```\r\n> \r\n> 20231021 updated: change `pip install numpy=1.26rc1` to `pip install numpy` (the latest `numpy` will be installed, no need to specify it)\r\n\r\nDespite the use of `conda create -y -n env00 -c conda-forge \"libblas=*=*accelerate\" pip` command, the actual pip-installed numpy (`pip install numpy`) is still built on openblas.\r\n\r\nAccording to this code, when running `np.show_config()` in the `env00` environment to view the np information, numpy is actually built based on openblas.\r\n\r\nSee the following function output message:\r\n\r\n```yml\r\n\"Build Dependencies\": {\r\n    \"blas\": {\r\n      \"name\": \"openblas64\",\r\n      \"found\": true,\r\n      \"version\": \"0.3.23.dev\",\r\n      \"detection method\": \"pkgconfig\",\r\n      \"include directory\": \"\/opt\/arm64-builds\/include\",\r\n      \"lib directory\": \"\/opt\/arm64-builds\/lib\",\r\n      \"openblas configuration\": \"USE_64BITINT=1 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS= NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SANDYBRIDGE MAX_THREADS=3\",\r\n      \"pc file directory\": \"\/usr\/local\/lib\/pkgconfig\"\r\n    },\r\n    \"lapack\": {\r\n      \"name\": \"dep4347366432\",\r\n      \"found\": true,\r\n      \"version\": \"1.26.1\",\r\n      \"detection method\": \"internal\",\r\n      \"include directory\": \"unknown\",\r\n      \"lib directory\": \"unknown\",\r\n      \"openblas configuration\": \"unknown\",\r\n      \"pc file directory\": \"unknown\"\r\n    }\r\n  },\r\n```\r\n\r\nSo I'm a little confused as to what the problem is.","I apologize for the oversight in my previous response. I've since learned that `pip install numpy` utilizes `openblas`. I'm uncertain about the root of the issue. I'll re-open this for more experienced contributors to investigate.","Since the above problem does not exist in versions built against OpenBLAS and Intel MKL, I speculate that this bug may come from the upstream libraries: Apple Accelerate and netlib blas?\r\n\r\nI have learned through research that these basic BLAS libraries can indeed give incorrect results, I am posting [this link](https:\/\/fortran-lang.discourse.group\/t\/how-many-blas-libraries-have-this-error\/4454) here as an example, it should be noted that the bug mentioned in this external link may NOT correlate with the bug in this issue. This requires further investigation, and if it is determined that the bug is coming from upstream libraries, it should be reported to Apple Inc. as well as netlib blas.","numpy\/numpy#25007"],"labels":["00 - Bug","component: numpy.linalg"]},{"title":"BUG: Inconsistent behavior of np.interp between Windows and Linux with period","body":"### Describe the issue:\r\n\r\nWhen using `np.interp` with a period, I get completely different results between Windows and Linux (see below).\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\neps = float(np.finfo(np.float64).eps)\r\n\r\nprint(\r\n    np.interp(\r\n        [0.0, 120.0, 134.99, 135.0, 145.0, 239.99, 240.0, 241.0, 254.99, 359.99, 360.0],\r\n        [\r\n            0.0,\r\n            135.0 - eps,\r\n            135.0,\r\n            240.0 - eps,\r\n            240.0,\r\n            255.0 - eps,\r\n            255.0,\r\n            360.0 - eps,\r\n            360.0,\r\n        ],\r\n        [5.0, 5.0, 12.0, 12.0, 8.0, 8.0, 5.0, 5.0, 5.0],\r\n        period=360,\r\n    )\r\n)\r\n\r\nprint(\r\n    np.interp(\r\n        [0.0, 120.0, 134.99, 135.0, 145.0, 239.99, 240.0, 241.0, 254.99, 359.99, 360.0],\r\n        [\r\n            0.0,\r\n            135.0 - eps,\r\n            135.0,\r\n            240.0 - eps,\r\n            240.0,\r\n            255.0 - eps,\r\n            255.0,\r\n            360.0 - eps,\r\n            360.0,\r\n        ],\r\n        [5.0, 5.0, 12.0, 12.0, 8.0, 8.0, 5.0, 5.0, 5.0],\r\n        # period=360,\r\n    )\r\n)\r\n```\r\n\r\n\r\n### Outputs Windows vs. Linux:\r\n\r\n```shell\r\n# Windows (expected behavior)\r\n[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]\r\n[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]\r\n\r\n# Linux\r\n[ 5.         11.22222222 11.99948148  5.          5.66666667 11.99933333\r\n  8.          8.          8.          5.          5.        ]\r\n[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\nLinux (docker python:3.10-slim):\r\n\r\n```\r\n1.25.2\r\n3.10.12 (main, Jul 28 2023, 05:41:22) [GCC 12.2.0]\r\n```\r\n\r\nWindows (Windows 10):\r\n\r\n```\r\n1.25.2\r\n3.10.5 (tags\/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]\r\n```\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["Just for reference I get this on my M2 and Intel Macs, natively and in Ubuntu:22:04 Docker:\r\n\r\n```\r\n# python3 io\/interp_check.py \r\n[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]\r\n[ 5.  5.  5. 12. 12. 12.  8.  8.  8.  5.  5.]\r\n```\r\n\r\nIntel Mac info, in Docker container:\r\n\r\n```\r\n# python3 -c 'import sys, numpy; print(numpy.__version__); print(sys.version)'\r\n1.26.0b1\r\n3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\n```\r\n\r\n```\r\n# python3 -c 'import numpy; print(numpy.show_runtime())' \r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.26.0b1',\r\n  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',\r\n  'uname': uname_result(system='Linux', node='1cfe9d422cad', release='5.15.49-linuxkit-pr', version='#1 SMP Thu May 25 07:17:40 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n```","I am wondering if these difference are not due to low precision SVML and we swapped that out.  Windows would not use that.  Of course it isn't guaranteed, there could just be other numerical precision differences."],"labels":["00 - Bug"]},{"title":"DOC: character+size string typenames not documented","body":"### Issue with current documentation:\n\nAFAICT, the \"character+size\" typenames (\"u1\", \"i4\", \"f8\", etc.) are not documented, whether at https:\/\/numpy.org\/doc\/stable\/user\/basics.types.html or at https:\/\/numpy.org\/doc\/stable\/user\/basics.types.html.\n\n### Idea or request for content:\n\n_No response_","comments":["Are you referring to these ones under Array-protocol type strings?   https:\/\/numpy.org\/doc\/stable\/reference\/arrays.dtypes.html#specifying-and-constructing-data-types\r\n\r\n","Ah yes, indeed, I missed that page (looks like I messed up one of the links in the original report).\r\nI guess it would still be nice to explicitly list the supported numeric sizes (i1, i2, i4, i8, b1, b2, b4, b8, f2, f4, f8, c8, c16?).\r\nAlso it's not so obvious that \"b\" should mean int8 but b1 (literally signed byte with size 1 byte) should mean \"bool\".","`b1` also came up recently, see https:\/\/github.com\/numpy\/numpy\/pull\/24501\r\n\r\nMaybe there should be a discussion after the table that PR modifies that explains the weird `b1` naming and also describes the character-bitsize codes for the other type as well."],"labels":["04 - Documentation"]},{"title":"DOC: interpretation of terms in ``quantile`` documentation","body":"### Issue with current documentation:\r\n\r\nPlease forgive this as-yet-unformed question - but I am struggling with the documentation for `quantile`.   The documentation has this:\r\n\r\n\"\"\"\r\nGiven a vector V of length n, the q-th quantile of V is the value q of the way from the minimum to the maximum in a sorted copy of V.\r\n\"\"\"\r\n\r\nHowever - it's not clear to me what this means.   \r\n\r\nAssume in what follows that V is sorted.\r\n\r\nFirst \"minimum\" and \"maximum\" would normally refer to `min(V)` and `max(V)` but I'm sure we actually mean the minimum and maximum index - so 0 and `n - 1` in Python terms.   So I think this is referring to the distance along the line starting at 0 and ending at n-1, and therefore the \"quantile positions\" (I made up this term) corresponding to each value in V are:\r\n\r\n```python\r\nqps = np.arange(n) \/ (n - 1)\r\n```\r\n\r\nIs that correct?\r\n\r\nIs it correct to use the term \"plotting positions\" for my \"quantile positions\" `qps`?\r\n\r\nNext we have this:\r\n\r\n\"\"\"\r\nThe values and distances of the two nearest neighbors as well as the method parameter will determine the quantile if the normalized ranking does not match the location of q exactly.\r\n\"\"\"\r\n\r\nI was interested in the cases where the normalized ranking *does* match the location of q exactly - where I'm assuming the \"location of q\" means my `qps` above.\r\n\r\nSo I'm taking q for each element `V[i]` to be `qps[i]` - is that right?  Therefore `q = np.quantile(V, qps[i], method=m) == V[i]` for all valid values of m?\r\n\r\nIf so, it's easy to show that, for many of the methods, that isn't the case.\r\n\r\nJust as an example:\r\n\r\n```python\r\nn = 4\r\nV = np.arange(n)  # The values.\r\nqps = np.arange(n) \/ (n - 1)  # Quantile positions\r\n# linear is the default\r\nassert np.allclose(  # Passes\r\n    np.quantile(V, qps, method='linear'), V)\r\nassert np.allclose(. # Fails\r\n    np.quantile(V, qps, method='closest_observation'), V)\r\n```\r\n\r\nHave I interpreted these correctly?  If so, or even if not, maybe the docstring needs a rewrite?\r\n\r\n### Idea or request for content:\r\n\r\n_No response_","comments":["Sorry - here are the methods where `np.quantile(V, qps[i], method=m) != V[i]`:\r\n\r\n```python\r\n# linear is the default\r\nmethods = ('linear',  # The default\r\n           'inverted_cdf',\r\n           'averaged_inverted_cdf',\r\n           'closest_observation',\r\n           'interpolated_inverted_cdf',\r\n           'hazen',\r\n           'weibull',\r\n           'median_unbiased',\r\n           'normal_unbiased')\r\n\r\nmatches = {}\r\nfor method in methods:\r\n    matches[method] = np.allclose(\r\n        np.quantile(V, qps, method=method), V)\r\nmatches\r\n```\r\n\r\nwith output:\r\n\r\n```python\r\n{'linear': True,\r\n 'inverted_cdf': True,\r\n 'averaged_inverted_cdf': True,\r\n 'closest_observation': False,\r\n 'interpolated_inverted_cdf': False,\r\n 'hazen': False,\r\n 'weibull': False,\r\n 'median_unbiased': False,\r\n 'normal_unbiased': False}\r\n```","> Is it correct to use the term \"plotting positions\" for my \"quantile positions\" qps?\r\n\r\nThat looks fair to me. In the notation of reference [1] of `np.quantile`, plotting positions are:\r\n\r\n$$p_k = \\frac{k - \\alpha}{n - \\alpha - \\beta + 1}$$\r\n\r\nwhere $n$ is the number of elements, $k \\in [1, ..., n]$, and $\\alpha$ and $\\beta$ are constants. According to the seventh definition of empirical quantiles given in [1], $\\alpha = \\beta = 1$, so this becomes\r\n\r\n$$p_k = \\frac{k - 1}{n - 1}$$\r\n\r\nThis corresponds with your \"quantile positions\", so they are a certain choice of \"plotting positions\".\r\n\r\n> Is that correct?\r\n\r\nIn the sense that `np.quantile`'s default `method='linear'` is based on this definition of plotting positions, yes. This is illustrated by code (similar in spirit to yours):\r\n\r\n```python3\r\nimport numpy as np\r\nrng = np.random.default_rng(234984259234582435)\r\nn = 10\r\nV = rng.random(size=n)\r\nqps = np.arange(n) \/ (n - 1)\r\nquantiles = np.quantile(V, qps)\r\nnp.testing.assert_allclose(quantiles, np.sort(V))\r\n```\r\n\r\n> So I'm taking q for each element `V[i]` to be `qps[i]` - is that right?\r\n\r\nAs you found, this is true for some methods (if `V` is sorted).\r\n\r\n> Therefore `q = np.quantile(V, qps[i], method=m) == V[i]` for all valid values of `m`?\r\n\r\nBased on the first sentence of the notes, that is what I might expect. Based on [1], though, I wouldn't expect it to be the case for all methods. Some are based on different conventions for the plotting positions. For instance, `method='weibull'` (Definition 6 in [1]) is based on $\\alpha = \\beta = 0$, so we have a different identity.\r\n\r\n```python3\r\nqps = np.arange(1, n + 1) \/ (n + 1)\r\nquantiles = np.quantile(V, qps, method='weibull')\r\nnp.testing.assert_allclose(quantiles, np.sort(V))\r\n```\r\n\r\n> If so, or even if not, maybe the docstring needs a rewrite?\r\n\r\nYes. I'll take a look when I'm done with gh-15692.\r\n\r\n","I've looked into this a little more. It looks like there were some recent improvements in gh-22118, but they tried to work within the existing framework. I think it could be simpler if we follow [1] a bit more closely:\r\n\r\n<img width=\"680\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/6570539\/ddeb3ef6-4b7d-4c33-b88b-cede3874fd77\">\r\n<img width=\"677\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/6570539\/5b8f0243-94f3-4c0d-a144-025bc447a58f\">\r\n\r\nWould a PR along these lines address your concern @matthew-brett? Some code to confirm these definitions against what `quantile` does is below.\r\n\r\n<details>\r\n\r\n```python3\r\nimport numpy as np\r\nrng = np.random.default_rng(356935893569344576)\r\nn = 43  # arbitrary\r\na = rng.random(size=n)\r\nq = rng.random(size=100*n)  # arbitrarily many\r\nx = np.sort(a)\r\n\r\nms = dict(interpolated_inverted_cdf=lambda q: 0, \r\n          hazen=lambda q: 1\/2, \r\n          weibull=lambda q: q,  \r\n          linear=lambda q: 1-q, \r\n          median_unbiased=lambda q: (q + 1)\/3,\r\n          normal_unbiased=lambda q: q\/4 + 3\/8)\r\n\r\nfor method, mfun in ms.items():\r\n  m = mfun(q)\r\n  j = (q*n + m) \/\/ 1 - 1\r\n  g = (q*n + m) % 1\r\n\r\n  j = np.asarray(j, dtype=int)\r\n  jp1 = j + 1\r\n  j[j < 0] = 0\r\n  jp1[jp1 > n-1] = n-1\r\n\r\n  res = (1-g) * x[j] + g * x[jp1]\r\n  ref = np.quantile(a, q, method=method)\r\n  np.testing.assert_allclose(res, ref)\r\n\r\ngs = dict(lower=lambda q: 0, \r\n          higher=lambda q: 1,\r\n          nearest=lambda q: (q*(n-1) % 1) > 0.5,  \r\n          midpoint=lambda q: 0.5)\r\n\r\nfor method, gfun in gs.items():\r\n  gfun = gs[method]\r\n\r\n  j = q*(n-1) \/\/ 1\r\n  g = gfun(q)\r\n\r\n  j = np.asarray(j, dtype=int)\r\n  jp1 = j + 1\r\n  j[j < 0] = 0\r\n  jp1[jp1 > n-1] = n-1\r\n\r\n  res = (1-g) * x[j] + g * x[jp1]\r\n  ref = np.quantile(a, q, method=method)\r\n  np.testing.assert_allclose(res, ref)\r\n\r\ngs = dict(inverted_cdf=lambda q: q*n > 0, \r\n          averaged_inverted_cdf= lambda q: (1 + (q*n > 0)) \/ 2)\r\n\r\nfor method, gfun in gs.items():\r\n  gfun = gs[method]\r\n\r\n  m = 0\r\n  j = (q*n + m - 1) \/\/ 1\r\n  g = gfun(q)\r\n\r\n  j = np.asarray(j, dtype=int)\r\n  jp1 = j + 1\r\n  j[j < 0] = 0\r\n  jp1[jp1 > n-1] = n-1\r\n\r\n  res = (1-g) * x[j] + g * x[jp1]\r\n  ref = np.quantile(a, q, method=method)\r\n  np.testing.assert_allclose(res, ref)\r\n```\r\n\r\n<\/details>\r\n \r\n@melissawm it looked like you helped with the documentation of this function before. Do you think this would be helpful, or shoudl we try to work within the framework that is there?\r\n","I wonder whether there is a compromise here, which introduces the problem - as above, and then goes onto the default `linear` case, using something like the current text, while flagging that other methods differ.  Only then, introduce the rather difficult-to-grok general formula from the reference.   My worry it that the formula from [1] does provide a general framework for describing all implementations, but at the expense of making it quite a bit harder to follow the default implementation.\r\n\r\nJust as a side-note - I find `x` to be a little confusing as the name for a sorted copy of `a` because we are also implying a plot with plotting positions on the x-axis (which are not `x`) and quantile values on the y-axis (which are not the same as, but closer to `x`).  ","> Only then, introduce the rather difficult-to-grok general formula from the reference.\r\n\r\nI actually tried to strike that compromise. \r\n\r\nI used `(1-g)*x[j] + g*x[j+1]` to define linear interpolation, which is needed by the `linear` case, and it happens to be the same in the linear and general case. That is really all that appears up here from the difficult-to-grok formula from [1].\r\n\r\n\"`j` and `g` are the integer and fractional components of `q * (n-1)`, where `n` is the number of elements in the sample\" is how I present particular values used in interpolation for the `linear` case. (I elaborated by showing how we get the integer and fractional components with `\\\\ 1` and `% 1`, but I can remove that part if you prefer.)\r\n\r\nThis is not the full formula used in [1].\r\n\r\n<img width=\"593\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/6570539\/a3cb0de6-6b64-471e-8aa2-b4f305a99028\">\r\n\r\nI omit `m` until moving on to the general case, I never need the inequality (because it contains basically the same information as $j = \\lfloor p n + m \\rfloor$ but is less explicit about how $j$ is actually calculated), and the initial formulas for $j$ and $g$ are interpreted in English as \"fractional and integer components\" of a simple expression. Isn't it quite a bit simpler than the presentation in [1]?\r\n\r\nI imagine it could potentially be simpler if we used *only* English to describe the linear method first, before giving the mathematical description. But I wanted to avoid the imprecision of the current English description and the awkwardness that follows from trying to correct it. Feel free to make a suggestion, though. \r\n\r\nAnother possibility is to just to phrase the beginning part as though only the linear method is being described. After finishing the linear method, we reveal that exactly the same formula is used for the other methods, but `j` and `g` differ.\r\n\r\nAnd feel free to pick a different variable than `x`; that's just a standard symbol used for an array in `scipy.stats` and it matches the paper. Would you prefer `y`?","Personally - yes - I would prefer `y` to `x` at least.\r\n\r\nI'm sorry for not reading your changes more deeply - but yes - what I meant was close to what you suggest:\r\n\r\n> Another possibility is to just to phrase the beginning part as though only the linear method is being described.\r\n\r\nbut - make clear up front that this is a simplification for the sake of exposition.  I think it's helpful to consider that case, where the plotting positions and quantiles correspond - and then complexify later.","OK. \r\n\r\n---\r\n\r\nGiven a sample `a` from an underlying distribution, `quantile` provides a nonparametric estimate of the inverse cumulative distribution function.\r\n\r\nBy default (`method='linear'`), this is done by interpolating between adjacent elements in ``y``, a sorted copy of `a`:\r\n\r\n```\r\n(1-g)*y[j] + g*y[j+1]\r\n```\r\n\r\nwhere the index `j` and coefficient `g` are the integral and fractional components of `q * (n-1)`, and `n` is the number of elements in the sample. \r\n\r\nThis a special case of Equation 1 of H&F [1]. More generally, \r\n\r\n- `j = (q*n + m - 1) \/\/ 1`, and \r\n- `g = (q*n + m - 1) % 1`, \r\n\r\nwhere `m` is defined differently for each value of the `method` parameter below.\r\n\r\n| `method`                   | # in H&F [1] | `m`         |\r\n| -------------------------- | ------------ |-------------|\r\n| `interpolated_inverted_cdf`| 4            | `0`         |\r\n| `hazen`                    | 5            | `1\/2`       |\r\n| `weibull`                  | 6            | `q`         |\r\n| `linear` (default)         | 7            | `1 - q`       |\r\n| `median_unbiased`          | 8            | `q\/3 + 1\/3`   |\r\n| `normal_unbiased`          | 9            | `q\/4 + 3\/8` |\r\n\r\nNote that indices `j` and `j + 1` are clipped to the range `0` to `n-1` when the results of the formula would be beyond the allowed range of non-negative indices.\r\n\r\nThe methods above are all continuous functions of probability `q`. Methods 1-3 of H&F [1] provide three discontinuous estimators, where `j` is defined as above and `m` and `g` are defined as follows.\r\n\r\n1. `inverted_cdf`: `m = 0` and `g = int(q*n > 0)`\r\n2. `averaged_inverted_cdf`: `m = 0` and `g = (1 + int(q*n > 0)) \/ 2`\r\n3. `closest_observation`: `m = -1\/2` and `1 - int((g == 0) & (j%2 == 0))`\r\n\r\nFor backward compatibility with previous versions of NumPy, `quantile` provides four additional discontinuous estimators. Like `method='linear'`, all have `m = 1 - q` so that `j = q*(n-1) \/\/ 1`, but `g` is defined as follows.\r\n\r\n- `lower`: `g = 0`\r\n- `midpoint`: `g = 0.5`\r\n- `higher`: `g = 1`\r\n- `nearest`: `g = (q*(n-1) % 1) > 0.5`","I think this would be an improvement to the currently difficult reading. You can ping me in a PR if you like.","Thanks. There were some updates to the original notes with the addition of weights, though. Can you suggest how the new material would be incorporated into this?","At the position where all the methods are explained.\r\nMy PR for weights just changed the top and bottom part of the notes section, but all the middle 95% of text is untouched."],"labels":["04 - Documentation"]},{"title":"ufuncs don't work on `recarray`","body":"### Describe the issue:\r\n\r\n`np.recarray` ufuncs don't work, even though they are documented. E.g. [`np.recarray.sum`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.recarray.sum.html#numpy.recarray.sum)\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nx = np.array([(1.0, 2), (3.0, 4)], dtype=[('x', '<f8'), ('y', '<i8')])\r\nx = x.view(np.recarray)\r\nx.sum()  # Raises numpy.core._exceptions._UFuncNoLoopError\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\user\\miniconda3\\envs\\oct\\lib\\site-packages\\numpy\\core\\_methods.py\", line 49, in _sum\r\n    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\r\nnumpy.core._exceptions._UFuncNoLoopError: ufunc 'add' did not contain a loop with signature matching types (dtype((numpy.record, [('x', '<f8'), ('y', '<i8')])), dtype((numpy.record, [('x', '<f8'), ('y', '<i8')]))) -> None\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.25.2\r\n3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["Passing an `axis` parameter doesn't work either. I'd expect the ufuncs to at least work over each field.","It's a common enough desire, but it doesn't really fit into the ufunc architecture. This is a missing desirable feature, but not one that is likely to ever be added. It would be good if this limitation were better documented, which is why I have switched the issue label. By and large, structured\/record arrays are basically containers, and you can do math on the individual fields once you pull them out, but not math operations on all of the fields at once.\r\n\r\nWe do [document how to register new ufunc loops](https:\/\/numpy.org\/doc\/1.25\/user\/c-info.ufunc-tutorial.html#example-numpy-ufunc-with-structured-array-dtype-arguments) for individual structures, though.","I understand that it doesn't fit into ufunc. So these exposed public APIs never work on any recarray without custom C extensions? Is that a good motivation to not only remove the docs, but also remove them from the recarray methods?","They are inherited from `np.ndarray`. Hard to remove them, per se. They'll still exist on true `ndarray` objects with structured dtypes (which we largely recommend over using `recarray`, in any case), so the most we can really do is change the documentation.","Sounds good. I'll think about the docs. We should definitely link to https:\/\/numpy.org\/doc\/1.25\/user\/c-info.ufunc-tutorial.html#example-numpy-ufunc-with-structured-array-dtype-arguments","@rkern Sorry just one more question- could you very briefly explain why its hard to apply ufuncs to every attribute separately?","The [ufunc infrastructure](https:\/\/numpy.org\/doc\/stable\/dev\/internals.code-explanations.html#universal-functions) is based around collections of C loop functions based on the dtypes of the inputs and outputs. Given the arrays which it is passed, the ufunc will look up which C loop function to use based on what's available and execute that loop function. `np.add`, for instance, has 22 different C loop functions, one for each of the basic dtypes that have an addition operation defined on them:\r\n\r\n```python\r\n>>> np.add.types\r\n['??->?',\r\n 'bb->b',\r\n 'BB->B',\r\n 'hh->h',\r\n 'HH->H',\r\n 'ii->i',\r\n 'II->I',\r\n 'll->l',\r\n 'LL->L',\r\n 'qq->q',\r\n 'QQ->Q',\r\n 'ee->e',\r\n 'ff->f',\r\n 'dd->d',\r\n 'gg->g',\r\n 'FF->F',\r\n 'DD->D',\r\n 'GG->G',\r\n 'Mm->M',\r\n 'mm->m',\r\n 'mM->M',\r\n 'OO->O']\r\n```\r\n\r\nFor ufuncs to operate on structured arrays (i.e. arrays with ad hoc dtypes with multiple fields), one would have to implement a C loop function for each of those fielded dtypes, explicitly. The example I linked to is one such, for one particular fielded dtype.","I will note that things look a bit different internally now, giving us more flexibility in adding such a new loop.  The issue is (and always has been), that you can only add one such loop for all structured dtypes.  The new machinery allows you to figure out a bit more though w.r.t. what to return exactly, etc.\r\n\r\nOne thing that nobody is working on but that is plausible would be making it easy to creat a new DType to type things more strictly and then explicitly allow this, like:\r\n```\r\n@something\r\nclass CoordinateDType:\r\n    x: float64\r\n    y: float64\r\n```\r\nwhich then could have the ability to make it very simply to say that `np.add` should just be component wise addition.  The internal machinery to do this exists very much (casting does it and is identical).  But ","@seberg I love this idea and I'd love to work on this. Could you point me to which new internal machinery you are referring to? Maybe link to the source file or some docs? Thanks"],"labels":["04 - Documentation"]},{"title":"BUG: Using the latex presentation of a Polynomial with coefficients of object dtype raises an error","body":"### Describe the issue:\n\nThe latex presentation is automatically used in environments like Jupyter notebook and Spyder. For coefficients of object dtype an exception is generated since `np.signbit` is not available.\n\n### Reproduce the code example:\n\n```python\nimport fractions\r\nfrom numpy.polynomial import Polynomial\r\n\r\nf=fractions.Fraction(2,3)\r\np=Polynomial([f, f])\r\np._repr_latex_()\n```\n\n\n### Error message:\n\n```shell\nTraceback (most recent call last):\r\n\r\n  Cell In[2], line 6\r\n    p._repr_latex_()\r\n\r\n  File C:\\develop\\env311\\Lib\\site-packages\\numpy\\polynomial\\_polybase.py:468 in _repr_latex_\r\n    elif not np.signbit(c):\r\n\r\nTypeError: ufunc 'signbit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n```\n\n\n### Runtime information:\n\n1.25.1\r\n3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n\n### Context for the issue:\n\n_No response_","comments":["Bit of a newcomer to these areas, but curious about the resolution of this issue.\r\n\r\nI've reproduced the same issue as described by the reporter of this issue; the formatting method definitely seems to support `fractions.Fraction`; `numpy.sign` also definitely supports `fractions.Fraction`, so it seems like the correct resolution here is to ensure support with `numpy.signbit`.  \r\n\r\nAlternatively, one could change the implementation of `numpy\\polynomial\\_polybase.py:468` to use a different sign method; the easiest thing to do here would probably be something like checking for `c<0` but there must be reasons that it was implemented this way that I'm not familiar with.\r\n\r\nIf someone familiar with this could point out the correct resolution path, I think it could be a pretty simple fix.\r\n\r\n","In https:\/\/github.com\/numpy\/numpy\/pull\/21760 the comparison with `c>0` is implemented. Changing the signbit seems harder, as there might be all kinds of objects where no signbit is implemented."],"labels":["00 - Bug"]},{"title":"DOC, CI: Add a \"skip\" option to only run docs-related CI checks","body":"Currently, for a regular docs PR we should not skip GitHub actions since that is where the [artifacts redirector](https:\/\/github.com\/numpy\/numpy\/blob\/main\/.github\/workflows\/circleci.yml) job is run.\r\n\r\nWe could add a `[docs only]` CI option that is equivalent to running only CircleCI, i.e.\r\n\r\n`[skip azp][skip cirrus][skip travis][skip actions]`\r\n\r\n**but** runs the artifacts redirector job.","comments":["When using `[skip azp][skip cirrus][skip travis][skip actions]` on the last commit on #24138, the redirector shows up, so maybe all we need is an alias. But wouldn't processing the alias require running some CI?","Travis CI is already gone, and hopefully Azure will soon be gone to. In that case we'd need `[skip cirrus] [skip actions]`. That could be aliased, but that then indeed requires more logic, so it may not be worth it. In particular, on GHA we'd need a \"check skip\" job that runs before _every_ other job, like we have now on Azure.\r\n\r\nOne related thing I've been thinking about is automatically checking if it's a docs-only PR, and skipping jobs based on that. It would also need trigger logic, which has the same problem as a `[docs only]` label. But it'd probably be more useful, since so many doc PRs are made by new contributors who aren't going to use the magic snippet in a commit message."],"labels":["04 - Documentation","component: CI"]},{"title":"ENH: Change default values for \"domain\" and \"window\" parameters of ``Polynomial`` class","body":"### Proposed new feature or change:\n\nThe current default values for both `domain` and `window` parameters are both ``np.array([-1, 1])``. On the other hand, if we call ``__init__`` with 3 parameters, then `domain` and `window` always get cast to `double`. This leads to the following behaviour:\r\n```python\r\n>>> p1 = P(np.array([1,2,3]))\r\n>>> p1\r\nPolynomial([1., 2., 3.], domain=[-1,  1], window=[-1,  1], symbol='x')\r\n>>> p2 = P(np.array([1,2,3]), np.array([-1, 1]), np.array([-1,1]))\r\n>>> p2\r\nPolynomial([1., 2., 3.], domain=[-1.,  1.], window=[-1.,  1.], symbol='x')\r\n>>> p1.domain.dtype\r\ndtype('int64')\r\n>>> p2.domain.dtype\r\ndtype('float64')\r\n```\r\nChanging the default values of the parameters ``domain`` and ``window`` to ``np.array([-1., 1.])`` will increase consistency between constructors ``Polynomial(coef)`` and ``Polynomial(coef, domain, window)``.","comments":["The conversion of the input arguments happens in `numpy.polynomial.polyutils.as_series`. Besides checking dimensions, it converts all input to a numpy array of type double, complex double or object. \r\n\r\nNote that also `numpy.polynomial.polyzero` is of integer type, even though the zero polynomial has coefficients `np.array([0.])`. (similar for `numpy.polynomial.polyone` and `numpy.polynomial.polyx`).\r\n\r\nIt is indeed odd that one cannot create the polynomial `p1` by setting the `domain` and `window` directly. Changing the code (e.g. https:\/\/github.com\/numpy\/numpy\/compare\/main...eendebakpt:polynomial_float_domain) and running unit testing results in no errors (expect for a test on the representation).\r\n\r\nMaybe there are cases with custom objects where the difference matters, but I could not find any examples. I tried creating something with `fractions.Fraction`, but that seems to work exactly the same (with either integer or float domain). For example \r\n```\r\nimport fractions\r\nfrom numpy.polynomial import Polynomial\r\n\r\nf=fractions.Fraction(2,3)\r\none=fractions.Fraction(1,1)\r\nzero=fractions.Fraction(0,1)\r\np=Polynomial([f, f])\r\nprint(p)\r\nprint(p.domain)\r\ny=p(f)\r\nprint(y)\r\nprint(type(y))\r\n\r\np=Polynomial([f, f], domain=[zero, one], window=[zero, one])\r\nprint(p)\r\nprint(p.domain)\r\ny=p(f)\r\nprint(y)\r\nprint(type(y))\r\n```\r\nhas output\r\n```\r\n2\/3 + 2\/3 x\r\n[-1  1]\r\n1.1111111111111112\r\n<class 'numpy.float64'>\r\n2\/3 + 2\/3 x\r\n[Fraction(0, 1) Fraction(1, 1)]\r\n10\/9\r\n<class 'fractions.Fraction'>\r\n```\r\n"],"labels":["01 - Enhancement","component: numpy.polynomial"]},{"title":"ENH: Speed up complex-real matrix multiplication","body":"### Proposed new feature or change:\r\n\r\nGiven a complex-valued matrix `C` of size `MxN` and a real-valued one `R` of size `NxQ` it should be possible to perform the `C @ R` operation between the two matrices using one strided `GEMM` call, exploiting the contiguous nature of the real and imaginary parts.\r\n\r\nI suppose MATLAB is performing something similar as I've noticed a 2x difference in execution time between `C * R` and `(R' * C')'`.","comments":["I've had a brief look at the ufunc machinery and couldn't see a simple way to implement this, the operands are directly coerced to a common type and there seems to be no way to define a custom C function for a given LHS+RHS pair.\r\n\r\nAny idea?"],"labels":["01 - Enhancement"]},{"title":"BUG: test failures on 32-bit platforms: i686 Linux and armv7l","body":"### Proposed new feature or change:\r\n\r\nIs it possible to run tests suite for 32-bit platforms in CI so it won't regress?\r\n\r\nHere are tests failures for numpy 1.25.1 on i686-linux (32-bit userspace on 64-bit kernel):\r\n\r\n```\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestRemainder::test_float_remainder_overflow - AssertionError: FloatingPointError not raised by divmod\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1\r\n= 3 failed, 35327 passed, 1637 skipped, 1308 deselected, 30 xfailed, 5 xpassed, 341 warnings in 280.71s (0:04:40) =\r\n```\r\n\r\nDetailed output:\r\n\r\n<details>\r\n\r\n```\r\nlib\/python3.10\/site-packages\/numpy\/typing\/tests\/test_runtime.py::TestRuntimeProtocol::test_issubclass[_NestedSequence] PASSED [100%]\r\n\r\n=================================== FAILURES ===================================\r\n_______________ TestUfunc.test_identityless_reduction_huge_array _______________\r\n\r\nself = <numpy.core.tests.test_ufunc.TestUfunc object at 0xdb5bb418>\r\n\r\n    @requires_memory(6 * 1024**3)\r\n    def test_identityless_reduction_huge_array(self):\r\n        # Regression test for gh-20921 (copying identity incorrectly failed)\r\n>       arr = np.zeros((2, 2**31), 'uint8')\r\nE       ValueError: Maximum allowed dimension exceeded\r\n\r\nself       = <numpy.core.tests.test_ufunc.TestUfunc object at 0xdb5bb418>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py:1622: ValueError\r\n_________________ TestRemainder.test_float_remainder_overflow __________________\r\n\r\nself = <numpy.core.tests.test_umath.TestRemainder object at 0xdb6e98c8>\r\n\r\n    @pytest.mark.skipif(IS_WASM, reason=\"fp errors don't work in wasm\")\r\n    def test_float_remainder_overflow(self):\r\n        a = np.finfo(np.float64).tiny\r\n        with np.errstate(over='ignore', invalid='ignore'):\r\n            div, mod = np.divmod(4, a)\r\n            np.isinf(div)\r\n            assert_(mod == 0)\r\n        with np.errstate(over='raise', invalid='ignore'):\r\n            assert_raises(FloatingPointError, np.divmod, 4, a)\r\n        with np.errstate(invalid='raise', over='ignore'):\r\n>           assert_raises(FloatingPointError, np.divmod, 4, a)\r\n\r\na          = 2.2250738585072014e-308\r\ndiv        = inf\r\nmod        = 0.0\r\nself       = <numpy.core.tests.test_umath.TestRemainder object at 0xdb6e98c8>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:828: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n..\/8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12\/lib\/python3.10\/unittest\/case.py:738: in assertRaises\r\n    return context.handle('assertRaises', args, kwargs)\r\n        args       = (<ufunc 'divmod'>, 4, 2.2250738585072014e-308)\r\n        context    = None\r\n        expected_exception = <class 'FloatingPointError'>\r\n        kwargs     = {}\r\n        self       = <numpy.testing._private.utils._Dummy testMethod=nop>\r\n..\/8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12\/lib\/python3.10\/unittest\/case.py:200: in handle\r\n    with self:\r\n        args       = [4, 2.2250738585072014e-308]\r\n        callable_obj = <ufunc 'divmod'>\r\n        kwargs     = {}\r\n        name       = 'assertRaises'\r\n        self       = None\r\n..\/8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12\/lib\/python3.10\/unittest\/case.py:223: in __exit__\r\n    self._raiseFailure(\"{} not raised by {}\".format(exc_name,\r\n        exc_name   = 'FloatingPointError'\r\n        exc_type   = None\r\n        exc_value  = None\r\n        self       = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>\r\n        tb         = None\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>\r\nstandardMsg = 'FloatingPointError not raised by divmod'\r\n\r\n    def _raiseFailure(self, standardMsg):\r\n        msg = self.test_case._formatMessage(self.msg, standardMsg)\r\n>       raise self.test_case.failureException(msg)\r\nE       AssertionError: FloatingPointError not raised by divmod\r\n\r\nmsg        = 'FloatingPointError not raised by divmod'\r\nself       = <unittest.case._AssertRaisesContext object at 0xc6dde5b0>\r\nstandardMsg = 'FloatingPointError not raised by divmod'\r\n\r\n..\/8hiddxp62lqzs9mrlizhn96akvzhvm39-python3-3.10.12\/lib\/python3.10\/unittest\/case.py:163: AssertionError\r\n______________________________ TestKind.test_int _______________________________\r\n\r\nself = <numpy.f2py.tests.test_kind.TestKind object at 0xda77e400>\r\n\r\n    def test_int(self):\r\n        \"\"\"Test `int` kind_func for integers up to 10**40.\"\"\"\r\n        selectedintkind = self.module.selectedintkind\r\n    \r\n        for i in range(40):\r\n>           assert selectedintkind(i) == selected_int_kind(\r\n                i\r\n            ), f\"selectedintkind({i}): expected {selected_int_kind(i)!r} but got {selectedintkind(i)!r}\"\r\nE           AssertionError: selectedintkind(19): expected 16 but got -1\r\nE           assert -1 == 16\r\nE            +  where -1 = <fortran function selectedintkind>(19)\r\nE            +  and   16 = selected_int_kind(19)\r\n\r\ni          = 19\r\nselectedintkind = <fortran function selectedintkind>\r\nself       = <numpy.f2py.tests.test_kind.TestKind object at 0xda77e400>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py:20: AssertionError\r\n=============================== warnings summary ===============================\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_numeric.py::TestNonarrayArgs::test_dunder_round_edgecases[2147483647--1]\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_numeric.py:200: RuntimeWarning: invalid value encountered in cast\r\n    assert_equal(round(val, ndigits), round(np.int32(val), ndigits))\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1935: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    x_f64 = np.float64(x_f32)\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1944: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert_array_max_ulp(myfunc(x_f64), np.float64(y_true128),\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1940: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert_equal(myfunc(x_f64), np.float64(y_true128))\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123])) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([[123]])) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"b\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"h\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"i\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"l\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"B\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"f\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"d\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234])) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([[234]])) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234]).astype(\"b\")) + 22) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"h\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"i\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"l\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"B\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"f\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/qs1ipdxdi47r3a0b5dgcqsxxs5rn9n4y-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"d\")) - 234.0) <= err\r\n\r\n-- Docs: https:\/\/docs.pytest.org\/en\/stable\/how-to\/capture-warnings.html\r\n=========================== short test summary info ============================\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestRemainder::test_float_remainder_overflow - AssertionError: FloatingPointError not raised by divmod\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1\r\n= 3 failed, 35327 passed, 1637 skipped, 1308 deselected, 30 xfailed, 5 xpassed, 341 warnings in 280.71s (0:04:40) =\r\n```\r\n\r\n<\/details>\r\n","comments":["On the armv7l-linux the situation is even worse (32-bit userspace on 64-bit kernel):\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py::TestEnvPrivation::test_impossible_feature_enable - AssertionError: Failed to generate error\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py::Test_ARM_Features::test_features - AssertionError: Failure Detection\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data16-escape16-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd89d12b0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data17-escape17-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd896c6d0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data18-escape18-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd8903cd0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data19-escape19-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd83852f8>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data20-escape20-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd832b8f8>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data21-escape21-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd872bee0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_real - AssertionError: selectedrealkind(16): expected 10 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_quad_precision - AssertionError: selectedrealkind(32): expected 16 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/random\/tests\/test_generator_mt19937.py::TestMultinomial::test_multinomial_pvals_float32 - Failed: DID NOT RAISE <class 'ValueError'>\r\nFAILED lib\/python3.10\/site-packages\/numpy\/random\/tests\/test_randomstate.py::TestMultinomial::test_multinomial_pvals_float32 - Failed: DID NOT RAISE <class 'ValueError'>\r\n= 14 failed, 34392 passed, 1124 skipped, 1308 deselected, 31 xfailed, 4 xpassed, 341 warnings in 667.39s (0:11:07) =\r\n```\r\n\r\nDetailed output:\r\n\r\n<details>\r\n\r\n```\r\nlib\/python3.10\/site-packages\/numpy\/typing\/tests\/test_runtime.py::TestRuntimeProtocol::test_issubclass[_NestedSequence] PASSED [100%]\r\n\r\n=================================== FAILURES ===================================\r\n_______________ TestEnvPrivation.test_impossible_feature_enable ________________\r\n\r\nself = <numpy.core.tests.test_cpu_features.TestEnvPrivation object at 0xeaf713b8>\r\n\r\n    def test_impossible_feature_enable(self):\r\n        \"\"\"\r\n        Test that a RuntimeError is thrown if an impossible feature-enabling\r\n        request is made. This includes enabling a feature not supported by the\r\n        machine, or disabling a baseline optimization.\r\n        \"\"\"\r\n    \r\n        if self.UNAVAILABLE_FEAT is None:\r\n            pytest.skip(\"There are no unavailable features to test with\")\r\n        bad_feature = self.UNAVAILABLE_FEAT\r\n        self.env['NPY_ENABLE_CPU_FEATURES'] = bad_feature\r\n        msg = (\r\n            f\"You cannot enable CPU features \\\\({bad_feature}\\\\), since \"\r\n            \"they are not supported by your machine.\"\r\n        )\r\n        err_type = \"RuntimeError\"\r\n        self._expect_error(msg, err_type)\r\n    \r\n        # Ensure that only the bad feature gets reported\r\n        feats = f\"{bad_feature}, {self.BASELINE_FEAT}\"\r\n        self.env['NPY_ENABLE_CPU_FEATURES'] = feats\r\n        msg = (\r\n            f\"You cannot enable CPU features \\\\({bad_feature}\\\\), since they \"\r\n            \"are not supported by your machine.\"\r\n        )\r\n>       self._expect_error(msg, err_type)\r\n\r\nbad_feature = 'ASIMDHP'\r\nerr_type   = 'RuntimeError'\r\nfeats      = 'ASIMDHP, None'\r\nmsg        = ('You cannot enable CPU features \\\\(ASIMDHP\\\\), since they are not supported '\r\n 'by your machine.')\r\nself       = <numpy.core.tests.test_cpu_features.TestEnvPrivation object at 0xeaf713b8>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py:318: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <numpy.core.tests.test_cpu_features.TestEnvPrivation object at 0xeaf713b8>\r\nmsg = 'You cannot enable CPU features \\\\(ASIMDHP\\\\), since they are not supported by your machine.'\r\nerr_type = 'RuntimeError', no_error_msg = 'Failed to generate error'\r\n\r\n    def _expect_error(\r\n        self,\r\n        msg,\r\n        err_type,\r\n        no_error_msg=\"Failed to generate error\"\r\n    ):\r\n        try:\r\n            self._run()\r\n        except subprocess.CalledProcessError as e:\r\n            assertion_message = f\"Expected: {msg}\\nGot: {e.stderr}\"\r\n            assert re.search(msg, e.stderr), assertion_message\r\n    \r\n            assertion_message = (\r\n                f\"Expected error of type: {err_type}; see full \"\r\n                f\"error:\\n{e.stderr}\"\r\n            )\r\n            assert re.search(err_type, e.stderr), assertion_message\r\n        else:\r\n>           assert False, no_error_msg\r\nE           AssertionError: Failed to generate error\r\nE           assert False\r\n\r\nerr_type   = 'RuntimeError'\r\nmsg        = ('You cannot enable CPU features \\\\(ASIMDHP\\\\), since they are not supported '\r\n 'by your machine.')\r\nno_error_msg = 'Failed to generate error'\r\nself       = <numpy.core.tests.test_cpu_features.TestEnvPrivation object at 0xeaf713b8>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py:184: AssertionError\r\n_______________________ Test_ARM_Features.test_features ________________________\r\n\r\nself = <numpy.core.tests.test_cpu_features.Test_ARM_Features object at 0xeaf718c8>\r\n\r\n    def test_features(self):\r\n        self.load_flags()\r\n        for gname, features in self.features_groups.items():\r\n            test_features = [self.cpu_have(f) for f in features]\r\n>           assert_features_equal(__cpu_features__.get(gname), all(test_features), gname)\r\nE           AssertionError: Failure Detection\r\nE            NAME: 'NEON_FP16'\r\nE            ACTUAL: True\r\nE            DESIRED: False\r\nE           \r\n###########################################\r\n### Extra debugging information\r\n###########################################\r\n-------------------------------------------\r\n--- NumPy Detections\r\n-------------------------------------------\r\n{MMX: False, SSE: False, SSE2: False, SSE3: False, SSSE3: False, SSE41: False, POPCNT: False, SSE42: False, AVX: False, F16C: False, XOP: False, FMA4: False, FMA3: False, AVX2: False, AVX512F: False, AVX512CD: False, AVX512ER: False, AVX512PF: False, AVX5124FMAPS: False, AVX5124VNNIW: False, AVX512VPOPCNTDQ: False, AVX512VL: False, AVX512BW: False, AVX512DQ: False, AVX512VNNI: False, AVX512IFMA: False, AVX512VBMI: False, AVX512VBMI2: False, AVX512BITALG: False, AVX512FP16: False, AVX512_KNL: False, AVX512_KNM: False, AVX512_SKX: False, AVX512_CLX: False, AVX512_CNL: False, AVX512_ICL: False, AVX512_SPR: False, VSX: False, VSX2: False, VSX3: False, VSX4: False, VX: False, VXE: False, VXE2: False, NEON: True, NEON_FP16: True, NEON_VFPV4: True, ASIMD: True, FPHP: False, ASIMDHP: False, ASIMDDP: False, ASIMDFHM: False}\r\n-------------------------------------------\r\n--- SYS \/ CPUINFO\r\n-------------------------------------------\r\nprocessor       : 0    \r\nmodel name      : ARMv8 Processor rev 1 (v8l)\r\nBogoMIPS        : 50.00 \r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt lpae evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 8\r\nCPU variant     : 0x3\r\nCPU part        : 0xd0c \r\nCPU revision    : 1 \r\nE           \r\nprocessor       : 1    \r\nmodel name      : ARMv8 Processor rev 1 (v8l)\r\nBogoMIPS        : 50.00 \r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt lpae evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 8\r\nCPU variant     : 0x3\r\nCPU part        : 0xd0c \r\nCPU revision    : 1 \r\nE           \r\nprocessor       : 2    \r\nmodel name      : ARMv8 Processor rev 1 (v8l)\r\nBogoMIPS        : 50.00 \r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt lpae evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 8\r\nCPU variant     : 0x3\r\nCPU part        : 0xd0c \r\nCPU revision    : 1 \r\nE           \r\nprocessor       : 3    \r\nmodel name      : ARMv8 Processor rev 1 (v8l)\r\nBogoMIPS        : 50.00 \r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt lpae evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 8\r\nCPU variant     : 0x3\r\nCPU part        : 0xd0c \r\nCPU revision    : 1 \r\nE           \r\n....mE           \r\n-------------------------------------------\r\n--- SYS \/ AUXV   \r\n-------------------------------------------\r\n[Errno 2] No such file or directory: '\/bin\/true'\r\n\r\nfeatures   = ['NEON', 'HALF']\r\ngname      = 'NEON_FP16'\r\nself       = <numpy.core.tests.test_cpu_features.Test_ARM_Features object at 0xeaf718c8>\r\ntest_features = [False, False]\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py:77: AssertionError\r\n_______________ TestUfunc.test_identityless_reduction_huge_array _______________\r\n\r\nself = <numpy.core.tests.test_ufunc.TestUfunc object at 0xe81844c0>\r\n\r\n    @requires_memory(6 * 1024**3)\r\n    def test_identityless_reduction_huge_array(self):\r\n        # Regression test for gh-20921 (copying identity incorrectly failed)\r\n>       arr = np.zeros((2, 2**31), 'uint8')\r\nE       ValueError: Maximum allowed dimension exceeded\r\n\r\nself       = <numpy.core.tests.test_ufunc.TestUfunc object at 0xe81844c0>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py:1622: ValueError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data16-escape16-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72c7490>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [0.5, 0.5, 0.5, inf]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([0.5, 0.5, 0.5, inf], dtype=float32)\r\ndata       = [0.5, 0.5, 0.5, inf]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>,\r\n <ufunc 'arccosh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72c7490>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd89d1250>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd89d12b0>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd89d1250>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data17-escape17-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72756a0>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [0.5, 0.5, 0.5, inf, 0.5, 0.5, ...]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5,\r\n       0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5,\r\n       0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5,\r\n       inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf,\r\n       0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5,\r\n       0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5,\r\n       0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5,\r\n       inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf,\r\n       0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5,\r\n       0.5, 0.5, inf, 0.5, 0.5, 0.5, inf, 0.5, 0.5, 0.5, inf],\r\n      dtype=float32)\r\ndata       = [0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf,\r\n 0.5,\r\n 0.5,\r\n 0.5,\r\n inf]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>,\r\n <ufunc 'arccosh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72756a0>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd896c658>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd896c6d0>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd896c658>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data18-escape18-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72a38b0>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [inf, 1.0, 1.0, 1.0]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([inf,  1.,  1.,  1.], dtype=float32)\r\ndata       = [inf, 1.0, 1.0, 1.0]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe72a38b0>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd8903c58>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd8903cd0>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd8903c58>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data19-escape19-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe7252ac0>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [inf, 1.0, 1.0, 1.0, inf, 1.0, ...]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,\r\n        1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,\r\n        1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,\r\n        1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1.,\r\n       inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,\r\n        1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,\r\n        1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,\r\n        1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1.,\r\n       inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1., inf,\r\n        1.,  1.,  1., inf,  1.,  1.,  1., inf,  1.,  1.,  1.],\r\n      dtype=float32)\r\ndata       = [inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0,\r\n inf,\r\n 1.0,\r\n 1.0,\r\n 1.0]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe7252ac0>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd8385280>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd83852f8>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd8385280>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data20-escape20-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe71ffcd0>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [inf]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([inf], dtype=float32)\r\ndata       = [inf]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe71ffcd0>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd832b880>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd832b8f8>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd832b880>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n__ TestSpecialFloats.test_unary_spurious_fpexception[data21-escape21-f-sqrt] ___\r\n\r\nself = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe722eee0>\r\nufunc = <ufunc 'sqrt'>, dtype = 'f', data = [inf, inf, inf, inf, inf, inf, ...]\r\nescape = [<ufunc 'cos'>, <ufunc 'sin'>, <ufunc 'tan'>, <ufunc 'arccos'>, <ufunc 'arcsin'>, <ufunc 'spacing'>, ...]\r\n\r\n    @pytest.mark.parametrize(\"ufunc\", UFUNCS_UNARY_FP)\r\n    @pytest.mark.parametrize(\"dtype\", ('e', 'f', 'd'))\r\n    @pytest.mark.parametrize(\"data, escape\", (\r\n        ([0.03], LTONE_INVALID_ERR),\r\n        ([0.03]*32, LTONE_INVALID_ERR),\r\n        # neg\r\n        ([-1.0], NEG_INVALID_ERR),\r\n        ([-1.0]*32, NEG_INVALID_ERR),\r\n        # flat\r\n        ([1.0], ONE_INVALID_ERR),\r\n        ([1.0]*32, ONE_INVALID_ERR),\r\n        # zero\r\n        ([0.0], BYZERO_ERR),\r\n        ([0.0]*32, BYZERO_ERR),\r\n        ([-0.0], BYZERO_ERR),\r\n        ([-0.0]*32, BYZERO_ERR),\r\n        # nan\r\n        ([0.5, 0.5, 0.5, np.nan], LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.nan]*32, LTONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0], ONE_INVALID_ERR),\r\n        ([np.nan, 1.0, 1.0, 1.0]*32, ONE_INVALID_ERR),\r\n        ([np.nan], []),\r\n        ([np.nan]*32, []),\r\n        # inf\r\n        ([0.5, 0.5, 0.5, np.inf], INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, np.inf]*32, INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0], INF_INVALID_ERR),\r\n        ([np.inf, 1.0, 1.0, 1.0]*32, INF_INVALID_ERR),\r\n        ([np.inf], INF_INVALID_ERR),\r\n        ([np.inf]*32, INF_INVALID_ERR),\r\n        # ninf\r\n        ([0.5, 0.5, 0.5, -np.inf],\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([0.5, 0.5, 0.5, -np.inf]*32,\r\n         NEG_INVALID_ERR + INF_INVALID_ERR + LTONE_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf, 1.0, 1.0, 1.0]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf], NEG_INVALID_ERR + INF_INVALID_ERR),\r\n        ([-np.inf]*32, NEG_INVALID_ERR + INF_INVALID_ERR),\r\n    ))\r\n    def test_unary_spurious_fpexception(self, ufunc, dtype, data, escape):\r\n        if escape and ufunc in escape:\r\n            return\r\n        # FIXME: NAN raises FP invalid exception:\r\n        #  - ceil\/float16 on MSVC:32-bit\r\n        #  - spacing\/float16 on almost all platforms\r\n        if ufunc in (np.spacing, np.ceil) and dtype == 'e':\r\n            return\r\n        array = np.array(data, dtype=dtype)\r\n>       with assert_no_warnings():\r\n\r\narray      = array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\r\n       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\r\n       inf, inf, inf, inf, inf, inf], dtype=float32)\r\ndata       = [inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf,\r\n inf]\r\ndtype      = 'f'\r\nescape     = [<ufunc 'cos'>,\r\n <ufunc 'sin'>,\r\n <ufunc 'tan'>,\r\n <ufunc 'arccos'>,\r\n <ufunc 'arcsin'>,\r\n <ufunc 'spacing'>,\r\n <ufunc 'arctanh'>]\r\nself       = <numpy.core.tests.test_umath.TestSpecialFloats object at 0xe722eee0>\r\nufunc      = <ufunc 'sqrt'>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1816: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <contextlib._GeneratorContextManager object at 0xd872bf58>, typ = None\r\nvalue = None, traceback = None\r\n\r\n    def __exit__(self, typ, value, traceback):\r\n        if typ is None:\r\n            try:\r\n>               next(self.gen)\r\nE               AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd872bee0>]\r\n\r\nself       = <contextlib._GeneratorContextManager object at 0xd872bf58>\r\ntraceback  = None\r\ntyp        = None\r\nvalue      = None\r\n\r\n..\/7sabgh5043q47z01anhr04md95jb2x7i-python3-3.10.12\/lib\/python3.10\/contextlib.py:142: AssertionError\r\n______________________________ TestKind.test_int _______________________________\r\n\r\nself = <numpy.f2py.tests.test_kind.TestKind object at 0xe6caae98>\r\n\r\n    def test_int(self):\r\n        \"\"\"Test `int` kind_func for integers up to 10**40.\"\"\"\r\n        selectedintkind = self.module.selectedintkind\r\n    \r\n        for i in range(40):\r\n>           assert selectedintkind(i) == selected_int_kind(\r\n                i\r\n            ), f\"selectedintkind({i}): expected {selected_int_kind(i)!r} but got {selectedintkind(i)!r}\"\r\nE           AssertionError: selectedintkind(19): expected 16 but got -1\r\nE           assert -1 == 16\r\nE            +  where -1 = <fortran function selectedintkind>(19)\r\nE            +  and   16 = selected_int_kind(19)\r\n\r\ni          = 19\r\nselectedintkind = <fortran function selectedintkind>\r\nself       = <numpy.f2py.tests.test_kind.TestKind object at 0xe6caae98>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py:20: AssertionError\r\n______________________________ TestKind.test_real ______________________________\r\n\r\nself = <numpy.f2py.tests.test_kind.TestKind object at 0xe6b3d9b8>\r\n\r\n    def test_real(self):\r\n        \"\"\"\r\n        Test (processor-dependent) `real` kind_func for real numbers\r\n        of up to 31 digits precision (extended\/quadruple).\r\n        \"\"\"\r\n        selectedrealkind = self.module.selectedrealkind\r\n    \r\n        for i in range(32):\r\n>           assert selectedrealkind(i) == selected_real_kind(\r\n                i\r\n            ), f\"selectedrealkind({i}): expected {selected_real_kind(i)!r} but got {selectedrealkind(i)!r}\"\r\nE           AssertionError: selectedrealkind(16): expected 10 but got -1\r\nE           assert -1 == 10\r\nE            +  where -1 = <fortran function selectedrealkind>(16)\r\nE            +  and   10 = selected_real_kind(16)\r\n\r\ni          = 16\r\nselectedrealkind = <fortran function selectedrealkind>\r\nself       = <numpy.f2py.tests.test_kind.TestKind object at 0xe6b3d9b8>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py:32: AssertionError\r\n_________________________ TestKind.test_quad_precision _________________________\r\n\r\nself = <numpy.f2py.tests.test_kind.TestKind object at 0xe6bee640>\r\n\r\n    @pytest.mark.xfail(platform.machine().lower().startswith(\"ppc\"),\r\n                       reason=\"Some PowerPC may not support full IEEE 754 precision\")\r\n    def test_quad_precision(self):\r\n        \"\"\"\r\n        Test kind_func for quadruple precision [`real(16)`] of 32+ digits .\r\n        \"\"\"\r\n        selectedrealkind = self.module.selectedrealkind\r\n    \r\n        for i in range(32, 40):\r\n>           assert selectedrealkind(i) == selected_real_kind(\r\n                i\r\n            ), f\"selectedrealkind({i}): expected {selected_real_kind(i)!r} but got {selectedrealkind(i)!r}\"\r\nE           AssertionError: selectedrealkind(32): expected 16 but got -1\r\nE           assert -1 == 16\r\nE            +  where -1 = <fortran function selectedrealkind>(32)\r\nE            +  and   16 = selected_real_kind(32)\r\n\r\ni          = 32\r\nselectedrealkind = <fortran function selectedrealkind>\r\nself       = <numpy.f2py.tests.test_kind.TestKind object at 0xe6bee640>\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py:45: AssertionError\r\n________________ TestMultinomial.test_multinomial_pvals_float32 ________________\r\n\r\nself = <numpy.random.tests.test_generator_mt19937.TestMultinomial object at 0xe4929550>\r\n\r\n    def test_multinomial_pvals_float32(self):\r\n        x = np.array([9.9e-01, 9.9e-01, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09,\r\n                      1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09], dtype=np.float32)\r\n        pvals = x \/ x.sum()\r\n        random = Generator(MT19937(1432985819))\r\n        match = r\"[\\w\\s]*pvals array is cast to 64-bit floating\"\r\n>       with pytest.raises(ValueError, match=match):\r\nE       Failed: DID NOT RAISE <class 'ValueError'>\r\n\r\nmatch      = '[\\\\w\\\\s]*pvals array is cast to 64-bit floating'\r\npvals      = array([4.9999997e-01, 4.9999997e-01, 5.0505045e-10, 5.0505045e-10,\r\n       5.0505045e-10, 5.0505045e-10, 5.0505045e-10, 5.0505045e-10,\r\n       5.0505045e-10, 5.0505045e-10], dtype=float32)\r\nrandom     = Generator(MT19937) at 0xDA5DD980\r\nself       = <numpy.random.tests.test_generator_mt19937.TestMultinomial object at 0xe4929550>\r\nx          = array([9.9e-01, 9.9e-01, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09,\r\n       1.0e-09, 1.0e-09, 1.0e-09], dtype=float32)\r\n\r\nlib\/python3.10\/site-packages\/numpy\/random\/tests\/test_generator_mt19937.py:145: Failed\r\n________________ TestMultinomial.test_multinomial_pvals_float32 ________________\r\n\r\nself = <numpy.random.tests.test_randomstate.TestMultinomial object at 0xe49960a0>\r\n\r\n    def test_multinomial_pvals_float32(self):\r\n        x = np.array([9.9e-01, 9.9e-01, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09,\r\n                      1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09], dtype=np.float32)\r\n        pvals = x \/ x.sum()\r\n        match = r\"[\\w\\s]*pvals array is cast to 64-bit floating\"\r\n>       with pytest.raises(ValueError, match=match):\r\nE       Failed: DID NOT RAISE <class 'ValueError'>\r\n\r\nmatch      = '[\\\\w\\\\s]*pvals array is cast to 64-bit floating'\r\npvals      = array([4.9999997e-01, 4.9999997e-01, 5.0505045e-10, 5.0505045e-10,\r\n       5.0505045e-10, 5.0505045e-10, 5.0505045e-10, 5.0505045e-10,\r\n       5.0505045e-10, 5.0505045e-10], dtype=float32)\r\nself       = <numpy.random.tests.test_randomstate.TestMultinomial object at 0xe49960a0>\r\nx          = array([9.9e-01, 9.9e-01, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09, 1.0e-09,\r\n       1.0e-09, 1.0e-09, 1.0e-09], dtype=float32)\r\n\r\nlib\/python3.10\/site-packages\/numpy\/random\/tests\/test_randomstate.py:183: Failed\r\n=============================== warnings summary ===============================\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_numeric.py::TestNonarrayArgs::test_dunder_round_edgecases[2147483647--1]\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_numeric.py:200: RuntimeWarning: invalid value encountered in cast\r\n    assert_equal(round(val, ndigits), round(np.int32(val), ndigits))\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1935: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    x_f64 = np.float64(x_f32)\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1944: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert_array_max_ulp(myfunc(x_f64), np.float64(y_true128),\r\n\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\nlib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestAVXUfuncs::test_avx_based_ufunc\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py:1940: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert_equal(myfunc(x_f64), np.float64(y_true128))\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123])) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:17: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([[123]])) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"b\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"h\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"i\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"l\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"B\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"f\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py: 20 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_integer.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert t(array([123], \"d\")) == 123\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:23: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234])) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([[234]])) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:25: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234]).astype(\"b\")) + 22) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"h\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"i\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"l\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"B\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"f\")) - 234.0) <= err\r\n\r\nlib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py: 16 warnings\r\n  \/nix\/store\/yv7zrmqbiw2jm4wygw91l1hvhq7k3ilr-python3.10-numpy-1.25.1\/lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_return_real.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n    assert abs(t(array([234], \"d\")) - 234.0) <= err\r\n\r\n-- Docs: https:\/\/docs.pytest.org\/en\/stable\/how-to\/capture-warnings.html\r\n=========================== short test summary info ============================\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py::TestEnvPrivation::test_impossible_feature_enable - AssertionError: Failed to generate error\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_cpu_features.py::Test_ARM_Features::test_features - AssertionError: Failure Detection\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_ufunc.py::TestUfunc::test_identityless_reduction_huge_array - ValueError: Maximum allowed dimension exceeded\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data16-escape16-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd89d12b0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data17-escape17-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd896c6d0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data18-escape18-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd8903cd0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data19-escape19-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd83852f8>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data20-escape20-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd832b8f8>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_umath.py::TestSpecialFloats::test_unary_spurious_fpexception[data21-escape21-f-sqrt] - AssertionError: Got warnings: [<warnings.WarningMessage object at 0xd872bee0>]\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_int - AssertionError: selectedintkind(19): expected 16 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_real - AssertionError: selectedrealkind(16): expected 10 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/f2py\/tests\/test_kind.py::TestKind::test_quad_precision - AssertionError: selectedrealkind(32): expected 16 but got -1\r\nFAILED lib\/python3.10\/site-packages\/numpy\/random\/tests\/test_generator_mt19937.py::TestMultinomial::test_multinomial_pvals_float32 - Failed: DID NOT RAISE <class 'ValueError'>\r\nFAILED lib\/python3.10\/site-packages\/numpy\/random\/tests\/test_randomstate.py::TestMultinomial::test_multinomial_pvals_float32 - Failed: DID NOT RAISE <class 'ValueError'>\r\n= 14 failed, 34392 passed, 1124 skipped, 1308 deselected, 31 xfailed, 4 xpassed, 341 warnings in 667.39s (0:11:07) =\r\n```\r\n\r\n<\/details>\r\n","We do have CI jobs that should cover this:\r\n- 32-bit Linux, see [Azure config](https:\/\/github.com\/numpy\/numpy\/blob\/e841e6337eca767065e27129903dea5ae199fde3\/azure-pipelines.yml#L60)\r\n- 32-bit Python on Windows, see [GHA job](https:\/\/github.com\/numpy\/numpy\/blob\/e841e6337eca767065e27129903dea5ae199fde3\/.github\/workflows\/windows.yml#L88)\r\n- armv7 (running a subset of tests under QEMU), see [GHA job](https:\/\/github.com\/numpy\/numpy\/blob\/e841e6337eca767065e27129903dea5ae199fde3\/.github\/workflows\/linux_simd.yml#L165)\r\n\r\nIt's not water-tight, but that's basically impossible. These look like a fairly limited set of failures that we should fix up - but it's business as usual, not a major gap in our CI config as far as I can tell."],"labels":["00 - Bug"]},{"title":"MAINT: Improve performance of polynomial operations (2)","body":"For many polynomial operations (e.g. add, multiply, negate) the operation takes place on the coefficients of the operands and then a new `Polynomial` is created. For the creation of the new `Polynomial` we can skip the validation of input arguments, as they are either passed from the operands (which have already been validated) or constructed with internal methods (which have validated output).\r\n\r\nBenchmark results:\r\n```\r\np + q: Mean +- std dev: [main0] 31.5 us +- 2.1 us -> [prx] 17.4 us +- 0.6 us: 1.81x faster\r\np * q: Mean +- std dev: [main0] 32.4 us +- 1.3 us -> [prx] 18.3 us +- 1.0 us: 1.77x faster\r\n2 * p: Mean +- std dev: [main0] 24.3 us +- 1.3 us -> [prx] 11.7 us +- 0.4 us: 2.08x faster\r\n-p: Mean +- std dev: [main0] 14.6 us +- 0.6 us -> [prx] 2.50 us +- 0.11 us: 5.84x faster\r\n\r\nGeometric mean: 2.50x faster\r\n```\r\nwith script:\r\n```\r\nimport pyperf\r\nfrom numpy.polynomial import Polynomial\r\n\r\nsetup = \"\"\"\r\nfrom numpy.polynomial import Polynomial\r\n\r\np=Polynomial([.5,4,5])\r\nq=Polynomial([1,0,5])\r\n\"\"\"\r\n\r\nrunner = pyperf.Runner()\r\nrunner.timeit(name=f\"p + q\", stmt=\"_ = p + q\", setup=setup)\r\nrunner.timeit(name=f\"p * q\", stmt=\"_ = p * q\", setup=setup)\r\nrunner.timeit(name=f\"2 * p\", stmt=\"_ = 2 * q\", setup=setup)\r\nrunner.timeit(name=f\"-p\", stmt=\"_ = - p\", setup=setup)\r\n```\r\nNotes:\r\n\r\n* The `as_series` and `ABCPolyBase.__init__` have an additional flag `_validate_input`. The default is `True` (so the PR is backwards compatible), but for internal operations we set it to `False`\r\n* Testing was performed by with https:\/\/github.com\/numpy\/numpy\/pull\/24499 included in both main and the PR. Without https:\/\/github.com\/numpy\/numpy\/pull\/24499 there is still a significant performance gain\r\n* We can do even more optimization if we refactor the `polyadd` and\/or `polyutils._add` to have a `_validate_input` flag as well. But this requires more refactoring and the `polyadd` is public, so this has not yet been done.\r\n*  The modifications have been made for `__add__`, `__neg__`, `__mul__` and `__rmul__`. If this PR is considered a merge candidate, we can modify other operations as well.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":[],"labels":["component: numpy.polynomial","03 - Maintenance"]},{"title":"BUG: ufuncs created with np.frompyfunc do not recognize identity in reduce in numpy 1.25.X","body":"### Describe the issue:\r\n\r\nStarting in numpy 1.25.0, it appears that the `ufunc.reduce` method does not recognize the `identity` argument for ufuncs created with `np.frompyfunc`\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nimport operator\r\n\r\nprint(np.__version__)\r\n\r\nx = np.arange(10)\r\nmask = x % 2 == 0\r\nprint(np.add.reduce(x, where=mask))\r\n\r\nmy_add = np.frompyfunc(operator.add, nin=2, nout=1, identity=0)\r\nprint(my_add.reduce(x, where=mask))\r\n```\r\n\r\n\r\n### Error message:\r\n\r\nResult in numpy 1.24.4:\r\n\r\n```shell\r\n1.24.4\r\n20\r\n20\r\n```\r\nResult in numpy 1.25.0:\r\n```shell\r\n1.25.0\r\n20\r\n```\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-7-a7d31df3c91a>](https:\/\/localhost:8080\/#) in <cell line: 11>()\r\n      9 \r\n     10 my_add = np.frompyfunc(operator.add, nin=2, nout=1, identity=0)\r\n---> 11 print(my_add.reduce(x, where=mask))\r\n\r\nValueError: reduction operation 'add (vectorized)' does not have an identity, so to use a where mask one has to specify 'initial'\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.25.0\r\n3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\n>>> print(numpy.show_runtime())\r\n[{'numpy_version': '1.25.0',\r\n  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',\r\n  'uname': uname_result(system='Linux', node='e87715ee78c7', release='5.15.109+', version='#1 SMP Fri Jun 9 10:57:30 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-7a851222.3.23.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 2,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23'}]\r\nNone\r\n```\r\n\r\n### Context for the issue:\r\n\r\nI am implementing `jax.numpy.frompyfunc` and had to special-case the tests in order to work around this bug: https:\/\/github.com\/google\/jax\/pull\/17275","comments":["I bisected this to https:\/\/github.com\/numpy\/numpy\/pull\/20970.","Maybe we should back #20970 out of 1.26? @seberg, thoughts?\r\n\r\nEdit: and also the 1.25 maintenance branch","I suspect fixing this is easier than backing it out, which may create other conflicts.","This is a bit weird, because what changed here is a bug fix in a sense:\r\n* `my_add.reduce(np.array([1, 2], dtype=object), where=[True, False])`  **always** failed.  This is because we don't assume the identity for object dtype reductions normally.\r\n* The code used the wrong dtype to check for \"object\":  That of the input array and not that of the loop.\r\n\r\nYou will see the same change for `np.add`.  We could probably work-around this for `frompyfunc` by just forcing the `identity` to be always used (leading to things like `my_add.reduce([\"a\", \"b\"])` stopping to work when `identity` is provided.)","Looks like this was a regression in 1.25.0 so important to fix - but not specific to 2.0. So I will bump the milestone to 2.1.0 to get this off of the critical path for 2.0.","Actually if we do fix it relatively soon, ideally it will be backported to 1.26.5 as well."],"labels":["00 - Bug","09 - Backport-Candidate"]},{"title":"BUG: masked std and median on unmasked array result in invalid masked array","body":"### Describe the issue:\r\n\r\nSince version 1.24, the code example below results in a masked array where the data array and the mask array don't have the same shape\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nprint(np.__version__)\r\n\r\nrng = np.random.default_rng(0)\r\n\r\ndata = rng.normal(size=(2, 101))\r\n\r\ndata[:, 2] = np.nan\r\n\r\nstd = np.ma.std(data, axis=1)\r\nmedian = np.ma.median(data, axis=1)\r\n\r\nprint(\"median:\")\r\nprint(repr(median))\r\nprint(\"std:\")\r\nprint(repr(std))\r\n\r\ndeviation = data - median[:, np.newaxis]\r\n\r\ncomparison = deviation < 0.5 * std[:, np.newaxis]\r\n\r\nprint(comparison.shape, comparison.mask.shape)\r\nprint(comparison)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n\r\nOutput under 1.23:\r\n\r\n```\r\n1.23.5\r\nmedian:\r\nmasked_array(data=[nan, nan],\r\n             mask=False,\r\n       fill_value=1e+20)\r\nstd:\r\nmasked_array(data=[--, --],\r\n             mask=[ True,  True],\r\n       fill_value=1e+20,\r\n            dtype=float64)\r\n(2, 101) (2, 101)\r\n[[-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- --]\r\n [-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\r\n  -- -- -- --]]\r\n```\r\n\r\n\r\nOutput under 1.25 (also 1.24):\r\n```\r\n1.25.2\r\nmedian:\r\nmasked_array(data=[nan, nan],\r\n             mask=False,\r\n       fill_value=1e+20)\r\nstd:\r\nmasked_array(data=[--, --],\r\n             mask=[ True,  True],\r\n       fill_value=1e+20,\r\n            dtype=float64)\r\n(2, 101) (2, 1)\r\nTraceback (most recent call last):\r\n  File \"\/home\/mnoethe\/test_numpy_ma_std.py\", line 23, in <module>\r\n    print(comparison)\r\n  File \"\/home\/mnoethe\/.local\/conda\/envs\/numpy-1.25\/lib\/python3.10\/site-packages\/numpy\/ma\/core.py\", line 3997, in __str__\r\n    return str(self._insert_masked_print())\r\n  File \"\/home\/mnoethe\/.local\/conda\/envs\/numpy-1.25\/lib\/python3.10\/site-packages\/numpy\/ma\/core.py\", line 3991, in _insert_masked_print\r\n    _recursive_printoption(res, mask, masked_print_option)\r\n  File \"\/home\/mnoethe\/.local\/conda\/envs\/numpy-1.25\/lib\/python3.10\/site-packages\/numpy\/ma\/core.py\", line 2437, in _recursive_printoption\r\n    np.copyto(result, printopt, where=mask)\r\nValueError: could not broadcast where mask from shape (2,2) into shape (2,100)\r\n```\r\n\r\n\r\n\r\n### Runtime information:\r\n\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) '\r\n            '[GCC 12.3.0]',\r\n  'uname': uname_result(system='Linux', node='e5b-dell-12', release='5.14.0-1051-oem', version='#58-Ubuntu SMP Fri Aug 26 05:50:00 UTC 2022', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL',\r\n                                    'AVX512_SPR']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': '\/home\/mnoethe\/.local\/conda\/envs\/numpy-1.25\/lib\/libopenblasp-r0.3.23.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 20,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23'}]\r\n\r\n### Context for the issue:\r\n\r\nMost confusingly, the example above works fine with numpy 1.25 if the shape of the data array is `(2, 100)` (just one element smaller in the last dimension).","comments":["I'm not entirely sure if this solves your problem but I think this is resolved in the most recent `2.0.0.dev0+git20230830.b73a5ae` version. \r\n![Screenshot 2023-09-01 at 8 01 05 PM](https:\/\/github.com\/numpy\/numpy\/assets\/24905907\/87c795e2-36b7-42ec-80fa-58926cf27802)\r\n","Why do std and median have different masks?\r\n\r\nWhy is the median Nan unmasked but std masked?"],"labels":["00 - Bug","component: numpy.ma"]},{"title":"DOC: add description of dtype b1 in arrays.dtypes.rst","body":"Closes #23366, closes #24627\r\n\r\nThe documentation of https:\/\/numpy.org\/doc\/stable\/reference\/arrays.dtypes.html is incomplete. \r\nSection '**Array-protocol type strings**' provides a list of characters and the type they represent, among these are the following:\r\n```\r\n'?'  boolean\r\n'b'  (signed) byte\r\n```\r\n\r\nThe section also specifies the following:\r\n> The first character specifies the kind of data and the remaining characters specify the number of bytes per item\r\n\r\nThe type \"b1\" is an exception to this rule, but it isn't mentioned. According to the text this would indicate that b -> 'type byte' and 1 -> '1 byte'. But ``b1`` actually indicates a bool, so this dtype is specified by _both_ the first letter as well as the number. See the code snippet and the resulting output below.\r\n\r\n```\r\n>>> numpy.dtype('?')\r\ndtype('bool')\r\n>>> numpy.dtype('b')\r\ndtype('int8')\r\n>>> numpy.dtype('b1')\r\ndtype('bool')\r\n```\r\n\r\nI therefore modified the doc, mentioning ``b1`` as being an exception to the rule, and also adding it to the list of types.","comments":["Thanks @dvorst - seems reasonable, maybe @ngoldbaum can take a look?","Sorry for missing this, I was on vacation last week.\r\n\r\nI think it would be nice to have a little more discussion explaining what the character-bitsize typecodes are in general, and that `b1` for boolean is a weird exception to a more general naming scheme.","See https:\/\/github.com\/numpy\/numpy\/issues\/24627 for a related ask.","@ngoldbaum, good suggestion, I checked the types a bit further, and the description of the documentation acutally doesn't match in quite some cases. The code below produces a table with all the possible kinds on the index, and number of bytes as the columns (empty column is when no number of bytes is provided). As shown, many combinations are not valid, which is not mentioned in the documentation. I'll update the PR somewhere after the weekend.\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nkinds = [\"?\", \"b\", \"B\", \"i\", \"u\", \"f\", \"c\", \"m\", \"M\", \"O\", \"S\", \"a\", \"U\", \"V\"]\r\nbytes = [\"\", \"1\", \"2\", \"3\", \"4\", \"5\"]\r\ndtypes = {}\r\nfor byte in bytes:\r\n    dtypes[byte] = {}\r\n    for kind in kinds:\r\n        dtype = kind + byte\r\n        try:\r\n            dtypes[byte][kind] = str(np.dtype(dtype))\r\n        except:\r\n            dtypes[byte][kind] = 'ERROR'\r\ndf = pd.DataFrame(dtypes)\r\ndisplay(df)\r\n\r\ndf = df.applymap(lambda x: x.replace('|', '\\|') if isinstance(x, str) else x)\r\nprint(df.to_markdown())\r\n```\r\n|    |             | 1     | 2       | 3     | 4       | 5     |\r\n|:---|:------------|:------|:--------|:------|:--------|:------|\r\n| ?  | bool        | ERROR | ERROR   | ERROR | ERROR   | ERROR |\r\n| b  | int8        | bool  | ERROR   | ERROR | ERROR   | ERROR |\r\n| B  | uint8       | ERROR | ERROR   | ERROR | ERROR   | ERROR |\r\n| i  | int32       | int8  | int16   | ERROR | int32   | ERROR |\r\n| u  | ERROR       | uint8 | uint16  | ERROR | uint32  | ERROR |\r\n| f  | float32     | ERROR | float16 | ERROR | float32 | ERROR |\r\n| c  | \\|S1        | ERROR | ERROR   | ERROR | ERROR   | ERROR |\r\n| m  | timedelta64 | ERROR | ERROR   | ERROR | ERROR   | ERROR |\r\n| M  | datetime64  | ERROR | ERROR   | ERROR | ERROR   | ERROR |\r\n| O  | object      | ERROR | ERROR   | ERROR | object  | ERROR |\r\n| S  | \\|S0        | \\|S1  | \\|S2    | \\|S3  | \\|S4    | \\|S5  |\r\n| a  | \\|S0        | \\|S1  | \\|S2    | \\|S3  | \\|S4    | \\|S5  |\r\n| U  | <U0         | <U1   | <U2     | <U3   | <U4     | <U5   |\r\n| V  | \\|V0        | \\|V1  | \\|V2    | \\|V3  | \\|V4    | \\|V5  |","Looking at the table description, I agree that it's not quite right. Here's my understanding of how it works:\r\n\r\n`i`, `u` and `f` are shorthands for integer, unsigned integer, and float. The integers all have 1-byte, 2-byte, 4-byte, and 8-byte variants, while the floats have 2, 4, and 8-byte variants. Your table doesn't go up to 8, so it doesn't have the 64 bit variants. You're also missing the \"c\" shortcode for complex dtypes, which also has a `c16` variant (e.g. 64 bit complex float, which is 128 bits wide).\r\n\r\nIt doesn't make sense to talk about 3 or 5-byte wide numerical dtypes, but it does make sense to talk about string, bytes, or structured array dtypes with arbitrary widths, that's why those dtypes support arbitrary widths.\r\n\r\nThe other dtype codes correspond to a specific dtype that only has a single width, so it doesn't make sense to talk about the width variants. Except for `b1` which for whatever reason lost to time exists and corresponds to bool.\r\n\r\nbytes are always int8, bools are also always int8, datetime and timedelta are represented as integers, and python objects also are just a fixed width pointer internally.\r\n\r\nDoes that make things a little clearer? Do you think you can update the table and description to capture all that?"],"labels":["04 - Documentation"]},{"title":"MAINT: Optimize performance of polynomial operations","body":"Improve the performance of polynomial operations by improving the internal `_get_coefficients`:\r\n\r\n* Reorder type check for the most common operation\r\n* Use `all()` method of numpy arrays instead of `np.all` for the window and domain attributes\r\n\r\nBenchmark results\r\n``` \r\np + q: Mean +- std dev: [main] 35.7 us +- 1.3 us -> [pr] 31.8 us +- 0.6 us: 1.12x faster\r\np * q: Mean +- std dev: [main] 37.3 us +- 1.1 us -> [pr] 33.9 us +- 1.1 us: 1.10x faster\r\n\r\nGeometric mean: 1.11x faster\r\n```\r\nwith script\r\n```\r\nimport pyperf\r\nfrom numpy.polynomial import Polynomial\r\n\r\nsetup = \"\"\"\r\nfrom numpy.polynomial import Polynomial\r\n\r\np=Polynomial([.5,4,5])\r\nq=Polynomial([1,0,5])\r\n\"\"\"\r\n\r\nrunner = pyperf.Runner()\r\nrunner.timeit(name=f\"p + q\", stmt=\"_ = p + q\", setup=setup)\r\nrunner.timeit(name=f\"p * q\", stmt=\"_ = p * q\", setup=setup)\r\n```\r\n \r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":[],"labels":["03 - Maintenance"]},{"title":"ENH: Add madvise for memmap objects","body":"Adds madvise to memmap. See issue https:\/\/github.com\/numpy\/numpy\/issues\/13172 for more information\r\n\r\n@seberg  we discussed shortly on the sprint day last week about a `TypeError` being raised when the memmap is not backed by any memory, but I could not find a reasonable case where this can happen. It is always backed when constructed. So I changed the access on `base` to `_mmap` so I can make test for the error, since `base` is not writable. Then the function is also more consistent with the `__getitem__` function that also uses `_mmap`. But now the error message is misleading, because `_mmap` can be `None`, while `base` is not `None`, so the memmap it is still backed up. I don't think this `None` check actually makes sense in the current version and would remove it and switch usage everywhere to `base` in memmap.\r\n\r\nWhat can actually happen is that the mmap is closed, and one can get a segfault because of invalid accesses (`my_memmap.base.close()` using functions marked as public). Maybe we test for this instead?\r\n```python\r\nif not(self.base.close):\r\n    raise TypeError(\"Memory map is closed and can therefore not be accessed.\")\r\n```\r\n\r\nWhat do you think?\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n        https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n        https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Using `_mmap` seems fine, just tweak the error message a bit.  I suspect that you are right and we should not lose `_mmap` unless we also use the `memmap` class itself.\r\n\r\nI am not quite sure what happens for `view = arr[1:]` if you then try to use madvise, etc.?  Would this always fail due to the pagesize problem?  In general users should maybe mostly use madvise on the original array and not the `view`.\r\n\r\nNot sure I feel we need to clean up the error for Python (Python's mmap should maybe do this), but OTOH, it doesn't see terrible?\r\n\r\n> What can actually happen is that the mmap is closed, and one can get a segfault because of invalid accesses \r\n\r\nRight, but this is an existing problem that I don't think we can do anything about."],"labels":["01 - Enhancement"]},{"title":"ENH: Reduce overhead of numpy.polynomial","body":"### Proposed new feature or change:\r\n\r\nThe `numpy.polynomial` classes have quite some overhead in the calculations. This is a collection of ideas to reduce the overhead\r\n\r\n* https:\/\/github.com\/numpy\/numpy\/pull\/24473\r\n* https:\/\/github.com\/numpy\/numpy\/pull\/24467\r\n* For several operations on polynomials (e.g. addition, substraction) the `ABCPolyBase._get_coefficients` takes time.  The check `np.all(self.domain == other.domain)` can be replaced with either `np.array_equal(self.domain, other.domain).all()` or `(self.domain == other.domain).all()` (and similar for the `window`). For benchmark\r\n\r\n```\r\nfrom numpy.polynomial import Polynomial\r\nimport timeit\r\n\r\np = Polynomial([1,2,3])\r\nq = Polynomial([3,2,1])\r\n\r\ndt=timeit.timeit('p + p - q', globals={'p': p, 'q': q}, number=40_000)\r\nprint(dt)\r\n```\r\nthe timings are\r\n```\r\n2.51 seconds (main)\r\n2.35 seconds (np.array_equal)\r\n2.2 seconds (.all() )\r\n```\r\n\r\nThe fastest option does assume the `domain` already is a numpy array. Current main and the `np.array_equal` option cast (with `asarray`). https:\/\/github.com\/numpy\/numpy\/pull\/24499\r\n\r\n* The `numpy.polynomial.polyutils.as_series` is called in many places. Also in internal methods where many of the input checks (e.g. on input dimensions) are not required. We can add a flag `internal : bool` to `as_series` than will skip several of the checks. Draft PR: #24531\r\n\r\n\r\nProfiling results on main (2023-8-22) for `p * q` with `p=Polynomial([1,2,3]); q=Polynomial([3,0,3])`:\r\n![np_polynomial_mul](https:\/\/github.com\/numpy\/numpy\/assets\/883786\/db20517a-19cd-46e0-b7e4-957b4cce381b)\r\n\r\n","comments":["Hello,\r\nI would like to make a contribution to Numpy and I found this enhancement on the issue list.\r\nI don't have that much free time, so it would likely take a while for me to work on this issue.\r\n\r\nI have a couple questions:\r\nIs this enhancement a high priority for Numpy development?\r\nDo you know of other issues that are likely a higher priority than this one?\r\n\r\nFor context, I'm a first-time contributor to this project, so I'm not really sure what the important issues are.\r\n\r\n","I just saw that you already have a PR out for this issue that hasn't yet been merged in.\r\nI will therefore let you continue working on this.\r\n\r\nDo you still wish to get this PR merged?"],"labels":["01 - Enhancement","component: numpy.polynomial"]},{"title":"BUG: np.asarray return a copy with shared memory","body":"### Describe the issue:\n\nAccording to `np.asarray(a)` [docs](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.asarray.html#numpy-asarray), it may return the original array or a copy.\r\nIn the case below, while `np.asarray(a, dtype='object') is not a`, - `np.shares_memory(np.asarray(a, dtype='object'), a)` given that a is copied.\r\n\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nimport pickle\r\nt = np.array([None, None, 123], dtype='object')\r\ntp = pickle.loads(pickle.dumps(t))\r\ng = np.asarray(tp, dtype='object')\r\n\r\nassert g is not tp\r\nassert np.shares_memory(g, tp)  # Unexpected\r\ng[0] = '1'\r\nassert tp[0] == '1'  # Unexpected\r\n\r\n\r\n# The issue seems to be originated here\r\nassert not np.dtype('O') is pickle.loads(pickle.dumps(np.dtype('O'))) \r\nassert np.dtype('O') == pickle.loads(pickle.dumps(np.dtype('O')))\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n1.25.2\r\n3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) '\r\n            '[GCC 11.3.0]',\r\n  'uname': uname_result(system='Linux', node='itay-jether', release='6.2.0-26-generic', version='#26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX',\r\n                                'AVX512_CLX',\r\n                                'AVX512_CNL',\r\n                                'AVX512_ICL'],\r\n                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/itay\/miniforge3\/envs\/jpy310\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-5007b62f.3.23.dev.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23.dev'},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/itay\/miniforge3\/envs\/jpy310\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.18'},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/itay\/miniforge3\/envs\/jpy310\/lib\/python3.10\/site-packages\/cvxopt.libs\/libopenblasp-r0-5c2b7639.3.23.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23'},\r\n {'architecture': 'Prescott',\r\n  'filepath': '\/home\/itay\/miniforge3\/envs\/jpy310\/lib\/python3.10\/site-packages\/scs.libs\/libopenblas-r0-f650aae0.3.3.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 1,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'disabled',\r\n  'user_api': 'blas',\r\n  'version': None},\r\n {'filepath': '\/home\/itay\/miniforge3\/envs\/jpy310\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0',\r\n  'internal_api': 'openmp',\r\n  'num_threads': 8,\r\n  'prefix': 'libgomp',\r\n  'user_api': 'openmp',\r\n  'version': None}]\n\n### Context for the issue:\n\nI'm using pandas and have pickled dataframes, when using `df.astype(str)` the dataframe is changed in-place\r\nsee [issue](https:\/\/github.com\/pandas-dev\/pandas\/issues\/54654) in pandas","comments":["FWIW, in 1.23.5, `g is tp` as expected. Looking at `git blame`, I wonder if #23404 might be responsible. Another difference is how `g.dtype is tp.dtype` operates in each of the versions; in 1.23.5, that is also `True` but in 1.25.1, that is `False`; that is, in 1.25.1, roundtripping an object array through `pickle` doesn't preserve the identity of the `dtype` object.\r\n\r\nThat said, the examples suggest a little more than what the rest of the docstring claims. The docstring doesn't claim that the given object is returned, though this is often the case. It only claims that no copy (implicitly, \"of the data\") is performed if none is needed, given the other arguments. And that's what happens here. A new `ndarray` instance is created, but it points to the same data. It seems like pandas is testing if two arrays are copies of the data by testing the identity of the `ndarray` objects themselves, and that's never been a valid test.","I think it's worth adjusting our examples, at least, to not suggest that `g is tp` is a valid way to test if the data is copied, and it might be worth clarifying our statement about \"no copy\" in the body of the docstring to specifically call out that it will not copy the data, but might create a new array object.","I think it probably makes sense for us to try to make the pickle behavior more sensible, I would naively expect `np.dtype(\u201cobject\u201d)` to be a singleton and there seems to be code that was relying on it. Let\u2019s see if we can bisect this across a build system change\u2026","I bisected this to #21995. I suspect that pickle is going through the code path that calls `NewFromDescrAndBase`. Maybe we could check for a singleton first?","Ah no, not quite right. The behavior of getting back a dtype object that is not the same as the object dtype singleton when unpickling an object array is older, #21995 just made it so this older behavior led to a new array object getting created.","Hey,\r\n\r\nI'll be diving into this np.asarray pickle issue tomorrow.\r\n\r\nLooks like the crux might be with how np.dtype('O') is pickled\/unpickled and its interaction with np.asarray. \r\n\r\nI'll start by isolating the problem in the code and running some tests, see what's going on under the hood.\r\n\r\nAny quick pointers before I jump in?\r\n\r\nCheers,\r\nJacob","You'll want to look at how dtype objects are pickled and unpickled. The dtype object (what numpy internally calls a descriptor) is defined in `numpy\/core\/src\/multiarray\/descriptor.c`. The pickling operation is handled by `arraydecr_reduce` and `arraydescr_setstate`. Looking at the `setstate` implementation, right now unpickling always builds a new dtype object from the content of the pickle. It may be possible to detect when the pickled dtype is exactly the same as a built-in singleton dtype like `object` and re-use the singleton instead of the partially reconstructed dtype, I don't know offhand if that's tricky to implement.","Thanks, so I have been doing a bit of digging. When unpickling a NumPy array, is there a mechanism in place to ensure that if the dtype of the array corresponds to one of the built-in singleton dtype objects, the singleton is reused instead of creating a new instance? I'm concerned about the potential memory and performance overhead of having redundant dtype objects in memory as this is likely where it is coming from.\r\n\r\nIf not these are a few ideas that may work?\r\n\r\n**Object Signature Matching:** This would involve comparing the signature of the unpickled dtype with built-in singleton dtypes and reusing if a match is found.\r\n**Global dtype Cache:** A cache of frequently-used dtypes could be maintained and consulted during the unpickling process.\r\n**Pickle Metadata Annotation:** We could modify the pickling process to include specific metadata for dtypes, which can then guide the unpickling process.\r\n**Enhancing the __reduce__ and __setstate__ methods:** Modifying these methods for dtype objects might allow us to better handle the reuse of built-in dtypes during unpickling. Given that this is TODO in the NumPy `numpy\/core\/src\/multiarray\/descriptor.c` this could be the first attempt. \r\n\r\nBut would be great to get some wider opinions on the best strategy before I try any of them \r\n\r\nBest,\r\nJacob ","Honestly, I think the best strategy is to fix the examples in the docstring. No one should be using `is` to test whether or not `asarray()` had to make a copy of the data. It's never guaranteed. Expending a lot of effort to get this one case fixed is just obscuring the other cases that will inevitably arise. It still won't enable anyone to write code using `is` to do that test.","I agree about the `is` test for ndarray comparison. I think doing `is` checks for dtype comparisons is slightly more defensible, but as this issue points out there are corner cases and it's probably not safe either. I wouldn't object to a change to the dtype pickling code so that singletons get returned in sensible contexts but I also don't think it's worth spending a lot of time on if your ultimate goal is making `is` comparisons work more sensibly.","Would a note like \"*Note: The use of the `is` operator in the examples above is for illustrative purposes only. In practice, the behavior of `np.asarray()` in terms of copying is not always guaranteed and can depend on various factors. Therefore, one should avoid using `is` to determine if `np.asarray()` made a copy of the data.\" suffice? Or do we need more complex examples?","It's more informative and less verbose to change the examples themselves to show the right thing. The right way to do these checks is not more complicated.","I'd like to point out that the data-api standard [specifies](https:\/\/data-apis.org\/array-api\/latest\/API_specification\/generated\/array_api.asarray.html#array_api.asarray) a `copy` keyword which should guarantee a copy. Is numpy conformant to the data-api specification?","No, but that's not relevant to this reported issue.","Based on our discussion here, I've made changes to the asarray docstring. I have submitted a PR addressing this issue: [#24714](https:\/\/github.com\/numpy\/numpy\/pull\/24714).\r\n\r\nAny feedback or further suggestions would be appreciated!\r\n\r\nBest,\r\nJacob"],"labels":["00 - Bug"]},{"title":"BUG: Inaccurate left tail of `random.Generator.dirichlet` at small `alpha`","body":"The Dirichlet generator produces too many small values or outright zeros when the concentration parameter is low. In the following example, I compare it to a way of generating the values that I presume to be accurate, which is representing the Dirichlet as normalized Gamma variables, but doing the calculation in logarithm space. This requires an up to date scipy, since `stats.loggamma` was made accurate for low values only recently.\r\n\r\nAlthough the problem only affects values less than machine epsilon, the Dirichlet variable is often used to represent a probability distribution, and sometimes probabilities matter multiplicatively. That said, I don't know of use cases where this would be a problem. I encountered it myself only because I was using numpy's generator as a reference for unit tests of a PPL.\r\n\r\n```python\r\nimport functools\r\n\r\nimport numpy as np\r\nfrom scipy import stats, special\r\nfrom matplotlib import pyplot as plt\r\n\r\nnsamples = 100000\r\nalpha = 0.005\r\ndim = 2\r\n\r\nalpha = np.repeat(alpha, dim) \/ dim\r\n\r\ndef accurate_dirichlet_rvs(alpha, size=(), random_state=None):\r\n\r\n    # scaffolding\r\n    random_state = np.random.default_rng(random_state)\r\n    alpha = np.asarray(alpha)\r\n    assert alpha.ndim >= 1\r\n    size = np.broadcast_shapes(alpha.shape[:-1], size)\r\n    alpha = np.broadcast_to(alpha, size + alpha.shape[-1:])\r\n\r\n    # actual computation\r\n    lny = stats.loggamma.rvs(alpha, random_state=rng)\r\n        # scipy's loggamma is the distribution of the logarithm of a gamma var\r\n    norm = special.logsumexp(lny, axis=-1, keepdims=True)\r\n    return np.exp(lny - norm)\r\n\r\nfig, axs = plt.subplots(2, 1, num='dirichlet', clear=True, figsize=[6.4, 8], layout='constrained')\r\n\r\nrng = np.random.default_rng([2023, 8, 21, 13, 50])\r\n\r\nsamples_numpy = rng.dirichlet(alpha, size=nsamples)\r\nsamples_accurate = accurate_dirichlet_rvs(alpha, size=nsamples, random_state=rng)\r\n\r\ndef plot_empirical_cdf(ax, samples, **kw):\r\n    edges = np.sort(samples)\r\n    values = np.linspace(0, 1, len(samples))[1:]\r\n    return ax.stairs(values, edges, **kw)\r\n\r\nfor i, ax in enumerate(axs):\r\n    plot_empirical_cdf(ax, samples_numpy[:, i], label=\"numpy\")\r\n    plot_empirical_cdf(ax, samples_accurate[:, i], label=\"presumed accurate\")\r\n    ax.set(\r\n        xlabel=f'Component {i} of a Dirichlet variate',\r\n        ylabel='CDF',\r\n        xscale='log',\r\n    )\r\n    ax.axvline(np.finfo(float).eps, color='black', linestyle='--', label='machine epsilon')\r\n\r\naxs[0].legend(title=rf'$\\alpha = {alpha.tolist()}$')\r\naxs[0].set(title=\"Numpy's Dirichlet generator left tail at small alpha\")\r\n\r\nplt.show()\r\n```\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/9684110\/efd7eaaf-ca98-4446-94f9-a81128eb3db1)\r\n\r\n\r\n### Runtime information\r\n\r\n```\r\n1.25.2\r\n3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 '\r\n            '(clang-1300.0.29.30)]',\r\n  'uname': uname_result(system='Darwin', node='ghijkl.local', release='22.5.0', version='Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:24 PDT 2023; root:xnu-8796.121.2~5\/RELEASE_ARM64_T6000', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP', 'ASIMDDP'],\r\n                      'not_found': ['ASIMDFHM']}}]\r\nNone\r\n```","comments":["Hello, is it possible for me to take care of doing an MR to resolve this issue?","I mention @WarrenWeckesser in the hope he pops up","@maissa-gallah we don't assign issues, so if you believe you have a solution for this problem please check our [Contributing guide](https:\/\/numpy.org\/devdocs\/dev\/index.html) and go for it. Cheers!"],"labels":["component: numpy.random"]},{"title":"NEP 51 transition for downstream libraries using doctest","body":"### Issue with current documentation:\n\nAs noted in the [backward compatibility section of NEP 51](https:\/\/numpy.org\/neps\/nep-0051-scalar-representation.html#backward-compatibility), one of the places where the updated scalar representations is likely to cause issues is in (vanilla) doctests in downstream libraries. Since vanilla doctest performs test evaluation by string matching, any test that returns a numpy scalar will be changing (and thus failing) when going from numpy v1. -> v2.0, e.g.\r\n\r\n```python\r\n>>> a = np.array([1, 2, 3])\r\n>>> a[0]\r\n1\r\n```\r\n\r\nwill become:\r\n\r\n```python\r\n>>> a = np.array([1, 2, 3])\r\n>>> a[0]\r\nnp.int64(1)\r\n```\r\n\r\nNEP 51 doesn't really offer concrete advice on how to address this upcoming compatibility break. The current text:\r\n\r\n> It may be necessary to adopt tools for doctest testing to allow approximate value checking for the new representation.\r\n\r\nThis is generally good advice as tools like `pytest-doctestplus` should be more robust to the scalar repr change, but there are likely many libraries that depend on numpy which use vanilla doctest for whom adopting a new doctesting framework represents a non-trivial amount of effort.\r\n\r\nTherefore, I think it's worthwhile to consider alternatives to aid in downstream projects transitioning to the new NumPy 2.0 scalar reprs.\r\n\r\n### Background\r\n\r\nThis issue popped up for NetworkX, which tests against the numpy nightly wheels. See networkx\/networkx#6846.\r\n\r\nThe way that NetworkX decided to deal with this was to use `np.set_printoptions` to configure the doctest runner to pin numpy reprs back to an older version. Networkx uses `pytest` to run doctests with the `--doctest-modules` flag, so we were able to update the test configuration with something like:\r\n\r\n```python\r\n# in conftest.py\r\n\r\n@pytest.fixture(autouse=True)\r\ndef np_legacy_scalar_repr(doctest_namespace):\r\n    import numpy as np\r\n    np.set_printoptions(legacy=\"1.21\")\r\n```\r\n\r\nSee networkx\/networkx#6856 for details.\r\n\r\nThe advantages of this approach are:\r\n - doctests can continue to run & check that outputs are numerically correct (ignoring the repr change)\r\n - allows libraries to avoid complicating their CI with multiple jobs for `numpy<2.0` and `numpy>=2.0` for the sake of doctests\r\n - Transitioning to 2.0 is straightforward: when numpy 2.0 becomes the minimum supported version for the library, revert the legacy print config & update any doctests that are returning numpy scalars to use the 2.0 version.\n\n### Idea or request for content:\n\nI *think* the above generalizes to downstream projects which depend on vanilla doctest. If anyone has feedback I'd love to hear it! If this seems like a sensible plan, then I'd propose to add it to the documentation; perhaps in the \"transitioning to numpy 2.0\" document.\r\n\r\nRelated to this: will `printoptions` get a `\"1.26\"` legacy option?\r\n\r\n","comments":["I think we should amend the NEP to clearly indicate __how__ \"to adopt tools for doctest testing\", and would support adding a 1.26 legacy option. I seem to recall we discussed adding it at one point. ","We've done something similar at astropy (https:\/\/github.com\/astropy\/astropy\/pull\/15096). A disadvantage is that the documentation will show outdated results until numpy 2.0 becomes the minimum version. In that respect, a doctest module that allows both `2.0` and `np.float64(2.0)` to match might be nicer, as one could update the docs but still have them pass on older numpy. But obviously, someone would have to write that! So, in the meantime, :+1: to having some helpful text in the NEP!","> In that respect, a doctest module that allows both `2.0` and `np.float64(2.0)` to match might be nicer, as one could update the docs but still have them pass on older numpy. But obviously, someone would have to write that!\r\n\r\nIf someone is willing to do that work, then it seems like a good idea to do so indeed. However, it is extra and not-really-required work, so I'd say it's also fine to not do this if there's no volunteer for it. \r\n\r\nThe AstroPy issue approach seems fine, and this option is basically \"tidy us over until we're done with changing all the repr's to numpy-2.0-compatible)\". What NetworkX does is different (and, I'd argue, not good practice and technically unsupported by numpy) - structurally running doctests on a range of versions of dependencies is not the purpose of doctesting. A single version of any package normally has a single repr for any given object. And package authors should be free to improve that repr, as we have done here. With good reason of course, because it may cause work downstream, as this issue shows. \r\n\r\nDoctesting should have the purpose of validating the docs for basic correctness, which means the examples execute without errors and match string output for a single version of the package. Hence it should be run once, in one CI job. Anything beyond that I'd consider unsupported.\r\n\r\n> I think we should amend the NEP to clearly indicate **how** \"to adopt tools for doctest testing\", and would support adding a 1.26 legacy option\r\n\r\n+1 for the legacy option, if someone wants to contribute it. Same for the NEP: it's accepted and fine as is - if there's a volunteer to improve it, +1 for accepting the improvements. But I would not say @seberg as NEP author or any other NumPy maintainer has to be responsible for this. It's a nontrivial amount of work to say \"how to adopt tools\", because you can only do so after you write that tool.\r\n\r\nOur actual guidance I believe should simply be: \"change the repr's in your docs to the new defaults\".\r\n","I have something to auto-fix docstrings (at least in many cases), but upstreaming to pytest-doctestplus is unfortunately delayed for a bit more right now.  I think it is easy to write a hook or option for doctestplus to do replacements.\r\n\r\nWe have the legacy option, mentioning it clearer in the NEP would be nice.  I would be happy to have a contribution to make that printoption eternal so that you can forever disable the type being included in the repr, though (i.e. effectively get the old printing)."],"labels":["04 - Documentation"]},{"title":"MAINT: Optimize performance of np.common_type","body":"In this PR we optimize the performance of `np.common_type` with 3 optimizations:\r\n\r\n* We avoid repeated checks for complex input types, once a complex type has been detected\r\n* Since we have a type available, we can avoid a call to `iscomplexobj` and call `issubclass(t, _nx.complexfloating)` directly\r\n* We first check the output of `array_precision.get(t, None)`. If the output is not `None`, we cannot have an integer type\r\n\r\nBenchmark script:\r\n```\r\nimport numpy as np\r\n\r\nx=np.array([1.])\r\ny=np.array([1])\r\nc=np.array([1.+1j])\r\n\r\n%timeit np.common_type(x, x)\r\n%timeit np.common_type(x, y)\r\n%timeit np.common_type(c, x)\r\n%timeit np.common_type(c, c)\r\n```\r\n\r\nThe result of the benchmark for main and the different optimizations:\r\n```\r\nMain:\r\n    1.77 \u00b5s \u00b1 48.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.75 \u00b5s \u00b1 36.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.79 \u00b5s \u00b1 40.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.78 \u00b5s \u00b1 40.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n\r\nAvoid repeated checks in complex\r\n    1.74 \u00b5s \u00b1 1.08 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.73 \u00b5s \u00b1 41.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.41 \u00b5s \u00b1 37.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.42 \u00b5s \u00b1 34.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n\r\nDirect use of `issubclass(t, _nx.complexfloating)` (plus previous optimizations)\r\n    1.24 \u00b5s \u00b1 36.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.17 \u00b5s \u00b1 33.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.17 \u00b5s \u00b1 49 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.16 \u00b5s \u00b1 36.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    \r\nFirst check `array_precision.get(t, None)` to avoid `issubclass` check on integer type (plus previous optimizations)\r\n    1.06 \u00b5s \u00b1 25.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.16 \u00b5s \u00b1 42.8 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1 \u00b5s \u00b1 44.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n    1.01 \u00b5s \u00b1 41.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000,000 loops each)\r\n```\r\n\r\nNote: the performance of `np.common_type` turned up in profiling the overhead operations with `np.polynomial.Polynomial`.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["An alternative approach is to use a lru cache on the `dtype.type`. The implementation is\r\n```\r\nfrom numpy.lib.type_check import *\r\nimport numpy as np\r\nfrom numpy.lib.type_check import  array_function_dispatch, _common_type_dispatcher, _nx,array_precision, array_type\r\nfrom functools import lru_cache\r\n\r\n@lru_cache\r\ndef common_type_internal(*array_types):\r\n    # identical to the original implementation, but passing the types instead of the arrays \r\n    is_complex = False\r\n    precision = 0\r\n    for t in array_types:\r\n        is_complex = is_complex or issubclass(t, _nx.complexfloating)\r\n\r\n        p = array_precision.get(t, None)\r\n        if p is None:\r\n            if issubclass(t, _nx.integer):\r\n                p = 2  # array_precision[_nx.double]\r\n            else:\r\n                raise TypeError(\"can't get common type for non-numeric array\")\r\n        precision = max(precision, p)\r\n    if is_complex:\r\n        return array_type[1][precision]\r\n    else:\r\n        return array_type[0][precision]\r\n\r\n@array_function_dispatch(_common_type_dispatcher)\r\ndef common_type(*arrays):\r\n    return common_type_internal(*[a.dtype.type for a in arrays])\r\n```\r\nThe performance then increases from roughly 1.7 us (main) \/ 1 us (this pr)  to .7 us (tested with python 3.10, for python 3.12+ the gain is probably more due to the decreased overhead of the generator `[a.dtype.type for a in arrays]`) ","If we ever want to support a no-gil interpreter, will we have to reconsider the use of caches inside NumPy (or add locks around cached function results)?","> If we ever want to support a no-gil interpreter, will we have to reconsider the use of caches inside NumPy (or add locks around cached function results)?\r\n\r\nI assume that when no-gil arrives, the `lru_cache` will continue to be thread safe (for backwards compatibility reasons), but off course that would have to be checked carefully. That does not mean it will be efficient. In this case I would be happy with just this PR. The lru_cache was an experiment to see how far performance could be pushed. ","I am not sure I am in favor, because we should use `result_type` for this.  Maybe we should just make `common_type` use that internally.  Or allow `promote_types` to take an arbitrary numbre of dtypes and call that from `common_type` directly.\r\n\r\nI suspct that may subtly change some corner cases, but we eithr live with that or just deprecate the function completely?","That said, adding a form of caching to `result_type`, or its lower level implementation, may actually make sense;  I have seen downstream try to squeeze out extra performance often enough here...  (although I think this is often due to `__array_function__` overhead in `rsult_type` being silly high for dtypes due to lookup of `__array_function__` failing for those.","> I am not sure I am in favor, because we should use `result_type` for this. Maybe we should just make `common_type` use that internally. Or allow `promote_types` to take an arbitrary numbre of dtypes and call that from `common_type` directly.\r\n> \r\n> I suspct that may subtly change some corner cases, but we eithr live with that or just deprecate the function completely?\r\n\r\nAn implemenation of `common_type` using `result_type` is:\r\n```\r\nimport numpy as np\r\nfrom numpy.core import _multiarray_umath\r\n\r\ndef common_type(*arrays):\r\n    v = _multiarray_umath.result_type(*arrays)\r\n    if np.issubdtype(v, np.integer): # rather slow\r\n        return np.float64\r\n    return v.type\r\n\r\n````\r\nThe performance is not as good as this PR, but perhaps the `np.issubdtype(v, np.integer)` part can be made faster. And I suspect the behaviour is a bit different for corner cases. We use `_multiarray_umath` here to skip some dispatcher overhead\r\n\r\nAdding a cache to `np.result_type` directly does not work, since we are allowed to pass arrays (which are not hashable). ","@seberg I reimplemented `common_type` using the `result_type`, see #24656. \r\n\r\nWith caches performance can perhaps be improved even more, but for me either this PR or #24656 would be enough.","Closing in favor of #24668","@seberg I reopened this PR as it is a relatively straightforward and backwards compatible way to improve the performance of `common_type`. A quick search https:\/\/github.com\/search?q=np.common_type&type=code shows the `common_type` is also used outside the numpy codebase, so not breaking backwards compatibility is the safer option.\r\n"],"labels":["01 - Enhancement"]},{"title":"Avoid spurious double cast in _monotonicity","body":"Rewrite the underlying C implementation of the monotonicity check in a generic fashion, allowing the use on arrays with comparable types.\r\n\r\nCloses #11022\r\n\r\ncc @eric-wieser ","comments":["Ping?","Hi @LemonBoy - while we wait for a second review can you fix the conflicts that are showing up now? Thanks!"],"labels":["03 - Maintenance"]},{"title":"TST: add some tests of np.log for complex input.","body":"These are values where some platforms might lose precision.\r\n\r\n[skip circle]\r\n","comments":["Thanks. The failing tests are helpful (points out where we should be blocklisting clog on musllinux, cygwin, and pyodide). I wonder if at some point we should adopt the [cpython cmath tests](https:\/\/github.com\/python\/cpython\/blob\/main\/Lib\/test\/test_cmath.py) with the extensive [test cases](https:\/\/github.com\/python\/cpython\/blob\/main\/Lib\/test\/cmath_testcases.txt) for at least `cdouble`. "],"labels":["05 - Testing"]},{"title":"ENH: Add endpoint argument to np.digitize","body":"Make the function slightly more useful by allowing the user to specify whether the first\/last bin includes both edges.\r\nThis way the function behavior aligns more closely to the one of np.histogram while retaining the half-open default.\r\n\r\nThe logic is a bit hairy because of the interaction with the `right` parameter and the ascending\/descending monotonicity of the bins, suggestions welcome.\r\n\r\nDocumentation could be better, again suggestions are more than welcome.\r\n\r\nCloses #4217","comments":[],"labels":["01 - Enhancement"]},{"title":"BUG: support masked arrays in `recfunctions.drop_fields`","body":"Unlike documented, `drop_fields` always ignored mask from input and returned `ndarray`.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["Comment a bit from the side, as I worked quite a bit on `np.ma` before: basically, most numpy functions do not by default work with masked arrays - that's why there are all the special functions inside `np.ma`. These days, that should no longer be necessary, but the way to improve things would be to define `__array_function__` on `np.ma.MaskedArray` and deal with things there; generally, the rest of numpy should not have to know `MaskedArray` exists!"],"labels":["00 - Bug"]},{"title":"CI: provide older version of dev wheels rather than overwriting the latest","body":"Once a downstream CI starts to fail due to a new nightly version, it would be extremely helpful to be able to go back a few versions for debugging and testing.\r\n\r\nSomewhat related (but I'm happy to open a separate issue for it), is  that while I understand that currently there are technical limitations, having the git hash in the version would be super useful, too. But even having the date would be nicer than the extremely generic `dev0`","comments":["FWIW, SciPy uses the same versioning (or non-versioning) of wheels uploaded to https:\/\/anaconda.org\/scientific-python-nightly-wheels","I suppose it's the same underlying problem with meson, so once there is a solution for numpy it can be ported to scipy, too. Or do you want me to open this same issue at scipy as well?","I could be wrong, but my understanding is that labeling the wheels is a problem, but the numpy version should be more informative. That would be new, I think. Looking in one of yesterday's wheels I see\r\n```\r\nversion = \"2.0.0.dev0+git20230814.2c45a56\"\r\n```\r\n\r\nwhich is what `numpy.__version__` should show. I haven't tried it, though.","Yes, I wasn't clear I suppose. I see that version in CI where the script explicitly calls `numpy.__version__`, but e.g. `pip freeze` still shows only `2.0.0.dev0`, neither it is shown in the filename.","There is a long discussion at mesonbuild\/meson-python#159 about allowing the pyproject.toml `version` attribute to be dynamic. If I understand the last few comments correctly, this feature is still not available in meson-python. \r\n\r\n> Or do you want me to open this same issue at scipy as well?\r\n\r\nNo, no need for a new issue there. I was hoping they had solved the problem for us","We had git hashes in wheel naming for scipy a while back, but went to overwriting the same dev0 file. The rationale was that one has to have continuous clear out of older nightly wheels otherwise they accumulate rapidly because of the wide matrix. This will accelerate (at least for scipy) because the wheel manufacture is happening nightly in the run up to numpy 2. Now that building scipy\/numpy is a lot faster because of meson it would probably be easier to `git bisect` to find out the exact commit where something has gone wrong. Albeit, that would be more annoying for a downstream project.\r\n\r\nAnother solution is to print out exact git hashes for numpy\/scipy in downstream CI. If the downstream CI runs regularly (e.g. once per day) then one will know very quickly which nightly wheel went wrong.",">  otherwise they accumulate rapidly \r\n\r\nIt turns out the site has a [retention policy](https:\/\/github.com\/scientific-python\/specs#261) enforced by a [github action](https:\/\/github.com\/scientific-python\/upload-nightly-action\/blob\/main\/.github\/workflows\/remove-wheels.yml): every day there is a cron job that leaves the last 5 wheel versions and removes artifacts that are more than 30 days old.","Downstream, in slow\/smaller packages, we tend to do weekly crons. \n\nThe biggest issue really is that even if I know what day it all went wrong, I'm unable to reinstall the latest passing combination. Which would be helpful, e.g. even just identifying which dev dependency is the source of the problem.","Oh, and building from source: it's a me problem, and not that relevant here (well, someone else has the same problem): I still can't build scipy from source, meson has an issue with my pyenv pythons and paths, etc. and I haven't managed to build it from source for a few months now. Numpy is similarly painful, but we found one workaround for it at Scipy23, that kind of works (in practice it's not at all convenient).\n\nSo, at the end, I switched to using the nighlies, too when I need something \"close enough to dev\" \ud83e\udd37\u200d\u2640\ufe0f","> meson has an issue with my pyenv pythons and paths\r\n\r\nNot sure about SciPy, but I've been building NumPy with a pyenv python since the meson transition with only a few bumps at first and no issues for a while now. I'm on Linux though, if that matters. If you think looking at my setup would be helpful feel free to ping me.","Thanks, I'll take you up on it and follow up. (The current workaround is to work with a temporary venv, so it is really not convenient, and even the non meson <3.12 ways are not working)","I would like to keep the metadata in `pyproject.toml` fully static, as discussed before. If it's useful to append a build number for nightlies (so the wheel name contains `2.0.0.dev0-N` with `N` an integer like the number of commits or some such thing), this could be done post-hoc in the CI script for uploading.\r\n\r\n> Thanks, I'll take you up on it and follow up.\r\n\r\nThanks, it'd be great to sort this out. I wasn't aware of any problem or bug report. This should work, happy to help too if needed.\r\n"],"labels":["14 - Release","component: CI"]},{"title":"BUG: test_mem_policy failing without ninja","body":"### Describe the issue:\r\n\r\nTests are failing on Mac OSX, m1, Python 3.11.4. Seems to occur both on clang and gcc builds.\r\n\r\nSpecifically the test_mem_policy tests give an error seemingly related to an inability to build the test code in those tests\r\n\r\n```\r\nUndefined symbols for architecture arm64:\r\n  \"_main\", referenced from:\r\n     implicit entry\/start for main executable\r\nld: symbol(s) not found for architecture arm64\r\n```\r\n\r\nAnd, subsequently, perhaps caused by this initial build failure\r\n\r\n```\r\n> Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\r\n\/Users\/douglas.turnbull\/ws\/numpy\/venv\/bin\/meson compile -C .\r\n\r\n> ERROR: Could not detect Ninja v1.8.2 or newer\r\n```\r\n\r\n### Reproduce the code example:\r\n\r\n```\r\n python runtests.py -v -m full    \r\n```\r\n\r\nbut also, this recreates the issue faster\r\n\r\n```python\r\npython -m pytest numpy\/core\/tests\/test_mem_policy.py\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nSnippet from meson log\r\n\r\nDetecting Apple linker via: `cc -Wl,-v` -> 1\r\nstderr:\r\n@(#)PROGRAM:ld  PROJECT:ld64-857.1\r\nBUILD 23:13:29 May  7 2023\r\nconfigured to support archs: armv6 armv7 armv7s arm64 arm64e arm64_32 i386 x86_64 x86_64h armv6m armv7k armv7m armv7em\r\nLibrary search paths:\r\n.\r\n\/opt\/homebrew\/lib\r\n\/usr\/local\/lib\r\n\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX.sdk\/usr\/lib\r\nFramework search paths:\r\n\/Library\/Developer\/CommandLineTools\/SDKs\/MacOSX.sdk\/System\/Library\/Frameworks\/\r\nUndefined symbols for architecture arm64:\r\n  \"_main\", referenced from:\r\n     implicit entry\/start for main executable\r\nld: symbol(s) not found for architecture arm64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\nI'm running Python 3.11.4 installed with asdf. I'm running in a virtual environment created with `python3 -m venv venv`.\r\n\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n2.0.0.dev0+git20230813.104addf\r\n3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 (clang-1403.0.22.14.1)]\r\n```\r\n\r\n```\r\n>>> print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '2.0.0.dev0+git20230813.104addf',\r\n  'python': '3.11.4 (main, Jun 20 2023, 17:23:00) [Clang 14.0.3 '\r\n            '(clang-1403.0.22.14.1)]',\r\n  'uname': uname_result(system='Darwin', node='[REDACTED]', release='22.6.0', version='Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6\/RELEASE_ARM64_T6000', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP', 'ASIMDDP'],\r\n                      'not_found': ['ASIMDFHM']}}]\r\n```\r\n\r\n### Context for the issue:\r\n\r\nI was trying to get started building\/testing a pull request adding popcount to numpy - #21429 and seeing if I could help finish it. I'm new to numpy development, but not to python \/ C development.\r\n\r\nLargely I can ignore these failures for my case (I think) but wanted to flag\/share them.","comments":["> `python runtests.py -v -m full`\r\n\r\nAh, there we go. We only have one CI job for macOS arm64, and it's already been converted to Meson. `runtests.py` still uses the `numpy.distutils` based build, and is untested. The `test_mem_policy.py` changes are recent, and apparently that broke something.\r\n\r\n\r\n> I was trying to get started building\/testing a pull request adding popcount to numpy - #21429 and seeing if I could help finish it. I'm new to numpy development, but not to python \/ C development.\r\n\r\nWelcome! Great to have your help on `bitwise_count` (and beyond:) ).\r\n\r\nYou jumped in right when we are switching build systems for the first time in ~15 years, so you may see a few hiccups. The easiest way forward here is probably to use the new build system. If you install [spin](https:\/\/pypi.org\/project\/spin\/) and run `spin test`, I suspect the problem will go away.\r\n","I can reproduce this by uninstalling ninja after building, and then installing `pip install -r test_requirements.txt`. The problem is that we should specify a dependency on `ninja` in `test_requirements.txt`, but cannot since it breaks the pyodide build. So if you somehow get to a point where you do not have ninja in your system under test, using the meson-based c-extension builds in tests will fail. \r\n\r\nWe do not catch this in our CI since we skip these tests on pyodide and elsewhere always install ninja before testing.\r\n\r\nOne solution would be to skip those tests if ninja is unavailable. Another would be to add ninja to `test_requirements.txt` but not use that file in pyodide (it seems there is [no way to express platform dependencies in pyodide's pip](https:\/\/github.com\/numpy\/numpy\/pull\/24153#issuecomment-1634211789)).","I couldn't find it, but there must be a marker for pyodide? Ideally `platform_python_implementation` must point to `pyodide` and not `cython` and we should be able to skip it.","> there must be a marker for pyodide\r\n\r\nDo you mean in the requirements file? No, there is none, see [my attempts here](https:\/\/github.com\/numpy\/numpy\/pull\/24153#issuecomment-1634211789). I had a nice discussion with @hoodname who clarified that because of the way pyodide works, `pip install` is actually running with a CPython, not pyoidide, underneath. Additionally there are a few things wrong with depending on a pip-installed ninja package:\r\n- users (like this one) may not use the `test_requirements.txt` file and we should make a minimal attempt to accommodate that situation. This test is particularly brittle since it needs a compiler, meson, and ninja.\r\n- ninja is not really a python package and there are other ways to install it, via choco, apt, yum or `wget | mv`","I think just erroring out in the test itself if `ninja` is missing in the way to go here."],"labels":["00 - Bug"]},{"title":"BUG: umath: Fix log1p for complex inputs.","body":"Reimplement the complex log1p function.  Use the log1p trick from Theorem 4 of Goldberg's paper \"What every computer scientist should know about floating-point arithmetic\".  Include special handling of an input with imaginary part 0.0 to ensure the sign of the imaginary part of the result is correct and consistent with the complex log function.\r\n\r\nCloses gh-22609.\r\n","comments":["It looks like the failures on Emscripten\/Pyodide, musllinux_x86_64 and Cygwin are because the underlying complex log function `clog`  provided by the standard math library is not very accurate.\r\n\r\nIf I run a C program that computes `w = clog(CMPLX(1.0, 1e-12))` and compile it with musl-gcc on my Linux machine,  `clog` returns `w = 1e-12j`.  The correct value is `wtrue = 5e-25 + 1e-12j`, which gives a relative error of `abs(w - wtrue)\/abs(wtrue) = 5e-13`.  (`musl-gcc --version` shows `x86_64-linux-gnu-gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0`.)  With clang on a Mac and with gcc on Linux, the value is computed correctly.  For some other inputs, the relative error of the musl-based `clog` can be as high as 5e-10.\r\n\r\nThe unit test tolerances can be loosened a bit, but I'm tempted to make the tolerances of the tests platform-dependent.\r\n\r\n\r\n\r\n","> the underlying complex log function clog provided by the standard math library is not very accurate.\r\n\r\nWe can [blocklist `clog` on more platforms](https:\/\/github.com\/numpy\/numpy\/blob\/2c45a565912951befccf4e4c5481ae1d2072ca5b\/numpy\/core\/src\/common\/npy_config.h#L51) (currently only MSVC), and use [our implementation](https:\/\/github.com\/numpy\/numpy\/blob\/2c45a565912951befccf4e4c5481ae1d2072ca5b\/numpy\/core\/src\/npymath\/npy_math_complex.c.src#L273). I guess we should be reproducing the [cpython cmath tests](https:\/\/github.com\/python\/cpython\/blob\/main\/Lib\/test\/test_cmath.py) to catch cases like this. "],"labels":["00 - Bug"]},{"title":"BUG: Fix datetime hash to be a bit more sane and start on timedelta","body":"This is a start, it should work for datetime64, I am not attempting to do anything about timedelta yet, because we don't really have the timedeltastruct so it needs more reorganization.\r\n\r\nI don't like this, but TBH, I don't see that it is better to add a bad hack into pandas than to add the same bad hack into NumPy...\r\n\r\n---\r\n\r\nThis is moving the start from Pandas in https:\/\/github.com\/pandas-dev\/pandas\/pull\/50960 to NumPy.  I chose to move it into the datetime.c file, but I don't care about where it lives.  We can probably cut down the hash calculation a little, but TBH, I doubt it matters in practice and we can change it later.\r\n\r\nTimedelta is proving more annoying.  We need to cut corners even more heavily probably because a huge number of casts\/comparisons that are allowed probably shouldn't be allowed to begin with.\r\n\r\nCC @jbrockmendel","comments":["If you're inclined to adapt the tests from https:\/\/github.com\/pandas-dev\/pandas\/pull\/50960, i think the most relevant one would be something like\r\n\r\n```\r\ndef test_unique_datetime64_mismatched_unit(self):\r\n    arr = np.array(\r\n        [np.datetime64(1, \"ms\"), np.datetime64(1000, \"us\")], dtype=object\r\n    )\r\n    res = np.unique(arr)\r\n    np.testing.assert_array_equal(res, arr[1:])\r\n```","NumPy unique doesn't use hashing even for objects, could use a set, but I already check for equality and hash equality in the test?","> NumPy unique doesn't use hashing even for objects, could use a set, but I already check for equality and hash equality in the test?\r\n\r\nOK never mind then"],"labels":["00 - Bug"]},{"title":"BUG: np.vectorize doesn't work inside class definition","body":"### Describe the issue:\n\n`np.vectorize` doesn't work when used on a method inside a class definition, whether it's used as a function or as a decorator. The docstring for `vectorize` says that `pyfunc` is \"A python function or method\".\r\n\r\n#9349 was working on a fix by making `vectorize` follow the descriptor protocol, but was closed when #23514 made `np.vectorize` work as a decorator. However, #23514 didn't implement the `vectorize.__get__` method which would fix this.\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nclass C:\r\n    def __init__(self, y):\r\n        self.y = y\r\n        self.inside_init = np.vectorize(self.pyfunc)  # current workaround\r\n    \r\n    @np.vectorize\r\n    def decorated(self, x):\r\n        return x + self.y\r\n    \r\n    def pyfunc(self, x):\r\n        return x + self.y\r\n    \r\n    inside_class = np.vectorize(pyfunc)\r\n\r\nc = C(1)\r\na = np.array([1, 2])\r\n\r\nc.inside_init(a)  # works\r\nc.decorated(a)  # error\r\nc.inside_class(a)  # error\n```\n\n\n### Error message:\n\n```shell\n[...]\r\nTypeError: C.decorated() missing 1 required positional argument: 'x'\r\n[...]\r\nTypeError: C.pyfunc() missing 1 required positional argument: 'x'\n```\n\n\n### Runtime information:\n\n1.25.2\r\n3.11.4 (tags\/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\r\n[{'numpy_version': '1.25.2',\r\n  'python': '3.11.4 (tags\/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 '\r\n            '64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='LAPTOP-11P3HCJO', release='10', version='10.0.19045', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\n\n### Context for the issue:\n\n_No response_","comments":["`inside_class = np.vectorize(pyfunc)` works to vectorize the function `def pyfunc(self, x):`, but at that particular point in the execution, note that it has two arguments, `self` and `x`. It's exactly that function, no more or less. It's not a method yet until the class definition is complete and Python assembles the actual class object at the end.\r\n\r\nThe \"workaround\" is the one correct way to implement this. The decorator won't (and can't) work on the method definition inside a class, and that isn't what was intended to be described by the docstring. When it said \"method\" there it meant an actual `instancemethod` as in the workaround case. This is a documentation issue.","No, you're right, but it's an enhancement. The error was just that gh-23514 did not in fact supercede gh-9349."],"labels":["01 - Enhancement"]},{"title":"BUG: f2py scalar converters are too forgiving","body":"The scalar converters in f2py are from a different age and time and are crazy forgiving.  For example, conversion to double amounts to something like:\r\n```\r\ndef to_double(obj):\r\n    try:\r\n       return float(obj)\r\n    except:\r\n       pass\r\n    try:\r\n       return complex(obj).real\r\n    except:\r\n       pass\r\n    if not isinstance(obj, (str, bytes)):\r\n       return to_double(obj[0])\r\n    \r\n    raise  # the except: pass is not correct above of course.\r\n```\r\nit allows complex values, and nested sequences and doesn't even check if that nested sequence has only length one at least.\r\n\r\nI ran into this because of our deprecation warning on `float(np.array([1.]))`, in f2py code paths SciPy doesn't notice the warning when raising as an error, since it just continues to the sequence unpacking.\r\n\r\nNot sure what we should do, it might be nice to very gently deprecate this, downstream may want to explicitly allow 1-element arrays in some cases for example.\r\n(Optimizers are one example where allowing 1-element arrays seems rather acceptable, since you know you are minimizing a scalar value.)","comments":[],"labels":["component: numpy.f2py"]},{"title":"DOC: clarify if astype is meant to be called on numpy.ma.MaskedArray","body":"### Issue with current documentation:\n\nSeems like there is no mention of `astype` in the documentation (https:\/\/numpy.org\/doc\/stable\/reference\/routines.ma.html) for `Masked array operations`.\r\nIs `astype` meant to be called on `numpy.ma.MaskedArray`?  When we call it the return type is typed to be `ndarray`, even though if we print it we see it's `numpy.ma.MaskedArray`.\r\n\r\nThis isn\u2019t clear from the documentation (ver. 1.25) for `numpy.ma.MaskedArray.astype` (https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.ma.MaskedArray.astype.html), but it looks exactly the same as doc for `numpy.ndarray.astype `(https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.ndarray.astype.html).\r\n\r\nReproducible example(mypy 1.4.1):\r\n\r\n```\r\nimport numpy as np\r\n\r\ndata = np.ma.MaskedArray([1,2,3], mask=[False, False, True])\r\nreveal_type(data) \r\n# Revealed type is \"numpy.ma.core.MaskedArray[Any, Any]\"\r\n\r\nreveal_type(data.astype('int64', copy=True)) \r\n# Revealed type is \"numpy.ndarray[Any, numpy.dtype[Any]]\"\r\n```\n\n### Idea or request for content:\n\nCan it be more clear from the documentation is `astype` meant to be called on `numpy.ma.MaskedArray` or not?","comments":["Thanks, looks like a real issue, but a typing one only.  The returned value at runtime is a masked array.","This is mostly a limitation introduced by the lack of higher order type variables, meaning the affected methods would have to be explicitly redeclared with the correct `ndarray` subclass as return type (yes, this affects all `ndarray` subclasses and extends to many other methods beyond `astype`). To put it simply: with aforementioned limitation this is a highly tedious and non-trivial task, especially since `MaskedArray` in general is largely unannotated."],"labels":["00 - Bug","Static typing"]},{"title":"ENH: np.pad is time consuming in 3D ... for nothing ","body":"### Describe the issue:\n\nHi,\r\nSurprisingly, when going from 2D to 3D on large arrays, a zero-padding becomes very CPU time consuming.\r\nIs it normal ?\r\nPatrick\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nimport time\r\n\r\nsize = 1000\r\narr2d = np.random.random((size, size))\r\narr3d = np.random.random((size, size, size))\r\n\r\nt0 = time.perf_counter()\r\narr2d_pad = np.pad(arr2d, pad_width=0)\r\nt1 = time.perf_counter()\r\narr3d_pad = np.pad(arr3d, pad_width=0)\r\nt2 = time.perf_counter()\r\n\r\nprint(arr2d_pad.all() == arr2d.all())\r\nprint(arr3d_pad.all() == arr3d.all())\r\n\r\nprint(f\"zero-padding in 2D: {t1-t0}s\")\r\nprint(f\"zero-padding in 3D: {t2-t1}s\")\n```\n\n\n### Error message:\n\n```shell\nTrue\r\nTrue\r\nzero-padding in 2D: 0.012127699999837205s\r\nzero-padding in 3D: 3.2117499000014504s\n```\n\n\n### Runtime information:\n\n1.24.3\r\n3.10.11 (tags\/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n\n### Context for the issue:\n\n_No response_","comments":["Is the \"for nothing\" you're referring to the fact that `pad_width` is zero? If so, It looks like there isn't any kind of early-return in the `pad` implementation in this case so there's your explanation. A PR adding an early return path that returns a copy of the input array would likely be welcomed. \r\n\r\nThe implementation is in Python so this doesn't require any C coding knowledge to implement. `np.pad` is defined [here](https:\/\/github.com\/numpy\/numpy\/blob\/e6d5a198668b57d360f59f48d3c28861eea9b780\/numpy\/lib\/arraypad.py#L534) but most of the script's runtime is spent in the `_pad_simple` helper on [this line](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/lib\/arraypad.py#L124) that uses slicing to copy the contents of the original array into the new padded array. There's no reason to use slicing for this if `pad_width` is zero and a copy will likely be a lot faster.\r\n\r\nIf your \"for nothing\" is just referring to the shapes of the arrays, I think it makes sense that a 3D array with 1000 times more entries would take about a thousand times longer to pad.","Thanks @ngoldbaum for your response.\r\nOriginally I was working on skimage.measure.block_reduce(). Downscaling a 1024x1024 image to a 512x512 (or less) image is common (the same in 3D), and most of the time, the block reduction requires no padding (this \"zero-padding\" could nevertheless represent over 60% of CPU time... for \"nothing\") .\r\nConcerning _pad_simple indeed, you are right. An early return would significantly reduce the CPU time in \"my\" case.\r\nI will try to propose a PR.","I was thinking to add `if not np.any(pad_width): return array.copy()` [here]( https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/lib\/arraypad.py#L743) until I figured out that the cost in 3D for big arrays was mostly due to the creation of the returned array and not to the pad processing itself.\r\nTherefore, in the case of skimage.measure.block_reduce(), I think the solution is just not to call np.pad() when pad_width=0.","Do you think an extra \"Will return a copy even if no padding is done\" [here](https:\/\/github.com\/numpy\/numpy\/blob\/e6d5a198668b57d360f59f48d3c28861eea9b780\/numpy\/lib\/arraypad.py#L641) in the return value documentation would have helped?"],"labels":["00 - Bug","57 - Close?"]},{"title":"Compiler warnings using gcc 13.","body":"This:\r\n\r\n```\r\n..\/numpy\/core\/src\/multiarray\/alloc.c: In function \u2018PyDataMem_RENEW\u2019:\r\n..\/numpy\/core\/src\/multiarray\/alloc.c:274:9: warning: pointer \u2018ptr\u2019 may be used after \u2018realloc\u2019 [-Wuse-after-free]\r\n  274 |         PyTraceMalloc_Untrack(NPY_TRACE_DOMAIN, (npy_uintp)ptr);\r\n      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n..\/numpy\/core\/src\/multiarray\/alloc.c:272:14: note: call to \u2018realloc\u2019 here\r\n```\r\n\r\nThere are more in NumPy 1.26,x but I will look into fixing them from main.\r\n","comments":["I once tried to get rid of this kind of warning in skimage.  Unlike here, the usage there was actually not strictly C standard conform in theory, but what I learned was that I couldn't figure out was how to trick GCC into not warning.\r\nIn other words: the right avenue for this specific one might be to try to suppress the warning (I guess with a pragma?)...","> try to suppress the warning\r\n\r\nI didn't see anything wrong with the code either. GCC, or maybe RedHat, seems to be enabling more of these compiler flags. We could disable that flag, or just ignore it."],"labels":["16 - Development","03 - Maintenance"]},{"title":"BUG: alignment not enforced in rec.fromarrays","body":"### Describe the issue:\r\n\r\nIt is my understanding that the fields of an aligned recarray should be appriate to each of the datatypes in the record. However, when a field containing a first subfield w a large type follows an array with an odd number of elements of a small type, the 2nd field is misaligned.\r\n\r\n### Reproduce the code example:\r\n```python\r\nimport numpy as np\r\n\r\ndtx = np.dtype(np.int8, align=True)\r\nax = np.array([1], dtype=dtx)\r\ndty = np.dtype((np.record, [('a', np.int64)]), align=True)\r\nay = np.rec.array([2],dtype=dty)\r\nvalues = [ax, ay]\r\nfields = [('x', dtx, (1,)), ('y', dty)]\r\nrec_array = np.rec.fromarrays(\r\n    values, dtype=np.dtype(fields, align=True)\r\n)\r\nprint(rec_array.dtype.fields['y'])\r\n# prints: (dtype((numpy.record, [('a', '<i8')])), 1)\r\n#\r\n# Note that \"1\" alignment can't be right for subfield \"a\" of record \"y\"\r\n\r\nfrom numpy.lib.recfunctions import repack_fields\r\nrec_array = repack_fields(rec_array, align=True, recurse=True)\r\nprint(rec_array.dtype.fields['y'])\r\n# still prints: (dtype((numpy.record, [('a', '<i8')])), 1)\r\n\r\n\r\n### Error message:\r\n\r\nWhen this record array is passed to numba cuda, it results in an alignment error.\r\n\r\n### Runtime information:\r\n\r\nIn [3]: import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.24.3\r\n3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:57:19) [GCC 11.3.0]\r\n\r\n\r\n### Context for the issue:\r\n\r\nGPU programming is increasingly important, but requires proper alignment to handle structured data.","comments":["As a temporary workaround, an alignment fix:\r\n```py\r\ndef fixup_dtype_alignment(dtype: np.dtype[Any]) -> np.dtype[Any]:\r\n    \"\"\"\r\n    Fix up alignment of dtype.\r\n\r\n    A workaround for: https:\/\/github.com\/numpy\/numpy\/issues\/24339\r\n\r\n    Args:\r\n        dtype: dtype to fix up\r\n\r\n    Returns: dtype with alignment fixed up\r\n    \"\"\"\r\n    return _fixup_dtype_alignment(dtype)[0]\r\n\r\n\r\ndef _fixup_dtype_alignment(dtype: np.dtype[Any]) -> tuple[np.dtype[Any], int]:\r\n    \"\"\"\r\n    Fix up alignment of dtype recursively.\r\n\r\n    Aligns each element so that its contents are aligned, given what\r\n    is before it in the structure.\r\n\r\n    Args:\r\n        dtype: dtype to fix up\r\n\r\n    Returns fixed dtype and largest individual element size.\r\n\r\n    \"\"\"\r\n    if dtype.names is None:\r\n        if dtype.subdtype is not None:\r\n            sub_dtype, layout = dtype.subdtype[:2]\r\n            sub_dtype, size = _fixup_dtype_alignment(sub_dtype)\r\n            return np.dtype((sub_dtype, layout)), size\r\n        return dtype, dtype.itemsize\r\n    names = list[str]()\r\n    formats = list[Any]()\r\n    offsets = list[int]()\r\n    element_size = 0\r\n    offset = 0\r\n    for name in dtype.names:\r\n        field = dtype.fields[name]  # type: ignore\r\n        sub_dtype = cast(np.dtype[Any], field[0])\r\n        # offset = cast(int, field[1])\r\n        sub_dtype, size = _fixup_dtype_alignment(sub_dtype)\r\n        element_size = max(element_size, size)\r\n        if offset % size != 0:\r\n            offset += size - offset % size\r\n        names.append(name)\r\n        formats.append(sub_dtype)\r\n        offsets.append(offset)\r\n        offset += sub_dtype.itemsize\r\n    if offset % element_size != 0:\r\n        offset += element_size - offset % element_size\r\n    if dtype.type == np.record:\r\n        new_dtype: np.dtype[Any] = np.dtype(\r\n            (\r\n                np.record,\r\n                dict(\r\n                    names=names,\r\n                    formats=formats,\r\n                    offsets=offsets,\r\n                    itemsize=offset,\r\n                ),\r\n            ),\r\n            align=True,\r\n        )\r\n    else:\r\n        new_dtype = np.dtype(\r\n            dict(\r\n                names=names, formats=formats, offsets=offsets, itemsize=offset\r\n            ),\r\n            align=True,\r\n        )\r\n    return (new_dtype, element_size)\r\n```","Just as a clarification, the problem seems to be with the `(np.record, ...)` syntax, so already occurs earlier:\r\n```python\r\ndty = np.dtype((np.record, [('a', np.int64)]), align=True)\r\nassert dty.alignment == dty['a'].alignment\r\n```\r\nfails, but shouldn't with `align=True`.  (maybe it should even succeed without `align=True`)","Hi I made a pr to fix the issue, may i request for some help reviewing it? thank you!\r\n\r\nThe issue seems to occur when there is a base type when creating a dtype, as the alignment flag is not being propogated beyond the base type"],"labels":["00 - Bug"]},{"title":"BUG: Setting an object array via slicing with nested lists invokes the objects `__array__` methods unnecessarily","body":"### Describe the issue:\r\n\r\nI have been running into issues with trying to use Numpy object arrays to contruct ND-Arrays of Python objects where I want to take advantage of the ND-indexing and other array features, but the values of the array are Python objects, which themselves have an `__array__` method defined, but the `__array__` method is inefficient and should be avoided.\r\n\r\nIn these cases I am not able to construct an object array from a nested list via slicing without invoking the objects `__array__` method.\r\n\r\nA simplified example of where this is a problem is as follows:\r\n\r\nConsider a skeleton implementation of a Python class for N-qubit Pauli matrices. These objects have an efficient representation (for simplicity I will just use the length N string labels), but can also be represented inefficiently as shape (2 ** N, 2 **N) complex matrices. Suppose this class defines an `__array__` method to convert to these inefficient complex arrays like shown in the bellow code example.\r\n\r\nNow suppose I want to construct an `object` ND-array of Paulis.  The simplest case would be\r\n\r\n```python\r\narr1 = np.empty([1], dtype=object)\r\narr1[0] = Pauli(\"X\")\r\n```\r\nThis does not invoke the `Pauli.__array__` method and returns `array([Pauli(X)], dtype=object)`.\r\n\r\nHowever if I try and set via slicing \r\n\r\n```python\r\narr2 = np.empty([1], dtype=object)\r\narr2[:] = [Pauli(\"X\")]\r\n# Prints: Pauli.__array__ called\r\n```\r\n This invokes the array method, even though the returned array is an object array with the original objects as values (`np.all(arr1 == arr2)` is True).\r\n \r\nComplete code shown bellow, where you can see where this becomes a problem for example by setting `nq=20` which would require ~17TB memory per Pauli for each `__array__` return.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nclass Pauli:\r\n    \"\"\"Basic N-qubit Pauli\"\"\"\r\n\r\n    _mats = {\r\n        \"I\": np.eye(2, dtype=complex),\r\n        \"X\": np.array([[0, 1], [1, 0]], dtype=complex),\r\n        \"Y\": np.array([[0, -1j], [0, 1j]], dtype=complex),\r\n        \"Z\": np.array([[1, 0], [0, -1]], dtype=complex),\r\n    }\r\n\r\n    def __init__(self, label):\r\n        \"\"\"Initialize a Pauli from a string label\"\"\"\r\n        self.label = label\r\n     \r\n    def __repr__(self):\r\n        return f\"Pauli({self.label})\"\r\n    \r\n    def to_matrix(self):\r\n        \"\"\"Convert to a (2**N, 2**N) complex matrix\"\"\"\r\n        ret = np.eye(1, dtype=complex)\r\n        for i in self.label:\r\n            ret = np.kron(self._mats[i], ret)\r\n        return ret\r\n\r\n    def __array__(self, dtype=None):\r\n        # Print to know when the `__array__` method is invoked\r\n        print(\"Pauli.__array__ called\")\r\n        mat = self.to_matrix()\r\n        if dtype:\r\n            mat = mat.astype(dtype)\r\n        return mat\r\n\r\n    def __eq__(self, other):\r\n        if isinstance(other, Pauli):\r\n            return self.label == other.label\r\n        return False\r\n\r\n# Create an array-like nested list of Pauli objects\r\nnq = 1\r\npaulis = [[Pauli(nq * \"I\"), Pauli(nq * \"X\")],\r\n          [Pauli(nq * \"Y\"), Pauli(nq * \"Z\")]]\r\n\r\n# Initialize and empty array and set values to nested list elements\r\narr = np.empty([2, 2], dtype=object)\r\narr[:] = paulis\r\n\r\n# This print `Pauli.__array__ called` 4 times, so invokes it for each Pauli\r\n# Still returns an object array with the `Pauli` objects as values.\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\nMy main numpy runtime is\r\n```\r\n1.23.5\r\n3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \r\n[Clang 13.0.1 ]\r\n```\r\n\r\nI also tried on Numpy 1.25.2 and get same results.\r\n\r\n\r\n### Context for the issue:\r\n\r\nThis is probably quite a specific edge case, but it is something I've been struggling with for awhile and have had to come up with various messy work arounds to avoid the issue.","comments":["Assuming that you only have nested `list`s and your objects don't subclass from `list`, here is a somewhat robust workaround for the time being:\r\n\r\n```python\r\ndef flatten_lol(lol):\r\n    for sub in lol:\r\n        if isinstance(sub, list):\r\n            yield from flatten_lol(sub)\r\n        else:\r\n            yield sub\r\n\r\ndef shape_lol(lol):\r\n    child = lol[0]\r\n    if isinstance(child, list):\r\n        return (len(lol),) + shape_lol(child)\r\n    else:\r\n        return (len(lol),)\r\n\r\ndef array_from_lol(lol):\r\n    shape = shape_lol(lol)\r\n    arr = np.fromiter(flatten_lol(lol), dtype=object).reshape(shape)\r\n    return arr\r\n```","Agree that we should try to avoid this.  One detail to think about is what happens in these two cases:\r\n```\r\nnp.array([None, Pauli(...)], dtype=object)\r\nnp.array([Pauli(...), None], dtype=object)\r\n```\r\nare we OK with not calling `__array__()` in the first, but doing so in the second or should we be precise and only not call things when the user requested a maximum number of dimensions (which is the case here implicitly).\r\n\r\nWe have all the information needed to adjust this either way (or can add it easily).  The code that does this is part of `PyArray_DiscoverDTypeAndShape_Recursive` in `array_coercion.c`.","To be clear, it's not `np.array(nested_list)` that's being reported here; that's probably hopeless. Rather `arr[:] = nested_list`, which is what we usually recommend as the workaround.","Yes, I am aware `np.array(nested_list)` doesn't have the maximum depth information while `arr[...] = nested_list` does.  But, it does use the same machinery, effectively somthing like `np.asarray(nested_list, dtype=arr.dtype, maxdims=arr.ndim)`.","In which case, no, I would expect no difference between the two cases."],"labels":["00 - Bug"]},{"title":"BUG: astype changes the shape of structured arrays","body":"### Describe the issue:\r\n\r\nAs of numpy 1.25, the following deprecation warning appears when setting the dtype of an array that will overflow:\r\n\r\n> \/home\/sarah\/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 248 to int8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)\r\nwill give the desired result (the cast overflows).\r\n  arr = np.array(dat, dtype=mydtype)\r\n\r\nHowever, if the dtype is a multipart structured array, the suggested fix duplicates all the values, thus changing the shape of the array, as can be seen in the example code below.\r\n\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np \r\nmydtype = [('re','i1'),('im','i1')]\r\n\r\ndat = [(2,158),(6,856),(248,35)]\r\n\r\narr = np.array(dat, dtype=mydtype)\r\n# works but is deprecated\r\n\r\narr2 = np.array(dat).astype(mydtype)\r\nprint(arr,arr2)\r\n\r\nprint(\"Is arr equal to arr2 ? \", np.array_equal(arr,arr2))\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```\r\nsarah@sarah-optiplex-ubuntu:~$ python dep.py\r\n\/home\/sarah\/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 158 to int8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)\r\nwill give the desired result (the cast overflows).\r\n  arr = np.array(dat, dtype=mydtype)\r\n\/home\/sarah\/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 856 to int8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)\r\nwill give the desired result (the cast overflows).\r\n  arr = np.array(dat, dtype=mydtype)\r\n\/home\/sarah\/dep.py:6: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 248 to int8 will fail in the future.\r\nFor the old behavior, usually:\r\n    np.array(value).astype(dtype)\r\nwill give the desired result (the cast overflows).\r\n  arr = np.array(dat, dtype=mydtype)\r\n[( 2, -98) ( 6,  88) (-8,  35)] [[(  2,   2) (-98, -98)]\r\n [(  6,   6) ( 88,  88)]\r\n [( -8,  -8) ( 35,  35)]]\r\nIs arr equal to arr2 ?  False \r\n```\r\n\r\n### Runtime information:\r\n\r\nimport sys, numpy; print(numpy.__version__); print(sys.version) : \r\n1.25.1\r\n3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\n\r\n\r\nprint(numpy.show_runtime()) :\r\n\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.25.1',\r\n  'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]',\r\n  'uname': uname_result(system='Linux', node='sarah-optiplex-ubuntu', release='5.15.0-78-generic', version='#85-Ubuntu SMP Fri Jul 7 15:25:09 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n\r\n\r\n\r\n### Context for the issue:\r\n\r\nWorking around this bug requires multiple difficult-to-read lines of code, when it should by working similarly to how the deprecated code worked. ","comments":["In this case you will have to provide a large enough integer to the structured dtype, i.e. go via `arr.astype(\"i8,i8\").astype(\"i1,i1\")` to use the astype trick."],"labels":["00 - Bug"]},{"title":"BUG: Fix piecewise implicit return type conversion","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\nCloses #24155 and #19755.\r\n\r\nFix for the bug which resulted in the output of piecewise function to be same dtype as the input. Now, the output dtype matches with the dtype of funclist output instead of the input.","comments":[],"labels":["00 - Bug","component: numpy.lib"]},{"title":"NumPy 2.0 development status & announcements","body":"The purpose of this issue is to serve as a brief \"umbrella issue\" which (a) links out to some key design proposals and other places where design changes and guidance for the 2.0 release are described, and (b) everyone who is interested can subscribe to in order to get updates from maintainers around the 2.0 release (not everyone may want to subscribe to the mailing list). This issue will remain pinned for high visibility.\r\n\r\nThe tentative release date for the first release candidate of NumPy 2.0 is around 1 Mar 2024, and the final release 6-8 weeks later.\r\n\r\n_EDIT: this issue is for announcements, please do not start technical discussions here_\r\n\r\n\r\n**Design changes \/ proposals**\r\n\r\n- NumPy 2.0 Project Board: https:\/\/github.com\/orgs\/numpy\/projects\/9\r\n- Main NumPy Enhancement Proposals for 2.0:\r\n\r\n    - [NEP 53 - Evolving the NumPy C-API for NumPy 2.0](https:\/\/numpy.org\/neps\/nep-0053-c-abi-evolution.html)\r\n    - [NEP 52 - Python API cleanup for NumPy 2.0](https:\/\/numpy.org\/neps\/nep-0052-python-api-cleanup.html)\r\n    - [NEP 50 - Promotion rules for Python scalars](https:\/\/numpy.org\/neps\/nep-0050-scalar-promotion.html)\r\n\r\n- Slides from the NumPy 2.0 Developer Meeting in April'23: https:\/\/github.com\/numpy\/archive\/tree\/main\/2.0_developer_meeting\r\n\r\n\r\n**Key guidance for users and downstream package authors**\r\n\r\n1. If you rely on the NumPy C API (e.g. via direct use in C\/C++, or via Cython code that uses NumPy), please add a `numpy<2.0` requirement in your package's dependency metadata (for releases only, not on your main dev branch). _Rationale: the NumPy C ABI will change in 2.0, so any compiled extension modules that rely on NumPy are likely to break, they need to be recompiled._\r\n2. If you rely on a large API surface from NumPy's Python API, also consider adding the same ` numpy<2.0` requirement to your metadata. _Rationale: we will do a significant cleanup (see NEP 52), so unless you only use modern\/recommended functions and object, your code is likely to require at least some adjustments._\r\n3. Consider cleaning up your code. E.g. remove `from numpy import *`, or importing any private modules like `numpy.core`. See https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/tests\/test_public_api.py#L114-L126 for what we consider public\/private. If it's not in the NumPy docs or in the list of public modules there, don't use it!\r\n4. Plan to do a release of your own packages which depend on `numpy` shortly after the first NumPy 2.0 release candidate is released (probably in Dec 2023). _Rationale: at that point, you can release packages that will work with both 2.0 and 1.X, and hence your own end users will not be seeing much\/any disruption (you want `pip install mypacackage` to continue working on the day NumPy 2.0 is released)._\r\n5. Consider testing against NumPy nightlies in your own CI. We publish those at https:\/\/anaconda.org\/scientific-python-nightly-wheels\/numpy, and have documented that as a stable location at https:\/\/numpy.org\/devdocs\/dev\/depending_on_numpy.html. _Rationale: this will detect potential issues in your code so you can fix them well ahead of the NumPy 2.0 release_.\r\n","comments":["@andyfaff in an attempt to prep for a next step where we do meaningful ABI breaks (changing the default integer may be that already).\r\nCould we get SciPy to upload nightlies that are build against the NumPy nightlies?  They don't have to pass\/run the tests with it, but would have to use it at build time to get newer headers.\r\n\r\nThe next in line might be matplotlib, but SciPy doing it would go a long way I suspect.\r\n\r\nExample changes:\r\n* @mtsokol might want to move the import, it would be nice to avoid the warning downstream (although probably not vital).\r\n* Opaquify the `PyArray_Descr` struct.  This requires version sensitive macros for accessing the fields (We can probably retain most fields position, but I don't think all).\r\n* The int64 on windows adds a way to get the default integer (new, version sensitive macro).  You will want to use it with an `#ifdef`, but if it is used anywhere the right thing would be to compile with new NumPy.","I don't know what the scipy plan is to handle that transition. i.e. when do we need to make a release that only builds against numpy2.0 and newer.\r\nI'd have to look into how to do it, I wouldn't want to overly complicate the existing wheel build (too many conditionals make it harder to handle). I wonder if making another workflow file would be easier?\r\n\r\n@rgommers, could we handle making _all_ nightlies against numpy 2.0, or do we need to keep building against already released numpy?","It depends on whether those scipy nightlies will then be ABI-compatible with both numpy 2.0-dev and with 1.25.x. That is the plan for the final 2.0 release, but will it be the case during the development cycle @seberg?","Yes, of course they will be ABI compliant to both.  I am not really planning on a compat package anymore anyway, I suspect we have few enough things that a few header-only functions are sufficient (and *far* easier to manage).","e.g. at the moment the scipy pyproject.toml file will want to build against `numpy==1.22.4` for Python 3.10 wheels, that's what the last scipy would've been built against. If we start making wheels against `numpy>=2.0` then I think, but am not sure, that the numpy versionw in pyproject.toml would need to be bumped to the oldest ABI compatible version (1.25.2?).\r\n\r\nI know how to amend the build process, but am not sure how to amend the metadata (oldest-supported-numpy as well?) so that we don't get a raft of complaints from people using too old a version of numpy.","You don't need `oldest-supported-numpy` anymore (since 1.25.x) unless you need it for things beyond being compatible with older NumPy versions.","Great, then in the nightlies we can just use an unpinned `numpy` build requirement, and install numpy from the scientific-python nightlies bucket, that should work fine.","@andyfaff that'd be a change that is fine to make in SciPy's `main` branch. There are already notes in `pyproject.toml` about this topic. \r\n\r\nThis is a pretty detailed discussion for this tracking issue. How about we leave it here at \"SciPy nightlies will switch to building against NumPy nightlies soon\" and then circle back here once that works to recommend other projects do the same?","Following up on the discussion above, are there now SciPy nightlies being built with NumPy 2? If so, how would one get them? If not, do we have a sense of when they might be available?","@jakirkham yes, the scipy nightlies are being built against `numpy>=2.0.0.dev0`. They are available from https:\/\/anaconda.org\/scientific-python-nightly-wheels\/scipy\/files.","Thanks Andrew! \ud83d\ude4f","> Plan to do a release of your own packages which depend on numpy shortly after the first NumPy 2.0 release candidate is released (probably in Dec 2023).\r\n\r\nWould it be useful to do a bit more extensive pre-release schedule than with a regular (minor) NumPy release? Maybe alpha builds every 2 or 3 weeks starting soon, then a month or two ahead every week a beta, and then the last week release candidates.\r\n\r\nSince it's such a large release it would be nice being able to test extensively against pre-releases, and having a bit more time for that. It would also engage other communities and maintainers which could offer valuable perspectives before the final release is made.","There are regular nightly builds which should aid in prerelease testing","> There are regular nightly builds which should aid in prerelease testing\r\n\r\nAgreed, I think there is additional value for \"formal\" pre-releases, which are published on GitHub and PyPI, can be installed with `pip install numpy --pre`, and feedback can be delivered on. It gives a concrete and easy targets to install and test against.","We don't want to do that, certainly not alpha's - and probably no beta's either, only RCs. A lot of things are in flux, may be reverted, etc. We're in the middle of ABI changes, and having any pre-release up on PyPI before the ABI changes are complete may be quite disruptive, both immediately and because pip may cache wheels with an intermediate state of the ABI.\r\n\r\nUntil we're ready for wider testing by anyone who may have `--pre` jobs, interested folks should test against https:\/\/anaconda.org\/scientific-python-nightly-wheels\/numpy.\r\n","Upper version pins in `requires` biting things again...  The recommendation makes sense for releases (especially if the releases are regular), but it is no good for development versions.\r\n\r\nThere are two plausible views, [I can see](https:\/\/github.com\/numpy\/numpy\/pull\/24949#issuecomment-1770739742):\r\n* Only release versions should have an upper version pin (unless you know its broken).  Dev branches should not (for many packages it may be better to risk releasing without than vice versa).\r\n* Use **`numpy<=2.0.0.dev0`** specifically allowing trying to use our nightlies.\r\n\r\nI am tempted to adjust the recommendation to `numpy<=2.0.0.dev0` as a safe and workable path for packages who feel more comfortable adding an upper version pin to ensure that official release versions have one.","An update on timeline: there are ~6 topics on the roadmap that still require time and are important to get in for 2.0. At the community meeting two days ago we discussed that we'll likely need another 2 months or so to get those all done and are therefore targeting a first release candidate around the end of the year. The period from RC to final release is at least 6 weeks as usual, but it's possible that that will stretch out more - that depends on how many issues get reported.","I agree with adjusting the pinning recommendation from https:\/\/github.com\/numpy\/numpy\/issues\/24300#issuecomment-1770748369. In https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/27899, scikit-learn needed to explicitly install NumPy 2.0 from the nightly index because pandas had `numpy<2.0` on the dev branch.","`numpy<=2.0.0.dev0` in released versions of downstream packages does not make sense to me. We have already broken ABI and API pretty substantially, so if a package builds against 1.2X and then pick up the latest `2.0.0.dev0` that will not work.\r\n\r\n> Only release versions should have an upper version pin (unless you know its broken). \r\n. ...\r\n> because pandas had `numpy<2.0` on the dev branch.\r\n\r\nAgreed. It seems like this needs a \"for releases only\" addition - adding now. That was clearly intended, one should never pin anything on their main branch unless there is a known source-level incompatibility.","Thanks all for your hard work on this. Should\r\n> The tentative release date for NumPy 2.0 is end of December 2023.\r\n\r\nbe updated based on https:\/\/github.com\/numpy\/numpy\/issues\/24300#issuecomment-1783511140 to state that the first RC will be end of Dec with the release expected in early Feb 2024?","Thanks @braingram, updated now.","Hi all, one more update on timing. We just reviewed all the remaining work items that are deemed important for NumPy 2.0 in the community meeting - they're all in progress or close to the finish line and we're reasonably confident that in one month from now they'll all be done and we can have the first release candidate at the end of January. So this moves the timeline by 1 month.","Hi @rgommers \u2013 I'm having trouble squaring the advice above:\r\n\r\n> at that point, you can release packages that will work with both 2.0 and 1.X, and hence your own end users will not be seeing much\/any disruption (you want pip install mypacackage to continue working on the day NumPy 2.0 is released).\r\n\r\n... with the general documentation on ABI compatibility (from https:\/\/numpy.org\/doc\/1.21\/user\/depending_on_numpy.html#build-time-dependency)\r\n\r\n> Because the NumPy ABI is only forward compatible, you must build your own binaries (wheels or other package formats) against the lowest NumPy version that you support (or an even older version).\r\n\r\nMy experience so far is that if packages are built against 1.X, they don't work with numpy nightly wheels because of a hard-coded ABI version check. But if I build against 2.X, the *Build-time dependency* docs quoted above suggest that we shouldn't expect the result to be compatible with numpy 1.X at runtime.\r\n\r\nHow should downstream package authors plan to build wheels for release to ensure compatibility with both NumPy 2.0 and NumPy 1.X?\r\n","> My experience so far is that if packages are built against 1.X, they don't work with numpy nightly wheels because of a hard-coded ABI version check.\r\n\r\nYes, this will not work indeed.\r\n\r\n> But if I build against 2.X, the Build-time dependency docs quoted above suggest that we shouldn't expect the result to be compatible with numpy 1.X at runtime.\r\n\r\n(could you update the link? it goes to something private)\r\n\r\nThe summary here is:\r\n- This has always been true, _however_ there was a change in NumPy 1.25 where the _exported_ C API is no longer that for the current NumPy version but a much older one (1.19), so things got better compat wise.\r\n- In addition, 2.0 has a compat shim that bridges the ABI differences, in order to make wheels built against 2.0 also work with 1.X by default. \r\n\r\n> How should downstream package authors plan to build wheels for release to ensure compatibility with both NumPy 2.0 and NumPy 1.X?\r\n\r\nBuild against 2.0 after the ABI-stable 2.0.0rc1 arrives, and declare a runtime dependency of >=1.2x.y (any x.y here should work).","Link updated, sorry about that","https:\/\/numpy.org\/doc\/1.26\/dev\/depending_on_numpy.html#build-time-dependency should have more info on the \"exported C API version\" topic already than those 1.21 docs. \r\n\r\nThe 2.0 compat thing isn't anywhere yet except for in this issue I think. We should add it to that same \"for downstream distributors\" page, and then link it from https:\/\/numpy.org\/devdocs\/numpy_2_0_migration_guide.html.","Ah, thanks. I just found the 1.21 docs via google search; I hadn't seen the 1.26 update.","Is there any documentation about how to publish NumPy 2 wheels for downstream packages.  As I understand things, packages might need 2 versions - 1 for NumPy <= 1.99 and a second for NumPy >= 2, for each version\/ABI supported on PyPI.  Will this be possible?\r\n\r\nEdit: Just read some I missed above, and it sounds like any build against NumPy 2 will also work on NumPy >= 1.20.x, assuming that other API changes have been accounted for (e.g., the removal of `issubsctype` which has no Deprecation\/FutureWarning).\r\n\r\nFWIW the removal of `issubsctype` will affect some less maintained packages, e.g., patsy < 0.5.5. ","> it sounds like any build against NumPy 2 will also work on NumPy >= 1.20.x, \r\n\r\nIndeed - and it has to be that way (it's not possible to upload two wheels where the only difference is the numpy version used during the build). No docs yet though, we should fix that before RC1 comes out. \r\n","It's been documented since 1.25: https:\/\/numpy.org\/doc\/stable\/dev\/depending_on_numpy.html#build-time-dependency\r\nMaybe you think it should be clarified, otherwise it should just be mentioned again in the release notes wherever it says that you must compile with 2.0.","> It's been documented since 1.25:\r\n\r\nAll it says is: _\"At the time of NumPy 1.25, NumPy 2.0 is expected to be the next release of NumPy. The NumPy 2.0 release is expected to require a different pin, since NumPy 2+ will be needed in order to be compatible with both NumPy 1.x and 2.x.\"_ That is a bit too sparse, as the questions here highlight. I just opened gh-25512 to add more extensive 2.0-specific docs.","@rgommers Thanks for the write up.  I suppose there is another important practical issue for downstream supporting NumPy 2. \r\n\r\nSuppose a project depends on some mid-stream projects, e.g., `statsmodels -> pandas, scipy -> numpy`.  statsmodels can only really support NumPy 2 if it is running on pandas and scipy that also support numpy 2.  While I haven't seen any plans, I suspect that this will mean that the minimums for the mid-stream projects will be whatever is released after NumPy 2 is GA.  This makes it seem like there is practically 1 set of requirements for NumPy 1 and a different one for NumPy 2.  Is there any practical way to enforce this? \r\n\r\nI think if all projects where capped, e.g., `numpy>=1.xx.0,<2` than it would work fine, since only those that explicitly remove the cap could be used with .  However, with lots of versions missing caps it seems less easy.\r\n","> I suspect that this will mean that the minimums for the mid-stream projects will be whatever is released after NumPy 2 is GA. This makes it seem like there is practically 1 set of requirements for NumPy 1 and a different one for NumPy 2. Is there any practical way to enforce this?\r\n\r\nI don't think that needs to be the case - and we certainly wouldn't want to enforce this (rather the opposite, we tried hard not to make this kind of a split).\r\n\r\n> However, with lots of versions missing caps it seems less easy.\r\n\r\nThis isn't a necessary requirement - all you need is the most recent pre-2.0-compatible version to contain a `<2` cap. Reason: if a user does `pip install statsmodels` now, they get numpy 1.xx for any version of `pandas`. If they do `pip install statsmodels` and statsmodels has something like `pandas >1.4; scipy >1.8` as requirements, then that won't go wrong - they'll still get the newest versions of everything.\r\n\r\nThe only thing that can go wrong with missing caps for older versions is if users explicitly do things like `pip install statsmodels \"pandas==1.4\"`. That may then pull in `numpy` 2.0 and that breaks. But it's break with a clean error message about a numpy version mismatch, and can then easily be remedied by the user (upgrade pandas, or downgrade numpy).\r\n\r\nWe probably do need to pay some attention to the help page users will find from the ABI mismatch error message, that will get some traffic.","> We probably do need to pay some attention to the help page users will find from the ABI mismatch error message, that will get some traffic.\r\n\r\nRight, there should be a clear place to google for that tells you to downgrade NumPy or try upgrading the dependency (I am not quite sure when it is useful, but saw an example where `python -v` import tracing was needed to see which module was compiled with the wrong version).\r\n\r\nOtherwise, I think this is a non-issue beyond users accidentally creating bad combinations as Ralf said.  Unless you hae to *build* against the oldest supported intermediate package (i.e. like we used to force you to build with the oldest NumPy you want to support)?","Am I correct the the recommendation for numpy 2 compatibility releases would to be remove the build dependency on `oldest-supported-numpy`, and to replace with a dependency on `numpy~=2.0.0`?","Yes indeed, dropping that - I'll update the README of `oldest-supported-numpy` with a prominent note.\r\n\r\n> and to replace with a dependency on `numpy~=2.0.0`?\r\n\r\nNo, that is too restrictive (it means `>=2.0.0,<2.1`. The upper bound, if used at all, should be `<3` or `<2.3`.","Full deprecation notice for `oldest-supported-numpy` at https:\/\/github.com\/scipy\/oldest-supported-numpy\/pull\/84 (comments there very welcome).","FWIW, the top post links to https:\/\/numpy.org\/neps\/nep-0053-c-abi-evolution.html, which is a bit out of date (I started looking at that instead of the more update to date documentation ..). For example it still mentions a ``numpy2_compat`` package (I don't know if you want to update NEPs after the fact, but otherwise a disclaimer that not everything is up to date might help)","Arg, you are right, need to add at least add a note that the `numpy2_compat` thought is descoped (not sure how much for a full rewrite I am up to, especially short term unfortunately).","Question for downstream packages releasing more or less now: the general guideline given above is to start releasing wheels (not nightly wheels) built against numpy 2.0 once the first release candidate is out (now targeted for Feb 1, 2024). \r\nThat means that a package is that released this month should still build against numpy 1.x and add the `numpy<2` upper pin. But do you know if there are still changes planned that will break the ABI the coming weeks? Or might it already be generally safe to actually release a numpy-2-built wheel right now? Or best to keep with numpy 1.x this month?","> But do you know if there are still changes planned that will break the ABI the coming weeks? Or might it already be generally safe to actually release a numpy-2-built wheel right now?\r\n\r\nI believe one or two breaks are still planned. So best to stay with 1.x for now. ","Hi all! Some time ago we added a new rule to `ruff` linter, `NPY201`, which updates the codebase to a NumPy 2.0 compatible version.\r\nYou can find details here: https:\/\/docs.astral.sh\/ruff\/rules\/numpy2-deprecation\/ (it's still in a \"preview\" mode but available since `0.1.4` release).","Hi all, quick update: we're going to need slightly more time, but the finish line is in sight. In today's community meeting we went through the O(10) items still open for the [2.0.0 milestone](https:\/\/github.com\/numpy\/numpy\/milestone\/118) and concluded that there's only a couple of blocking items left (primarily two necessary C API changes, and one NEP 56 item). These are planned to be completed within the next two weeks, so we should see `2.0.0rc1` in 2-3 weeks from now.","We are currently releasing scikit-learn and are looking at the transition to NumPy 2.0.\r\n\r\n@rgommers we were wondering if in the release process of NumPy, it is conceivable for NumPy to release a last RC that would be the same as 2.0.0 and provide a week for us (and the downstream packages) to build against it. We ensure in this case to not have any downtime and avoid a potential corner case that we saw below.\r\n\r\nIn our previous release, we pin `numpy < 2` but we realized a corner case: if something trigger the following `pip install numpy==2.0 scikit-learn`, then `pip` would end up trying to install an old version of scikit-learn that does not have the pin and that is not compatible as well.\r\n","> release a last RC that would be the same as 2.0.0 and provide a week for us (and the downstream packages) to build against it.\r\n\r\nYes, we'll leave more than enough time and won't do the final 2.0.0 release until at least all the most widely used libraries like scikit-learn, pandas, scipy, etc. are compatible. Otherwise things will turn into a mess indeed.\r\n\r\nThe regular time period from a numpy RC to the final release is a month or more; I think we'll have 6 weeks at minimum between RC1 and final release (perhaps more depending on how downstream releases go).","> In our previous release, we pin `numpy < 2` but we realized a corner case: if something trigger the following `pip install numpy==2.0 scikit-learn`, then `pip` would end up trying to install an old version of scikit-learn that does not have the pin and that is not compatible as well.\r\n\r\nYes, `pip` selecting older versions is obviously problematic, but getting that changed is being discussed since forever and I don't see it landing any time soon.\r\n\r\nScikit-learn really should always put an upper bound on numpy, not doing so is clearly wrong. Your 2.0-compatible release should either use `<2.x+N` or `<3` (see https:\/\/numpy.org\/devdocs\/dev\/depending_on_numpy.html#runtime-dependency-version-ranges). Major NumPy releases are ABI breaks, so not using at least `<3` will set you up for the same problem in case we'd get a numpy 3.0 in a few years (may not happen, but who knows). Given how much you use of the NumPy API, I'd strongly recommend using the `<2.x+N` rule (with N=3).","> Major NumPy releases are ABI breaks, so not using at least <3 will set you up for the same problem in case we'd get a numpy 3.0 in a few years (may not happen, but who knows). Given how much you use of the NumPy API, I'd strongly recommend using the <2.x+N rule (with N=3).\r\n\r\nThanks. Point taken. I'll bring up on the table to be discussed on our side. This looks reasonable to me.","> These are planned to be completed within the next two weeks, so we should see `2.0.0rc1` in 2-3 weeks from now.\r\n\r\nSorry folks, looks like that was slightly optimistic, we need another similar amount of time. The last bits turned out to be a little larger than expected. We will get there soon!","The `maintenance\/2.0.x` branch has been created, so we're almost there - beta 1 will arrive without hours to days. I'll re-post from the mailing list:\r\n\r\nLet me give the optimistic and pessimistic timelines. Optimistic:\r\n\r\n- 2.0.0b1 later today\r\n- 2.0.0rc1 (ABI stable) in 7-10 days\r\n- 2.0.0 final release in 1 month\r\n\r\nPessimistic:\r\n\r\n- 2.0.0b1 within a few days\r\n- 2.0.0rc1 (ABI stable) in 2 weeks\r\n- 2.0.0rc2 in 4 weeks\r\n- 2.0.0rc3 in 6 weeks\r\n- 2.0.0 final release in 8 weeks\r\n\r\nFor projects which have nontrivial usage of the NumPy API (and especially if they also use the C API), I'd recommend:\r\n1. Check whether things work with 2.0.0b1, ideally asap so if there is anything we missed we can catch it before rc1. Perhaps do a pre-release of your own package\r\n2. Do a final release after 2.0.0rc1 - ideally as soon as possible after, and definitely before the final 2.0.0 release\r\n\r\nFor (2), note that there are a ton of packages that do not have correct upper bounds, so if you haven't done your own new release that is compatible with both 2.0.0 and 1.26.x *before* 2.0.0 comes out, the users of your project are likely to have a hard time.","Apologies if this is the incorrect place to ask - I perused around the documentation and issues and haven't seen it mentioned, but didn't want to open up an entire issue based on this. If it is preferable to take this somewhere else, kindly let me know and I'll happily delete this and relocate it. \r\n\r\nIs there currently any sense on whether or not the breaking changes, especially those in the C-API, will affect the numpy SWIG interface compatibility? Or should it continue to work as expected? I understand the binary compatibility will be broken, which is fine, just trying to anticipate impacts to C and C++ libraries that leverage SWIG as a bridge to numpy. ","Hi @keltonhalbert, great question. I'm fairly sure that the SWIG interface is completely untested inside this repo. A quick search doesn't turn up usage of removed C API for me, but I wouldn't be surprised if for example the `long` -> `intp` default integer change requires a tweak to `tools\/swig\/numpy.i`. It'd be great if you could check your code with `2.0.0b1` and see if it works.\r\n\r\n> If it is preferable to take this somewhere else, kindly let me know and I'll happily delete this and relocate it.\r\n\r\nNew issues are preferable to commenting on this one, since this is an announcement issue. If you find anything in SWIG support isn't working, could you please open a new issue?","Hi @rgommers, \r\n\r\nThank you for the quick response and clarity on how to handle any potential issues further. I'll try building with 2.0.0b1 and open a new issue if anything starts to go sideways. \r\n\r\nAlso, a preliminary congrats on the forthcoming milestone release. I appreciate the work you all put into developing and maintaining this library!"],"labels":["62 - Python API","63 - C API","Tracking \/ planning"]},{"title":"BUG: \"import numpy\"  hangs","body":"### Describe the issue:\r\n\r\nThe problem is that `import numpy` hangs forever if sys.stdin is being read simultaneously in a separate thread AND the script that imports numpy is started with `Popen`, which is configured to forward stdin, stdout, stderr to subprocess.PIPE. \r\n\r\nTo reproduce the issue, run `python reproduce.py`. It should print \"Numpy imported\" but hangs forever instead.\r\n\"Numpy imported\" is printed if the line that spawns wait_for_stop is commented.\r\n\r\n### Reproduce the code example:\r\n\r\npython reproduce.py:\r\n```\r\nimport subprocess\r\nimport sys\r\n\r\nproc = subprocess.Popen([sys.executable, 'numpy_script.py'],\r\n                        stdin=subprocess.PIPE,\r\n                        stdout=subprocess.PIPE,\r\n                        stderr=subprocess.PIPE,\r\n                        bufsize=0,\r\n                        creationflags=subprocess.CREATE_NO_WINDOW)\r\n\r\nprint(proc.stdout.read())\r\n```\r\n\r\nnumpy_script.py:\r\n```\r\nimport sys\r\nimport _thread\r\nimport threading\r\n\r\n\r\ndef wait_for_stop():\r\n    sys.stdin.read()\r\n    _thread.interrupt_main()\r\n\r\n\r\nthreading.Thread(target=wait_for_stop, daemon=True).start()\r\n\r\nimport numpy\r\n\r\nprint('Numpy imported')\r\n```\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.25.1\r\n3.10.5 (tags\/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]\r\n```\r\n\r\n```\r\n>>> print(numpy.show_runtime())\r\n[{'numpy_version': '1.25.1',\r\n  'python': '3.10.5 (tags\/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 '\r\n            '64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', node='FI-L-7376992', release='10', version='10.0.19044', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Haswell',\r\n  'filepath': 'C:\\\\Users\\\\ADMIN\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\numpy-bug-Txf2mLmS-py3.10\\\\Lib\\\\site-packages\\\\numpy\\\\.libs\\\\libopenblas64__v0.3.23-gcc_10_3_0.dll',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 12,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23'}]\r\nNone\r\n>>>\r\n```\r\n\r\n### Context for the issue:\r\n\r\n`import numpy` should never hang.","comments":["I think this may be a limitation of capturing `stdin` on windows.\r\nIs this specific to NumPy or does it happen with other large libraries as well? ","I just came here to report this exact bug. I don't know of any other library that has this issue (in my use-case, doing a language server where the stdin is expected to be used as the communication channel with VSCode this makes it quite annoying to use numpy as it's easy to inadvertently do an import after the communication is in place and that will always hang).\r\n\r\nDumping the stack trace, the place where it's stuck (using `Python 3.10.12` and `numpy 1.26.4` on Windows 11) is at:\r\n\r\n```\r\n-------------------------------------------------------------------------------\r\n Thread Thread-1 (in_thread)  (daemon: False)\r\n\r\n File \"python_install_dir\\lib\\threading.py\", line 973, in _bootstrap\r\n   self._bootstrap_inner()\r\n File \"python_install_dir\\lib\\threading.py\", line 1016, in _bootstrap_inner\r\n   self.run()\r\n File \"python_install_dir\\lib\\threading.py\", line 953, in run\r\n   self._target(*self._args, **self._kwargs)\r\n File \"...\\snippet1.py\", line 49, in in_thread\r\n   line = stdin.readline()\r\n\r\n-------------------------------------------------------------------------------\r\n Thread MainThread  (daemon: False)\r\n\r\n File \"...\\snippet1.py\", line 112, in <module>\r\n   import numpy\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"python_install_dir\\lib\\site-packages\\numpy\\__init__.py\", line 144, in <module>\r\n   from numpy.__config__ import show as show_config\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"python_install_dir\\lib\\site-packages\\numpy\\__config__.py\", line 4, in <module>\r\n   from numpy.core._multiarray_umath import (\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"python_install_dir\\lib\\site-packages\\numpy\\core\\__init__.py\", line 24, in <module>\r\n   from . import multiarray\r\n File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"python_install_dir\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 10, in <module>\r\n   from . import overrides\r\n File \"<frozen importlib._bootstrap>\", line 1078, in _handle_fromlist\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n File \"python_install_dir\\lib\\site-packages\\numpy\\core\\overrides.py\", line 8, in <module>\r\n   from numpy.core._multiarray_umath import (\r\n File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\r\n File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n File \"<frozen importlib._bootstrap_external>\", line 1176, in create_module\r\n File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n```"],"labels":["00 - Bug"]},{"title":"BUG: (Typing) \"Invalid self argument\" `...[integer[...]]` for `__isub__`","body":"### Describe the issue:\r\n\r\nSimilar to https:\/\/github.com\/numpy\/numpy\/issues\/23796, and I hope it's not a false report again.\r\n\r\nmypy 1.4.1 fails to type-check the code example below. Since `__isub__` works for all (signed or unsigned) integer types that I found, I think it should accept `np.integer` as well, not just `np.unsignedinteger`.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nfrom typing import Any\r\n\r\nimport numpy as np\r\nimport numpy.typing as npt\r\n\r\n# Mypy fail\r\nresult_i: npt.NDArray[np.integer[Any]] = np.arange(5, dtype=np.uint8)\r\nresult_i -= 5\r\n\r\n# Mypy passes\r\nresult_u: npt.NDArray[np.unsignedinteger[Any]] = np.arange(5, dtype=np.uint8)\r\nresult_u -= 5\r\n\r\nresult_int = np.arange(5, dtype=int)\r\nresult_int -= 5\r\n\r\nresult_int_ = np.arange(5, dtype=np.int_)\r\nresult_int_ -= 5\r\n\r\nresult_intc = np.arange(5, dtype=np.intc)\r\nresult_intc -= 5\r\n\r\nresult_up = np.arange(5, dtype=np.uintp)\r\nresult_up -= 5\r\n\r\nresult_u8 = np.arange(5, dtype=np.uint8)\r\nresult_u8 -= 5\r\n\r\nresult_u16 = np.arange(5, dtype=np.uint16)\r\nresult_u16 -= 5\r\n\r\nresult_u32 = np.arange(5, dtype=np.uint32)\r\nresult_u32 -= 5\r\n\r\nresult_u64 = np.arange(5, dtype=np.uint64)\r\nresult_u64 -= 5\r\n\r\nresult_ip = np.arange(5, dtype=np.intp)\r\nresult_ip -= 5\r\n\r\nresult_i8 = np.arange(5, dtype=np.int8)\r\nresult_i8 -= 5\r\n\r\nresult_i16 = np.arange(5, dtype=np.int16)\r\nresult_i16 -= 5\r\n\r\nresult_i32 = np.arange(5, dtype=np.int32)\r\nresult_i32 -= 5\r\n\r\nresult_i64 = np.arange(5, dtype=np.int64)\r\nresult_i64 -= 5\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nbug.py:8: error: Invalid self argument \"ndarray[Any, dtype[integer[Any]]]\" to attribute function \"__isub__\" with type \"Callable[[ndarray[Any, dtype[unsignedinteger[_NBit1]]], _SupportsArray[dtype[bool_ | unsignedinteger[Any]]] | _NestedSequence[_SupportsArray[dtype[bool_ | unsignedinteger[Any]]]] | bool | _NestedSequence[bool] | bool_ | int | integer[Any]], ndarray[Any, dtype[unsignedinteger[_NBit1]]]]\"  [misc]\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.25.1\r\n3.11.4 (tags\/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["While I would as a general rule of thumb recommend using `signedinteger`\/`unsignedinteger` over just `integer` (mostly to remain mindful of i8 + u8 -> f8 promotions), the issue you report is admittedly a very real and an area wherein the type annotations can be improved; in its current state `integer` overloads are completely absent for a the `__i<x>__` overloads."],"labels":["00 - Bug","Static typing"]},{"title":"BUG: f2py generates errors during C language phase","body":"### Describe the issue:\n\nI'm attempting to compile the subroutines for the DIVA differential equations solver from Math 77. The errors occur during the C language compilation phase. One of the error messages follows.\r\n\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:792:12: error: expected ';' after expression\r\n    divaf_t divaf_cb = { Py_None, NULL, 0 };\r\n           ^\r\n           ;\r\n\r\nThe source code is attached in the file below.\r\n\r\n[diva.f.txt](https:\/\/github.com\/numpy\/numpy\/files\/12205354\/diva.f.txt)\r\n\n\n### Reproduce the code example:\n\n```python\nSee the file above for the source code.\n```\n\n\n### Error message:\n\n```shell\nThe error messages and tracebacks follow.\r\n\r\nINFO: compile options: '-DNPY_DISABLE_OPTIMIZATION=1 -I\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9 -I\/Users\/user\/opt\/anaconda3\/lib\/python3.9\/site-packages\/numpy\/core\/include -I\/Users\/user\/opt\/anaconda3\/include\/python3.9 -c'\r\nINFO: clang: \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c\r\nINFO: clang: \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/fortranobject.c\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:385:9: warning: variable 'capi_j' set but not used [-Wunused-but-set-variable]\r\n    int capi_j,capi_i = 0;\r\n        ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:581:9: warning: variable 'capi_j' set but not used [-Wunused-but-set-variable]\r\n    int capi_j,capi_i = 0;\r\n        ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:792:12: error: expected ';' after expression\r\n    divaf_t divaf_cb = { Py_None, NULL, 0 };\r\n           ^\r\n           ;\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:792:5: error: use of undeclared identifier 'divaf_t'\r\n    divaf_t divaf_cb = { Py_None, NULL, 0 };\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:792:13: error: use of undeclared identifier 'divaf_cb'\r\n    divaf_t divaf_cb = { Py_None, NULL, 0 };\r\n            ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:792:24: error: expected expression\r\n    divaf_t divaf_cb = { Py_None, NULL, 0 };\r\n                       ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:793:5: error: use of undeclared identifier 'divaf_t'\r\n    divaf_t *divaf_cb_ptr = &divaf_cb;\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:793:14: error: use of undeclared identifier 'divaf_cb_ptr'\r\n    divaf_t *divaf_cb_ptr = &divaf_cb;\r\n             ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:793:30: error: use of undeclared identifier 'divaf_cb'\r\n    divaf_t *divaf_cb_ptr = &divaf_cb;\r\n                             ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:795:5: error: use of undeclared identifier 'divaf_typedef'\r\n    divaf_typedef divaf_cptr;\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:796:12: error: expected ';' after expression\r\n    divao_t divao_cb = { Py_None, NULL, 0 };\r\n           ^\r\n           ;\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:796:5: error: use of undeclared identifier 'divao_t'\r\n    divao_t divao_cb = { Py_None, NULL, 0 };\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:796:13: error: use of undeclared identifier 'divao_cb'\r\n    divao_t divao_cb = { Py_None, NULL, 0 };\r\n            ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:796:24: error: expected expression\r\n    divao_t divao_cb = { Py_None, NULL, 0 };\r\n                       ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:797:5: error: use of undeclared identifier 'divao_t'\r\n    divao_t *divao_cb_ptr = &divao_cb;\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:797:14: error: use of undeclared identifier 'divao_cb_ptr'\r\n    divao_t *divao_cb_ptr = &divao_cb;\r\n             ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:797:30: error: use of undeclared identifier 'divao_cb'\r\n    divao_t *divao_cb_ptr = &divao_cb;\r\n                             ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:799:5: error: use of undeclared identifier 'divao_typedef'\r\n    divao_typedef divao_cptr;\r\n    ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:822:72: error: use of undeclared identifier 'divaf_cb'\r\n        capi_kwlist,&tspecs_capi,&y_capi,&f_capi,&kord_capi,&neq_capi,&divaf_cb.capi,&divao_cb.capi,&idimt_capi,&idimy_capi,&idimf_capi,&idimk_capi,&iopt_capi,&PyTuple_Type,&divaf_xa_capi,&PyTuple_Type,&divao_xa_capi))\r\n                                                                       ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:822:87: error: use of undeclared identifier 'divao_cb'\r\n        capi_kwlist,&tspecs_capi,&y_capi,&f_capi,&kord_capi,&neq_capi,&divaf_cb.capi,&divao_cb.capi,&idimt_capi,&idimy_capi,&idimf_capi,&idimk_capi,&iopt_capi,&PyTuple_Type,&divaf_xa_capi,&PyTuple_Type,&divao_xa_capi))\r\n                                                                                      ^\r\n\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c:911:22: error: use of undeclared identifier 'divaf_cb'\r\nif(F2PyCapsule_Check(divaf_cb.capi)) {\r\n                     ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n2 warnings and 20 errors generated.\r\nerror: Command \"clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem \/Users\/user\/opt\/anaconda3\/include -arch x86_64 -I\/Users\/user\/opt\/anaconda3\/include -fPIC -O2 -isystem \/Users\/user\/opt\/anaconda3\/include -arch x86_64 -ftrapping-math -DNPY_DISABLE_OPTIMIZATION=1 -I\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9 -I\/Users\/user\/opt\/anaconda3\/lib\/python3.9\/site-packages\/numpy\/core\/include -I\/Users\/user\/opt\/anaconda3\/include\/python3.9 -c \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c -o \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.o -MMD -MF \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.o.d\" failed with exit status 1\r\n(base) user@Mac-Pro diva.f_for_f2py % \r\n(base) user@Mac-Pro diva.f_for_f2py % cp \/var\/folders\/2r\/4bw6nw0x58z0_ybx632_h14m0000gq\/T\/tmpsa0z02z5\/src.macosx-10.9-x86_64-3.9\/divamodule.c doit.c\r\n(base) user@Mac-Pro diva.f_for_f2py %\n```\n\n\n### Runtime information:\n\nThe command used to run f2py is:\r\n\r\nf2py3  --fcompiler=gnu95 --f77exec=\/usr\/local\/bin\/gfortran -c -m diva diva.f   \n\n### Context for the issue:\n\nI need to use the DIVA integrator for a Python script I'm writing to integrate orbits. DIVA aligns with the form of the equations of motion being used as well as the ability to numerically integrate the variational equations simultaneously. ","comments":["I can reproduce this issue on Linux (Ubuntu 22.10) with Python 3.11 and NumPy 1.25.1.\r\n\r\nI haven't done a deep dive into how f2py works but I'll play around with it a bit over the next week to see if I can come up with a fix.","It's been a while since I opened this issue. Is there any resolution on the horizon? \r\n"],"labels":["00 - Bug","component: numpy.f2py"]},{"title":"MAINT: Simplify ufunc.at and advanced indexing logic","body":"I had to try this out a bit, unfortunately the slowdown is a bit more than I had hoped (and then it looked like starting with advanced indexing.\r\n\r\nFor @mattip the focus would on the second commit, which is the `ufunc.at` change.\r\n\r\nThe idea here is the following:\r\n* Keep a buffer of pointers into the indexed array. \r\n* This is filled with a single machinery from all the index arrays (with 1 being somewhat special).  (At the moment, the super-fast indexing path is still special here.)\r\n* Use this also for the case where we also have slice dimensions (\"subspace\"), to keep things more similar.\r\n \r\nThis means:\r\n* The logic in the main advanced indexing cast gets somewhat easier.\r\n* The logic in `ufunc.at` inner-loops gets much simpler\r\n* `ufunc.at` special-loops can generalize to N-D or other cases much easier.\r\n\r\nPerformance-wise, it is a bit of a mixed bag:\r\n* `ufunc.at` seems 35-50% slower unfortunately...  I had really hoped the difference would be less\r\n  * (the first bug-fix that adjusted the buffer in-place actually also is slower of course)\r\n  * Playing with the buffer-size might shift it slightly, but didn't seem to do much really.\r\n  * Slowdown below on M1, I think my intel linux laptop may be have a bit less slowdown.\r\n* The advanced indexing using the new approach seem typically faster.  Some fluctuations are probably unrelated (and the largest ones almost certainly!)\r\n* Anything that was not easy to do before in `ufunc.at` will of course be much faster.\r\n\r\nI am a bit bummed by the slow-down.  OTOH, if @mattip isn't very bothered by it, I think I prefer the more general and simpler code this gives for `ufunc.at` loops (and advanced indexing.\r\n\r\n**EDIT\/UPDATE:**  I realized that if I have dedicate function for this, it is much easier to specialize it for the index arrays being contiguous (implementation isn't polished yet).\r\nThat recovers about half of the slowdown in `ufunc.at`, since chances are we don't do the same specialization for the `ufunc.at` loops.\r\n\r\n<details>  <summary>  Performance results (with contiguous specialization)  <\/summary>\r\n\r\n|   | [ee16c222]  | main |   Ratio | Benchmark (Parameter)                                                                |\r\n|----------|-------------------------------------|--------------------------------------------|---------|--------------------------------------------------------------------------------------|\r\n| +        | 11.9\u00b10.01ms                         | 15.3\u00b10.04ms                                |    1.29 | bench_ufunc.At.time_sum_at_random(<class 'numpy.uint16'>)                            |\r\n| +        | 11.9\u00b10.02ms                         | 15.2\u00b10.04ms                                |    1.28 | bench_ufunc.At.time_sum_at(<class 'numpy.int16'>)                                    |\r\n| +        | 11.9\u00b10.03ms                         | 15.3\u00b10.07ms                                |    1.28 | bench_ufunc.At.time_sum_at(<class 'numpy.uint16'>)                                   |\r\n| +        | 11.9\u00b10.02ms                         | 15.3\u00b10.06ms                                |    1.28 | bench_ufunc.At.time_sum_at_random(<class 'numpy.int16'>)                             |\r\n| +        | 10.2\u00b10.03ms                         | 13.0\u00b10.1ms                                 |    1.28 | bench_ufunc.At.time_sum_at_random(<class 'numpy.int8'>)                              |\r\n| +        | 10.2\u00b10.02ms                         | 12.9\u00b10.03ms                                |    1.27 | bench_ufunc.At.time_sum_at(<class 'numpy.int8'>)                                     |\r\n| +        | 10.2\u00b10.01ms                         | 12.9\u00b10.03ms                                |    1.27 | bench_ufunc.At.time_sum_at_random(<class 'numpy.uint8'>)                             |\r\n| +        | 10.2\u00b10.04ms                         | 12.9\u00b10ms                                   |    1.26 | bench_ufunc.At.time_sum_at(<class 'numpy.uint8'>)                                    |\r\n| +        | 12.5\u00b10.02ms                         | 15.2\u00b10.05ms                                |    1.22 | bench_ufunc.At.time_maximum_at(<class 'numpy.uint16'>)                               |\r\n| +        | 13.8\u00b10.01ms                         | 16.9\u00b10ms                                   |    1.22 | bench_ufunc.At.time_sum_at(<class 'numpy.float32'>)                                  |\r\n| +        | 14.0\u00b10.03ms                         | 17.1\u00b10.1ms                                 |    1.22 | bench_ufunc.At.time_sum_at(<class 'numpy.uint32'>)                                   |\r\n| +        | 13.8\u00b10.01ms                         | 16.9\u00b10.02ms                                |    1.22 | bench_ufunc.At.time_sum_at_random(<class 'numpy.float32'>)                           |\r\n| +        | 14.0\u00b10.01ms                         | 17.0\u00b10.02ms                                |    1.21 | bench_ufunc.At.time_maximum_at(<class 'numpy.float32'>)                              |\r\n| +        | 12.5\u00b10.02ms                         | 15.2\u00b10.08ms                                |    1.21 | bench_ufunc.At.time_maximum_at(<class 'numpy.int16'>)                                |\r\n| +        | 14.0\u00b10.1ms                          | 16.9\u00b10.06ms                                |    1.21 | bench_ufunc.At.time_sum_at(<class 'numpy.int32'>)                                    |\r\n| +        | 14.0\u00b10.01ms                         | 16.9\u00b10.01ms                                |    1.21 | bench_ufunc.At.time_sum_at_random(<class 'numpy.int32'>)                             |\r\n| +        | 14.0\u00b10.01ms                         | 16.9\u00b10.06ms                                |    1.21 | bench_ufunc.At.time_sum_at_random(<class 'numpy.uint32'>)                            |\r\n| +        | 15.3\u00b10.02ms                         | 18.2\u00b10.1ms                                 |    1.19 | bench_ufunc.At.time_maximum_at(<class 'numpy.float64'>)                              |\r\n| +        | 14.3\u00b10.01ms                         | 16.9\u00b10.05ms                                |    1.19 | bench_ufunc.At.time_maximum_at(<class 'numpy.int32'>)                                |\r\n| +        | 14.3\u00b10.03ms                         | 16.9\u00b10.08ms                                |    1.19 | bench_ufunc.At.time_maximum_at(<class 'numpy.uint32'>)                               |\r\n| +        | 15.3\u00b10.06ms                         | 18.0\u00b10.02ms                                |    1.18 | bench_ufunc.At.time_sum_at(<class 'numpy.float64'>)                                  |\r\n| +        | 15.3\u00b10.03ms                         | 18.1\u00b10.01ms                                |    1.18 | bench_ufunc.At.time_sum_at_random(<class 'numpy.float64'>)                           |\r\n| +        | 15.4\u00b10.09ms                         | 18.1\u00b10.01ms                                |    1.17 | bench_ufunc.At.time_sum_at(<class 'numpy.int64'>)                                    |\r\n| +        | 15.4\u00b10.07ms                         | 18.0\u00b10.01ms                                |    1.17 | bench_ufunc.At.time_sum_at(<class 'numpy.uint64'>)                                   |\r\n| +        | 15.4\u00b10.02ms                         | 18.1\u00b10.05ms                                |    1.17 | bench_ufunc.At.time_sum_at_random(<class 'numpy.int64'>)                             |\r\n| +        | 15.4\u00b10.01ms                         | 18.1\u00b10.01ms                                |    1.17 | bench_ufunc.At.time_sum_at_random(<class 'numpy.uint64'>)                            |\r\n| +        | 15.6\u00b10.05ms                         | 18.2\u00b10.1ms                                 |    1.16 | bench_ufunc.At.time_maximum_at(<class 'numpy.int64'>)                                |\r\n| +        | 15.6\u00b10.04ms                         | 18.1\u00b10.02ms                                |    1.16 | bench_ufunc.At.time_maximum_at(<class 'numpy.uint64'>)                               |\r\n| +        | 11.3\u00b10.01ms                         | 12.9\u00b10.03ms                                |    1.15 | bench_ufunc.At.time_maximum_at(<class 'numpy.int8'>)                                 |\r\n| +        | 11.3\u00b10.02ms                         | 12.9\u00b10.02ms                                |    1.15 | bench_ufunc.At.time_maximum_at(<class 'numpy.uint8'>)                                |\r\n| +        | 86.0\u00b10.04\u03bcs                         | 97.2\u00b10.4\u03bcs                                 |    1.13 | bench_indexing.Indexing.time_op('object', 'indexes_', 'np.ix_(I, I)', '=1')          |\r\n| +        | 4.56\u00b10.01\u03bcs                         | 5.12\u00b10.2\u03bcs                                 |    1.12 | bench_indexing.Indexing.time_op('int16', 'indexes_', ':,I', '=1')                    |\r\n| +        | 86.6\u00b10.4\u03bcs                          | 97.1\u00b10.6\u03bcs                                 |    1.12 | bench_indexing.Indexing.time_op('object', 'indexes_rand_', 'np.ix_(I, I)', '=1')     |\r\n| +        | 4.62\u00b10.01\u03bcs                         | 5.15\u00b10\u03bcs                                   |    1.11 | bench_indexing.Indexing.time_op('float64', 'indexes_', ':,I', '=1')                  |\r\n| +        | 4.62\u00b10.02\u03bcs                         | 5.11\u00b10\u03bcs                                   |    1.11 | bench_indexing.Indexing.time_op('int64', 'indexes_', ':,I', '=1')                    |\r\n| +        | 4.62\u00b10.01\u03bcs                         | 5.11\u00b10.01\u03bcs                                |    1.11 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', ':,I', '=1')               |\r\n| +        | 4.78\u00b10\u03bcs                            | 5.32\u00b10.01\u03bcs                                |    1.11 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', ':,I', '=1')               |\r\n| +        | 4.56\u00b10.01\u03bcs                         | 5.03\u00b10.01\u03bcs                                |    1.1  | bench_indexing.Indexing.time_op('float16', 'indexes_', ':,I', '=1')                  |\r\n| +        | 4.57\u00b10.01\u03bcs                         | 5.03\u00b10.01\u03bcs                                |    1.1  | bench_indexing.Indexing.time_op('float16', 'indexes_rand_', ':,I', '=1')             |\r\n| +        | 4.63\u00b10.01\u03bcs                         | 5.10\u00b10\u03bcs                                   |    1.1  | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', ':,I', '=1')             |\r\n| +        | 4.80\u00b10.01\u03bcs                         | 5.29\u00b10.01\u03bcs                                |    1.1  | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', ':,I', '=1')          |\r\n| +        | 4.62\u00b10.07\u03bcs                         | 5.02\u00b10.01\u03bcs                                |    1.09 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', ':,I', '=1')               |\r\n| +        | 1.71\u00b10.01\u03bcs                         | 1.84\u00b10.03\u03bcs                                |    1.08 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'complex64')        |\r\n| +        | 1.71\u00b10\u03bcs                            | 1.85\u00b10\u03bcs                                   |    1.08 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'float64')          |\r\n| +        | 1.71\u00b10\u03bcs                            | 1.84\u00b10\u03bcs                                   |    1.08 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'int64')            |\r\n| +        | 1.71\u00b10\u03bcs                            | 1.85\u00b10.02\u03bcs                                |    1.08 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'longdouble')       |\r\n| +        | 1.71\u00b10.02\u03bcs                         | 1.83\u00b10.01\u03bcs                                |    1.07 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'float32')          |\r\n| +        | 1.71\u00b10\u03bcs                            | 1.83\u00b10.01\u03bcs                                |    1.07 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'int32')            |\r\n| +        | 1.71\u00b10.01\u03bcs                         | 1.82\u00b10.01\u03bcs                                |    1.06 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'int16')            |\r\n| +        | 1.71\u00b10.01\u03bcs                         | 1.80\u00b10.01\u03bcs                                |    1.05 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 1), 'float16')          |\r\n| -        | 1.75\u00b10\u03bcs                            | 1.64\u00b10.01\u03bcs                                |    0.94 | bench_indexing.Indexing.time_op('float16', 'indexes_', 'I', '=1')                    |\r\n| -        | 1.74\u00b10\u03bcs                            | 1.63\u00b10.01\u03bcs                                |    0.94 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', 'I', '=1')                 |\r\n| -        | 1.88\u00b10\u03bcs                            | 1.77\u00b10\u03bcs                                   |    0.94 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'complex128')       |\r\n| -        | 1.86\u00b10\u03bcs                            | 1.75\u00b10\u03bcs                                   |    0.94 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'int64')            |\r\n| -        | 2.02\u00b10\u03bcs                            | 1.90\u00b10.01\u03bcs                                |    0.94 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'longdouble')       |\r\n| -        | 1.88\u00b10.01\u03bcs                         | 1.76\u00b10\u03bcs                                   |    0.93 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'complex64')        |\r\n| -        | 1.88\u00b10.01\u03bcs                         | 1.75\u00b10.01\u03bcs                                |    0.93 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'float64')          |\r\n| -        | 1.90\u00b10.03\u03bcs                         | 1.77\u00b10.01\u03bcs                                |    0.93 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'int16')            |\r\n| -        | 5.60\u00b10.01\u03bcs                         | 5.12\u00b10.07\u03bcs                                |    0.92 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', ':,I', '')               |\r\n| -        | 1.79\u00b10.05\u03bcs                         | 1.65\u00b10.03\u03bcs                                |    0.92 | bench_indexing.Indexing.time_op('int16', 'indexes_', 'I', '=1')                      |\r\n| -        | 64.6\u00b10.1ms                          | 59.3\u00b10.06ms                                |    0.92 | bench_ufunc.At2D.time_add_at_sliced                                                  |\r\n| -        | 5.59\u00b10.01\u03bcs                         | 5.06\u00b10.01\u03bcs                                |    0.91 | bench_indexing.Indexing.time_op('float16', 'indexes_', ':,I', '')                    |\r\n| -        | 5.59\u00b10.01\u03bcs                         | 5.07\u00b10.01\u03bcs                                |    0.91 | bench_indexing.Indexing.time_op('float16', 'indexes_rand_', ':,I', '')               |\r\n| -        | 22.2\u00b10.3\u03bcs                          | 20.2\u00b10.2\u03bcs                                 |    0.91 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'i,O')              |\r\n| -        | 5.57\u00b10.01\u03bcs                         | 5.04\u00b10.01\u03bcs                                |    0.9  | bench_indexing.Indexing.time_op('float32', 'indexes_', ':,I', '')                    |\r\n| -        | 5.58\u00b10.01\u03bcs                         | 5.04\u00b10.01\u03bcs                                |    0.9  | bench_indexing.Indexing.time_op('int32', 'indexes_', ':,I', '')                      |\r\n| -        | 5.59\u00b10.01\u03bcs                         | 5.04\u00b10\u03bcs                                   |    0.9  | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', ':,I', '')                 |\r\n| -        | 22.4\u00b10.01\u03bcs                         | 20.2\u00b10.02\u03bcs                                |    0.9  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'i,O')           |\r\n| -        | 2.06\u00b10\u03bcs                            | 1.82\u00b10\u03bcs                                   |    0.89 | bench_indexing.Indexing.time_op('float32', 'indexes_', 'I', '=1')                    |\r\n| -        | 2.07\u00b10.01\u03bcs                         | 1.83\u00b10.02\u03bcs                                |    0.89 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', 'I', '=1')               |\r\n| -        | 5.69\u00b10.1\u03bcs                          | 5.08\u00b10.03\u03bcs                                |    0.89 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', ':,I', '')                 |\r\n| -        | 2.06\u00b10.01\u03bcs                         | 1.83\u00b10.03\u03bcs                                |    0.89 | bench_indexing.Indexing.time_op('int32', 'indexes_', 'I', '=1')                      |\r\n| -        | 26.0\u00b10.2\u03bcs                          | 23.1\u00b10.04\u03bcs                                |    0.89 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'i,O')              |\r\n| -        | 9.14\u00b10.01\u03bcs                         | 8.11\u00b10\u03bcs                                   |    0.89 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'complex128')       |\r\n| -        | 13.1\u00b10.03\u03bcs                         | 11.6\u00b10.1\u03bcs                                 |    0.89 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'i,O')              |\r\n| -        | 6.19\u00b10.1\u03bcs                          | 5.42\u00b10.03\u03bcs                                |    0.88 | bench_indexing.Indexing.time_op('complex128', 'indexes_', ':,I', '')                 |\r\n| -        | 2.06\u00b10\u03bcs                            | 1.82\u00b10\u03bcs                                   |    0.88 | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', 'I', '=1')                 |\r\n| -        | 9.08\u00b10\u03bcs                            | 8.02\u00b10.01\u03bcs                                |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'float16')          |\r\n| -        | 8.79\u00b10\u03bcs                            | 7.71\u00b10.02\u03bcs                                |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'float32')          |\r\n| -        | 9.09\u00b10.01\u03bcs                         | 8.02\u00b10.01\u03bcs                                |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'int16')            |\r\n| -        | 8.78\u00b10.01\u03bcs                         | 7.71\u00b10\u03bcs                                   |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'int32')            |\r\n| -        | 8.17\u00b10.01\u03bcs                         | 7.18\u00b10.09\u03bcs                                |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'longdouble')       |\r\n| -        | 26.4\u00b10.3\u03bcs                          | 23.3\u00b10.03\u03bcs                                |    0.88 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'i,O')           |\r\n| -        | 6.70\u00b10.1\u03bcs                          | 5.81\u00b10.02\u03bcs                                |    0.87 | bench_indexing.Indexing.time_op('complex128', 'indexes_rand_', ':,I', '')            |\r\n| -        | 1.83\u00b10\u03bcs                            | 1.60\u00b10.01\u03bcs                                |    0.87 | bench_indexing.Indexing.time_op('float16', 'indexes_', 'I', '')                      |\r\n| -        | 8.48\u00b10.02\u03bcs                         | 7.40\u00b10.02\u03bcs                                |    0.87 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'float16')          |\r\n| -        | 8.47\u00b10.01\u03bcs                         | 7.39\u00b10.01\u03bcs                                |    0.87 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'int16')            |\r\n| -        | 8.18\u00b10.04\u03bcs                         | 7.09\u00b10.01\u03bcs                                |    0.87 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'complex64')        |\r\n| -        | 8.18\u00b10.01\u03bcs                         | 7.11\u00b10.01\u03bcs                                |    0.87 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'float64')          |\r\n| -        | 8.17\u00b10.02\u03bcs                         | 7.08\u00b10.01\u03bcs                                |    0.87 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 3), 'int64')            |\r\n| -        | 1.84\u00b10\u03bcs                            | 1.58\u00b10\u03bcs                                   |    0.86 | bench_indexing.Indexing.time_op('float16', 'indexes_rand_', 'I', '')                 |\r\n| -        | 6.01\u00b10.01\u03bcs                         | 5.14\u00b10.02\u03bcs                                |    0.86 | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', ':,I', '')               |\r\n| -        | 3.03\u00b10.01\u03bcs                         | 2.60\u00b10.01\u03bcs                                |    0.86 | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', 'I', '=1')               |\r\n| -        | 5.18\u00b10.02\u03bcs                         | 4.44\u00b10\u03bcs                                   |    0.86 | bench_indexing.Indexing.time_op('int32', 'indexes_', ':,I', '=1')                    |\r\n| -        | 5.19\u00b10.02\u03bcs                         | 4.45\u00b10\u03bcs                                   |    0.86 | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', ':,I', '=1')               |\r\n| -        | 3.03\u00b10.01\u03bcs                         | 2.60\u00b10.01\u03bcs                                |    0.86 | bench_indexing.Indexing.time_op('int64', 'indexes_', 'I', '=1')                      |\r\n| -        | 3.21\u00b10.01\u03bcs                         | 2.76\u00b10\u03bcs                                   |    0.86 | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', 'I', '=1')            |\r\n| -        | 8.63\u00b10.1\u03bcs                          | 7.43\u00b10.03\u03bcs                                |    0.86 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'complex128')       |\r\n| -        | 7.87\u00b10.01\u03bcs                         | 6.78\u00b10.01\u03bcs                                |    0.86 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'complex64')        |\r\n| -        | 5.26\u00b10.06\u03bcs                         | 4.47\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('float32', 'indexes_', ':,I', '=1')                  |\r\n| -        | 5.24\u00b10.07\u03bcs                         | 4.46\u00b10.06\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', ':,I', '=1')             |\r\n| -        | 3.05\u00b10.01\u03bcs                         | 2.61\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('float64', 'indexes_', 'I', '=1')                    |\r\n| -        | 5.95\u00b10.06\u03bcs                         | 5.06\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('int16', 'indexes_', ':,I', '')                      |\r\n| -        | 1.90\u00b10.07\u03bcs                         | 1.61\u00b10\u03bcs                                   |    0.85 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', 'I', '')                   |\r\n| -        | 6.03\u00b10.04\u03bcs                         | 5.13\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', ':,I', '')                 |\r\n| -        | 3.03\u00b10.02\u03bcs                         | 2.58\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', 'I', '=1')                 |\r\n| -        | 3.26\u00b10.06\u03bcs                         | 2.78\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', 'I', '=1')                 |\r\n| -        | 6.00\u00b10.02\u03bcs                         | 5.11\u00b10.01\u03bcs                                |    0.85 | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', ':,I', '')            |\r\n| -        | 8.01\u00b10.1\u03bcs                          | 6.77\u00b10.02\u03bcs                                |    0.85 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'int64')            |\r\n| -        | 7.97\u00b10.1\u03bcs                          | 6.77\u00b10.1\u03bcs                                 |    0.85 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'longdouble')       |\r\n| -        | 12.2\u00b10.05\u03bcs                         | 10.4\u00b10.2\u03bcs                                 |    0.85 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'O')             |\r\n| -        | 14.1\u00b10.7\u03bcs                          | 12.0\u00b10.01\u03bcs                                |    0.85 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'complex128')    |\r\n| -        | 2.71\u00b10.02\u03bcs                         | 2.27\u00b10.01\u03bcs                                |    0.84 | bench_indexing.Indexing.time_op('float32', 'indexes_', 'I', '')                      |\r\n| -        | 1.97\u00b10.01\u03bcs                         | 1.65\u00b10.04\u03bcs                                |    0.84 | bench_indexing.Indexing.time_op('int16', 'indexes_', 'I', '')                        |\r\n| -        | 2.72\u00b10.01\u03bcs                         | 2.27\u00b10.01\u03bcs                                |    0.84 | bench_indexing.Indexing.time_op('int32', 'indexes_', 'I', '')                        |\r\n| -        | 12.1\u00b10.04\u03bcs                         | 10.2\u00b10.2\u03bcs                                 |    0.84 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'O')                |\r\n| -        | 8.06\u00b10.1\u03bcs                          | 6.78\u00b10.01\u03bcs                                |    0.84 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'float64')          |\r\n| -        | 6.93\u00b10.01\u03bcs                         | 5.84\u00b10.01\u03bcs                                |    0.84 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'int32')            |\r\n| -        | 2.70\u00b10.01\u03bcs                         | 2.24\u00b10.03\u03bcs                                |    0.83 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', 'I', '')                 |\r\n| -        | 6.19\u00b10.01\u03bcs                         | 5.11\u00b10.02\u03bcs                                |    0.83 | bench_indexing.Indexing.time_op('float64', 'indexes_', ':,I', '')                    |\r\n| -        | 6.17\u00b10.01\u03bcs                         | 5.11\u00b10.01\u03bcs                                |    0.83 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', ':,I', '')                 |\r\n| -        | 7.03\u00b10.1\u03bcs                          | 5.87\u00b10.07\u03bcs                                |    0.83 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((1000, 2), 'float32')          |\r\n| -        | 7.99\u00b10\u03bcs                            | 6.61\u00b10.06\u03bcs                                |    0.83 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'float64')          |\r\n| -        | 2.71\u00b10.01\u03bcs                         | 2.21\u00b10.02\u03bcs                                |    0.82 | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', 'I', '')                   |\r\n| -        | 6.19\u00b10.01\u03bcs                         | 5.10\u00b10.01\u03bcs                                |    0.82 | bench_indexing.Indexing.time_op('int64', 'indexes_', ':,I', '')                      |\r\n| -        | 7.68\u00b10.01\u03bcs                         | 6.27\u00b10.04\u03bcs                                |    0.82 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'complex128')       |\r\n| -        | 8.13\u00b10.01\u03bcs                         | 6.64\u00b10\u03bcs                                   |    0.82 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'longdouble')       |\r\n| -        | 60.2\u00b10.1\u03bcs                          | 48.7\u00b10.5\u03bcs                                 |    0.81 | bench_indexing.Indexing.time_op('object', 'indexes_', 'np.ix_(I, I)', '')            |\r\n| -        | 7.99\u00b10\u03bcs                            | 6.51\u00b10.01\u03bcs                                |    0.81 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'float16')          |\r\n| -        | 7.99\u00b10.01\u03bcs                         | 6.50\u00b10.01\u03bcs                                |    0.81 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'float32')          |\r\n| -        | 7.98\u00b10\u03bcs                            | 6.50\u00b10\u03bcs                                   |    0.81 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'int16')            |\r\n| -        | 7.99\u00b10.04\u03bcs                         | 6.49\u00b10.01\u03bcs                                |    0.81 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'int32')            |\r\n| -        | 7.98\u00b10.01\u03bcs                         | 6.49\u00b10.01\u03bcs                                |    0.81 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'int64')            |\r\n| -        | 5.63\u00b10.09\u03bcs                         | 4.52\u00b10.01\u03bcs                                |    0.8  | bench_indexing.Indexing.time_op('complex64', 'indexes_rand_', ':,I', '')             |\r\n| -        | 7.71\u00b10.02\u03bcs                         | 6.20\u00b10.01\u03bcs                                |    0.8  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 3), 'complex64')        |\r\n| -        | 5.70\u00b10.02\u03bcs                         | 4.53\u00b10.01\u03bcs                                |    0.79 | bench_indexing.Indexing.time_op('complex64', 'indexes_', ':,I', '')                  |\r\n| -        | 32.8\u00b10.7\u03bcs                          | 24.8\u00b10.4\u03bcs                                 |    0.76 | bench_indexing.Indexing.time_op('float16', 'indexes_', 'np.ix_(I, I)', '')           |\r\n| -        | 32.3\u00b10.03\u03bcs                         | 24.6\u00b10.3\u03bcs                                 |    0.76 | bench_indexing.Indexing.time_op('float16', 'indexes_rand_', 'np.ix_(I, I)', '')      |\r\n| -        | 33.1\u00b10.8\u03bcs                          | 25.2\u00b10.8\u03bcs                                 |    0.76 | bench_indexing.Indexing.time_op('int16', 'indexes_', 'np.ix_(I, I)', '')             |\r\n| -        | 32.8\u00b10.05\u03bcs                         | 24.9\u00b10.3\u03bcs                                 |    0.76 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', 'np.ix_(I, I)', '')        |\r\n| -        | 65.0\u00b10.06\u03bcs                         | 49.7\u00b10.4\u03bcs                                 |    0.76 | bench_indexing.Indexing.time_op('object', 'indexes_rand_', 'np.ix_(I, I)', '')       |\r\n| -        | 5.92\u00b10.03\u03bcs                         | 4.49\u00b10.05\u03bcs                                |    0.76 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'complex128')    |\r\n| -        | 5.90\u00b10.03\u03bcs                         | 4.46\u00b10.04\u03bcs                                |    0.76 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'complex64')     |\r\n| -        | 5.89\u00b10.04\u03bcs                         | 4.47\u00b10.05\u03bcs                                |    0.76 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'int16')         |\r\n| -        | 32.8\u00b10.02\u03bcs                         | 24.5\u00b10.3\u03bcs                                 |    0.75 | bench_indexing.Indexing.time_op('complex64', 'indexes_', 'np.ix_(I, I)', '')         |\r\n| -        | 32.8\u00b10.3\u03bcs                          | 24.5\u00b10.2\u03bcs                                 |    0.75 | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', 'np.ix_(I, I)', '')        |\r\n| -        | 5.89\u00b10.04\u03bcs                         | 4.41\u00b10\u03bcs                                   |    0.75 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'float16')       |\r\n| -        | 5.93\u00b10.01\u03bcs                         | 4.47\u00b10.03\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'float32')       |\r\n| -        | 5.91\u00b10.03\u03bcs                         | 4.43\u00b10.01\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'float64')       |\r\n| -        | 5.91\u00b10.04\u03bcs                         | 4.42\u00b10.01\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'int32')         |\r\n| -        | 5.91\u00b10.03\u03bcs                         | 4.43\u00b10.01\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'longdouble')    |\r\n| -        | 6.69\u00b10.04\u03bcs                         | 5.01\u00b10.05\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'float16')          |\r\n| -        | 6.70\u00b10.04\u03bcs                         | 5.01\u00b10.05\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'float64')          |\r\n| -        | 6.86\u00b10.05\u03bcs                         | 5.16\u00b10.05\u03bcs                                |    0.75 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'longdouble')       |\r\n| -        | 6.28\u00b10.02\u03bcs                         | 4.62\u00b10.04\u03bcs                                |    0.74 | bench_indexing.Indexing.time_op('complex128', 'indexes_', 'I', '')                   |\r\n| -        | 34.3\u00b10.2\u03bcs                          | 25.4\u00b10.4\u03bcs                                 |    0.74 | bench_indexing.Indexing.time_op('complex128', 'indexes_rand_', 'np.ix_(I, I)', '')   |\r\n| -        | 33.4\u00b10.2\u03bcs                          | 24.6\u00b10.3\u03bcs                                 |    0.74 | bench_indexing.Indexing.time_op('int32', 'indexes_', 'np.ix_(I, I)', '')             |\r\n| -        | 5.97\u00b10.04\u03bcs                         | 4.42\u00b10.01\u03bcs                                |    0.74 | bench_indexing.IndexingWith1DArr.time_getitem_ordered((2, 1000, 1), 'int64')         |\r\n| -        | 6.75\u00b10.04\u03bcs                         | 4.99\u00b10.05\u03bcs                                |    0.74 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'float32')          |\r\n| -        | 6.74\u00b10.04\u03bcs                         | 4.96\u00b10.01\u03bcs                                |    0.74 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'int16')            |\r\n| -        | 6.70\u00b10.04\u03bcs                         | 4.95\u00b10\u03bcs                                   |    0.74 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'int32')            |\r\n| -        | 6.74\u00b10.1\u03bcs                          | 4.99\u00b10.04\u03bcs                                |    0.74 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'int64')            |\r\n| -        | 6.28\u00b10.07\u03bcs                         | 4.57\u00b10.01\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('complex128', 'indexes_rand_', 'I', '')              |\r\n| -        | 33.0\u00b10.2\u03bcs                          | 24.2\u00b10.05\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('float32', 'indexes_', 'np.ix_(I, I)', '')           |\r\n| -        | 32.7\u00b10.3\u03bcs                          | 24.0\u00b10.03\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', 'np.ix_(I, I)', '')      |\r\n| -        | 33.4\u00b10.6\u03bcs                          | 24.3\u00b10.02\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('float64', 'indexes_', 'np.ix_(I, I)', '')           |\r\n| -        | 32.9\u00b10.3\u03bcs                          | 24.0\u00b10.01\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', 'np.ix_(I, I)', '')      |\r\n| -        | 33.1\u00b11\u03bcs                            | 24.1\u00b10.03\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', 'np.ix_(I, I)', '')        |\r\n| -        | 33.4\u00b10.6\u03bcs                          | 24.4\u00b10.02\u03bcs                                |    0.73 | bench_indexing.Indexing.time_op('int64', 'indexes_', 'np.ix_(I, I)', '')             |\r\n| -        | 33.4\u00b10.5\u03bcs                          | 24.3\u00b10.3\u03bcs                                 |    0.73 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', 'np.ix_(I, I)', '')        |\r\n| -        | 34.3\u00b10.1\u03bcs                          | 24.8\u00b10.3\u03bcs                                 |    0.72 | bench_indexing.Indexing.time_op('complex128', 'indexes_', 'np.ix_(I, I)', '')        |\r\n| -        | 33.4\u00b10.5\u03bcs                          | 24.2\u00b10.06\u03bcs                                |    0.72 | bench_indexing.Indexing.time_op('complex64', 'indexes_rand_', 'np.ix_(I, I)', '')    |\r\n| -        | 33.4\u00b10.6\u03bcs                          | 24.0\u00b10.06\u03bcs                                |    0.72 | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', 'np.ix_(I, I)', '')   |\r\n| -        | 14.1\u00b12\u03bcs                            | 10.1\u00b10.03\u03bcs                                |    0.72 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'complex64')     |\r\n| -        | 6.90\u00b10.06\u03bcs                         | 4.89\u00b10.03\u03bcs                                |    0.71 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'float16')       |\r\n| -        | 6.94\u00b10.06\u03bcs                         | 4.92\u00b10.02\u03bcs                                |    0.71 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'float64')       |\r\n| -        | 6.92\u00b10.08\u03bcs                         | 4.88\u00b10.03\u03bcs                                |    0.71 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'int16')         |\r\n| -        | 7.14\u00b10.06\u03bcs                         | 5.05\u00b10.04\u03bcs                                |    0.71 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'longdouble')    |\r\n| -        | 6.99\u00b10.04\u03bcs                         | 4.86\u00b10.04\u03bcs                                |    0.7  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'complex128')       |\r\n| -        | 6.94\u00b10.06\u03bcs                         | 4.87\u00b10.05\u03bcs                                |    0.7  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'float32')       |\r\n| -        | 6.93\u00b10.06\u03bcs                         | 4.88\u00b10.04\u03bcs                                |    0.7  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'int32')         |\r\n| -        | 6.93\u00b10.07\u03bcs                         | 4.87\u00b10.04\u03bcs                                |    0.7  | bench_indexing.IndexingWith1DArr.time_setitem_ordered((2, 1000, 1), 'int64')         |\r\n| -        | 7.04\u00b10.07\u03bcs                         | 4.87\u00b10.04\u03bcs                                |    0.69 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 2), 'complex64')        |\r\n| -        | 37.4\u00b10.1\u03bcs                          | 25.1\u00b10.03\u03bcs                                |    0.67 | bench_indexing.Indexing.time_op('complex64', 'indexes_', 'np.ix_(I, I)', '=1')       |\r\n| -        | 38.9\u00b10.03\u03bcs                         | 25.6\u00b10.02\u03bcs                                |    0.66 | bench_indexing.Indexing.time_op('complex128', 'indexes_rand_', 'np.ix_(I, I)', '=1') |\r\n| -        | 4.08\u00b10.01\u03bcs                         | 2.70\u00b10.01\u03bcs                                |    0.66 | bench_indexing.Indexing.time_op('complex64', 'indexes_', 'I', '')                    |\r\n| -        | 4.08\u00b10.02\u03bcs                         | 2.68\u00b10.03\u03bcs                                |    0.66 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', 'I', '')                   |\r\n| -        | 35.3\u00b10.02\u03bcs                         | 23.3\u00b10.2\u03bcs                                 |    0.66 | bench_indexing.Indexing.time_op('int64', 'indexes_rand_', 'np.ix_(I, I)', '=1')      |\r\n| -        | 4.09\u00b10.01\u03bcs                         | 2.69\u00b10.01\u03bcs                                |    0.66 | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', 'I', '')              |\r\n| -        | 4.09\u00b10.02\u03bcs                         | 2.64\u00b10.01\u03bcs                                |    0.65 | bench_indexing.Indexing.time_op('complex64', 'indexes_rand_', 'I', '')               |\r\n| -        | 4.17\u00b10.07\u03bcs                         | 2.70\u00b10.04\u03bcs                                |    0.65 | bench_indexing.Indexing.time_op('float64', 'indexes_', 'I', '')                      |\r\n| -        | 4.07\u00b10.01\u03bcs                         | 2.64\u00b10.01\u03bcs                                |    0.65 | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', 'I', '')                 |\r\n| -        | 4.14\u00b10.07\u03bcs                         | 2.70\u00b10.02\u03bcs                                |    0.65 | bench_indexing.Indexing.time_op('int64', 'indexes_', 'I', '')                        |\r\n| -        | 4.15\u00b10.06\u03bcs                         | 2.70\u00b10.01\u03bcs                                |    0.65 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', 'I', '')                   |\r\n| -        | 39.9\u00b10.07\u03bcs                         | 25.5\u00b10.1\u03bcs                                 |    0.64 | bench_indexing.Indexing.time_op('complex128', 'indexes_', 'np.ix_(I, I)', '=1')      |\r\n| -        | 37.8\u00b10.6\u03bcs                          | 24.4\u00b10.1\u03bcs                                 |    0.64 | bench_indexing.Indexing.time_op('complex64', 'indexes_rand_', 'np.ix_(I, I)', '=1')  |\r\n| -        | 34.0\u00b10.02\u03bcs                         | 21.6\u00b10.4\u03bcs                                 |    0.64 | bench_indexing.Indexing.time_op('float16', 'indexes_rand_', 'np.ix_(I, I)', '=1')    |\r\n| -        | 35.3\u00b10.01\u03bcs                         | 22.6\u00b10.02\u03bcs                                |    0.64 | bench_indexing.Indexing.time_op('float64', 'indexes_rand_', 'np.ix_(I, I)', '=1')    |\r\n| -        | 36.1\u00b10.5\u03bcs                          | 23.0\u00b10.4\u03bcs                                 |    0.64 | bench_indexing.Indexing.time_op('longdouble', 'indexes_', 'np.ix_(I, I)', '=1')      |\r\n| -        | 35.5\u00b10.03\u03bcs                         | 22.9\u00b10.1\u03bcs                                 |    0.64 | bench_indexing.Indexing.time_op('longdouble', 'indexes_rand_', 'np.ix_(I, I)', '=1') |\r\n| -        | 34.7\u00b10.1\u03bcs                          | 22.0\u00b10.2\u03bcs                                 |    0.63 | bench_indexing.Indexing.time_op('float32', 'indexes_', 'np.ix_(I, I)', '=1')         |\r\n| -        | 35.3\u00b10.03\u03bcs                         | 22.4\u00b10.05\u03bcs                                |    0.63 | bench_indexing.Indexing.time_op('float64', 'indexes_', 'np.ix_(I, I)', '=1')         |\r\n| -        | 34.1\u00b10.06\u03bcs                         | 21.6\u00b10.4\u03bcs                                 |    0.63 | bench_indexing.Indexing.time_op('int16', 'indexes_rand_', 'np.ix_(I, I)', '=1')      |\r\n| -        | 35.3\u00b10.02\u03bcs                         | 22.3\u00b10.09\u03bcs                                |    0.63 | bench_indexing.Indexing.time_op('int64', 'indexes_', 'np.ix_(I, I)', '=1')           |\r\n| -        | 34.0\u00b10.03\u03bcs                         | 21.2\u00b10.04\u03bcs                                |    0.62 | bench_indexing.Indexing.time_op('float16', 'indexes_', 'np.ix_(I, I)', '=1')         |\r\n| -        | 34.5\u00b10.03\u03bcs                         | 21.5\u00b10.01\u03bcs                                |    0.62 | bench_indexing.Indexing.time_op('float32', 'indexes_rand_', 'np.ix_(I, I)', '=1')    |\r\n| -        | 34.3\u00b11\u03bcs                            | 21.2\u00b10.07\u03bcs                                |    0.62 | bench_indexing.Indexing.time_op('int16', 'indexes_', 'np.ix_(I, I)', '=1')           |\r\n| -        | 34.5\u00b10.3\u03bcs                          | 21.3\u00b10.1\u03bcs                                 |    0.62 | bench_indexing.Indexing.time_op('int32', 'indexes_', 'np.ix_(I, I)', '=1')           |\r\n| -        | 34.7\u00b10.4\u03bcs                          | 21.6\u00b10.04\u03bcs                                |    0.62 | bench_indexing.Indexing.time_op('int32', 'indexes_rand_', 'np.ix_(I, I)', '=1')      |\r\n| -        | 9.38\u00b10.3\u03bcs                          | 5.49\u00b10.01\u03bcs                                |    0.59 | bench_indexing.IndexingWith1DArr.time_setitem_ordered((1000, 1), 'O')                |\r\n| -        | 97.7\u00b10.08\u03bcs                         | 24.4\u00b10.04\u03bcs                                |    0.25 | bench_ufunc.At2D.time_add_at_2d                                                      |","comments":[],"labels":["03 - Maintenance"]},{"title":"Cirrus CI free usage going away - job runtime issues & credits","body":"This is getting kinda annoying: `[skip cirrus]` is broken and the wheel build and other CI jobs are running way too often. It was probably broken by the addition of other logic like always triggering on PRs with build-related label.","comments":["What do you want to happen with the build label? I use it when I want to test wheel builds. It is a bit annoying, as simply adding a label will retrigger the builds.","Labels are for indicating what a PR or issue is about, so I add it to build-related PRs. And running wheel builds by default is fine, but the `[skip cirrus]` should supercede that (just like it does for `[skip actions]` on GHA).","> but the [skip cirrus] should supercede\r\n\r\nFair enough, but some labels do trigger wheel builds, so they aren't only about what the PR or issue is about. Not sure how we could change that unless we want to add a `[build wheels]` option.","Why `[skip cirrus]` doesn't work is explained by https:\/\/github.com\/numpy\/numpy\/blob\/7fc72776b972bfbfdb909e4b15feb0308cf8adba\/.cirrus.star#L23-L25\r\n\r\nIt looks like it's not the build label though, we run wheel builds on Cirrus always. E.g. look at this doc-only PR with no labels: gh-24277. This wastes a huge amount of compute time.","Cirrus is a bit different. One problem is that it cannot be manually run, I think it needs something in the *.yml file for that. I also think (only) wheels are built and tested, I'm not sure how a build can be made without that. It is on my problem list, but not annoying enough to spend time on it. Maybe @andyfaff has some thoughts.",".cirrus.star is triggered to run with a lot of GH events on the numpy\/numpy repo, e.g. if there are commits to PRs, commits to branches, PRs are opened, labels are attached to PRs, tags are pushed, merges, etc.\r\n\r\nHere is the current logic for numpy's cirrus CI, as contained in .cirrus.star:\r\n\r\n- only run jobs only on the `numpy\/numpy` repo.\r\n- (do a wheels build if a 'nightly' cron job is requested)\r\n- for all other triggers (and there can be a lot of them) the configuration script looks at the commit message of the SHA, provided by CIRRUS_CHANGE_IN_REPO, for the event. If there is `[skip cirrus]` or `[skip ci]` in the message then don't run any CI.\r\n- otherwise do the wheel build and the macosx_arm64 runs.\r\n\r\nThis logic will dictate that the wheels always get built if there is no `[skip cirrus]` in the commit message.\r\n\r\n>  [skip cirrus] is broken and the wheel build and other CI jobs are running way too often. \r\n\r\nI'm pretty sure [skip cirrus] works for commits to PRs. But for other events if the commit message for the SHA doesn't contain those words then the wheel build and macosx_arm64 will run. For example adding labels to a PR will trigger these runs if the last commit to the PR doesn't contain the magic words.\r\n\r\nIt's not clear to me from previous comments what additional logic is being requested. The following environment variables may be useful in reducing the number of runs made:\r\n\r\n- `CIRRUS_PR | PR number if current build was triggered by a PR` - we may be able to query Github as to what occurred in the PR to trigger cirrus-ci. We already query GH in cirrus.star.\r\n- `CIRRUS_PR_LABELS | comma separated list of PR's labels if current build was triggered by a PR` - examine the labels to see if `36 - build` is present. e.g. if you look in the debugging info for https:\/\/cirrus-ci.com\/build\/4758553810960384 you'll see that label present.\r\n- `CIRRUS_TAG | Tag name if current build was triggered by a new tag`.\r\n\r\nPossible extra logic that could be done:\r\n\r\n- don't automatically do wheel build unless ...\r\n- it's a nightly job\r\n- scan SHA message for `[wheel build]`, do wheel build if present\r\n- scan `CIRRUS_PR_LABELS` for `36 - Build`, do wheel build if present\r\n- build wheels for a tag event\r\n\r\n#### Relevant links:\r\n[CIRRUS_CHANGE_IN_REPO](https:\/\/cirrus-ci.org\/guide\/writing-tasks\/#environment-variables)\r\n[numpy's .cirrus.star config](https:\/\/github.com\/numpy\/numpy\/blob\/main\/.cirrus.star)\r\n[numpy's wheel build config](https:\/\/github.com\/numpy\/numpy\/blob\/main\/tools\/ci\/cirrus_wheels.yml)\r\n[numpy's macosx_arm64 config](https:\/\/github.com\/numpy\/numpy\/blob\/main\/tools\/ci\/cirrus_macosx_arm64.yml)\r\n[cirrus environment variables](https:\/\/cirrus-ci.org\/guide\/writing-tasks\/#environment-variables)\r\n\r\nEDIT:\r\n[skip cirrus] wasn't working, fixed in #24285.","Having just said that I see an issue at https:\/\/github.com\/numpy\/numpy\/blob\/main\/.cirrus.star#L27. I'll open a PR.","You can examine what we requested from the GH API in 24282. The github request is:\r\n`https:\/\/api.github.com\/repos\/numpy\/numpy\/git\/commits\/cad8595a8c86c173285d82b61f6797ff24324364`.\r\n\r\nThis is what is returned:\r\n\r\n```\r\n{\r\n  \"sha\": \"cad8595a8c86c173285d82b61f6797ff24324364\",\r\n  \"node_id\": \"C_kwDOAA3dP9oAKGNhZDg1OTVhOGM4NmMxNzMyODVkODJiNjFmNjc5N2ZmMjQzMjQzNjQ\",\r\n  \"url\": \"https:\/\/api.github.com\/repos\/numpy\/numpy\/git\/commits\/cad8595a8c86c173285d82b61f6797ff24324364\",\r\n  \"html_url\": \"https:\/\/github.com\/numpy\/numpy\/commit\/cad8595a8c86c173285d82b61f6797ff24324364\",\r\n  \"author\": {\r\n    \"name\": \"Andrew Nelson\",\r\n    \"email\": \"andyfaff@gmail.com\",\r\n    \"date\": \"2023-07-29T00:15:13Z\"\r\n  },\r\n  \"committer\": {\r\n    \"name\": \"Andrew Nelson\",\r\n    \"email\": \"andyfaff@gmail.com\",\r\n    \"date\": \"2023-07-29T00:15:13Z\"\r\n  },\r\n  \"tree\": {\r\n    \"sha\": \"1c9f90fbbcff3162542b6663e0fe75a86e819bb4\",\r\n    \"url\": \"https:\/\/api.github.com\/repos\/numpy\/numpy\/git\/trees\/1c9f90fbbcff3162542b6663e0fe75a86e819bb4\"\r\n  },\r\n  \"message\": \"CI: correct URL in cirrus.star [skip cirrus]\",\r\n  \"parents\": [\r\n    {\r\n      \"sha\": \"422854fa8dc501e5fcbd713093fdee04e7e9ebb8\",\r\n      \"url\": \"https:\/\/api.github.com\/repos\/numpy\/numpy\/git\/commits\/422854fa8dc501e5fcbd713093fdee04e7e9ebb8\",\r\n      \"html_url\": \"https:\/\/github.com\/numpy\/numpy\/commit\/422854fa8dc501e5fcbd713093fdee04e7e9ebb8\"\r\n    }\r\n  ],\r\n  \"verification\": {\r\n    \"verified\": false,\r\n    \"reason\": \"unsigned\",\r\n    \"signature\": null,\r\n    \"payload\": null\r\n  }\r\n}\r\n```\r\n\r\nThe cirrus CI didn't run because [skip cirrus] is in `dct['message']`.","I think we should be able to limit the wheel build to whatever is in the GHA wheels build, if that's desired.\r\n\r\nEDIT: apart from manual trigger, not sure how to do that.","Thanks for the fix @andyfaff! \r\n\r\nGiven the major reduction in free resources available per 1 Sep (see https:\/\/cirrus-ci.org\/blog\/2023\/07\/17\/limiting-free-usage-of-cirrus-ci\/), I think we have a lot more work to do here unfortunately (and may consider buying some credits).\r\n\r\nRegarding the label-based trigger, I think there are two things wrong with it:\r\n- the `Build` label is wrong for this, it should be a dedicated and clearly named label like `trigger-cirrus`\r\n- it's a problem that this label, once added to a PR, tends to stay on it and then the wheel builds run for every subsequent push. typically what's intended is a one-off \"check wheel builds\". It doesn't really make sense (resource-usage wise) to run the full battery of wheel builds on every push to a PR.\r\n\r\nGiven the above and that our resource usage at the current rate (see screenshot below) is completely unsustainable and would run at ~$2,500\/month (or ~$1,400 after the upcoming price reductions also announced in the blog post linked above) if we'd have to pay for it from 1 Sep, I'd much prefer to get rid of label-based triggering completely. Manual wheel build triggers should be rare and reserved to maintainers who know what they are doing and are able to push an empty commit with the correct commands in the commit message.\r\n\r\n<img width=\"1143\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/b4cf538f-c17b-43b8-8fea-3c0257ce360a\">\r\n\r\nCPU usage is also bad on `cibuildwheel` jobs (and note that credits go per CPU-minute, i.e. per core rather than per job); we need to ensure to use 2 cores for pytest:\r\n\r\n<img width=\"567\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/13c64cc6-4aee-4b1c-8077-bc5c1c884e07\">\r\n\r\nI'll note that on jobs with 2 CPUs, using `-n auto` also isn't great, since pytest-xdist translates that to `-n4` rather than `-n2` and its scaling of parallelism is terrible and performance improvement tends to get negative from 4 jobs already. `-n2` on 2-core jobs is already far from linear, 3 is getting questionable but still gains typically, >=4 quickly decreases performance. \r\n\r\n[Example log](https:\/\/cirrus-ci.com\/task\/5755150707458048) from a recent `macos_arm64_test` run showing we get 4 pytest-xdist workers:\r\n```\r\n$ \/Users\/admin\/numpy-dev\/bin\/python3.10 -m pytest --rootdir=\/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cirrus-ci-build\/build-install\/usr\/lib\/python3.10\/site-packages -n auto -m 'not slow' numpy\r\n============================= test session starts ==============================\r\nplatform darwin -- Python 3.10.6, pytest-7.4.0, pluggy-1.2.0\r\nrootdir: \/private\/var\/folders\/76\/zy5ktkns50v6gt5g8r0sf6sc0000gn\/T\/cirrus-ci-build\/build-install\/usr\/lib\/python3.10\/site-packages\r\nconfigfile: ..\/..\/..\/..\/..\/pytest.ini\r\nplugins: hypothesis-6.82.0, xdist-3.3.1\r\ncreated: 4\/4 workers\r\n4 workers [34198 items]\r\n```\r\n\r\nHere is the full list of jobs and runtimes for a single run:\r\n\r\n<img width=\"680\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/9a13a4d0-5d81-4680-beb6-3dd5ce3a1c7b\">\r\n\r\nThat's a total of ~222 CPU minutes for wheel builds per run, divided in\r\n- 77.5 * 2 = 144 min on Linux aarch64 = $0.145 per run\r\n- 33.5 * 4 = 134 min on macOS = $0.67 per run\r\n\r\nSo each wheel build costs about $0.80 each time it's triggered - this is a lot. We also have issues with some tests in the full test suite that need fixing (e.g., the slow typing tests shouldn't be run by default, they're the same on all platforms and take well over a minute). But most importantly, we should not be triggering wheel builds so much, they're only very rarely useful.\r\n","As you can see in gh-24289, that PR - which only tweaked a code comment in a `meson.build` file and I added the `04 - Documentation` label to - still triggers a full set of wheel builds on Cirrus (I cancelled them manually after they started running).","And then after a merge to main, it's running yet again: https:\/\/cirrus-ci.com\/build\/5871472732798976.","This is the relevant code in `.cirrus.star`:\r\n\r\n```starlark\r\n    # Obtain commit message for the event. Unfortunately CIRRUS_CHANGE_MESSAGE\r\n    # only contains the actual commit message on a non-PR trigger event.\r\n    # For a PR event it contains the PR title and description.\r\n    SHA = env.get(\"CIRRUS_CHANGE_IN_REPO\")\r\n    url = \"https:\/\/api.github.com\/repos\/numpy\/numpy\/git\/commits\/\" + SHA\r\n    dct = http.get(url).json()\r\n    # if \"[wheel build]\" in dct[\"message\"]:\r\n    #     return fs.read(\"ci\/cirrus_wheels.yml\")\r\n\r\n    if \"[skip cirrus]\" in dct[\"message\"] or \"[skip ci]\" in dct[\"message\"]:\r\n        return []\r\n\r\n    # add extra jobs to the cirrus run by += adding to config\r\n    config = fs.read(\"tools\/ci\/cirrus_wheels.yml\")\r\n    config += fs.read(\"tools\/ci\/cirrus_macosx_arm64.yml\")\r\n\r\n    return config\r\n```\r\n\r\nI don't see any label-based triggers, also not in `tools\/ci\/cirrus_*`. I think what we need here is to (a) uncomment the lines with `[wheel build]`, and (b) delete the line `config += fs.read(\"tools\/ci\/cirrus_wheels.yml\")`.","@rgommers, see #24286","I'll try to fix some of the test suite invocation and runtime issues. EDIT: see gh-24291","I'm currently experimenting with ccache for scipy builds (which use meson). Would the numpy macosx_arm64 benefit from this?","I think so - not by much though, given that the whole build is less than a minute and ~10 seconds of that is the configure stage. So if ccache helps by a factor of ~2x, it may save 20 sec or so. ","Is there a way to tie the cirrus CI builds into the successful run of the smoke test from github actions?","@mattip , I'm not sure. It might be possible to have manual triggering if desired, https:\/\/cirrus-ci.org\/guide\/writing-tasks\/#manual-tasks","I think this can probably be closed now","The skip\/run logic is fixed (thanks!), but  gh-24291 still needs finishing and then we need to deal with Cirrus CI credits. So let me re-title this issue rather than close it.","Current state after 12 days in August - this is looking pretty good, ~3x over the free limit:\r\n\r\n<img width=\"1022\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/633f0295-99c2-48d1-9eaf-64a01b8c7367\">\r\n\r\nWe haven't done many wheel builds though in August, and we do need those soon for the 1.26.x releases. Finishing up gh-24291 should be useful there. And then we'll probably end up with a O($150\/month) bill that we can figure out if we're happy with and if so, the logistics of paying it.","We're 19 credits away from an outage, so at this rate another 5 days or so. I'll have a look at buying some credits or wiring up a credit card tomorrow.","Cirrus upped the free credits from 40 to 50, and we're at 41 now - so no problems so far. I've bought a bunch of credits and opened gh-24695 to enable using them.","After 1.5 days of usage we used 1.02 credits. It actually quite nice that you get to see how much each run costs:\r\n\r\n<img width=\"956\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/f4540093-4762-449b-b2ee-6eabe35beaf3\">\r\n\r\nmacOS arm64 is a little expensive \ud83e\udd14. This is what the docs say right now:\r\n\r\n- _1000 minutes of 1 virtual CPU for Linux platform for 3 compute credits_\r\n- _1000 minutes of 1 virtual CPU for FreeBSD platform for 3 compute credits_\r\n- _1000 minutes of 1 virtual CPU for Windows platform for 4 compute credits_\r\n- _1000 minutes of 1 Apple Silicon CPU for 15 compute credits_\r\n\r\nI had interpreted the Apple Silicon CPU as being the whole CPU, not a CPU core. Since you can only get an instance with 4 cores, I thought pricing would end up similar to that for Windows - but it's 4x more.\r\n\r\nThese jobs were all from maintainers; the external contributor PRs continue to run jobs but don't consume credits. I guess we'll have to see what happens when Cirrus starts to enforce the free credit limit.\r\n\r\nSo right now the $1\/day consumption isn't too worrying, but if consumption goes we may have to think about redoing how we trigger the macOS job.","For the record, the NumPy Steering Council signed off on my proposal to spend credits - up to max $200\/month. \r\n\r\nMy goal would be to stay below $100\/month, and that seems to be feasible. And the invoicing and consumption reporting seems reasonably smooth, so all good so far.","There are a couple more things to try, manual triggering of the Mac run, or a Cron job e.g. every couple of days.","We used $45 in the first 28 days. I just added $99, so we're good for quite a while now. The consumption is close to what I estimated before.","We're at $0 now. There's a couple of issues:\r\n- Credits consumption went too fast. We did not use 99 credits in the last 35 days, more like 40. \r\n- SciPy has the opposite issue, zero credits were subtracted in the last 6 weeks. Since I paid both with the same credit card, it looks like the SciPy credit usage got subtracted from the NumPy funds \ud83e\udd14.\r\n- I cannot add more credits right now, somehow Cirrus is not liking my credit card anymore.\r\n\r\nI have to follow up with them, but in the meantime CI jobs may stop running. ","I worst comes to worst, I also have a credit card :wink:","I've added $98 today (20 Dec '23), they're accepting my credit card again. Don't know what happened, assuming some validation issue. I still need to follow up with Cirrus CI about issues with mixing NumPy\/SciPy credits and getting better invoices.","Hmm, with all the recent changes to wheel builds we're now up to about $2.75 per run, that's a lot more than higher up. \r\n\r\n<img width=\"613\" alt=\"image\" src=\"https:\/\/github.com\/numpy\/numpy\/assets\/98330\/16e5babd-f9e5-42fe-9ce0-38947d6c45e7\">\r\n\r\nReasons:\r\n- We added `musllinux_aarch64`, doubling the duration of the Linux jobs\r\n- We added double the number of macOS jobs, for Accelerate wheels\r\n- The test suite has gotten slower by a significant amount\r\n\r\nOnly the last reason is fixable.","In anticipation of lots of wheel builds in the run-up to 2.0.0rc1 I added $121 more credits - we're at 164 credits as of today (7 Jan 2024). Should be enough until sometime in Feb. Next up: testing the reimbursement process.","Bought another $97 in credits (note: slightly different amount each time is to ensure the invoices are easier to tell apart). With a number of the macOS arm64 jobs now migrated to GHA, this should hopefully last us a little while.","Just a note on support quality: I noticed that a single job ran for ~17 minutes longer yesterday, and hence consumed more credits. I emailed Cirrus support; they were already on it, and fixed the issue with a comprehensive explanation within an hour or so, with credits returned and all previous jobs in the last month also audited. Impressive.","@rgommers This is an example that supports the argument some economists make for leaving spending decisions to the individual :)"],"labels":["component: CI"]},{"title":"asv fails to run on windows when pkg-config and OpenBLAS are set up properly","body":"running either \r\n```\r\npython runtests.py --bench-compare <commit1> <commit2>\r\n```\r\n\r\nor \r\n```\r\nspin bench --compare <commit1> <commit2>\r\n```\r\n\r\nwill end up calling \r\n```\r\ncd benchmark; asv continuous --factor 1.05 --bench main <commit1> <commit2>\r\n```\r\nThen asv creates a virtualenv for each commit, and uses a vanilla `pip install` to build NumPy. While this can work on macOS and linux (and use the system OpenBLAS), it will fail to correctly use OpenBLAS on windows, since the build also needs to\r\n- copy the OpenBLAS DLL into the correct place\r\n- build the `_distributor_init.py` file. In CI this is done via additional post-install code:\r\n\r\nIs there a way to, if OpenBLAS is found, to do this as part of the build?\r\n\r\nHere is what the various CI invocations to do these steps look like: \r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/374e73d0690d5caa6dca45af8b181b0472594498\/.github\/workflows\/windows_meson.yml#L63-L72\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/374e73d0690d5caa6dca45af8b181b0472594498\/.github\/workflows\/windows_clangcl.yml#L66-L75\r\n\r\n(on azure this also shows the complicated command line needed)\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/374e73d0690d5caa6dca45af8b181b0472594498\/azure-steps-windows.yml#L38-L48\r\n\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/374e73d0690d5caa6dca45af8b181b0472594498\/tools\/wheels\/cibw_before_build.sh#L39-L56","comments":["I think this is difficult to make work in any reasonable way. Basically, the poor design of Windows and the poor design of Python packaging tooling intersect here - `pip install numpy --no-binary` on Windows is also fundamentally broken, for the same reason.\r\n\r\nOne hack could be to let `asv` run `delvewheel` only on Windows. ","I assume we could get meson to build the `_distributor_init.py` file built as part of the build scripts, conditioned on propogating some OpenBLAS config value. Could we get the DLL copied as part of the install step if we conditionally declare it a data file?","Or maybe this all should wait for OpenBLAS to become a wheel, which would basically solve the whole problem.","> Or maybe this all should wait for OpenBLAS to become a wheel, which would basically solve the whole problem.\r\n\r\nYes, good point. +1 for waiting. It's not like this is a new problem, I don't think this ever worked, the `numpy.distutils` build has the same problems."],"labels":["28 - Benchmark"]},{"title":"BUG: libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled","body":"### Describe the issue:\n\nWe tested some Python apps  in anticipation of CET support being merged into upstream Linux kernel in the near future.\r\nhttps:\/\/www.phoronix.com\/news\/Linux-6.4-Shadow-Stack-Coming\r\nWhile testing CentOS Stream 9 with a custom kernel with shadow stack enabled and Glibc with CET support, we were not able to import \"numpy\". \r\n\r\n\n\n### Reproduce the code example:\n\n```python\n$ python3\r\nPython 3.9.17 (main, Jun 26 2023, 00:00:00) \r\n[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\nTraceback (most recent call last):\r\n...\n```\n\n\n### Error message:\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/__init__.py\", line 23, in <module>\r\n    from . import multiarray\r\n  File \"\/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/multiarray.py\", line 10, in <module>\r\n    from . import overrides\r\n  File \"\/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/overrides.py\", line 8, in <module>\r\n    from numpy.core._multiarray_umath import (\r\nImportError: \/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/..\/..\/numpy.libs\/libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/__init__.py\", line 139, in <module>\r\n    from . import core\r\n  File \"\/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/__init__.py\", line 49, in <module>\r\n    raise ImportError(msg)\r\nImportError: \r\n\r\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\r\n\r\nImporting the numpy C-extensions failed. This error can happen for\r\nmany reasons, often due to issues with your setup or how NumPy was\r\ninstalled.\r\n\r\nWe have compiled some common reasons and troubleshooting tips at:\r\n\r\n    https:\/\/numpy.org\/devdocs\/user\/troubleshooting-importerror.html\r\n\r\nPlease note and check the following:\r\n\r\n  * The Python version is: Python3.9 from \"\/usr\/bin\/python3\"\r\n  * The NumPy version is: \"1.25.1\"\r\n\r\nand make sure that they are the versions you expect.\r\nPlease carefully study the documentation linked above for further help.\r\n\r\nOriginal error was: \/home\/bkc\/.local\/lib\/python3.9\/site-packages\/numpy\/core\/..\/..\/numpy.libs\/libquadmath-96973f99.so.0.0.0: rebuild shared object with SHSTK support enabled\n```\n\n\n### Runtime information:\n\n>>> import sys\r\n>>> print(sys.version)\r\n3.9.17 (main, Jun 26 2023, 00:00:00) \r\n[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]\r\n\n\n### Context for the issue:\n\nThere is a workaround for this, setting the environmental variable:\r\n**GLIBC_TUNABLES=glibc.cpu.x86_shstk=permissive**\r\nwill disable CET functionality and allows loading numpy shared libraries.\r\nHowever, the prudent solution is to actually rebuild all numpy shared libraries with SHSTK support enabled.\r\nThanks.","comments":["> rebuild all numpy shared libraries with SHSTK \r\n\r\nThe library in question comes from the CentOS7 distribution and is already installed in the docker image we use to build the NumPy wheels. It is [installed with gfortran](https:\/\/github.com\/pypa\/manylinux\/blob\/129380e3700b0c52f261ebba0250568e6dedec7a\/docker\/build_scripts\/install-runtime-packages.sh#L72). Is there a RPM containing the proper gofrtran built to use SHSTK?\r\n\r\nIs there a compilation flag needed for SHSTK?","The gfortran command should support all the options supported by the gcc command. \r\nThe relevant flags are: **-fcf-protection=[full|branch|return|none|check]**\r\nhttps:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Instrumentation-Options.html\r\n\r\nYou can check if any is binary compiled with CET flags:\r\n\r\n**$ readelf -n <path\/to\/binary> | grep SHSTK**\r\n","We are not compiling with gfortran, rather including in the NumPy wheel a system library from CentOS7 built by the OS distributor in an RPM. Is CentOS7\/Redhat7 updating all rpms to support this? Will such libraries run unmodified on kernels without CET\/ShadowStack support?\r\n\r\nReading the link you pointed to, I am not sure which of those alternatives enables SHSTK. I see a reference to shadow stack in the [`-fsanitize_shadow-call-stack`](https:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Instrumentation-Options.html#index-fsanitize_003dshadow-call-stack) option, but that is only for aarch64. ","The compiler flag to support CET is  **-fcf-protection**. \r\nhttps:\/\/gcc.gnu.org\/onlinedocs\/gcc\/Instrumentation-Options.html#index-fcf-protection\r\nThe binaries compiled with the CET support run without problems with kernels with or without  CET. In fact all major Linux distros have been building\/distributing their packages with CET support already for a few years. CET has been around for a few years,  it just took a long time to get the CET support into Linux kernel.\r\n\r\nYou can check if any binary is compiled with CET by yourself, for example:\r\n<code>\r\n$ readelf -n \/usr\/bin\/bash | grep SHSTK\r\n      Properties: x86 feature: IBT, SHSTK\r\n<\/code>\r\nIf the numpy binaries are actually built by distro providers, I can file this issue with them.\r\n\r\n","Thanks for the clarifications. The libquadmath lib is copied out of the Docker image based on centos:7 used in building the `manylinux2014` wheel. The gcc tool set comes from [this line ](https:\/\/github.com\/pypa\/manylinux\/blob\/129380e3700b0c52f261ebba0250568e6dedec7a\/docker\/build_scripts\/install-runtime-packages.sh#L99) which installs `gcc-toolset-12-gcc-gfortran`. So in order to solve this issue, we need a new build of [the RPM](https:\/\/kojihub.stream.centos.org\/koji\/rpminfo?rpmID=732207), which will probably not happen. That manylinux2014 tag (also known as manylinux_2_17) and centos:7 will [go end-of-life](https:\/\/github.com\/mayeut\/pep600_compliance) on June 30 2024. We could start building `manylinux_2_28` wheels in addition to the `manylinux2014` ones. These are based on the `almalinux:8`, and when I run the `quay.io\/pypa\/manylinux_2_28_x86_64` docker image I see that it supports `SHSTK`:\r\n```\r\n# readelf -n \/usr\/lib64\/libquadmath.so.0.0.0 | grep SHSTK\r\n      Properties: x86 feature: IBT, SHSTK\r\n```","Building `manylinux_2_28` wheels seems like a good idea to me. It will get us some other benefits too probably, because of fewer bugs in the newer glibc.\r\n\r\nDo we know how urgent this is? When would we expect to see the first distro shipping this in a way that would break the latest numpy wheels?","The user mode shadow stack support is already upstreamed in Linux kernel 6.6-rc2,","Apparently Fedora Rawhide [uses this rc2 kernel in its release](https:\/\/packages.fedoraproject.org\/pkgs\/kernel\/kernel\/). "],"labels":["00 - Bug"]},{"title":"MAINT: Improve wording of numpy.pad error message","body":"Closes #24215.","comments":["You'll need to update the tests too, the text of the error message is tested for.\r\n\r\nFor what it's worth, this isn't a typo, a secondary definition of \"integral\" is \"of or relating to integers\".\r\n\r\nThat said, I think the new error message is probably more understandable, since the usage of \"integral\" here is somewhat technical and is clearly confusing at least to the PR submitter and probably many more users.","What @ngoldbaum  said. The failing tests in `numpy\/lib\/tests\/test_arraypad.py` are looking for the old exception string, you need to update them."],"labels":["03 - Maintenance"]},{"title":"BUG: undefined behavior detected by ubsan in sanitizer CI runs","body":"Once all these issues are fixed the CI job running with the sanitizers turned on can have `UBSAN_OPTIONS=halt_on_error=1` in its `spin test` invocation so that new UB doesn't get introduced in the future.\r\n\r\n* `NULL` array shape passed to memcpy\r\n\r\n```\r\nnumpy\/array_api\/tests\/test_creation_functions.py::test_asarray_copy \r\n..\/numpy\/core\/src\/multiarray\/array_coercion.c:1047:13: runtime error: null pointer passed as argument 2, which is declared to never be null\r\n```\r\n\r\nHappening because `memcpy` is receiving a `NULL` second argument, ultimately because the array coercion machinery is getting passed an incompletely initialized array object created internally inside numpy as part of ufunc reduction.\r\n\r\nThere's another similar issue with subarrays:\r\n\r\n```\r\nnumpy\/core\/tests\/test_arrayprint.py::TestArray2String::test_structure_format_mixed \r\n..\/numpy\/core\/src\/multiarray\/ctors.c:674:13: runtime error: null pointer passed as argument 2, which is declared to never be null\r\n```\r\n\r\nAnother one in `TestBool`:\r\n\r\n```\r\nnumpy\/core\/tests\/test_multiarray.py::TestBool::test_cast_from_void \r\n..\/numpy\/core\/src\/multiarray\/scalarapi.c:276:9: runtime error: null pointer passed as argument 2, which is declared to never be null\r\n```\r\n\r\nThis one seems to be from not doing error handling for the `scalar_value` function.\r\n\r\n* `NULL` passed to qsort\r\n\r\n```\r\nnumpy\/core\/tests\/test_datetime.py::TestDateTime::test_datetime_busday_offset \r\n..\/numpy\/core\/src\/multiarray\/datetime_busdaycal.c:234:5: runtime error: null pointer passed as argument 1, which is declared to never be null\r\n```\r\n\r\n* Misaligned cast in boolean indexing\r\n\r\n```\r\nnumpy\/array_api\/tests\/test_set_functions.py::test_inverse_indices_shape[unique_all] \r\n..\/numpy\/core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x6080000c75a2 for type 'unsigned int', which requires 4 byte alignment\r\n0x6080000c75a2: note: pointer points here\r\n 00 00  01 01 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00\r\n              ^ \r\n..\/numpy\/core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x6080000c75a1 for type 'unsigned int', which requires 4 byte alignment\r\n0x6080000c75a1: note: pointer points here\r\n 00 00 00  01 01 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00\r\n              ^ \r\nnumpy\/core\/tests\/test_api.py::test_copyto_fromscalar \r\n..\/numpy\/core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x60200000c6f2 for type 'unsigned int', which requires 4 byte alignment\r\n0x60200000c6f2: note: pointer points here\r\n 00 00  00 01 00 00 00 01 00 00  00 00 00 00 00 00 00 00  02 11 00 00 10 00 00 00  14 00 80 5b 00 00\r\n              ^ \r\nnumpy\/core\/tests\/test_mem_overlap.py::TestUFunc::test_unary_ufunc_1d_manual \r\n..\/numpy\/core\/src\/multiarray\/common.h:288:31: runtime error: load of misaligned address 0x6190020f597f for type 'unsigned int', which requires 4 byte alignment\r\n0x6190020f597f: note: pointer points here\r\n fc fd fe ff 00  01 02 03 04 05 06 07 08  09 0a 0b 0c 0d 0e 0f 10  11 12 13 14 15 16 17 18  19 1a 1b\r\n             ^ \r\n\r\n```\r\n\r\n\r\nThe first report is coming from a Python line doing something like:\r\n\r\n```\r\nnp.array([False])[np.array([True])]\r\n```\r\n\r\nSomehow this creates a C iterator in numpy's boolean indexing internals that has a stride of 1, ultimately leading to a cast from uninitialized data (I think). I can't reproduce this outside the array API tests so it's something a bit more complicated than just the snippet above.\r\n\r\n* Invalid bit shift\r\n\r\n```\r\nnumpy\/lib\/tests\/test_mixins.py::TestNDArrayOperatorsMixin::test_forward_binary_methods \r\n..\/numpy\/core\/src\/npymath\/npy_math_internal.h.src:657:18: runtime error: left shift of negative value -1\r\n```\r\n\r\n```\r\nnumpy\/core\/tests\/test_ufunc.py::TestUfunc::test_ufunc_at_basic[a1] \r\n..\/numpy\/core\/src\/umath\/_rational_tests.c:54:24: runtime error: left shift of 1 by 31 places cannot be represented in type 'int'\r\n```\r\n\r\n\r\n* Int overflows in datetime casts\r\n\r\nQuite a few of these, just an example:\r\n\r\n```\r\nnumpy\/core\/tests\/test_casting_unittests.py::TestCasting::test_time_to_time[M8[ms]-M8[ns]-Casting.safe-None-1000000-1] \r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: -9223372036854775808 * 1000000 cannot be represented in type 'long int'\r\n..\/numpy\/core\/src\/multiarray\/dtype_transfer.c:865:25: runtime error: signed integer overflow: 9223372036854775807 * 1000000 cannot be represented in type 'long int'\r\n\r\nnumpy\/core\/tests\/test_casting_unittests.py::TestCasting::test_time_to_time[M8[4D]-M8[1M]-Casting.same_kind-None-None-denom8]\r\n..\/numpy\/core\/src\/multiarray\/datetime.c:478:8: runtime error: signed integer overflow: 4 * 9223372036854775807 cannot be represented in type 'long int'\r\n```\r\n\r\n* Int overflow in einsum tests\r\n\r\n```\r\nnumpy\/core\/tests\/test_einsum.py::TestEinsum::test_einsum_broadcast \r\n..\/numpy\/core\/src\/multiarray\/einsum_sumprod.c.src:620:33: runtime error: signed integer overflow: 9223365439786057728 + 13194139533312 cannot be represented in type 'long int'\r\n..\/numpy\/core\/src\/multiarray\/arraytypes.c.src:3789:13: runtime error: signed integer overflow: 9223365439786057728 + 13194139533312 cannot be represented in type 'long int'\r\nPASSED\r\n```\r\n\r\n* Int overflow in int128 tests\r\n\r\n```\r\nnumpy\/core\/tests\/test_extint128.py::test_safe_binop \r\n..\/numpy\/core\/src\/common\/npy_extint128.h:21:14: runtime error: signed integer overflow: -9223372036854775808 + -9223372036854775808 cannot be represented in type 'long int'\r\n..\/numpy\/core\/src\/common\/npy_extint128.h:35:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long int'\r\n..\/numpy\/core\/src\/common\/npy_extint128.h:56:14: runtime error: signed integer overflow: -9223372036854775808 * -9223372036854775808 cannot be represented in type 'long int'\r\n\r\n```\r\n\r\n* Int overflow in TestWritebackIfCopy\r\n\r\n```\r\nnumpy\/core\/tests\/test_multiarray.py::TestWritebackIfCopy::test_choose_mod_raise \r\n..\/numpy\/core\/src\/multiarray\/iterators.c:1302:44: runtime error: signed integer overflow: -3617008641903833651 * 3 cannot be represented in type 'long int'\r\n```\r\n\r\n* Int overflow in nditer tests\r\n\r\n```\r\nnumpy\/core\/tests\/test_nditer.py::test_iter_too_large_with_multiindex \r\n..\/numpy\/core\/src\/multiarray\/nditer_api.c:497:20: runtime error: signed integer overflow: 1152921504606846976 * 1024 cannot be represented in type 'long int'\r\n```\r\n\r\n* Int overflows in NEP 50 tests\r\n\r\n```\r\nnumpy\/core\/tests\/test_nep50_promotions.py::test_nep50_weak_integers[i] \r\n..\/numpy\/core\/src\/umath\/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 2147483647 cannot be represented in type 'int'\r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 2147483647 cannot be represented in type 'int'\r\nnumpy\/core\/tests\/test_nep50_promotions.py::test_nep50_weak_integers[l] \r\n..\/numpy\/core\/src\/umath\/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long int'\r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long int'\r\nnumpy\/core\/tests\/test_nep50_promotions.py::test_nep50_weak_integers[q]\r\n ..\/numpy\/core\/src\/umath\/scalarmath.c.src:62:14: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long long int'\r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 100 + 9223372036854775807 cannot be represented in type 'long long int'\r\n```\r\n\r\n* Int overflows in overflow tests\r\n\r\n```\r\nnumpy\/core\/tests\/test_scalarmath.py::test_scalar_integer_operation_overflow[--i] \r\n..\/numpy\/core\/src\/umath\/scalarmath.c.src:71:14: runtime error: signed integer overflow: -2147483648 - 2147483647 cannot be represented in type 'int'\r\n\r\nnumpy\/core\/tests\/test_scalarmath.py::test_scalar_integer_operation_overflow[--l] \r\n..\/numpy\/core\/src\/umath\/scalarmath.c.src:71:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long int'\r\n\r\nnumpy\/core\/tests\/test_scalarmath.py::test_scalar_integer_operation_overflow[--q] \r\n..\/numpy\/core\/src\/umath\/scalarmath.c.src:71:14: runtime error: signed integer overflow: -9223372036854775808 - 9223372036854775807 cannot be represented in type 'long long int'\r\n\r\nnumpy\/core\/tests\/test_umath.py::TestRationalFunctions::test_gcd_overflow \r\n..\/numpy\/core\/src\/npymath\/npy_math_internal.h.src:636:33: runtime error: negation of -9223372036854775808 cannot be represented in \r\n\r\nnumpy\/core\/tests\/test_ufunc.py::test_ufunc_input_floatingpoint_error[0] \r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: -9223372036854775808 + -9223372036854775808 cannot be represented in type 'long int'\r\n```\r\n\r\nPresumably where we intentionally want overflow to happen we need to check for overflow before actually doing an overflowing operation in C.\r\n\r\n* Int overflow in ufunc at inner loops\r\n\r\n```\r\nnumpy\/core\/tests\/test_ufunc.py::TestUfunc::test_ufunc_at_inner_loops[multiply-i] \r\n..\/numpy\/core\/src\/umath\/loops.c.src:461:29: runtime error: signed integer overflow: 6058426 * 397 cannot be represented in type 'int'\r\n..\/numpy\/core\/src\/umath\/loops_autovec.dispatch.c.src:76:9: runtime error: signed integer overflow: 6058426 * 397 cannot be represented in type 'int'\r\n```","comments":["Hmmm, initial ones seem like things that need fixing (I am a bit surprised we get away with them).  I thought we worked around undefined behavior in the bit-shifts.\r\n\r\nWith integer overflows, I am not sure.  We always just said \"it's UB in C and that is what you get\", and I am not sure how reasonable it is to protect against it.  I suspect we rely on 2-complement behavior in at least a few places ourselves though.\r\n(In either case, I somewhat suspect the thing there will be tell UBSAN to not worry about it, ideall we might do that targeted to ufuncs and casting though).","UBsan has support for suppression files so once we figure out which of the int overflows are there on purpose and we are fine with we can just add suppressions for them.","Stumbling on this.  @hawkinsp do you maybe already have a suppression file for some of NumPy that you could share so we can make our CI strict?"],"labels":["00 - Bug","Project","sprintable - C"]},{"title":"BUG: `np.geomspace()` raises AttributeError if `stop >= 2 ** 64`","body":"### Describe the issue:\r\n\r\nWhen `np.geomspace(start, stop)` is called with large values of `stop` (greater than or equal to $2^{64}$), an AttributeError is thrown. \r\n\r\nFor example, `np.geomspace(1, 10 ** 20)` gives an error. Expected behavior is to not error and instead to yield identical output to `np.logspace(0, 20)` (which functions correctly).\r\n\r\nInterestingly, `np.geomspace(1, 10 ** 19)` gives correct behavior and does not yield an error. `np.geomspace(1, 10 ** 21)` does yield an error, as do higher values of `stop`.\r\n\r\nReproducibility of this bug is verified to persist across multiple computers, multiple NumPy versions, and multiple Python versions (see below).\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nnp.geomspace(1, 10 ** 20)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nIn [3]: np.geomspace(1, 10 ** 20)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nAttributeError: 'int' object has no attribute 'log10'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-dfd50b4fec30> in <module>\r\n----> 1 np.geomspace(1, 10 ** 20)\r\n\r\n~\\anaconda3\\lib\\site-packages\\numpy\\core\\function_base.py in geomspace(start, stop, num, endpoint, dtype, axis)\r\n    443         _nx.negative(out_sign, out=out_sign, where=both_negative)\r\n    444\r\n--> 445     log_start = _nx.log10(start)\r\n    446     log_stop = _nx.log10(stop)\r\n    447     result = logspace(log_start, log_stop, num=num,\r\n\r\nTypeError: loop of ufunc does not support argument 0 of type int which has no callable log10 method\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\nBug reproduces on Computer 1:\r\n\r\nNumPy 1.25.1\r\nPython 3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]\r\n\r\n-----\r\n\r\nBug also confirmed to reproduce on Computer 2:\r\n\r\nNumPy 1.24.3\r\nPython 3.10.11 | packaged by Anaconda, Inc. | (main, Apr 20 2023, 18:56:50) [MSC v.1916 64 bit (AMD64)]\r\n\r\n### Context for the issue:\r\n\r\nUsage of `np.geomspace(1, 10 ** 20)` seems like a core, significant error in the basic functionality of one of NumPy's spacing functions.\r\n\r\nWhile one can of course work around this by running `np.logspace(1, 20)` (which does not produce an error), the `geomspace` function is widely used enough that I would think this bug would affect a ton of NumPy users.","comments":["The first thing that happens to the inputs is they are turned into arrays. The problem is that `10**20` is too big for any of our integer dtypes, so it turns into a `dtype=object` array. That forces the calculations to be done as `dtype=object` rather than `dtype=float`.\r\n\r\nThe issue isn't really the magnitude of the _number_, per se. If you use a float (i.e. either `10.**20` or `1e20`), everything works fine. Most people do use floats as inputs here, so that's why it probably hasn't really affected that many people.\r\n\r\n```python\r\n>>> np.geomspace(1, 10.**20, 21)\r\narray([1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10, 1.e+11, 1.e+12, 1.e+13, 1.e+14, 1.e+15, 1.e+16, 1.e+17, 1.e+18, 1.e+19, 1.e+20])\r\n```\r\n\r\nI do think it's worth a note in the docstring, if not a fix. I suspect that a fix may be more difficult than it might seem at first blush, but possibly we can catch this particular exception and yield a more informative error message.","Just to note that for ops that are float only, NEP 50 would fix this.  (And does already in the current experimental opt-in).","I'm not seeing that it does in 1.25.1. `geomspace()` converts the inputs to arrays before calling `result_type()`, which, if I'm reading NEP 50 right, basically sidesteps the NEP 50 machinery.\r\n\r\n```python\r\n>>> import numpy as np\r\n\r\n>>> np.__version__\r\n'1.25.1'\r\n\r\n>>> np._set_promotion_state('weak')\r\n\r\n>>> np._get_promotion_state()\r\n'weak'\r\n\r\n>>> np.geomspace(1, 10**20, 21)\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nAttributeError: 'int' object has no attribute 'log10'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[5], line 1\r\n----> 1 np.geomspace(1, 10**20, 21)\r\n\r\nFile ~\/.local\/share\/virtualenvs\/scratch-D2q_Geod\/lib\/python3.10\/site-packages\/numpy\/core\/function_base.py:445, in geomspace(start, stop, num, endpoint, dtype, axis)\r\n    442     _nx.negative(stop, out=stop, where=both_negative)\r\n    443     _nx.negative(out_sign, out=out_sign, where=both_negative)\r\n--> 445 log_start = _nx.log10(start)\r\n    446 log_stop = _nx.log10(stop)\r\n    447 result = logspace(log_start, log_stop, num=num,\r\n    448                   endpoint=endpoint, base=10.0, dtype=dtype)\r\n\r\nTypeError: loop of ufunc does not support argument 0 of type int which has no callable log10 method\r\n```","Sorry, you are right.  I first writing that we _can_ change it by saying something like `log` should go to float for all python integers.\r\nThen must have tested the wrong thing and was surprised it worked.","How we should handle complex values in the input?","The current behavior with complex inputs is fine. Probably useless, but fine.","I just realized that this is a place where the gh-24214 idea should be helpful:  Integrating the current `result_type` into the array conversion would be sufficient to make things pass (with NEP 50 enabled)."],"labels":["00 - Bug"]},{"title":"BUG: floor_divide is not normal when calculating the input containing\u2026","body":"Fixes #23860","comments":["A few comments to hopefully allow you to continue here (maybe @ganesh-k13 will take a look at it after that):\r\n* We need tests which cover the different branches\r\n  * The annotation (\"files changed\" tab) show uncovered code paths, these are probably correct; we should add tests for them here (at least for any added branch).\r\n* There is no reason to diverge the float, double, and longdouble implementations here\r\n* I would avoiding unnecessary branches if the result will be correct from divmod.  Explicitly returning `NaN` will also silence warnings we may want to see explicitly  (these can be set manually, though).  OTOH, it may be that divmod sets warnings we would prefer not seeing when it returns a `NaN` for the remainder.\r\n  (I am not sure if this applies to branches added)","thank you for the review, I have made the changes and added unit tests. I diverged floor_divide from float and longdouble implementations so they do not have to do the extra work as inf is always a double, or is it better to have less duplicate code?","Just a note here, but floor_divide needs to be consistent with remainder, which is why divmod is used.","Hmm, yeah, maybe we have to make all fixes in `divmod`.  We still need the additional branch (or a broader branch) just like we had for `b == 0` because we want to avoid generating the `NaN` of the remainder.  But right, divmod and remainder would now give different results for the division part also.\r\n(I suspect the change from -1. to -0. is right, but I am not 100% sure yet.)","I have made the changes in divmod instead of floor divide and added tests for mod, divmod and floor divide. The pipeline is failing with 429 error when calling numpy, is that an issue on my side?"],"labels":["00 - Bug"]},{"title":"MAINT: add a fuzzing test to try to introduce segfaults","body":"Adds the fuzzing test used to surface two segfaults in numpy\/numpy#24023 with a slightly cleaned-up loop and check counts sized to finish in less than a minute. Note that this surfaces another intermittent segfault probably in `array.choose` that is beyond my ability to debug on a Mac:\r\n\r\n```\r\n...\r\nchecking method: `byteswap`\r\nchecking method: `choose`\r\nzsh: segmentation fault  python test_segfault.py\r\nmikedh@luna tests % python -c \"import numpy; print(numpy.__version__)\"\r\n1.25.1\r\nmikedh@luna tests % python --version\r\nPython 3.9.2\r\nmikedh@luna tests % uname -a\r\nDarwin luna.localdomain 22.5.0 Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7\/RELEASE_X86_64 x86_64\r\n```\r\n\r\nWhile this *catches* things, especially in a build matrix, it still takes quite a bit of work to hunt down the guilty arguments (especially since segfaults kill everything). I played with writing args to a tempfile but it was unacceptably slow for a unit test. I totally understand if the project doesn't want to run this in the large test matrix.\r\n\r\nThis is mostly deterministic, but it does check values produced by `numpy.random`. I can change this to always be seeded from a constant unless that is somehow already done in the test framework. \r\n","comments":["Sounds good! I got a reproducer on `choose` but it's still very intermittent, I'll try to clean it up into a bug report. For this PR:\r\n\r\n- I'll rename to `test_junk_paths.py`\r\n- Refactor loops to be pytest fixtures\r\n- see if we can de-duplicate sample data in a way that still catches the issues surfaced in `1.25.0`\r\n- aim to reduce test to ~10s\r\n","Perhaps you could use hypothesis rather than a home-grown fuzzer? It provides a structured way to explore property based testing. It saves state between runs, so will allow capturing input cases that are problematic.\r\n\r\nAdding this to every CI run would be too expensive, but maybe we could add a marker to run it only rarely.","There is also ossfuzz from google, which this might actually fit into well?  (Mainly also pointing it out in case you find it interesting.)\r\nRight now they have something for NumPy, but it only fuzzes boring calls to loadtxt, IIRC: https:\/\/github.com\/google\/oss-fuzz\/tree\/master\/projects\/numpy","Yeah, using a fuzzing library could be desirable, although adding a dependency to numpy is above my pay grade \ud83d\ude04. I think there is perhaps an argument that the needs here are a little more special case than a general-purpose fuzzer, and a self-contained script is usually easier to maintain. Either way I made the following changes based on suggestions:\r\n\r\n- I reduced runtime to 1.2s on my laptop by reducing the number of cases checked (i.e. checking an array with every byte order and dtype) and verified that it was still catching the errors surfaced in `numpy==1.25.0`\r\n- I refactored the argument generation to be a pytest fixture.\r\n- I changed the argument generation to use `itertools.product` and added a check to make sure it wasn't including any duplicates. \r\n- I renamed to `test_junk_calls.py`\r\n\r\n","Do we want to pursue this?  I am fine with just putting it in.  It found some nice bugs! OTOH, it would be more useful to fuzz also functions and offload it to not run regularly.  But if we don't integrate it in the test-suite (and thus it is very fast), I am not usre we will actually end up running it often enough to be useful.","Let's discuss it at a community\/triage meeting.","@ngoldbaum do you want to make a call either way?  It seems somewhat useful, but I agree with Matti that the real deal would be some property based (hypothesis) or a dedicated extensive fuzzer (like ossfuzz).","I think it probably makes sense to pull this in, since the test has a much faster runtime than when this PR was initially proposed (although I haven't checked that today). Hypothesis or something would be better, but that requires someone to wire it up, and this already exists and is finding real bugs in poorly tested error paths in numpy.\r\n\r\nHowever, the test failures are real and need to be fixed before this can be merged. There's a segfault on pypy that needs to be looked at, the full tests are failing because of the warning level test, and it looks like the build with assertion turned and some windows builds had issues as well, although the build log has been deleted on azure so I triggered another run."],"labels":["03 - Maintenance"]},{"title":"CI: Re-enable dependabot or manually update test dependencies?","body":"It looks like many of numpy's test dependencies haven't been updated since dependabot was disabled in 2021, see #20268.\r\n\r\nIn the meantime, github [fixed the issue](https:\/\/github.blog\/changelog\/2022-11-07-dependabot-pull-requests-off-by-default-for-forks\/) causing spam PRs on forks of NumPy at the end of last year.\r\n\r\nWould it make sense to turn dependabot on again now that forks won't be spammed with PRs anymore? I think existing forks may need to toggle a setting, new forks should have that setting turned off by default.\r\n\r\nIf there's no appetite for turning on dependabot again, would a PR that updates the test dependencies be OK?","comments":["Dependabot is running now, but only github-actions are kept updated, see  `.\/.github\/dependabot.yml`. I've thought of extending it to tools.\r\n\r\nEDIT: I don't know if specific files can be added to the list kept updated, but that would be convenient.","I'll note that there's a number of CI jobs that use unpinned test and build dependencies, so we're not missing any coverage.\r\n\r\nPinning everything is an option, but we're losing coverage of different versions of test dependencies at that point, so I'm not sure that that's actually a good thing. Or do we have failures due to new versions of dependencies too often?\r\n\r\nIf there's a need to pin everything, there's multiple ways of doing it: manual pinning, requirements files, or relying more on conda lock files like scikit-learn does. I don't know how happy scikit-learn is with that approach, but given that our most important dependencies are non-Python ones (BLAS\/LAPACK libraries, compilers) I think that has the potential to significantly improve our test coverage. So it's worth looking into that either way. \r\n\r\n> If there's no appetite for turning on dependabot again, would a PR that updates the test dependencies be OK?\r\n\r\nA PR for that is always welcome. My opinion is that Dependabot's maintainers have proven themselves to be utterly untrustworthy (it's really really not okay to ignore a major spam issue for 3 years) _and_ that the Dependabot PRs are low-value noise mostly. So I'd much prefer not to re-enable Dependabot.","I mostly enabled dependabot to improve our security score, but it could  also increase the danger of new upstream bugs. It has been useful a few times when there have been upstream fixes, in particular for rtools and changes in choco. Chasing down bugs and fixes is a bit easier with dependabot tracking our dependencies."],"labels":["component: CI"]},{"title":"BUG: numpy.piecewise returns data of the same type as the input type, which is undesirable","body":"### Describe the issue:\n\nGenerally, a piecewise function can take values of any type as an input and return numbers of any type as an output. However, in its current implementation `numpy.piecewise` forces the output type to be equal to the input type, wich can lead to erroneous output.\r\n\r\nThis is because the result of the evaluation is being prepared with `output = numpy.zeros_like(input)`, which copies the `dtype` of the input (cf: https:\/\/github.com\/numpy\/numpy\/blob\/v1.25.0\/numpy\/lib\/function_base.py#L751 )\r\n\r\nMaybe this is actually desired behavior and not a bug, which is a viewpoint I would kindly disagree with :)\n\n### Reproduce the code example:\n\n```python\nIn [4]: if True:\r\n   ...:     import numpy as np\r\n   ...:\r\n   ...:     f = lambda x: 1\/x\r\n   ...:\r\n   ...:     xint = 10\r\n   ...:     xflt = 10.\r\n   ...:     result_int_input = np.piecewise(xint, (xint<0, xint>=0), (0, f))\r\n   ...:     result_flt_input = np.piecewise(xflt, (xflt<0, xflt>=0), (0, f))\r\n   ...:\r\n\r\nIn [5]: result_int_input\r\nOut[5]: array(0)\r\n\r\nIn [6]: result_flt_input\r\nOut[6]: array(0.1)\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n    1.23.2\r\n    3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]\n\n### Context for the issue:\n\nIt took me 20min and looking into numpy's source to find out why my code is failing.","comments":["Upon closer inspection, the docstring actually states that the output type will be equal to the input type. As stated above, I think this behavior is undesirable --- for instance, since Python 3, it's generally expected that an expression like `1\/int(var)` will return `1\/float(int(var))`. \r\n","Related to #19755"],"labels":["00 - Bug"]},{"title":"ENH: use `PYTHONWARNDEFAULTENCODING=1` to fix optional encoding warnings in Python 3.10+","body":"Under Python 3.11, though for some reason I haven't investigated _not_ 3.10,\r\n\r\n```python\r\nimport numpy.testing\r\n```\r\ntriggers the following `EncodingWarning`\r\n```\r\n$ python -X warn_default_encoding demo.py\r\nnumpy\/testing\/_private\/utils.py:1251: EncodingWarning: 'encoding' argument not specified.\r\n  output = subprocess.run(cmd, capture_output=True, text=True)\r\n```\r\n\r\nIt would be great if Numpy added this to CI and fixed however many errors that turns up; with Pytest 7.4.0 and later running clean it's now reasonably practical to do so.  \r\n\r\nYou may want a two-stage approach where you fix all the current cases in Numpy first (like https:\/\/github.com\/HypothesisWorks\/hypothesis\/pull\/3619), and then revisit enforcing it in CI once upstream projects have cleaned up their own warnings (like https:\/\/github.com\/HypothesisWorks\/hypothesis\/pull\/3691, where I hit this issue).","comments":["I see. We need to add `encoding=\"utf-8\"` a bit everywhere to make that warning go away, and remove it for python 3.15+, according to [PEP 686](https:\/\/peps.python.org\/pep-0686\/). That warning was added for python3.11, so one the 3.11+ CI runs should add the env variable.","The warning exists from Python 3.10, though there were some related small changes in 3.11.  \r\n\r\nWe don't actually need to remove the encoding arguments in 3.15 - as I understand it Python will then default to using uft-8 by default, but end-users can still disable that so libraries should probably keep specifying encoding when we know which of  utf-8 or locale is preferred."],"labels":["sprintable"]},{"title":"ENH: Improve error emitted by np.sign(None)","body":"### Proposed new feature or change:\n\nCurrently the error emitted by calling `np.sign` on a `None` object is unclear\r\n\r\n```\r\nIn [2]: np.sign(None)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In [2], line 1\r\n----> 1 np.sign(None)\r\n\r\nTypeError: '<' not supported between instances of 'NoneType' and 'int'\r\n```\r\n\r\nI think that a better error should be raised.\r\n\r\nWould like to try and fix - if this is accepted","comments":["It doesn't seem too bad and `TypeError` is the correct error type. However, improvements to error messages are always welcome as long as they don't negatively impact performance or maintainability too much. So you're welcome to try @alonme.\r\n\r\nHere are a few more:\r\n```python\r\n>>> np.sign(np)\r\nTraceback (most recent call last):\r\n  Cell In[7], line 1\r\n    np.sign(np)\r\nTypeError: '<' not supported between instances of 'module' and 'int'\r\n\r\n>>> np.sign(3.5)\r\n1.0\r\n>>> np.sign(3)\r\n1\r\n>>> np.sign(True)\r\nTraceback (most recent call last):\r\n  Cell In[10], line 1\r\n    np.sign(True)\r\nUFuncTypeError: ufunc 'sign' did not contain a loop with signature matching types <class 'numpy.dtypes.BoolDType'> -> None\r\n``` "],"labels":["03 - Maintenance"]},{"title":"ENH: Add ``matrix_transpose`` function","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nThis PR adds the initial steps for a `np.matrix_transpose` function to match the newly added `.mT` attribute. It is still a WIP, here is what I think still needs to be done:\r\n\r\n- [x] Release note\r\n- [x] Masked array support\r\n- [x] Type hints\r\n\r\nIn addition, I'm not sure if I have put the function in the correct location. If anyone has a recommendation for a better location, please let me know.\r\n\r\ncloses https:\/\/github.com\/numpy\/numpy\/issues\/24035","comments":["LGTM.","This looks about ready. Is the only question about what to do with the masked array return type?","This is ready from my end barring that one question","@seberg  is this ready to be merged?","This was partially superseded by gh-25155 I think, we've got a `matrix_transpose` in the main namespace and in `linalg`. However, the masked array support still seems relevant. Am I right in thinking that the rest can be dropped, and this PR updated to add `np.ma.matrix_transpose` only?","I'm happy with that. When the NumPy maintainers reach consensus I'll be happy to implement it"],"labels":["01 - Enhancement"]},{"title":"ENH: C++ SIMD wrapper","body":"### Proposed new feature or change:\n\nContinuing from and separately to https:\/\/github.com\/numpy\/numpy\/pull\/24018, let's discuss SIMD intrinsics here.\r\n\r\nI understand there is an ongoing effort to replace(?) the NEP38 macros with a C++ wrapper (https:\/\/github.com\/numpy\/numpy\/pull\/21057).\r\n\r\n> part that is because Sayed has momentum for the \"custom\" universal intrinsics\r\n\r\nUnfortunately it seems there is some duplication of effort :( [Highway](https:\/\/github.com\/google\/highway) has been under development since 2017 with contributions from about a dozen engineers, and open sourced in 2019 with several dozen open-source collaborators since then (mostly bugfixes).\r\n\r\nWouldn't we get further by collaborating, perhaps by extending Highway with Numpy-specific operations? This would make it easier to share code and onboard developers (Highway has 2.6k Github stars) as opposed to a custom wrapper only used by Numpy.\r\n\r\nWouldn't it also make sense to benefit from all the ongoing maintenance efforts? This is quite costly (multiple patches per week) given all the platforms\/compilers to support.\r\n\r\n> @jan-wassenberg , I think this would just be something like MaskedGatherLoad and MaskedBlendedStore\r\n\r\nThanks, makes sense. FYI it is possible to emulate these by IfThenElse on the indices, replacing invalid ones with a safe\/dummy address. (We found this to have no observable perf impact on x86.)\r\nWe can soon add native versions of these, though, because it would be more obvious\/convenient for user code.\r\n","comments":["Having uniformity across projects would be desirable and be a more efficient use of resources. I believe @seiko2plus is busy at the moment, but it might be good to setup some mutual discussion about coordination. IIRC, numpy's intrinsics have their roots in opencv.","Sounds good. Would be happy to meet: janwas at google (com), CEST timezone.","Hi @jan-wassenberg great to have you here, lets start the discussion by separating each point you mentioned on the description\r\n\r\n> I understand there is an ongoing effort to replace \r\n\r\nThat's true. We want to get rid of the custom template sources in NumPy's C implementation and transition to using C++ templates for simplicty. This shift will also allow us to leverage the benefits of using sizeless SIMD extensions, such as ARM SVE.\r\n\r\n> Unfortunately it seems there is some duplication of effort :( \r\n\r\nHaving duplicate work isn't an excuse to refuse adopting the highway. We can take a deep breath and rethink our approach, with NumPy's interests in mind.\r\n\r\n> Highway has been under development since 2017 with contributions from about a dozen engineers, and open sourced in 2019 with several dozen open-source collaborators since then (mostly bugfixes).\r\n\r\nAre we in a comparison situation between Numpy's universal intrinics and Highway? IMHO, numbers do not necessarily reflect the quality of work.\r\n\r\n> Wouldn't we get further by collaborating, perhaps by extending Highway with Numpy-specific operations? This would make it easier to share code and onboard developers\r\n\r\nSame question goes to new C++ simd standards. Supporting sizeless SIMD extensions within std::experimental::simd is possible and wouldn't be that hard.\r\n\r\n>  custom wrapper only used by Numpy\r\n\r\nThat's true, the custom wrapper is designed exclusively for NumPy's use and not intended to be exposed to other projects. Keeping it minimal is essential to handle the variety of SIMD instructions efficiently without sinking inside a lake of functions.\r\n\r\nWe expand universal intrinsics based on the SIMD kernels' needs, and we rely on advanced testing units that use Python as an interface to test each universal intrinsic independently to allow us to detect any possible bugs and to validate sanity of the implementation.\r\n\r\n> Wouldn't it also make sense to benefit from all the ongoing maintenance efforts? This is quite costly (multiple patches per week) given all the platforms\/compilers to support.\r\n\r\nAgrees but during the implementation of any new kernel, the focus should be on the specific requirements and optimizations needed for that kernel, rather than being limited by the high-level interface. Constantly proposing new generic intrinsics can potentially slow down the progress of the work and may eventually lead us to implement our own custom intrinsics, diverging from the upstream.\r\n\r\n> I think this would just be something like MaskedGatherLoad and MaskedBlendedStore\r\nThanks, makes sense. FYI it is possible to emulate these by IfThenElse on the indices, replacing invalid ones with a safe\/dummy address. (We found this to have no observable perf impact on x86.)\r\n\r\nI think the suggestion related to supporting partial operations on both contiguous and non-contiguous memory access, with both single and pair offsets. Pair offsets is crucial for complex kernels that require 64-bit\/32-bit stride alignment.\r\n\r\nPlease take a look at the currently supported memory operations in the new reference, which is included in pull request #21057. You can access the reference documentation at the following link: https:\/\/output.circle-artifacts.com\/output\/job\/6a58c19b-2137-4b00-80a7-50e532c4d7b3\/artifacts\/0\/doc\/build\/html\/reference\/simd\/intrinsics\/memory.html","@charris,\r\n> it might be good to setup some mutual discussion about coordination.\r\n\r\nHow about the next community meeting?","In general a community call has participants who may not want to discuss the intricacies of C++ templating, so if the discussion can remain on the level of \"do we want to use a third party library or our own solution\" without diving into too much technical detail that would be good. ","Sounds good to me then we can follow up within triage meeting for more technical.","Thanks @seiko2plus :)\r\n\r\n> We want to get rid of the custom template sources in NumPy's C implementation and transition to using C++ templates for simplicty. This shift will also allow us to leverage the benefits of using sizeless SIMD extensions, such as ARM SVE.\r\n\r\nNice! Is RISC-V also planned?\r\n\r\n> We can take a deep breath and rethink our approach, with NumPy's interests in mind.\r\n\r\nSounds good.\r\n\r\n> IMHO, numbers do not necessarily reflect the quality of work.\r\n\r\nAgreed. I mention it because it is easy to underestimate the surprisingly high cost of maintenance. One interesting statistic is that we have 532 commits mentioning 'fix', so on average 2 per week over 4 years. This can be compiler bugs, or a combination of arch+compiler+flags that was not tested in CI, or implementation bugs.\r\n\r\n> Same question goes to new C++ simd standards.\r\n\r\nUnfortunately the std::experimental::simd only provides a [small set of operations](https:\/\/en.cppreference.com\/w\/cpp\/experimental\/simd\/simd), almost only the basic operators and unit-stride memory accesses, which is far too limited for many apps including ours and likely numpy.\r\n\r\n> Supporting sizeless SIMD extensions within std::experimental::simd is possible and wouldn't be that hard.\r\n\r\nInteresting, I thought this would be very difficult given that std::experimental::simd wraps vectors in a class, which is not allowed with SVE (except using Arm's special compiler) nor RVV. Do you know of some way to avoid that?\r\n\r\n> Keeping it minimal is essential to handle the variety of SIMD instructions efficiently without sinking inside a lake of functions.\r\n\r\nhm, if starting from scratch, I can see that one would want to target the minimal set. But given that a [large number of operations](https:\/\/github.com\/google\/highway\/blob\/master\/g3doc\/quick_reference.md#operations) is already working in Highway, isn't the question how many numpy-specific ones remain to be added, versus the cost of a custom solution?\r\n\r\n> the focus should be on the specific requirements and optimizations needed for that kernel, rather than being limited by the high-level interface. Constantly proposing new generic intrinsics can potentially slow down the progress of the work and may eventually lead us to implement our own custom intrinsics, diverging from the upstream.\r\n\r\nCan you help me understand your concern here? Do you see any limitations of the general Highway approach?\r\n\r\n> I think the suggestion related to supporting partial operations on both contiguous and non-contiguous memory access, with both single and pair offsets. Pair offsets is crucial for complex kernels that require 64-bit\/32-bit stride alignment.\r\n\r\nI see, thanks. Looks like there are three functionalities not yet in Highway. Our `LoadInterleaved2` would have to be extended with `*Till` support (only load partial vectors). That seems quite doable.\r\nLooks like Loadn\/LoadnPair can mostly be implemented on top of `Gather*`, with some specializations.\r\n\r\nLookup128 could also be added. I am curious whether we would ever have more than 32 elements in the tables?\r\n\r\nI will aim to attend the Wednesday community meeting. Hope to see you there, though I might be slowed down by a nose\/throat infection :)","This seems like a good and timely high-level discussion to have. It'd be great to get more clarify on the trade-offs. One key thing that changed since [NEP 38](https:\/\/numpy.org\/neps\/nep-0038-SIMD-optimizations.html) was written four years ago is that now we have C++ code in the code base, and are gradually increasing that usage. C++-only libraries were not in the picture at all back then. Besides Highway, there's also [xsimd](https:\/\/xsimd.readthedocs.io\/en\/latest\/index.html), and @serge-sans-paille has inquired before about interest in adopting it in NumPy.\r\n\r\nGiven that @mattip suggested in gh-24018 that \"_maybe NumPy should not be in the business of providing highly specialized inner loop functions, and we should instead provide an easily pluggable interface _\", I'll register my opposite opinion here (mostly in agreement with what @Mousius wrote): Performance matters, and defaults matter. Significant performance improvements of heavily used NumPy functions are impactful - much more so than most of the effort on the many niche functions and features that NumPy carries around. Many downstream library authors and end users work very hard to optimize code, or resort to things like `multiprocessing` or `dask` (which is much more labor-intensive and clumsy than better _single-threaded_ performance).\r\n\r\nWhat I do agree with is that we need a maintainable strategy, and if we can improve on that then we should. \r\n\r\nA side note on timelines & the current SIMD framework: we have exactly one month to go until the Python 3.12.0rc1 release, and 3 months until the final 3.12.0 release. We're already able to put up binaries for the rc1 stage after the switch to Meson, but not including SIMD implementation. That's not essential for rc1 (but it is desirable), while for the final release it is essential. So we're going to hammer that into shape fairly soon. Those changes will be backported from `main` to a `maintenance\/1.26.x` branch which will be created off of `maintenance\/1.25.x`. So we ideally shouldn't have much churn in `main` until that backport is done (regular changes are fine, but I'd prefer no significant architectural changes).\r\n\r\nI'm looking in a bit of detail at Highway for the first time. The changes in gh-24018 look pleasantly small. It looks like Highway supports a number of the things we need - compiler support and architecture support look fine (I'll note in comparison here that `xsimd` misses VSX), and the build system integration looks pretty flexible and reasonably well documented.\r\n\r\nThe current conversation is mostly about the pros and cons of universal intrinsics as implemented in NumPy vs. in Highway, and what's needed there & maintainability. The dynamic dispatch capabilities in Highway don't seem directly relevant to that, however they are interesting to me, given that that's our current biggest headache with build system migration.\r\n\r\n> > Constantly proposing new generic intrinsics can potentially slow down the progress of the work and may eventually lead us to implement our own custom intrinsics, diverging from the upstream.\r\n> \r\n> Can you help me understand your concern here? Do you see any limitations of the general Highway approach?\r\n\r\nNot trying to answer for @seiko2plus, but one thing I've seen regularly is that PRs add new universal intrinsics as they need them, in the same PR. If that gets replaced by having to open an upstream PR to Highway first, waiting for that to get merged, then updating the git submodule (which pulls in other upstream changes too), and then making the relevant change to the NumPy function of interest, that'd be a pain. So to avoid that extra pain, the set of primitives that Highway supports should be pretty complete already (more so than NumPy's universal intrinsics) for NumPy's needs.","> since NEP 38 was written four years ago is that now we have C++ code in the code base, and are gradually increasing that usage. C++-only libraries were not in the picture at all back then\r\n\r\nMakes sense.\r\n\r\n> It'd be great to get more clarify on the trade-offs.\r\n\r\nI think the key point here is avoiding or reducing tech debt; adopting an existing SIMD framework reduces the vast effort of dealing with compiler\/implementation bugs on an ongoing basis. \r\n\r\nxsimd is also a mature framework with over 1500 commits. My understanding, and please correct me if mistaken: it has a complete math library and supports x86+Arm; unfortunately the design predates SVE\/RVV and cannot deal with non-constexpr vector lengths. For SVE, the library would have to be built with advance knowledge of the CPU's vector size.\r\n\r\n> Performance matters, and defaults matter. Significant performance improvements of heavily used NumPy functions are impactful\r\n\r\nAgreed, good defaults would instantly benefit the many users.\r\n\r\n> The dynamic dispatch capabilities in Highway don't seem directly relevant to that, however they are interesting to me, given that that's our current biggest headache with build system migration.\r\n\r\n:) About 7 years ago, our predecessor to Highway used the build system to compile for multiple targets. This was indeed a pain and I wholeheartedly recommend our current within-C++-code approach instead.\r\n\r\n> the set of primitives that Highway supports should be pretty complete already (more so than NumPy's universal intrinsics)\r\n\r\nGot it. Yes, I believe this is the case. Highway does have an extensive set of ops, particularly reorder\/swizzle which are some of the trickier ones to implement. And we are happy to implement new ones ourselves, or collaborate on that.\r\n\r\n> I've seen regularly is that PRs add new universal intrinsics as they need them, in the same PR.\r\n\r\nAh, I see. Yes, this was convenient in the early days of Highway, co-developed with JPEG XL. We're still adding ops, but much less frequently. Perhaps one approach could be to develop numpy features with any new op implemented as scalar code, and #ifdef-ed out once Highway comes with the feature and a corresponing #define. That way it works immediately and we could defer Highway updates until a convenient time. I'm also open to other suggestions?","\r\n> xsimd is also a mature framework with over 1500 commits. My understanding, and please correct me if mistaken: it has a complete math library and supports x86+Arm; unfortunately the design predates SVE\/RVV and cannot deal with non-constexpr vector lengths. For SVE, the library would have to be built with advance knowledge of the CPU's vector size.\r\n\r\nxsimd core developer here: I second your understanding, we only support SVE with size known at compile-time. It's also to be noted that we don't support PPC nor wasm at the moment. We have plan to do so, but nothing is going to happen before end of 2023.\r\n\r\n> :) About 7 years ago, our predecessor to Highway used the build system to compile for multiple targets. This was indeed a pain and I wholeheartedly recommend our current within-C++-code approach instead.\r\n\r\nI'm curious about the implementation details there, i.e. how can this be achieved without compiler support?\r\n\r\n\r\n","> Nice! Is RISC-V also planned?\r\n\r\nWe don't have that many generic intrinsics, but since we have a robust testing unit that can verify the correctness of the implementations, including execution inside qemu, why not? We already have support for IBM zsystem as well.\r\n\r\n> it is easy to underestimate the surprisingly high cost of maintenance.\r\n\r\nI didn't mean to underestimate your efforts. What I meant is that we can't solely rely on numbers as the main factor of choice. I truly appreciate the work you do there.\r\n\r\n> only provides a [small set of operations](https:\/\/en.cppreference.com\/w\/cpp\/experimental\/simd\/simd),\r\n\r\nThe link you mentioned is related to data type contractions\/operators, and it provides additional\r\noperations through function overloading.\r\nYou can refer to the main documentation page [the main documentation page](https:\/\/en.cppreference.com\/w\/cpp\/experimental\/simd) for more details.\r\nWhile it may not cover all the required intrinsics, the current ones are generally sufficient.\r\nAny additional intrinsics can be implemented externally based on the specific needs of each project. Personally, I find this approach favorable because,\r\nin reality, x86 alone has over 6000 instructions, and it is not feasible to map most of them.\r\n\r\n\r\n> isn't the question how many numpy-specific ones remain to be added,\r\n\r\nWhat I meant is that if the purpose of using Google Highway was to facilitate code sharing among the C++ communities,\r\nit would have been better to support the standard (std) instead.\r\n\r\n> Interesting, I thought this would be very difficult given that std::experimental::simd wraps vectors in a class, which is not allowed with SVE\r\n\r\nThe compiler forbids it for convention reasons only, but it is still treated as internal storage like any other SIMD extensions.\r\nSo, you can cheat it and rely on the optimizer instead by duplicating the memory access.\r\nSomething like the following should work without any regression (I haven't tested it):\r\n\r\n```C++\r\ntemplate <typename TLane>\r\nclass Vec;\r\n\r\ntemplate <>\r\nclass Vec<int32_t> {\r\n  public:\r\n    Vec(const svint32_t &vec) {\r\n        svst1_s32(svptrue_b32(), data_, vec);\r\n    }\r\n\r\n    operator svint32_t() const {\r\n        return svld1_s32(svptrue_b32(), data_);\r\n    }\r\n\r\n  private:\r\n    \/\/ Untouchable, neither read nor write, to avoid breaking the series of duplicating.\r\n    \/\/ Rely only on the constructor and the implicit cast above.\r\n    int32_t data_[256\/sizeof(int32_t)];\r\n};\r\n```\r\n\r\n> versus the cost of a custom solution?\r\n\r\nThe main cost is the infrastructure, while the interface itself isn't a big cost.\r\nThe current two solutions have different approaches to handling both static and dynamic dispatching.\r\n\r\nHighway relies heavily on the compiler by using #pragma and target attributes,\r\nwhich makes it highly portable but also less flexible.\r\nIt may have some platform-specific bugs and can be difficult to trace and debug.\r\n\r\nOn the other hand, Universal intrinsics rely on the toolchain, which means it uses compiler flags.\r\nIt is not as portable, but it offers greater flexibility,, and support for any platform. It is also easier to trace and debug.\r\n\r\nCurrently, I am fully focused on migrating our implementation to Meson before the end of this month.\r\nIf we decide to use Highway, perhaps I should abandon the current work and switch to using the Highway dispatcher instead.\r\nIt will require a lot of additional work too, but it will be a more long-term solution.\r\n\r\n> Can you help me understand your concern here? Do you see any limitations of the general Highway approach?\r\n\r\nI think Ralf explained exactly what I meant. Generic intrinsics can be problematic and may lead to\r\nperformance regressions if the intrinsics were not designed to be fair across all architectures,\r\nespecially when it comes to 128-bit extensions.\r\n\r\nThe issue here is finding a middle ground, which can only be achieved during the implementation of the SIMD kernels.\r\nNo matter how comprehensive Highway is, we will always need to extend it to meet the specific needs of NumPy unless\r\nif we dediced to move any future complicated kernels to the upstream.\r\n\r\nIt's important to note that Google has its own policies and interests, and it may decide to drop support for architectures\r\nlike PPC64 at any time. Is there a plan to include zSystem support in Highway as well? It's important to us since we already support it.\r\nCan we expect a guarantee to continue supporting the current architectures regardless of the associated burden?\r\n\r\n> Our LoadInterleaved2 would have to be extended with *Till support (only load partial vectors).\r\n\r\nJust to clarify, pair load\/store intrinsics do not perform any interleave or de-interleave operation.\r\nTheir purpose is to handle complex load and store operations with misaligned strides,\r\nsuch as supporting a 64-bit stride on a 128-bit complex data type. We can discuss in more detail at a later time.\r\n\r\n> Lookup128 could also be added. I am curious whether we would ever have more than 32 elements in the tables?\r\n\r\nThe main purpose of this intrinsic is to provide a mapping to the `_mm512_permutex2var_*` functions, so it perform\r\n32\/bit elements on 32-bit data types and 16 elements on 64-bit datatypes and it was need during the replacement\r\nof intel SVML.\r\n\r\n> xsimd is also a mature framework with over 1500 commits. My understanding, and please correct me if mistaken: it has a complete math library and supports x86+Arm;\r\n\r\nIf I'm not mistaken the xsimd math operations are based on boost\/simd, but I don't think they will meet our requirements in terms of\r\nsupported ranges and precision. So far, it seems that SLEEF is the only viable option, along with Intel SVML, that is available to us.\r\n\r\n> :) About 7 years ago, our predecessor to Highway used the build system to compile for multiple targets.\r\n> This was indeed a pain and I wholeheartedly recommend our current within-C++-code approach instead.\r\n\r\nUsing a build system to compile for multiple targets can be more efficient, but it may not be as portable as relying on compiler attributes.\r\nIt's another tradeoff to consider I guess.\r\n\r\n\r\n\r\n\r\n","@serge-sans-paille thanks for confirming.\r\n\r\n> I'm curious about the implementation details there, i.e. how can this be achieved without compiler support?\r\n\r\nThere is some documentation of this in https:\/\/github.com\/google\/highway\/blob\/master\/g3doc\/quick_reference.md#static-vs-dynamic-dispatch and https:\/\/github.com\/google\/highway\/blob\/master\/g3doc\/faq.md#boilerplate. In short, we re-include a source file after changing the codegen via #pragma target, supported by clang\/gcc.\r\n\r\n> What I meant is that we can't solely rely on numbers as the main factor of choice. I truly appreciate the work you do there.\r\n\r\n:)\r\n\r\n> it provides additional operations through function overloading.\r\n\r\nAh, right. But those are still only a handful. Many are still missing, e.g. interleaved load\/store, compress\/expand, lzcnt, saturated add\/sub, 128-bit compare\/minmax, fixed-point mul, scatter\/gather(!) and the various swizzle\/reordering.\r\n\r\n> What I meant is that if the purpose of using Google Highway was to facilitate code sharing among the C++ communities, it would have been better to support the standard (std) instead.\r\n\r\nI understand, but ISO standarization would be a very heavyweight and time-intensive process for adding any new operations. We concluded that the approach being standardized (Vc library, designed around 2010) does not meet our needs because it is unsuitable for RVV. Also, AFAIK it is not supported on MSVC, see https:\/\/github.com\/microsoft\/STL\/discussions\/1714 .\r\n\r\n> So, you can cheat it and rely on the optimizer instead by duplicating the memory access.\r\n\r\nInteresting, it's worth a try because clang is pretty good about optimizing load\/store. But unfortunately both it and gcc generate very poor code: https:\/\/gcc.godbolt.org\/z\/PTP59PGsf\r\nAnd this approach would definitely not be suitable for RVV, where vectors can be as large as 64KiB.\r\n\r\n> The main cost is the infrastructure, while the interface itself isn't a big cost.\r\n\r\nhm, we spent quite a while studying NEON \/ x86 and VSX before deciding that a portable interface is feasible and desirable. Coming up with interfaces such as ReorderWidenMulAccumulate that works equally well on all platforms is also nontrivial. By infrastructure, do you mean CI? That is indeed costly to set up, but it's a one-time thing. My experience has been that ongoing maintenance is more costly.\r\n\r\n> Highway relies heavily on the compiler by using #pragma and target attributes, which makes it highly portable but also less flexible.\r\n\r\nDo you have an example of why or how this would be less flexible?\r\n\r\n> Universal intrinsics rely on the toolchain, which means it uses compiler flags.\r\n\r\nWe had previously used that approach internally but concluded it is too dangerous (caused production outages). Compiling entire translation units with -mavx2 risks ODR violations which can crash the binary. Consider a TU which includes a header defining some standard library function. The linker can choose any of the (allegedly equivalent) versions of this function for the various TUs. But if it happens to choose the one from the file compiled with -mavx2, then AVX2 code may leak into the binary and crash older CPUs. This approach would require extreme discipline to avoid including any header which may define functions.\r\n\r\n> Generic intrinsics can be problematic and may lead to performance regressions if the intrinsics were not designed to be fair across all architectures, especially when it comes to 128-bit extensions.\r\n\r\nGood point. We do indeed aim for fairness, and ops are only added when they do not cause performance cliffs (penalizing any platform).\r\n\r\n> No matter how comprehensive Highway is, we will always need to extend it to meet the specific needs of NumPy\r\n\r\nYes, that seems likely. As mentioned, we are open to collaborating or implementing new ops, and there is also a path for using a temporary op implementation until added to Highway and the submodule is updated.\r\n\r\n> It's important to note that Google has its own policies and interests, and it may decide to drop support for architectures like PPC64 at any time.\r\n\r\nFair point. FYI there are also substantial contributions by non-Googlers. See below on support policy.\r\n\r\n> Is there a plan to include zSystem support in Highway as well? It's important to us since we already support it.\r\n\r\nWe haven't yet had a use case for it. If numpy does decide to potentially use Highway, we can commit to supporting an existing implementation of zSystem (perhaps based on yours?). If it cross-compiles with clang and can be tested via standard QEMU, it can go into our CI. If it cross-compiles via clang\/gcc and can be tested with a new QEMU (possibly with extra flags), then we can support it via manual testing before each release.\r\n\r\n> Can we expect a guarantee to continue supporting the current architectures regardless of the associated burden?\r\n\r\nGood question. We can add a policy to the readme, saying targets will remain supported as long as they compile\/run in QEMU as mentioned above. Would that work for you?\r\n\r\n> Just to clarify, pair load\/store intrinsics do not perform any interleave or de-interleave operation.\r\n> Their purpose is to handle complex load and store operations with misaligned strides\r\n\r\nAh, I see, thanks for correcting my misunderstanding. It seems that these functions operate in units of 2 lanes.\r\n\r\n> The main purpose of this intrinsic is to provide a mapping to the _mm512_permutex2var_* functions, so it perform 32\/bit elements on 32-bit data types and 16 elements on 64-bit datatypes and it was need during the replacement of intel SVML.\r\n\r\nGot it. No problem, we can add such an intrinsic, even if it is a bit niche.\r\n\r\n> So far, it seems that SLEEF is the only viable option, along with Intel SVML, that is available to us.\r\n\r\nSLEEF is nice software and even supports SVE, though not yet RVV. I believe it is possible for Highway and SLEEF to coexist. To get the full benefit of runtime dispatch, we can also consider swapping out SLEEF's portability layer with Highway.\r\n","> > So far, it seems that SLEEF is the only viable option, along with Intel SVML, that is available to us.\r\n> \r\n> SLEEF is nice software and even supports SVE, though not yet RVV. I believe it is possible for Highway and SLEEF to coexist. To get the full benefit of runtime dispatch, we can also consider swapping out SLEEF's portability layer with Highway.\r\n\r\nSLEEF is no longer maintained (last commit April 2021), so I'd suggest taking things that are useful from SLEEF is fine, but other than that it seems no longer very relevant. Even before the flow of commits stopped, I remember it was a real headache to get SLEEF fixes that PyTorch needed merged.","> Interesting, it's worth a try because clang is pretty good about optimizing load\/store. But unfortunately both it and gcc generate very poor code: https:\/\/gcc.godbolt.org\/z\/PTP59PGsf\r\nAnd this approach would definitely not be suitable for RVV, where vectors can be as large as 64KiB.\r\n\r\nI made some changes to your example by modifying the default behavior of the move, and it works quite well on GCC. You can take a look at [godbolt link](https:\/\/gcc.godbolt.org\/z\/hs14YnbcG). It appears that this trick allows frameworks like xsimd or Vec to support SVE without encountering any issues.\r\n\r\n>  And this approach would definitely not be suitable for RVV, where vectors can be as large as 64KiB.\r\n \r\nIt is still possible for this trick to work. Perhaps using a dummy pointer or inline assembly could be applicable. I will consider a way to address this in the future. However, this example is not relevant to our current situation. I provided it as an example for frameworks that rely on class objects.\r\n\r\n> By infrastructure, do you mean CI? That is indeed costly to set up, but it's a one-time thing. My experience has been that ongoing maintenance is more costly.\r\n\r\nNot only in the CI, but what I specifically meant was the dispatching mechanism as part of our build system. It allows end users to disable or enable features at both compile-time and runtime. The testing unit also imposes a significant cost on us, especially because we had to implement a C Python module specifically for that purpose.\r\n\r\n> Do you have an example of why or how this would be less flexible?\r\n\r\nI can begin by addressing the first issue, which involves the MSVC compiler. We need to determine how to pass the `\/AVX512` flag, to instruct the compiler to optimize operations for `AVX512` also to avoid any possible bugs on 32-bit for example same issue for `AVX2`.\r\n\r\nRegarding the second issue, we need to find a solution to manage a large number of CPU features. I kindly request you to review the currently supported features within our dispatching mechanism at https:\/\/numpy.org\/devdocs\/reference\/simd\/build-options.html#supported-features.\r\n\r\nAs for the third issue, we might encounter challenges with dispatching a function pointer rather than detecting and calling routine on each time the kernel called note that it should be compatible with the C code since we currently initialize the the optimized function pointers at load of numpy from C code.\r\n\r\nThis for now, need more time to think deeply about any other possible issues\r\n\r\n> We had previously used that approach internally but concluded it is too dangerous (caused production outages). Compiling entire translation units with -mavx2 risks ODR violations which can crash the binary.\r\n\r\nWe just use a unique namespace for each target here an example from the current C++ wrapper:\r\n```C\r\n\/\/\/ the following macro just add prefix for the current target\r\nNPY_CPU_DISPATCH_CURFX(X) \r\n```\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/0f9fced5dfdb54451cfd917ec67f84e00f2c34c2\/numpy\/core\/src\/common\/simd\/simd.hpp#L6\r\n\r\nThen we use it again as simd alias here again after we getting done from the definitions:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/0f9fced5dfdb54451cfd917ec67f84e00f2c34c2\/numpy\/core\/src\/common\/simd\/simd.hpp#L147-L149\r\n\r\nSimilar trick been used by OpenCV and still works for years.\r\n\r\n> Consider a TU which includes a header defining some standard library function. The linker can choose any of the (allegedly equivalent) versions of this function for the various TUs\r\n\r\nI understand your point. To avoid the issue you mentioned, it's advisable to keep your kernels inside the source code and refrain from defining non-inlined functions within cache-able objects, however this cases almost impossible to face . Also its important to mention that by passing compiler flags for the entire unit allow us to get benefit from compiler optimization, which has its own advantages.\r\n\r\n> We haven't yet had a use case for it. If numpy does decide to potentially use Highway, we can commit to supporting an existing implementation of zSystem (perhaps based on yours?).\r\n\r\nI'm more than welcome to cooperate and add support for zSystem, even if we decide not to use Highway.\r\n\r\n > Good question. We can add a policy to the readme, saying targets will remain supported as long as they compile\/run in QEMU as mentioned above. Would that work for you?\r\n\r\nSounds great! I think IBM can provide a server for testing e.g. linuxone or count on Travis CI, similar to what we do.\r\n\r\n\r\nEDIT: I have updated the godbolt link to keep the Highway code as-is and still the trick works without any issues.","I had a browse through the JPEG XL implementation and how it uses Highway - it reads quite pleasantly. A lot of that is C++ vs. C though, the C++ code in gh-21057 also is a huge improvement. E.g., this kind of C code:\r\n```C\r\nnpyv_@sfx@  a5 = npyv_load_@sfx@(src1 + npyv_nlanes_@sfx@ * 4);\r\n```\r\nturns into this C++ code:\r\n```cpp\r\nauto a5 = op(Load(src1 + nlanes * 4));\r\n```\r\nEither way it seems like we get sane names of ops:)\r\n\r\n","> turns into this C++ code:\r\n\r\n> ```C\r\n>> auto a5 = op(Load(src1 + nlanes * 4));\r\n> ```\r\njust:\r\n```C++\r\nauto a5 = Load(src1 + nlanes * 4);\r\n```\r\n`op` is the operation derived from object function.\r\n\r\nOne of the advantages of gh-21057 over Highway is that it doesn't require tags for most of its functionality. However, the most important advantage is the testing unit.\r\n\r\nThe new testing unit, located at gh-24069, is a complete re-implementation of the current C module. It allows you to access the C++ intrinsics with almost the same signature, but from Python. This is a great way not only for testing, but also for designing the SIMD kernels.\r\n\r\nOur current implementation:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/core\/tests\/test_simd.py\r\n\r\nAnd the new one:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/5184296a7df7d9d4fcaf3d667831d8314a6eb3a2\/numpy\/core\/src\/common\/simd\/test\/tests\/test_simd.py\r\n\r\nIt allows us to extensively test numerous intrinsics with different data types without writing a lot of code and provide pretty trace messages, unlike the Highway testing unit:\r\nhttps:\/\/github.com\/google\/highway\/tree\/master\/hwy\/tests\r\n\r\n","> SLEEF is no longer maintained (last commit April 2021), so I'd suggest taking things that are useful from SLEEF is fine\r\n\r\nGot it. Yes, we can copy\/port code then.\r\n\r\n> by modifying the default behavior of the move, and it works quite well on GCC.\r\n\r\nOh, surprising that it was just a matter of providing a move ctor. Nice!\r\n\r\n> I can begin by addressing the first issue, which involves the MSVC compiler. We need to determine how to pass the \/AVX512 flag\r\n\r\nAs discussed yesterday, it seems the simplest\/safest path for Windows is to use clang-cl and\/or MinGW.\r\n\r\n> Regarding the second issue, we need to find a solution to manage a large number of CPU features. I kindly request you to review the currently supported features within our dispatching mechanism at https:\/\/numpy.org\/devdocs\/reference\/simd\/build-options.html#supported-features.\r\n\r\nThat's a long list :) Highway currently targets the following 'clusters' of features:\r\n\r\n* SSE2 (any x64)\r\n* SSSE3 (~Intel Core)\r\n* SSE4 (~Nehalem)\r\n* AVX2 (~Haswell)\r\n* AVX3 (~Skylake)\r\n* AVX3_DL (~Icelake)\r\n* AVX3_ZEN4 (Zen4).\r\n\r\nWe will likely add AVX3_SPR soon for SapphireRapids and its F16\/AMX.\r\n\r\n> we might encounter challenges with dispatching a function pointer rather than detecting and calling routine on each time the kernel called note that it should be compatible with the C code since we currently initialize the the optimized function pointers at load of numpy from C code.\r\n\r\nAh, now I understand after the meeting. We will also be able to obtain function pointers from Highway, instead of calling them.\r\n\r\n> We just use a unique namespace for each target\r\n\r\nRight, this is also what Highway does. To be concrete, here is an example of the problem:\r\n```\r\n#include <cmath>  \/\/ unsafe\r\nnamespace np::NPY_CPU_DISPATCH_CURFX { \/* safe functions because in namespace *\/ }\r\n```\r\nAs you mention, we have to be very careful to not define any functions, i.e. not include any headers.\r\nThis is very difficult, things creep in over time.\r\n\r\n> I'm more than welcome to cooperate and add support for zSystem, even if we decide not to use Highway.\r\n\r\nGreat! We'd welcome a pull request. Let's start with a minimum skeleton. I'd recommend searching for HWY_PPC8; inserting a new target will require updates at each of those spots.\r\n\r\n> Sounds great!\r\n\r\nGreat, we will add such a statement.\r\n\r\nFYI on test parametrization, I see:\r\n```\r\n@mark.parametrize('TLane', [\r\n    uint8_t, int8_t, uint16_t, int16_t,\r\n    uint32_t, int32_t, uint64_t, int64_t,\r\n    float_, double\r\n])\r\n```\r\nThis could probably be shortened by a helper function similar to the Highway tests' ForAllTypes, right?\r\nI'm curious what kind of tracing you like to see in tests?","This issue and the productive discussion in the NumPy community meeting on Wednesday are included in the NEP draft on this topic: gh-24138.\r\n\r\nI suggest that we keep technical discussion on C++, individual intrinsics, etc. on this issue, and the high-level more social\/impact\/decision-focused discussion on gh-24138.","Sounds good. For anyone interested, here's how one can add a new target to Highway: https:\/\/github.com\/google\/highway\/pull\/1532.","> I suggest that we keep technical discussion on C++, individual intrinsics, etc. on this issue, and the high-level more social\/impact\/decision-focused discussion on https:\/\/github.com\/numpy\/numpy\/pull\/24138.\r\n\r\nI agree, sound good to me.\r\n\r\n> Got it. Yes, we can copy\/port code then.\r\n\r\nToo hard to read, I couldn't fully understand many kernels there.\r\n\r\n> Oh, surprising that it was just a matter of providing a move ctor. Nice!\r\n\r\nYeah, I can't wait to see it being used by `xsimd` or even the `std`. Hopefully, they will give me some credit. @serge-sans-paille, have you tried a similar solution before that supports variable sizes of SVE instead of only fixed sizes?\r\n\r\n> Ah, now I understand after the meeting. We will also be able to obtain function pointers from Highway, instead of calling them.\r\n\r\nIs there an example available for dispatching an exported C++ function pointer from a C function?\r\n\r\n> As discussed yesterday, it seems the simplest\/safest path for Windows is to use clang-cl and\/or MinGW.\r\n\r\nAre there any other platforms besides MSVC that are experiencing similar issues?\r\n\r\n> Right, this is also what Highway does. To be concrete, here is an example of the problem:\r\n> #include <cmath>  \/\/ unsafe\r\n\r\nIt is safe as long as the calls to cmath functions are being used within the source code within static functions or anonymous namespaces. I have not encountered such an issue with OpenCV for many years, so I believe it should be fine. Additionally, the potential performance improvement achieved by using compiler flags is also significant. It provides a more global approach to SIMD optimization throughout the entire sources, not limited to just the dispatched functions. In the future, it is possible that `AVX2` on `x86` for example may become the minimum baseline requirement, and some users may rely on our baseline functionality to confidently increase their minimum feature requirements or reduced if they still need to support old hardware.\r\n\r\n> Great, we will add such a statement.\r\n\r\nIs there a possibility for modify this policy in the future? Who will be responsible for maintaining the continuous integration (CI)? Currently, it appears that only `x86` is supported with a limited number of compilers working on Linux only.\r\n\r\n> This could probably be shortened by a helper function similar to the Highway tests' ForAllTypes, right?\r\n\r\nThis was left as it is for readability purposes because not all intrinsics support all data types. Some tests specifically require float or double data types, while others exclusively require all or few integer data types.\r\n\r\n>  I'm curious what kind of tracing you like to see in tests?\r\n\r\nYou can't imagine how much time I've saved by testing the intrinsics using Python, particularly IPython, which allows me to debug errors on the fly. In the past, when I was dealing with Google Testing in OpenCV, I used to face difficulties. However, Pytest has proven to be an amazing tool, especially with its visualization capabilities and incredible fixtures. It did take me some time to understand it, but it is definitely worth every second. I highly recommend giving it a try and second try if you have try it before :).\r\n\r\nAdditionally, during my exploration of the testing unit of Highway, I observed that there is no safeguard against compiler optimization. Is my understanding correct? How do you ensure that you are testing the targeted intrinsic against the expected data without any interference from static optimization?\r\n\r\n\r\nEDIT: I have removed the part of comment related to defining C macros, as it only exists in universal intrinsics. Highway has done a good job in this matter, and I apologize for my confusion since I didn't fully checked the full sources. I will address this issue in a separate comment after refactor it as downside of the current C implementation of universal intrinics.\r\n","One significant issue that exists in the current C implementation of universal intrinsics should be addresed in #24138 , which is our reliance on C macros for implementing these intrinsics, too ugly makes any contributor almost suffer only the maintainer knows what's going on :xD.\r\n\r\nI have been eagerly anticipating the moment when NumPy support C++ and, specifically, C++17, as it would allow us to eliminate these cumbersome macros. I believe this issue will continue to escalate with the expansion of the interfaces until it becomes uncontrollable, so we gonna have to deal with it.\r\n\r\nThe good thing is that the current universal intrinsics are not extensive, requiring a maximum of one month of work potentially to eliminate these kind of macros or at least most of them, Additionally, this shift will enable us to expose SSE, AVX2, and AVX512 functionalities together or NEON\/ASIMD next to SVE in different namespaces such as simd128, simd256, following a similar approach to OpenCV rather than using tags or almost similar too vector class library. Since the current option is to use configuration macros to reduce the number of lanes, e.g. NPY_SIMD_FORCE_128, I think it is related to the comment you made [here](https:\/\/github.com\/numpy\/numpy\/pull\/24138#discussion_r1255776293).\r\n\r\nHowever, my plan is to declare all the universal intrinsics as template functions, which I have already done with gh-21057, then we count on SFINAE, and constexpr to define these intrinsics.\r\nThis transition will occur once we completely replace the current C code. Additionally, some architectures like ppc64 and s390x already have a friendly interface due to the imposition of Altivec, so no extra efforts are required for them. This approach is not entirely new, as xsimd also highway has already adopted a similar approach. I hope to further improve upon it if that of course of we decided to continue using our universal intrinics.","Spent a bit of time prototyping today, just to illustrate how easily it drops into the existing framework (note it uses the C interface, none of the C++ magic @seiko2plus has put together so it can definitely be cleaned further) with Meson and despite the memory loads being wrong it does function. I also did the environment variables, which was mostly deciphering the old logic, hopefully it's a lot prettier with Highway:\r\n\r\nSee branch diff: https:\/\/github.com\/numpy\/numpy\/compare\/main...Mousius:numpy:hwy-spike?expand=1\r\n\r\nDiffering outputs for the various feature detection lists:\r\n```\r\nFeatures:\r\nCurrent: {'MMX': False, 'SSE': False, 'SSE2': False, 'SSE3': False, 'SSSE3': False, 'SSE41': False, 'POPCNT': False, 'SSE42': False, 'AVX': False, 'F16C': False, 'XOP': False, 'FMA4': False, 'FMA3': False, 'AVX2': False, 'AVX512F': False, 'AVX512CD': False, 'AVX512ER': False, 'AVX512PF': False, 'AVX5124FMAPS': False, 'AVX5124VNNIW': False, 'AVX512VPOPCNTDQ': False, 'AVX512VL': False, 'AVX512BW': False, 'AVX512DQ': False, 'AVX512VNNI': False, 'AVX512IFMA': False, 'AVX512VBMI': False, 'AVX512VBMI2': False, 'AVX512BITALG': False, 'AVX512FP16': False, 'AVX512_KNL': False, 'AVX512_KNM': False, 'AVX512_SKX': False, 'AVX512_CLX': False, 'AVX512_CNL': False, 'AVX512_ICL': False, 'AVX512_SPR': False, 'VSX': False, 'VSX2': False, 'VSX3': False, 'VSX4': False, 'VX': False, 'VXE': False, 'VXE2': False, 'NEON': False, 'NEON_FP16': False, 'NEON_VFPV4': False, 'ASIMD': False, 'FPHP': False, 'ASIMDHP': False, 'ASIMDDP': False, 'ASIMDFHM': False}\r\nHighway: {'SVE2_128': False, 'SVE_256': False, 'SVE2': False, 'SVE': False, 'NEON': True, 'NEON_WITHOUT_AES': True, 'SCALAR': True}\r\nBaseline: \r\nCurrent: ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD']\r\nHighway: ['NEON_WITHOUT_AES', 'SCALAR']\r\nDispatch: \r\nCurrent: ['ASIMDHP', 'ASIMDDP', 'ASIMDFHM']\r\nHighway: ['NEON']\r\n```","> Too hard to read, I couldn't fully understand many kernels there.\r\n\r\nIt is indeed quite hard to read. But we can consider mechanically porting via search+replace of their op names?\r\n\r\n> Is there an example available for dispatching an exported C++ function pointer from a C function?\r\n\r\nNot yet, it's on my TODO, thanks for bringing it up.\r\n\r\n> Are there any other platforms besides MSVC that are experiencing similar issues?\r\n\r\nI haven't seen, or had reports of, issues on other compilers.\r\n\r\n> It is safe as long as the calls to cmath functions are being used within the source code within static functions\r\n\r\nhm, my understanding is that we can get into trouble if any function defined in this TU is not inlined. That seems to depend on luck and compiler flags (also much more likely in fast\/debug builds)? I understand you haven't encountered any such issue, but we have :\/\r\n\r\n> it is possible that AVX2 on x86 for example may become the minimum baseline requirement, and some users may rely on our baseline functionality to confidently increase their minimum feature requirements\r\n\r\nThat would be nice. FYI it is possible to specify -march=haswell -maes and Highway will use HWY_AVX2 as the baseline. Or even #define HWY_BASELINE_TARGETS HWY_AVX2 and then older targets will at least be skipped.\r\n\r\n> Is there a possibility for modify this policy in the future?\r\n\r\nI suppose it could change, but FYI it has been approved by my manager.\r\n\r\n> Who will be responsible for maintaining the continuous integration (CI)? Currently, it appears that only x86 is supported with a limited number of compilers working on Linux only.\r\n\r\nOur internal CI tests wasm, msvc opt+dbg, clang-cl opt+dbg, arm7 opt+dbg, arm8 opt+dbg, sve, sve2, various x86 targets and sanitizers, and several Android+iOS. POWER is currently manually tested before releases.\r\n\r\nAny thoughts on how we should add more to the Github actions?\r\n\r\n> However, Pytest has proven to be an amazing tool, especially with its visualization capabilities and incredible fixtures.\r\n\r\nIt sounds useful :)\r\n\r\n> I observed that there is no safeguard against compiler optimization. Is my understanding correct? How do you ensure that you are testing the targeted intrinsic against the expected data without any interference from static optimization?\r\n\r\nYes and no. Thanks for pointing that out. I recently observed that the compiler is able to see through math functions and return a constexpr result. We can use Unpredictable1() as an input to defeat this. But we also check debug-mode builds. Thus we are comparing constexpr evaluation against unoptimized calls to the intrinsics, which seems useful and sufficient, right?\r\n\r\n> rather than using tags\r\n\r\nFYI the purpose of the tags is not (only) to distinguish between AVX\/SSE versions. It also communicates user-specified limits on the vector size that we want, similar to the NPY_SIMD_FORCE_128 you mention.\r\nIsn't it useful to vary that limit per algorithm, or even mix full and half vectors in the same kernel?\r\n\r\n> Spent a bit of time prototyping today, just to illustrate how easily it drops into the existing framework\r\n\r\nVery cool to see your prototype with enable\/disable and an example kernel! Were there any pain points or things you'd like to see improved?","Just a comment here: the sleef library is in the process to change the maintainers, as noticed here: [SLEEF is changing maintainer](https:\/\/github.com\/shibatch\/sleef\/discussions\/472). So there is still hope for the excellent sleef library."],"labels":["component: SIMD"]},{"title":"key missing `spin` commands for developer workflow","body":"After the switch to Meson by default, the contributor workflow got a bit disrupted. The key things to address quickly are:\r\n\r\n- [x] Building the docs. There is no `spin doc`.\r\n- [x] Running the benchmarks. There is no `spin bench` (most important), and once that's there we also need `bench --compare` to compare two branches. @rdevulap brought this up as a blocker on Slack already.\r\n- [x] Testing: `spin test` should not run the slow tests by default; for further customization to match `runtests.py`\/`dev.py` (e.g., `-s`, `-t`, etc.) see https:\/\/github.com\/scientific-python\/spin\/issues\/40.\r\n\r\nThe above are the most important ones, and should be enough to fix the immediate pain points (Cc @stefanv).\r\n\r\nFor the rest, there's still a bit of a gap between NumPy's `spin` and SciPy's `dev.py`, see `--help` on the top-level and individual commands. E.g.,\r\n\r\n`spin --help`:\r\n\r\n```\r\n$ spin --help\r\nUsage: spin [OPTIONS] COMMAND\r\n            [ARGS]...\r\n\r\n  Developer tool for numpy\r\n\r\nOptions:\r\n  --help  Show this message and exit.\r\n\r\nBuild:\r\n  build  \ud83d\udd27 Build package with Meson\/ninja and install\r\n  test   \ud83d\udd27 Run tests\r\n\r\nEnvironments:\r\n  shell    \ud83d\udcbb Launch shell with PYTHONPATH set\r\n  ipython  \ud83d\udcbb Launch IPython shell with PYTHONPATH set\r\n  python   \ud83d\udc0d Launch Python shell with PYTHONPATH set\r\n\r\n```\r\n\r\n`python dev.py --help`\r\n\r\n```\r\n$ python dev.py --help\r\n                                                                                                   \r\n Usage: dev.py [OPTIONS] COMMAND [ARGS]...                                                         \r\n                                                                                                   \r\n Developer Tool for SciPy                                                                          \r\n Commands that require a built\/installed instance are marked with \ud83d\udd27.                              \r\n                                                                                                   \r\n python dev.py --build-dir my-build test -s stats                                                  \r\n                                                                                                   \r\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 --help                             Show this message and exit.                                  \u2502\r\n\u2502 --build-dir           BUILD_DIR    \ud83d\udd27 Relative path to the build directory.                     \u2502\r\n\u2502                                    [default: build]                                             \u2502\r\n\u2502 --no-build        -n               \ud83d\udd27 Do not build the project (note event python only          \u2502\r\n\u2502                                    modification require build).                                 \u2502\r\n\u2502 --install-prefix      INSTALL_DIR                                                               \u2502\r\n\u2502                                    \ud83d\udd27 Relative path to the install directory. Default is        \u2502\r\n\u2502                                    -install.                                                    \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 build & testing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 build  \ud83d\udd27 Build & install package on path.                                                      \u2502\r\n\u2502 test   \ud83d\udd27 Run tests.                                                                            \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 static checkers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 lint  \ud83d\udca8 Run linter on modified files and check for disallowed Unicode characters and           \u2502\r\n\u2502       possibly-invalid test names.                                                              \u2502\r\n\u2502 mypy  \ud83d\udd27 Run mypy on the codebase.                                                              \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 environments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 shell    \ud83d\udd27 Start Unix shell with PYTHONPATH set.                                               \u2502\r\n\u2502 python   \ud83d\udd27 Start a Python shell with PYTHONPATH set.                                           \u2502\r\n\u2502 ipython  \ud83d\udd27 Start IPython shell with PYTHONPATH set.                                            \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 documentation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 doc             \ud83d\udd27 Build documentation.                                                         \u2502\r\n\u2502 refguide-check  \ud83d\udd27 Run refguide check.                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 release \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 notes    \ud83d\udcd2 Release notes and log generation.                                                   \u2502\r\n\u2502 authors  \ud83d\udcd2 Generate list of authors who contributed within revision interval.                  \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n\u256d\u2500 benchmarking \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 bench  \ud83d\udd27 Run benchmarks.                                                                       \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\n```\r\n","comments":["I think `spin test -s` is still missing. ~spin test~ `spin bench` and `spin doc` seem complete?\r\nThe others are still missing:\r\n- [x] `spin lint`\r\n- [x] `spin mypy`\r\n- [ ] `spin refguide-check` although perhaps energy would be better spent on getting doctest to work\r\n- [x] `spin notes`\r\n- [x] `spin authors`\r\n\r\nI will try to sneak `spin test -m full` into another PR where the diff is (still missing a doc change)\r\n```diff\r\ndiff --git a\/.spin\/cmds.py b\/.spin\/cmds.py\r\nindex 3476b043a..53407fe2d 100644\r\n--- a\/.spin\/cmds.py\r\n+++ b\/.spin\/cmds.py\r\n@@ -126,7 +126,8 @@ def test(ctx, pytest_args, markexpr, n_jobs, tests, verbose):\r\n         pytest_args = ('numpy',)\r\n \r\n     if '-m' not in pytest_args:\r\n-        pytest_args = ('-m', markexpr) + pytest_args\r\n+        if markexpr != \"full\":\r\n+            pytest_args = ('-m', markexpr) + pytest_args\r\n \r\n     if (n_jobs != \"1\") and ('-n' not in pytest_args):\r\n         pytest_args = ('-n', str(n_jobs)) + pytest_args\r\n```","Thanks @stefanv for the hard work, and @mattip for following up. This is starting to look much better.\r\n\r\nI tested `spin docs`, and immediately had to `Ctrl-C` it, because it started running `$ pip install -q -r doc_requirements.txt`. It's never okay I think for a development command to invoke `pip`, and in this case it even does so in a conda env which is likely to break that env. What `docs` should do instead is trigger `build` and then run `make html`.","@charris please add anything you feel is missing here","I'll additionally make changes to the CI to use the new spin commands and flags.","Thanks @ganesh-k13, that sounds good."],"labels":["16 - Development"]},{"title":"BUG: assert_equal does not distinguish scalars from 0-dim arrays","body":"### Describe the issue:\r\n\r\nI suspect this is part of the larger issue of consistency (resp. lack thereof) around scalars vs. 0-dim arrays.\r\n\r\nBut it's worse in this case, because now any user of the various `assert_*` functions from `numpy.testing` will be themselves unable to ensure that the behaviour is as intended.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> from numpy.testing import assert_equal\r\n>>> assert_equal(1.0, np.array(1, dtype=np.int64))  # passes?!?\r\n>>> assert_equal(1.0, np.array(2, dtype=np.int64))  # changed value -> fails\r\nTraceback (most recent call last):\r\n[...]\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.25.0\r\n3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]\r\n```\r\n\r\n```\r\n>>> print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.25.0',\r\n  'python': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) '\r\n            '[MSC v.1936 64 bit (AMD64)]',\r\n  'uname': uname_result(system='Windows', release='10', version='10.0.22621', machine='AMD64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n```\r\n\r\n### Context for the issue:\r\n\r\n`numpy.testing` is a key component for ensuring people can build their own libraries on top of numpy. \"equal\" should actually mean equal type, shape, content, etc. (as it usually does...), also for 0-dim arrays.","comments":["Workaround is now:\r\n```python\r\n        # assert_equal does not distinguish scalars and 0-dim arrays of the same value\r\n        def assert_really_equal(x, y):\r\n            assert type(x) == type(y), f\"types not equal: {type(x)}, {type(y)}\"\r\n            assert_equal(x, y)\r\n```\r\n\r\nNeedless to say, this feels a bit ridiculous.","FYI: In NumPy 1.24.0, the `strict` parameter [was added to](https:\/\/github.com\/numpy\/numpy\/pull\/21595) [`assert_array_equal`](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.testing.assert_array_equal.html), but it will still consider a Python or NumPy scalar equal to an array scalar of size 0, e.g. `assert_array_equal(np.int64(0), np.array(0, dtype=np.int64), strict=True)` will pass, because they have the same shape and dtype.  Perhaps it should be even stricter, and fail in that case?","My guess is that there are a giant amount of tests which do `assert_equal(arr, [1, 2, 3])` including in SciPy.\r\n\r\nThe strict parameter could be even stricter, I have no opinion on whether it should check types; maybe types are just meant to be checked explicitly.","I think what's confusing is that other `assert_*` can turn out quite strict in many other aspects, which is why there's `assert_allclose` etc., which go a bit less hard. I remember[^1] that failed on different types (yet my example above compares 1.0 & 1 successfully). That makes it seem even more of a surprise if big differences in (container) types are suddenly ignored.\r\n\r\n[^1]: though that wasn't `assert_equal` I guess; I just tried and fail to reproduce that\r\n\r\nI don't have strong opinions on how this should be implemented, but I guess a `strict=` keyword would make sense (especially as it could be similarly added to other methods; i.e. it'd be well-defined what `assert_allclose(..., strict=True)` would mean: up to a defined tolerance, but no type variance).\r\n\r\nI also think that making the existing `strict=True` even stricter doesn't even count as \"breakage\", because whoever's using that from 1.24 is _already_ explicitly opting into \"the default is too loose for me\".","I just want to point out that this is true:\r\n\r\n```\r\nIn [2]: np.array(1, dtype=np.int64) == 1.0\r\nOut[2]: True\r\n```\r\n\r\nSo it would be very surprising to me if `assert_equal(np.array(1, dtype=np.int64), 1.0)` were `False`. That said, making `struct=True` check the types makes sense to me. Searching github there don't seem to be many consumers of that keyword yet, so making the semantics more strict might be possible without a lot of downstream breakage.","> I just want to point out that this is true:\r\n\r\nAbsolutely, because the requirements of what most people need for processing their _data_  is a by-value comparison.\r\n\r\nHowever, for testing (which `numpy.testing.assert_*` fall under quite clearly), you often need to check more tightly, because it's necessary - for example - to maintain other invariants (like using the array API on the result). My argument is: if numpy doesn't offer a strict(er) version, then it becomes really hard to test _at all_.","There are two things to keep in mind:\r\n* Sentiment about type strictness increased a lot over time and is also not universally agreed upon (the opposite bug report wouldn't surprise me if changed).\r\n* I suspect that NumPy and SciPy each would have hundreds of test failures alone if this was changed.  (Yes, they are easy, but is it worth it?)\r\n\r\nWe discussed `strict=` addition and tightening that kwarg which I suspect we would get consensus on.  Maybe you can suggest other new API also?\r\nBut attempting a noisy transition seems like asking too much from other projects.","> But attempting a noisy transition seems like asking too much from other projects.\r\n\r\nTo be clear, I'm not suggesting a changing of the default. Just adding _some_ way to be strict about this. \r\n\r\n> Maybe you can suggest other new API also?\r\n\r\nFor me it'd be completely fine to (re-)use a `strict=` keyword for that. I'd also be fine with `assert_strict_equal`, but I kinda prefer the keyword because it could also be added to other methods like `assert_allclose`.\r\n\r\n"],"labels":["01 - Enhancement"]},{"title":"BUG: numpy.histogram should raise an error for string arrays","body":"### Describe the issue:\n\n`numpy.histogram` returns negative values for some bins when the data is provided as list of numerical strings.\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\ndata =  [100]*10 + [200]*14\r\nbins = list(range(0, 2050, 50))\r\n\r\n# the string version\r\nprint(list(np.histogram([str(x) for x in data], bins=bins)[0]))\r\n# [24, -24, 10, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14]\r\n\r\n# the int version\r\nprint(list(np.histogram([int(x) for x in data], bins=bins)[0]))\r\n# [0, 0, 10, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n1.21.5\r\n3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\r\n\n\n### Context for the issue:\n\nI post this issue to throw light on the fact that some issue might lie deeper within numpy and cause other bugs even if the `numpy.histogram` function specificly has never been designed to work with a list of strings.\r\nThe reason this is significant IMHO is that the result is quite similar yet faulty instead of raising an error or being total garbage: it poses the risk of the resulting data being trusted and not being recognized as errorneous.","comments":["The `numpy.histogram` function expects numerical data as input, typically as a NumPy array or a list of numbers. When you provide a list of numerical strings instead, the function may not behave as expected and can produce unexpected results, such as negative values for some bins.\r\nThis behavior occurs because when you provide a list of strings, `numpy.histogram` tries to convert them to numeric values before performing the histogram calculation. If the conversion fails for some elements, it can result in invalid or unexpected values.\r\n\r\nTo resolve this issue, you need to ensure that your data is in the correct numeric format before passing it to `numpy.histogram`. You can convert the list of numerical strings to a list of numbers using the appropriate conversion function, such as `float()` or `int()`.\r\n\r\n```\r\nimport numpy as np\r\n\r\ndata = ['1', '2', '3', '4', '5']\r\n\r\n# Convert the list of strings to a list of numbers\r\nnumeric_data = list(map(float, data))\r\n\r\n# Calculate the histogram\r\nhist, bins = np.histogram(numeric_data, bins=5)\r\n```\r\n\r\nIn this example, `map(float, data)` converts each element in the `data` list from a string to a float, creating a new list of numeric values called `numeric_data`. You can then pass `numeric_data` to `numpy.histogram` to calculate the histogram without encountering negative values in the bins.\r\n\r\nWe can adjust the conversion function (`float()` or `int()`) based on the specific type of numerical data you are working with.\r\n\r\nWhen `numpy.histogram` encounters a string that cannot be converted to a number, it will raise a ValueError and produce unexpected values in the histogram bins. In some cases, you might even see negative values in the bins.\r\n\r\n```\r\ndata1 = ['1', '2', '3', '4', '5', 'abc']\r\n\r\nhist1, bins1 = np.histogram(data1, bins=5)\r\nprint(hist1)\r\n```\r\n\r\nThe error you encountered, `TypeError: cannot perform reduce with flexible type`, occurs when numpy.histogram tries to perform reductions on data that has a flexible type, such as strings or a mix of different data types.\r\n\r\nTo resolve this issue, you need to ensure that your data is in a consistent numerical format before passing it to `numpy.histogram`. \r\n\r\n\r\n","`histogram` relies on `searchsorted` under the hood, which has very different results when you cast the `bin_edges` to string dtypes (which happens under the hood).\r\n\r\nThis gets even more harry in the case where a user passes in one of the pre-defined str bin types (e.g. `\"auto\"`, `\"fd\"`, etc.). Perhaps it's worth raising for arrays with string\/bytes dtypes?","Can we add code to convert string to float? if input is string and can be converted to float. Incase of string can't be converted to float we can raise ValueError.","In general, no, we don't want to do such conversions implicitly. Converting from strings to floats should be explicit use of `.astype()`.","So what is solution? or it is user's resposibility to provide proper input type.","It is the user's responsibility to provide the proper input type.\r\n\r\nIt is our responsibility to make sure that `np.histogram()` raises an error when given an improper input type like strings instead of failing silently with nonsense outputs.","> Perhaps it's worth raising for arrays with string\/bytes dtypes?\r\n\r\nSeems sensible. Added the sprintable label, since `histogram` is implemented in Python so no C skills are needed to add a check.","> In general, no, we don't want to do such conversions implicitly. Converting from strings to floats should be explicit use of .astype().\r\n\r\nThe problem is that last time I tried it, it was a bit complicated to remove string promotion.  (IIRC one of the main issues was astropy being confused by masked arrays being confused because `np.ma.masked` is implicitly a double...)\r\n\r\nI don't care about it surviving, but this arguably makes sense:\r\n```python\r\n>>> np.histogram([str(x) for x in data], sorted([str(x) for x in bins]))\r\n(array([ 0, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0]),\r\n array(['0', '100', '1000', '1050', '1100', '1150', '1200', '1250', '1300',\r\n        '1350', '1400', '1450', '150', '1500', '1550', '1600', '1650',\r\n        '1700', '1750', '1800', '1850', '1900', '1950', '200', '2000',\r\n        '250', '300', '350', '400', '450', '50', '500', '550', '600',\r\n        '650', '700', '750', '800', '850', '900', '950'], dtype='<U4'))\r\n```\r\n\r\nMaybe it helps a bit to identify the root cause:\r\n1. We allow promoting integers and strings to string (I do think this is the main root-cause; I think it is important, urgent, and timely to address, but its difficult.)\r\n2. `np.searchsorted()` uses this standard promotion and decides it is OK to convert the inputs to string.\r\n   * That might even give semi-reasonable results, but it gets completely garbled up because it converts the array being searched.  And converting to strings changes the order!\r\n\r\nSolution?  Ideally, maybe it belongs into searchsorted, since it shares the problem.  We do need to reject mixed strings\/non-string explicitly probably, just like `np.less.resolve_dtypes((bins.dtype, data.dtype, None))` would also raise an error.  In principle probably also some bad time promotions, but I am not sure I care.\r\n\r\n\r\nEDIT: Missed that @rossbar already correctly pointed out that this originates in searchsorted!","Going to take a stab at this (SciPy sprints).","gh-24167 and gh-24170 are already open PRs for this issue.","Apologies, missed the other PRs."],"labels":["00 - Bug","sprintable"]},{"title":"Attributes on the objects dispatched via _ArrayFunctionDispatcher","body":"### Steps to reproduce:\r\n\r\nIn `numpy>=1.25`, this is the behavior:\r\n\r\n```py\r\n>>> import numpy as np\r\n>>> type(np.mean)\r\n<class 'numpy._ArrayFunctionDispatcher'>\r\n>>> import inspect\r\n>>> inspect.isfunction(np.mean)\r\nFalse\r\n>>> inspect.ismethod(np.mean)\r\nFalse\r\n>>> from numpy import _ArrayFunctionDispatcher\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name '_ArrayFunctionDispatcher' from 'numpy' (\/home\/adrin\/miniforge3\/envs\/skops\/lib\/python3.11\/site-packages\/numpy\/__init__.py)\r\n```\r\n\r\nI would think `inspect.isfunction(np.mean)` should return `True`, maybe?\r\n\r\nAlso, it's odd that the `type(np.mean)` returns something which cannot be imported, and the actual import is `from numpy.core._multiarray_umath import _ArrayFunctionDispatcher`.\r\n\r\nEnded up observing this since our dispatch mechanism in `skops` which was dispatching `ufunc` was ignored since these are no more `ufunc`s (I think they were before?)\r\n\r\n","comments":["It quacks enough to pass `inspect.isroutine`, which is the best that is possible as far as I can tell, see also gh-23307.  Not sure what you are doing, but maybe it also makes sense to use `type(np.mean)`.\r\nThis was a Python function type, and isn't anymore, there is nothing more to the change (except of course a huge speedup in dispatching) really everything else drops out from that, whether surprising or not.","Well, we're dispatching based on the type of objects, in this case `np.mean`, and now this type is a _private_ thing, which indicates we shouldn't be using it. But I think it's reasonable to expect that the type of something like `np.mean` to be a _public_ kinda thing.","Ah well, I don't mind adding it somewhere, but also don't know a particularly good place that is clearly \"public\".","`numpy.core.ArrayFunctionDispatcher`? ","I think this type should not be exposed. We should be free to make this kind of change to what is internal infrastructure, so using `inspect.isroutine` seems much more reasonable than exposing this type.","@rgommers this type is exposed via `type(np.mean)`, and it's exposed the wrong way. I really do think the type of something like `np.mean` should be public, but even if you decide not to make it public, one should be able to import whatever the output of `type(obj)` is, and in this case, `numpy._ArrayFunctionDispatcher` cannot be imported, but is reported to be the type of `np.mean`. We should at least fix this inconsistency.","With current `main`:\r\n```python\r\n>>> type(np.mean)\r\n<class 'numpy._ArrayFunctionDispatcher'>\r\n>>> from numpy._core.overrides import _ArrayFunctionDispatcher\r\n>>> type(np.mean) is _ArrayFunctionDispatcher\r\nTrue\r\n>>> from numpy._core._multiarray_umath import _ArrayFunctionDispatcher\r\n>>> type(np.mean) is _ArrayFunctionDispatcher\r\nTrue\r\n```\r\n\r\nThe `__module__` attribute of `np.mean` is explicitly set to `'numpy'` inside the `array_function_dispatch` decorator. That goes through several layers of decorators and `functools.wraps`. I don't really know how to unravel that without spending more time than I have for this. If there's an easy way to make `type(np.mean)` return what you want here, changing that seems okay. I'd say it's very low-prio for us though, since no one really should make use of anything that is private.\r\n\r\n\r\n\r\n> this type is exposed via `type(np.mean)`, and it's exposed the wrong way\r\n\r\nI would not call that \"exposed\". Using `type` like that is very non-idiomatic. `inspect.isroutine` should do all you need here, there's nothing you can\/should do with a private type in Python.","The module is hard-coded in the C type structure, very simple to change for someone who wants to."],"labels":["component: __array_function__"]},{"title":"BUG: Unsupported callable in ``inspect.getfullargspec`` of ``np.any``","body":"### Describe the issue:\n\nwhen I try to use inspect.getfullargspec to get np.any, raises `unsupported callable`\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nimport inspect\r\nimport sys\r\nprint(np.__version__); print(sys.version)\r\ninspect.getfullargspec(getattr(np, 'any'))\n```\n\n\n### Error message:\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile ~\/anaconda3\/lib\/python3.10\/inspect.py:1285, in getfullargspec(func)\r\n   1268 try:\r\n   1269     # Re: `skip_bound_arg=False`\r\n   1270     #\r\n   (...)\r\n   1282     # getfullargspec() historically ignored __wrapped__ attributes,\r\n   1283     # so we ensure that remains the case in 3.3+\r\n-> 1285     sig = _signature_from_callable(func,\r\n   1286                                    follow_wrapper_chains=False,\r\n   1287                                    skip_bound_arg=False,\r\n   1288                                    sigcls=Signature,\r\n   1289                                    eval_str=False)\r\n   1290 except Exception as ex:\r\n   1291     # Most of the times 'signature' will raise ValueError.\r\n   1292     # But, it can also raise AttributeError, and, maybe something\r\n   1293     # else. So to be fully backwards compatible, we catch all\r\n   1294     # possible exceptions here, and reraise a TypeError.\r\n\r\nFile ~\/anaconda3\/lib\/python3.10\/inspect.py:2467, in _signature_from_callable(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\r\n   2466 if _signature_is_builtin(obj):\r\n-> 2467     return _signature_from_builtin(sigcls, obj,\r\n   2468                                    skip_bound_arg=skip_bound_arg)\r\n   2470 if isinstance(obj, functools.partial):\r\n\r\nFile ~\/anaconda3\/lib\/python3.10\/inspect.py:2274, in _signature_from_builtin(cls, func, skip_bound_arg)\r\n   2273 if not s:\r\n-> 2274     raise ValueError(\"no signature found for builtin {!r}\".format(func))\r\n   2276 return _signature_fromstr(cls, func, s, skip_bound_arg)\r\n\r\nValueError: no signature found for builtin <function any at 0x102cd8a30>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[7], line 1\r\n----> 1 inspect.getfullargspec(getattr(np, 'any'))\r\n\r\nFile ~\/anaconda3\/lib\/python3.10\/inspect.py:1295, in getfullargspec(func)\r\n   1285     sig = _signature_from_callable(func,\r\n   1286                                    follow_wrapper_chains=False,\r\n   1287                                    skip_bound_arg=False,\r\n   1288                                    sigcls=Signature,\r\n   1289                                    eval_str=False)\r\n   1290 except Exception as ex:\r\n   1291     # Most of the times 'signature' will raise ValueError.\r\n   1292     # But, it can also raise AttributeError, and, maybe something\r\n   1293     # else. So to be fully backwards compatible, we catch all\r\n   1294     # possible exceptions here, and reraise a TypeError.\r\n-> 1295     raise TypeError('unsupported callable') from ex\r\n   1297 args = []\r\n   1298 varargs = None\r\n\r\nTypeError: unsupported callable\n```\n\n\n### Runtime information:\n\n1.25.0\r\n3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]\n\n### Context for the issue:\n\n_No response_","comments":["I am OK if anyone wants to enhance this by adding sensible dunders (if possible, I am not sure if `__code__` is sensible, although we could add it).\r\n\r\nThis just returned `*args, **kwargs` before and is semi-deprecated by Python with `inspect.signature` being preferred and that follows `__wrapped__` and thus gives the correct thing here.\r\n\r\nNote that none of our C defined functions will work either, although `__text_signature__` should be able to fix that in principle and would be a good enhancement as well.\r\n\r\nUnless this somehow severely limits you, I think this is an enhancement.\r\n\r\n(The change is due to moving dispatching for `__array_function__` to C which mades it *much* faster.)","Get it, tks\n","Updating `__internal_signature__` will only affect `inspect.signature` as it internally calls `inspect._signature_fromstr()` method.\r\nhttps:\/\/stackoverflow.com\/questions\/25847035\/what-are-signature-and-text-signature-used-for-in-python-3-4\/25847066#25847066\r\n\r\n`inspect.getfullargspec()` would not work as it does not do that. Instrospection does not work due to its internal call to `ufunc` which is implemented in the c extension. I have added a wrapper function that calls the original `any()`. So interospection on the method will work as it doesn't directly call `ufunc\/c implemention`. \r\n\r\nWould this be a good approach or would this add unneccessary loc to the code?\r\n"],"labels":["01 - Enhancement"]},{"title":"`test_format.py::test_python2_python3_interoperability` fails on Python 3.12","body":"The failing test is:\r\n```\r\nFAILED lib\/tests\/test_format.py::test_python2_python3_interoperability\r\n```\r\nwith:\r\n```\r\n  E         File \"<unknown>\", line 1\r\n  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \r\n  E                                                              ^\r\n  E       SyntaxError: invalid decimal literal\r\n```\r\n\r\nFull traceback:\r\n\r\n<details>\r\n\r\n```\r\n____________________ test_python2_python3_interoperability _____________________\r\n  \r\n  fp = <_io.BufferedReader name='\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'>\r\n  version = (1, 0), max_header_size = 10000\r\n  \r\n      def _read_array_header(fp, version, max_header_size=_MAX_HEADER_SIZE):\r\n          \"\"\"\r\n          see read_array_header_1_0\r\n          \"\"\"\r\n          # Read an unsigned, little-endian short int which has the length of the\r\n          # header.\r\n          import ast\r\n          import struct\r\n          hinfo = _header_size_info.get(version)\r\n          if hinfo is None:\r\n              raise ValueError(\"Invalid version {!r}\".format(version))\r\n          hlength_type, encoding = hinfo\r\n      \r\n          hlength_str = _read_bytes(fp, struct.calcsize(hlength_type), \"array header length\")\r\n          header_length = struct.unpack(hlength_type, hlength_str)[0]\r\n          header = _read_bytes(fp, header_length, \"array header\")\r\n          header = header.decode(encoding)\r\n          if len(header) > max_header_size:\r\n              raise ValueError(\r\n                  f\"Header info length ({len(header)}) is large and may not be safe \"\r\n                  \"to load securely.\\n\"\r\n                  \"To allow loading, adjust `max_header_size` or fully trust \"\r\n                  \"the `.npy` file using `allow_pickle=True`.\\n\"\r\n                  \"For safety against large resource use or crashes, sandboxing \"\r\n                  \"may be necessary.\")\r\n      \r\n          # The header is a pretty-printed string representation of a literal\r\n          # Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte\r\n          # boundary. The keys are strings.\r\n          #   \"shape\" : tuple of int\r\n          #   \"fortran_order\" : bool\r\n          #   \"descr\" : dtype.descr\r\n          # Versions (2, 0) and (1, 0) could have been created by a Python 2\r\n          # implementation before header filtering was implemented.\r\n          #\r\n          # For performance reasons, we try without _filter_header first though\r\n          try:\r\n  >           d = ast.literal_eval(header)\r\n  \r\n  ast        = <module 'ast' from '\/opt\/python\/cp312-cp312\/lib\/python3.12\/ast.py'>\r\n  encoding   = 'latin1'\r\n  fp         = <_io.BufferedReader name='\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'>\r\n  header     = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n  header_length = 70\r\n  hinfo      = ('<H', 'latin1')\r\n  hlength_str = b'F\\x00'\r\n  hlength_type = '<H'\r\n  max_header_size = 10000\r\n  struct     = <module 'struct' from '\/opt\/python\/cp312-cp312\/lib\/python3.12\/struct.py'>\r\n  version    = (1, 0)\r\n  \r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/format.py:625: \r\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n  \r\n  node_or_string = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n  \r\n      def literal_eval(node_or_string):\r\n          \"\"\"\r\n          Evaluate an expression node or a string containing only a Python\r\n          expression.  The string or node provided may only consist of the following\r\n          Python literal structures: strings, bytes, numbers, tuples, lists, dicts,\r\n          sets, booleans, and None.\r\n      \r\n          Caution: A complex expression can overflow the C stack and cause a crash.\r\n          \"\"\"\r\n          if isinstance(node_or_string, str):\r\n  >           node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\r\n  \r\n  node_or_string = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n  \r\n  \/opt\/python\/cp312-cp312\/lib\/python3.12\/ast.py:66: \r\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n  \r\n  source = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n  filename = '<unknown>', mode = 'eval'\r\n  \r\n      def parse(source, filename='<unknown>', mode='exec', *,\r\n                type_comments=False, feature_version=None):\r\n          \"\"\"\r\n          Parse the source into an AST node.\r\n          Equivalent to compile(source, filename, mode, PyCF_ONLY_AST).\r\n          Pass type_comments=True to get back type comments where the syntax allows.\r\n          \"\"\"\r\n          flags = PyCF_ONLY_AST\r\n          if type_comments:\r\n              flags |= PyCF_TYPE_COMMENTS\r\n          if feature_version is None:\r\n              feature_version = -1\r\n          elif isinstance(feature_version, tuple):\r\n              major, minor = feature_version  # Should be a 2-tuple.\r\n              if major != 3:\r\n                  raise ValueError(f\"Unsupported major version: {major}\")\r\n              feature_version = minor\r\n          # Else it should be an int giving the minor version for 3.x.\r\n  >       return compile(source, filename, mode, flags,\r\n                         _feature_version=feature_version)\r\n  E         File \"<unknown>\", line 1\r\n  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \r\n  E                                                              ^\r\n  E       SyntaxError: invalid decimal literal\r\n  \r\n  feature_version = -1\r\n  filename   = '<unknown>'\r\n  flags      = 1024\r\n  mode       = 'eval'\r\n  source     = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n  type_comments = False\r\n  \r\n  \/opt\/python\/cp312-cp312\/lib\/python3.12\/ast.py:52: SyntaxError\r\n  \r\n  During handling of the above exception, another exception occurred:\r\n  \r\n      @pytest.mark.xfail(IS_WASM, reason=\"Emscripten NODEFS has a buggy dup\")\r\n      def test_python2_python3_interoperability():\r\n          fname = 'win64python2.npy'\r\n          path = os.path.join(os.path.dirname(__file__), 'data', fname)\r\n          with pytest.warns(UserWarning, match=\"Reading.*this warning\\\\.\"):\r\n  >           data = np.load(path)\r\n  \r\n  fname      = 'win64python2.npy'\r\n  path       = '\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'\r\n  \r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/test_format.py:535: \r\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/npyio.py:453: in load\r\n      return format.read_array(fid, allow_pickle=allow_pickle,\r\n          N          = 6\r\n          _ZIP_PREFIX = b'PK\\x03\\x04'\r\n          _ZIP_SUFFIX = b'PK\\x05\\x06'\r\n          allow_pickle = False\r\n          encoding   = 'ASCII'\r\n          fid        = <_io.BufferedReader name='\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'>\r\n          file       = '\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'\r\n          fix_imports = True\r\n          magic      = b'\\x93NUMPY'\r\n          max_header_size = 10000\r\n          mmap_mode  = None\r\n          own_fid    = True\r\n          pickle_kwargs = {'encoding': 'ASCII', 'fix_imports': True}\r\n          stack      = <contextlib.ExitStack object at 0x7fdf547276e0>\r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/format.py:786: in read_array\r\n      shape, fortran_order, dtype = _read_array_header(\r\n          allow_pickle = False\r\n          fp         = <_io.BufferedReader name='\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'>\r\n          max_header_size = 10000\r\n          pickle_kwargs = {'encoding': 'ASCII', 'fix_imports': True}\r\n          version    = (1, 0)\r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/format.py:628: in _read_array_header\r\n      header = _filter_header(header)\r\n          ast        = <module 'ast' from '\/opt\/python\/cp312-cp312\/lib\/python3.12\/ast.py'>\r\n          encoding   = 'latin1'\r\n          fp         = <_io.BufferedReader name='\/tmp\/tmp.KtBhpBVwgx\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy'>\r\n          header     = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n          header_length = 70\r\n          hinfo      = ('<H', 'latin1')\r\n          hlength_str = b'F\\x00'\r\n          hlength_type = '<H'\r\n          max_header_size = 10000\r\n          struct     = <module 'struct' from '\/opt\/python\/cp312-cp312\/lib\/python3.12\/struct.py'>\r\n          version    = (1, 0)\r\n  ..\/venv\/lib\/python3.12\/site-packages\/numpy\/lib\/format.py:575: in _filter_header\r\n      for token in tokenize.generate_tokens(StringIO(s).readline):\r\n          StringIO   = <class '_io.StringIO'>\r\n          last_token_was_number = False\r\n          s          = \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"\r\n          token      = TokenInfo(type=55 (OP), string='(', start=(1, 50), end=(1, 51), line=\"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\")\r\n          token_string = '('\r\n          token_type = 55\r\n          tokenize   = <module 'tokenize' from '\/opt\/python\/cp312-cp312\/lib\/python3.12\/tokenize.py'>\r\n          tokens     = [TokenInfo(type=55 (OP), string='{', start=(1, 0), end=(1, 1), line=\"{'descr': '<f8', 'fortran_order': False, 'shape':...er'\", start=(1, 17), end=(1, 32), line=\"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\"), ...]\r\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n  \r\n  source = <built-in method readline of _io.StringIO object at 0x7fdf44818c40>\r\n  encoding = None, extra_tokens = True\r\n  \r\n      def _generate_tokens_from_c_tokenizer(source, encoding=None, extra_tokens=False):\r\n          \"\"\"Tokenize a source reading Python code as unicode strings using the internal C tokenizer\"\"\"\r\n          if encoding is None:\r\n              it = _tokenize.TokenizerIter(source, extra_tokens=extra_tokens)\r\n          else:\r\n              it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)\r\n  >       for info in it:\r\n  E         File \"<string>\", line 1\r\n  E           {'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \r\n  E                                                              ^\r\n  E       SyntaxError: invalid decimal literal\r\n  \r\n  encoding   = None\r\n  extra_tokens = True\r\n  info       = (55, '(', (1, 50), (1, 51), \"{'descr': '<f8', 'fortran_order': False, 'shape': (2L,), }           \\n\")\r\n  it         = <_tokenize.TokenizerIter object at 0x7fdf45f009d0>\r\n  source     = <built-in method readline of _io.StringIO object at 0x7fdf44818c40>\r\n  \r\n  \/opt\/python\/cp312-cp312\/lib\/python3.12\/tokenize.py:526: SyntaxError\r\n```\r\n\r\n<\/details>\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> np.load('\/usr\/lib\/python3.11\/site-packages\/numpy\/lib\/tests\/data\/win64python2.npy')\r\narray([1., 1.])\r\n```\r\n\r\nShould be due to the `tokenize` rewrite in Python 3.12, which has a known behavior change where invalid syntax is no longer parsed. Cc @lysnikolaou for visibility. It looks like this was a binary `.npy` file saved in Python 2.x with a `2L` long int literal, and in Python 3 that isn't valid syntax. I think the fix is to delete the file and the test. @rkern or anyone else, any objections to that?","comments":["The `npy` cleanup has a `_filter_header` function which is supposed to clean up the `L` when necessary.  That said, we also have a warning in place that this is at least very slow in 1.25, so maybe we can get away with just failing to load such ancient files.\r\n\r\nOTOH, it would be a bit nicer if `_filter_header`'s use of the tokenizer has a simple solution.  Support for structured dtypes might make it hard to get completely right though.\r\n\r\nA pragmatic mid-way may be to just use a simple regex replace like `[ (]\\d+L` and escalating the warning a bit that there is a theoretical chance of messing up structured dtype names along the way.","But, maybe nevermind...\r\n\r\nUnless anyone has serious concerns, I am very happy with just raising a more explicit error (that says that the file may have been created with Python 2), and be done with it unless we get a request to add such a hack.","Is the failing syntax still there? I checked the failing test's source code and didn't find any `2L` mentions. Or do I misunderstand the issue?","Should still be there, search for the `_filter_header` function that was mentioned or run the tests on python 3.12 to track it down.\r\n\r\nEDIT: Of course the test is skipped on 3.12, so you need to unskip it to see the failure..."],"labels":["00 - Bug","component: numpy.lib"]},{"title":"MAINT: apply pyupgrade to numpy sources","body":"### Proposed new feature or change:\r\n\r\nI would like to:\r\n1. run [pyupgrade](https:\/\/github.com\/asottile\/pyupgrade) on numpy sources (typically `pyupgrade --py39-plus`),\r\n2. add it as a pre-commit hook if there's interest.\r\n\r\nThe objective is to make use of recent Python features, such as f-strings, consistent with the current minimal Python version supported. However, most suggestions are consistent with Python 3.5 or event earlier. I feel it is easier to manually upgrade the code, before trying to automate.\r\n\r\nSee \u201c[Implemented features](https:\/\/github.com\/asottile\/pyupgrade#implemented-features)\u201d for a general overview of the changes suggested by [pyupgrade](https:\/\/github.com\/asottile\/pyupgrade). \r\n\r\nI have already created pull requests with pyupgrade suggestions, some of which have been merged:\r\n\r\n* #23826\r\n* #23833 (still in progress)\r\n* #23834\r\n* #23837\r\n* #23977\r\n* #23980","comments":["A few notes:\r\n1. Most importantly, [pyupgrade](https:\/\/github.com\/asottile\/pyupgrade) doesn't simply report issues, rather it fixes them.\r\n2. Also, it is \u201cuncompromising\u201d like [black](https:\/\/github.com\/psf\/black), in the sense that you cannot disable classes of issues. It has few options:\r\n   ```\r\n     -h, --help            show this help message and exit\r\n     --exit-zero-even-if-changed\r\n     --keep-percent-format\r\n     --keep-mock\r\n     --keep-runtime-typing\r\n     --py3-plus, --py3-only\r\n     --py36-plus\r\n     --py37-plus\r\n     --py38-plus\r\n     --py39-plus\r\n     --py310-plus\r\n     --py311-plus\r\n   ```\r\n   Most of the suggestions make sense, but my experience with other modules is that maintainers may dislike some of the opinionated pyupgrade changes, particularly:\r\n   * [Set literals](https:\/\/github.com\/asottile\/pyupgrade#set-literals)\r\n   * [Dictionary comprehensions](https:\/\/github.com\/asottile\/pyupgrade#dictionary-comprehensions)\r\n   * [redundant `open` modes](https:\/\/github.com\/asottile\/pyupgrade#redundant-open-modes)\r\n   \r\n   Yet, they make sense if you think about it. For example, I am less surprised by the result of `{\"string\"}` (`{'string'}`) than the result of `set(\"string\")` (`{'g', 'i', 'n', 'r', 's', 't'}`).\r\n3. You may need to run pyupgrade twice. These changes are applied sequentially:\r\n   * [printf-style string formatting](https:\/\/github.com\/asottile\/pyupgrade#printf-style-string-formatting) (from old prtinf-style formatting to `format()`)\r\n   * [f-strings](https:\/\/github.com\/asottile\/pyupgrade#f-strings) (from `format()` to f-strings)\r\n4. Some changes break formatting, so pyupgrade is best run together with [black](https:\/\/github.com\/psf\/black) or some other code formatter. For example:\r\n   ```python\r\n   \"Some text: %s %s %s %s %s %s\" % (parameter_a, parameter_b,\r\n                                     parameter_c, parameter_d,\r\n                                     parameter_e, parameter_f\r\n   )\r\n   ```\r\n   yields:\r\n   ```python\r\n   \"Some text: {} {} {} {} {} {}\".format(parameter_a, parameter_b,\r\n                                     parameter_c, parameter_d,\r\n                                     parameter_e, parameter_f\r\n   )\r\n   ```\r\n5. While changing all occurrences of `format()` to f-strings will not always be possible or reasonable, at least not without adding temporary variables, pyupgrade will sometimes give up in cases where f-strings do make sense.\r\n\r\nFor the above reasons, I thought it would be best to review classes of issues fixed by pyugrade in a project, one pull request at a time for each class of changes. This, of course, is somehow tedious, since pyupgrade does not provide a way to apply only a single rule at a time for easier reviewing, but I can do that, or rather have already started as you can see.","@DimitriPapadopoulos thanks for the PRs so far. Some cleanups of really old\/outdated constructs may be worth doing. We discourage style-only PRs however; large-scale changes like converting everything to f-strings are a lot of churn, hard to review, and typically not worth it. And we don't want to use pre-commit. So I'd say judiciously chosen PRs welcome, but we'd rather not apply more tools like `pyupgrade` in a blanket fashion.","It's just that f-strings are not only about style, they're supposed to be faster too. But I understand, it is not worth optimising one line of output here and there.","Alternatively you could use [Ruff's `UP` category](https:\/\/docs.astral.sh\/ruff\/rules\/#pyupgrade-up). Which is faster, configurable if any specific codemod is unwanted, and has safer autofixes."],"labels":["03 - Maintenance"]},{"title":"TST: refactor test_new_policy test","body":"Currently `test_new_policy` in `core\/tests\/test_mem_policy.py` sets a memory policy and then runs the `core` and `ma` test suite. This can be confusing, since any test failure in those suites will appear twice: once in the regular tests, and once in the recursive `test_new_policy` tests. It would be better to have a separate CI run for that test.","comments":["xref https:\/\/github.com\/numpy\/numpy\/pull\/23838#issuecomment-1596135269","Thanks Matti. It's not only confusing but far too slow. The `pytest.mark.slow` decorator doesn't cut it here, that's more for the 0.5-5 second range. This should be marked as `xslow` (as in SciPy) or only enabled via some opt-in.","I'll note that gh-23838 disables the test completely until this issue is addressed. It takes to much time when iterating on build changes, and there's quite a bit of that to do right now.","This issue shouldn't block `2.0.0rc1`, but it'd be good to revisit before the final release, since this test has now not run in a long time. Hence keeping the milestone."],"labels":["17 - Task","05 - Testing"]},{"title":"BUG: Buffer protocol doesn't support read-write access for numpy records","body":"### Describe the issue:\n\nNumpy does not allow you to use the buffer-protocol to get a read-write buffer for numpy record.  It requires you to ask for the record read-only.  This is in spite of the fact that the pointer returned using the buffer protocol does, in fact, point to a read-write piece of memory and one ought to be able to modify it.\r\n\r\nUsing the buffer protocol is the recommended way to read and write \"array-like objects\" in C.  To use the buffer protocol for a numpy record, you have to lie and request a read-only buffer, and then write to it anyway.  The older __array_interface__ works just fine.\n\n### Reproduce the code example:\n\n```python\n>>> import numpy as np\r\n>>> x = np.zeros(2, dtype='d,d,i')\r\n\r\n# memoryview mimics the behavior of the buffer protocol\r\n>>> memoryview(x).readonly\r\nFalse\r\n>>> memoryview(x[0]).readonly\r\nTrue\r\n\r\nOn the other hand\r\n\r\n>>> x.__array_interface__['data']   # returns tuple (address, read-only)\r\n(105553143159680, False)\r\n>>> x[0].array_interface__['data']\r\n(105553143159680, False)\r\n\r\nI also wrote some C code which I linked in and called from Python\r\n\r\nvoid foo(PyObject *object) {\r\n    Py_buffer view;\r\n    PyObject_GetBuffer(object, &view, PyBUF_CONTIG_RO);\r\n    if (view.obj) {\r\n        double *data = (double *)view.buf;\r\n        *data += 1;\r\n    }\r\n    PyBuffer_Release(&view);\r\n}\r\n\r\nboth foo(x) and foo(x[0]) correctly implemented an element of the array. The buffer really does point do the actual data and not a copy of it.\r\n\r\nUsing PyBUF_CONTIG gives on error on foo(x[0]).\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\nnumpy.__version__\r\n'1.25.0'\r\n>>> sys.version\r\n'3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]'\r\n>>> numpy.show_runtime()\r\n[{'numpy_version': '1.25.0',\r\n  'python': '3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 '\r\n            '(clang-1300.0.29.30)]',\r\n  'uname': uname_result(system='Darwin', node='Franks-14-Mac-Book-Pro-2.local', release='22.4.0', version='Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3\/RELEASE_ARM64_T6000', machine='arm64')},\r\n {'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP', 'ASIMDDP'],\r\n                      'not_found': ['ASIMDFHM']}},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/Users\/fy\/Library\/Python\/3.10\/lib\/python\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 10,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.23'}]\r\n>>> \r\n\n\n### Context for the issue:\n\nI should be able to use the buffer protocol to read and write a numpy record without lying about my intentions to write to it.","comments":["We can probably change this, although I don't like that our scalars are mutable this way; it also doesn't really make much sense to act differently in different places.  (I suspect there may be ways to create structured scalars which do not allow to write, but I would have to search a bit.)\r\n\r\nSo, I am a bit curious about you needing to mutate the scalars (although, NumPy turning 0-D arrays to scalars might be enough of a reason).\r\n\r\nI.e. the fact that we return a read-only version for the buffer protocol was maybe even intentional to some degree.\r\nThe main reason why its tricky to make the structured scalar immutable is that it would break `arr[0][\"field\"] = value` (which would otherwise only work as `arr[\"field\"][0] = value`).\r\n\r\n","If I recall correctly there were issues around correctly handling the buffer protocol and padding (if needed).","My use case is that I am creating a numpy record that looks like a C structure, and then passing that numpy record to C code.  I need to find out the actual address of the numpy record in my SWIG wrapper to give to C. \r\n\r\nSlightly longer history.  All my numpy records are currently created as the sole element of a numpy array:\r\n```\r\narray = np.zeros(1, dtype=\"i,i,i,i,d,d,d\"). # my actual descriptor is more complicated\r\nrecord = array[0]\r\n```\r\nand in this case, I can get from a numpy record to its actual address by means of base:\r\n```\r\nPyObject *base = PyObject_getattr_string(\"base\");  \/\/ get back the array\r\nvoid* address = PyArray_DATA(base);\r\n```\r\nHowever I will soon be needing to handle arrays of records, and the above trick will only work with the 0-th element.\r\n\r\nI can correct this by using the C side of the [__array_interface__ protocol:\r\n](https:\/\/numpy.org\/doc\/stable\/reference\/arrays.interface.html)\r\n```\r\n    PyObject *capsule = PyObject_GetAttrString(object, \"__array_struct__\");\r\n    PyArrayInterface* interface = PyCapsule_GetPointer(capsule, NULL);\r\n    void *address = interface->data;\r\n    Py_DECREF(capsule);\r\n```\r\nThis works fine, and it correctly reports that the data is read-write.  But the documentation says this protocol is \"legacy\", and that new code should use the buffer protocol.  Hence my surprising discovery that:\r\n```\r\n    Py_buffer view;\r\n    PyObject_GetBuffer(object, &view, PyBUF_CONTIG);\r\n    void *address = (void *)view.buf;\r\n    PyBuffer_Release(&view);  \r\n    return result;\r\n```\r\nfails.  If I lie to it and use `PyBUF_CONTIG_RO`, I *can* get the record address and write to it anyway.\r\n\r\nUntil this is fixed (or declined), can I ask for advice? Am I better off using the legacy __array_interface__ or lying to the buffer protocol? Both feel dangerous in different ways.\r\n\r\n","Does using this instead work?\r\n```python\r\nrecord = np.zeros((), dtype=\"i,i,i,i,d,d,d\"). # my actual descriptor is more complicated\r\n```\r\nThis gives you an array rather than a scalar, which should be read-write.","The numpy record is used a lot on both the C side and on the Python side. The bigger project is a Python wrapper around a NASA toolkit. It would be tricky explaining to users (most of them are astronomers, not programmers) that they have to type `record[0].field`* instead of the more natural `record.field`. The toolkit also wants to return arrays of records, and the user will pick one of them and pass it back to the toolkit.\r\n\r\nBut in general, there ought to be a natural way of modifying a record in C, shouldn't there?\r\n\r\n[* Yes, I'm actually using rec.array.  But that detail was irrelevant to the original bug report.]\r\nThanks for your help.\r\n\r\n","Just double checking.  Until this bug is resolved one way or the other, is there a recommended way to get the address of a record?  Is there any intent to deprecate array_interface?"],"labels":["00 - Bug"]},{"title":"ENH: Introduce multiple pair parameters in the 'repeat' function","body":"This feature enables pairs of parameters to be passed to the NumPy's [repeat](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.repeat.html) function: tuple arguments can now be passed to the 'repeats' parameter and the \u2018axis\u2019 parameter can now also receive a sequence of integers. Data types are properly checked. Flattened output arrays (non-specified axes) and repeats broadcasted to the size of the paired axis are also taken into account. For instance, if two pairs of arguments are used and the second one doesn't have an axis specified, a flat output array is returned. This feature was first suggested in #21435.\r\n\r\nMoreover, the multiple repeats are processed in ascending order, meaning the repeats that result in a smaller size of its axis in the intermediate output array are processed first. This adjustment renders a processing time reduction of approximately 50% in significantly large repeats (i.e. over 100 repeats per axis). \r\n\r\nThis enhancement makes the repeat function more versatile and elegant. The greater the number of dimensions of the input array to be repeated over a axis, the more useful this feature is.\r\n\r\nUsage example:\r\n\r\n <code>>>> x = np.array([[1,2],[3,4]])<\/code>\r\n<code>>>> x = np.repeat(x, ([3, 3], [1, 2]), (1, 0))\r\n    array([[1, 1, 1, 2, 2, 2],\r\n              [3, 3, 3, 4, 4, 4],\r\n              [3, 3, 3, 4, 4, 4]])<\/code>\r\n<code>>>> x = np.repeat(x, (3, [1, 2], 1), (1, 0))\r\n    array([1, 1, 1, 2, 2, 2,\r\n           3, 3, 3, 4, 4, 4,\r\n           3, 3, 3, 4, 4, 4])\r\n<\/code>\r\n","comments":["This seems to me to be better as a separate function that will use `np.repeat()` as a primitive rather than extending the and complicating the semantics of `np.repeat()` itself.\r\n\r\nEither way, this is the kind of expansion of the API that needs to be discussed on the mailing list first.","Expanding a bit on @rkern's comment:\r\n\r\nThere is some discussion of [adding `repeat` to the Array API](https:\/\/github.com\/data-apis\/array-api\/issues\/187) in a future revision, along with a number of other commonly-used APIs. Extending the signature would move NumPy further away from the signature used in other array-processing libraries, and would need to be considered carefully.","Thank you for your input! \r\nI would like to point out that this feature is able to halve the process time for repeats that result in a very large array size on the repeated dimension by processing the multiple repeats in ascending order (smallest resulting axis size first). \r\nHere are some of the improvements that were consistently managed locally:\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/87664900\/1d9b0fb8-3884-40cd-aa7f-e78e5a7a59df)\r\n\r\nThere is no problem in creating a new separate function - 'repeats' - that uses 'repeat' and preserves all the new functionalities that were implemented. \r\nI can either bring forward those changes or keep the current feature as it is. \r\nEither way, any input is greatly appreciated."],"labels":["01 - Enhancement"]},{"title":"ENH: C-API function for slicing ndarray[i:j]","body":"### Proposed new feature or change:\n\nI discussed this with @seberg a few weeks ago.  In pandas we have a hotspot in the cython code where we slice an ndarray.  AFAICT the slicing itself is fast, but having to make a python-space call is slow.  bc the relevant call happens inside a loop, this adds up.\r\n\r\nSpecific request: a numpy C-API function for ndarray[i:j]","comments":["In many cases it would probably make sense to re-use a single array object, but that requires getting your hands dirty (and a lot of care on ownership!).\r\n\r\nI will guestimate that a dedicate function (which older Python versions actually had) would reduce the overhead by about a factor of 2.  Most of the rest will be the array creation\/destruction itself.\r\n\r\nInternally, this goes through `get_view_from_index`, but that:\r\n* uses an internal representation of the index and actually holds on to the original `slice` object (because there is no C entry).\r\n* Would be a lot more complex to use.\r\n\r\nWhat would we want?  An optional dimension along which to slice, or even slicing multiple dimensions?  I guess pandas would really only need one thing.\r\n\r\n---\r\n\r\nI am wondering if an example for how to implement a small C-function yourself might actually be more useful?","I came across this issue and thought implementing a simple function to perform a slice operation using the NumPy C-API might be a nice way to learn about creating NumPy extensions. With @seberg's suggestion that a small C function might be of use in mind, I created: https:\/\/github.com\/ianharris\/c-ndarray-slice\r\n\r\nHopefully it is something that is somewhat close to what was in mind in the discussion above, and is also reasonably robust and useful.","thanks for taknig a look ian.  IIUC the implementation you posted will make a copy, is that correct?  For a slice I expect a view.","Given all of the ways that this needs to be restricted from general slicing\/indexing in order to get something fast, and the potential benefit that a single object that continually gets modified in the loop instead might provide instead, I'm going to suggest that this is not a candidate for a new API function. It seems like something that's good to implement near where it's needed and those extra restrictions make sense.","@jbrockmendel yes, it is a copy. I naively hadn't thought about a view. I'll need to take another look to see how a view could be done.\r\n\r\nFor your use case I'm guessing you can slice along any dimension? And you mention processing in a loop, do you take slices of slices? Or could you assume that an array is only ever sliced once?","> For your use case I'm guessing you can slice along any dimension? And you mention processing in a loop, do you take slices of slices? Or could you assume that an array is only ever sliced once?\r\n\r\nThe use case I have in mind is always 2D and slicing on axis=0.  And the loop isn't on a single ndarray, but over a sequence of ndarrays.","@jbrockmendel the zero axis simplifies things. Had to do a bit of research to understand some NumPy internals but I think I have a plan now. I'll write some updated code for this tomorrow.","An updated version that no longer requires a copy: https:\/\/github.com\/ianharris\/c-ndarray-slice\/tree\/view-0-axis","Thanks @ianharris.\r\n\r\n@WillAyd if this doesn't land in numpy, what do you think of vendoring it?  Unfortunately we just [refactored away](https:\/\/github.com\/pandas-dev\/pandas\/pull\/54568) the main place where we would have used it, so making use of it would require re-implementing specialized Block subclasses.  Then we'd implement NumpyBlock._slice that would use this directly.  The main place where I think it might make a difference would be in _get_block_for_concat_plan where we call slice_block_columns in a (nested) loop.  Unfortunately I don't have a good estimate on how big the perf impact would be.","I don't see a problem doing this in pandas itself. We already have the capsule extension module so this (and other similar functions) could be added there to be exposed to our existing extensions"],"labels":["Project","63 - C API"]},{"title":"DOC: Adding a depreciated note on Gitpod development guide.","body":"### Issue with current documentation:\n\n**Related Issue**: #23786\r\n\r\nThe Gitpod support has been removed from the newer version of NumPy, as concluded in the above issue. Consequently, the Gitpod section will be removed from the upcoming version of the documentation.\r\n\r\nAs requested by @Mukulikaa and @rossbar in the pull request generated for the related issue https:\/\/github.com\/numpy\/doc\/pull\/15, a deprecated note has been added to the [Gitpod development guide](https:\/\/numpy.org\/doc\/stable\/dev\/development_gitpod.html) to inform users about the removal of Gitpod support.\r\n\r\n**Changes\/solution for the above problem**\r\nI have generated a pull request to implement these changes, which can be found at: https:\/\/github.com\/numpy\/doc\/pull\/16. I am eagerly awaiting the community's review and approval of the pull request for the merge.\n\n### Idea or request for content:\n\n_No response_","comments":[],"labels":["04 - Documentation"]},{"title":"AttributeError: 'numpy.float32' object has no attribute 'log'","body":"### Describe the issue:\n\nWhen I ran these code lines, it appeared this issue: \r\n\r\n\r\n\r\nThis is the adata_pb.X array:\r\n\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/130348585\/5fc36940-7eed-4668-aa66-ff2c37559b8e)\r\n\r\n\r\nThank you so much for helping me!\r\n\r\n\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nadata_pb.layers[\"counts\"] = adata_pb.X.copy()\r\nadata_pb.obs[\"lib_size\"] = np.sum(adata_pb.layers[\"counts\"], axis = 1) \r\nadata_pb.obs[\"log_lib_size\"] = np.log(adata_pb.obs[\"lib_size\"])\n```\n\n\n### Error message:\n\n```shell\nAttributeError                            Traceback (most recent call last)\r\nAttributeError: 'numpy.float32' object has no attribute 'log'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[53], line 1\r\n----> 1 adata_pb.obs[\"log_lib_size\"] = np.log(adata_pb.obs[\"lib_size\"])\r\n\r\nFile ~\/anaconda3\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:2113, in NDFrame.__array_ufunc__(self, ufunc, method, *inputs, **kwargs)\r\n   2109 @final\r\n   2110 def __array_ufunc__(\r\n   2111     self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any\r\n   2112 ):\r\n-> 2113     return arraylike.array_ufunc(self, ufunc, method, *inputs, **kwargs)\r\n\r\nFile ~\/anaconda3\/lib\/python3.10\/site-packages\/pandas\/core\/arraylike.py:402, in array_ufunc(self, ufunc, method, *inputs, **kwargs)\r\n    399 elif self.ndim == 1:\r\n    400     # ufunc(series, ...)\r\n    401     inputs = tuple(extract_array(x, extract_numpy=True) for x in inputs)\r\n--> 402     result = getattr(ufunc, method)(*inputs, **kwargs)\r\n    403 else:\r\n    404     # ufunc(dataframe)\r\n    405     if method == \"__call__\" and not kwargs:\r\n    406         # for np.<ufunc>(..) calls\r\n    407         # kwargs cannot necessarily be handled block-by-block, so only\r\n    408         # take this path if there are no kwargs\r\n\r\nTypeError: loop of ufunc does not support argument 0 of type numpy.float32 which has no callable log method\n```\n\n\n### Runtime information:\n\nI am using python 3.10.9 and numpy 1.23.5\n\n### Context for the issue:\n\n_No response_","comments":["This seems like a bug in Pandas rather than NumPy. I see pandas-dev\/pandas#33492 which is showing a similar error message.","> This seems like a bug in Pandas rather than NumPy. I see [pandas-dev\/pandas#33492](https:\/\/github.com\/pandas-dev\/pandas\/issues\/33492) which is showing a similar error message.\r\n\r\nI did not see any solution in this thread :3"],"labels":["00 - Bug"]},{"title":"DOC: numpy.char.rjust cuts strings to size \"width\" instead of just padding like str.rjust does","body":"### Issue with current documentation:\r\n\r\nHello everyone, \r\n\r\nin the documentation https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.char.rjust.html it is stated that numpy.char.rjust calls str.rjust element-wise.\r\n\r\nThough, str.rjust returns the original string in case the length of the string is greater than the specified _width_ (https:\/\/docs.python.org\/3\/library\/stdtypes.html#str.rjust) which is not the case for numpy.char.rjust, which cuts strings off if their length is greater than the given _width_. \r\n\r\n```python\r\nimport numpy as np\r\n    \r\n# numpy.char.rjust\r\narr = np.array(['Mississippi', 'Rome', 'Washington'])\r\nresult = np.char.rjust(arr, 7)\r\nprint(result)\r\n# Output: ['Mississ' '   Rome' 'Washing']\r\n```\r\n\r\n### Idea or request for content:\r\n\r\nThis is hinted at by the explanation of the _width_-argument \"The_ length of the resulting strings\" but is different than the built-in function's behaviour and should be explained as such, in case this is even intentional.\r\n\r\nTo me, it seems more logical for the resulting strings not to be cut but instead only to be padded and to preserve the original's function's behaviour, but if that is not wanted for numpy, the documentation should reflect that. In that case, a short example or warning would be in order. ","comments":["The main difference between `numpy.char.rjust` and `str.rjust` is in the input they accept and the output they return.\r\n\r\n`numpy.char.rjust` is a NumPy function that operates on arrays of strings. It accepts an array-like object (a) containing strings and right-justifies each element of the array in a string of length width. The resulting output is an `ndarray` containing the right-justified strings.\r\n\r\nOn the other hand, `str.rjust` is a method available for individual string objects in Python. It right-justifies a single string (s) within a string of length width and returns the resulting right-justified string.\r\n\r\n                import numpy as np\r\n                \r\n                # Using numpy.char.rjust\r\n                arr = np.array(['apple', 'banana', 'cherry'])\r\n                result = np.char.rjust(arr, 8)\r\n                print(result)\r\n                # Output: ['   apple' '  banana' '  cherry']\r\n                \r\n                # Using str.rjust\r\n                s = 'hello'\r\n                result = s.rjust(10)\r\n                print(result)\r\n                # Output: '     hello'\r\n\r\n\r\nSo, the key differences lie in the input type (array-like vs. individual string) and the output type (ndarray vs. single string).","@rohan472000 while this is mostly true, it does not address the issue that I pointed out: Unlike `str.rjust`, which [according to the documentation cited above](https:\/\/docs.python.org\/3\/library\/stdtypes.html#str.rjust) just returns the original string, if it's longer than the specified _width_, numpy's version returns an `ndarray` with every string exactly of length _width_, even if that means cutting some of the string off. \r\n\r\nIn my mind, this is not expected or wanted behaviour, but if it is, the documentation should mention it or warn about it. At the time of writing, the [docs](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.char.rjust.html) still state `str.rjust` is called element-wise, which suggests a different behaviour. ","@Archern4r  I overlooked that important distinction.\r\n\r\nThe main difference between `numpy.char.rjust` and `str.rjust` is in how they handle strings that are longer than the specified width.\r\n\r\n`numpy.char.rjust` behaves differently from `str.rjust` when it comes to strings that are longer than the specified width. According to the current implementation of numpy.char.rjust, if a string's length exceeds the specified width, it will be truncated to fit within that width. This means that characters from the end of the string will be removed to match the specified width.\r\n\r\nOn the other hand, `str.rjust` returns the original string unchanged if its length is greater than or equal to the specified width. It does not truncate or modify the original string.\r\n\r\n            import numpy as np\r\n            \r\n            # Using numpy.char.rjust\r\n            arr = np.array(['apple', 'banana', 'cherry'])\r\n            result = np.char.rjust(arr, 6)\r\n            print(result)\r\n            # Output: [' apple' 'banana' 'cherry']\r\n            \r\n            # Using str.rjust\r\n            s = 'hello'\r\n            result = s.rjust(6)\r\n            print(result)\r\n            # Output: 'hello'\r\n\r\n\r\nIn the above example, `numpy.char.rjust` truncates the first element of the array 'apple' to fit within the specified width of 6, resulting in ' apple'. In contrast, `str.rjust` returns the original string 'hello' because its length is already equal to the specified width.\r\n\r\nTherefore, it is important to note that the behavior of `numpy.char.rjust` differs from the behavior of `str.rjust`, especially when dealing with strings that are longer than the specified width. The documentation for `numpy.char.rjust` should reflect this behavior to avoid confusion.","@rohan472000 did you write that original response and the updated one with ChatGPT? Because it has exactly the writing patterns of ChatGPT and it also does ChatGPT-like errors in the code and in its logic. \r\n\r\nIn the example above, _apple_ is not truncated but instead it is padded. Also, your example with `str.rjust` is plain wrong, the output is not what python would actually return. Both examples do not display the problematic behaviour\r\n\r\nWhile I appreciate the effort to help, I feel the issue will not be addressed by just rehearsing the functionalities of the two methods over and over.\r\n\r\nCode to replicate the issue would be like:\r\n\r\n        import numpy as np\r\n        \r\n        # numpy.char.rjust\r\n        arr = np.array(['Mississippi', 'Rome', 'Washington'])\r\n        result = np.char.rjust(arr, 7)\r\n        print(result)\r\n        # Output: ['Mississ' '   Rome' 'Washing']\r\n        \r\n        # str.rjust\r\n        s = 'Washington'\r\n        s_2 = 'Rome'\r\n        print(s_2.rjust(7))\r\n        print(s.rjust(7))\r\n        # Output:    Rome\r\n                  Washington","![Screenshot (210)](https:\/\/github.com\/numpy\/numpy\/assets\/96521078\/2ea88e2c-ad85-4913-b0d1-ad94a51bfba7)\r\n\r\n\r\nI recommend chatGPT only for documenting purpose, not for coding. ","The `# Output: 'hello'`-line in your 2nd-answer example is not padded, although it is padded in your code screenshot - that's what I was referring to. ChatGPT itself claims to have written your 2nd answer. \r\n\r\nLet's please concentrate on the issue stated in the OP or any relevant information. ","This is happening because numpy stores string data in a fixed-width representation, as indicated in the dtype:\r\n\r\n```\r\nIn [4]: result.dtype\r\nOut[4]: dtype('<U7')\r\n```\r\n\r\n`U7` means each entry in the array contains at most 7 unicode code points. Everything after that is truncated. The docstring is correct in that the function just calls `str.rjust` on each array element, the issue is how the result is stored.\r\n\r\nCurrently `np.char.rjust` doesn't allow you to override the dtype of the result. It would be a nice, relatively straightforward improvement to allow specifying the result dtype size in all of the functions in `np.char`. Failing that, a doc improvement as suggested by the OP of this issue is worth doing, along with the other `np.char` functions where the output dtype is implicitly controlled by a width parameter. If you'd like to try that yourself, the code lives in the repository at [`numpy\/core\/defchararray.py`](https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/core\/defchararray.py), unlike most things in NumPy you'll only need to edit Python code so this is an unusually easy contribution for a first-time numpy contributor.\r\n\r\nFWIW, I'm also actively working on a [variable-width string dtype](https:\/\/github.com\/numpy\/numpy-user-dtypes\/tree\/main\/stringdtype) which doesn't have this issue:\r\n\r\n```\r\nIn [1]: from stringdtype import StringDType\r\n\r\nIn [2]: import numpy as np\r\n\r\nIn [3]: arr = np.array(['Mississippi', 'Rome', 'Washington'], dtype=StringDType())\r\n   ...: result = np.char.rjust(arr, 7)\r\n   ...: print(result)\r\n['Mississippi' '   Rome' 'Washington']\r\n```\r\n\r\nHopefully I'll be able to get it into numpy for numpy 2.0.","Well, rjust uses:\r\n```\r\n    a_arr = numpy.asarray(a)\r\n    width_arr = numpy.asarray(width)\r\n    size = int(numpy.max(width_arr.flat))\r\n    if numpy.issubdtype(a_arr.dtype, numpy.bytes_):\r\n        fillchar = asbytes(fillchar)\r\n    return _vec_string(\r\n        a_arr, type(a_arr.dtype)(size), 'rjust', (width_arr, fillchar))\r\n\r\n```\r\nTBH, I think it would be fair to set it to `max(size, a_arr.dtype.size)` (not valid code) with a release note."],"labels":["04 - Documentation"]},{"title":"ENH: Implement the fft extension in numpy.array_api","body":"### Proposed new feature or change:\n\nThis is just a tracking issue. I will probably work on this at some point. The [fft extension](https:\/\/data-apis.org\/array-api\/latest\/extensions\/fourier_transform_functions.html) in the array API needs to be implemented in `numpy.array_api`. This is somewhat blocked on (or should at least be developed in conjunction with) proper support in the [array API test suite](https:\/\/github.com\/data-apis\/array-api-tests).\r\n\r\nSee the discussion at https:\/\/github.com\/numpy\/numpy\/issues\/23880#issuecomment-1578050219. The functions should be mostly the same as those in `np.fft`, but there might be some small differences (hence the need for test suite support). ","comments":["Hi Aaron, here are two small differences that I have found. These are the only differences in the signatures that I have found, so I agree that the functions are mostly the same as those in `np.fft`.\r\n\r\n1. `fftfreq` and `rfftfreq` have the extra optional `device` keyword.\r\narray API: `fftfreq(n, \/, *, d=1.0, device=None)`\r\nNumPy: `fftfreq(n, d=1.0)`\r\n2. for the basic fft functions, the default value of `norm` is now `'backward'` not `None`.\r\narray API: `norm: Literal['backward', 'ortho', 'forward'] = 'backward'`\r\nNumPy: `fft(a, n=None, axis=-1, norm=None)`\n\nI don't know if 2. would require any work to be done. From the [NumPy docs](https:\/\/numpy.org\/doc\/stable\/reference\/routines.fft.html):\n> None is an alias of the default option \"backward\" for backward compatibility.","closed by gh-25317?"],"labels":["component: numpy.array_api"]},{"title":"BUG: floor_divide is not normal when  calculating the input containing inf ","body":"### Describe the issue:\r\n\r\nAccording to the IEEE 754 standard, when there is inf or -inf in the input of a division operation, the following processing method will be used:\r\n\r\n1. If the divisor is 0 and the dividend is infinite, the result is NaN (not a number).\r\n\r\n2. If both the divisor and the dividend are positive or negative infinity, the result is NaN.\r\n\r\n3. If the divisor is positive or negative infinity and the dividend is a finite number, the result is positive or negative infinity, depending on the sign bit.\r\n\r\n4. If the divisor is a finite number and the dividend is positive or negative infinity, the result is 0 or negative 0, depending on the sign bit.\r\n\r\nso\uff0c I think  that  the third point still applies to the floor_divide method.  Is that correct?\r\n\r\nThis case as follow, why the result is nan and -1? I think the right result is `[inf, -inf, -inf, inf, -0., -0., -0., -0.]`\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nprint(\"numpy_floor: \", np.floor_divide([np.inf, -np.inf, np.inf, -np.inf, -0.1000, -3.1406, 0.1000,  3.1406], [3.1406, 3.1406, -3.1406, -3.1406, np.inf, np.inf, -np.inf, -np.inf]))\r\n\r\n# numpy_floor:  [nan nan nan nan -1. -1. -1. -1.]\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\n1.21.6\r\n3.7.8 (default, Dec  7 2022, 18:47:01) \r\n[GCC 7.5.0]\r\n\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["I would have to think about it more, you may be right.  But just to note that NumPy uses the same logic as pure Python:\r\n```\r\n>>> float(\"inf\") \/\/ 1\r\nnan\r\n```\r\nThis probably drops out of the fact that internally a more complex `divmod()` is used.","Is there any theoretical basis or judgment logic for this result `float(\"inf\") \/\/ 1 => nan`? I'm confused, thanks","It's a consequence of the implementation of `float.__floordiv__` that starts by computing `mod = x % y` then finds the result by `div = (x - mod) \/ y` (plus some extra cleanup). For most finite inputs, this works better than truncating `x \/ y`. I'm not sure anyone thought to test the case of `inf \/\/ finite` (I certainly don't see any in the test suite). That result is likely not intentional, but a side effect of the general formula.","Hi I have made a pr with my attempt at this, may I request for some help reviewing it? thank you!","Hi I made changes to the pr according the the comments, may i request for a review? thank you!"],"labels":["00 - Bug","component: numpy.ufunc"]},{"title":"ENH: Add offset parameter to lib.format.open_memmap","body":"### Changes\r\n - Added `offset` parameter to `np.lib.format.open_memmap`\r\n\r\n\r\n### TODO\r\n - [ ] Add docstrings\r\n - [ ] Add tests (?)\r\n - [ ] Add release notes","comments":["Please add some context: why is this needed? Changes to APis usually need to hit the mailing list and be agreed upon. Also tests are missing.","@ganesh-k13 do you have an opinion.  If getting the offset is easy from the zip file, this might be a good approach for your gh-23823?  If it isn't very natural in that context, I would also be interested in your motivation @cTatu.","Yeah it definitely reduces a few lines for gh-23823. Perhaps exposing useful preset offsets may make more sense (to get magic for example), unless there is a different motivation for this change.","OK, if @ganesh-k13 is happy and it's a good approach for the `npz` use-case, I am :+1: on this.\r\n\r\nThe other PR would test this indirectly but since `format.py` is at least half public we should add a test here, I think.","Hi, thank you for your fast response.\r\n\r\nThe main motivation for this change is that for our distributed-computing framework we add a 4-byte identifier at the beginning of each numpy array that is written to disk. So this new parameter is a good solution to ignore that identifier.\r\nAlso, the `numpy.memmap` function already has this parameter so I thought it was natural that this function also had it since they do practically the same thing.\r\n\r\nI also find useful the idea of having preset offsets.\r\nRegarding the tests I could copy the offset test used in `np.memmap`","Ping @cTatu do you want to continue with the tests?  If you don't soon, I would encourage @ganesh-k13 to integrate this change into his PR and supersede this (or just fix it up here)."],"labels":["01 - Enhancement","component: numpy.lib"]},{"title":"MAINT: Turn old string formatting into f-strings (part 1)","body":"Do not modify `f2py` yet, it's perhaps the most complex part.\r\n\r\n<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n","comments":["The patch was created with an automated tool and largely manually verified?","This kind of change results in many conflicts to solve, both for me during the review and for other committers after the merge. Perhaps it's best to apply changes gradually, in small batches. This is the 1st batch.","I admit I am not a big fan of this kind of PR: it seems like churn I would like to avoid.\r\nIn any case, do not modify distutils, it is deprecated.","My idea was to clean up existing code base, so that _pyupgrade_ or _ruff_ can be added to CI jobs later on.","I want to be positive about this type of change in general, but I have to admit that we churning the repo a lot currently, and adding this on top feels like playing with fire especially at this time.","I understand. Is there a better time to push such changes? What if they were limited to parts of the code base (which parts?) that don't change that much?"],"labels":["03 - Maintenance"]},{"title":"ENH: Add mmap support for `NpzFile`","body":"### Changes\r\n\r\n- Added mmap_mode to `NpzFile`\r\n- Added zip support for `np.lib.format.open_memmap`\r\n\r\n### Credits\r\nThe core logic is based on @f0k comment: https:\/\/github.com\/numpy\/numpy\/issues\/5976#issuecomment-701322834\r\n\r\n### Todo\r\n\r\n- [ ] Refactor common code and the dirty bits. Marked with `XXX`\r\n- [ ] Add docstrings\r\n- [ ] Fix\/add tests\r\n- [ ] Bench (?)\r\n- [ ] Handle memory issues (?)\r\n\r\nresolves #5976","comments":["Yeah I noticed that. Makes sense to fix that first. I'll take a look at introducing something like context manager or a proxy object and then come back to this. Thanks for pointing to the issue. "],"labels":["01 - Enhancement","component: numpy.lib","56 - Needs Release Note.","59 - Needs tests","25 - WIP"]},{"title":"BUG: docs build CI job is not failing on error","body":"One of the docs examples has a missing import, and correctly produces an error in the CI log, yet the overall status is a very misleading green pass.\r\n\r\nhttps:\/\/app.circleci.com\/pipelines\/github\/numpy\/numpy\/19500\/workflows\/f8df500a-7943-4420-a3e7-bcee6f1b4b92\/jobs\/31883\r\n\r\n```\r\nreading sources... [100%] user\/basics.io.genfromtxt .. user\/whatisnumpy\r\n\r\n\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/numpy\/__init__.py:docstring of numpy.bartlett:78: WARNING: Exception occurred in plotting numpy-bartlett-1\r\n from \/home\/circleci\/repo\/doc\/source\/reference\/generated\/numpy.bartlett.rst:\r\nTraceback (most recent call last):\r\n  File \"\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/matplotlib\/sphinxext\/plot_directive.py\", line 481, in _run_code\r\n    exec(code, ns)\r\n  File \"<string>\", line 3, in <module>\r\nNameError: name 'plt' is not defined\r\n\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/numpy\/__init__.py:docstring of numpy.blackman:70: WARNING: Exception occurred in plotting numpy-blackman-1\r\n from \/home\/circleci\/repo\/doc\/source\/reference\/generated\/numpy.blackman.rst:\r\nTraceback (most recent call last):\r\n  File \"\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/matplotlib\/sphinxext\/plot_directive.py\", line 481, in _run_code\r\n    exec(code, ns)\r\n  File \"<string>\", line 3, in <module>\r\nNameError: name 'plt' is not defined\r\n\/home\/circleci\/repo\/doc\/source\/dev\/depending_on_numpy.rst:: ERROR: Anonymous hyperlink mismatch: 1 references but 0 targets.\r\nSee \"backrefs\" attribute for IDs.\r\n\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/numpy\/__init__.py:docstring of numpy.kaiser:100: WARNING: Exception occurred in plotting numpy-kaiser-1\r\n from \/home\/circleci\/repo\/doc\/source\/reference\/generated\/numpy.kaiser.rst:\r\nTraceback (most recent call last):\r\n  File \"\/home\/circleci\/repo\/venv\/lib\/python3.9\/site-packages\/matplotlib\/sphinxext\/plot_directive.py\", line 481, in _run_code\r\n    exec(code, ns)\r\n  File \"<string>\", line 3, in <module>\r\nNameError: name 'plt' is not defined\r\nlooking for now-outdated files... none found\r\npickling environment... done\r\n```","comments":[],"labels":["00 - Bug","05 - Testing"]},{"title":"MAINT: Overhaul and refactor NPY_OS_WIN*","body":"When #20884 added `NPY_OS_WIN64`, the meaning of `NPY_OS_WIN32` became ambiguous; some might take it to mean \"32-bit only\", while others might assume its original meaning of \"any Windows\".\r\n\r\nBesides that, the directives as written confuse the nature of `_WIN32`, which I understand to be always defined in any native Windows toolchain. Thus `#define NPY_OS_WIN64` would never be reached.\r\n\r\nThis commit disambiguates by removing `NPY_OS_WIN32` (in case anyone uses it in future without looking again at its definition), and adding `NPY_OS_WINDOWS` with `NPY_OS_WIN_xxBIT` children, in the pattern of `NPY_OS_BSD`.\r\n\r\nIt also removes `NPY_OS_MINGW`, which for the same reason never got defined. MINGW is not an OS, but a toolchain like MSVC. Maybe there could be `NPY_BUILD_MINGW` and `NPY_BUILD_MSVC` or some such?\r\n\r\nI'm not attached to this particular solution, or the names I've chosen. Input is welcome.\r\n\r\n@Mousius: I won't pretend to understand the stack alignment problem in #23399. In [loops_trigonometric.dispatch.c.src](https:\/\/github.com\/Mousius\/numpy\/commit\/fe5472fa4eae131ff9646d7c980c6c4081c10386#diff-4ce69850e0011abbbeb8d176091ca7297e13cc85da738df8c5d0616f54a3365aR47), can you confirm you wanted to force inlining on all windows systems, not just 32-bit?","comments":["@cbrt64 iirc the stack alignment is helping to encourage the compiler to place things in the correct registers whilst also working around the compiler not wanting to index into them directly (we don't have `get`\/`set` for lanes iirc). The `FINLINE` was for the 32-bit windows calling pattern which places everything on stack, the theory was it was overflowing due to some alignment issues; on 64-bit windows I believe this function should be safe as the 64-bit calling convention will use the floating point registers instead.\r\n\r\nTLDR; It **should** only need to inline on 32-bit windows","So, the current main branch is broken?  I am OK with changing the macros a bit, but it should have a release note.  It is probably also too late for doing this for the next release.\r\n\r\n@cbrt64 can I convince you to make a minimal fixup to merge quickly?  Sorry that I messed it up on the other PR.","I can do that. I'm thinking just use `_WIN64` like downstream is the simplest for now. Also it's already tested. Do you want a new PR, and keep this one for later, or just force push here?"],"labels":["03 - Maintenance"]},{"title":"Write release notes in markdown","body":"I noticed that NEP-51 was in markdown. I would like to propose that we start writing release notes in markdown, the reasons being:\r\n\r\n- We could post the release notes directly on the github page without translation. The current  translation isn't perfect and requires some work to fix up.\r\n- People tend to use markdown anyway when writing release note fragments, in particular single backticks for code bits. This also requires some work to fix up, mostly remembering the correct vim incantation to fix them all (I should write it down). Markdown is also common in the commit titles which tends to produce links when translated.\r\n\r\nI would guess that most folks on github are more familiar with markdown than rst, so using it shouldn't be a big deal as far as learning is concerned.","comments":["Hmmm, I would like markdown, and I think towncrier would be happy with it (I guess we might move away from that at some point, but a different thing).\r\n\r\nWhat I am a bit unsure about: The `.rst` file is included with sphinx right now, giving us easy doc linking.  I am not sure that is important, but I think we would have to give up on it?  (@rossbar, thoughts?)","Sphinx now accepts markdown, you can check that NEP-51 shows up. The reference formatting would need fixing, might be a towncrier thing to check.","Hmmm, is this just a confusion due to a screw up by me?  I guess you mean this: https:\/\/github.com\/numpy\/numpy\/blob\/main\/doc\/neps\/nep-0053-c-abi-evolution.md  (the file is also a lot of rst syntax, silly mix of things probably...)\r\n\r\nBut this NEP is just missing from the website currently.","Sphinx does claim to support markdown.","I certainly believe that! It may well just be the auto-inclusion of the file that is the problem right now (or the file actually being so much rst like that things fail).\r\n\r\nStill wonder if @rossbar knows what we would have to do to make it work (and how links to e.g. functions are formatted in markdown).","It will be quite a lot of work to add support for markdown in NEPs.\r\n- The script to create the TOC assumes a suffix of `rst [here](https:\/\/github.com\/numpy\/numpy\/blob\/f67467c21a1797becde3097661996f60df4080ff\/doc\/neps\/tools\/build_index.py#L21) and [here](https:\/\/github.com\/numpy\/numpy\/blob\/f67467c21a1797becde3097661996f60df4080ff\/doc\/neps\/tools\/build_index.py#L31), this is relatively easy to change\r\n- This [`.. _NEP53:`](https:\/\/github.com\/numpy\/numpy\/blob\/f67467c21a1797becde3097661996f60df4080ff\/doc\/neps\/nep-0053-c-abi-evolution.md?plain=1#L1) top line to create an anchor will not work\r\n- The [table of attributes](https:\/\/github.com\/numpy\/numpy\/blob\/f67467c21a1797becde3097661996f60df4080ff\/doc\/neps\/nep-0053-c-abi-evolution.md?plain=1#L7) at the top of the document will need reformatting\r\n\r\nThe document itself is in RST format, so should probably be renamed:\r\n- The titles are RST format\r\n- The code blocks are RST format\r\n- The links are RST format\r\n\r\nBut other than that, it all works :)","> But other than that, it all works :)\r\n\r\nhahaha, but the question here is about release note snippets, which is probably more worthwhile.  Limiting to release notes should remove at least two of the difficulties.","> I noticed that NEP-51 was in markdown.\r\n\r\nI'm confused by this - it looks to me like it is rst: https:\/\/raw.githubusercontent.com\/numpy\/numpy\/main\/doc\/neps\/nep-0051-scalar-representation.rst\r\n\r\n> Sphinx now accepts markdown, you can check that NEP-51 shows up.\r\n\r\nAFAIK (at least, from a quick skim of the sphinx 7 changelog) markdown support is still provided by sphinx extensions like `myst-parser`, not natively supported. NumPy could start using these extensions: e.g. scipy recently added myst-markdown support to their docs.  Note however there are some potential rough edges - for example, the myst extensions currently pin sphinx to 2 versions behind the currently supported release: see executablebooks\/myst-nb#493"],"labels":["04 - Documentation","03 - Maintenance"]},{"title":"PERF: arr[mask] vs arr.take vs ...","body":"### Describe the issue:\n\nI spoke the other day with @seberg about performance in indexing operations like `arr[:, mask]`.  He said that `arr[:, mask]` would in general be more performant than `arr.take(mask.nonzero()[0], axis=1)`.  This sent me down a rabbit hole and I ended up profiling a few variants that should be equivalent:\r\n\r\n```\r\ndef get_times(nrows, ncols, frac):\r\n    np.random.seed(3244634)\r\n    arr = np.random.randn(nrows, ncols)\r\n\r\n    mask = arr[:, 0] > frac\r\n    indices = mask.nonzero()[0]\r\n\r\n    results = {}\r\n\r\n    if nrows >= 10**6:\r\n        results[\"mask\"] = %timeit -o arr[mask]\r\n        results[\"take\"] = %timeit -o arr.take(indices, axis=0)\r\n        results[\"getitem\"] = %timeit -o arr[indices]\r\n        results[\"mask_T\"] = %timeit -o arr.T[:, mask].T\r\n        results[\"take_T\"] = %timeit -o arr.T.take(indices, axis=1).T\r\n        results[\"getitem_T\"] = %timeit -o arr.T[:, indices].T\r\n        results[\"pd_take\"] = %timeit -o pd.core.array_algos.take._take_nd_ndarray(arr, indices, 0, None, False)\r\n    else:\r\n        results[\"mask\"] = %timeit -o -n 100 arr[mask]\r\n        results[\"take\"] = %timeit -o -n 100 arr.take(indices, axis=0)\r\n        results[\"getitem\"] = %timeit -o -n 100 arr[indices]\r\n        results[\"mask_T\"] = %timeit -o -n 100 arr.T[:, mask].T\r\n        results[\"take_T\"] = %timeit -o -n 100 arr.T.take(indices, axis=1).T\r\n        results[\"getitem_T\"] = %timeit -o -n 100 arr.T[:, indices].T\r\n        results[\"pd_take\"] = %timeit -o -n 100 pd.core.array_algos.take._take_nd_ndarray(arr, indices, 0, None, False)\r\n\r\n    times = {key: results[key].average for key in results}\r\n    return times\r\n\r\nall_times = {}\r\nfor nrows in [10**4, 10**5, 10**6, 10**7]:\r\n    for ncols in [1, 2, 5, 10, 20]:\r\n        for frac in [0.01, 0.1, 0.5, 0.9, 0.99]:\r\n            key = (nrows, ncols, frac)\r\n            print(key)\r\n            times = get_times(nrows, ncols, frac)\r\n            all_times[key] = times\r\n\r\n\r\ndf = pd.DataFrame(all_times).T\r\ndf.index.names = [\"nrows\", \"ncols\", \"frac\"]\r\n```\r\n\r\nResults:\r\n```\r\n                         mask      take   getitem    mask_T    take_T  getitem_T   pd_take\r\nnrows    ncols frac                                                                       \r\n10000    1     0.01  0.000077  0.000016  0.000011  0.000065  0.000016   0.000012  0.000007\r\n               0.10  0.000062  0.000015  0.000011  0.000063  0.000015   0.000011  0.000008\r\n               0.50  0.000046  0.000009  0.000008  0.000048  0.000010   0.000008  0.000006\r\n               0.90  0.000033  0.000007  0.000005  0.000033  0.000009   0.000006  0.000004\r\n               0.99  0.000031  0.000007  0.000005  0.000031  0.000006   0.000005  0.000005\r\n         2     0.01  0.000112  0.000018  0.000058  0.000113  0.000041   0.000058  0.000020\r\n               0.10  0.000106  0.000015  0.000055  0.000103  0.000039   0.000053  0.000018\r\n               0.50  0.000076  0.000011  0.000036  0.000078  0.000029   0.000038  0.000013\r\n               0.90  0.000049  0.000007  0.000023  0.000051  0.000023   0.000023  0.000009\r\n               0.99  0.000045  0.000006  0.000020  0.000047  0.000020   0.000020  0.000008\r\n         5     0.01  0.000113  0.000020  0.000062  0.000115  0.000112   0.000063  0.000027\r\n               0.10  0.000107  0.000019  0.000056  0.000109  0.000107   0.000059  0.000023\r\n               0.50  0.000076  0.000013  0.000040  0.000078  0.000089   0.000040  0.000018\r\n               0.90  0.000055  0.000009  0.000026  0.000053  0.000069   0.000029  0.000014\r\n               0.99  0.000050  0.000009  0.000023  0.000051  0.000067   0.000024  0.000013\r\n         10    0.01  0.000116  0.000027  0.000065  0.000120  0.000266   0.000065  0.000030\r\n               0.10  0.000110  0.000025  0.000062  0.000114  0.000259   0.000062  0.000027\r\n               0.50  0.000080  0.000020  0.000042  0.000081  0.000207   0.000041  0.000025\r\n               0.90  0.000058  0.000013  0.000028  0.000057  0.000179   0.000030  0.000017\r\n               0.99  0.000052  0.000013  0.000025  0.000052  0.000173   0.000025  0.000016\r\n         20    0.01  0.000127  0.000041  0.000072  0.000126  0.000590   0.000074  0.000045\r\n               0.10  0.000116  0.000041  0.000070  0.000119  0.000581   0.000072  0.000048\r\n               0.50  0.000091  0.000029  0.000049  0.000090  0.000479   0.000052  0.000034\r\n               0.90  0.000062  0.000019  0.000033  0.000063  0.000425   0.000033  0.000022\r\n               0.99  0.000058  0.000017  0.000029  0.000055  0.000383   0.000030  0.000020\r\n100000   1     0.01  0.000615  0.000145  0.000097  0.000611  0.000148   0.000096  0.000040\r\n               0.10  0.000586  0.000136  0.000092  0.000583  0.000136   0.000092  0.000039\r\n               0.50  0.000441  0.000092  0.000062  0.000438  0.000092   0.000062  0.000030\r\n               0.90  0.000308  0.000057  0.000039  0.000316  0.000057   0.000038  0.000023\r\n               0.99  0.000288  0.000049  0.000034  0.000285  0.000061   0.000047  0.000025\r\n         2     0.01  0.001057  0.000147  0.000544  0.001058  0.000380   0.000544  0.000151\r\n               0.10  0.001004  0.000136  0.000510  0.001011  0.000370   0.000516  0.000139\r\n               0.50  0.000743  0.000094  0.000346  0.000737  0.000274   0.000347  0.000098\r\n               0.90  0.000483  0.000061  0.000214  0.000482  0.000204   0.000213  0.000061\r\n               0.99  0.000437  0.000054  0.000186  0.000437  0.000192   0.000188  0.000056\r\n         5     0.01  0.001110  0.000189  0.000607  0.001111  0.001161   0.000601  0.000223\r\n               0.10  0.001106  0.000181  0.000586  0.001133  0.001522   0.000652  0.000256\r\n               0.50  0.001214  0.000160  0.000452  0.000907  0.001049   0.000473  0.000154\r\n               0.90  0.000614  0.000112  0.000308  0.000695  0.000953   0.000311  0.000120\r\n               0.99  0.000535  0.000075  0.000214  0.000568  0.000656   0.000215  0.000085\r\n         10    0.01  0.001496  0.000472  0.001007  0.001597  0.006103   0.000835  0.000374\r\n               0.10  0.001505  0.000541  0.000852  0.001342  0.004994   0.000934  0.000464\r\n               0.50  0.001103  0.000219  0.000570  0.001040  0.004436   0.000470  0.000200\r\n               0.90  0.000566  0.000127  0.000270  0.000564  0.003215   0.000279  0.000128\r\n               0.99  0.000500  0.000109  0.000238  0.000497  0.003131   0.000250  0.000117\r\n         20    0.01  0.002713  0.001858  0.002482  0.003748  0.015310   0.002545  0.002491\r\n               0.10  0.003524  0.002116  0.002632  0.003054  0.015110   0.002466  0.002005\r\n               0.50  0.002123  0.001221  0.001536  0.002157  0.013702   0.001749  0.001330\r\n               0.90  0.000777  0.000225  0.000420  0.000808  0.011379   0.000368  0.000230\r\n               0.99  0.000582  0.000201  0.000334  0.000636  0.011296   0.000332  0.000163\r\n1000000  1     0.01  0.006450  0.001673  0.001226  0.006999  0.001945   0.001264  0.000658\r\n               0.10  0.006332  0.001648  0.001277  0.006851  0.001628   0.001126  0.000540\r\n               0.50  0.004786  0.001044  0.000751  0.004799  0.001137   0.000740  0.000426\r\n               0.90  0.003254  0.000669  0.000458  0.003273  0.000668   0.000430  0.000259\r\n               0.99  0.003316  0.000533  0.000398  0.003231  0.000547   0.000401  0.000239\r\n         2     0.01  0.012048  0.003198  0.007490  0.012737  0.010760   0.007934  0.003018\r\n               0.10  0.012960  0.003020  0.006884  0.012664  0.010378   0.007215  0.002756\r\n               0.50  0.009799  0.002221  0.005121  0.009651  0.008768   0.005023  0.002215\r\n               0.90  0.006719  0.001807  0.003327  0.006596  0.007651   0.003242  0.001741\r\n               0.99  0.006023  0.001619  0.002833  0.006001  0.006960   0.002937  0.001699\r\n         5     0.01  0.016283  0.006817  0.010179  0.016183  0.032980   0.009955  0.006914\r\n               0.10  0.015775  0.006343  0.009606  0.015628  0.032905   0.009766  0.006785\r\n               0.50  0.012279  0.005227  0.007719  0.012313  0.029272   0.008127  0.005674\r\n               0.90  0.010435  0.004113  0.007098  0.010487  0.026350   0.007374  0.004572\r\n               0.99  0.010019  0.003792  0.006680  0.010030  0.025908   0.006767  0.004254\r\n         10    0.01  0.021566  0.012923  0.016585  0.020971  0.100662   0.016741  0.013307\r\n               0.10  0.021519  0.012595  0.016652  0.021303  0.102315   0.015738  0.012211\r\n               0.50  0.018872  0.010349  0.013955  0.019079  0.097062   0.013947  0.010597\r\n               0.90  0.013848  0.006708  0.010081  0.013791  0.092073   0.010157  0.006989\r\n               0.99  0.012577  0.005956  0.009257  0.012645  0.092085   0.009183  0.006303\r\n         20    0.01  0.034928  0.024172  0.028407  0.035009  0.339010   0.028514  0.024243\r\n               0.10  0.034200  0.023771  0.027740  0.033919  0.341899   0.027568  0.023931\r\n               0.50  0.028142  0.019522  0.023404  0.028250  0.329279   0.023414  0.019702\r\n               0.90  0.018316  0.011894  0.015192  0.018337  0.317028   0.015234  0.011950\r\n               0.99  0.016700  0.010726  0.013791  0.016736  0.314524   0.013483  0.010789\r\n10000000 1     0.01  0.075609  0.023387  0.017521  0.077880  0.023426   0.016926  0.013651\r\n               0.10  0.073974  0.021879  0.016341  0.073254  0.021880   0.016493  0.013057\r\n               0.50  0.057912  0.015818  0.013160  0.058059  0.015887   0.013158  0.010421\r\n               0.90  0.038654  0.009910  0.008896  0.038642  0.009897   0.008997  0.007295\r\n               0.99  0.037461  0.009286  0.008410  0.037014  0.009149   0.008468  0.007257\r\n         2     0.01  0.118148  0.028719  0.063149  0.117240  0.144011   0.063728  0.029231\r\n               0.10  0.112227  0.027425  0.058993  0.113615  0.140042   0.060131  0.027359\r\n               0.50  0.084482  0.022841  0.049230  0.084631  0.129548   0.048744  0.022950\r\n               0.90  0.062597  0.018733  0.032421  0.062519  0.119284   0.031893  0.018181\r\n               0.99  0.061172  0.017828  0.029592  0.058220  0.118524   0.029290  0.018449\r\n         5     0.01  0.230349  0.129534  0.160529  0.212056  0.443435   0.161078  0.130775\r\n               0.10  0.220766  0.123149  0.163095  0.216893  0.534799   0.161821  0.123609\r\n               0.50  0.122455  0.052631  0.075124  0.121537  0.426743   0.077600  0.054333\r\n               0.90  0.105465  0.043727  0.070794  0.104175  0.403974   0.070125  0.048561\r\n               0.99  0.100057  0.037517  0.067227  0.099688  0.399199   0.066469  0.044731\r\n         10    0.01  0.348990  0.246103  0.289579  0.347541  1.469184   0.282955  0.244499\r\n               0.10  0.331451  0.228151  0.271604  0.328380  1.430803   0.272612  0.235470\r\n               0.50  0.270255  0.182151  0.224866  0.255167  1.323136   0.224629  0.185771\r\n               0.90  0.197208  0.118689  0.157459  0.191277  1.238476   0.159513  0.121140\r\n               0.99  0.127579  0.059402  0.096368  0.127093  1.167290   0.096096  0.063261\r\n         20    0.01  0.595607  0.473539  0.538422  0.597438  3.788825   0.520809  0.459594\r\n               0.10  0.557066  0.442453  0.502142  0.554500  3.696630   0.501840  0.437989\r\n               0.50  0.437317  0.332252  0.401684  0.442417  3.487455   0.397814  0.326209\r\n               0.90  0.281199  0.209124  0.252269  0.283901  3.302511   0.250339  0.207368\r\n               0.99  0.249273  0.181366  0.221780  0.248647  3.265589   0.219971  0.182562\r\n```\r\n\r\nThe first thing that stands out is that the transposed `take` can be much slower than the regular indexing:\r\n\r\n```\r\ndf2 = df[[\"mask\", \"take\", \"getitem\"]]\r\ndf3 = df[[\"mask_T\", \"take_T\", \"getitem_T\"]]\r\ndf3.columns = df2.columns\r\n\r\ndiv = df3 \/ df2\r\n\r\n>>> div.min()\r\nmask       0.747117\r\ntake       0.857143\r\ngetitem    0.824561\r\ndtype: float64\r\n>>> div.max()\r\nmask        1.381496\r\ntake       56.199005\r\ngetitem     1.382353\r\ndtype: float64\r\n```\r\n\r\n(The only cases where `div[\"take\"] < 1` have `ncols=1`)\r\n\r\nIn the pandas _take_nd_ndarray there is a check for f-contiguous arrays that I suspect was implemented because of this difference.\r\n\r\nThe pandas version behaves slightly differently from the numpy version, checking for -1 entries which are then filled with a fill_value.  Because of that my expectation going in was that it would underperform regular ndarray.take.\r\n\r\nBased on my conversation with seberg I expected the masking to be faster than take, but we have `(df[\"mask\"] > df[\"take\"]).all()`, by a factor of between 1.25 and 8.1.\r\n\r\nNot sure if this is actionable, but wanted to post it somewhere.\n\n### Reproduce the code example:\n\n```python\nNA\n```\n\n\n### Error message:\n\n```shell\nNA\n```\n\n\n### Runtime information:\n\nNA\n\n### Context for the issue:\n\nNA","comments":["Hello sir,\r\nI am very new to the open source contribution and want contribute to the above issue can you please tell me approach or sort of an idea to fix the above issue.\r\n","@Jay-sanjay this is not a simple issue to dive into.\r\n\r\n@jbrockmendel ah, thanks.  I have to check this closer.  I was thinking mostly of the \"dense\" case (where you have a 1-d array that gets indexed).  In this case, masks always get converted to integer indices.  One issue is maybe\/probably, that the out-of-bounds indexing check is maybe not inlined (I have to double check the code).\r\n\r\nIt is a bit annoying that the masked\/boolean version seems quite a bit slower, it does a `nonzero()` effectively, so maybe it is how it is...\r\n\r\nWould there be a specific thing that would help pandas a lot to look into?","> Would there be a specific thing that would help pandas a lot to look into?\r\n\r\nNot off the top of my head"],"labels":["00 - Bug"]},{"title":"Eager to Contribute to the NumPy Project and Assistance Request","body":"Dear NumPy Team,\r\n\r\nI recently came across the NumPy project and I am thrilled to see the amazing work being done by the community. As someone with a strong background in C, C++, Java, and Python, I believe I can contribute effectively to the project and help take it to new heights.\r\n\r\nAlthough I'm relatively new to the world of open source, I am extremely enthusiastic about becoming a part of it. I am eager to apply my skills and knowledge to a meaningful project like NumPy. However, I find myself unsure about where to get started and how my specific skills can fit into the project.\r\n\r\nI would greatly appreciate your guidance and advice on how I can best contribute to NumPy. If you could provide some direction or suggest specific areas where my expertise can be of value, it would be immensely helpful. I am open to taking on programming tasks, code reviews, documentation, or any other responsibilities that align with my skill set.\r\n\r\nAdditionally, while exploring the NumPy repository on GitHub (https:\/\/github.com\/numpy\/numpy), I noticed that the environment is currently experiencing failures.\r\n![image](https:\/\/github.com\/numpy\/numpy\/assets\/92091342\/3a2dddc7-053e-4b01-afc0-3caa2dcc6107)\r\n\r\n I believe this issue might be related to the development and deployment processes(Devops) . I would be more than happy to dive into this matter and contribute towards its resolution. If you could provide more information about this environment failure and assign the issue to me, I will gladly work on it.\r\nYour guidance and support would be invaluable in helping me understand the project's infrastructure and development processes. I am confident that by collaborating with the community, I can make a positive impact and help improve the NumPy project.\r\n\r\nThank you for your time and consideration. I look forward to your response and the opportunity to contribute to the NumPy project. @mattip @rgommers @charris @seberg @rossbar \r\n\r\nBest regards,\r\n@SANTHOSH-MAMIDISETTI","comments":["Thanks for the offer to help. I would suggest you start with the [how to contribute](https:\/\/numpy.org\/contribute\/) page.\r\n\r\nI think the error you pointed to is the [gitpod docker image deployment](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/4505870556\/jobs\/7932056067#step:9:490), which has been disabled. I think we should delete it (@charris \/ @seberg is this https:\/\/github.com\/numpy\/numpy\/settings\/environments the link to that deployment?)","> Thanks for the offer to help. I would suggest you start with the [how to contribute](https:\/\/numpy.org\/contribute\/) page.\r\n> \r\n> I think the error you pointed to is the [gitpod docker image deployment](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/4505870556\/jobs\/7932056067#step:9:490), which has been disabled. I think we should delete it (@charris \/ @seberg is this https:\/\/github.com\/numpy\/numpy\/settings\/environments the link to that deployment?)\r\n\r\nSubject: Re: Contribution Inquiry and Request for DevOps Tasks\r\n\r\nDear @mattip,\r\n\r\nThank you for your prompt response and for directing me to the [how to contribute](https:\/\/numpy.org\/contribute\/) page. I have carefully reviewed the provided documentation. While I am eager to contribute to the project, it seems that there are no specific guidelines or tasks related to DevOps at the moment. Please correct me if I missed any relevant information.\r\n\r\nIn light of this, I wanted to inquire if there are any tasks available that fall under the categories of `good first issue` or `difficulty - easy` . I believe working on such tasks would provide me with valuable experience and help me contribute effectively to the project. If there are any existing tasks fitting this description, I would greatly appreciate it if you could provide me with the relevant links or assign me to them directly.\r\n\r\nThank you once again for your assistance. I look forward to your guidance and the opportunity to contribute to the numpy community.\r\n\r\nBest regards,\r\n@SANTHOSH-MAMIDISETTI ","@SANTHOSH-MAMIDISETTI ,\nRather than ` good-first-issue ` we have ` sprintable `, since most of the issues we have in numpy are not very beginner friendly.\n\nYou can take a look at those issues as a start.","Hello , sir I have found a major bug in the code base\r\nthis is regarding the in built  function for  inverse of a matrix which is giving completely wrong output \r\nfor some test case's . It is literally given inverse for a matrix whose inverse don't even exist\r\nI want to fix this bug.","Hello @Jay-sanjay - please be aware that floating point issues can create unexpected results when trying to invert singular matrices. If you think you have found a bug, please file a [Bug report](https:\/\/github.com\/numpy\/numpy\/issues\/new?assignees=&labels=00+-+Bug&projects=&template=bug-report.yml&title=BUG%3A+%3CPlease+write+a+comprehensive+title+after+the+%27BUG%3A+%27+prefix%3E) with a full reproductible example and the details requested in the template. Thanks!"],"labels":["33 - Question"]},{"title":"BUG: Various tests failing in connection with float128 on AMD Barcelona","body":"### Describe the issue:\n\nI'm compiling numpy 1.24.2 with python 3.10.8 and gcc 12.2.0 on our old opteron based hpc. The compilation (through easybuild) seems to complete without errors. When running the tests (`numpy.test(verbose=2)`) I see multiple failures, most if not all of them in connection with dtype `g`\/`float128` producing `nan` values. \n\n### Reproduce the code example:\n\n```python\nimport numpy\r\nnumpy.test(verbose=2)\n```\n\n\n### Error message:\n\n```shell\nFirst error (will attach full test log in comment)\r\n\r\n=================================== FAILURES ===================================\r\n___________________________ TestCreation.test_zeros ____________________________\r\n\r\nself = <numpy.core.tests.test_multiarray.TestCreation object at 0x7fa3a28b39d0>\r\n\r\n    def test_zeros(self):\r\n        types = np.typecodes['AllInteger'] + np.typecodes['AllFloat']\r\n        for dt in types:\r\n            d = np.zeros((13,), dtype=dt)\r\n            assert_equal(np.count_nonzero(d), 0)\r\n            # true for ieee floats\r\n>           assert_equal(d.sum(), 0)\r\nE           AssertionError: \r\nE           Items are not equal:\r\nE            ACTUAL: nan\r\nE            DESIRED: 0\r\n\r\nd          = array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float128)\r\ndt         = 'g'\r\nself       = <numpy.core.tests.test_multiarray.TestCreation object at 0x7fa3a28b39d0>\r\ntypes      = 'bBhHiIlLqQpPefdgFDG'\r\n\r\n\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/tests\/test_multiarray.py:943: AssertionError\n```\n\n\n### Runtime information:\n\n`1.24.2`\r\n\r\n`3.10.8 (main, May  9 2023, 22:42:54) [GCC 12.2.0]`\r\n\r\n\r\n```\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['POPCNT'],\r\n                      'not_found': ['SSSE3',\r\n                                    'SSE41',\r\n                                    'SSE42',\r\n                                    'AVX',\r\n                                    'F16C',\r\n                                    'FMA3',\r\n                                    'AVX2',\r\n                                    'AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'BARCELONA',\r\n  'filepath': '\/sw\/easybuild\/amd\/k10\/2022b\/software\/OpenBLAS\/0.3.21-GCC-12.2.0\/lib\/libopenblas_barcelonap-r0.3.21.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 16,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'openmp',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21'},\r\n {'filepath': '\/sw\/easybuild\/amd\/k10\/2022b\/software\/GCCcore\/12.2.0\/lib64\/libgomp.so.1.0.0',\r\n  'internal_api': 'openmp',\r\n  'num_threads': 16,\r\n  'prefix': 'libgomp',\r\n  'user_api': 'openmp',\r\n  'version': None}]\r\nNone\r\n```\n\n### Context for the issue:\n\n_No response_","comments":["Full test log: \r\n[numpy_test.log](https:\/\/github.com\/numpy\/numpy\/files\/11463839\/numpy_test.log)\r\n","I wonder if the lack of SSSE3 is causing problems. @r-devulap any thoughts?","It would probably be good to see the build log.  You are compiling on the identical machine and not copying the binaries, I presume?  This seems likely a wrong long double setup, although a decent amount of things seem to succeed, but maybe that is just random.  In which case it may not be very troubling.  I see one more failure around `repr`.\r\n\r\nAlso, how did you install everything else?  Since this is an old HPC system, I wonder if you are using very old system libs (aside from that new compiler).","Thanks for looking into this. The system libs may be old but I'm not sure how relevant they are since I'm compiling against a toolchain that I bootstrapped on the same machine (again using Easybuild), including GCC 12.2, binutils, OpenBLAS, FlexiBLAS, FFTW, scaLAPACK, etc. \r\nI'm attaching the full easybuild log which should include all of the build output. I think the relevant part starts around line 2634. Let me know if there's anything else I can provide or try to get to the bottom of this. \r\n\r\n[easybuild-SciPy-bundle-2023.02-20230510.123253.rMdXK.log](https:\/\/github.com\/numpy\/numpy\/files\/11477267\/easybuild-SciPy-bundle-2023.02-20230510.123253.rMdXK.log)\r\n","As @seberg says, there is something wrong with the long double setup. From your logs: \r\n\r\n```\r\nx_f32      = array([37.45401], dtype=float32)\r\nx_f128     = array([nan], dtype=float128)\r\n```\r\n\r\nwhere, `x_f128 = np.longdouble(x_f32)`. Can't yet immediately think of a reason why a simple conversion fails.","Not sure it will be helpful, but @3rdcycle can you try something like:\r\n```\r\nimport numpy as np\r\nx = np.array([1], dtype=np.float32)\r\nprint(x.astype(np.longdouble))\r\nres = np.zeros(1, dtype=np.longdouble)\r\nnp.copyto(res, x)\r\nprint(res)\r\nprint(res.tobytes())\r\nprint(\"Try to probe 0.1 a bit (used to identify kind of long double):\")\r\nprint(np.array(\"0.1\", dtype=np.longdouble))\r\nprint(np.array(\"0.1\", dtype=np.longdouble).tobytes())\r\nprint(np.array(\"1\", dtype=np.longdouble) \/ np.array(\"10\", dtype=np.longdouble))\r\nprint((np.array(\"1\", dtype=np.longdouble) \/ np.array(\"10\", dtype=np.longdouble)).tobytes())\r\n```\r\n\r\nI recall seeing a float32 failure somewhere in the tests, so it may yet be something else like SSE3\/SSE3?","@seberg this is the result I get:\r\n\r\n```\r\n[1.]\r\n[1.]\r\nb'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\xff?\\x00\\x00\\x00\\x00\\x00\\x00'\r\nTry to probe 0.1 a bit (used to identify kind of long double):\r\n0.1\r\nb'\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb?\\xde\\x01\\x00\\x00\\x00\\x00'\r\n0.1\r\nb'\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb?\\x9c\\x80\\x8b\\x7f\\x00\\x00'\r\n```\r\n\r\nI tried to run the first failing test in isolation and to my surprise it passes:\r\n\r\n```\r\n$ pytest test_multiarray.py::TestCreation::test_zeros\r\n================================================================================================ test session starts =================================================================================================\r\nplatform linux -- Python 3.10.8, pytest-7.2.0, pluggy-1.0.0\r\nrootdir: \/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\r\nplugins: hypothesis-6.68.2, xdist-3.1.0\r\ncollected 1 item                                                                                                                                                                                                     \r\n\r\ntest_multiarray.py .                                                                                                                                                                                           [100%]\r\n\r\n================================================================================================= 1 passed in 0.20s ==================================================================================================\r\n```\r\n\r\nWhen I run the whole test suite that test fails consistently \ud83e\udd28.\r\n\r\n","Hmmm, IIRC there was a test failure unrelated to longdouble somewhere also.  I think Matti's point about SSE3 availability may be related.  I thought maybe its using the garbage, but the garbage differs between the two last examples and both print fine and give the expected result (interesting that it writes 12 bytes, not 10, but I guess it makes sense).\r\n\r\nI am suspecting Matti's initial thought is likely right.  Something to do with the older instruction set.  Would be nice to know which test makes things start to break (if there is one), but not sure how to best track that down (i.e. is it a something that uses SSE3 explicitly?)","According to what I find in `npy_cpu_dispatch_config.h` SSSE3 is a dispatchable optimization. My arch does support SSE3 (one less 'S'):\r\n\r\n``` c\r\n\/*\r\n * AUTOGENERATED DON'T EDIT\r\n * Please make changes to the code generator (distutils\/ccompiler_opt.py)\r\n*\/\r\n#define NPY_WITH_CPU_BASELINE  \"SSE SSE2 SSE3\"\r\n#define NPY_WITH_CPU_DISPATCH  \"SSSE3 SSE41 POPCNT SSE42 AVX F16C FMA3 AVX2 AVX512F AVX512CD AVX512_SKX AVX512_CLX AVX512_CNL AVX512_ICL\"\r\n```\r\n\r\nSo in my understanding the SSSE3 code paths should never get used at all.\r\n\r\n\r\nI ran some of the other failing tests individually to see if I could narrow it down some more. In `test_multiarray.py::TestCreation::test_zeros_like_like_zeros` (line 64534 in my logs) I found that the longdouble test fails **only if ran after the floating point (`f`) test**:\r\n\r\n``` python\r\nimport numpy as np\r\n\r\n# test zeros_like returns the same as zeros\r\nfor c in 'fg':\r\n    print(f\"Typecode: {c}\")\r\n    d = np.zeros((3,3), dtype=c)\r\n    print(d)\r\n    print(f\"{np.zeros_like(d) == d}\")\r\n```\r\nResult:\r\n```\r\nTypecode: f\r\n[[0. 0. 0.]\r\n [0. 0. 0.]\r\n [0. 0. 0.]]\r\n[[ True  True  True]\r\n [ True  True  True]\r\n [ True  True  True]]\r\nTypecode: g\r\nTraceback (most recent call last):\r\n  File \"\/home\/pikett\/nptest.py\", line 7, in <module>\r\n    print(d)\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 1588, in _array_str_implementation\r\n    return array2string(a, max_line_width, precision, suppress_small, ' ', \"\")\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 736, in array2string\r\n    return _array2string(a, options, separator, prefix)\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 513, in wrapper\r\n    return f(self, *args, **kwargs)\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 546, in _array2string\r\n    lst = _formatArray(a, format_function, options['linewidth'],\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 889, in _formatArray\r\n    return recurser(index=(),\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 876, in recurser\r\n    nested = recurser(index + (-i,), next_hanging_indent,\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 845, in recurser\r\n    word = recurser(index + (-i,), next_hanging_indent, next_width)\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 799, in recurser\r\n    return format_function(a[index])\r\n  File \"\/tmp\/eb-i46u0s0e\/tmpzz3f8r08\/lib\/python3.10\/site-packages\/numpy\/core\/arrayprint.py\", line 1039, in __call__\r\n    return dragon4_positional(x,\r\nTypeError: 'NoneType' object cannot be interpreted as an integer\r\n```\r\n\r\n\r\nWhereas if I replace the `f` in the typecode sequence with something else: \r\n``` python\r\nimport numpy as np\r\n\r\n# test zeros_like returns the same as zeros\r\nfor c in 'gg':\r\n    print(f\"Typecode: {c}\")\r\n    d = np.zeros((3,3), dtype=c)\r\n    print(d)\r\n    print(f\"{np.zeros_like(d) == d}\")\r\n```\r\n\r\nResult:\r\n```\r\nTypecode: g\r\n[[0. 0. 0.]\r\n [0. 0. 0.]\r\n [0. 0. 0.]]\r\n[[ True  True  True]\r\n [ True  True  True]\r\n [ True  True  True]]\r\nTypecode: g\r\n[[0. 0. 0.]\r\n [0. 0. 0.]\r\n [0. 0. 0.]]\r\n[[ True  True  True]\r\n [ True  True  True]\r\n [ True  True  True]]\r\n```\r\n\r\nI've tried other type code sequences (`fdg`, `iefdg`, `iedg`), as long as `f` runs before `g`, `g` breaks. I have no idea what to make of this.\r\n\r\n","Almost sounds like a compiler bug. Were you able to compile with other gcc\/numpy versions?","What compiler flags are being used?","@charris I haven't tried with other toolchains yet (need to build those first which will take a while). As for the compiler flags, here are the ones that were used e.g. for the multiarray code:\r\n```\r\ncompile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DHAVE_CBLAS -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/multiarray -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/umath -Inumpy\/core\/include -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/include\/numpy -Ibuild\/src.linux-x86_64-3.10\/numpy\/distutils\/include -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/npysort -Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/Python\/3.10.8-GCCcore-12.2.0\/include\/python3.10 -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/npymath -c'\r\nextra options: '-msse -msse2 -msse3'\r\n``` \r\n\r\n\r\nFrom where I stand there are a few things I can try now:\r\n- Build numpy with `setuptools < 60.0` as advised by the deprecation warning at the start of the build\r\n- Build numpy with `--cpu-baseline=native` and  `--cpu-dispatch=none` (not sure if it's worth trying this - if it did hit illegal instructions I guess I would have seen a core dump)\r\n- Build numpy with an older toolchain to see if it might be a compiler bug (any recommended versions?)","I think that misses a bunch of things, so here is the rest:\r\n```\r\nINFO: compile options: '-Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/Python\/3.10.8-GCCcore-12.2.0\/include\/python3.10 -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/common -Ibuild\/src.linux-x86_64-3.10\/numpy\/core\/src\/npymath -c'\r\nextra options: '-msse -msse2 -Werror'\r\nINFO: CCompilerOpt.cc_test_flags[1077] : testing flags (-msse3)\r\nINFO: C compiler: gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -O2 -ftree-vectorize -march=native -fno-math-errno -fPIC -O2 -ftree-vectorize -march=native -fno-math-errno -fPIC -O1 -march=native -fno-math-errno -fPIC -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/FFTW\/3.3.10-GCC-12.2.0\/include -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include -I\/sw\/easybuild\/amd\/k10\/2022b\/software\/FlexiBLAS\/3.2.1-GCC-12.2.0\/include\/flexiblas -fPIC\r\n```\r\nIts a bit weird how there is a `-O1` towards the end, `-fno-math-errno` should be harmless I think (dunno where it comes from, but it probably makes sense almost everywhere if the compiler isn't smart enough anyway).  None of this is weird, `-ftree-vectorize` comes from NumPy probably.\r\n\r\nAnything that is older than GCC 12 and not ancient probably makes sense, but I have no recommendation.  I am not sure the other two are likely to matter (although I am almost surprised it works with newer setuptools).","I went back and switched to an older compiler as suggested by @seberg. I first used GCC 10 to compile numpy 1.22.3 - this passes all tests. I then used the same compiler to compile 1.24.2 - this builds fine but the same tests fail as before with GCC 12. I'm going to take another in depth look next week to see if I can spot any other differences in my build setup but at this point I don't believe the compiler is really the issue.","I did some more tests with GCC 12 and numpy versions 1.22.3, 1.23.5, 1.24.0 and 1.24.2. Tests seem to pass for anything < 1.24, regardless of the compiler version. Unfortunately, I don't have time to dig into this any further for the moment. It may be helpful if anyone with a Barcelona CPU could confirm (or refute) that the following fails on 1.24 for them too: \r\n\r\n``` python\r\nimport numpy as np\r\n\r\n# Replace 'f' with anything else and the 'g' test will pass\r\nfor c in 'fg':\r\n    d = np.zeros((3,3), dtype=c)\r\n    print(np.zeros_like(d) == d)\r\n```\r\n\r\nResult:\r\n```\r\n[[ True  True  True]\r\n [ True  True  True]\r\n [ True  True  True]]\r\n[[False  True  True]\r\n [ True  True  True]\r\n [ True  True  True]]\r\n```","@seiko2plus may have some background to have an idea of what may be going on.  If this has to do with our SIMD code, it would have to be as part of the `==` implementation for float32 leading to issues in later code that is not (or only auto) vectorized, though.\r\n\r\nUnless the old CPU model means we are bound to run into issues?  (in which case I wonder if there might be an easy way to hack some sanity checks on startup to inform users)","I couldn't produce the test failure, I used the compiler `(GCC) 12.2.0` also disabled all default dispatched features, and only enabled `SSE SSE2 SSE3` similar to `BARCELONA` capabilities.\r\n\r\n> Replace 'f' with anything else and the 'g' test will pass\r\n\r\nThis issue may be related to the x87 FPU operating environment (compiler bug).\r\n\r\n@3rdcycle, Have you tried to build NumPy without SIMD features?\r\n```\r\npython setup.py build --cpu-baseline=none --cpu-dispatch=none install --user\r\n```\r\nand without enabling any optimizations:\r\n```\r\npython setup.py build --disable-optimization install --user \r\n```\r\n\r\n","Thanks for the suggestions @seiko2plus. It seems that these options don't make a difference in my case though. The only way I can make the tests pass seems to be by adding `-march=x86-64`, neither `-march=barcelona` or `-march=native` would work with any combinations of the options you suggested. \r\n\r\nWould this observation be compatible with the compiler bug you mentioned?"],"labels":["00 - Bug"]},{"title":"MAINT: speed up matmul #23588","body":"<!--         ----------------------------------------------------------------\r\n                MAKE SURE YOUR PR GETS THE ATTENTION IT DESERVES!\r\n                ----------------------------------------------------------------\r\n\r\n*  FORMAT IT RIGHT:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#writing-the-commit-message\r\n\r\n*  IF IT'S A NEW FEATURE OR API CHANGE, TEST THE WATERS:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#get-the-mailing-list-s-opinion\r\n\r\n*  HIT ALL THE GUIDELINES:\r\n      https:\/\/numpy.org\/devdocs\/dev\/index.html#guidelines\r\n\r\n*  WHAT TO DO IF WE HAVEN'T GOTTEN BACK TO YOU:\r\n      https:\/\/www.numpy.org\/devdocs\/dev\/development_workflow.html#getting-your-pr-reviewed\r\n-->\r\n\r\nShould provide a solution for #23588\r\n\r\nFurther speed\/memory optimizations are still possible (I'll work on it these days), but initial working solution that does the trick. Open for feedback already.\r\n\r\nCloses gh-23123, gh-23588","comments":["Thanks for the PR. Please add benchmark results so we can measure the speedup.","@xor2k you asked yesterday about what to do here.\r\nFirst, we should have very basic additional benchmarks I think.  Nothing fancy, just one that shows a good speedup.  That is largely because it is de-facto policy to have a chance of avoiding future regression.\r\n\r\nThis is maybe mainly a general improvement, on 1.23.4 (probably with openblas), I get:\r\n```\r\nIn [1]: a = np.ones((10000, 2, 4))[:, :, ::2]\r\nIn [2]: %timeit a @ a\r\n78.4 \u00b5s \u00b1 204 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\nIn [3]: a = np.ones((10000, 2, 2))\r\nIn [4]: %timeit a @ a\r\n507 \u00b5s \u00b1 537 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```\r\nwith your branch, the first one goes into the same as the contiguous one:\r\n```\r\nIn [2]: %timeit a @ a\r\n615 \u00b5s \u00b1 2.98 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n```\r\nWe can see that the blas overhead is significant.\r\n\r\nI don't want to map that out exactly, and yes different blas versions will behave very differently.  Honestly, we also can opt to ignore it, because currently we already run into the \"slow\" path when the input is contiguous.\r\nBut maybe we can find a *very rough* heuritistc to avoid the 10x slowdown?!\r\n\r\nAs for the discussion about order.  Lets ignore it, I think it is irrelevant, sorry.  If arrays are tiny, the cache will take care of things.  If (working) arrays are large, the matmul should dominate a huge amount anyway.\r\n\r\n---\r\n\r\nThe other point is that we still need to fix the error handling if the malloc fails.  If it fails, you need to grab the GIL and set a memory error, you can grep for `NPY_ALLOW_C_API_DEF` to see the pattern.  Admittedly, its slightly terrifying, because without using the new API (which is a lot more work), we may call the inner-loop multiple times and rely on `malloc` failing the same way each time.\r\nIt should be safe in the sense that we definitely get a memory error at the end, even if the `malloc` later decides to be successful.\r\n\r\nAs far as I understood Matti, he also thought that going into the (potentially super slow) fallback path when we are so low on memory, is probably not useful.","Okay, I understand. However, this barely affects this pull request as this is an issue that affects the use of BLAS in general, so it has affected Numpy even before my pull request. Other programming languages have reported that \"issue\" (won't call it bug) as well, see a long discussion with benchmarks going back to 2013 e.g.\r\n\r\nhttps:\/\/github.com\/JuliaLang\/julia\/issues\/3239\r\n\r\nSo one can basically compile Numpy without BLAS support, e.g. by running\r\n```\r\nNPY_BLAS_ORDER= NPY_LAPACK_ORDER= pip install -e .\r\n```\r\nin a test venv\/conda environment and many small matrix multiplications will be much faster. I suggest to open a fresh issue for the many-small-matmul performance. I have some ideas already.\r\n\r\nI'll be on vacation next week but will be available the week after.","> @xor2k you asked yesterday about what to do here. First, we should have very basic additional benchmarks I think. Nothing fancy, just one that shows a good speedup. That is largely because it is de-facto policy to have a chance of avoiding future regression.\r\n> \r\n> This is maybe mainly a general improvement, on 1.23.4 (probably with openblas), I get:\r\n> \r\n> ```\r\n> In [1]: a = np.ones((10000, 2, 4))[:, :, ::2]\r\n> In [2]: %timeit a @ a\r\n> 78.4 \u00b5s \u00b1 204 ns per loop (mean \u00b1 std. dev. of 7 runs, 10,000 loops each)\r\n> In [3]: a = np.ones((10000, 2, 2))\r\n> In [4]: %timeit a @ a\r\n> 507 \u00b5s \u00b1 537 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n> ```\r\n> \r\n> with your branch, the first one goes into the same as the contiguous one:\r\n> \r\n> ```\r\n> In [2]: %timeit a @ a\r\n> 615 \u00b5s \u00b1 2.98 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n> ```\r\n> \r\n> We can see that the blas overhead is significant.\r\n>\r\n\r\nThe main issue is that BLAS does not support \"batched matrix multiply\" BMM, which would be really useful in cases like that, compare\r\n\r\nhttps:\/\/developer.nvidia.com\/blog\/cublas-strided-batched-matrix-multiply\/\r\nhttps:\/\/www.intel.com\/content\/www\/us\/en\/developer\/articles\/technical\/introducing-batch-gemm-operations.html\r\n\r\nIn PyTorch, they have it:\r\n\r\nhttps:\/\/pytorch.org\/docs\/stable\/generated\/torch.bmm.html\r\nhttps:\/\/stackoverflow.com\/questions\/62015172\/how-to-find-c-source-code-of-torch-bmm-of-pytorch\r\n\r\nBMM would allow to skip the outer loop and take the batch dimension (10000) as an argument into the accelerated call instead of making it a loop of size 10000. \r\n\r\nI see these options:\r\n1. Ignore the problem. It existed before, introduces a inefficiency of a magnitude and my commit only worsens it by a few percent.\r\n2. Try to figure out how exactly PyTorch does a BMM on a CPU and try to reimplement it\r\n3. Use PyTorch as a backend for Numpy\r\n4. Provide an option `method=force_naive` to the matmul function so that the user can override that manually\r\n5. Run a mini-benchmark when calling `import numpy`. Importing Numpy takes very long anyway, a few milliseconds of benchmark won't change that a lot. Then, use these results of the benchmark to decide whether to use naive multiplication or BLAS on a per-call basis.\r\n6. Run Benchmarks from 5 at build time. Would complicate the build though and the systems where Numpy will be run might differ from those where it was build. \r\n7. Hard-code some plausible thresholds based on benchmarks with modern systems. I've got a MacBook M1 and Ryzen 5950X at hand.\r\n\r\nAny thoughts?","Only 1 and 7 sound reasonable to me everything else is unnecessarily complicated or just impossible.","I would argue that ignoring the problem is perfectly fair (vote for option 1). \r\n\r\nThis pull request aims to fix a ~100 times slow down caused by completely disregarding BLAS and defaulting to an extremely na\u00efve matrix multiply. The only users that will notice a performance reduction are ones that are explicitly taking advantage of this _bug\/feature_ to trick numpy into avoiding BLAS for small arrays by breaking the strides.\r\n\r\nIn any case, a 100 times speedup on large operations (seconds) is worth more than a small offset on small operations. Overhead is always present in numpy and users tend to be aware of this trade-off which gives good performance for large operations.","Sorry for my late answer. I have made some tests and unfortunately could not come up with safe values to hardcode. I would also vote for option 1.","Could you run the benchmarks again?"],"labels":["03 - Maintenance"]},{"title":"ENH: Update numpy.typing.ArrayLike to allow specification of both dtype and shape\/ndim","body":"### Proposed new feature or change:\n\nPlease update ArrayLike to include a strict shape or range of shapes, or `ndim`, but not both.\r\n\r\nRationale: I would like to be able to specify that custom type annotation RGBImage should be something akin to `TRGBImage=numpy.typing.NDArray[dtype=np.uint8, ndim=3]`\r\n\r\nBy similar reasoning, the following use cases would also be relevant\r\n\r\n```\r\nTMask = numpy.typing.NDArray[dtype=bool, ndim=2]\r\nTClusterMap =  numpy.typing.NDArray[dtype=int, ndim=2]\r\nTHeatmap = numpy.typing.NDArray[dtype=float, ndim=2]\r\n```","comments":["So `npt.NDArray` was specifically introduced in order to provide a more conveniently accessible type alias for expressing arrays with a given (`np.generic`-based) data type and an arbitrary\/unknown dimensionality and shape; being equivalent to `np.ndarray[Any, np.dtype[~T]]`. Introducing an equivalent type alias for arrays with a known size & shape has been on my wish list for a long time, though a blocker here is the lack of shape-typing support in mypy, the lack of which makes me extremely relucant to move forward with any shape-related typing changes (xref https:\/\/github.com\/numpy\/numpy\/issues\/16544).\r\n\r\nAs for in the mean time? You could considering using [PEP 593](https:\/\/peps.python.org\/pep-593\/)'s [`typing.Annotated`](https:\/\/docs.python.org\/3\/library\/typing.html#typing.Annotated), which allows you to attach arbitrary metadata to an annotation (even though this extra metadata will generally be useless for typecheckers). Something like this for example:\r\n\r\n```python\r\nfrom typing import Annotated\r\nimport numpy as np\r\nimport numpy.typing as npt\r\n\r\nTMask = Annotated[npt.NDArray[np.bool_], 2]\r\nTClusterMap =  Annotated[npt.NDArray[np.int64], 2]\r\nTHeatmap =  Annotated[npt.NDArray[np.float64], 2]\r\n```\r\n\r\nLastly, there was a proposal some time ago (see [PEP 637](https:\/\/peps.python.org\/pep-0637\/)) to allow indexing with keyword arguments, but as it got rejected the likes of `NDArray[dtype=bool, ndim=2]` are simply a no-go.","Dear Bas van Beek,\n\nThank you for your quick reply. In my codebase, I did end up using\ntyping.Annotated as you demonstrated. My only concern is that it is\nrelevant to specify either shape or ndim in the type, but not both (as ndim\nis inferred from shape)\n\nIm not super familiar with python type system yet. Maybe it would be\npossible to make TypedNDArray which inherits from ndarray, and just has the\nclass variables Dtype, Shape, and Ndim set explicitly\n\nThank you,\nMichael Sohnen\n\n\n\nOn Fri, May 12, 2023 at 9:03\u202fAM Bas van Beek ***@***.***>\nwrote:\n\n> So npt.NDArray was specifically introduced in order to provide a more\n> conveniently accessible type alias for expressing arrays with a given (\n> np.generic-based) data type and an arbitrary\/unknown dimensionality and\n> shape; being equivalent to np.ndarray[Any, np.dtype[~T]]. Introducing an\n> equivalent type alias for arrays with a known size & shape has been on my\n> wish list for a long time, though a blocker here is the lack of\n> shape-typing support in mypy, the lack of which makes me extremely relucant\n> to move forward with any shape-related typing changes (xref #16544\n> <https:\/\/github.com\/numpy\/numpy\/issues\/16544>).\n>\n> As for in the mean time? You could considering using PEP 593\n> <https:\/\/peps.python.org\/pep-593\/>'s typing.Annotated\n> <https:\/\/docs.python.org\/3\/library\/typing.html#typing.Annotated>, which\n> allows you to attach arbitrary metadata to an annotation (even though this\n> extra metadata will generally be useless for typecheckers). Something like\n> this for example:\n>\n> from typing import Annotatedimport numpy as npimport numpy.typing as npt\n> TMask = Annotated[npt.NDArray[np.bool_], 2]TClusterMap =  Annotated[npt.NDArray[np.int64], 2]THeatmap =  Annotated[npt.NDArray[np.float64], 2]\n>\n> Lastly, there was a proposal some time ago (see PEP 637\n> <https:\/\/peps.python.org\/pep-0637\/>) to allow indexing with keyword\n> arguments, but as it got rejected the likes of NDArray[dtype=bool, ndim=2]\n> are simply a no-go.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/numpy\/numpy\/issues\/23745#issuecomment-1545713345>, or\n> unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AHALSE5OCUY2IWCGHV53SKLXFYYLDANCNFSM6AAAAAAX5D7ETA>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n","You can already do something like `Vec3: typing.TypeAlias = np.ndarray[typing.Literal[3], np.dtype[T]]`, where `T = TypeVar('T', bound=np.generic)`."],"labels":["01 - Enhancement","57 - Close?","Static typing"]},{"title":"ENH: np.hypot(x1: complex, x2: complex) -> complex","body":"hypot currently doesn't work for complex inputs (and this is not documented).  I can't see any good reason for this to not be implemented.\r\n\r\n```\r\n>>> import numpy as np\r\n>>> np.hypot(1,1j)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: ufunc 'hypot' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\r\n```","comments":["What would be wanted? `sqrt(|z1|^2+|z2|^2)` or without the absolute values? I assume with (i.e., output is real), but `np.square` does `z^2` not `|z|^2` (sadly).","I think this not implemented on purpose. Python complex math [cmath module](https:\/\/docs.python.org\/3\/library\/cmath.html) does not implement `hypot`, and as far as I can tell it is not part of the [C99 complex spec](https:\/\/en.cppreference.com\/w\/c\/numeric\/complex). Is there an accepted (i.e. standards-based) way to compute this?","And what is the use case for this operation on complexes?","We would simply want `sqrt(x1**2+x2**2)`, but computed in a numerically stable way without risk of overflow.   These are the same reasons the ordinary hypot is included in most math libraries.   The expression `sqrt(x1**2+x2**2)` comes up frequently in numerical linear algebra, where the inputs can be complex.  My use case is less common, in some power series calculations involving Bessel functions.","So, you actually want `x1**2`, not `|x1|^2`? Could you give an actual example for the complex case?","If you want - but it's very specialized and won't mean much.   If you want to integrate `J0(b*t)*cos(c*x)\/t`, where `t=sqrt(a**2-x**2)`, over `x=0...a`, and where `b` and `c` are complex, then there is a closed formula in terms of `hypot(b,c)`.","I'm curious about the usage in linear algebra. I'd expect to see a complex version in LAPACK, but I [only see one for reals](https:\/\/www.netlib.org\/lapack\/explore-html-3.6.1\/d7\/d43\/group__aux_o_t_h_e_rauxiliary_gacf4c47c2f593fb3a4e842bca6df1240b.html#gacf4c47c2f593fb3a4e842bca6df1240b).\r\n\r\nI guess I wouldn't reject a contribution of one that adapts the simple real-valued scheme that pulls the larger argument out of the square root, but I'd want to see some tests that confirm that it does help, which I think might be original research that I'm not sure anyone's going to volunteer to do. There will also need to be tests to ensure that the rearrangement is doing the right thing with respect to branch cuts and signed zeros and all of that annoying complex stuff.","[Julia](https:\/\/github.com\/JuliaLang\/julia\/issues\/31941) and [MATLAB](https:\/\/www.mathworks.com\/help\/matlab\/ref\/hypot.html) implement `hypot(z1, z2)` as `sqrt(abs(z1)**2 + abs(z2)**2)`, FWIW.","Given the prior art, it seems to me that the only definition that we could consider adding is the one returning a real value, and that is not the one requested.  (Whether it was thought through or not, but the Julia discussion seems to have a tendency towards that definition at least in hindsight.)\r\n\r\nThe way to go for you is probably a custom ufunc outside of NumPy.  If you use such tools, you could and don't want to dive into how that works, you could also write it using a tool like `@numba.vectorize`.","Ok, I can manage with my own implementation.   But could at least the docs be updated to mention that hypot only works for floats?","> But could at least the docs be updated to mention that hypot only works for floats?\r\n\r\nHappy if someone has a look if it makes sense, we have probably quite a few other similar cases...\r\n\r\nTBH, I think it is more useful to update the error message to include the complex information, chances are you see the error before you read the docs anyway.  (But I am pretty sure we have an issue elsewhere for that.)"],"labels":["01 - Enhancement","23 - Wish List","57 - Close?"]},{"title":"BUG: Stack overflow on double inheritance from numpy.flexible and numpy.ma.core.MaskedArray","body":"### Describe the issue:\n\n```\r\n>>> import numpy\r\n>>> numpy.__version__\r\n'1.24.3'\r\n>>> class X(numpy.flexible, numpy.ma.core.MaskedArray): pass\r\n... \r\n>>> with memoryview(X()): pass\r\n... \r\nzsh: segmentation fault  python\r\n```\n\n### Reproduce the code example:\n\n```python\nimport numpy\r\nclass X(numpy.flexible, numpy.ma.core.MaskedArray): pass\r\nwith memoryview(X()): pass\n```\n\n\n### Error message:\n\n```shell\n(lldb) target create \"\/Users\/jelle\/py\/venvs\/py311\/bin\/python\"\r\nCurrent executable set to '\/Users\/jelle\/py\/venvs\/py311\/bin\/python' (arm64).\r\n(lldb) r \r\nProcess 62767 launched: '\/Users\/jelle\/py\/venvs\/py311\/bin\/python' (arm64)\r\nPython 3.11.1 (main, Dec 21 2022, 16:19:04) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\n>>> numpy.__version__\r\n'1.24.3'\r\n>>> class X(numpy.flexible, numpy.ma.core.MaskedArray):\r\n...     pass\r\n... \r\n>>> with memoryview(X()): pass\r\n... \r\nProcess 62767 stopped\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x16edfffc0)\r\n    frame #0: 0x00000001a4226994 libsystem_malloc.dylib`nanov2_allocate_from_block + 8\r\nlibsystem_malloc.dylib`nanov2_allocate_from_block:\r\n->  0x1a4226994 <+8>:  stp    x28, x27, [sp, #0x20]\r\n    0x1a4226998 <+12>: stp    x26, x25, [sp, #0x30]\r\n    0x1a422699c <+16>: stp    x24, x23, [sp, #0x40]\r\n    0x1a42269a0 <+20>: stp    x22, x21, [sp, #0x50]\r\nTarget 0: (python) stopped.\r\n(lldb) bt\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=2, address=0x16edfffc0)\r\n  * frame #0: 0x00000001a4226994 libsystem_malloc.dylib`nanov2_allocate_from_block + 8\r\n    frame #1: 0x00000001a42261e0 libsystem_malloc.dylib`nanov2_allocate + 128\r\n    frame #2: 0x00000001a42260fc libsystem_malloc.dylib`nanov2_malloc + 64\r\n    frame #3: 0x00000001a4243748 libsystem_malloc.dylib`_malloc_zone_malloc + 156\r\n    frame #4: 0x00000001000a0998 python`_PyObject_Malloc + 64\r\n    frame #5: 0x00000001001eedcc python`state_init + 164\r\n    frame #6: 0x00000001001ecf1c python`_sre_SRE_Pattern_match + 224\r\n    frame #7: 0x0000000100058a2c python`method_vectorcall_FASTCALL_KEYWORDS_METHOD + 144\r\n    frame #8: 0x000000010004e02c python`PyObject_VectorcallMethod + 144\r\n    frame #9: 0x0000000100104b30 python`check_matched + 108\r\n    frame #10: 0x00000001001035ac python`warn_explicit + 1284\r\n    frame #11: 0x000000010010487c python`do_warn + 828\r\n    frame #12: 0x0000000100102eac python`PyErr_WarnEx + 72\r\n    frame #13: 0x00000001036ada78 _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromTypeObject + 496\r\n    frame #14: 0x00000001036acbb8 _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 212\r\n    frame #15: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200\r\n    frame #16: 0x000000010009e7dc python`PyObject_GetAttr + 100\r\n    frame #17: 0x000000010009e720 python`PyObject_GetAttrString + 72\r\n    frame #18: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488\r\n    frame #19: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200\r\n    frame #20: 0x000000010009e7dc python`PyObject_GetAttr + 100\r\n    frame #21: 0x000000010009e720 python`PyObject_GetAttrString + 72\r\n    frame #22: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488\r\n    frame #23: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200\r\n    frame #24: 0x000000010009e7dc python`PyObject_GetAttr + 100\r\n    frame #25: 0x000000010009e720 python`PyObject_GetAttrString + 72\r\n(lots more of this, snipped)\r\n    frame #279490: 0x00000001036acccc _multiarray_umath.cpython-311-darwin.so`PyArray_DescrFromScalar + 488\r\n    frame #279491: 0x000000010009ef28 python`_PyObject_GenericGetAttrWithDict + 200\r\n    frame #279492: 0x000000010009e7dc python`PyObject_GetAttr + 100\r\n    frame #279493: 0x0000000100138d50 python`_PyEval_EvalFrameDefault + 17896\r\n    frame #279494: 0x00000001001346c8 python`_PyEval_Vector + 200\r\n    frame #279495: 0x000000010004e300 python`object_vacall + 256\r\n    frame #279496: 0x000000010004e564 python`PyObject_CallFunctionObjArgs + 48\r\n    frame #279497: 0x0000000103603278 _multiarray_umath.cpython-311-darwin.so`PyArray_NewFromDescr_int + 1268\r\n    frame #279498: 0x00000001035fa958 _multiarray_umath.cpython-311-darwin.so`PyArray_View + 84\r\n    frame #279499: 0x0000000103688110 _multiarray_umath.cpython-311-darwin.so`array_view + 324\r\n    frame #279500: 0x000000010005867c python`method_vectorcall_FASTCALL_KEYWORDS + 136\r\n    frame #279501: 0x000000010004ccd4 python`PyObject_Vectorcall + 80\r\n    frame #279502: 0x000000010013cf1c python`_PyEval_EvalFrameDefault + 34740\r\n    frame #279503: 0x00000001001346c8 python`_PyEval_Vector + 200\r\n    frame #279504: 0x000000010004c64c python`_PyObject_FastCallDictTstate + 272\r\n    frame #279505: 0x000000010004d394 python`_PyObject_Call_Prepend + 148\r\n    frame #279506: 0x00000001000b6b68 python`slot_tp_new + 88\r\n    frame #279507: 0x00000001000b3550 python`type_call + 84\r\n    frame #279508: 0x000000010004c834 python`_PyObject_MakeTpCall + 344\r\n    frame #279509: 0x000000010013cf1c python`_PyEval_EvalFrameDefault + 34740\r\n    frame #279510: 0x0000000100134598 python`PyEval_EvalCode + 272\r\n    frame #279511: 0x000000010018c5a8 python`PyRun_InteractiveOneObjectEx + 744\r\n    frame #279512: 0x000000010018bad0 python`_PyRun_InteractiveLoopObject + 152\r\n    frame #279513: 0x000000010018b97c python`_PyRun_AnyFileObject + 96\r\n    frame #279514: 0x000000010018c234 python`PyRun_AnyFileExFlags + 68\r\n    frame #279515: 0x00000001001ac1dc python`Py_RunMain + 2196\r\n    frame #279516: 0x00000001001ac424 python`pymain_main + 324\r\n    frame #279517: 0x00000001001ac4c4 python`Py_BytesMain + 40\r\n    frame #279518: 0x000000010057508c dyld`start + 520\n```\n\n\n### Runtime information:\n\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.24.3\r\n3.11.1 (main, Dec 21 2022, 16:19:04) [Clang 14.0.0 (clang-1400.0.29.202)]\r\n>>> print(numpy.show_runtime())\r\n[{'simd_extensions': {'baseline': ['NEON', 'NEON_FP16', 'NEON_VFPV4', 'ASIMD'],\r\n                      'found': ['ASIMDHP', 'ASIMDDP'],\r\n                      'not_found': ['ASIMDFHM']}},\r\n {'architecture': 'armv8',\r\n  'filepath': '\/Users\/jelle\/py\/venvs\/py311\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 8,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21'}]\r\nNone\r\n```\n\n### Context for the issue:\n\nNo concrete use case; I found this while trying to instantiate all pairs of buffer classes to look for real-world impact of python\/cpython#104297.","comments":["Thanks!  `numpy.flexible` is an abstract base class really (or maybe should be one).  It should not be valid to subclass it in Python at all.\r\n(I am a bit surprised nothing prevents this, since masked array inherits from ndarray and ndarray and flexible have incompatible layouts.)","The memoryview() part in my original report was unnecessary; just instantiating the class is enough:\r\n\r\n```\r\n>>> class X(numpy.flexible, numpy.ma.core.MaskedArray): pass\r\n... \r\n>>> X()\r\nzsh: segmentation fault  python\r\n```\r\n\r\n","@seberg could you expand on \"`numpy.flexible` is an abstract base class\" and \"It should not be valid to subclass it in Python\"? I think abstract base classes are designed to be subclasses, no? Or do you mean `numpy.flexible` is an internal NumPy class, and should not be exposed to users. In any case, removing the ability to create a subclass of `numpy.flexible` should break this line, and allowing it seems like a bug in CPython. https:\/\/github.com\/numpy\/numpy\/blob\/250e1479ce342d9d7ab8a592508f6ce892d4c98b\/numpy\/core\/src\/multiarray\/multiarraymodule.c#L464\r\n\r\nReopening for discussion.","I think CPython is fine with allowing it in C presuming you know what you do, but not sure it is intentional or not.\r\n\r\nYes, I suppose you are right.  Subclassing it makes in theory sense (honestly, I don't think it does in practice, you cannot have a flexible user dtype in the old machinery anyway).\r\n\r\nSo yes, it makes a bit more sense to allow it, but move the actual buffer stuff to the concrete subclasses, becasuse there is no reason to have anything related in the superclass.  I am not even sure if it is used there, so maybe it can just be deleted from there."],"labels":["00 - Bug"]},{"title":"BUG: Cannot install numpy on UOS system.","body":"### Describe the issue:\n\nI am trying to install numpy on python3.11.\r\nI am using the UOS system.\r\nBut I get an error and cannot install it.\r\n\r\n#error Unknown CPU, please report this to numpy maintainers with \\\r\n            ^~~~~\r\n      error: Command \"gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/npymath -Inumpy\/core\/include -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/include\/numpy -Ibuild\/src.linux-sw_64-3.11\/numpy\/distutils\/include -Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/home\/uos\/OCR\/venv\/include -I\/usr\/local\/include\/python3.11 -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/common -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/npymath -c numpy\/core\/src\/npymath\/npy_math.c -o build\/temp.linux-sw_64-3.11\/numpy\/core\/src\/npymath\/npy_math.o -MMD -MF build\/temp.linux-sw_64-3.11\/numpy\/core\/src\/npymath\/npy_math.o.d\" failed with exit status 1\r\n\r\nPlease let me know what is the solution.\n\n### Reproduce the code example:\n\n```python\nI cannot install\n```\n\n\n### Error message:\n\n```shell\n#error Unknown CPU, please report this to numpy maintainers with \\\r\n            ^~~~~\r\n      error: Command \"gcc -pthread -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/npymath -Inumpy\/core\/include -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/include\/numpy -Ibuild\/src.linux-sw_64-3.11\/numpy\/distutils\/include -Inumpy\/core\/src\/common -Inumpy\/core\/src -Inumpy\/core -Inumpy\/core\/src\/npymath -Inumpy\/core\/src\/multiarray -Inumpy\/core\/src\/umath -Inumpy\/core\/src\/npysort -Inumpy\/core\/src\/_simd -I\/home\/uos\/OCR\/venv\/include -I\/usr\/local\/include\/python3.11 -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/common -Ibuild\/src.linux-sw_64-3.11\/numpy\/core\/src\/npymath -c numpy\/core\/src\/npymath\/npy_math.c -o build\/temp.linux-sw_64-3.11\/numpy\/core\/src\/npymath\/npy_math.o -MMD -MF build\/temp.linux-sw_64-3.11\/numpy\/core\/src\/npymath\/npy_math.o.d\" failed with exit status 1\n```\n\n\n### Runtime information:\n\nI cannot install\n\n### Context for the issue:\n\nI cannot install","comments":["What is UOS? What architecture and compilers does it support? \r\nWe have undocumented tier support for operating systems: glibc on x86_64 and aarch64, musl x86_64 and arm64, windows, and macOS are explicitly supported. AIX, ppc64 and s390x and others are not officially supported but we will accept patches as long as they seem reasonable. Perhaps you could contribute UOS support if you wish, on a \"untested, use at your own risk\" basis","Maybe UOS is https:\/\/en.wikipedia.org\/wiki\/Unity_Operating_System ?","UOS is linux system.","Hello brother, I have encountered the same problem as you. I have also encountered an unknown CPU issue when installing numpy1.19.5 source code on a server with SW_64 architecture and operating system Kylin. Python version 3.8.12. Have you resolved this issue?\r\n\u5144\u5f1f\u4f60\u597d\uff0c\u6211\u9047\u5230\u4e86\u548c\u4f60\u4e00\u6837\u7684\u95ee\u9898\uff0c\u6211\u5728sw_64\u67b6\u6784\u7684\u670d\u52a1\u5668\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3akylin\u4e0a\u5b89\u88c5numpy1.19.5\u7684\u6e90\u7801\u4e5f\u9047\u5230\u4e86unknown CPU\u7684\u95ee\u9898\uff0cpython\u7248\u672c\u76843.8.12\uff0c\u8bf7\u95ee\u4f60\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4e86\u5417\uff1f","> What is UOS? What architecture and compilers does it support? We have undocumented tier support for operating systems: glibc on x86_64 and aarch64, musl x86_64 and arm64, windows, and macOS are explicitly supported. AIX, ppc64 and s390x and others are not officially supported but we will accept patches as long as they seem reasonable. Perhaps you could contribute UOS support if you wish, on a \"untested, use at your own risk\" basis\r\n\r\nI have encountered the same problem\uff0cMy server is a domestically produced server in China, with an architecture of SW_64 and an operating system of KylinV10. Will numpy provide support for SW_64 architecture servers in the future?","As stated above, someone who has this platform will have to do the work. It will not magically appear. Some clues how to do this are in the file where the error is triggered. "],"labels":["00 - Bug"]},{"title":"ENH: `dtype` argument in `random.Generator.normal`","body":"### Proposed new feature or change:\n\nIt would be great to support a `dtype` argument in [`random.Generator.normal`](https:\/\/numpy.org\/doc\/stable\/reference\/random\/generated\/numpy.random.Generator.normal.html#numpy.random.Generator.normal).\r\n\r\n`dtype` is already supported the similar function `random.Generator.standard_normal` for `float32` and `float64` as of `numpy==1.24.3`.  Perhaps this behavior can be easily ported over.","comments":["Seems like a logical path to me for enhancement.\r\nWorking on it\ud83d\udc4d","The omission is deliberate, but could be reconsidered. We don't want to add `dtype=` flexibility to all of the methods, so we restricted it to just those that are fundamental building blocks (`integers()`, `standard_*()`) where we could make use of the reduced precision to use different algorithms. I'm not particularly opposed to extending `dtype=` flexibility to the methods associated with the `standard_*()` methods, but not particularly enthusiastic about it either.\r\n\r\n@DuanBoomer This involves a policy choice rather than just work, so it might not be a good place for a first contribution.","@rkern Thank you for the clarification\nI didn't knew it was a deliberate omission. I just saw the issue and tried to fix it.\nCan you still take some time to just see if the changes that I made would work?\n\nI know it won't be accepted but still would love to hear your feedback. \nPlease take a look at it, Thank you in advance.","@ngoldbaum's comment on your PR is correct. That modification does not address the feature request.","Thank you so much for the feedback @ngoldbaum and @rkern , I will try to improve the cpython implementation on my fork just to learn something out of it.\nOne last request can you please guide me how to check the \"numpy.Generator.normal\" functions parameters if i update them.\n\nExample: If a real user goes and types numpy.Generator.normal he will usually see the parameters to be included in it when using a editor like VS Code. \nHow can I see the same thing?\n","`normal` makes use of some support functions that handle broadcasting and reshaping of inputs and `size` arguments.  I believe these are only available for C `double`, and so implementing an  effective C `float` path (one that would be 32-bit throughout, rather than 64 bit and then casting down) would require appropriate versions of these. "],"labels":["01 - Enhancement","component: numpy.random"]},{"title":"BUG: Typing of `np.ndarray.__bool__()` is incorrect","body":"### Describe the issue:\n\nSee code below.\r\n\r\nThe revealed type is `bool`.  But the code will raise an exception.  The revealed type should be `NoReturn` .  `bool(x)` raises as well.\r\n\r\nNote - there is discussion at https:\/\/github.com\/microsoft\/pyright\/issues\/5039 about having `pyright` recognize that the `if` statement is invalid, and I also started a discussion in the typing community here:  https:\/\/github.com\/python\/typing\/discussions\/1398\r\n\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nx = np.array([1,2,3])\r\nreveal_type(x.__bool__())\r\nif x:\r\n    print(\"hey\")\r\nelse:\r\n    print(\"no\")\n```\n\n\n### Error message:\n\n```shell\nFrom pyright:\r\n\r\nType of \"x.__bool__()\" is \"bool\"\n```\n\n\n### Runtime information:\n\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.24.3\r\n3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:28:38) [MSC v.1929 64 bit (AMD64)]\r\n>>> print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\n\n### Context for the issue:\n\nI'm trying to make the typing between pandas and numpy consistent.\r\n","comments":["Exceptions are not intended to be type hinted.\r\n\r\n[PEP484](https:\/\/peps.python.org\/pep-0484\/#exceptions) states:\r\n\r\n> No syntax for listing explicitly raised exceptions is proposed. Currently the only known use case for this feature is documentational, in which case the recommendation is to put this information in a docstring.\r\n\r\n`NoReturn` is rather explicitly only intended for methods that _never_ return normally, not just those that _may_ raise exceptions.\r\n\r\nThe linked discussions are about allowing objects to opt-out of `if x:` checks and implicit `bool` calls (and having type checkers be able to recognize that).\r\n\r\nThis is _not_ the case for the numpy code, even though it _can_ raise, there is a code path (where both conditions fail) where it will actually return a boolean. The errors raised are intended to be informative when someone has an ambiguous conditional.","> This is _not_ the case for the numpy code, even though it _can_ raise, there is a code path (where both conditions fail) where it will actually return a boolean. The errors raised are intended to be informative when someone has an ambiguous conditional.\r\n\r\nI guess the question is whether you could create an overload for the cases where it would return a boolean versus where it would raise with the ambiguous error message.  \r\n\r\nI'm guessing that if the array is of length 1, it's a valid operation, which means you can't detect it in a static typing context.\r\n","Well specifically it is for cases where the input has _ndim_ of 0 and `dtype` of `bool`, both of which you cannot detect via static typing. (Not just length 1, only scalars)\r\n\r\nPandas will always raise because you cannot have an ndim of 0 (I think, not super well versed in pandas land, but in my quick testing).\r\n\r\nA numpy scalar (ndim of 0) gets promoted to ndim of 1 for a pandas series:\r\n\r\n```python\r\n>>> np.array(9).ndim\r\n0\r\n>>> pd.Series(np.array(9)).ndim\r\n1\r\n```\r\n\r\nDataFrames are specifically required to be ndim of 2 at instantiation\r\n\r\nPandas also introduces the NA scalar, which has ambiguous truth and raises always.\r\n\r\nSo yes, you can probably type hint pandas things as NoReturn, but that is not actually inconsistent with numpy and numpy's return annotation (while it only actually works for a narrow set of inputs) is correct.","Improving this is blocked by someone working on full support for shape\/dimension typing.  Only with that information could you guess the correct `NoReturn`.\r\n\r\nSo I think, it is just a part of gh-16544.  Although it is so old that it might make sense to create a new issue pointing to what the typing world has invented since then.","At moment tensorflow stubs I put NoReturn [here](https:\/\/github.com\/python\/typeshed\/blob\/f9551c4fea9912b1657c43f4b5f29771bc2eb08a\/stubs\/tensorflow\/tensorflow\/__init__.pyi#L115). Tensorflow has similar behavior for typical usage. Scalar bools are allowed to be used with `__bool__` otherwise it is exception. It's also exception in some extra cases too (v1 mode) but those cases are becoming less common. I am undecided on it. Similar to numpy there is no shape types yet in stubs for tensorflow and looks like a long hard path. Although at same time mypy doesn't have this special NoReturn behavior and pyright doesn't bundle tensorflow stubs so I guess not many people would have been affected yet by it.\r\n\r\nI'll probably end up following numpy's decision. I think it is both a common mistake to use >1d tensor in a condition, but it'd also be common false positive.\r\n\r\nedit: One notable thing about this new rule testing it on some code is that while,\r\n\r\n```python\r\nif x:\r\n  ...\r\n```\r\n\r\nis a type error when x's `__bool__` type is NoReturn, \r\n\r\n```python\r\nif bool(x):\r\n  ...\r\n```\r\n\r\nis fine. So it's possible to explicitly mark that you think bool is safe and pyright won't flag it. A bit unsure if that's intentional, but it is nice way to indicate you think this usage is fine. For codebase I work on I got 11 hits with new rule with 2 being bugs and 9 being false positives (guaranteed scalars). With shape\/rank types I think 9 false positives could mostly be understood by type system. That's for about 30k lines of fairly well tested code. Back when our tests were lower\/more scripty setting I could guess the error count for shape issues like this being higher.","A problem here is that mypy for example has yet to implement PEP 646 support (xref https:\/\/github.com\/python\/mypy\/issues\/12280), which is the absolute minimum required for implementing anything related to shape typing."],"labels":["00 - Bug","57 - Close?","Static typing"]},{"title":"BUG: vectorize is unannotated which leads to doc strings being useless","body":"### Describe the issue:\r\n\r\nVScode can't correctly hint the wrapped function's docstring, hinting instead about vectorize:\r\n\r\n![Peek 2023-04-28 09-33](https:\/\/user-images.githubusercontent.com\/78506690\/235112237-469fa13c-9fec-4b02-9be2-1b3c1ab38c26.gif)\r\n\r\nbut the `__doc__` is passed on correctly.\r\n\r\n[In this issue](https:\/\/github.com\/microsoft\/pylance-release\/issues\/4292) it was suggested this is becuase of annotations.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef foo(x):\r\n    \"\"\"\r\n    I am a foo function\r\n    don't put zero into me\r\n    \"\"\"\r\n    return 1\/x\r\n\r\n\r\n\r\n@np.vectorize\r\ndef vectorizedFoo(x):\r\n    \"\"\"\r\n    I am a vectorized foo function\r\n    don't put zero into me\r\n    \"\"\"\r\n    return 1\/x\r\n\r\nprint(vectorizedFoo.__doc__)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\nnumpy version: 1.23.5\r\nsys version : 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]\r\n\r\n### Context for the issue:\r\n\r\nWithout good hinting np vectorize is almost useless for complex functions, since the user always has to peek the definition to get the docstring","comments":["The code example below uses annotations to the wrapped functions using a decorator, which VSCode can use to correctly hint the docstring. Hope this helps\r\n\r\n```py\r\nimport numpy as np\r\n\r\ndef annotate(func):\r\n    def wrapper(*args, **kwargs):\r\n        return func(*args, **kwargs)\r\n    wrapper.__annotations__ = func.__annotations__\r\n    return wrapper\r\n\r\n@annotate\r\ndef foo(x: float) -> float:\r\n    \"\"\"\r\n    I am a foo function\r\n    don't put zero into me\r\n    \"\"\"\r\n    return 1\/x\r\n\r\n@np.vectorize\r\n@annotate\r\ndef vectorizedFoo(x: float) -> float:\r\n    \"\"\"\r\n    I am a vectorized foo function\r\n    don't put zero into me\r\n    \"\"\"\r\n    return 1\/x\r\n\r\nprint(vectorizedFoo.__doc__)\r\n\r\n```","It doesn't. \r\n\r\n![obrazek](https:\/\/user-images.githubusercontent.com\/78506690\/235147882-abd98193-30be-4399-ba5d-3a35bf8f5a86.png)\r\n","@Quacken8  I'm going to take a look at `np.vectorize` to see if it could be annotated to indicate the correct docstring of the wrapped function.","@mac-lawson do you still plan on working on this? If not, I can take care of it.","It's not entirely clear to me to what extent this is a python typing, runtime or VScode issue? If it's the former then I don't believe there's anything we can improve upon without the introduction of false positives. Namely, statically we can't provide any guarantees about the return type without resorting to unions (_i.e._ is it an array, scalar, a tuple with either of them?) and the parameters in the `vectorize.__call__` signature are heavily influences by the literal value of vectorizes `signature` argument, meaning the that we can't express it via `typing.Paramspec` + `typing.Concatenate`."],"labels":["00 - Bug"]},{"title":"BUG: Shouldn't numpy's floating types (e.g. `np.float_`) and python's native `float` type be compatible?","body":"### Describe the issue:\r\n\r\nWhen trying to type hint functions accepting scalars, I get unexpected error messages from my type checker (Pylance). As `np.float_` is a subtype of python's native `float` type, I would expect to be able to pass a variable with type `np.float_` when a native `float` is expected.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef idx(x: float) -> float:\r\n    return x\r\n\r\narr = np.arange(5.0, dtype=np.float_)\r\nsum = np.sum(arr)\r\nidx(sum)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\nArgument of type \"float_\" cannot be assigned to parameter \"x\" of type \"float\" in function \"idx\"\r\n  \"float_\" is incompatible with \"float\" Pylance (reportGeneralTypeIssues)\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n1.24.1\r\n3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:27:40) [GCC 11.3.0]\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}}]\r\nNone\r\n\r\n### Context for the issue:\r\n\r\nThis issue makes it either impossible or much harder to correctly type hint functions such that they are compatible with numpy.","comments":[">As np.float_ is a subtype of python's native float type\r\n\r\n~~That's what the documentation says, but it seems that this isn't actually the case.~~\r\n\r\n```\r\n>>>np.float_\r\n<class 'numpy.float64'>\r\n```\r\n\r\nSince this isn't actually the Python float type (only equivalent to it), this is probably why PyLance thinks that the two are incompatible.","`np.float64` (which `np.float_` is just an alias for) is a subclass of `float`.\r\n\r\n```\r\n>>> np.float64.mro()\r\n[numpy.float64,\r\n numpy.floating,\r\n numpy.inexact,\r\n numpy.number,\r\n numpy.generic,\r\n float,\r\n object]\r\n```","FWIW, I'd probably just not use `np.float_` there. Idiomatically, I'd just use `dtype=float` or `dtype=np.float64`. `np.float_` and `np.int_` are really there more for reference than for use in `dtype=` arguments.","Even if I change `np.float_` to `np.float64`, I still get an equivalent error message. I have submitted a corresponding bug report to [microsoft\/pylance-release](https:\/\/github.com\/microsoft\/pylance-release) (see microsoft\/pylance-release#4332).","I think Pylance is correct here. The type float64 (at least in the type stubs) is defined like this:\r\n\r\n```python\r\nfloat64 = floating[_64Bit]\r\n```\r\n\r\nwhere `floating` is a class that isn't derived from float:\r\n\r\n```python\r\nclass floating(inexact[_NBit1]):\r\n    ...\r\n```\r\n\r\nThe MRO list at runtime isn't used by Pylance\/Pyright. It's determined solely by the type heirarchy we can query statically. \r\n\r\nIf I change the definition of floating in the type stub to be like so:\r\n\r\n```python\r\nclass floating(inexact[_NBit1], float):\r\n   ...\r\n```\r\n\r\nThen the error reported by Pylance goes away. ","Well, `floating` doesn't subclass from `float`, only `float64` is.","Hmm then similarly you could define float64 as,\r\n\r\n```python\r\nclass float64(floating[_64Bit], float):\r\n ...\r\n```","> Hmm then similarly you could define float64 as,\r\n\r\nYou could, though the reason why the likes of `np.floating` subclasses (among a few others) are modeled as parametrized generics in the stub files is because, to put it mildly, it's incredibly difficult to determine the output bitsize for a given operation with the currently available typing tools and as such we generally have to resort to simply returning `np.floating[Any]` (arbitrary\/unknown-sized float). Now, due to the presence of the `Any`-parameter and the fact that the likes of `np.float64` are treated as sub-types but not proper subclasses, it means that you can freely cast from one to the other (while you cannot do the same for superclass -> subclass without some heavy handed use of `typing.cast` lest typecheckers can and will complain).\r\n\r\nThe downside of this approach is that information about the `float` superclass is lost from `np.float64`, simply because the various `np.floating` sub-types are all treated as exactly equivalent (barring their bit size). It might be an option instead to pretend that `np.floating` as a whole inherits from `float` (and the similarly with `np.complexfloating` and `complex`). This would technically be a lie, though I suspect it's a minor one and one that might be worth the effort (especially with the autrocious state of statically duck-typing scalars).\r\n\r\n"],"labels":["00 - Bug","Static typing"]},{"title":"DOC: clarification about the interaction of numpy \/ f2py with Fortran procedures","body":"### Issue with current documentation:\r\n\r\nPrior to filing a PR about the presentation of f2py [here](https:\/\/numpy.org\/doc\/stable\/f2py\/f2py.getting-started.html#the-quick-way), I would like to reach out for the maintainers of numpy for some discussion.\r\n\r\nLimiting to the first example \"the quick way\", `fib1.f` currently presented is written in FORTRAN77 equally known as fixed style.  Modern Fortran (everything since and including Fortran 90, with a last revision 2018, and standard 2023 in preparation) follows set of modernized rules, the statement `implicit none` might be one known equally by those outside Fortran.  I rewrote the procedure as in snippet `fib1a.f90` which `f2py` by\r\n\r\n``` shell\r\npython -m numpy.f2py -c fib1a.f90 -m fib\r\n```\r\n\r\naccepts to wrap into an extension module.  Python can use it to replicate replicate the computation `a = np.zeros(8, 'd')` to yield `[0. 1. 1. 2. 3. 5. 8. 13.]`.\r\n\r\nObservation 1:  Line 6 defines double precision as suitable for the gfortran compiler which is applied on the elements of array `a(n)`.  With the advent of Fortran 2003, `iso_fortran_env` ([ref](https:\/\/fortranwiki.org\/fortran\/show\/iso_fortran_env)) offers a compiler independent definition of precision which can be used instead.  While fine for gfortran, and apparently fine for `f2py`, the subsequent use of the .so extension module by Python did not follow through with either version `fib2a.f90` (which loads from `iso_fortran_env` only what is relevant to for \"double precision\" of a floating number), or `fib2b.f90` (which does not impose such a constrain what other procedure definitions this Fortran intrinsic module might contain).  For either one of the two, their use in the minimal Python example terminates with the report\r\n\r\n```\r\n[0. 0. 0. 0. 0. 0. 0. 0.]\r\ncorrupted size vs. prev_size\r\nAborted\r\n```\r\n\r\nQuestion: Does the use of `iso_fortran_env` in the Fortran procedure require a different syntax in the Python script to use the wrapper module?  Does `d` in `a = np.zeros(8, 'd')` relate to double precision of reals (Fortran) \/ floating numbers (numpy)?\r\n\r\n-----\r\n\r\nObservation 2:  Especially Fortran subroutines are safer in use with an explicit definition if processed data shall\r\n+ only enter a procedure without being altered by this procedure (`intent(in)`)\r\n+ shall return to the main program, possibly altered (`intent(out)`), or\r\n+ if the exchange between main program and routine shall be freely bidirectional, including the possibility that the procedure alters these values (`intent(in out)`, or functional equivalent, `intent(inout)`).  Hence I derived `fib3a.f90` from `fib1a.f90` which compiles well and yields the anticipated result `[ 0.  1.  1.  2.  3.  5.  8. 13.]`.  However the use of these keywords and the double precision provided by Fortran's intrinsic `iso_fortran_env` module now yields the result `[0. 0. 0. 0. 0. 0. 0. 0.]`\r\n\r\nQuestion: Here, I do not know how to interpret the result for the second case.  Do you recommend a different input syntax on level of numpy?\r\n\r\n-----\r\n\r\nThe observations refer to an instance of Linux Debian 12\/bookworm with Python (3.11.2) with numpy (1.24.2), gcc and gfortran (12.2.0-14) as provided by this Linux distributions.  The Fortran modules were considered as \"good enough\" because their compilation to yield object files in a pattern of\r\n\r\n``` shell\r\ngfortran .\/fib3a.f90 -c -std=f2018 -Wall -Wextra -Wpedantic\r\n```\r\n\r\ndid not yield an error.  The two warnings e.g.\r\n\r\n```\r\n$ gfortran .\/fib3a.f90 -c -std=f2018 -Wall -Wextra -Wpedantic\r\n.\/fib3a.f90:16:15:\r\n\r\n   10 |    do i = 1, n\r\n      |              2 \r\n......\r\n   16 |       a(i) = a(i - 1) + a(i - 2)\r\n      |               1\r\nWarning: Array reference at (1) out of bounds (0 < 1) in loop beginning at (2) [-Wdo-subscript]\r\n.\/fib3a.f90:16:26:\r\n\r\n   10 |    do i = 1, n\r\n      |              2            \r\n......\r\n   16 |       a(i) = a(i - 1) + a(i - 2)\r\n      |                          1\r\nWarning: Array reference at (1) out of bounds (-1 < 1) in loop beginning at (2) [-Wdo-subscript]\r\n```\r\n\r\nwere noted, though not considered as relevant for the discussion.  All relevant source code is in the zip archive below.\r\n\r\n[2023-04-21_f2py_discussion.zip](https:\/\/github.com\/numpy\/numpy\/files\/11299204\/2023-04-21_f2py_discussion.zip)\r\n\r\n\r\n\r\n### Idea or request for content:\r\n\r\n_No response_","comments":["Hi, sorry for getting to this belatedly. At the moment the intrinsic `iso_fortran_env` is not really supported, I recall adding some basic tests for compilation, will look into this.","Ah, right so we currently 'support' the `iso_fortran_env` in the context of the type dictionary users can supply, via `.f2py_f2cmap`. To fix the compilation of `fib2a.f90`:\r\n\r\n```fortran\r\n! file: fib2a.f90\r\nsubroutine fib(a, n)\r\n   ! calculate first Fibonacci numbers\r\n   use iso_fortran_env, only: dp => real64\r\n   implicit none\r\n   integer :: i, n\r\n   real(kind=dp) :: a(n)\r\n\r\n   do i = 1, n\r\n   if (i .eq. 1) then\r\n      a(i) = 0.0_dp\r\n   elseif (i .eq. 2) then\r\n      a(i) = 1.0_dp\r\n   else\r\n      a(i) = a(i - 1) + a(i - 2)\r\n   end if\r\n   end do\r\nend subroutine fib\r\n! end file fib2a.f90\r\n```\r\n\r\nWe can write a type specification file `.f2py_f2cmap` given as:\r\n\r\n```\r\ndict(real=dict(dp='double'))\r\n```\r\n\r\nNote that this is kind of a hack, but it does work:\r\n\r\n```bash\r\nf2py -c fib2a.f90 -m fib2 --f2cmap .f2py_f2cmap\r\npython -c \"import numpy as np; import fib2; a = np.zeros(8, 'd'); fib2.fib(a); print(a)\"\r\n[ 0.  1.  1.  2.  3.  5.  8. 13.]\r\n```\r\nOther type specifications can be written out similarly\r\nI guess this is then something to be clarified in the documentation.\r\n\r\nLet me know if there are more questions about this, @nbehrnd .","@HaoZeke Thank you for getting back in touch.  I equally had to revisit the suggestion.  Not knowing well enough the inner working of numpy your description of \"kind of a hack, but it does work\" is understood as you \/ as other maintainers of numpy would prefer to have some additional time to adjust how numpy internally works and the approach is a preliminary one.\r\n\r\nThis is why I returned to the other approach seen to explicitly define the numerical precision in Fortran independent to `iso_fortran_env`, i.e. in lines of \r\n\r\n```fortran\r\ninteger, parameter :: dp = selected_real_kind(15, 50)\r\n```\r\n\r\nas in `fib1a.f90` perhaps easier for numpy to interact with.  With the intent to substitute the snippets of old FORTRAN77 by more contemporary Fortran90+, I imagine already this could be more welcoming to the users of numpy.  (It is influenced by Daniel Price' [Fortran lectures](https:\/\/www.youtube.com\/watch?v=d_ZNWPNzspg) with his recommendation \"if there is a choice, prefer the modern Fortran\".)\r\n\r\nBoth the original snippet of FORTRAN as well as the slightly modernized one however seem to interact with current numpy differently, than in the showcase of getting started\/the quick way.  The instructions in the info box successfully create a an array of eight zeros, to fill only the first six fields however fail.  Because `n` no longer defaults to `len(a)` (as reported by `print(fib1.fib.__doc__)`), but to `shape(a, 0)` I speculate, could it be `shape(a, 1)` were a better (default) choice for a 1D array? \r\n\r\n[2023-05-22_f2py_discussion.zip](https:\/\/github.com\/numpy\/numpy\/files\/11533701\/2023-05-22_f2py_discussion.zip)\r\n"],"labels":["04 - Documentation","component: numpy.f2py"]},{"title":"ENH: add no_whitespace to numpy.fromstring","body":"In reference to #23350, added an optional parameter that allows the user to state whether they want whitespace to not be returned as -1. Will add documentation in a second\r\n","comments":["Does this also solve #18435?\r\nCould you add the samples from the issues as examples to the documentation, and show how using `whitespace` changes the results?\r\n","The [build is failing](https:\/\/github.com\/numpy\/numpy\/actions\/runs\/5163952840\/jobs\/9302559279?pr=23613#step:4:3049):\r\n```\r\nnumpy\/core\/src\/multiarray\/multiarraymodule.c:2378:43: warning: \\\r\n    passing argument 1 of \u2018PyUnicode_AsUTF8\u2019 makes pointer from integer \r\n    without a cast [-Wint-conversion]\r\n```","@ngoldbaum the intent is that every whitespace charachter is replaced with the value the user provides. I don't know how `\\n` is handled down to the metal to provide a good answer.\r\nIm configuring the fromstring method now, but since its called elsewhere and i've changed it's behavior, expectedly, stuff breaks, hence i haven't been able to check functionallity. I will go on to patch the usage of the method whenever needed but it's a timely process and in the meantime i had expected some feedback concerning the direction of my changes. Here's hoping i get that now","I tried to give some feedback in [a comment](https:\/\/github.com\/numpy\/numpy\/pull\/23613#issuecomment-1575135414) above. We need to weigh the combinatory explosion of various kwargs (bad) against the ability to parse more files (good). How will this interface with all other options? Are there clear examples in the documentation of the use of this new option? And as a basic condition for spending review time on the particular PR: are all tests passing?"],"labels":["01 - Enhancement"]},{"title":"BUG: matmul (@ overload) and dot significant performance differences for non-contiguous arrays (noblas) ","body":"### Describe the issue:\r\n\r\nThe implementation of `matmul` does some basic checks to verify if the array can be passed to `gemm` and if it deems that this is not posssible it will fallback on a `noblas` routine which has no regard for memory cache resulting in ~100 slowdowns. Here is the offending bit of [source code](https:\/\/github.com\/numpy\/numpy\/blob\/6f55bbf049db3fd50994a97ec66665f5685ec5be\/numpy\/core\/src\/umath\/matmul.c.src#L217).\r\n\r\n`dot` on the other hand is a lot more flexible, attempting to make a copy before passing the array to the blas routine. For small arrays the difference is not significant but for large arrays this results in much better performance. This behavior is explicitly seen in the [source code](https:\/\/github.com\/numpy\/numpy\/blob\/669cd13c692cfe8476e24dad3d42bbbd94547727\/numpy\/core\/src\/common\/cblasfuncs.c#L221) (line 234 and 243 are the bad-stride copies) and can be confirmed by profiling as shown by [hpaulj](https:\/\/stackoverflow.com\/a\/76010625\/17638323).\r\n\r\nNeither of the docs pages for `dot` or `matmul` reference this behavior. On the contrary the `dot` page states: _If both a and b are 2-D arrays, it is matrix multiplication, but using [matmul](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.matmul.html#numpy.matmul) or a @ b is preferred._\r\n\r\nThere are a handful of similar issues open:\r\n - #23123 \r\n - [complextype](https:\/\/stackoverflow.com\/q\/64914877\/17638323)\r\n - [views](https:\/\/stackoverflow.com\/q\/76009612\/17638323)\r\n - #23260 \r\n\r\nI am not sure what the best fix is. Updating the docs would be a good start. Perhaps `matmul` should try to copy too? at least for large arrays...\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\nimport numpy as np\r\nfrom timeit import timeit\r\nN = 2600\r\nxx = np.random.randn(N, N) \r\nyy = np.random.randn(N, N) \r\n\r\nx = xx[::2, ::2]\r\ny = yy[::2, ::2]\r\nassert np.shares_memory(x, xx)\r\nassert np.shares_memory(y, yy)\r\n\r\ndot = timeit('np.dot(x,y)', number = 10, globals = globals())\r\nmatmul = timeit('np.matmul(x,y)', number = 10, globals = globals())\r\n\r\nprint('time for np.matmul: ', matmul)\r\nprint('time for np.dot: ', dot)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n```shell\r\ntime for np.matmul:  29.04214559996035\r\ntime for np.dot:  0.2680714999441989\r\n```\r\n\r\n\r\n### Runtime information:\r\n\r\n```\r\n> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.24.2\r\n3.11.2 (tags\/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\r\n> print(numpy.show_runtime())\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2'],\r\n                      'not_found': ['AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'Prescott',\r\n  'filepath': 'C:\\\\Users\\\\______\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\numpy\\\\.libs\\\\libopenblas64__v0.3.21-gcc_10_3_0.dll',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 24,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21'}]\r\nNone\r\n```\r\n\r\n### Context for the issue:\r\n\r\nThe scientific computing community regularly uses the `@` shorthand on large arrays. Developers expect that with Numpy they do not need to think about CPU cache and expect operations to run close to CPU limits rather than memory bandwidth limits.\r\n\r\nAt the very least a warning should be provided to users such that they know why their code is running orders of magnitude slower than expected.","comments":["Thanks for the details report @merny93! \r\n\r\n> I am not sure what the best fix is. Updating the docs would be a good start. Perhaps `matmul` should try to copy too? at least for large arrays...\r\n\r\nThe difference is so large that I'd say that the `matmul` implementation should be changed to match `dot` here. Documenting this is probably not as helpful; only useful if we figure out that we can't fix this in a reasonable timescale and ask the user to do the copy themselves to avoid the performance penalty.","This is a large problem for me as well - not from caching concerns, but from threading. I work with large matrices on multi-core machines, and if sometimes `np.matmul` properly uses all my cores (having delegated to MKL) and sometimes it only uses one core - the noblas fallback is apparently single-threaded - this has seriously bad implications for the performance of my code. I can personally just default to always using `np.dot` instead of `np.matmul` but I'm seeing `@` operator more and more in the code of my dependencies, where I don't easily have the ability to defend against this fallback behavior from `np.matmul`.\r\n\r\nEDIT: By \"bad implications for the performance of my code\" I mean I've observed exactly this happening while trying to run research code, which is what brought me to the bugs forum to search for why `matmul` seems to be single-threaded even though I have a good multi-core blas installed.","I was taking a look at this, hoping to fix it. Based on the call graph we hit this piece of code that has a comment: https:\/\/github.com\/numpy\/numpy\/blob\/6f55bbf049db3fd50994a97ec66665f5685ec5be\/numpy\/core\/src\/umath\/matmul.c.src#L492-L497\r\n\r\n@mattip (original commit via gh-12365), if we make it ccontiguous before checking `is_blasable2d`, fix the issue? That being said, I could not find how `dot` is making it ccontiguous. Does `@array_function_from_c_func_and_dispatcher` make a copy before the dispatch?\r\n","> I was taking a look at this, hoping to fix it. Based on the call graph we hit this piece of code that has a comment:\r\n> \r\n> https:\/\/github.com\/numpy\/numpy\/blob\/6f55bbf049db3fd50994a97ec66665f5685ec5be\/numpy\/core\/src\/umath\/matmul.c.src#L492-L497\r\n> \r\n> @mattip (original commit via [gh-12365](https:\/\/github.com\/numpy\/numpy\/issues\/12365)), if we make it ccontiguous before checking `is_blasable2d`, fix the issue? That being said, I could not find how `dot` is making it ccontiguous. Does `@array_function_from_c_func_and_dispatcher` make a copy before the dispatch?\r\n\r\nGood research so far! `dot` is making it C continuous by calling `PyArray_NewCopy`:\r\nhttps:\/\/github.com\/numpy\/numpy\/blob\/6f55bbf049db3fd50994a97ec66665f5685ec5be\/numpy\/core\/src\/common\/cblasfuncs.c#L233-L241\r\n\r\nI think the most straightforward approach could be to just try to make a temporary copy in `matmul.c.src`, around line 497 and only if that copy fails (due to not enough memory), resort to the `matmul_inner_noblas` call. However, due to the structure of `ufunc` (and as far as I understand it), the references to the original PyArray objects are gone, which, in the implementation of e.g. `cblas_matrixproduct` would still be available. So one would either need to somehow\r\n1. construct a new `PyArray` in `matmul.c.src`, around line 497 (e.g. with `PyArray_NewFromDescr`) or\r\n2. resort to a more low-level approach like some manual `malloc` and make strides C continuous with loops or\r\n3. move the `is_blasable2d` check up to a function call where the array pointers are still available or\r\n4. pass the array pointers down somehow to do it similarly to `cblas_matrixproduct`\r\n\r\nDid I get that right so far?","You can just return an error if it fails due to not enough memory.  But you do need to allocate memory in the inner loop there (for each call of the inner-loop).\r\n\r\n2. is the only workable thing of those.  We should have some copy functions that could be re-used in the iterator code (would have to search a bit also).\r\n\r\nThe only alternative might be to check whether we can force the contiguity in the iterator.  But that would probably need some awkward iterator flags, so I am not sure how worthwhile it is.","The last line in the comment of gh-12365 agrees with @seberg that the inner loop should be doing the copying \r\n> We could adopt the strategy from linalg where the inner loops copy data if needed, rather than at the entrance\r\n\r\nI think this would be a non-trivial undertaking. "],"labels":["00 - Bug","Project"]},{"title":"DOC: There is a typo in the tutorial documentation of \"np.tensordot()\"","body":"### Issue with current documentation:\n\nThe current description of np.tensordot() is\r\n ```\r\nWhen `axes` is integer_like, the sequence for evaluation will be: first\r\nthe -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\r\nNth axis in `b` last.\r\n```\r\nHowever the source code of it is\r\n```\r\nexcept Exception:\r\n    axes_a = list(range(-axes, 0))\r\n    axes_b = list(range(0, axes))\r\nelse:\r\n    axes_a, axes_b = axes\r\n```\r\nThere is a conflict. For example, when N=10 list(range(0,10)) and list(range(-10,0)) are the following, respectively:\r\n```\r\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \r\nand\r\n[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1]\r\n```\r\nSo I think it should be \"first 0 and N-1th axis in b last\" rather than \"first 0 and Nth axis in b last\".\r\n\n\n### Idea or request for content:\n\nTherefore, I modified the description to\r\n```\r\nWhen `axes` is integer_like, the sequence for evaluation will be: first\r\nthe -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and\r\n(N-1)th axis in `b` last. When N-1 is smaller than 0, or when -N is larger\r\nthan -1, the element of 'a' and 'b' are defined as the 'axes'.\r\n```\r\nAnd I add an example to use the default parameter, as follows:\r\n```\r\n\"traditional\" examples:\r\n>>> # An example on integer_like\r\n>>> a_0 = np.array([[1,2],[3,4]])\r\n>>> b_0 = np.array([[5,6],[7,8]])\r\n>>> c_0 = np.tensordot(a_0,b_0,0)\r\n>>> c_0.shape\r\n(2, 2, 2, 2)\r\n>>> c_0\r\narray([[[[ 5,  6],\r\n         [ 7,  8]],\r\n        [[10, 12],\r\n         [14, 16]]],\r\n       [[[15, 18],\r\n         [21, 24]],\r\n        [[20, 24],\r\n         [28, 32]]]])\r\n>>> # An example on array_like\r\n```\r\nI submit the PR here https:\/\/github.com\/numpy\/numpy\/pull\/23547\r\nI hope it can be modified at an early date, because I found many people are confused for this description at stack overflow and other places.","comments":[],"labels":["04 - Documentation"]},{"title":"BUG: empty-np.array indexing does not work, but empty-tuple indexing works instead","body":"### Describe the issue:\n\nAs the subject says, see code example attached\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\n\r\nM = np.array([[1.23, 4.56, 7.89],[2.34,5.67, 8.91],[3.45, 6.78, 9.01]])  # np.ndarray of shape (3,3)\r\n\r\n# Trying to slice some rows\/columns from this matrix works both using non-empty and empty tuples:\r\nM[:,(0,2)]  # get a submatrix with first and third column of the original one\r\nM[(0,1),:]  # get a submatrix with first and second row of the original one\r\nM[(),:]  # get an empty-matrix of shape (0,3)\r\nM[:,()]  # get an empty-matrix of shape (3,0)\r\n\r\n# However, if we use numpy 1D arrays for indexing instead of tuples...\r\n\r\n# everything fine for non-empty 1D arrays...\r\nassert np.all(M[:,np.array((0,2))] == M[:,(0,2)])  # OK\r\nassert np.all(M[np.array((0,1)),:] == M[(0,1),:])  # OK\r\n\r\n# but with empty np.arrays an error is thrown\r\nM[:,np.array(())]\n```\n\n\n### Error message:\n\n```shell\nIndexError: arrays used as indices must be of integer (or boolean) type\n```\n\n\n### Runtime information:\n\n1.23.5\r\n3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:27:40) [GCC 11.3.0]\r\n\r\n\n\n### Context for the issue:\n\nDon't know if this is intended behaviour, but I'm optimizing some code with numba (using numba 0.57.0rc1 with python 3.11) in nopython mode, and I'm replacing standard tuples with np.arrays where possible. There's a point in the code when I perform slicing\/indexing and then doing an np.sum, and sometimes it happens to have empty index sets with zero sum - hence, empty tuples or empty np.arrays - I cannot use the latter ones due to this.","comments":["The error is clear here:\r\n\r\n> IndexError: arrays used as indices must be of integer (or boolean) type\r\n\r\n`np.array(())` is an array of type `np.float64`. I suppose we could extend the error message to include the actual type used.","ok, so it was only due to the fact that empty np.arrays have float as default type. Forcing dtype=int made it work, thanks. A more complete error message containing the array type would be welcome as well."],"labels":["00 - Bug"]},{"title":"BUG: isscalar(None) should be True, is False","body":"### Describe the issue:\r\n\r\n`None` is clearly not an array, hence scalar. I couldn't find a previous issue, but I really doubt this is intentional - at least it makes no sense to me, and it makes it very hard to nicely generic code, which should survive a user having a `None` somewhere, and not suddenly jumping form the scalar into the array path.\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> np.isscalar(None)\r\nFalse\r\n>>> np.__version__\r\n'1.24.2'\r\n```","comments":["[It's documented](https:\/\/numpy.org\/doc\/1.24\/reference\/generated\/numpy.isscalar.html) (falls in the category of \"other builtin objects\"). As the docstring mentions, \"In most cases `np.ndim(x) == 0` should be used instead of this function\".","Thanks. What is the purpose of this divergence between `np.isscalar` and `np.ndim(x) == 0`, and if there ever existed a good reason for it, is that reason still relevant today ? Without further context, I think that's an unnecessary footgun (small, but still), and `np.isscalar` should be taught to be more like `np.ndim(x) == 0`. \r\n\r\nI guess the trickiest part would be handling of zero-dimensional numpy arrays, but even if that is left untouched, `isscalar` should not give a false negative for builtins.","You can see previous discussion in gh-19690 for instance. We've never been particularly consistent or precise about what exactly a \"scalar\" means; there are a couple of different contexts where you want different answers, and we mostly just want to consider `isscalar()` to be deprecated. To implement one of those vague semantics, `isscalar()` returns `True` only for an enumerated set of types and `False` for anything else. I'm not sure I'd go so far as to flip that and return `False` only for an enumerated set of conditions and `True` for anything else. But I'd probably be willing to consider adding `None` to the enumerated set of objects that treated as scalars here, especially since it's frequently an `NA` scalar value for string `pandas.Series`. But mostly we want to people to stop using `isscalar()`, if they can. I do agree that it's a footgun."],"labels":["00 - Bug"]},{"title":"DOC: clarify int type constraint in numpy.bincount","body":"This doc string change clarifies the `int` dtype constraint in `np.bincount()`.\r\n\r\nThe type of input elements is not only required to be positive integers, as stated, but it must actually be of the native int type. Otherwise, it will raise a TypeError when the input type is wider than the native array index type.\r\n\r\nThis was discovered while debugging the https:\/\/github.com\/mikedh\/trimesh library on a 32-bit system. This library uses `np.int64` gratuitously, which causes many unit test to fail: https:\/\/salsa.debian.org\/3dprinting-team\/trimesh\/-\/jobs\/4103005#L2836\r\n\r\nIt would be very helpful if this is clearly explained in the numpy documentation.\r\n\r\nFixes: #23526 \r\n\r\n","comments":["Thanks for the PR. I'm a little confused about what you mean, though. If I'm understanding the problem correctly, then I think the code would still work without error if you used `np.int8` or `np.int16`, which aren't the native int type. A more accurate doc example might be \"this code can't use `dtype=np.int64` on 32-bit architecture\".","You're right, a smaller data type will also work.\r\nI was too focused on the wrong assumptions made in the Python module I was debugging (the developers used int64 all over the place), and I didn't even think of that.\r\n\r\nNevertheless, I think the limitation should be made clear. I'll try to rewrite the text.","> EDIT: Also, the documentation on line 931 has to be tweaked.\r\n\r\nDone.","> Out of curiosity, is this something that should be called out? I suspect that using `np.int64` on a 32-bit system would cause problems everywhere, not just with bincount.\r\n\r\nAside from `bincount`, I identified two other functions that depend on a native index type: `take` and `repeat`, at least that's the ones that were causing problems in the trimesh module. I suppose there may be others as well.","> Out of curiosity, is this something that should be called out? I suspect that using `np.int64` on a 32-bit system would cause problems everywhere, not just with bincount.\r\n\r\nIt's not obvious that this is the case, though, since 32-bit systems can still support 64-bit integers.\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/23038451\/how-does-a-32-bit-processor-support-64-bit-integers\r\n\r\n`np.float128` can also be used just fine on x86_64 architecture (and maybe 32-bit systems, although I have no way of checking), so the disparity between the size of the dtype and the architecture isn't always a problem.","The actual requirement is more like we need the input to be `safe`-castable to the integer type used for indexing; i.e. `np.intp`. All of the affected functions are using the inputs as indices (or using them to manipulate indices). Lots of other functions work fine with `np.int64` on 32-bit systems; you can add, multiply, subtract them all you like, but when you want to use them as indices, we need to convert them to `np.intp` first, and the default `safe` casting will raise an error in such cases because that would be a downcast.\r\n\r\nJust a note, `np.intp` isn't the \"native integer dtype\"; we usually call the result of `np.dtype(int)` the native integer dtype, i.e. whatever a C `long` is. For very annoying reasons, on 64-bit Windows, these are not the same. The 64-bitness specifies the integer type of memory addresses (and thus indices), so `np.intp` is `np.int64`. But `np.dtype(int)` is still `np.int32` because Windows decided to keep C `long` types 32-bit.","Does that mean `dtype=int` really is the appropriate type and will work on all platforms?\r\nBecause that's the way I've worked around the problems in trimesh: https:\/\/salsa.debian.org\/3dprinting-team\/trimesh\/-\/blob\/master\/debian\/patches\/02-array-index-32bit-architectures.patch\r\n","No, `dtype=np.intp`.","> No, `dtype=np.intp`.\r\n\r\nOh. I see. That's quite confusing. :disappointed: ","FWIW, I am hoping we can change `intp` to be the default integer type in NumPy 2.0 (and that hopfully could be released by next year.  Changes behavior a fair bit on windows, but for most users getting int64 rather than int32 really shouldn't matter in practice.\r\n\r\nIn many places (apparently not bincount), NumPy uses \"same-kind\" casting for indexing related operations, because otherwise the int64 ->int32 step not going through is tedious.  In practice nobody really notices that, but this is unsafe (you can generate very large integer index values that should raise an error but don't).\r\n\r\n(It would be nice to find a solution for this too, but we do not have it.)","A possible solution could be to add new casting modes to [ndarray.astype](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.ndarray.astype.html):\r\n* Always downcast, but print a warning when there is data loss\r\n* Downcast when there is no data loss, but raise an error if a value exceeds the range of the target type\r\n* Downcast with saturation, i.e. limit the range to the target type and never exceed the positive or negative maximum value\r\n\r\nAll of these would come at a performance cost, but some vectorized optimization may be possible."],"labels":["04 - Documentation"]},{"title":"BUG: Error not raised for an unsafe cast when using casting=\"safe\"","body":"### Describe the issue:\n\nWhen using a safe cast, numpy should raise if the value cannot be safely casted. In this case, the int64 18014398509481983 cannot be safely casted to a float64, so an error should be raised.\r\n\r\nSimilar issue discussed in https:\/\/github.com\/apache\/arrow\/issues\/34901\n\n### Reproduce the code example:\n\n```python\n>>> import numpy as np\r\n>>> np.__version__\r\n'1.24.2'\r\n\r\n# Bug: No safety error for initial int64 -> float64 conversion\r\n>>> np.array([18014398509481983]).astype(\"float64\", casting=\"safe\").astype(str)\r\narray(['1.8014398509481984e+16'], dtype='<U32')\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n```\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX',\r\n                                'AVX512_CLX'],\r\n                      'not_found': ['AVX512_CNL', 'AVX512_ICL']}}]\r\nNone\r\n>>>\r\n>>> print(np.__version__)\r\n1.24.2\r\n>>> sys.version\r\n'3.8.10 (tags\/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]'\r\n>>>\r\n```\n\n### Context for the issue:\n\n_No response_","comments":["@MatteoRaso I understand float64 to int64 is not always safe, but the floating point specifications require for integral values between 2^53 and 2^54 for safe conversion to be a multiple of 2. You can see in the example above, the floating value is 18014398509481984.0 when the original int64 was 18014398509481983. It seems like if casting=\"safe\" is specified, then this operation should raise, instead of changing data.","This is the behavior I observed:\r\n\r\n```\r\n>>> arr = np.array([18014398509481983])\r\n>>> arr\r\narray([18014398509481983], dtype=int64)\r\n>>> arr.astype(\"float64\", casting=\"safe\")[0]\r\n1.8014398509481984e+16\r\n```\r\n\r\nI get your point that the cast from float64 to int64 raises, but it seems like the original cast from int64 to float64 itself changed the data,  since the value changed from 18014398509481983 to 18014398509481984.0, so it seems like that cast itself was not safe. The floating point specification mentions that Integers between 2^53 and 2^54 = 18,014,398,509,481,984 round to a multiple of 2 (even number), which seems like for a safe conversion, should raise as well if data is not preserved and it has to round.\r\n","> This is the behavior I observed:\r\n> \r\n> ```\r\n> >>> arr = np.array([18014398509481983])\r\n> >>> arr\r\n> array([18014398509481983], dtype=int64)\r\n> >>> arr.astype(\"float64\", casting=\"safe\")[0]\r\n> 1.8014398509481984e+16\r\n> ```\r\n> \r\n> I get your point that the cast from float64 to int64 raises, but it seems like the original cast from int64 to float64 itself changed the data, since the value changed from 18014398509481983 to 18014398509481984.0, so it seems like that cast itself was not safe. The floating point specification mentions that Integers between 2^53 and 2^54 = 18,014,398,509,481,984 round to a multiple of 2 (even number), which seems like for a safe conversion, should raise as well if data is not preserved and it has to round.\r\n\r\nYou're right. I just re-checked the results of Numpy\r\n```\r\n>>> np.array([18014398509481983]).astype(\"float64\") == 18014398509481984\r\narray([ True])\r\n>>> np.array([18014398509481983]).astype(\"float64\") == 18014398509481983\r\narray([ True])\r\n```\r\nSorry about the mixup, please disregard my earlier comments. This is quite a problem, since Numpy considers this a safe cast. To fix this, Numpy would have to be able to identify this as an unsafe cast, which would be hard. It;d also mean that Numpy will have to change its casting rules to disallow \"safe\" conversions from `int64` to `float64`, which can have serious consequences downstream. In any case, fixing this would have to be a fairly large project, not something that can be done in a few lines.","Yea its not a trivial fix, and likely lots of downstream ramifications. But wanted to bring this up either way as a similar issue was being discussed in arrow.","There is a long history for this:  The core is how NumPy still does promotion (in legacy code paths).  That is NumPy mixes up promotion and casting: `float64 + int64 -> float64` (OK), but that doesn't mean that it has to consider it \"safe\" to do that cast in principle.\r\nBut right now that is how NumPy thinks promotion works in many places: Promotion just uses the concept of \"safe\" casting.  (Removing that in the mid-term probably leads to its own set of surprises, but I won't disagree that it may be better.)"],"labels":["00 - Bug"]},{"title":"DOC: deprecated test syntax","body":"### Issue with current documentation:\n\nGood afternoon.\r\n\r\nI made a build of the source code from the main branch and ran the tests.\r\nI got an error when specifying a test as a parameter `--tests TESTS, -t TESTS`\r\nWhen executing this code:\r\n\r\n```\r\npython runtests.py -v -t numpy\/core\/tests\/test_nditer.py\r\n```\r\nI get error: `ERROR: module or package not found: numpy\/core\/tests\/test_nditer.py (missing __init__.py?)`\r\n\r\nIf I use which syntax\r\n\r\n```\r\npython runtests.py -v -t numpy.core.tests.test_nditer\r\n```\r\n\r\nEverything works correctly\r\n\r\nIn the documentation, including the file  [development_environment.rst](https:\/\/github.com\/numpy\/numpy\/blob\/main\/doc\/source\/dev\/development_environment.rst#testing-builds), the first implementation option is indicated. \r\n\r\n## Runtime information:\r\n```\r\n>>> import sys, numpy; print(numpy.__version__); print(sys.version)\r\n1.25.0.dev0+1062.g7f1ce595c\r\n3.10.10 (main, Mar 23 2023, 17:28:09) [GCC 10.2.1 20210110]\r\n>>> print(numpy.show_runtime())\r\nWARNING: `threadpoolctl` not found in system! Install it by `pip install threadpoolctl`. Once installed, try `np.show_runtime` again for more detailed build information\r\n[{'numpy_version': '1.25.0.dev0+1062.g7f1ce595c',\r\n  'python': '3.10.10 (main, Mar 23 2023, 17:28:09) [GCC 10.2.1 20210110]',\r\n  'uname': uname_result(system='Linux', node='aa562c94224b', release='5.15.90.1-microsoft-standard-WSL2', version='#1 SMP Fri Jan 27 02:56:13 UTC 2023', machine='x86_64')},\r\n {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX',\r\n                                'AVX512_CLX',\r\n                                'AVX512_CNL',\r\n                                'AVX512_ICL'],\r\n                      'not_found': ['AVX512_KNL', 'AVX512_KNM']}}]\r\n```\r\n\r\nIf this documentation is outdated, I might update it.\n\n### Idea or request for content:\n\n_No response_","comments":["Hmmm, `runtests` does a bit of a round-about invocation of via calling `np.test()`.  Which looks like it ends up using `... --pyargs <test1> <test2>`.\r\n\r\nProbably pytest changed `--pyargs` to not accept the directory spelling anymore?  I wonder if we should just tweak `runtests` to also accept paths if that really changed.  OTOH, probably also OK to just document it and get used to the `.` spelling.  (although I think pytest prints the test names as path making copy pasting harder then)","@seberg I agree with you. Specifying the test via the path looks more convenient than what we have now. I'll look in the history of the reasons for these changes.","There is a point that I think `spin` should replace runtests relatively soon, either way.  \r\n\r\nSo rather than trying to hunt that down, that section should maybe just point to using spin instead:\r\n```\r\nspin test\r\n```\r\n(requires installing spin through conda-forge or pip of course)\r\n\r\nI suspect `spin` calls `pytest` more directly anyway, so that paths work fine.\r\n\r\n\r\nEDIT: ~IIRC, there may still be an issue that running tests in gdb is a bit harder due to subprocess use in `spin` right now.~  NVM, this is long fixed."],"labels":["04 - Documentation"]},{"title":"BUG: Polynomial degree doesn't account for 0 coefficient","body":"### Describe the issue:\n\nI was under the assumption that the `Polynomial.degree()` method yielded the degree of the polynomial, but after looking at the code in main and the return value in the docs, it just subtracts one from the number of coefficients, which doesn't account for a 0 in the highest degree coefficient (possible if another process determines the coefficients that get passed in).\n\n### Reproduce the code example:\n\n```python\nimport numpy as np\r\nnp.polynomial.Polynomial([1, 7, 4, 0]).degree()  # returns 3, but I would expect 2\n```\n\n\n### Error message:\n\n_No response_\n\n### Runtime information:\n\n1.23.1\r\n3.8.11 (default, Jul 29 2021, 14:57:32)\r\n[Clang 12.0.0 ]\n\n### Context for the issue:\n\nI'm happy to put in a PR if this is not by design.","comments":["There is a `trim` method for that. Zero in the numerical sense can be a little fuzzy, so removing it is left to the user's discretion.","Makes sense.","An example may be useful.","Agreed. Where would be the best spot for the example? The class itself, the `degree()` method, and\/or the `trim()` method?","My vote would be the examples section for the `degree` method docstring - perhaps it could be added to #23530"],"labels":["00 - Bug"]},{"title":"DOC: bincount documentation should mention native index type requirement","body":"### Issue with current documentation:\n\nThe [documentation for the bincount function](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.bincount.html) does not explain that the input must be of the native type used for array indexes.\r\n\r\nThis causes problems on 32-bit architectures, when a simple example like the following is executed with safety mode on:\r\n\r\n```python\r\nimport numpy as np\r\ndata = np.random.default_rng().integers(low=0, high=100, dtype=np.int64, size=10)\r\nnp.bincount(data)\r\n```\r\n\r\nResult:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bincount.py\", line 6, in <module>\r\n    np.bincount(data)\r\n  File \"<__array_function__ internals>\", line 200, in bincount\r\nTypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n```\r\n\r\nWith `dtype=int32` (fixed type) or `dtype.int_` or simply `int` (native int type), it works fine.\n\n### Idea or request for content:\n\nI'd suggest changing the example at https:\/\/github.com\/numpy\/numpy\/blob\/main\/numpy\/core\/multiarray.py#L948-L955 from:\r\n\r\n```\r\n    The input array needs to be of integer dtype, otherwise a\r\n    TypeError is raised:\r\n    >>> np.bincount(np.arange(5, dtype=float))\r\n    Traceback (most recent call last):\r\n      ...\r\n    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')\r\n    according to the rule 'safe'\r\n```\r\n\r\nto:\r\n\r\n```\r\n    The input array needs to be of the native integer index type, otherwise a\r\n    TypeError is raised:\r\n    >>> np.bincount(np.arange(5, dtype=float))\r\n    Traceback (most recent call last):\r\n      ...\r\n    TypeError: Cannot cast array data from dtype('float64') to dtype('int64')\r\n    according to the rule 'safe'\r\n\r\n    The simplest way to do this is by always using the Python int type or dtype.int_:\r\n    >>> np.bincount(np.arange(5, dtype=int))\r\n```\r\n","comments":["Are you going to work on this issue ?","I can submit a PR, if you think it makes sense?","I guess so,  when you  hover over the function it will show this description so it means that it has a value."],"labels":["04 - Documentation"]},{"title":"BUG: AVX-512 results differ from non-AVX-512 for sin, cos, exp, log, etc.","body":"### Describe the issue:\r\n\r\nIt appears that there is a small difference between functions executed using AVX-512 and not using AVX-512. Is this expected? My admittedly limited understanding is that computation with AVX-512 should produce results identical to other computations.\r\n\r\nFor example, `math.sin` and `np.sin` agree with AVX-512 disabled, but disagree by approximately`2**-53` (approximately double precision) depending on the input angle. `np.cos` is similar. I've also tested some of the other SIMD backed numpy functions, such as `np.exp` and `np.log` and they also return non-zero differences with the `math` functions unless AVX-512 is explicitly disabled or not available (such as on AMD or ARM).\r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n(py39) [asreimer@fedora ~]$ python\r\nPython 3.9.16 (main, Apr  3 2023, 12:24:01) \r\n[GCC 11.3.1 20220421 (Red Hat 11.3.1-3)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import math\r\n>>> import numpy as np\r\n>>> print('Difference of sines: {}'.format(np.sin(np.radians(45))-math.sin(np.radians(45))))\r\nDifference of sines: 1.1102230246251565e-16\r\n>>> print('Difference of exponentials: {}'.format(np.exp(12.312)-math.exp(12.312)))\r\nDifference of exponentials: -2.9103830456733704e-11\r\n>>> print('Difference of natural logorithms: {}'.format(np.log(1332.312)-math.log(1332.312)))\r\nDifference of natural logorithms: -8.881784197001252e-16\r\n>>> \r\n(py39) [asreimer@fedora ~]$ NPY_DISABLE_CPU_FEATURES=\"AVX512F,AVX512CD,AVX512_SKX,AVX512_CLX,AVX512_CNL,AVX512_ICL\" python\r\nPython 3.9.16 (main, Apr  3 2023, 12:24:01) \r\n[GCC 11.3.1 20220421 (Red Hat 11.3.1-3)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import math\r\n>>> import numpy as np\r\n>>> print('Difference of sines: {}'.format(np.sin(np.radians(45))-math.sin(np.radians(45))))\r\nDifference of sines: 0.0\r\n>>> print('Difference of exponentials: {}'.format(np.exp(12.312)-math.exp(12.312)))\r\nDifference of exponentials: 0.0\r\n>>> print('Difference of natural logorithms: {}'.format(np.log(1332.312)-math.log(1332.312)))\r\nDifference of natural logorithms: 0.0\r\n>>>\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\nDefault:\r\n\r\n    >>> import sys, numpy; print(numpy.__version__); print(sys.version); print(numpy.show_runtime())\r\n    1.24.2\r\n    3.9.16 (main, Apr  3 2023, 12:24:01) \r\n    [GCC 11.3.1 20220421 (Red Hat 11.3.1-3)]\r\n    [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                          'found': ['SSSE3',\r\n                                    'SSE41',\r\n                                    'POPCNT',\r\n                                    'SSE42',\r\n                                    'AVX',\r\n                                    'F16C',\r\n                                    'FMA3',\r\n                                    'AVX2',\r\n                                    'AVX512F',\r\n                                    'AVX512CD',\r\n                                    'AVX512_SKX',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL'],\r\n                          'not_found': ['AVX512_KNL', 'AVX512_KNM']}},\r\n     {'architecture': 'SkylakeX',\r\n      'filepath': '\/home\/asreimer\/venvs\/py39\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so',\r\n      'internal_api': 'openblas',\r\n      'num_threads': 4,\r\n      'prefix': 'libopenblas',\r\n      'threading_layer': 'pthreads',\r\n      'user_api': 'blas',\r\n      'version': '0.3.21'}]\r\n\r\nWith AVX-512 disabled via `NPY_DISABLE_CPU_FEATURES`:\r\n\r\n    >>> import sys, numpy; print(numpy.__version__); print(sys.version); print(numpy.show_runtime())\r\n    1.24.2\r\n    3.9.16 (main, Apr  3 2023, 12:24:01) \r\n    [GCC 11.3.1 20220421 (Red Hat 11.3.1-3)]\r\n    [{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                          'found': ['SSSE3',\r\n                                    'SSE41',\r\n                                    'POPCNT',\r\n                                    'SSE42',\r\n                                    'AVX',\r\n                                    'F16C',\r\n                                    'FMA3',\r\n                                    'AVX2'],\r\n                          'not_found': ['AVX512F',\r\n                                        'AVX512CD',\r\n                                        'AVX512_KNL',\r\n                                        'AVX512_KNM',\r\n                                        'AVX512_SKX',\r\n                                        'AVX512_CLX',\r\n                                        'AVX512_CNL',\r\n                                        'AVX512_ICL']}},\r\n     {'architecture': 'SkylakeX',\r\n      'filepath': '\/home\/asreimer\/venvs\/py39\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so',\r\n      'internal_api': 'openblas',\r\n      'num_threads': 4,\r\n      'prefix': 'libopenblas',\r\n      'threading_layer': 'pthreads',\r\n      'user_api': 'blas',\r\n      'version': '0.3.21'}]\r\n\r\n### Context for the issue:\r\n\r\nWhile the differences here are small, they are unexpected and can cause larger problems. For example, when performing vector rotations using rotation matrices, the small differences of 1e-16 can blow up to something closer to 1e-5. This is how I originally found this bug: finding differences in expected output while using rotation matrices on AMD and ARM based systems without AVX-512 compared to the results from an Intel system with AVX-512.","comments":["Note that this issue is not present in `numpy<=1.20.3`.","This is a speed-vs-consistency tradeoff. We accept differences of up to 4 ULP from the \"golden results\", depending on which implementations are used. You can use [`NPY_DISABLE_CPU_FEATURES`](https:\/\/numpy.org\/devdocs\/reference\/simd\/build-options.html#runtime-dispatch) and, once #22058 is merged, `NPY_ENABLE_CPU_FEATURES` to get consistent answers across different machines.\r\n\r\n> Note that this issue is not present in numpy<=1.20.3.\r\n\r\nI think older versions do not have SIMD accelerated loops.","Thank you, I appreciate the reply! For now, as you can see in the logs I posted, we have already been using the `NPY_DISABLE_CPU_FEATURES` variable. That was how I was able to confirm this issue was specific to AVX-512.\r\n\r\nI really like the idea of #22058. Making SIMD opt-out when it breaks consistency causes headaches, especially when the release notes don't include warnings or notes about this being the case (maybe they do? I couldn't find anything searching yesterday, but I could have overlooked something, it was a long day). I say this not to be critical, but to add some weight to making this an opt-in feature. Users should knowingly opt-in to making speed-vs-consistency tradeoffs like this, but I know this is a tricky line to walk as a developer.","I would be happy to close this when https:\/\/github.com\/numpy\/numpy\/pull\/22137 is merged. I think that is a sensible solution to this issue.","I think this can be closed now, but it is still not documented, see https:\/\/github.com\/numpy\/numpy\/issues\/22059","For future travelers, if you are looking for bitwise reproducibility in these functions, set the environment variable `NPY_ENABLE_CPU_FEATURES=none`. The string \"none\" doesn't matter, it just needs to be defined and non-empty.","I agree with @asreimer that I wish this was opt-in. I could even imagine it printing a warning when these features are available but _not_ opted into, but the current opt-out default is surprising to me.\r\n\r\nI bet it is surprising to a lot of other people too, but the reason they aren't commenting here is because they never figure out the root cause of why they keep getting different results every time they run their code on a different computer. It took me quite some time to debug where in a complex data processing pipeline the discrepancies were sneaking in, and I did _not_ expect it to be `np.log`. `np.log` is a trivial amount of the computation I'm doing and turning off the optimizations doesn't cause even a perceptible performance difference for my overall task.","> I think this can be closed now, but it is still not documented\r\n\r\n@zmbc actually, we removed the biggest precision offenders again (not sure if in NumPy 1.25 or 1.26), but not sure how exact a match you are hoping for.\r\n\r\nFWIW, even *without* any SIMD code there are many things that are expected to differ from platform to platform (hardware *or* software difference, like OS\/math library versions):  it's a deep stack and computers don't make it easy to get the exact same results."],"labels":["00 - Bug"]},{"title":"DOC: Representing big float32 numbers as string","body":"### Issue with current documentation:\n\nWhile working with float numbers using `np.float32` datatype I noticed that using rather big numbers (which does not have fraction) and printing them (or just representing as string) can mislead.\r\n\r\nExample code below shows, how representing `np.float32` number as string can mislead. \r\n```python\r\nimport numpy as np\r\n\r\nnum = np.nextafter(np.float32(134217720), np.float32(np.inf))\r\nprint(num)  # 134217730\r\n```\r\nNumber _134217720_ belongs to the interval $[2^{26}, 2^{27}]$, where precision (or the smallest difference between two nearest numbers) is _8_. So if we what to take the next after _134217720_ we should get _134217728_ (which can directly stored as float32 without conversion error unlike _134217730_). \r\n\r\nEven if _134217728_ casted to `np.float32` datatype and we want to represent it as a string, we will get _134217730_\r\n```python3\r\nnum = np.float32(134217728)\r\nprint(num) # 134217730\r\n```\r\nGoing deeper, I found that there is a `format_float_positional` [function](https:\/\/github.com\/numpy\/numpy\/blob\/v1.24.0\/numpy\/core\/arrayprint.py#L1130-L1219) which is responsible for representing float as a string (I don't know how do `__str__` or `__repr__`  functions work for `np.float32` but looks like they have the same behavior). It has a _unique_ parament which is *True* by default. From [documentation](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.format_float_positional.html#numpy.format_float_positional):\r\n> If True, use a digit-generation strategy which gives the shortest representation which uniquely identifies the floating-point number from other values of the same type, by judicious rounding\r\n\r\nSo it means that _134217730_ is a shortest unique representation for _134217728_ made by **judicious** rounding (Dragon4 algorithm), which looks weird. And as the number becomes bigger inaccuracy by rounding increases.\r\nOf course this kind of representation is more convenient when you work with small numbers, but in this case (especially if you don't about float representation at all) it misleads.\n\n### Idea or request for content:\n\nSo I don't know is it a bug, but may be it's a good idea to make a remark in docs describing that effect starting with large numbers.","comments":[],"labels":["04 - Documentation"]},{"title":"ENH: speed up einsum with optimize using batched matmul","body":"This PR enables dispatching *all* pairwise calls in `einsum` when `optimize=` is turned on to batched matrix multiply (or non-batched `matmul` and `multiply` as special cases). For a short explanation of the implementation see here - https:\/\/github.com\/jcmgray\/einsum_bmm.\r\n\r\nCurrently falling back to `c_einsum` for these, when there exist batch indices, induces potentially many orders of magnitude slow downs (#22604, e.g. 7000x), particularly in the case of large or high dimensional tensors.\r\n\r\nAfter fairly extensive benchmarking (below), I *believe* that this should be a uniform improvement across essentially all the cases when one would use `optimize=True`. This is partly enabled by the modern performance of matmul which in the past was not always faster than `einsum`. However, I know `einsum` has an enormous number of uses and I'd gladly accept more suggestions for benchmarks, and would understand people's hesitation! \r\n\r\n(@dgasmith, @seberg)\r\n\r\n## Other notes:\r\n- This replaces all the `can_blas` logic and `tensordot` calls, and simplifies everything into the `bmm_einsum`, which encapsulates how `tensordot` is implemented\r\n- When there are no contracted indices, `numpy.multiply` is used for slightly better performance\r\n- The current implementation can simply pass `out` and other kwargs on to `mutiply` and `matmul`, unlike tensordot, meaning `einsum('ij,jk->ik', x, y, optimize=True, out=z)` should write directly to `z`\r\n- The fusing allows `einsum(eq, ..., optimize=True)` to be used when there are more than 32 indices involved (#5744).\r\n- The low overhead of the actuall `bmm_einsum` implementation comes from caching what operations to use based on `(eq, x.shape, y.shape)`, using `@functools.lru_cache(2**12)`.\r\n- the performance broadly speaking is brought in line with `torch.einsum`\r\n\r\n## TODO: \r\n- [ ] update docs?\r\n- [ ] decide cache size?\r\n- [ ] should I lint the existing `einsumfunc.py` code?\r\n\r\n## Benchmarks\r\n\r\nIn the following the kernels are the following:\r\n\r\n- `no optimize`: the base `np.einsum(eq, x, y)` i.e. `c_einsum`\r\n- `optimize + dot`: the current `np.einsum(eq, x, y, optimize=True)` which calls `tensordot` where possible, and also induces some overhead from the potential path optimization (even though there are only two terms in the following, the equation must be checked and parsed etc.)\r\n- `optimize + bmm`: the proposed `np.einsum(eq, x, y, optimize=True)` which calls batched matmul, and still induces some overhead from the potential path optimization\r\n- `no optimize + bmm`: this calls the pairwise bmm einsum directly, and is provided simply to show what overhead comes from the bmm impl, and what from the potential path optimization \r\n\r\nThe main two to compare are `optimize + dot` to `optimize + bmm`. Overall I'd summarize the results as the following:\r\n\r\n1. when a contraction is memory bound, all methods are pretty much equivalent\r\n2. when a contraction involves no batch dimensions, performance of `dot` is retained\r\n3. when a contraction involves batch dimensions, performance can be orders of magnitude faster\r\n\r\nAll dimensions are size `n` unless otherwise noted.\r\n\r\n![vec_inner](https:\/\/user-images.githubusercontent.com\/8982598\/229239233-f79f335c-677c-42ad-bb98-ec230ffff4ba.png)\r\n\r\n![vec_outer](https:\/\/user-images.githubusercontent.com\/8982598\/229239247-47423fbd-5728-4ddb-b318-088e13cc9c3e.png)\r\n\r\n![square_matmul](https:\/\/user-images.githubusercontent.com\/8982598\/229245134-4708158d-f566-493c-bdbe-8f6bd2986b98.png)\r\n\r\n![square_matvec](https:\/\/user-images.githubusercontent.com\/8982598\/229245235-cf629050-ff83-4f00-ab7c-6087b5c09365.png)\r\n\r\n\r\nFor `batch_matmul_small` the sizes are `(n, 2, 2), (n, 2, 2)`.\r\n\r\n![batch_matmul_small](https:\/\/user-images.githubusercontent.com\/8982598\/229239252-995bcf6d-15de-476a-80b6-8bb2f0127b3e.png)\r\n\r\n![square_vecmat](https:\/\/user-images.githubusercontent.com\/8982598\/229239254-3938e60e-837f-452f-961d-6d6b771dbca9.png)\r\n\r\n![batch_matmul_equal](https:\/\/user-images.githubusercontent.com\/8982598\/229239255-a53912ca-06e3-4949-8d3c-915d344ae549.png)\r\n\r\nFor `batch_matmul_large` the sizes are `(10, n, n), (10, n, n)`.\r\n\r\n![batch_matmul_large](https:\/\/user-images.githubusercontent.com\/8982598\/229239257-bdb12726-8b3a-4de6-96f9-af32cd3565d2.png)\r\n\r\n![hadamard](https:\/\/user-images.githubusercontent.com\/8982598\/229239259-03cb8172-4f69-4127-a888-24da8b324ee9.png)\r\n\r\n![hadamard_unalinged](https:\/\/user-images.githubusercontent.com\/8982598\/229239261-d2c10947-c2bc-4a92-a12e-fc0f4f22a22c.png)\r\n\r\n![CCSDT_1](https:\/\/user-images.githubusercontent.com\/8982598\/229239262-20e7dce6-bb1f-4226-93f6-2067be2f14e9.png)\r\n\r\n![interleaved_dot](https:\/\/user-images.githubusercontent.com\/8982598\/229239263-8abb6eb0-cdd5-43e0-a118-51b18e716ca0.png)\r\n\r\n![interleaved_batched_dot](https:\/\/user-images.githubusercontent.com\/8982598\/229239264-1ef48196-1036-4418-9007-ae583c0d0a3a.png)\r\n\r\nIn `random_extreme` the shapes are taken as the following:\r\n```python\r\n[\r\n    (n, 4, 3, 4, 3, 4, 2, 4, 2, n, 2, n, 3, 2, 4),\r\n    (2, 4, n, n, 4, 4, 4, 3, 4, 4, 4, 3, 4)\r\n]\r\n```\r\n\r\n![random_extreme](https:\/\/user-images.githubusercontent.com\/8982598\/229239265-1bf3da61-b35f-4300-8aee-55367d0af183.png)\r\n\r\nIn `many_small_dims_dot` the shapes are `([2] * n, [2] * n)` and half in an interleaved pattern are contracted.\r\n\r\n![many_small_dims_dot](https:\/\/user-images.githubusercontent.com\/8982598\/229239268-f90edf60-7c64-4437-807b-48b8004d7710.png)\r\n\r\nIn `many_small_dims_batched_dot` the shapes are `([2] * n, [2] * n)` and a third in an interleaved pattern are contracted and a third in an interleaved pattern are batch indices.\r\n\r\n![many_small_dims_batched_dot](https:\/\/user-images.githubusercontent.com\/8982598\/229245350-91a26a21-82c5-4a5b-aa7b-eff192882ca1.png)\r\n\r\n","comments":["No rush on my part, but wondering if there are any thoughts about this change or things I can do for this PR?","TBH, I had hoped @dgasmith can give a recommendation.  The timings do look like this should work pretty generically, although its easy to miss cases, I guess.\r\n\r\n`matmul` does currently have a pretty big flaw with strided inputs unfortunately (there is another PR that would address this), would that be related here?","Hi @seberg, ah yes I just checked and currently that matmul behaviour would result in a performance regression for the specific case that arrays reach the matmul call (after preparatory reshape\/transpose etc) with whichever strides trigger the non-blas, no-copy call (#23588).\r\n\r\n```python\r\nimport numpy as np\r\nd = 1000\r\nx = np.random.randn(d, d)\r\ny = np.random.randn(d, d) + 1j * np.random.randn(d, d)\r\n\r\n%%timeit\r\nz = np.einsum('ij,jk->ik', x, y.real, optimize=True)\r\n# -->\r\n# 11.5 ms \u00b1 473 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)  (current dot backend)\r\n# -->\r\n# 897 ms \u00b1 23.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)  (new matmul backend)\r\n```\r\n\r\nIf the solution in #23752 is to do the copy when needed then that is very much aligned with the logic of `optimize=True`, i.e. trade memory and copying for being able to BLAS-it. Else one could check in the einsum_bmm call but preferably it would be deferred to `matmul` I suppose.\r\n","> TBH, I had hoped @dgasmith can give a recommendation. The timings do look like this should work pretty generically, although its easy to miss cases, I guess.\r\n\r\nIt is hard for me to say, we benchmarked fairly throughly with `optimize=True`; however, the deployed result had a large number of edge cases that we missed. That being said, localizing the code to `optimize=True` is a good move to let everyone switch on\/off performance. ","Rather than wait on progress in https:\/\/github.com\/numpy\/numpy\/issues\/23588 would it be reasonable to simply require contiguity before the matmul in the `bmm_einsum` function here? \r\n\r\nI.e. do we expect there to be cases where the arrays are not contiguous, and we expect it to be faster to leave them so?\r\n\r\nMy guess is that this is only the case for certain small contractions that are not the focus of when people use `optimize=True`. \r\n\r\nOf course having some rough heuristic in `matmul` itself would be cleaner."],"labels":["01 - Enhancement"]},{"title":"ENH: Support new-style custom dtypes in the buffer protocol","body":"### Proposed new feature or change:\n\nCurrently the buffer protocol is supported only for a limited number of legacy dtypes (see this [switch statement](https:\/\/github.com\/numpy\/numpy\/blob\/3e77f904a9cc82e03d20824425e30edcd8088484\/numpy\/core\/src\/multiarray\/buffer.c#L362)).\r\n\r\nIn principle it could be possible to encode dtype information *in* the format string and simply export the buffer as a `void*` buffer, leaving it up to consumers to parse the exported bytes using the format string. Consumers of the format strings (e.g. cython or any other downstream code working with numpy's buffers) would then need to add support for new dtypes, but at least they'd have a path forward.\r\n\r\nSee the comment from @eric-wieser https:\/\/github.com\/numpy\/numpy\/issues\/4983#issuecomment-573121067 for more details on how this might work. In principle this is complementary to fixing support for datetimes, so we could also close https:\/\/github.com\/numpy\/numpy\/issues\/4983.\r\n\r\nSee also the discussion about this in the context of legacy custom dtypes: https:\/\/github.com\/numpy\/numpy\/issues\/18442.\r\n\r\nIt would probably also be worth checking for any prior discussion about this in CPython's discussion channels or any relevant PEPs.","comments":["I saw this this morning and tried a bit.  The thing to note is that Python's `memoryview` doesn't care at all about what is in the format string, so it is OK to put arbitrary stuff inside.  Now, I would want a tentative OK from upstream, but I suspect that can be arranged.\r\n\r\nThe diff below, simply encodes the dtype as `[numpy.dtypes#DateTime64#s]` (`module, name, *args`.  Ignore the `#` a `,` is probably nicer.  It ends up calling a simple protocol: `numpy.dtypes.DateTime64._dtype_from_buffer_fmt(*args, byteorder=\"=\")`.  That could be short-circuit.  I didn't import the module here, but if there are no security concerns with just importing (we are not calling _arbitrary_ code after all like pickle) we could do that (but we would trust dtype authors to implement things in a way that doesn't call arbitrary code).\r\n\r\nThis works, in the sense that `np.array(memoryview(datetime_arr))` round-trips.  Of course item access fails with:\r\n```\r\nNotImplementedError: memoryview: unsupported format [numpy.dtypes#DateTime64#s]\r\n``` \r\nwhich is just the same as if the format character would be `e`.\r\n\r\n<details>\r\n\r\n<summary> Proof of concept diff <\/summary>\r\n\r\n```diff\r\ndiff --git a\/numpy\/core\/_internal.py b\/numpy\/core\/_internal.py\r\nindex c78385880..786ad833e 100644\r\n--- a\/numpy\/core\/_internal.py\r\n+++ b\/numpy\/core\/_internal.py\r\n@@ -655,6 +655,8 @@ def _dtype_from_pep3118(spec):\r\n     return dtype\r\n \r\n def __dtype_from_pep3118(stream, is_subdtype):\r\n+    import numpy.dtypes\r\n+\r\n     field_spec = dict(\r\n         names=[],\r\n         formats=[],\r\n@@ -704,7 +706,15 @@ def __dtype_from_pep3118(stream, is_subdtype):\r\n         # Data types\r\n         is_padding = False\r\n \r\n-        if stream.consume('T{'):\r\n+        if stream.consume(\"[\"):\r\n+            identifier = stream.consume_until(\"]\")\r\n+            module, name, *args = identifier.split(\"#\")\r\n+            identifier = getattr(sys.modules[module], name)\r\n+            byteorder = stream.byteorder if stream.byteorder != \"@\" else \"=\"\r\n+            value = identifier._dtype_from_buffer_fmt(\r\n+                    *args, byteorder=byteorder)\r\n+            align = value.alignment\r\n+        elif stream.consume('T{'):\r\n             value, align = __dtype_from_pep3118(\r\n                 stream, is_subdtype=True)\r\n         elif stream.next in type_map_chars:\r\ndiff --git a\/numpy\/core\/src\/multiarray\/buffer.c b\/numpy\/core\/src\/multiarray\/buffer.c\r\nindex c9d881e16..efb8dd7f0 100644\r\n--- a\/numpy\/core\/src\/multiarray\/buffer.c\r\n+++ b\/numpy\/core\/src\/multiarray\/buffer.c\r\n@@ -393,7 +393,7 @@ _buffer_format_string(PyArray_Descr *descr, _tmp_string_t *str,\r\n         case NPY_CFLOAT:       if (_append_str(str, \"Zf\") < 0) return -1; break;\r\n         case NPY_CDOUBLE:      if (_append_str(str, \"Zd\") < 0) return -1; break;\r\n         case NPY_CLONGDOUBLE:  if (_append_str(str, \"Zg\") < 0) return -1; break;\r\n-        \/* XXX NPY_DATETIME *\/\r\n+        case NPY_DATETIME:     if (_append_str(str, \"[numpy.dtypes#DateTime64#s]\") < 0) return -1; break;\r\n         \/* XXX NPY_TIMEDELTA *\/\r\n         case NPY_OBJECT:       if (_append_char(str, 'O') < 0) return -1; break;\r\n         case NPY_STRING: {\r\ndiff --git a\/numpy\/dtypes.py b\/numpy\/dtypes.py\r\nnew file mode 100644\r\nindex 000000000..817f2d395\r\n--- \/dev\/null\r\n+++ b\/numpy\/dtypes.py\r\n@@ -0,0 +1,6 @@\r\n+import numpy as np\r\n+\r\n+class DateTime64:\r\n+    @classmethod\r\n+    def _dtype_from_buffer_fmt(cls, unit, byteorder=\"=\"):\r\n+        return np.dtype(f\"{byteorder}M8[{unit}]\")\r\n```\r\n"],"labels":["01 - Enhancement","component: numpy._core","component: numpy.dtype"]},{"title":"BUG: ma.zeros_like sets mask","body":"```np.ma.zeros_like(np.ma.array([0,1],mask=[0,1]))```\r\nmasked_array(data=[0, --],\r\n             mask=[False,  True],\r\n       fill_value=999999)\r\n\r\nThis behaviour (copying the mask) is counterintuitive, dangerous (it's caught me out twice so far) and seems to be contrary to https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.ma.zeros_like.html\r\n","comments":["Hmm. It does seem counterintuitive, the mask should be initialized to False. Do any tests break if you change this?","Is the intended behaviour that the mask is not preserved? If so, then it appears that I can change `ma.zeros_like` so that it accepts a `keep_mask=False` parameter and all the unit tests still pass. ","Also, would the same behaviour be expected in `ma.ones_like`?","Yes, I forgot to mention that ones_like has the same problem","> This behaviour (copying the mask) is counterintuitive\r\n\r\nI'm sure there are users out there who depend on this behavior though. I don't think it's particularly counterintuitive that masks are copied over when creating new MaskedArrays from existing MaskedArrays. This behavior should be documented though.","I bet there are a similar number who (as happened to me) think they\u2019re getting a \u2018clean\u2019 ma of zeros, and I wonder how many of them haven\u2019t spotted the error and are still using buggy code.","Also true - my point was more from the perspective of backwards compatibility. I wouldn't necessarily characterize the fact `ma.zeros_like` preserves the input mask (when the input is a masked array) as a bug. Therefore, we should be wary of changing the default behavior so we don't break people who depend on it. ","For what it is worth, I would have expected the input mask to be preserved. ","The reason it trips me up is that I nearly always use ma.zeros_like for the same use case - I want to generate the mean of a sequence (too long to store internally) of 2d ma's by summing and divide each sum by its count:\r\n\r\n```\r\nsums = np.ma.zeros((x,y))\r\nns = np.zeros((x,y),int)\r\nfor a in data:\r\n    good = ~a.mask\r\n    sums[good] += a[good]\r\n    ns[good] += 1\r\n\r\nsums[ns == 0] = np.ma.masked\r\nsums \/= ns\r\n```\r\n\r\nSometimes I forget and replace the first line with np.ma.zeros_like(a) (since I'm thinking of it like a non-masked array, just using the mask as a convenience to avoid 'divide by zero'), but this masks the sum if a is masked, silently wrecking my data. I suspect I'm not the only one to do it in this way, though I'm sure there's a cleverer way. Perhaps a warning could be issued if the argument is masked?","This also happens with `np.zeros_like` as well.\r\n\r\n```\r\n>>>np.zeros_like(np.ma.array([0,1],mask=[0,1]))\r\nmasked_array(data=[0, --],\r\nmask=[False, True],\r\nfill_value=999999)\r\n```\r\nLooking under the hood, it seems that the only real difference between `np.ma.zeros_like` and `np.zeros_like` is that `np.ma.zeros_like` always outputs a masked array. However, that's fairly easy to do with `np.zeros_like` as well.\r\n\r\n```\r\n>>>np.zeros_like(np.array([0,1])).view(np.ma.MaskedArray)\r\nmasked_array(data=[0, 0],\r\n             mask=False,\r\n       fill_value=999999)\r\n```\r\nIn my eyes, that's a very small change to justify having a MaskedArray version of `zeros_like`. I think it'd be a good idea to change `np.ma.zeros_like` so its behaviour meaningfully differs from `np.zeros_like`. Alternatively, if it's decided that `np.ma.zeros_like` should keep its current behaviour for backwards compatibility, then it might be a good idea to rewrite the function as something like\r\n\r\n```\r\ndef zeros_like(*args, **kwargs):\r\n    \"\"\"Documentation goes here\"\"\"\r\n    y = np.zeros_like(*args, **kwargs).view(MaskedArray)\r\n    return y\r\n```\r\nThat way, it would be easier to clean up and maintain the documentation, which is currently lacking."],"labels":["00 - Bug","component: numpy.ma"]},{"title":"BUG: np.arange produces wrong output for float16","body":"### Describe the issue:\r\n\r\nUsing `np.arange` to produce a float16 array with elements in reverse order returns an in correct output in certain cases. \r\n\r\n### Reproduce the code example:\r\n\r\n```python\r\n>>> np.arange(2048, -1, -1, dtype='e')\r\narray([2.048e+03, 2.047e+03, 2.046e+03, ..., 2.000e+00, 1.000e+00,\r\n       0.000e+00], dtype=float16)\r\n>>> np.arange(2049, -1, -1, dtype='e')\r\narray([2048., 2048., 2048., ..., 2048., 2048., 2048.], dtype=float16)\r\n```\r\n\r\n\r\n### Error message:\r\n\r\n_No response_\r\n\r\n### Runtime information:\r\n\r\n```\r\n>>> np.__version__\r\n'1.24.0'\r\n\r\n```\r\n\r\n```\r\n>>> print(np.show_runtime())\r\n[{'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3'],\r\n                      'found': ['SSSE3',\r\n                                'SSE41',\r\n                                'POPCNT',\r\n                                'SSE42',\r\n                                'AVX',\r\n                                'F16C',\r\n                                'FMA3',\r\n                                'AVX2',\r\n                                'AVX512F',\r\n                                'AVX512CD',\r\n                                'AVX512_SKX'],\r\n                      'not_found': ['AVX512_KNL',\r\n                                    'AVX512_KNM',\r\n                                    'AVX512_CLX',\r\n                                    'AVX512_CNL',\r\n                                    'AVX512_ICL']}},\r\n {'architecture': 'SkylakeX',\r\n  'filepath': '\/home\/raghuveer\/anaconda3\/envs\/np-py39\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so',\r\n  'internal_api': 'openblas',\r\n  'num_threads': 20,\r\n  'prefix': 'libopenblas',\r\n  'threading_layer': 'pthreads',\r\n  'user_api': 'blas',\r\n  'version': '0.3.21'}]\r\nNone\r\n\r\n```\r\n\r\n### Context for the issue:\r\n\r\n_No response_","comments":["This is due to precision limitations in float16. Past `2048.` we can only represent numbers in intervals of 2 https:\/\/en.wikipedia.org\/wiki\/Half-precision_floating-point_format#Precision_limitations. So `2049.` will be rounded to `2048.` In the implementation of arange the step is calculate by comparing the first value `2049.` with the next value of the array `2048.` but when they are converted to float16 they will both be `2048.` and `2048.` so the step will be `0.` hence the array will be filled with `2048.`. The length of the array is calculated before it is filled so will work as expected. ","Yet another reason to only ever use `arange` with integer types","The reason I noticed this is because `np.arange` is used to generate data for benchmarking sort: https:\/\/github.com\/numpy\/numpy\/blob\/7f1ce595cee3df09be57abde68e14688516dbe04\/benchmarks\/benchmarks\/bench_function_base.py#L148-L172 and because it produces the wrong data the benchmarking numbers for float16 aren't accurate. ","@r-devulap Would be happy for a fix to that.","Is there a point in fixing `np.arange` for float16? Or as @mattip says, we should really stick to integers when using `np.arange`?"],"labels":["00 - Bug"]}]
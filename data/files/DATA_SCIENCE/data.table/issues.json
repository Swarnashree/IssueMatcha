[{"title":"Enhance Error Message for using `:=` or `let` in Non-data.table-aware Environment","body":"**Description:**\r\n\r\nThis PR addresses issue #5721. The current error message generated when `:=` is used directly in an environment not aware of data.table (`cedta()` returns `FALSE`) can be misleading. This PR enhances the error message to provide more specific guidance to the user.\r\n\r\n**Changes Made:**\r\n- Added a check in the `[.data.table` function to detect the presence of `:=` or `let` in the `j` argument.\r\n- If `:=` or `let` is detected, a more specific error message is thrown, advising the user to ensure that data.table is imported and used explicitly in their package.\r\n\r\n**Proposed Error Message:**\r\n\r\n```\r\n\"[ was called on a data.table in an environment that is not data.table-aware (i.e. cedta()), but ':=' or 'let' was used, implying the owner of this call really intended for data.table methods to be called. See vignette('datatable-importing') for details on properly importing data.table.\"\r\n```\r\n\r\n\r\nCloses #5721\r\n","comments":["## [Codecov](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\nAttention: Patch coverage is `50.00000%` with `1 lines` in your changes are missing coverage. Please review.\n> Project coverage is 97.52%. Comparing base [(`b4ae3ef`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/commit\/b4ae3ef6b2a043b12092d822bd999201cc5330d5?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) to head [(`e237b3e`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?dropdown=coverage&src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n\n| [Files](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?dropdown=coverage&src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) | Patch % | Lines |\n|---|---|---|\n| [R\/data.table.R](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-Ui9kYXRhLnRhYmxlLlI=) | 50.00% | [1 Missing :warning: ](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) |\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #6019      +\/-   ##\n==========================================\n- Coverage   97.53%   97.52%   -0.01%     \n==========================================\n  Files          80       80              \n  Lines       14913    14915       +2     \n==========================================\n+ Hits        14545    14546       +1     \n- Misses        368      369       +1     \n```\n\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6019?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n","Can someone suggest, how to write tests for this error ? , this error only occurs in not data.table aware enviroment(cedta() == false) , how to mimic such environment in tests, I have tried searching for it , wasn't able to find anything helpful."],"labels":["help-wanted"]},{"title":"Introduce Source Hint in colnamesInt Function for Error Clarity","body":"**Summary:**\r\nAdded an optional `source` argument to the `colnamesInt` function to provide a hint about the source of errors.\r\n\r\n**Details:**\r\n- Added a new argument `source` to the `colnamesInt` function.\r\n- Used the `source` argument to provide hints about the source of errors.\r\n- This enhancement improves the clarity of error messages, making it easier for users to identify the source of errors.\r\n- Added the source wherever the function `colnamesInt` was called.\r\n- Writing test for the testing the accuracy of the updated error messages in `tests.Rraw`.\r\n\r\nThis PR Closes #5039 ","comments":["## [Codecov](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6008?dropdown=coverage&src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Project coverage is 97.50%. Comparing base [(`54f9048`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/commit\/54f904831cafc37ba841958fbdbc614b8baac4c8?dropdown=coverage&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) to head [(`c2fefb5`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6008?dropdown=coverage&src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n> Report is 1 commits behind head on master.\n\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #6008   +\/-   ##\n=======================================\n  Coverage   97.50%   97.50%           \n=======================================\n  Files          80       80           \n  Lines       14884    14884           \n=======================================\n  Hits        14513    14513           \n  Misses        371      371           \n```\n\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/6008?dropdown=coverage&src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n","The error messages should be meaningful. Adding the line number where the call took place provides no such information to the user and has only drawbacks over using traces, e.g. line numbers need to be updated every time code is added.\r\n\r\nI converted it back to a draft.","@MichaelChirico Is these changes in source Information enough or their is need for more context .","@MichaelChirico @ben-schwen @jangorecki  @HughParsonage can someone review this PR please and suggest if there is need for further changes :)","I guess the tricky part here are the nested calls where the function is not directly called by the user itself but happening as a subcall, e.g. as with `setdiff_` and `.duplicated.helper`, so I'm not sure about the source messages there.\r\n\r\nDefinitely already an improvement but still not perfect.","@jangorecki WDYT about `colnamesInt()` returning `NULL`, or negative integers, instead of erroring itself? Then let caller handle the error. Might be easier than trying to jam everything into this `source=` argument.\r\n\r\nAlternatively, we could have `colnamesInt()` signal a custom error class, and caller can `tryCatch()` its way to giving a more helpful error. WDYT?","NULL and negatives sounds good. We just need to differentiate each of the errors raised by it, so couple negative numbers.","> @jangorecki WDYT about `colnamesInt()` returning `NULL`, or negative integers, instead of erroring itself? Then let caller handle the error. Might be easier than trying to jam everything into this `source=` argument.\r\n> \r\n> Alternatively, we could have `colnamesInt()` signal a custom error class, and caller can `tryCatch()` its way to giving a more helpful error. WDYT?\r\n\r\nIt seems that this PR might not be suitable for addressing the issue, so should i close this PR and work on the alternate approach ? Additionally, which alternate approach is preferable \"letting caller handle the error directly\" or \" creating custom error class\" ."],"labels":["enhancement"]},{"title":"Remove exported [.data.table","body":"As noted this was for `DT()`; we can (possibly) restore the export if we restore `DT()`. For now, I find it strange to export such a method.","comments":["This breaks related tests:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/475115fd00e82008ff739406b0e4efe133c8b73e\/inst\/tests\/tests.Rraw#L17653-L17707\r\n\r\nShould we:\r\n\r\n 1. Comment out those tests\r\n 2. Add `` `[.data.table` = data.table:::`[.data.table` `` alias at the top of the suite","Why do we need to export it anyway? Wasn't there issue asking for that, which has been closer when that was added? Not precisely related to DT()","I haven't looked into it, presumably git blame will tell us some more history","Here's the relevant commit, presumably there's a smarter way to construct the call:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/pull\/5176\/files"],"labels":["breaking-change"]},{"title":"Run GHA on Windows, drop Appveyor test","body":"Now we have 3 different apps for CI: GHA, Appveyor, and GLCI (this is down from 4 since we dropped Travis).\r\n\r\nAIUI, we only need Appveyor to run CI on Windows, but GHA supports this. So we should add a Windows test to GHA and drop Appveyor for simplicity.","comments":["Windows is tested in GLCI so we could just drop appveyor.","Well if GHA supports Windows I'm in favor of using it to build it in our CI. Should we build it on R-current or R-devel? Will file the PR later","> Windows is tested in GLCI so we could just drop appveyor.\r\n\r\nhmm but we still need PRs CI to fail if they're not working on windows, otherwise I would've merged #5995 prematurely. is it possible for GLCI to post a GitHub comment if it fails?","yes it is possible to post comments @Anirban166 is working on that for #4687 ","But AFAIU GLCI is only run once nightly on master?","Yes GLCI run once a day, then it make sense to have some win job here then. GLCI does not fail as a GLCI, it builds the site and publish results of failed tests jobs. So the individual test jobs would have to report back to GH.","> Yes GLCI run once a day, then it make sense to have some win job here then. GLCI does not fail as a GLCI, it builds the site and publish results of failed tests jobs. So the individual test jobs would have to report back to GH.\r\n\r\nYes and if we merge multiple PRs on a single day then the report will always end up in the latest merged PR or we would need to run a second time."],"labels":["ci"]},{"title":"dcast() retains key incorrectly when run on .SD and the RHS contains a level matching the input key","body":"Seen while writing #5992.\r\n\r\nI'm not sure I've understood the root issue here, but this looks wrong:\r\n\r\n```r\r\ninput = CJ(a = letters, b = letters, c= letters)[, V := rnorm(.N)][, mean(V), by = .(a, b)]\r\nkey(input[, dcast(.SD, a ~ b, value.var = \"V1\")])\r\n# [1] \"a\" \"b\"\r\nkey(dcast(input, a ~ b, value.var = \"V1\"))\r\n# [1] \"a\"\r\n```\r\n\r\nThe key should be the same in both cases.","comments":["Seems also related to #4888 since output has multiple columns named `a`\r\n"],"labels":["bug"]},{"title":"dcast() should use GForce for fun.aggregate in some cases","body":"Compare:\r\n\r\n```r\r\nDT = CJ(a = letters, b = letters, c = letters, d = letters, e = letters)\r\nDT[, V := rnorm(.N)]\r\n\r\nsystem.time(a1 <- DT[, mean(V), by = .(a, b, c, d)][, dcast(.SD, a + b + c ~ d, value.var = \"V1\")])\r\n#    user  system elapsed \r\n#   1.050   0.040   0.857 \r\nsystem.time(a2 <- dcast(DT, a + b + c ~ d, value.var = \"V\", fun.aggregate = mean))\r\n#    user  system elapsed \r\n#   3.270   0.012   2.807 \r\n\r\n# (fix bug)\r\nsetkey(a1, a, b, c)\r\nall.equal(a1, a2)\r\n# [1] TRUE\r\n```\r\n\r\nThe former is faster because it uses GForce, but is much harder to grok \/ requires a strong understanding of {data.table} internals to come up with.\r\n\r\nThe latter is slower but may be more natural to write.\r\n\r\nIs it possible to invoke GForce outside `[` for cases like this?","comments":["Possible yes, but I think it would be better to nudge the user with a verbose message towards the faster query for the \"easy cases\" we are aware of.","Just saw the implementation is already to use `[` under the hood:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/40afa84c37063a17ec8a36e9eefa3507745d9923\/R\/fcast.R#L171\r\n\r\nSo it's just a matter of `[` seeing the actual call instead of `eval`. Can `env=` help here?"],"labels":["enhancement","performance","GForce","reshape"]},{"title":"Need examples in vignette on how the new programming interface should be used within a function. ","body":"The current \"Programming on data.table\" vignette does not provide any examples on how the new programming interface should be used within a function when passing multiple data variables to a function is required. \r\n\r\nBy looking at the examples in a recent article by [John MacKintosh](https:\/\/johnmackintosh.net\/blog\/2024-02-05-dt-programming\/), it seems that `eval` \/ `substitute` still needs to be used multiple times, which is not so different to the version before, while `eval` is suggested as a retired interface now.\r\n\r\n```\r\nstarwars_dt <- setDT(copy(starwars))\r\n\r\nmy_summarise_dt <- function(.dt, ...) {\r\n\r\n  vars <-  eval(substitute(alist(...)),\r\n                envir = parent.frame())\r\n\r\n  .dt[, lapply(.SD, mean, na.rm = TRUE),\r\n      .SDcols =c(\"mass\", \"height\"),\r\n      by = vars,\r\n      env = list(vars = substitute(vars))][]\r\n}\r\n```\r\n\r\nIs this the intended usage of the new programing interface?\r\n\r\nCould more examples be provided in the vignette, especially on how it should be used within a function?","comments":["Did you look into manual that was linked from vignette? Example is already there from the beginning.\r\n?substitute2\r\nExample uses base R.\r\n\r\nI feel this vignette is already too long and covers various cases from very simple to more difficult ones. I don't think we should try to put all to a vignette. Manual is better place for that.\r\n\r\nYour example lacks the use of function you defined. Also I would advise to use stackoverflow to questions on usage - other people may be looking for the same question in future.","> Did you look into manual that was linked from vignette? Example is already there from the beginning. ?substitute2 Example uses base R.\r\n> \r\n> I feel this vignette is already too long and covers various cases from very simple to more difficult ones. I don't think we should try to put all to a vignette. Manual is better place for that.\r\n> \r\n> Your example lacks the use of function you defined. Also I would advise to use stackoverflow to questions on usage - other people may be looking for the same question in future.\r\n\r\nThe most questions on stackoverflow in the past around programming on `data.table` are how to wrap it in a function, and pass variables as function arguments. This already can be achieved by using `substitute`\/`eval`. \r\n\r\nHowever by looking at the \"new programming interface\", I am confused what's new about it, and how's it different to the old way. \r\n\r\nThe programming interface from `dplyr` is easier to pass variable name as function argument like below. \r\n\r\n```\r\nvar_summary <- function(data, var) {\r\n  data %>%\r\n    summarise(n = n(), min = min({{ var }}), max = max({{ var }}))\r\n}\r\nmtcars %>% \r\n  group_by(cyl) %>% \r\n  var_summary(mpg)\r\n```\r\n\r\nI was then wondering whether `data.table`'s new interface make this easier as well?\r\n\r\nIn the past, I can just do `deparse`\/`substitute`\/`eval` to pass the variables from the data to the function argument\r\n\r\n```\r\noverlay <- function(lng, lat, data, basemap) {\r\n\r\n    lng_name <- deparse(substitute(lng))\r\n    lat_name <- deparse(substitute(lat))\r\n    data_name <- deparse(substitute(data))\r\n\r\n    geo <- substitute(list(lng, lat))\r\n    n_site <- dim(data)[1]\r\n    boundary_name <- grep(\"CODE|MAIN\", toupper(names(basemap)), value = TRUE)\r\n    site_over_map <- data[, eval(geo)] %>%\r\n        .[,  eval(boundary_name) := as.character(NA)]\r\n\r\n    coord <- st_as_sf(site_over_map[, 1:2], coords = 1:2, crs = st_crs(basemap))\r\n    overlay_ <- st_intersects(coord, basemap)\r\n    empty2NA_ <- sapply(overlay_, function(x) ifelse(length(x) == 0, NA, x))\r\n    code <- sapply(empty2NA_, function(x) {\r\n        ifelse(is.na(x), NA, basemap[[boundary_name]][x])\r\n    })\r\n    site_over_map[, 3] <- code\r\n\r\n    return(site_over_map)\r\n}\r\n```\r\nHowever, from other's explorations, the new interface is not too different from what can be done previously. \r\n\r\nTherefore, I was asking how the new programming interface could be used when `data.base` is to be used in a function, while function argument is variable names, whether such examples could be provided in the vignette so that people could know how to use them correctly. \r\n\r\nThe example in the original post is to pass several variable names as a function argument that to be used in `by`, this is also quite common in my user case. However, it still requires to use `eval` and `substitute` twice. \r\n ","If your function takes symbols which then need to pass downstream as symbols then you have to use `substitute`. This is not related to data.table but to Standard Evaluation rules in base R.\r\n\r\nUse of Standard Evaluation rather than NSE in the new `env` argument was design decision of mine. Therefore you cannot expect `env` argument to get symbols that are providing its value. For the same reason `.()` will not work as alias to `list()` in `env` arg as it would require NSE as well.\r\n\r\nBased on my experience I believe using NSE for meta-programming interface is a bad design decision.\r\nIf there is a consensus on adding NSE to `env` (because `.()` came up already) I fine about it, but I won't be codeowner of this part anymore.\r\n\r\nNevertheless it does not mean there is no easier way to solve your problem. There may be, haven't really looked into it. Maybe it will be easier for you to solve it just in base R and then apply on your data.table based function.","This is one basic use case which presents weakness (even it behaves consistently - still) of your example\r\n```r\r\ndata.frame(cyl=1, var=2, mpg=3) %>% \r\n  group_by(cyl) %>% \r\n  var_summary(mpg)\r\n```\r\nusing `env` we avoid this confusion\r\n```r\r\nvar_summary <- function(data, var) {\r\n  data[, .(n = .N, min=min(.var), max=max(.var)), by=cyl, env=list(.var=substitute(var))]\r\n}\r\n```\r\neven if we don't prefix `var` with a dot to `.var`\r\n```r\r\ndata[, .(n = .N, min=min(var), max=max(var)), by=cyl, env=list(var=substitute(var))]\r\n```\r\nit will still be consistent because `env` defines evaluation environment, therefore it is masking anything inside. Also because of that we know that in case of\r\n```r\r\ndata.frame(cyl=1, var=2, mpg=3)\r\n```\r\nwe will not end up using `var` column, where on the contrary using the `{{` expression nesting, we end up using code which is counter-intuitive, where R developers (not to be confused with tidyverse developers) may expect completely different output."],"labels":["programming"]},{"title":"Extend dcast() to support different fill= values for different columns","body":"As raised in #5980, this would bring us closer to parity with [`tidyr::pivot_wider()`](https:\/\/tidyr.tidyverse.org\/reference\/pivot_wider.html)'s `values_fill` argument. This seems especially useful for `length(value.var) > 1L` use cases, when the different `value.var` can quite reasonably need different `fill=` values.\r\n\r\nWe are free to choose our own way for the extended argument to work, but the `pivot_wider()` approach seems clear enough: when `fill` is a named list, the names are columns and the elements are values to be filled. I'm not sure if {tidyr} supports a default unnamed entry so that you could do, e.g. `fill = list(special = 1, 0)` so that most columns are filled with `0`, except for one `special` column. The only trouble is what to do with `fill = list(0)`; I think we need to interpret that as \"fill with the length-1 list with entry `0`\", not as \"`0` by default for all columns\".\r\n\r\nPS I also wonder if it wouldn't be easier to just re-use `setnafill()` to handle the `fill=` argument of `dcast()`, rather than the current logic in C which essentially duplicates that.","comments":[],"labels":["reshape"]},{"title":"Fatally failed CI jobs may not be listed on CI summary","body":"As explained in\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/5979\r\n\r\nWe need integration stage in CI to know the full list and fill up with failures those jobs that does not provide any artifacts (have fatal failure).\r\n\r\nhttps:\/\/rdatatable.gitlab.io\/data.table\/web\/checks\/check_results_data.table.html","comments":[],"labels":["ci"]},{"title":"[FR] Run clang-format on C files","body":"I'm reading through `rbindlist.c` and there are several lines like \r\n\r\n```c\r\n    for (int j=0; j<thisncol; ++j) { int tt=length(VECTOR_ELT(li,j)); if (tt>maxLen) { maxLen=tt; whichMax=j; } }\r\n```\r\n\r\nIt would be nice to run [clang-format](https:\/\/clang.llvm.org\/docs\/ClangFormat.html) over all of the c files in `src` just to make these lines more easily parsable.\r\n\r\nEigen and the Stan math have done this and there's a lot of discussion around the pros and cons. The main con being that you have one hard break in your git, but that can be resolved by adding a `.git-blame-ignore-revs` file that ignores the reformatting. A [`.clang-format`](https:\/\/github.com\/stan-dev\/math\/blob\/develop\/.clang-format) file can be included to modify any of the defaults as the team wishes \r\n","comments":["I have no vote here but consider this a strong _against_ not the _principle_ of a code formatter but the _default style_ of `clang-format`.   It a nutshell, it is very 'narrow and vertical' and a very strong departure of the style used in for example base R and here which tends to be more 'horizontal'.  It would be nice to agree a desired format before point-blank jumping into the fury of reformating a code base.  FWIW the only 'project in the vicinity' that I am aware of which actually defines a custom `clang-format` input file is [duckdb in this file](https:\/\/github.com\/duckdb\/duckdb\/blob\/main\/.clang-format).","Definitely +1 against the default. The full set of formatting options for clang-format are at the link below. Eigen for example ([link](https:\/\/gitlab.com\/libeigen\/eigen\/-\/blob\/master\/.clang-format?ref_type=heads)) uses 120 width columns instead of the default 80. \r\n\r\nhttps:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html","So does duckdb.  I would love to see a decent gallery of effects of different settings.  Do you know of one?\r\n","I'm definitely in favor of consistency, but agree with Dirk that using defaults is not likely to suit our codebase.\r\n\r\nThe way to go about this IMO is to incrementally enable rules -- this will likely require a dedicated contributor willing to spend time building such PRs. Rough outline:\r\n\r\n 1. Demonstrate the effect of a rule on a representative file, then maintainers can comment on whether it's suitable\r\n 2. Apply the rule globally, possibly rolling back if some cases are exposed that go against the initial discussion\r\n 3. Enable the rule in our .clang-format file & make sure CI will error on incremental PRs that violate the rule\r\n 4. Repeat. Steps 1&2 might be combined for rules that only impact a small number of lines.\r\n\r\nThat said, this will not be a priority until we can burn down some of the existing PR queue -- as Jan has emphasized a number of times, low-impact changes that create merge conflicts with existing high-priority PRs will just create maintainer toil that we'd rather avoid.","> I'm definitely in favor of consistency, but agree with Dirk that using defaults is not likely to suit our codebase.\r\n\r\nI also agree\r\n\r\n> The way to go about this IMO is to incrementally enable rules -- this will likely require a dedicated contributor willing to spend time building such PRs. Rough outline:\r\n\r\nThis feels like a lot of effort (there are like a 100 rules). For Stan we just turned off all the rules, skimmed the list of rules we thought would be good and added them. Adding a linter is a lot about agreeing on a style and you really need a main contributor to know what the team wants and to get agreement\r\n\r\n> That said, this will not be a priority until we can burn down some of the existing PR queue -- as Jan has emphasized a number of times, low-impact changes that create merge conflicts with existing high-priority PRs will just create maintainer toil that we'd rather avoid.\r\n\r\nYes I think waiting till after a major release would be a good idea. It's nice for future contributors at the pain of current PRs","Did you just conflate \/ confuse a discussion about `clang-format` (aka \"bad enough as it is with its default\") with the R linter whose defaults are truly [..... redacted on advise of counsel ...]  ?","Noting here some rules we should definitely apply when we get around to it, please feel free to edit this comment\r\n\r\n - `return(.);` --> `return .;`","[alignescapednewlines](https:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html#alignescapednewlines): I like `ENAS_Left` but `ENAS_Right` is also fine\r\n[AllowShortBlocksOnASingleLine](https:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html#allowshortblocksonasingleline) Setting all the `AllowShort*` to `SBS_Always` let's you keep one liners simple\r\n[ColumnLimit](https:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html#columnlimit): Eigen uses 120 which is nice, though personally I prefer 100. I use a horizontal screen and 120 can run over a bit with my normal font size\r\n[InsertBraces](https:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html#insertbraces): For the initial PR this should be run once separately as it can lead to malformed code\r\n[Pointer\/Qualifier\/ReferenceAlignment](https:\/\/clang.llvm.org\/docs\/ClangFormatStyleOptions.html#pointeralignment): I like `Left` personally but this one is up to `data.table`'s style\r\n","@SteveBronder That is helpful but you only show _where_ you would pull changes from but now _how_.  Can you add 'best guesses' or preferences for the _suggested values_ of these?   "],"labels":["internals"]},{"title":"revdep neonPlantEcology example\/vignette failures with new dcast coercion","body":"looks like #4586 caused a revdep to start failing checks:\r\n```\r\n* checking examples ... ERROR\r\nRunning examples in 'neonPlantEcology-Ex.R' failed\r\nThe error most likely occurred in:\r\n\r\n> base::assign(\".ptime\", proc.time(), pos = \"CheckExEnv\")\r\n> ### Name: npe_summary\r\n> ### Title: Get plant biodiversity information for NEON plots\r\n> ### Aliases: npe_summary\r\n> \r\n> ### ** Examples\r\n> \r\n> data(\"D14\")\r\n> plot_level <- neonPlantEcology::npe_summary(neon_div_object = D14, scale = \"plot\")\r\nLoading required namespace: vegan\r\nError in dcast.data.table(`_DT14`[, .(site, plotID, subplotID, eventID,  : \r\n  'x' is not atomic\r\nCalls: <Anonymous> ... as_tibble -> dt_eval -> eval_tidy -> dcast -> dcast.data.table\r\nExecution halted\r\n* checking for unstated dependencies in vignettes ... OK\r\n* checking package vignettes in 'inst\/doc' ... OK\r\n* checking running R code from vignettes ...\r\n  'neonPlantEcology.Rmd' using 'UTF-8'... OK\r\n  'using_npe.Rmd' using 'UTF-8'... OK\r\n NONE\r\n* checking re-building of vignette outputs ... ERROR\r\nError(s) in re-building vignettes:\r\n  ...\r\n--- re-building 'neonPlantEcology.Rmd' using rmarkdown\r\n\r\nQuitting from lines 107-122 [nspp] (neonPlantEcology.Rmd)\r\nError: processing vignette 'neonPlantEcology.Rmd' failed with diagnostics:\r\n'x' is not atomic\r\n--- failed re-building 'neonPlantEcology.Rmd'\r\n\r\n--- re-building 'using_npe.Rmd' using rmarkdown\r\n\r\nQuitting from lines 107-122 [nspp] (using_npe.Rmd)\r\nError: processing vignette 'using_npe.Rmd' failed with diagnostics:\r\n'x' is not atomic\r\n--- failed re-building 'using_npe.Rmd'\r\n\r\nSUMMARY: processing the following files failed:\r\n  'neonPlantEcology.Rmd' 'using_npe.Rmd'\r\n\r\nError: Vignette re-building failed.\r\nExecution halted\r\n```","comments":["@MichaelChirico any idea why this may be happening?","It looks like here's an MRE:\r\n\r\n```r\r\nDT <- data.table(a = c(1,2,1), b = 1:3, c = 0)\r\ndcast(DT, a~b, value.var=\"c\", fill=list(0))\r\n\r\n## ON master\r\n# Error in dcast.data.table(DT, a ~ b, value.var = \"c\", fill = list(0)) : \r\n#   'x' is not atomic\r\n\r\n## ON 1.15.2\r\ndcast(DT, a~b, value.var=\"c\", fill=list(0))\r\n# Key: <a>\r\n#        a     1     2     3\r\n#    <num> <num> <num> <num>\r\n# 1:     1     0     0     0\r\n# 2:     2     0     0     0\r\n```\r\n\r\nFYI @hadley it _looks_ like this is coming from a {dtplyr} translation of `pivot_wider` providing `fill=list()` to the `dcast()` call, which is a bit unusual, I wonder if the translation is WAI? Sorry I haven't dived into this aspect any further.\r\n\r\nhttps:\/\/github.com\/admahood\/neonPlantEcology\/blob\/2dd814157e5804174ded6c022100b1903ca883c2\/R\/diversity_data_prep.R#L1438-L1447","@jangorecki should we (1) extend `coerceAs()` to behave more like `coerceVector()` for `VECSXP` input (2) special-case the handling of `!isVectorAtomic(thisfill)` or (3) make the breaking change not to allow `fill = list()` to get coerced as before?","I would have expected some kind of error message like \"value.var is numeric but fill is list, so please change fill type to match value.var type\"\r\n\r\nrelated: with data.table release it is OK for fill=list when value.var is VECSXP, see below\r\n```r\r\n> (dt=data.table(i=1:2, j=c(\"a\",\"b\"), L=list(4, 5:6), num=7:8))\r\n       i      j      L   num\r\n   <int> <char> <list> <int>\r\n1:     1      a      4     7\r\n2:     2      b    5,6     8\r\n> dcast(dt, i ~ j, value.var=\"L\")\r\nKey: <i>\r\n       i      a      b\r\n   <int> <list> <list>\r\n1:     1      4       \r\n2:     2           5,6\r\n> dcast(dt, i ~ j, value.var=\"L\", fill=NA)\r\nKey: <i>\r\n       i      a      b\r\n   <int> <list> <list>\r\n1:     1      4     NA\r\n2:     2     NA    5,6\r\n> dcast(dt, i ~ j, value.var=\"L\", fill=list(c(\"miss\",\"ing\"),\"value\"))\r\nKey: <i>\r\n       i        a        b\r\n   <int>   <list>   <list>\r\n1:     1        4 miss,ing\r\n2:     2 miss,ing      5,6\r\n```\r\n\r\nand it is ok for fill to be list with elements of length=1, see below\r\n```r\r\n> dcast(dt, i ~ j, value.var=\"num\")\r\nKey: <i>\r\n       i     a     b\r\n   <int> <int> <int>\r\n1:     1     7    NA\r\n2:     2    NA     8\r\n> dcast(dt, i ~ j, value.var=\"num\", fill=0)\r\nKey: <i>\r\n       i     a     b\r\n   <int> <int> <int>\r\n1:     1     7     0\r\n2:     2     0     8\r\n> dcast(dt, i ~ j, value.var=\"num\", fill=list(0,1))\r\nKey: <i>\r\n       i     a     b\r\n   <int> <int> <int>\r\n1:     1     7     0\r\n2:     2     0     8\r\n```\r\n\r\nbut not OK for list element to be a vector of length>1, see below.\r\n```r\r\n> dcast(dt, i ~ j, value.var=\"num\", fill=list(0:1))\r\nError in dcast.data.table(dt, i ~ j, value.var = \"num\", fill = list(0:1)) : \r\n  'list' object cannot be coerced to type 'integer'\r\n```\r\n\r\nalso in the above examples it seems that if fill=list with more than one element, only the first element is used, and the others are ignored, so I would have expected a warning in this case, like \"fill is a list with 2 elements, but only the first element is used, so please remove the other elements of fill\"","IINM all of the existing behavior is just inherited from base R coercion, c.f.\r\n\r\n```r\r\nas.integer(list(0))      # 0L\r\nas.integer(list(0,1))    # 0:1\r\nas.integer(list(0:1))    # 'cannot be coerced' error\r\nas.integer(list(0, 1:2)) # 'cannot be coerced' error\r\n```","> @jangorecki should we (1) extend `coerceAs()` to behave more like `coerceVector()` for `VECSXP` input (2) special-case the handling of `!isVectorAtomic(thisfill)` or (3) make the breaking change not to allow `fill = list()` to get coerced as before?\r\n\r\nIt wasn't in its original scope but could be added.","If it is regression (as per first post) then it qualifies for patch release.","> If it is regression (as per first post) then it qualifies for patch release.\r\n\r\nThis is still in dev, not any CRAN release","Sorry, I don't remember enough about this function or dtplyr to offer any useful feedback \ud83d\ude1e ","@TysonStanley from {tidyfast} I think you may be most familiar with {tidyr} <-> {data.table} translation, if you wouldn't mind checking how `pivot_wider()` is being translated to `dcast()` to see if there's any improvement we can recommend.","Yep, can take a closer look.","A relatively quick review suggests the list option isn't working in `dtplyr::pivot_wider()` as intended. The way pivot_wider() normally works is you can give the values_fill arg a named list that applies the values differentially to each named variable. However, in the current `dtplyr` it just uses the first value encountered.\r\n\r\n``` r\r\nlibrary(data.table)\r\nlibrary(dplyr)\r\n#> \r\n#> Attaching package: 'dplyr'\r\n#> The following objects are masked from 'package:data.table':\r\n#> \r\n#>     between, first, last\r\n#> The following objects are masked from 'package:stats':\r\n#> \r\n#>     filter, lag\r\n#> The following objects are masked from 'package:base':\r\n#> \r\n#>     intersect, setdiff, setequal, union\r\nlibrary(tidyr)\r\nlibrary(dtplyr)\r\nlibrary(tibble)\r\n\r\ndf <- lazy_dt(tibble(g = c(1, 2), var = c(\"x\", \"y\"), val = c(1, 2), val2 = c(1, 2)))\r\ndt = as.data.table(df)\r\n\r\n# shows where NAs are and what the call looks like\r\ndf %>%\r\n  pivot_wider(names_from = var, values_from = val)\r\n#> Source: local data table [2 x 4]\r\n#> Call:   dcast(`_DT1`, formula = g + val2 ~ var, value.var = \"val\")\r\n#> \r\n#>       g  val2     x     y\r\n#>   <dbl> <dbl> <dbl> <dbl>\r\n#> 1     1     1     1    NA\r\n#> 2     2     2    NA     2\r\n#> \r\n#> # Use as.data.table()\/as.data.frame()\/as_tibble() to access results\r\n\r\n# should fill NA values for val with 0 and val2 with 10 but fills both with 0\r\ndf %>%\r\n  pivot_wider(names_from = var, values_from = c(val, val2), values_fill = list(val = 0, val2 = 10))\r\n#> Source: local data table [2 x 5]\r\n#> Call:   dcast(`_DT1`, formula = g ~ var, value.var = c(\"val\", \"val2\"), \r\n#>     fill = list(val = 0, val2 = 10))\r\n#> \r\n#>       g val_x val_y val2_x val2_y\r\n#>   <dbl> <dbl> <dbl>  <dbl>  <dbl>\r\n#> 1     1     1     0      1      0\r\n#> 2     2     0     2      0      2\r\n#> \r\n#> # Use as.data.table()\/as.data.frame()\/as_tibble() to access results\r\n\r\n# does the same thing as above\r\ndf %>%\r\n  pivot_wider(names_from = var, values_from = c(val, val2), values_fill = 0)\r\n#> Source: local data table [2 x 5]\r\n#> Call:   dcast(`_DT1`, formula = g ~ var, value.var = c(\"val\", \"val2\"), \r\n#>     fill = 0)\r\n#> \r\n#>       g val_x val_y val2_x val2_y\r\n#>   <dbl> <dbl> <dbl>  <dbl>  <dbl>\r\n#> 1     1     1     0      1      0\r\n#> 2     2     0     2      0      2\r\n#> \r\n#> # Use as.data.table()\/as.data.frame()\/as_tibble() to access results\r\n\r\n# does the same thing as above\r\ndcast(dt, formula = g ~ var, value.var = c(\"val\", \"val2\"), fill = 0)\r\n#> Key: <g>\r\n#>        g val_x val_y val2_x val2_y\r\n#>    <num> <num> <num>  <num>  <num>\r\n#> 1:     1     1     0      1      0\r\n#> 2:     2     0     2      0      2\r\n```\r\n\r\n<sup>Created on 2024-03-10 with [reprex v2.1.0](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\nI assume we could add functionality for a list to work like `dplyr::pivot_wider()` likely somewhere around [here](https:\/\/github.com\/Rdatatable\/data.table\/blob\/15c127e99f8d6aab599c590d4aec346a850f1334\/src\/fcast.c#L20)\r\n\r\nCan also PR a fix in the documentation for `dtplyr` if we aren't going to add this functionality.","Thanks for the investigation! I wondered if that might be what's going on. So, probably {dtplyr} should for now warn if `length(values_fill) > 1L`, and actually send `values_fill[[1L]]` (hand-waving away the case where the target type is `list` where we need to be more careful), since {data.table} only supports one `fill=` value for the full output.\r\n\r\nThat said, offering support for multiple `fill=` values seems reasonable enough, though there's not been any request for that behavior thus far. Filed #5990 as an FR to that end."],"labels":["revdep"]},{"title":"R 3.2.0 job","body":"It seems that bump of stated dependency from 3.1 to 3.2 missed CI job upgrade. There is still 3.1 job which is currently failing. It needs to be updated to 3.2 job.","comments":["Oops! I swore I made that change somewhere \ud83e\udd14 editing now..."],"labels":["ci"]},{"title":"fread cleanup prints error message in webR","body":"data.table is working quite fine in webR, but when using fread an error occurs (_**System errno 8 unmapping file: Bad file descriptor**_). Interestingly, the file loads but it throws an error neverthless. Same error occurs also for .gz files. Perhaps, it's just the fact that this runs in browser using wasm is not recognized, resulting in an error.\r\n\r\n```r\r\ninstall.packages(\"data.table\")\r\nlibrary(\"data.table\")\r\n\r\ndt = data.table(1:10)\r\nfwrite(dt,\"dt.csv\")\r\nlist.files()\r\ndtcsv = fread(\"dt.csv\", verbose = T)\r\nprint(dtcsv)\r\n\r\nsessionInfo()\r\n```\r\n\r\nResulting output:\r\n\r\n`> install.packages(\"data.table\")\r\nDownloading webR package: data.table\r\n\r\n> library(\"data.table\")\r\ndata.table 1.14.7 IN DEVELOPMENT built 2023-12-19 09:51:47 UTC using 1 threads (see ?getDTthreads).  Latest news: r-datatable.com\r\n**********\r\nThis development version of data.table was built more than 4 weeks ago. Please update: data.table::update_dev_pkg()\r\n**********\r\n**********\r\nThis installation of data.table has not detected OpenMP support. It should still work but in single-threaded mode.\r\nThis is Emscripten. This warning should not normally occur on Windows or Linux where OpenMP is turned on by data.table's configure script by passing -fopenmp to the compiler. If you see this warning on Windows or Linux, please file a GitHub issue.\r\n**********\r\n\r\n> dt = data.table(1:10)\r\n\r\n> fwrite(dt,\"dt.csv\")\r\n\r\n> list.files()\r\n[1] \"dt.csv\"\r\n\r\n> dtcsv = fread(\"dt.csv\", verbose = T)\r\nThis installation of data.table has not been compiled with OpenMP support.\r\n  omp_get_num_procs()            1\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         1\r\n  omp_get_max_threads()          1\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 1 threads with throttle==1024. See ?setDTthreads.\r\nfreadR.c has been passed a filename: dt.csv\r\n[01] Check arguments\r\n  Using 1 threads (omp_get_max_threads()=1, nth=1)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file dt.csv\r\n  File opened, size = 24 bytes.\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<V1>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  No sep and quote rule found a block of 2x2 or greater. Single column input.\r\n  Detected 1 columns on line 1. This line is either column names or first data row. Line starts as: <<V1>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 1\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 1 because (23 bytes from row 1 to eof) \/ (2 * 23 jump0size) == 0\r\n  Type codes (jump 000)    : 6  Quote rule 0\r\n  'header' determined to be true due to column 1 containing a string on row 1 and a lower type (int32) in the rest of the 10 sample rows\r\n  All rows were sampled since file is small so we know nrow=10 exactly\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : 6\r\n[10] Allocate memory for the datatable\r\n  Allocating 1 column slots (1 - 0 dropped) with 10 rows\r\n[11] Read the data\r\n  jumps=[0..1), chunk_size=1048576, total_size=20\r\nRead 10 rows x 1 columns from 24 bytes file in 00:00.004 wall clock time\r\n[12] Finalizing the datatable\r\n  Type counts:\r\n         1 : int32     '6'\r\n=============================\r\n   0.001s ( 25%) Memory map 0.000GB file\r\n   0.002s ( 50%) sep='' ncol=1 and header detection\r\n   0.000s (  0%) Column type detection using 10 sample rows\r\n   0.001s ( 25%) Allocation of 10 rows x 1 cols (0.000GB) of which 10 (100%) rows used\r\n   0.000s (  0%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 10 rows) using 1 threads\r\n   +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)\r\n   +    0.000s (  0%) Transpose\r\n   +    0.000s (  0%) Waiting\r\n   0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions\r\n   0.004s        Total\r\nSystem errno 8 unmapping file: Bad file descriptor\r\n\r\n> print(dtcsv)\r\n       V1\r\n    <int>\r\n 1:     1\r\n 2:     2\r\n 3:     3\r\n 4:     4\r\n 5:     5\r\n 6:     6\r\n 7:     7\r\n 8:     8\r\n 9:     9\r\n10:    10\r\n\r\n> sessionInfo()\r\nR version 4.3.2 (2023-10-31)\r\nPlatform: wasm32-unknown-emscripten (32-bit)\r\nRunning under: emscripten\r\n\r\nMatrix products: default\r\n\r\n\r\nlocale:\r\n[1] en_US.UTF-8 C           en_US.UTF-8 en_US.UTF-8 en_US.UTF-8 C          \r\n\r\ntime zone: Europe\/Berlin\r\ntzcode source: internal\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.7\r\n\r\nloaded via a namespace (and not attached):\r\n[1] tools_4.3.2     webr_0.2.2.9000\r\n![error_fread](https:\/\/github.com\/Rdatatable\/data.table\/assets\/151060752\/4bfd5319-2504-4ae3-b26f-a12583e6c8be)\r\n`\r\n","comments":["I really don't know how to debug webR issues. My instinct is to file with them first, however... cc @georgestagg","I believe this is a manifestation of an Emscripten issue, https:\/\/github.com\/emscripten-core\/emscripten\/issues\/20459, from the use of `munmap()` in `fread.c`.\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/07fd933e7d724d00d292b325da2cb5d79ecbcb77\/src\/fread.c#L152-L153\r\n\r\nUntil Emscripten's support for `mmap()` is improved, we might be able to patch data.table so that the file is not closed until after `munmap()` is run when running under Wasm. I could maintain such a change in our r-wasm fork, or upstream back here, depending on how the maintainers feel about that kind of patch. It would probably be handled by an `#ifdef __EMSCRIPTEN__`.\r\n\r\nIn the meantime, from my reading of the data.table source code, the routine attempts to carry on even after showing this error. So, as you have observed, at least in the short term things should continue to work under webR even though the error is emitted.\r\n\r\n","Thanks for the quick reply!\r\n\r\n> I could maintain such a change in our r-wasm fork, or upstream back here, depending on how the maintainers feel about that kind of patch.\r\n\r\nThinking aloud here a bit.\r\n\r\nI am fine with such a patch here (we'd have to assign any related issues to you I think).\r\n\r\nIf the issue might affect other installations of data.table (any OS\/environment which has the same trouble with `mmap()`?), here is definitely the \"right\" place to maintain it. If it's a more general issue we might benefit from some `.\/config` testing as well?\r\n\r\nIf it's truly isolated to Wasm environments, it may make more sense to maintain a patch in the fork, but OTOH we do maintain some `#ifdef` stuff related to Windows, so is that roughly the same?","> OTOH we do maintain some #ifdef stuff related to Windows, so is that roughly the same?\r\n\r\nYes, I would consider it roughly the same. Emscripten can be thought of as another OS, just extremely similar to Linux.\r\n\r\nI think the best practice for true Unix portability would be to assume that `mmap()` is not always available, test for it in `configure`, and include some fallback path, but that's a much bigger change to the system. Also, AFAICT this particular issue is an Emscripten only problem that might even go away in the future.\r\n\r\n> I am fine with such a patch here (we'd have to assign any related issues to you I think).\r\n\r\nOK. I'll create a short PR with the required `#ifdef __EMSCRIPTEN__` changes for Webassembly, and we can go from there.\r\n"],"labels":["platform-specific"]},{"title":"data.table::.N may not work as expected in several places","body":"```r\r\nDT = data.table(a = rep(1:5, 2L), b = 1:10)\r\nDT[, b[.N]]\r\n# [1] 10\r\nDT[, b[data.table::.N]]\r\n# integer(0)\r\n```\r\n\r\nOf course `.N` is populated to the `.SD` environment, but in evaluation this will pull the `NULL` value exported by `data.table`.\r\n\r\nNoticed while writing tests for #5955.\r\n\r\nWe can:\r\n\r\n 1. Just disallow this use case, explicitly documenting that `data.table::.N` is not supported & you must use `.N` alone. We could even do `delayedAssign(\".N\", stop(\"Don't reference .N directly.\"))`. Would need to investigate if `importFrom(data.table, .N)` triggers the delayed assignment.\r\n 2. Massage `j` up front to remove `data.table::` from any NSE placeholders. This will bea new (ideally small) overhead in `j` for all queries, though. That would obviate #5955, as by the time we start checking `jsub` for GForce eligibility, `data.table::` will already be removed.","comments":[],"labels":["consistency"]},{"title":"data.table::.N should not disable GForce","body":"              Apparently, I was too slow, just noticed that you didn't use = for assignments.\r\n\r\nMaybe we should also document that `data.table::.N` will always be evaluated as `NULL`.\r\n\r\n_Originally posted by @ben-schwen in https:\/\/github.com\/Rdatatable\/data.table\/issues\/5944#issuecomment-1959895514_\r\n            ","comments":[],"labels":["GForce"]},{"title":"`list` sub-class with `format()` method prints full contents","body":"I ran into this when converting a nested `tibble` (i.e. with a list column of `tibble`s) to a `data.table`. Normally, `list` columns print with something like `<tibble[3x1]>` in `data.table`; however, these columns printed as a string displaying the entirety of the nested `tibble`s contents (in my case, ~20k rows of data per `tibble`). This appears to be due to a check for the existence of a `format()` method for the column type in `data.table:::format_col()`. In this case, the list columns were `vctrs_list_of` class, which implements its own `format.vctrs_list_of()` method. See below reprex for an example.\r\n\r\nFixing this is easy enough (see below reprex for proposed solution), but it would change the default printing behavior of list-cols. Any list subclass would then have to implement a `format_list_item()` method to get special treatment. On the other hand, any list subclass *could* then implement that method and get special treatment.\r\n\r\nTo me, defaulting to the standard list print behavior is an improvement, given that `format()` methods for list-like classes are generally not going to product output suitable for a column in a `data.table` (which is why `format_list_item()` is needed in the first place).\r\n\r\nReprex is below, along with proposed solution. I'm happy to file a PR but wanted to make sure the change in defaults is acceptable in principle first. Thanks!\r\n\r\n``` r\r\n# Setup -----------------------------------------------------------------------\r\n\r\n# Use development version of {data.table}\r\ndata.table::update_dev_pkg()\r\n#> R data.table package is up-to-date at 8f8ef9343dcabefa9e4cb0af4251cbb74dae9f55 (1.15.99)\r\n# Attach internals\r\nns <- asNamespace(\"data.table\")\r\nname <- format(ns)\r\nattach(ns, name = name, warn.conflicts = FALSE)\r\n\r\n# Create example data\r\ndt <- data.table(\r\n  list_col = list(data.table(a = 1:3), data.table(a = 4:6)),\r\n  list_of_col = vctrs::list_of(data.table(a = 1:3), data.table(a = 4:6))\r\n)\r\n\r\n\r\n# Problem ---------------------------------------------------------------------\r\n# `format_col()` does not format `list_of` columns from {vctrs} as lists\r\n\r\n# Print `data.table`\r\nprint(dt)\r\n#>             list_col     list_of_col\r\n#>               <list> <vctrs_list_of>\r\n#> 1: <data.table[3x1]>         1, 2, 3\r\n#> 2: <data.table[3x1]>         4, 5, 6\r\n\r\n# Format individually\r\nformat_col(dt$list_col)\r\n#> [1] \"<data.table[3x1]>\" \"<data.table[3x1]>\"\r\nformat_col(dt$list_of_col)\r\n#> [1] \"1, 2, 3\" \"4, 5, 6\"\r\n\r\n\r\n# Solution --------------------------------------------------------------------\r\n\r\n# Current function definition:\r\n# format_col.default = function(x, ...) {\r\n#   if (!is.null(dim(x)))\r\n#     \"<multi-column>\"\r\n#   else if (has_format_method(x) && length(formatted<-format(x, ...))==length(x))\r\n#     formatted\r\n#   else if (is.list(x))\r\n#     vapply_1c(x, format_list_item, ...)\r\n#   else\r\n#     format(char.trunc(x), ...)\r\n# }\r\n\r\n# Swapping the order of the `else if` statements in `format_col()` will fix the issue\r\n# `format_col_updated()` formats `list_of` columns from {vctrs} as lists\r\nformat_col_updated = function(x, ...) {\r\n  if (!is.null(dim(x)))\r\n    \"<multi-column>\"\r\n  else if (is.list(x)) # Now comes before `has_format_method()` check\r\n    vapply_1c(x, format_list_item, ...)\r\n  else if (has_format_method(x) && length(formatted<-format(x, ...))==length(x))\r\n    formatted\r\n  else\r\n    format(char.trunc(x), ...)\r\n}\r\n\r\n# Replace as default method\r\nregisterS3method(\"format_col\", \"default\", format_col_updated)\r\n\r\n# Print `data.table`\r\nprint(dt)\r\n#>             list_col       list_of_col\r\n#>               <list>   <vctrs_list_of>\r\n#> 1: <data.table[3x1]> <data.table[3x1]>\r\n#> 2: <data.table[3x1]> <data.table[3x1]>\r\n\r\n# Print individually\r\n# Still formatted as list\r\nformat_col_updated(dt$list_col)\r\n#> [1] \"<data.table[3x1]>\" \"<data.table[3x1]>\"\r\n# Now also formatted as list\r\nformat_col_updated(dt$list_of_col)\r\n#> [1] \"<data.table[3x1]>\" \"<data.table[3x1]>\"\r\n\r\n# Clean up --------------------------------------------------------------------\r\ndetach(name, character.only = TRUE)\r\nrm(list = ls())\r\n```\r\n\r\n<sup>Created on 2024-02-21 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\n<details style=\"margin-bottom:10px;\">\r\n<summary>\r\nSession info\r\n<\/summary>\r\n\r\n``` r\r\nsessioninfo::session_info()\r\n#> \u2500 Session info \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n#>  setting  value\r\n#>  version  R version 4.3.2 (2023-10-31 ucrt)\r\n#>  os       Windows 11 x64 (build 22631)\r\n#>  system   x86_64, mingw32\r\n#>  ui       RTerm\r\n#>  language (EN)\r\n#>  collate  English_United States.utf8\r\n#>  ctype    English_United States.utf8\r\n#>  tz       America\/Chicago\r\n#>  date     2024-02-21\r\n#>  pandoc   3.1.1 @ C:\/Program Files\/RStudio\/resources\/app\/bin\/quarto\/bin\/tools\/ (via rmarkdown)\r\n#> \r\n#> \u2500 Packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n#>  package     * version date (UTC) lib source\r\n#>  cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.2)\r\n#>  data.table    1.15.99 2024-02-21 [1] local\r\n#>  digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.2)\r\n#>  evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.2)\r\n#>  fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.2)\r\n#>  fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.2)\r\n#>  glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.2)\r\n#>  htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.3.2)\r\n#>  knitr         1.45    2023-10-30 [1] CRAN (R 4.3.2)\r\n#>  lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.2)\r\n#>  magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.2)\r\n#>  purrr         1.0.2   2023-08-10 [1] CRAN (R 4.3.2)\r\n#>  R.cache       0.16.0  2022-07-21 [1] CRAN (R 4.3.2)\r\n#>  R.methodsS3   1.8.2   2022-06-13 [1] CRAN (R 4.3.1)\r\n#>  R.oo          1.25.0  2022-06-12 [1] CRAN (R 4.3.1)\r\n#>  R.utils       2.12.3  2023-11-18 [1] CRAN (R 4.3.2)\r\n#>  reprex        2.0.2   2022-08-17 [1] CRAN (R 4.3.2)\r\n#>  rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.2)\r\n#>  rmarkdown     2.25    2023-09-18 [1] CRAN (R 4.3.2)\r\n#>  rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.2)\r\n#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.2)\r\n#>  styler        1.10.2  2023-08-29 [1] CRAN (R 4.3.2)\r\n#>  vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.2)\r\n#>  withr         3.0.0   2024-01-16 [1] CRAN (R 4.3.2)\r\n#>  xfun          0.41    2023-11-01 [1] CRAN (R 4.3.2)\r\n#>  yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.2)\r\n#> \r\n#>  [1] D:\/ProgramFiles\/R\/R-4.3.2\/library\r\n#> \r\n#> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```\r\n\r\n<\/details>\r\n\r\n","comments":["Hi Jesse, thanks for writing. Is there a way to fix your issue in a way that maintains backward compatibility with the existing print output? One of the guiding principles of data.table is stability\/back-compatibility https:\/\/github.com\/Rdatatable\/data.table\/blob\/master\/GOVERNANCE.md#the-r-package so it would be much easier to accept a code contribution that does not change what is printed.","Hey Toby, thanks for getting back. The current behavior is actually *not* backwards compatible - list sub-classes print identically to bare lists through v1.14.10, and it's only in v1.15.0 and later that this \"print everything\" behavior occurs. The proposal is essentially to revert to the previous behavior, because there are cases where the current behavior is pathological, and I'm not sure the current behavior was actually intended (I can't find anything in NEWS that mentions it)."],"labels":["regression","print"]},{"title":"RFC: Feature completion for raw columns","body":"Creating as a placeholder for comment. This issue is similar to #3690 which sought to flesh out support for complex-valued columns in data.table.\r\n\r\nRaw-valued columns are actually much easier to support than complex (they are a simple type with no NAs), but I am less sure about use cases.\r\n\r\nSo please speak up if you find yourself using raw-valued columns in your work and run into constraints from lack of support in data.table. Workarounds (cast to integer) are easy enough but if there are legitimate use cases we should provide direct support for raw (it's mostly copy-pasting code from `INTSXP` branches with slight adjustments).","comments":["Paging @HughParsonage. I think this is maybe a dup of #5100. I would keep this one for the reason I just posted in the other thread.\r\n\r\nSee also #4588.","I draw your attention to my package https:\/\/cran.r-project.org\/package=factor256 which is basically an embryo of the use-case I'm pushing.\r\n\r\nhttps:\/\/github.com\/hughParsonage\/factor256"],"labels":["question"]},{"title":"frollmax is slow on descending sequences","body":"`#` [`Minimal reproducible example`](https:\/\/stackoverflow.com\/questions\/5963269\/how-to-make-a-great-r-reproducible-example); please be sure to set `verbose=TRUE` where possible!\r\n\r\n```\r\nif (!require(\"devtools\")) install.packages(\"devtools\")\r\nif (!require(\"RCRoll\")) devtools::install_github(\"davidcsterratt\/RCRoll\")\r\nif (!require(\"roll\")) install.packages(\"roll\")\r\nif (!require(\"data.table\")) devtools::install_github(\"Rdatatable\/data.table\", branch=\"frollmax\")\r\n\r\nbenchmark <- function(n, k, func) {\r\n  x = func(n)\r\n  start_time = Sys.time(); y.RCRoll = RCRoll::rollmin(x, k) ; print(paste(\"RCRoll:\", Sys.time() - start_time))\r\n  start_time = Sys.time(); y.roll = roll::roll_min(x, k) ; print(paste(\"roll:\", Sys.time() - start_time))\r\n  start_time = Sys.time(); y.data.table = data.table::frollmax(x, k) ; print(paste(\"data.table:\", Sys.time() - start_time))\r\n   return(list(\"RCRoll\"=y.RCRoll, \"roll\"=y.roll, \"data.table\"=y.data.table))\r\n}\r\n\r\nn <- 1E7\r\nk <- 1E3\r\n\r\n# Random normal\r\nout1 <- benchmark(n, k, rnorm)\r\n# All equal to 0\r\nout2 <- benchmark(n, k, function(n) {rep(0, n)})\r\n# Ascending\r\nout3 <- benchmark(n, k, function(n) {seq(0, length.out=n)})\r\n# Descending\r\nout4 <- benchmark(n, k, function(n) {seq(0, by=-1, length.out=n)})\r\n```\r\n\r\nOutput of the benchmarking code:\r\n```\r\n[1] \"RCRoll: 0.169624805450439\"\r\n[1] \"roll: 0.153290987014771\"\r\n[1] \"data.table: 0.0348567962646484\"\r\n[1] \"RCRoll: 0.0276196002960205\"\r\n[1] \"roll: 0.0650320053100586\"\r\n[1] \"data.table: 0.034125804901123\"\r\n[1] \"RCRoll: 0.0440917015075684\"\r\n[1] \"roll: 0.0651366710662842\"\r\n[1] \"data.table: 0.0270349979400635\"\r\n[1] \"RCRoll: 0.0287351608276367\"\r\n[1] \"roll: 0.0672793388366699\"\r\n[1] \"data.table: 3.9090781211853\"\r\n```\r\n`data.table::frollmax()` is considerably slower for descending sequences.\r\n\r\nNote that `RCRoll` is an unpublished package I wrote based on [the late Richard Harter's ascending minimum algorithm](https:\/\/richardhartersworld.com\/slidingmin\/) before I realised that the `roll` package seemed to be about as efficient (at least on one core). Note also that I've not compared the results and I've used the rolling min function from `roll`.\r\n\r\n`#` `Output of sessionInfo()`\r\n```\r\n> sessionInfo()\r\nR Under development (unstable) (2023-12-29 r85751)\r\nPlatform: x86_64-pc-linux-gnu\r\nRunning under: Ubuntu 22.04.3 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/local\/lib\/R\/lib\/libRblas.so \r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/lapack\/liblapack.so.3.10.0\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              \r\n [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    \r\n [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   \r\n [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       \r\n\r\ntime zone: Europe\/London\r\ntzcode source: system (glibc)\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.99 RcppRoll_0.3.0     zoo_1.8-12         roll_1.1.6        \r\n[5] RCRoll_0.1         devtools_2.4.5     usethis_2.2.2     \r\n\r\nloaded via a namespace (and not attached):\r\n [1] miniUI_0.1.1.1     compiler_4.4.0     promises_1.2.1     Rcpp_1.0.12       \r\n [5] stringr_1.5.1      later_1.3.2        fastmap_1.1.1      lattice_0.20-45   \r\n [9] mime_0.12          R6_2.5.1           htmlwidgets_1.6.4  profvis_0.3.8     \r\n[13] shiny_1.8.0        rlang_1.1.3        cachem_1.0.8       stringi_1.8.3     \r\n[17] httpuv_1.6.13      fs_1.6.3           RcppParallel_5.1.7 pkgload_1.3.3     \r\n[21] memoise_2.0.1      cli_3.6.2          magrittr_2.0.3     grid_4.4.0        \r\n[25] digest_0.6.34      xtable_1.8-4       remotes_2.4.2.1    lifecycle_1.0.4   \r\n[29] vctrs_0.6.5        glue_1.7.0         urlchecker_1.0.1   sessioninfo_1.2.2 \r\n[33] pkgbuild_1.4.3     purrr_1.0.2        tools_4.4.0        ellipsis_0.3.2    \r\n[37] htmltools_0.5.7   \r\n```\r\n\r\nP.S. @jangorecki This issue was prompted by your nice talk at EdinbR on Friday - I was the person who said I'd follow up after the meeting. ","comments":["Thank you for raising that and reproducible code.\r\n\r\nImplementation of rolling min and max was not following any paper, it was simply my idea how to extend online algo from rolling mean to make it support min\/max. It turned to be good enough so there was no need to look for better algos. Therefore there is a space for improvement here I believe.\r\n\r\nWhat is good is how narrow are data cases which ends up to have problems: only desc sequence (for max, and asc for min). To not reimplement algo, we could even reverse input and swap align arg, and then reverse results. Of course better to have proper algo.\r\n\r\nManual (possibly in rollmedian branch and not frollmax) already explains edge case where naive approach can be faster than this one. We could possibly add example how to work around of this.","Does this only apply in the case of monotone decreasing sequences? it would also be interesting to see something like\r\n\r\ny=-mx+eps, eps~N(0, s)\r\n\r\nand compare performance as s->0.\r\n\r\nif performance degrades more and more, that's much more worrisome than if it's only an issue exactly in the s=0 edge case","Whenever next element is always smaller, then it will be same bad as N:1","> Whenever next element is always smaller, then it will be same bad as N:1\r\n\r\nSo from an algorithmic point of view, it will be slow when we have a lot of inversions?","It depends where the inversions are relatively to windows size and max location within it. Decreasing sequence is an extreme case.\r\n\r\nNumber or nested finding max calls is reported with verbose, at least in rollmedian branch.","Glad that the report was helpful. I 'm not expert in these algorithms, but my main observation is that the rolling max\/min is surprisingly tricky compared to the rolling mean case! (Before doing some reading online, I started trying to write my own, and got stuck.)\r\n\r\n In terms of other test cases, it might be interesting to have some pink noise, i.e. like the rnorm case, but smoothed by a filter. It might also be interesting to check the scaling with the sequence length and the window length.\r\n\r\nAnyway, all the best with finding a good solution.","Probably algorithm used in median could be adapted for min\/max as well, but I don't think itnis worth the effort"],"labels":["froll"]},{"title":"Error in xn %chin% \"\" :   Internal error: savetl_init checks failed (0 100 0x5623af7280c0 0x56231e164630). please report to data.table issue tracker.","body":"I have to find a way to create a reprex for this because I cannot share the actual data but I am getting the error when running the following command:\r\ndup = duplicated(data_raw_wide, by = col_list)","comments":["@seanfryan I cannot reproduce your error.\r\n\r\nPlease create an example adding your sessionInfo().\r\n\r\n``` r\r\nlibrary(data.table)\r\n\r\nsample_10 <- function(seed) {\r\n  \r\n  set.seed(seed)\r\n  \r\n  values <- sample(LETTERS[1:5], 10L, replace = TRUE)\r\n  \r\n  set.seed(NULL)\r\n  \r\n  return(values)\r\n  \r\n}\r\n\r\ndt <- \r\n  lapply(1:10, sample_10) |>\r\n  as.data.table()\r\n\r\nduplicated(dt, by = 5:6)\r\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\r\nduplicated(dt, by = paste0(\"V\", 5:6))\r\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE\r\n\r\nsessionInfo()\r\n#> R version 4.2.3 (2023-03-15 ucrt)\r\n#> Platform: x86_64-w64-mingw32\/x64 (64-bit)\r\n#> Running under: Windows 10 x64 (build 19045)\r\n#> \r\n#> Matrix products: default\r\n#> \r\n#> locale:\r\n#> [1] LC_COLLATE=English_United States.utf8 \r\n#> [2] LC_CTYPE=English_United States.utf8   \r\n#> [3] LC_MONETARY=English_United States.utf8\r\n#> [4] LC_NUMERIC=C                          \r\n#> [5] LC_TIME=English_United States.utf8    \r\n#> \r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \r\n#> \r\n#> other attached packages:\r\n#> [1] data.table_1.14.10\r\n#> \r\n#> loaded via a namespace (and not attached):\r\n#>  [1] digest_0.6.34     withr_3.0.0       R.methodsS3_1.8.2 lifecycle_1.0.4  \r\n#>  [5] magrittr_2.0.3    reprex_2.1.0      evaluate_0.23     rlang_1.1.3      \r\n#>  [9] cli_3.6.2         rstudioapi_0.15.0 fs_1.6.3          R.utils_2.12.3   \r\n#> [13] R.oo_1.25.0       vctrs_0.6.5       styler_1.10.2     rmarkdown_2.25   \r\n#> [17] tools_4.2.3       R.cache_0.16.0    glue_1.7.0        purrr_1.0.2      \r\n#> [21] xfun_0.41         yaml_2.3.8        fastmap_1.1.1     compiler_4.2.3   \r\n#> [25] htmltools_0.5.7   knitr_1.45\r\n```\r\n\r\n<sup>Created on 2024-01-24 with [reprex v2.1.0](https:\/\/reprex.tidyverse.org)<\/sup>\r\n","session info?","R version 4.1.2 (2021-11-01)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Ubuntu 22.04.3 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/blas\/libblas.so.3.10.0\r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/lapack\/liblapack.so.3.10.0\r\n\r\nlocale:\r\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8\r\n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8\r\n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C\r\n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base\r\n\r\nother attached packages:\r\n [1] ecoenvutils_0.0.0.9000 testthat_3.2.0         readxl_1.4.3\r\n [4] janitor_2.2.0          geosphere_1.5-18       tictoc_1.2\r\n [7] dtplyr_1.3.1           data.table_1.14.10     devtools_2.4.5\r\n[10] usethis_2.2.2          lubridate_1.9.2        forcats_1.0.0\r\n[13] stringr_1.5.0          dplyr_1.1.2            purrr_1.0.1\r\n[16] readr_2.1.4            tidyr_1.3.0            tibble_3.2.1\r\n[19] ggplot2_3.4.2          tidyverse_2.0.0\r\n\r\nloaded via a namespace (and not attached):\r\n [1] pkgload_1.3.3     brio_1.1.3        shiny_1.7.5.1     sp_2.1-2\r\n [5] cellranger_1.1.0  remotes_2.4.2.1   sessioninfo_1.2.2 pillar_1.9.0\r\n [9] lattice_0.20-45   glue_1.7.0        digest_0.6.33     promises_1.2.1\r\n[13] snakecase_0.11.1  colorspace_2.1-0  unpivotr_0.6.3    htmltools_0.5.5\r\n[17] httpuv_1.6.12     pkgconfig_2.0.3   NCmisc_1.2.0      xtable_1.8-4\r\n[21] scales_1.2.1      processx_3.8.2    openxlsx_4.2.5.2  later_1.3.1\r\n[25] tzdb_0.3.0        timechange_0.2.0  generics_0.1.3    ellipsis_0.3.2\r\n[29] cachem_1.0.8      withr_2.5.0       cli_3.6.2         magrittr_2.0.3\r\n[33] crayon_1.5.2      mime_0.12         memoise_2.0.1     ps_1.7.5\r\n[37] fs_1.6.2          fansi_1.0.4       pkgbuild_1.4.2    profvis_0.3.8\r\n[41] tools_4.1.2       prettyunits_1.1.1 hms_1.1.3         lifecycle_1.0.4\r\n[45] munsell_0.5.0     writexl_1.4.2     zip_2.3.0         tidyxl_1.0.9\r\n[49] callr_3.7.3       compiler_4.1.2    rlang_1.1.3       grid_4.1.2\r\n[53] rstudioapi_0.14   htmlwidgets_1.6.2 miniUI_0.1.1.1    gtable_0.3.3\r\n[57] reader_1.0.6      R6_2.5.1          fastmap_1.1.1     utf8_1.2.3\r\n[61] rprojroot_2.0.4   desc_1.4.2        stringi_1.7.12    Rcpp_1.0.12\r\n[65] vctrs_0.6.5       tidyselect_1.2.0  urlchecker_1.0.1\r\n\r\n\r\nIt works on one dataset but throws the error for another dataset in same session, after throwing segfault from C stack overflow error.  ","@seanfryan please let us know if we're able to reproduce to error with a simulated dataset.\r\n\r\nTry to assign to this simulated dataset the same column classes your original dataset has."],"labels":["not reproducible"]},{"title":"Consider flipping to a decrement loop for readability inside rolling windows","body":"              Maybe more readable as `int j=k-1; j>=0; j--` and `x[i-j]` instead?\r\n\r\n_Originally posted by @MichaelChirico in https:\/\/github.com\/Rdatatable\/data.table\/pull\/5889#discussion_r1447128256_\r\n            ","comments":[],"labels":["froll"]},{"title":"Use an enum for 'algo' in roll source (over 0\/1)","body":"              algo 0 and algo 1 are less explanatory than algo fast and algo exact.\r\nEspecially for the case of frolladaptivemax users might be surprised what 0 and 1 mean.\r\n\r\n_Originally posted by @ben-schwen in https:\/\/github.com\/Rdatatable\/data.table\/pull\/5890#discussion_r1445897469_\r\n\r\nI made a similar comment in #5889.\r\n            ","comments":["This issue can be solved by replacing \"algo 0\"  to \"algo fast\" and \"algo 1\" to \"algo exact \" in froll.Rraw\r\nam i correct?","I think this on hold until we merge the froll PRs up to frollmax10 since it would create unnecessary merging problems.\r\n","Up to rollmedian branch, so 2-4 branches further than frollmax","> This issue can be solved by replacing \"algo 0\" to \"algo fast\" and \"algo 1\" to \"algo exact \" in froll.Rraw am i correct?\r\n\r\nAfter creating enum in C and substituting current print outputs from 0\/1 for strings. Ideally ensuring that string buffer limit defined for ans_t messages carrying will not be exceeded."],"labels":["froll"]},{"title":"frollmax10: deduplicate, support for adaptive&&partial","body":"- support for adaptive and partial\r\n- deduplicate code by using helper instead of macros\r\n- merged to master","comments":["## [Codecov](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5898?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Comparison is base [(`5376881`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/commit\/537688106718b72e04ddf2859c3ec61a5aed2dc0?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.46% compared to head [(`fff2b08`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5898?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.52%.\n> Report is 96 commits behind head on master.\n\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #5898      +\/-   ##\n==========================================\n+ Coverage   97.46%   97.52%   +0.05%     \n==========================================\n  Files          80       80              \n  Lines       14822    15117     +295     \n==========================================\n+ Hits        14447    14743     +296     \n+ Misses        375      374       -1     \n```\n\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5898?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n"],"labels":["froll"]},{"title":"frollmax9: give.names argument","body":"- give.names argument","comments":[],"labels":["froll"]},{"title":"frollmax8: deduplicate args handling","body":"- frollfun router function\r\n- deduplicate N arg handling\r\n- deduplicate partial and left adaptive logic","comments":[],"labels":["froll"]},{"title":"frollmax7: arg rename, use macros, Inf support","body":"- argument rename\r\n- simplify code with macros\r\n- Inf support","comments":[],"labels":["froll"]},{"title":"frollmax6: simplify frollmax adaptive","body":"- simplify frollmax adaptive\r\n- docs","comments":[],"labels":["froll"]},{"title":"frollmax5: NA support in frollmax","body":"- frollmax code reorg\r\n- frollmax NA support","comments":[],"labels":["froll"]},{"title":"frollmax4: frollapply adaptive","body":"- frollapply adaptive","comments":[],"labels":["froll"]},{"title":"frollmax3: partial arg","body":"- support of partial argument","comments":["PR description needs more detail -- imagine coming back to this PR in 10 years trying to figure out context. What's `partial`? Why did we add it?\r\n\r\nA quick sentence + issue link would suffice."],"labels":["froll"]},{"title":"frollmax2: code reorg, docs, tests","body":"- code reorganization by using C `frollfun` routing function\r\n- news\r\n- docs\r\n- tests","comments":["I've set the merge target here to frollmax1, would you mind doing similar for the subsequent PRs to make the relative diffs easier to see?","Yes, we can do it, but are we going to merge from 10->9->...->1 ? Or we switch back to master before merging?","> Yes, we can do it, but are we going to merge from 10->9->...->1 ? Or we switch back to master before merging?\r\n\r\nonce the merge target is deleted GH will automatically set master as the new target.\r\n\r\ncurrent form makes it hard to parallelize review, would only really make sense to review serially","PR overall looks good (great change!), I'll give final approval once `frollmax1` is finalized to see the diff on NEWS\/Rd more clearly","It seems that we will need to resolve same merge conflicts in each downstream PR and with every change in each PR amount of conflicts will only keep growing.\r\nWhen merging 1-15-99 to frollmax2:\r\n```\r\nAuto-merging R\/froll.R\r\nCONFLICT (content): Merge conflict in R\/froll.R\r\nAuto-merging inst\/tests\/froll.Rraw\r\nCONFLICT (content): Merge conflict in inst\/tests\/froll.Rraw\r\nAuto-merging man\/froll.Rd\r\nCONFLICT (content): Merge conflict in man\/froll.Rd\r\nAuto-merging src\/data.table.h\r\nCONFLICT (content): Merge conflict in src\/data.table.h\r\nAuto-merging src\/froll.c\r\nCONFLICT (content): Merge conflict in src\/froll.c\r\nAuto-merging src\/frollR.c\r\nCONFLICT (content): Merge conflict in src\/frollR.c\r\nAuto-merging src\/frolladaptive.c\r\nCONFLICT (content): Merge conflict in src\/frolladaptive.c\r\n```\r\nThen it feels that having single big PR may be better approach here. Still reviews could happen on 10 smaller PRs but none of them would needed to be merged, they would only serve as a smaller chunks of code to review at one moment, to look for bugs in the code. Then when all 10 PRs are checked for now bugs then the single bigger one could be merged.","> Still reviews could happen on 10 smaller PRs but none of them would needed to be merged\r\n\r\nUnfortunately it's not so simple, with the evolving diffs inherited from the earlier PRs, it will be increasingly hard to keep track of what the actual current state is as a reviewer.\r\n\r\nDid you get a chance to check out any of the recommended tools for PR stacking? The whole point there was to delegate these downstream merge conflicts to the tools, did that not work out as hoped?","yes\r\nhttps:\/\/graphite.dev\/docs\/squash-fold-split\r\n`gt split --by-commit` is doing exactly what I made to split `frollmax` branch","Looks like `gt modify` is the next step? https:\/\/graphite.dev\/docs\/update-mid-stack-branches","If `gt modify` is not working out (\ud83d\ude1e), cherry-picking commits added during review should work right? possibly with some rebase+force push. If so I can try to handle that part since I insisted on breaking up the PRs :)"],"labels":["froll"]},{"title":"deprecate programming interfaces: get, mget, eval","body":"Personally I am very firmly after backward compatibility (therefore adding to 2.0.0 milestone), but just noticed quite a list of open issues caused by legacy programming interfaces (get, mget, eval). I doubt below list is complete.\r\n\r\nBugs in special handling of scoping for variables\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/1985\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/4878\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/4798\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/1356\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/2647\r\n\r\nSpecial handling of scoping variable bugs during real\/standard use of `eval()`\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/1180\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/2176\r\n\r\nPossibly bug introduced by special handling of `eval()`\r\n- https:\/\/github.com\/Rdatatable\/data.table\/issues\/1857","comments":["I would be open to deprecating these sooner than 2.0.0, but we definitely need to let the new programming interface mature for some time (e.g. 2-4 releases) before going that route."],"labels":["programming"]},{"title":"implement frev() - base::rev() that allocates less","body":"              Since we know now how costly negation is, what about reverting in-place when possible? Reverting with copy costs 1\/2  of `rev`. Reverting in-place costs 1\/10 of `rev` (no parallelization).\r\n\r\n```r\r\nx = rnorm(1e8)\r\nsystem.time(t1 <- rev(x))\r\n#   user  system elapsed \r\n#  0.481   0.212   0.693 \r\nsystem.time(t2 <- reverse(x, copy=TRUE))\r\n#   user  system elapsed \r\n#  0.117   0.216   0.334 \r\nsystem.time(t3 <- reverse(x, copy=FALSE))\r\n#   user  system elapsed \r\n#  0.060   0.000   0.061 \r\nidentical(t1, t2)\r\nidentical(t1, t3)\r\n\r\nx = rnorm(1e8)\r\nn = rep(1e2, 1e8)\r\nsystem.time(a1 <- frollmax(x, n, align=\"left\", adaptive=TRUE))\r\n#    user  system elapsed \r\n#  14.330   3.147   7.669\r\nsystem.time(a2 <- reverse(frollmax(reverse(x, copy=TRUE), reverse(n, copy=TRUE), align=\"right\", adaptive=TRUE), copy=FALSE))\r\n#    user  system elapsed \r\n#   9.092   0.807   2.687 \r\nidentical(a1, a2)\r\n```\r\n\r\n_Originally posted by @ben-schwen in https:\/\/github.com\/Rdatatable\/data.table\/pull\/5441#discussion_r1443843574_\r\n            ","comments":["And possibly many functions as well.\r\nwhenever rev() is used we could cut its time to 1\/2 or even 1\/10 when it's safe to rev in place.","I turned the issue into more generic one. We could always save at least one allocation, of indices, and sometimes even of the results.","Noting a related r-devel bug to support ALTREP for rev():\r\n\r\nhttps:\/\/bugs.r-project.org\/show_bug.cgi?id=18406","We can use {lintr} to quickly identify usages inside the package (with more confidence over `grep` since {lintr} is AST-aware):\r\n\r\n```\r\nlintr::lint_package(linters = lintr::undesirable_function_linter(c(rev = NA)))\r\n```\r\n\r\n```\r\nR\/as.data.table.R:37:9: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n  val = rev(dimnames(provideDimnames(x)))\r\n        ^~~\r\nR\/as.data.table.R:39:39: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n    setattr(val, 'names', paste0(\"V\", rev(seq_along(val))))\r\n                                      ^~~\r\nR\/as.data.table.R:41:22: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n  setcolorder(ans, c(rev(head(names(ans), -1L)), \"N\"))\r\n                     ^~~\r\nR\/as.data.table.R:102:9: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n  val = rev(val)\r\n        ^~~\r\nR\/as.data.table.R:104:39: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n    setattr(val, 'names', paste0(\"V\", rev(seq_along(val))))\r\n                                      ^~~\r\nR\/as.data.table.R:106:93: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n    stop(\"Argument 'value.name' should not overlap with column names in result: \", brackify(rev(names(val))))\r\n                                                                                            ^~~\r\nR\/as.data.table.R:112:10: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n  dims = rev(head(names(ans), -1L))\r\n         ^~~\r\nR\/bmerge.R:92:100: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n      if (xclass==\"integer64\") { w=i; wc=ic; wclass=iclass; } else { w=x; wc=xc; wclass=xclass; nm=rev(nm) }  # w is which to coerce\r\n                                                                                                   ^~~\r\nR\/utils.R:43:27: style: [undesirable_function_linter] Function \"rev\" is undesirable.\r\n  length(x) - match(TRUE, rev(x)) + 1L\r\n                          ^~~\r\n```","@anirban166 @DorisAmoakohene please investigate the memory savings using atime"],"labels":["performance"]},{"title":"frollmax could hint pragma omp simd reduction(max:w)","body":"              Could hint `#pragma omp simd reduction(max:w)` for the inner loop to the compiler\r\n\r\n_Originally posted by @ben-schwen in https:\/\/github.com\/Rdatatable\/data.table\/pull\/5441#discussion_r1443534630_\r\n            ","comments":[],"labels":["froll"]},{"title":"fread crashes R (bus error) for large gzipped csv file when specifying a large number of nrows (but whole file works)","body":"I have large gzipped csv file with ~40 million rows. I can read the whole file into R fine:\r\n\r\n```\r\nfilename <- 'myfile.csv'\r\ntable_whole <- fread(filename)\r\n```\r\n\r\nI can also load about 5e6 rows into R fine:\r\n```\r\nfilename <- 'myfile.csv'\r\ntable_first5e6rows <- fread(filename, nrows=5e6)\r\n```\r\n\r\nBut, when I load 10e6 rows, it crashes R:\r\n\r\n```\r\nfilename <- 'myfile.csv'\r\ntable_first5e6rows <- fread(filename, nrows=1e7)\r\n *** caught bus error ***\r\naddress 0x20000180e, cause 'invalid alignment'\r\n```\r\n\r\nHere is the actual output with the actual file, with verbose on:\r\n```\r\n> library(data.table)\r\ndata.table 1.14.10 using 8 threads (see ?getDTthreads).  Latest news: r-datatable.com\r\n> fread('\/Users\/dbg\/Library\/CloudStorage\/Box-Box\/tcsl\/ngs_data\/2023.09.12.illumina_28\/out\/merged_df.csv.gz', nrows=1e7, verbose=T)\r\n  OpenMP version (_OPENMP)       202011\r\n  omp_get_num_procs()            16\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          16\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 8 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 8 threads (omp_get_max_threads()=16, nth=8)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file \/var\/folders\/r9\/rhdgkfwx0msg9ldkp_6ltypr0000gn\/T\/\/RtmpkxLujJ\/file1018d1112866\r\n  File opened, size = 10.67GB (11454682052 bytes).\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<itam_bc_o,itam_umi_o,costim_bc>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=','  with 100 lines of 94 fields using quote rule 0\r\n  Detected 94 columns on line 1. This line is either column names or first data row. Line starts as: <<itam_bc_o,itam_umi_o,costim_bc>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 94\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 100 because nrow limit (10000000) supplied\r\n  Type codes (jump 000)    : CCCC755555555555555555555555555555555555555555555555555555555555555555555555555555555555555555  Quote rule 0\r\n  'header' determined to be true due to column 5 containing a string on row 1 and a lower type (float64) in the rest of the 100 sample rows\r\n  All rows were sampled since file is small so we know nrow=100 exactly\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : CCCC755555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\r\n[10] Allocate memory for the datatable\r\n  Allocating 94 column slots (94 - 0 dropped) with 100 rows\r\n[11] Read the data\r\n  jumps=[0..1), chunk_size=1048576, total_size=11454676303\r\n  Too few rows allocated. Allocating additional 11999900 rows (now nrows=10000000) and continue reading from jump 0\r\n  jumps=[0..1), chunk_size=1048576, total_size=11454676303\r\n\r\n *** caught bus error ***\r\naddress 0x20000180e, cause 'invalid alignment'\r\n\r\nTraceback:\r\n 1: fread(\"\/Users\/dbg\/Library\/CloudStorage\/Box-Box\/tcsl\/ngs_data\/2023.09.12.illumina_28\/out\/merged_df.csv.gz\",     nrows = 1e+07, verbose = T)\r\n```\r\n\r\nI have other similarly sized gzipped CSVs (including some that are 20% larger) that work fine, so it is something about the combination of this file, and asking nrows to load in a large number of rows. \r\n\r\n[Here is a link to the file on box (700MB).](https:\/\/ucsf.box.com\/s\/2nkgo1u5vas5vycup1krukgaib6t6y0r) \r\n\r\nHere is my session info:\r\n```\r\n> sessionInfo()\r\nR version 4.3.2 (2023-10-31)\r\nPlatform: aarch64-apple-darwin20 (64-bit)\r\nRunning under: macOS Sonoma 14.1\r\n\r\nMatrix products: default\r\nBLAS:   \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/lib\/libRblas.0.dylib \r\nLAPACK: \/Library\/Frameworks\/R.framework\/Versions\/4.3-arm64\/Resources\/lib\/libRlapack.dylib;  LAPACK version 3.11.0\r\n\r\nlocale:\r\n[1] en_US.UTF-8\/en_US.UTF-8\/en_US.UTF-8\/C\/en_US.UTF-8\/en_US.UTF-8\r\n\r\ntime zone: America\/Los_Angeles\r\ntzcode source: internal\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.3.2\r\n```\r\n\r\n\r\n\r\n","comments":["did you try using the dev version of fread?\r\n```r\r\ndata.table::update_dev_pkg()\r\n```"],"labels":["fread"]},{"title":"test() should have an option too temporarily set env=","body":"              I would like to see a similar `test(testnum, my_computed_value, my_expected_value, env=list(SOME_ENV_VAR=\"VALUE\"))` -- I was recently working on #5807 which involves environment variables for getting default number of threads, and the tests would be much easier to understand\/modify with such an enhancement to test.\r\n\r\n_Originally posted by @tdhock in https:\/\/github.com\/Rdatatable\/data.table\/issues\/5845#issuecomment-1864863295_\r\n            ","comments":[],"labels":["tests"]},{"title":"Deprecate sep=\"\\n\"?","body":"https:\/\/github.com\/Rdatatable\/data.table\/blob\/6c1fd839e0e0257d13f07975ead0fe6fdfee2f61\/NEWS.1.md?plain=1#L1209\r\n\r\nThis NEWS item (1.11.0, 2018) mentions `sep=\"\\n\"` would be deprecated, but I don't know why. It seems to work fine:\r\n\r\n```r\r\nfread(\"a\\nb\\nc\", sep = \"\\n\")\r\n#    a\r\n# 1: b\r\n# 2: c\r\n```\r\n\r\nAnd here `sep` is set to `\\n` before sending on to C:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/6c1fd839e0e0257d13f07975ead0fe6fdfee2f61\/R\/fread.R#L12-L15\r\n\r\nI guess it's about clarity for Windows, where `sep=\"\\n\"` doesn't mean literally `\\n`, but \"end-of-line\", i.e., `\"\\n\"` is actually a \"sentinel\".\r\n\r\nBut it doesn't seem to have caused any issue in the interim. Do we want to follow through with deprecating `sep = \"\\n\"`?","comments":["If we don't know why to deprecate we can leave it as is, till someone come and ask for that. It feels it can be still useful.","I agree. Leaving open for now to get more opinions. If we leave things be, we should add a NEWS note about the change of plans."],"labels":["breaking-change"]},{"title":"DT[chr.vec] slower than tibble[chr.vec,] by sub-linear factors","body":"I recently wrote a blog post which analyzes time complexity of partial matching of `[.data.frame` (when looking up rows using a character vector, compared to the data frame row names) and compares with alternatives from data.table and tibble: https:\/\/tdhock.github.io\/blog\/2023\/df-partial-match\/ \r\nThe important part for these purposes is the second set of figures in the Comparison section, which shows that tibble is actually faster than data.table by constant factors. Now constant factors aren't a big deal, but this does suggest that there is some room for optimization\/improvement.\r\nA simplified version of the code from the blog is shown below:\r\n```r\r\nlibrary(data.table)\r\nworkaround.result <- atime::atime(\r\n  N=10^seq(1, 7, by=0.5),\r\n  setup={\r\n    N.v <- 1:N\r\n    N.ids <- paste0(\"cg\", sprintf(\"%06d\",N.v-1))\r\n    N.d <- data.frame(row.names=N.ids, N.v)\r\n    N.half <- as.integer(N\/2)\r\n    matching.str <- sample(N.ids, N, replace=F)\r\n    half.no.match <- c(matching.str[1:N.half], rep(\"FOO\",N.half) )\r\n    N.dt <- data.table(N.d, name=N.ids, key=\"name\")\r\n    N.tib <- tibble::tibble(N.d)\r\n  },\r\n  base=N.d[match(half.no.match, rownames(N.d)),,drop=F],\r\n  data.table=N.dt[half.no.match],\r\n  tibble=N.tib[half.no.match,],\r\n  seconds.limit=1)\r\nworkaround.refs <- atime::references_best(workaround.result)\r\nworkaround.pred <- predict(workaround.refs)\r\nplot(workaround.pred)\r\n```\r\n![image](https:\/\/github.com\/Rdatatable\/data.table\/assets\/932850\/64e8f468-28ec-4238-8713-5688160f54ab)\r\nThe log-log plot is computation time (seconds) vs data size N. The horizontal line and text labels shows the size N which each method is able to handle in 1 second. Because all the methods have the same asymptotic slope, that indicates they all have the same asymptotic complexity, but differ by constant factors. data.table could reduce these constant factors, ideally to be as fast as, or faster than tibble.","comments":["Related but different issues: \r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/3735\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/4485\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/4733\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/3735\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues?page=2&q=is%3Aopen+is%3Aissue+label%3Aperformance","Here is another more detailed test, which compares tibble\/data.frame with 1-2 columns, to data.table with 2 columns (all queries matching one of the rows).\r\n```r\r\nlibrary(data.table)\r\nworkaround.result <- atime::atime(\r\n  N=10^seq(1, 7, by=0.5),\r\n  setup={\r\n    N.v <- 1:N\r\n    N.ids <- paste0(\"cg\", sprintf(\"%06d\",N.v-1))\r\n    N.d <- data.frame(row.names=N.ids, N.v)\r\n    set.seed(1)\r\n    matching.str <- sample(N.ids, N, replace=F)\r\n    N.dt <- data.table(N.d, N.ids, key=\"N.ids\")\r\n    N.tib <- tibble::tibble(N.d)\r\n    N.d2 <- data.frame(N.d, N.ids)\r\n    N.tib2 <- tibble::tibble(N.d2)\r\n  },\r\n  base=N.d[match(matching.str, rownames(N.d)),,drop=F],\r\n  base2=N.d2[match(matching.str, rownames(N.d)),,drop=F],\r\n  data.table=N.dt[matching.str],\r\n  tibble=N.tib[matching.str,],\r\n  tibble2=N.tib2[matching.str,],\r\n  seconds.limit=1)\r\nworkaround.refs <- atime::references_best(workaround.result)\r\nplot(workaround.refs)+ggplot2::ggtitle(\"matching.str\")\r\nworkaround.pred <- predict(workaround.refs)\r\nplot(workaround.pred)+ggplot2::ggtitle(\"matching.str\")\r\n```\r\n![image](https:\/\/github.com\/Rdatatable\/data.table\/assets\/932850\/f232915d-fb46-4239-8705-823fae1f1eff)\r\nThe plot above shows the asymptotic reference lines, which indicate that tibble is definitely log-linear time, O(N log N), whereas data.table and base R seem to be asymptotically slower than that, which is confirmed by the differing slopes in the plot below,\r\n![image](https:\/\/github.com\/Rdatatable\/data.table\/assets\/932850\/6ec67487-4f6b-422c-9d04-de69971da9d6)\r\n\r\nThis indicates a sub-linear asymptotic difference (meaning data.table is faster than log-quadratic, but it looks like it is slower than log-linear).\r\n","@jangorecki @MichaelChirico @ben-schwen do any of you know what part of `[.data.table` is responsible for selecting rows which match i=character_vector on key? I tried looking at the code to see what the expected asymptotic complexity should be, but I was not sure what part of the code is responsible for this behavior.","AFAIR `dt[(key)]` does actually perform a join on the key column. I did not do any profiling but I would guess that the binary merge here\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/6c1fd839e0e0257d13f07975ead0fe6fdfee2f61\/R\/data.table.R#L507\r\n\r\ntakes most of the time. #4538 has the potential to improve this a lot but seems like a big pile of work","This is worst case scenario for a join, right? All target values are unique, so there is a lot of overhead to merge the two only to not get any benefit from the approach. `data.table` would be stronger with doing `as.factor` or using a dataset with less cardinality. \r\n\r\nAs far as improvements, I don't think character columns that are unique are strong candidates for keys. I feel like there are other examples of character subsetting not being the most performant. See also #3928 .\r\n\r\n","#4386 can bring some speed up because bmerge will not need to check for utf8 or ascii, that info will be collected in forder already. Although this has to be implemented in bmerge once #4386 is merged. Covered by https:\/\/github.com\/Rdatatable\/data.table\/issues\/4479"],"labels":["performance"]},{"title":"Hyperlinks to navigate between Vignettes?","body":"While going through the vignettes in a start-to-finish or continuous manner, one thing that may pose a mild inconvenience is navigating amongst the vignettes in order, also given that the statements toward the end of those connected vignettes (.N - 1) mention continuity (as in saying '*... in the next vignette ...*'), and could do with links to the next one. \r\n\r\nSince there are none, the only way for the reader to follow along (as far as I am aware of) is to either navigate back to the [wiki](https:\/\/github.com\/Rdatatable\/data.table\/wiki\/Getting-started) page if following that, or back to [here](https:\/\/rdatatable.gitlab.io\/data.table\/) if following the documentation hosted on gitlab.io (and then from the 'Vignettes' dropdown, proceed to select the next one, going below one step at a time). This back-and-forth flow can be avoided.\r\n\r\nAnd if not following those two starting points (highly unlikely, but could be possible), the specific order to sequentially follow among the connected vignettes is usually absent in the order they are enlisted by common commands (`vignette(package=\"data.table\")`, `browseVignettes(\"data.table\")`, `tools::getVignetteInfo(\"data.table\")[,3]`, etc.), which may be a tad confusing for beginners as to which one to jump to next (admittedly a rare case).\r\n\r\nBut again, navigating back one page and then hopping to the next one would only work assuming the reader began reading the vignette inside a browser in the first place, as going through the material within an R session (by say using `vignette(\"datatable-intro\", package=\"data.table\u201d)` inside RStudio) would invalidate this hyperlink to stay within, and move to the browser instead (perhaps this was a part of the rationale to not link?), although again there would be no direct way to navigate to the next vignette, and in this case even to go back to the 'Getting started' wiki or landing page for the gitlab.io hosted documentation. \r\n\r\nThus, I think it might be beneficial to include hyperlinks for at least transitioning between vignette chapters for user-friendliness.","comments":["order of vignettes on CRAN is alphabetical https:\/\/cloud.r-project.org\/web\/packages\/data.table\/\r\n```\r\n<html><body>\r\n<!--StartFragment-->\r\nVignettes: | Benchmarking data.table Frequently asked questions Importing data.table Introduction to data.table Keys and fast binary search based subset Reference semantics Efficient reshaping using data.tables Using .SD for Data Analysis Secondary indices and auto indexing\r\n-- | --\r\n\r\n\r\n<!--EndFragment-->\r\n<\/body>\r\n<\/html>Vignettes: \t[Benchmarking data.table](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-benchmarking.html)\r\n[Frequently asked questions](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-faq.html)\r\n[Importing data.table](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-importing.html)\r\n[Introduction to data.table](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-intro.html)\r\n[Keys and fast binary search based subset](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-keys-fast-subset.html)\r\n[Reference semantics](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-reference-semantics.html)\r\n[Efficient reshaping using data.tables](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-reshape.html)\r\n[Using .SD for Data Analysis](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-sd-usage.html)\r\n[Secondary indices and auto indexing](https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-secondary-indices-and-auto-indexing.html)\r\n```\r\nwe could consider numbering the vignettes, to make them appear in a specific order. just prefix vignette name with 01 02 etc","Numbering doesn't make sense because some are completely unrelated to each others. Better to just call them by the name rather than \"next\"\/\"previous\".","\"Soon\", R will support arbitrary vignette ordering:\r\n\r\nhttps:\/\/bugs.r-project.org\/show_bug.cgi?id=18576\r\n\r\nThe patch appears ready, it might be good to chime in & nudge that towards merge.","@MichaelChirico question if that new feature will be backward compatible or will required to depends on R 4.4","> @MichaelChirico question if that new feature will be backward compatible or will required to depends on R 4.4\r\n\r\nI guess you mean \"is it an error to use this feature with an older version of R\", I think worth asking in bugzilla."],"labels":["documentation"]},{"title":"Rename .Rraw to just .R?","body":"Is there any reason not to use the \"normal\" R extension on these files? I don't see anything that distinguishes them from other .R files. The disadvantages of .Rraw are (1) syntax highlighting is disabled by most software that would usually understand the .R extension and (2) new contributors may be confused about what's so special about these files that they get an unknown extension. I don't see any advantages.","comments":["It may be not good idea considering how big Rraw files are. From the perspective of requiring a lot resources for an IDE. RStudio could possibly try to figure out what packages are being used in the script to suggest to install missing ones.","After renaming we could also remove the .gitattributes file:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/34e02f26cfecd63fe18a90e0150f7c5c2998c01f\/.gitattributes#L2","> It may be not good idea considering how big Rraw files are. From the perspective of requiring a lot resources for an IDE. RStudio could possibly try to figure out what packages are being used in the script to suggest to install missing ones.\r\n\r\nFWIW:\r\n\r\n```\r\nf = \"inst\/tests\/tests.Rraw\"\r\nsystem.time(getParseData(parse(f)))\r\n#    user  system elapsed \r\n#   0.237   0.000   0.238\r\n\r\nsystem.time(xmlparsedata::xml_parse_data(parse(f)))\r\n#    user  system elapsed \r\n#   3.779   0.004   3.792\r\n\r\nsystem.time(lintr::get_source_expressions(f))\r\n#    user  system elapsed \r\n#  28.767   0.264  29.056 \r\n\r\nsystem.time(lintr::lint(f, lintr::missing_package_linter()))\r\n#    user  system elapsed \r\n#  29.535   0.627  30.234 \r\n```\r\n\r\ni.e. {lintr} takes quite a while to run `missing_package_linter()` on the file, but most of that is consumed just getting a normalized cache-optimized data structure. After that the actual lint logic runs in <1 second. Pure R parse data of the file also takes .25s. So presumably RStudio could run such script in <1 second total.","I would also look for the initial reason why Rraw was used rather than R.","It was changed back in 2011 648ea23 for \"Name change tests.R to tests.Rraw to pass R CMD check on 64bit R 2.14.0 (using Amazon EC2)\"\r\n\r\nSeems related to testthat \r\n\r\nHere's the comment\r\n\r\n```\r\n# As from v1.7.2, testthat doesn't run the tests.Rraw (hence file name change to .Rraw).\r\n# There were environment issues with system.time() (when run by test_package) that only\r\n# showed up when CRAN maintainers tested on 64bit. Matthew spent a long time including\r\n# testing on 64bit in Amazon EC2. Solution was simply to not run the tests.R from\r\n# testthat, which probably makes sense anyway to speed it up a bit (was running twice\r\n# before).\r\n```","Interesting, I never knew we once used {testthat}:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/648ea2389c31b683f59af725150ef033bb195f33\/DESCRIPTION#L8\r\n\r\nIt looks like that reason is no longer relevant then -- definitely comfortable with renaming.","Hey there! i want to solve this  issue, and it seems like a straightforward fix \u2013 just switching up the file extensions from \".Rraw\" to \".R\". Here's the lineup of all those \".Rraw\" files:\r\n\r\n```\r\nfroll.Rraw \r\ntests.Rraw\r\nother.Rraw \r\ntypes.Rraw \r\nnafill.Rraw \r\nbenchmark.Rraw \r\nprogramming.Rraw\r\n```\r\n is there anything else to be done for this?","Thanks again Nitish! Here is another issue that will create a lot of merge conflicts with existing PRs, so it's on the back burner for now. I'll try and add milestone 1.17.0 to issues that need to wait for the current PR queue to clear a bit before addressing.","> Thanks again Nitish! Here is another issue that will create a lot of merge conflicts with existing PRs, so it's on the back burner for now. I'll try and add milestone 1.17.0 to issues that need to wait for the current PR queue to clear a bit before addressing.\r\n\r\nThanks for info, i will work on something else for now  :) , and if you don't mind can you please suggest some issue i can work on as a beginner , because  right now  i am trying to get familiar with codebase by solving issues."],"labels":["internals","tests"]},{"title":"`value.var` columns are not present in the output of `dcast` even if they are included  in LHS of formula through `...`","body":"Hi,\r\n\r\n**Update**\r\n\r\n`dcast` output does not include last column of input data when it is included  in LHS of formula through `...`, `value.var` is missing and `fun.aggregate` is `length`.\r\n\r\nThis can be generalised, since under these conditions (`value.var` is missing and `fun.aggregate` is `length`), `value.var` becomes the last column. Therefore, the issue is that any `value.var` column will not be included in the output if it is not explicitly mentioned in the formula.\r\n\r\n\r\n\r\n**Issue as initially found:**\r\nI will explain this bug by means of a reproducible example, since I am not sure of its cause (and so, of the title of the issue).\r\n\r\nThe code\r\n```r\r\nlibrary(data.table)\r\ntest <- data.table(\"index\" = rep(letters[1:2],2), \"indexU\" = rep(letters[11:12],2), \"var1\" = rnorm(4,0,1), \"var2\" = rnorm(4,0,1))\r\n#print(test)\r\n\r\ntest <- dcast(test[, r := .I], r + ... + index ~ index, fun = length)\r\n#print(test)\r\n\r\ntest <- dcast(test, r + ... + indexU ~ indexU, fun = length)\r\n#print(test)\r\n```\r\nThe initial `test` data.table:\r\n```r\r\n   index indexU       var1       var2\r\n1:     a      k -0.6914432 1.06764510\r\n2:     b      l  1.5993563 0.33203747\r\n3:     a      k -1.4733880 0.07003668\r\n4:     b      l -0.2550293 1.67193531\r\n```\r\n`index` are the letters a,b and `indexU` are the letters k,l (where I place letters, it could be any categorical variable; these are character, but they may be factor class).\r\n\r\nThe first `dcast` works nicely, producing dummy variables for `index` categories as expected (thanks to https:\/\/stackoverflow.com\/a\/18881195\/997979), while keeping `index` variable. Its output:\r\n```r\r\n   r indexU       var1       var2 index a b\r\n1: 1      k -0.6914432 1.06764510     a 1 0\r\n2: 2      l  1.5993563 0.33203747     b 0 1\r\n3: 3      k -1.4733880 0.07003668     a 1 0\r\n4: 4      l -0.2550293 1.67193531     b 0 1\r\n```\r\n\r\nBut when I apply the second `dcast`, it does what expected for `indexU`, but it removes last dummy variable, `b`:\r\n```r\r\n   r       var1       var2 index a indexU k l\r\n1: 1 -0.6914432 1.06764510     a 1      k 1 0\r\n2: 2  1.5993563 0.33203747     b 0      l 0 1\r\n3: 3 -1.4733880 0.07003668     a 1      k 1 0\r\n4: 4 -0.2550293 1.67193531     b 0      l 0 1\r\n```\r\n\r\nWhy? How can I avoid this behaviour? \r\n\r\nThanks!\r\n\r\n\r\n\r\nI have just updated to `data.table` 1.14.10, but it happened to me with development version 1.14.9.\r\n\r\n```r\r\nR version 4.3.2 (2023-10-31 ucrt)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19045)\r\n\r\nMatrix products: default\r\n\r\n\r\nlocale:\r\n[1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8    LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                   LC_TIME=Spanish_Spain.utf8    \r\n\r\ntime zone: Europe\/Madrid\r\ntzcode source: internal\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.10\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.3.2 tools_4.3.2   \r\n\r\n```\r\n\r\n\r\n\r\n","comments":["Seems like an issue with ... in dcast formula\r\n?dcast says\r\n```\r\nThere are\r\n     two special variables: '.' represents no variable, while '...'\r\n     represents all variables not otherwise mentioned in 'formula';\r\n```\r\nThere is only one example with ...\r\n```r\r\n     DT <- data.table(v1 = rep(1:2, each = 6),\r\n                      v2 = rep(rep(1:3, 2), each = 2),\r\n                      v3 = rep(1:2, 6),\r\n                      v4 = rnorm(6))\r\n     dcast(DT, ... ~ v3, value.var=\"v4\") # same as v1+v2 ~ v3, value.var=\"v4\"\r\n```\r\nThere are a couple of tests with dcast and ...\r\n```\r\n  # FR #335 and DOC #332\r\n  set.seed(1L)\r\n  DT = data.table(a=sample(10), b=2013:2014, variable=rep(c(\"c\", \"d\"), each=10), value=runif(20))\r\n  test(1102.18, names(dcast(DT, a ~ ... + b, value.var=\"value\")), c(\"a\", \"c_2013\", \"c_2014\", \"d_2013\", \"d_2014\"))\r\n\r\n  # dcast.data.table new tests\r\n  # Fix for #1070 (special case of ... on LHS)\r\n  DT = data.table(label= month.abb[1:5], val=0)\r\n  test(1102.28, dcast(DT,... ~ label, value.var=\"val\", sum),\r\n                data.table(`.`=\".\", Apr=0, Feb=0, Jan=0, Mar=0, May=0, key=\".\"))\r\n\r\n```\r\n\r\nBelow is modification of original code, which shows that ... is not being interpreted as claimed (I manually changed the ... to all the other column names not mentioned in the formula)\r\n\r\n```r\r\n> (test <- data.table(index=c(\"a\",\"b\"), indexU=c(\"k\",\"l\"), var1=1:4, var2=5:8))\r\n    index indexU  var1  var2\r\n   <char> <char> <int> <int>\r\n1:      a      k     1     5\r\n2:      b      l     2     6\r\n3:      a      k     3     7\r\n4:      b      l     4     8\r\n> (out1 <- dcast(test[, r := .I], r + ... + index ~ index, fun = length))\r\nKey: <r, indexU, var1, var2, index>\r\n       r indexU  var1  var2  index     a     b\r\n   <int> <char> <int> <int> <char> <int> <int>\r\n1:     1      k     1     5      a     1     0\r\n2:     2      l     2     6      b     0     1\r\n3:     3      k     3     7      a     1     0\r\n4:     4      l     4     8      b     0     1\r\n> (out.dots <- dcast(out1, r + ... + indexU ~ indexU, fun = length))\r\nKey: <r, var1, var2, index, a, indexU>\r\n       r  var1  var2  index     a indexU     k     l\r\n   <int> <int> <int> <char> <int> <char> <int> <int>\r\n1:     1     1     5      a     1      k     1     0\r\n2:     2     2     6      b     0      l     0     1\r\n3:     3     3     7      a     1      k     1     0\r\n4:     4     4     8      b     0      l     0     1\r\n> (out.no.dots <- dcast(out1, r + var1 + var2 + index + a + b + indexU ~ indexU, fun = length))\r\nKey: <r, var1, var2, index, a, b, indexU>\r\n       r  var1  var2  index     a     b indexU     k     l\r\n   <int> <int> <int> <char> <int> <int> <char> <int> <int>\r\n1:     1     1     5      a     1     0      k     1     0\r\n2:     2     2     6      b     0     1      l     0     1\r\n3:     3     3     7      a     1     0      k     1     0\r\n4:     4     4     8      b     0     1      l     0     1\r\n```","Thanks @tdhock!\r\n\r\nThe issue seems to be when\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/f1be897218821ffd631bad4443e24fc56c85fd81\/R\/fcast.R#L119\r\n\r\nzooming, in\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/f1be897218821ffd631bad4443e24fc56c85fd81\/R\/fcast.R#L37\r\n\r\nand so in\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/f1be897218821ffd631bad4443e24fc56c85fd81\/R\/fcast.R#L40-L51\r\n\r\nsince, debugging `dcast(out1, r + var1 + var2 + index + a + b + indexU ~ indexU, fun = length)`) one gets\r\n```r\r\nvarnames\r\n[1] \"r\"      \"indexU\" \"var1\"   \"var2\"   \"index\"  \"a\"      \"b\"     \r\n> allvars\r\n[1] \"r\"      \"var1\"   \"var2\"   \"index\"  \"a\"      \"b\"      \"indexU\" \"b\"     \r\n> deparse_formula(as.list(formula)[-1L], varnames, allvars) |> print()\r\n[[1]]\r\n[[1]][[1]]\r\nr\r\n\r\n[[1]][[2]]\r\nvar1\r\n\r\n[[1]][[3]]\r\nvar2\r\n\r\n[[1]][[4]]\r\nindex\r\n\r\n[[1]][[5]]\r\na\r\n\r\n[[1]][[6]]\r\nb\r\n\r\n[[1]][[7]]\r\nindexU\r\n\r\n\r\n[[2]]\r\n[[2]][[1]]\r\nindexU\r\n\r\n```\r\nwhile debugging the problematic `dcast` (`dcast(out1, r + ... + indexU ~ indexU, fun = length)`) `b` is not in the output\r\n```r\r\n> varnames\r\n[1] \"r\"      \"indexU\" \"var1\"   \"var2\"   \"index\"  \"a\"      \"b\"     \r\nallvars\r\n[1] \"r\"      \"indexU\" \"b\"   \r\n> deparse_formula(as.list(formula)[-1L], varnames, allvars) |> print()\r\n[[1]]\r\n[[1]][[1]]\r\nr\r\n\r\n[[1]][[2]]\r\nvar1\r\n\r\n[[1]][[3]]\r\nvar2\r\n\r\n[[1]][[4]]\r\nindex\r\n\r\n[[1]][[5]]\r\na\r\n\r\n[[1]][[6]]\r\nindexU\r\n\r\n\r\n[[2]]\r\n[[2]][[1]]\r\nindexU\r\n```","cc @MichaelChirico, who has worked this function in some of its last commits (https:\/\/github.com\/Rdatatable\/data.table\/commit\/bdc6da98b6dc1276accd906d1ec0f0cd921bbfc4, https:\/\/github.com\/Rdatatable\/data.table\/commit\/44e20ba434ce5ee4b472b1667fe58132af10f06d)","thanks for reporting and investigating @iago-pssjd, it looks like you're close to understanding the issue, would you like to have a go at a PR? we are happy to help along the way.","@MichaelChirico  \r\n\r\nActually I am not sure where the issue is and I do not understand the \"function\" and \"interaction\" of the functions `check_formula` and `deparse_formula`.\r\nCould it be also due to the `allvars` argument passed to the function `deparse_formula`?\r\n\r\n\r\nIn the more nested `deparse_formula`, it returns `setdiff(varnames, allvars)`, so `\"var1\"   \"var2\"   \"index\"  \"a\"`, while `r` and `indexU` are got later, since they appear in the formula. But `\"b\"` is in `valnames` (in `check_formula` ), and so in `allvars`, not being in the formula it is not returned.\r\n\r\n`\"b\"` in `valnames` comes from `dcast`\r\n```r\r\nif (missing(value.var) && !missing(fun.aggregate) && identical(fun.aggregate, \r\n    length)) \r\n    value.var = names(data)[ncol(data)]\r\n  lvals = value_vars(value.var, names(data))\r\n  valnames = unique(unlist(lvals))\r\n```\r\nbut what is its function in `allvars`? And why nested `deparse_formula` returns `setdiff(varnames, allvars)`?\r\n\r\nMaybe, precisely in such case, `if (missing(value.var) && !missing(fun.aggregate) && identical(fun.aggregate, \r\n    length)) `, `valnames` passed to `check_formula` should be missing? This would seemingly solve this issue, were I do not want to aggregate through `\"b\"`, but there may be other casuistries.","BTW, I found the source of these functions in a commit introducing several features: https:\/\/github.com\/Rdatatable\/data.table\/commit\/25a74dfd8b738e5e5556144d38713b6b003a56f2","What makes dev easier is to use cc() function. You will find it in .dev dir. Then it's very straightforward to put extra print(); cc(); dcast(). cc() is fast rebuilding package so you don't wait for compilation.","> What makes dev easier is to use cc() function. You will find it in .dev dir. Then it's very straightforward to put extra print(); cc(); dcast(). cc() is fast rebuilding package so you don't want for compilation.\r\n\r\ngreat suggestion. where is cc() documented? I have never used it but I would like to. Can you please make a screencast next time you use it, so you could show others how?","> > What makes dev easier is to use cc() function. You will find it in .dev dir. Then it's very straightforward to put extra print(); cc(); dcast(). cc() is fast rebuilding package so you don't want for compilation.\r\n> \r\n> great suggestion. where is cc() documented? I have never used it but I would like to. Can you please make a screencast next time you use it, so you could show others how?\r\n\r\nIt's mentioned in Contributing wiki and documented in [.dev\/README.md](https:\/\/github.com\/Rdatatable\/data.table\/blob\/master\/.dev\/README.md)\r\n\r\nIn #5131 Jan and Matt also pointed out how they set their Rprofile to easily source `cc()`","FWIW we definitely see `...` used in LHS of `dcast()` in the wild:\r\n\r\nhttps:\/\/github.com\/search?q=lang%3AR+%2F%5Cn%5B%5E%23%5Cn%5D*%5B.%5D%5B.%5D%5B.%5D%5B%5E%22%27%7B%7D%5Cn%2C%5D*%5B%7E%5D%2F+%2Fdcast%5B%28%5D%2F+-path%3A.Rd&type=code\r\n\r\ne.g.\r\n\r\nhttps:\/\/github.com\/shahcompbio\/signals\/blob\/73f877382562b0a981e9418f3371163f6bf46a9f\/R\/combinedata.R#L28\r\nhttps:\/\/github.com\/ihmeuw\/ihme-modeling\/blob\/3e97f2c65399178a2f9de5113944a2684c4590a1\/gbd_2021\/mortality_code\/harmonize-implied-em\/upload_cc_draws.R#L93"],"labels":["reshape"]},{"title":"bump stated R dependency?","body":"I am quite happy staying on R 3.1.0, so it is not like we need to upgrade.\r\n\r\nOne practical aspect could be to simplify CI. Yet we have couple of CI jobs (3.1, 3.4.4, 3.5) testing different corner cases present of each of those versions. It turned out that CI minutes got now quite limited for free plans...\r\n\r\nBumping to 4.0.0 will allow us to use reference counting, although I am not sure if we have dev time to really work on that. That would be also huge bump of 6 years, from supporting environments set in 2014 to 2020.\r\nSuch big change could be also postponed to be introduced when major breaking changes would be landing in master branch as well. Otherwise bumping to 3.5 is some middle step.\r\n\r\nBefore any change we should definitely investigate what R version are data.table users using based on data from http:\/\/cran-logs.rstudio.com (see @arunsrinivasan 's https:\/\/github.com\/arunsrinivasan\/cran.stats)\r\n\r\nMy personal preference would be to support as old as feasible R version, possibly removing R 3.4.4 and R 3.5.0 CI jobs, and leaving R 3.1.0 job.","comments":["I would think that if CI minutes are limited we should test \r\n- oldest supported R version\r\n- current R-release\r\n- current R-devel\r\nmaybe keep the 3.4.4, 3.5 CI for now, and remove them later if we run into the CI time limit?","Rather than 3.4.4 and 3.5. I would prefer to have 6 jobs win\/macos * release\/devel\/oldrel so we can provide more binaries.\r\n\r\nAlso #5745 should help to reduce some minutes, so we don't have to freeze suggested deps and risk of breaking change sneaks unnoticed.","are you able to check how many minutes we are using on github actions? I can see that for my own account under settings -> billing and plans (but it says zero so i'm not sure that is correct). but I can not see usage for Rdatatable org.","No I am not able to check for Rdatatable org. I am able to check, same as you, for my own namespace.\r\nIt says\r\n```\r\n2,000 Actions minutes\/month\r\n500MB of Packages storage\r\n```\r\n\r\nOn GitLab free plan, as Rdatatable org, we have\r\n```\r\n50 000 minutes\/month\r\n10 GB\r\n```\r\n\r\nAs a user, I have also 2000 minutes\/month, but 10GB of storage\r\n\r\nSo the question is, how many compute minutes we get as Rdatatable org on github. For that I believe we need Matt as we privileges only on repository, an not the org.","Keeping support for very old R is great, but I don't think it makes sense to keep tracking arbitrarily old R version indefinitely. We are just signing ourselves up for ever-increasing tech debt with limited benefit. **Eventually people need to upgrade R**.\r\n\r\nBetter to set a policy of support and gradually start bringing our dependency forward.\r\n\r\nR 3.1.0 is nearly 10 years old:\r\n\r\nhttps:\/\/github.com\/wch\/r-source\/tree\/tags\/R-3-1-0\r\n\r\nI think even a policy of 5 years old ([R 3.5.0](https:\/\/github.com\/wch\/r-source\/tree\/tags\/R-3-5-0)) is quite generous, but 6 or 7 would also be fine.\r\n\r\nI don't think it's reasonable to expect {data.table} to cater to decade-old installations -- archived versions of {data.table} exist for this reason.","Let's see if there are active users of data.table on R 3.1.0. Sticking blindly to 5 (or 6 or 7) years rule doesn't sound to be great idea.\r\n\r\nNot sure what package that was, but once in production I had to use own fork of a package, and the only change was pushing stated dependency to older version. Everything worked fine. We don't want our users to be forced to do tricks like this by blindly following a \"5 years rule\".\r\n\r\nI do believe we should bump stated dependency but when we are ready to follow up with benefits it gives.","> when we are ready to follow up with benefits it gives\r\n\r\nso far we are too \"greedy\" about this. 3.2.0 does not bring (us) many direct benefits vs 3.1.0. But it gets us closer to more recent R where there are larger benefits. And I would rather gradually hit 3.2.0, 3.3.0, ... than jump suddenly to 3.6.0.\r\n\r\n> Everything worked fine\r\n\r\nI'm not worried about this. we are quite good about earmarking which code can be updated once we depend on certain R version. We will very quickly become incompatible with older R upon upgrade.","Is there a way (that I'm not aware of) for us to know what versions of R are downloading data.table without a survey?","Yes, described in the first post. CSV files have that field.","From December 2022 to November 2023. 365 days, 280 valid days (maybe missing or network error)\r\n```r\r\nl = list.files() # obtained from cran.stats\r\nd = rbindlist(lapply(l, function(f) {cat(f,\"\\n\",sep=\"\"); fread(f, showProgress=FALSE)[package==\"data.table\", .N, r_version]}))\r\nd[,sum(N),substr(r_version,1,3)][order(-V1)]\r\n```\r\n```\r\n 1: <NA> 2217752 3.364596e+01\r\n 2:  4.2 2010680 3.050443e+01\r\n 3:  4.3 1468433 2.227789e+01\r\n 4:  4.1  446147 6.768585e+00\r\n 5:  4.0  188071 2.853262e+00\r\n 6:  3.6  149188 2.263361e+00\r\n 7:  3.4   53199 8.070926e-01\r\n 8:  3.5   32396 4.914862e-01\r\n 9:  4.4   14310 2.170999e-01\r\n10:  3.3   10819 1.641372e-01\r\n11:  3.2     437 6.629814e-03\r\n12:  3.1       5 7.585599e-05\r\n```\r\nalmost 1% of users were on 3.4.\r\n0.1% were on 3.3\r\n","What do we know about the source for this data?\r\n\r\nE.g. we see 5 people on 3.1, how likely is it that's just someone like us running really old R for testing purposes?\r\n\r\n(either way I think it's clear we can bump to 3.2 ASAP and 3.3 in the subsequent release)","This is from cloud.r-project.org, which is widely used in CI setups. Therefore bias for 4.4, 4.3 and 4.2 may be there. There are tens of different mirrors so it is just a, little biased, sample :)"],"labels":["question"]},{"title":"rolling functions give.names could accept character vector","body":"character vector would simply provide names for the output","comments":[],"labels":["froll"]},{"title":"Cannot add list column to empty `data.table`","body":"Both inplace operations currently do not allow adding lists of lengths over 1. The problem arises on both `1.14.8` and current dev.\r\n\r\n``` r\r\nlibrary(data.table)\r\ndt = data.table()\r\nx = list(\"a\", 1)\r\ndt[, a := x]\r\n#> Error in `[.data.table`(dt, , `:=`(a, x)): Supplied 2 items to be assigned to 1 items of column 'a'. If you wish to 'recycle' the RHS please use rep() to make this intent clear to readers of your code.\r\nset(dt, j=\"a\", value=x)\r\n#> Error in set(dt, j = \"a\", value = x): Supplied 2 items to be assigned to 1 items of column 'a'. If you wish to 'recycle' the RHS please use rep() to make this intent clear to readers of your code.\r\n```\r\n\r\nAdding columns of other types works fine\r\n``` r\r\nlibrary(data.table)\r\ndt = data.table()\r\ndt[, b := 1:5][]\r\n#>        b\r\n#>    <int>\r\n#> 1:     1\r\n#> 2:     2\r\n#> 3:     3\r\n#> 4:     4\r\n#> 5:     5\r\n\r\ndt = data.table()\r\ndt[, c := letters[1:5]][]\r\n#>         c\r\n#>    <char>\r\n#> 1:      a\r\n#> 2:      b\r\n#> 3:      c\r\n#> 4:      d\r\n#> 5:      e\r\n```","comments":["I think the result makes sense; you just need to grad it into another list as you can see below.\r\n```\r\n> library(data.table)\r\n> dt = data.table()\r\n> x = list(\"a\", 1)\r\n> dt[, a := .(x)]\r\n> str(dt)\r\n\r\nClasses \u2018data.table\u2019 and 'data.frame':\t2 obs. of  1 variable:\r\n $ a:List of 2\r\n  ..$ : chr \"a\"\r\n  ..$ : num 1\r\n - attr(*, \".internal.selfref\")=<externalptr> \r\n \r\n > dt\r\n   a\r\n1: a\r\n2: 1\r\n```","@AngelFelizR I know how to work around the issue but `list(x) != x` e.g. on matrix columns...\r\n\r\n```r\r\nlibrary(data.table)\r\ndt = data.table()\r\nx = list(diag(2))\r\ndt[, a := x][]\r\n#> Warning in `[.data.table`(dt, , `:=`(a, x)): 2 column matrix RHS of := will be\r\n#> treated as one vector\r\n#>        a\r\n#>    <num>\r\n#> 1:     1\r\n#> 2:     0\r\n#> 3:     0\r\n#> 4:     1\r\n\r\ndt = data.table()\r\nx = list(diag(2))\r\ndt[, a := list(x)][]\r\n#>          a\r\n#>     <list>\r\n#> 1: 1,0,0,1\r\n```","You just need to be aware the error make sense in the default use of list that we use with lapply\r\n\r\n```r\r\n> dt = data.table()\r\n> x = list(\"a\", 1)\r\n> dt[, c(\"a\",\"b\") := x]\r\n> str(dt)\r\n\r\nClasses \u2018data.table\u2019 and 'data.frame':\t1 obs. of  2 variables:\r\n $ a: chr \"a\"\r\n $ b: num 1\r\n - attr(*, \".internal.selfref\")=<externalptr> \r\n```","I believe this issue may be caused by a automatic wrapping input on RHS (or `value` arg) into `list` call, which we do for user convenience.\r\nThere may inherited ambiguity caused by that.\r\n\r\nWhat if we want to add list of lists column? or list of list of lists column?\r\n\r\nAtomic columns are automatically enlisted, that's why code below works\r\n```r\r\nx[ c(\"a\",\"b\") := list(1:2, 2:3)]\r\nx[ c(\"a\") := 1:2]\r\n```\r\nto avoid this inherited ambiguity we would have to ask users to always write `list`, even for single column\r\n```r\r\nx[ c(\"a\") := list(1:2)]\r\n```\r\n\r\nI am afraid trying to fix this edge case we are just shifting problem to another edge case, but I am not against, as long as we have good unit tests included and revdeps passes.\r\n\r\nI may also be wrong, as the issue precisely mentions scalar columns, maybe this part we can fix without shifting problem elsewhere.","That would be a breaking change and I am not sure if that would improve the general experience","The current situation is the following:\r\n`SEXP assign(SEXP dt, SEXP rows, SEXP cols, SEXP nwecolnames, SEXP values)` checks the number of `nrow` of `dt`.\r\n\r\nCase 1: `dt` has already some rows, so it simply takes the number of elements in the first column.\r\n\r\nCase 2: `dt` has no rows yet. \r\na) `values` is a list, so `nrow` is set to the length of first element of `values`\r\nb) `values` is not a list, so `nrow` is set to the length of `values`\r\n\r\nI want to extend Case 2a) to better distinguish whether a list column is added or multiple columns (`length(values)` should be the same length of affected cols)\r\n\r\nedit: What is still open are weird cases of trying to add columns to a 0-row `data.table`"],"labels":["non-atomic column"]},{"title":"fread() cannot read \" \" as NA","body":"Possible Bug Report:\r\nHow can we fread(..., na.strings=\" \")?\r\nI see many open issues around \"na.strings\" but not this one I believe.\r\n\r\nIn this example I want is.na(fread(\"na_test.csv\")[1,V2]) == TRUE.\r\n\r\n> x <- matrix(c(\"0\",\"\",\" \",\"NA\"), ncol = 2)\r\n\r\nx\r\n     [,1] [,2]\r\n[1,] \"0\"  \" \" \r\n[2,] \"\"   \"NA\"\r\n\r\n> write.csv(x,\"na_test.csv\")\r\n> fread(\"na_test.csv\")\r\n\r\n   V1 V1 V2\r\n1:  1  0   \r\n2:  2 NA NA\r\n\r\n> fread(\"na_test.csv\", na.strings=\"\")\r\n\r\n   V1 V1 V2\r\n1:  1  0   \r\n2:  2 NA NA\r\n\r\n> fread(\"na_test.csv\", na.strings=\" \")\r\n\r\n   V1 V1 V2\r\n1:  1  0   \r\n2:  2 NA NA\r\n\r\nWarning message:\r\nIn fread(\"na_test.csv\", na.strings = \" \") :\r\n  na.strings[1]==\" \" consists only of whitespace, ignoring. Since strip.white=TRUE (default), use na.strings=\"\" to specify that any number of spaces in a string column should be read as \\<NA\\>.\r\n\r\n> fread(\"na_test.csv\", na.strings=\" \", strip.white=FALSE)\r\n\r\nError in fread(\"na_test.csv\", na.strings = \" \", strip.white = FALSE) : \r\n  na.strings[1]==\" \" consists only of whitespace, ignoring. But strip.white=FALSE. Use strip.white=TRUE (default) together with na.strings=\"\" to turn any number of spaces in string columns into \\<NA\\>.\r\n","comments":["Any solution so far?","No, this issue will be the first place where any news about it will appear","hi, i believe the text `NULL` might the same issue as the text ` ` does as reported in this issue?  here's a minimal reproducible example showing behavior that might be related?  https:\/\/stackoverflow.com\/q\/78013004\/1759499  thanks!!\r\n\r\n"],"labels":["fread"]},{"title":"join big tables takes too long","body":"Detected by db-benchmark. Join 1e9 for data.table does not finish within 4h. It used to not finish this test because of OOM when run on 128GB mem machine. Now it runs on 250GB so I would expect it to finish. Considering that joins should scales pretty well in data.table it sounds abnormal that 4h is not enough.\r\n\r\nc6id.metal machine\r\n250 GB ram\r\n128 cores\r\nhttps:\/\/github.com\/duckdblabs\/db-benchmark\/pull\/61\r\n\r\nif someone has access to c6id.metal, it would be useful to run join script using verbose=TRUE, and also extend timeout, to see if maybe little more time is enough for completion.","comments":["@Anirban166 @DorisAmoakohene this would be an interesting example to investigate, perhaps using atime","I can't quite follow that repo. What's the actual minimal code for the join that the benchmark is measuring? ","https:\/\/github.com\/duckdblabs\/db-benchmark\/blob\/master\/datatable\/join-datatable.R\r\nthis is the test script\r\n\r\ndata needs to be generated with\r\n```\r\nRscript _data\/join-datagen.R 1e9 NA 0 0\r\n```"],"labels":["benchmark"]},{"title":"Improve := error for case when cedta() dispatches [.data.frame","body":"The current error is not very helpful for a package author that forgot to `@import` data.table leading [.data.frame to dispatch as cedta() gives FALSE. So := is used correctly, but still get the error.\r\n\r\nDifficult (not impossible) to encounter once the package reaches CRAN caliber, more likely for package under early development.\r\n\r\nAlternatively, using := is a pretty good sign the author intended [.data.table, so we could consider changing the cedta() result, but I think that will be non-trivial.","comments":["What is the current error, and what do you suggest we change it to?","> What is the current error,\r\n\r\nThe normal error for when `:=` is invoked directly:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/e6076b02f746dd05d921ac355291fb42623f6c02\/R\/data.table.R#L2778\r\n\r\n> what do you suggest we change it to?\r\n\r\nNot sure yet, but the current message is misleading in this case (emphasis on \"ensure `x` is a data.table\", but `x` is in fact a data.table!).","Is this error message good enough to fix this issue ?\r\n\r\n\r\n\"It seems that `:=` is being used directly. Make sure that `data.table` is imported and used explicitly in your package. If you are intentionally using `:=` with a data table, ensure that the object being modified is indeed a data table. If you are encountering this error unexpectedly, it may be due to `data.table` methods being dispatched as `data.frame` methods. Check that `is.data.table(DT)` returns `TRUE` for your object `DT`. For more information on using `:=` and data tables, refer to the documentation by running `?`:=``.\"\r\n\r\n","> imported and used explicitly in your package\r\n\r\nBut this error might not come up in a package, or it might be caused by an issue in another package entirely.\r\n\r\nI am thinking the best way to address this may be in `[.data.table` itself, e.g. in this branch:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/3eefbcaa47a1c2fc28037574aee27992d083750e\/R\/data.table.R#L146-L155\r\n\r\nQuickly scan `j` for `:=` (and `let`), and throw a specific error if needed along the lines \"`[ was called on a data.table in an environment that is not data.table-aware (i.e. `cedta()`), but `:=` was used, implying the owner of this call really intended for data.table methods to be called. See vignette(\"datatable-importing\") for details on properly importing data.table\".\r\n\r\nOTOH, maybe using `:=` in `j` is a sign that `cedta()` should pass. If caller is using `:=`, it should be pretty clear they are data.table-aware. WDYT @jangorecki?","Isn't \/ wasn't `:=` operator used in some tidyverse package as well?\r\nI think they used it to substitute LHS of assignment because `:=` is lazy on LHS and RHS, unlike `=` where you cannot catch LHS. I don't follow API changes in tidyverse, and I think that may have been changed already, because there are better ways to achieve (like we did inside `env` var).","> Isn't \/ wasn't `:=` operator used in some tidyverse package as well?\n\nYes, it's used within their `mutate()` function for lazy evaluation. Pretty sure it's still implemented.\n","> I think they used it to substitute LHS of assignment because `:=` is lazy on LHS and RHS, unlike `=` where you cannot catch LHS.\r\n\r\nyes that's still surely used where needed. my suggestion is to look for := as root of j though, in same fashion as later in [.data.table, instead of recursive search which might find tidyverse false positives."],"labels":["documentation"]},{"title":"`fwrite()` should (possibly invisibly) return the file path ","body":"Now `fwrite()` returns `invisible()`, but I think it might be more useful if it returned the path to the created file. It's a trivial change and I don't think it would create issues with backward compatibility, since it's probably not likely that any code relies on  `fwrite()` not returning anything (although a similar change to `ggsave()` [did break a few scripts](https:\/\/github.com\/tidyverse\/ggplot2\/issues\/3379) \ud83e\udd23).","comments":["why the file path as opposed to `x`?\r\n\r\nthe latter behavior would be more like `print()` where a function used for side effects returns it's main argument (https:\/\/design.tidyverse.org\/out-invisible.html)","I guess because then you can work with the results of the write in the pipeline. It would be analogous to `fs::file_create()` in that page. Also, you already have easy access to `x` in your environment, while the path to the file might not be available since it might be created on based on data. For instance:\r\n\r\n```r\r\nDT[, fwrite(.SD, group), by = group]\r\n```\r\n\r\nWill create a bunch of files, but you don't have access to them (the result is an empty data.table). If `fwrite()` returned the created file you could do \r\n\r\n\r\n```r\r\nfiles <- DT[, fwrite(.SD, group), by = group]\r\n```\r\nand then use `files` in your code easily. ","> It would be analogous to `fs::file_create()`\r\n\r\nNot really, because the \"main argument\" of `file_create()` is the file path. The main argument for `fwrite()` is `x`.\r\n\r\n> you already have easy access to x in your environment\r\n\r\nNot necessarily, e.g. `fwrite(DT[...], file)` where now the output can be any table.\r\n\r\nYour code can be done currently like:\r\n\r\n```r\r\nfiles <- DT[, fwrite(.SD, .BY$group), by = group]$group\r\n```\r\n\r\nMore generally, I think it's just a difference between code like:\r\n\r\n```r\r\nDT[, by = group, {\r\n  ...\r\n  fwrite(.SD, file, ...)\r\n  .SD\r\n}]\r\n```\r\n\r\nand\r\n\r\n```r\r\nDT[, by = group, {\r\n  ...\r\n  fwrite(.SD, file, ...)\r\n  file\r\n}]\r\n```\r\n\r\nSo I'm still not sure why to prioritize one over the other.\r\n\r\nOne way to approach this empirically would be to find out which object is used more:\r\n\r\nhttps:\/\/github.com\/search?q=lang%3AR+%22fwrite%28%22+%22.SD%22+%22%7B%22&type=code","Oh, sorry, I meant `fs:file_copy(from, to)`, which returns `to` instead of `from`. \r\n\r\nOne issue with your examples is that you only grab the file name and not the full path. \r\n\r\nFor `fwrite(DT[...], file)`, you could always do `fwrite(saved_data <- DT[...], file)` .  I'd argue that if you're computing something inside the call to `fwrite()` then you probably don't care about the output in the rest of your code, otherwise you would save it to a variable and then save it. A more realistic example could be to save intermediate steps in a pipeline like:\r\n\r\n```r\r\nDT2 <- DT |> \r\n   _[...] |> \r\n   fwrite(\"file.csv\") |> \r\n   _[...]\r\n```\r\n\r\nBut the `%T>%` pipe might be more useful for that, as you are not relying on the return value of the intermediate function. \r\n\r\n  ","@eliocamp I think that adding a write step between several modification steps would affect code readability.\n\nIt will take more time to a new user to understand when we are saving the data in the script","I agree. That's why I think that `fwrite()` should return the path to the created file. So the next step after writing would be to do stuff with the file, which is relatively clear and straightforward. ","Got it.\n\nIt will prevent me from writing a for loop to achieve that task. I wouldn't have written the next code as it is really clever.\n\n```\nfiles <- DT[, fwrite(.SD, .BY$group), by = group]$group\n```\n\nIt can become much harder if we want to save the files in a folder, so returning the file's path seems to be a good a idea.\n\n```\nfiles <- DT[, fwrite(.SD,  paste0(\"data\/\", .BY$group, \".csv\")), by = group]\n```","Hi i want to work on this issue , @MichaelChirico I wanna know what you think should be returned `x` or `file path`,  and can you give me some idea about stuff need to be done for this issue :)","AFAIU there is no decision made yet what should be returned by `fwrite`. First step could be to scan popular packages and see what they do or what base does, e.g. check out the writing to disk methods for `utils::write.table`, `vroom`, `fst`, etc.","I would say if they return NULL then it is not very useful follow them. TRUE is already better result then NULL...","> I would say if they return NULL then it is not very useful follow them. TRUE is already better result then NULL...\r\n\r\nwhat do you think should be returned?","File name or path, depends what was provided"],"labels":["fwrite"]},{"title":"rolling median","body":"This branch was forked from rollfuns branch #5682 so till it is merged to master it is best to look at diff here https:\/\/github.com\/Rdatatable\/data.table\/compare\/rollfuns..rollmedian\r\n\r\n----\r\n\r\nThis is implementation of rolling median based on a novel the algorithm \"sort-median\" described by @suomela in 2014 in his paper [Median Filtering is Equivalent to Sorting](https:\/\/arxiv.org\/abs\/1406.1717). Thanks to Jukka for the paper and hints on extending the algo. \"sort-median\" scales very well, not only for size of input vector but also for size of rolling window. Implementation here is extended to address following limitations present in the paper:\r\n- size of input vector must by divisible by window size\r\n- window size must be uneven number","comments":["```r\r\nrollmedian = function(x, n) {\r\n  ans = rep(NA_real_, nx<-length(x))\r\n  if (n<=nx) for (i in n:nx) ans[i] = median(x[(i-n+1L):(i)])\r\n  ans\r\n}\r\nlibrary(data.table) ## uses 4 CPU threads\r\nset.seed(108)\r\nx = rnorm(1e5)\r\n\r\nn = 100\r\nsystem.time(rollmedian(x, n))\r\n#   user  system elapsed\r\n#  4.389   0.000   4.389\r\nsystem.time(frollapply(x, n, median, simplify=unlist))\r\n#   user  system elapsed\r\n#  5.603   0.163   1.465\r\nsystem.time(frollmedian(x, n))\r\n#   user  system elapsed\r\n#  0.016   0.000   0.009\r\n\r\nn = 1000\r\nsystem.time(rollmedian(x, n))\r\n#   user  system elapsed\r\n#  7.011   0.028   7.040\r\nsystem.time(frollapply(x, n, median, simplify=unlist))\r\n#   user  system elapsed\r\n#  8.720   0.173   2.240\r\nsystem.time(frollmedian(x, n))\r\n#   user  system elapsed\r\n#  0.015   0.004   0.009\r\n\r\nn = 10000\r\nsystem.time(rollmedian(x, n))\r\n#   user  system elapsed\r\n# 34.190   0.012  34.206\r\nsystem.time(frollapply(x, n, median, simplify=unlist))\r\n#   user  system elapsed\r\n# 40.500   0.456  10.288\r\nsystem.time(frollmedian(x, n))\r\n#   user  system elapsed\r\n#  0.023   0.016   0.016\r\n```","Benchmark vs pandas at https:\/\/github.com\/jangorecki\/rollbench","> Benchmark vs pandas at https:\/\/github.com\/jangorecki\/rollbench\r\n\r\nwhat's `pd2dt`? Run `frollmedian` from Python?","`pandas\/data.table`","Exact diff: https:\/\/github.com\/Rdatatable\/data.table\/compare\/a83b864c4b5af55a1f7139a2278622502ba2e488..f4e09b8741bcc9e30ff5b39fcafc9c4e68299a07"],"labels":["froll"]},{"title":"Add a CI check for translation quality","body":"To prevent regression on #5681. It will be easy enough, but waiting to invest while our CI is still broken at HEAD.","comments":["How does the check looks like? We could either keep it in ghcl where we check for codecov or in glci where we keep r cmd checks","It's open-ended (we can start building out quality checks), but to start with, just running `tools::checkPoFiles()`."],"labels":["ci","translation"]},{"title":"more rolling functions","body":"addressing more tasks from #2778\r\n\r\nbranched from adapt branch #5576 so till its merged to master it is best to look at diff here https:\/\/github.com\/Rdatatable\/data.table\/compare\/adapt..rollfuns","comments":["exact diff: https:\/\/github.com\/Rdatatable\/data.table\/compare\/319c02788d32169e1c9f26bbb59dee0012eaa5e1..a83b864c4b5af55a1f7139a2278622502ba2e488"],"labels":["froll"]},{"title":"Feature request: [fread] support for external file decompressor","body":"**Current Situation:**\r\nWhile `fwrite`'s gzip compression ist smooth and fast, `fread`'s decompression using R.utils is rather slow (at least on Windows).\r\nEven reading in only a few kb takes almost 3 seconds at my current machine, while importing the uncompressed file is finished in less than 0.01 s. Filtering the file through an external compressor instead like `fread('7z e -so example.txt.gz')` takes 0.3s.\r\n\r\nBasically there seems to be a general ~3s startup time required for import of all compressed files. In addition the decompression process itself - in particular for bz2 compressed files - seems to be slower than with filtering through 7z when files get somewhat larger. Furthermore, xz files are not supported at all by `R.utils::decompressFile`.\r\n\r\n**Requested extension:**\r\nWould it be possible to implement configuration of an external decompressor program that is automatically used as soon as a extension indicative for a compressed file is detected (like \".gz\", \".bz2\" or \".xz\") instead of `R.utils::decompressFile`?\r\n\r\nConfiguration could be done e.g. via an interface like `data.table.options(decompress='7z e -so')` (to configure a general decompressor for all known compressed file extensions) - or alternatively `data.table.options(decompress.xz='7z e -so')` (file extension specific compressor - here for files ending with \".xz\") after loading the package to set them for all later respective calls during the same session.\r\nThereafter, `fread('example.txt.xz')` would automatically be translated into `fread(cmd='7z e -so example.txt.xz')`\r\n\r\n**Benefit:**\r\n1. All compression formats supported by the used external compression program (in the example case `7z`) could be read without the need to implement the compression scheme in fread itself or relying on R.utils.\r\n2.  Avoids the requirement to always specify the complete decompression command in every call.\r\n\r\nThanks for considering","comments":[],"labels":["fread"]},{"title":"NumFOCUS funding","body":"pandas has been funded by NumFOCUS since 2015. data.table is superior to pandas in terms of syntax and speed and has been a vital part of R's data science ecosystem. Is it possible to get NumFOCUS funding for data.table?  \r\nhttps:\/\/numfocus.org\/project\/pandas","comments":["Agree, personally syntax is what discourage me the most from using pandas. Great idea about the funding. Question if numfocus funds R projects as well. Brief look at their website shows only python.","> Question if numfocus funds R projects as well.\r\n\r\nRather yes. The [website](https:\/\/numfocus.org\/projects-overview) states this information: \r\n\r\n> We support projects in various languages, including Python, Julia, R, C++, and JavaScript.","rOpenSci has been funded by NumFOCUS since 2014.\r\nhttps:\/\/numfocus.org\/project\/ropensci","https:\/\/numfocus.org\/projects-overview explains that creating a formal governance document is required before data.table could apply for fiscal sponsorship from NumFOCUS. \r\n```\r\nAdditional Requirements for Fiscal Sponsorship:\r\n\r\n    A transparent, publicly visible governance model\r\n    A roadmap outlining high priority work areas\r\n```\r\nThat page also says \"NumFOCUS projects are required to have a Code of Conduct (CoC): a user-facing, public statement on inclusivity and project culture. The CoC should include an explanation of what types of behavior are prohibited (and, ideally, what behavior is encouraged), as well as a procedure for reporting CoC complaints, an explanation of who is responsible for resolving CoC complaints (i.e. \u2013 a committee, names of project leaders, or similar), and the process by which CoC complaints will be addressed.\"","Now that we have a code of conduct, we can apply for NumFOCUS funding. The next deadline is April 15. Can someone please volunteer to draft the application, and share it with our community here on this issue, before submitting it to NumFOCUS for review before that deadline?","@tdhock thanks for re-upping this. I could draft an application but would like to hear others' opinion on what the up to $10k could be used on (possibly some developer time for specific feature requests, travel, etc.), including the vision for where the project could go with the funding.","As a side note, there are also funding opportunities at https:\/\/summerofcode.withgoogle.com\/. The deadline is probably too close. ","Thanks Wayne! I actually created a web page for GSOC, https:\/\/github.com\/rstats-gsoc\/gsoc2024\/wiki\/data.table and it is definitely not too late. We need to find an interested\/motivated student though!","I second that, and @tdhock thanks for bringing this up in our meeting yesterday - I made it a point to mention trying GSoC or the summer of code programs in general for the students I'm interviewing that have interest in contributing, since they've been using data.table already or have used in the past. (For e.g., this morning I talked to @damirpolat who is looking for opportunities as he'll graduate soon and he sounded willing to go for it, and I linked that page during my Zoom meeting with him)\r\n\r\nIt'd be interesting to see what projects we could come up with for multiple months' worth of work. Great potential for making progress towards major bug fixes and features!"],"labels":["help-wanted"]},{"title":"froll* not integer64-aware","body":"```r\r\nlibrary(bit64)\r\nlibrary(data.table)\r\n\r\nfrollmean(as.integer64(1:3), 2)\r\n# [1]            NA 9.881313e-324 9.881313e-324\r\n\r\nfrollsum(as.integer64(1:3), 2)\r\n# [1]            NA 1.482197e-323 2.470328e-323\r\nstructure(.Last.value, class = \"integer64\")\r\n# integer64\r\n# [1] 9218868437227407266 3                   5\r\n\r\nfrollapply(as.integer64(1:3), 2, \\(x) sum(sqrt(x)))\r\n# [1]            NA 5.366214e-162 6.993387e-162\r\n```","comments":["Input is always coerced to double (documented), therefore I don't see easy way to support int64. Unless using frollapply (from frollapply branch, not master). There class is retained when passed to FUN.","looks like coercion is skipping S3 dispatch as as.numeric(as.integer64(1:3)) gives the expected output.","There is a dedicated function that does coercion so putting it in is not that difficult."],"labels":["bit64","froll"]},{"title":"fread wrongly tries to load nested csv from properly quoted field &  provides no option to work around this","body":"When parsing a .csv file with data.table where one of the columns is quoted (`\"`) and contains nested CSVs (which includes a `sep` and `\\n`, but no nested `\"`), it seems data.table incorrectly identifies the nested CSV as the data to read instead of parsing the outer, main CSV: \r\n\r\n\r\n``` r\r\nlibrary(data.table)\r\n\r\ntf1 <- tempfile()\r\ntf2 <- tempfile()\r\n\r\nfwrite(iris, tf1)\r\ndt <- data.table(dataset = \"iris\", csv = paste0(readLines(tf1), collapse=\"\\n\"))\r\ndt\r\n#>    dataset\r\n#>     <char>\r\n#> 1:    iris\r\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          csv\r\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char>\r\n#> 1: Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species\\n5.1,3.5,1.4,0.2,setosa\\n4.9,3,1.4,0.2,setosa\\n4.7,3.2,1.3,0.2,setosa\\n4.6,3.1,1.5,0.2,setosa\\n5,3.6,1.4,0.2,setosa\\n5.4,3.9,1.7,0.4,setosa\\n4.6,3.4,1.4,0.3,setosa\\n5,3.4,1.5,0.2,setosa\\n4.4,2.9,1.4,0.2,setosa\\n4.9,3.1,1.5,0.1,setosa\\n5.4,3.7,1.5,0.2,setosa\\n4.8,3.4,1.6,0.2,setosa\\n4.8,3,1.4,0.1,setosa\\n4.3,3,1.1,0.1,setosa\\n5.8,4,1.2,0.2,setosa\\n5.7,4.4,1.5,0.4,setosa\\n5.4,3.9,1.3,0.4,setosa\\n5.1,3.5,1.4,0.3,setosa\\n5.7,3.8,1.7,0.3,setosa\\n5.1,3.8,1.5,0.3,setosa\\n5.4,3.4,1.7,0.2,setosa\\n5.1,3.7,1.5,0.4,setosa\\n4.6,3.6,1,0.2,setosa\\n5.1,3.3,1.7,0.5,setosa\\n4.8,3.4,1.9,0.2,setosa\\n5,3,1.6,0.2,setosa\\n5,3.4,1.6,0.4,setosa\\n5.2,3.5,1.5,0.2,setosa\\n5.2,3.4,1.4,0.2,setosa\\n4.7,3.2,1.6,0.2,setosa\\n4.8,3.1,1.6,0.2,setosa\\n5.4,3.4,1.5,0.4,setosa\\n5.2,4.1,1.5,0.1,setosa\\n5.5,4.2,1.4,0.2,setosa\\n4.9,3.1,1.5,0.2,setosa\\n5,3.2,1.2,0.2,setosa\\n5.5,3.5,1.3,0.2,setosa\\n4.9,3.6,1.4,0.1,setosa\\n4.4,3,1.3,0.2,setosa\\n5.1,3.4,1.5,0.2,setosa\\n5,3.5,1.3,0.3,setosa\\n4.5,2.3,1.3,0.3,setosa\\n4.4,3.2,1.3,0.2,setosa\\n5,3.5,1.6,0.6,setosa\\n5.1,3.8,1.9,0.4,setosa\\n4.8,3,1.4,0.3,setosa\\n5.1,3.8,1.6,0.2,setosa\\n4.6,3.2,1.4,0.2,setosa\\n5.3,3.7,1.5,0.2,setosa\\n5,3.3,1.4,0.2,setosa\\n7,3.2,4.7,1.4,versicolor\\n6.4,3.2,4.5,1.5,versicolor\\n6.9,3.1,4.9,1.5,versicolor\\n5.5,2.3,4,1.3,versicolor\\n6.5,2.8,4.6,1.5,versicolor\\n5.7,2.8,4.5,1.3,versicolor\\n6.3,3.3,4.7,1.6,versicolor\\n4.9,2.4,3.3,1,versicolor\\n6.6,2.9,4.6,1.3,versicolor\\n5.2,2.7,3.9,1.4,versicolor\\n5,2,3.5,1,versicolor\\n5.9,3,4.2,1.5,versicolor\\n6,2.2,4,1,versicolor\\n6.1,2.9,4.7,1.4,versicolor\\n5.6,2.9,3.6,1.3,versicolor\\n6.7,3.1,4.4,1.4,versicolor\\n5.6,3,4.5,1.5,versicolor\\n5.8,2.7,4.1,1,versicolor\\n6.2,2.2,4.5,1.5,versicolor\\n5.6,2.5,3.9,1.1,versicolor\\n5.9,3.2,4.8,1.8,versicolor\\n6.1,2.8,4,1.3,versicolor\\n6.3,2.5,4.9,1.5,versicolor\\n6.1,2.8,4.7,1.2,versicolor\\n6.4,2.9,4.3,1.3,versicolor\\n6.6,3,4.4,1.4,versicolor\\n6.8,2.8,4.8,1.4,versicolor\\n6.7,3,5,1.7,versicolor\\n6,2.9,4.5,1.5,versicolor\\n5.7,2.6,3.5,1,versicolor\\n5.5,2.4,3.8,1.1,versicolor\\n5.5,2.4,3.7,1,versicolor\\n5.8,2.7,3.9,1.2,versicolor\\n6,2.7,5.1,1.6,versicolor\\n5.4,3,4.5,1.5,versicolor\\n6,3.4,4.5,1.6,versicolor\\n6.7,3.1,4.7,1.5,versicolor\\n6.3,2.3,4.4,1.3,versicolor\\n5.6,3,4.1,1.3,versicolor\\n5.5,2.5,4,1.3,versicolor\\n5.5,2.6,4.4,1.2,versicolor\\n6.1,3,4.6,1.4,versicolor\\n5.8,2.6,4,1.2,versicolor\\n5,2.3,3.3,1,versicolor\\n5.6,2.7,4.2,1.3,versicolor\\n5.7,3,4.2,1.2,versicolor\\n5.7,2.9,4.2,1.3,versicolor\\n6.2,2.9,4.3,1.3,versicolor\\n5.1,2.5,3,1.1,versicolor\\n5.7,2.8,4.1,1.3,versicolor\\n6.3,3.3,6,2.5,virginica\\n5.8,2.7,5.1,1.9,virginica\\n7.1,3,5.9,2.1,virginica\\n6.3,2.9,5.6,1.8,virginica\\n6.5,3,5.8,2.2,virginica\\n7.6,3,6.6,2.1,virginica\\n4.9,2.5,4.5,1.7,virginica\\n7.3,2.9,6.3,1.8,virginica\\n6.7,2.5,5.8,1.8,virginica\\n7.2,3.6,6.1,2.5,virginica\\n6.5,3.2,5.1,2,virginica\\n6.4,2.7,5.3,1.9,virginica\\n6.8,3,5.5,2.1,virginica\\n5.7,2.5,5,2,virginica\\n5.8,2.8,5.1,2.4,virginica\\n6.4,3.2,5.3,2.3,virginica\\n6.5,3,5.5,1.8,virginica\\n7.7,3.8,6.7,2.2,virginica\\n7.7,2.6,6.9,2.3,virginica\\n6,2.2,5,1.5,virginica\\n6.9,3.2,5.7,2.3,virginica\\n5.6,2.8,4.9,2,virginica\\n7.7,2.8,6.7,2,virginica\\n6.3,2.7,4.9,1.8,virginica\\n6.7,3.3,5.7,2.1,virginica\\n7.2,3.2,6,1.8,virginica\\n6.2,2.8,4.8,1.8,virginica\\n6.1,3,4.9,1.8,virginica\\n6.4,2.8,5.6,2.1,virginica\\n7.2,3,5.8,1.6,virginica\\n7.4,2.8,6.1,1.9,virginica\\n7.9,3.8,6.4,2,virginica\\n6.4,2.8,5.6,2.2,virginica\\n6.3,2.8,5.1,1.5,virginica\\n6.1,2.6,5.6,1.4,virginica\\n7.7,3,6.1,2.3,virginica\\n6.3,3.4,5.6,2.4,virginica\\n6.4,3.1,5.5,1.8,virginica\\n6,3,4.8,1.8,virginica\\n6.9,3.1,5.4,2.1,virginica\\n6.7,3.1,5.6,2.4,virginica\\n6.9,3.1,5.1,2.3,virginica\\n5.8,2.7,5.1,1.9,virginica\\n6.8,3.2,5.9,2.3,virginica\\n6.7,3.3,5.7,2.5,virginica\\n6.7,3,5.2,2.3,virginica\\n6.3,2.5,5,1.9,virginica\\n6.5,3,5.2,2,virginica\\n6.2,3.4,5.4,2.3,virginica\\n5.9,3,5.1,1.8,virginica\r\nfwrite(dt, tf2)\r\nfread(tf2, verbose = TRUE) # bad output +warning\r\n#>   OpenMP version (_OPENMP)       201511\r\n#>   omp_get_num_procs()            128\r\n#>   R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n#>   R_DATATABLE_NUM_THREADS        unset\r\n#>   R_DATATABLE_THROTTLE           unset (default 1024)\r\n#>   omp_get_thread_limit()         16\r\n#>   omp_get_max_threads()          128\r\n#>   OMP_THREAD_LIMIT               16\r\n#>   OMP_NUM_THREADS                unset\r\n#>   RestoreAfterFork               true\r\n#>   data.table is using 16 threads with throttle==1024. See ?setDTthreads.\r\n#> freadR.c has been passed a filename: \/tmp\/RtmpdDWffF\/file208e3aff7a42\r\n#> [01] Check arguments\r\n#>   Using 16 threads (omp_get_max_threads()=128, nth=16)\r\n#>   NAstrings = [<<NA>>]\r\n#>   None of the NAstrings look like numbers.\r\n#>   show progress = 0\r\n#>   0\/1 column will be read as integer\r\n#> [02] Opening the file\r\n#>   Opening file \/tmp\/RtmpdDWffF\/file208e3aff7a42\r\n#>   File opened, size = 3.647KB (3735 bytes).\r\n#>   Memory mapped ok\r\n#> [03] Detect and skip BOM\r\n#> [04] Arrange mmap to be \\0 terminated\r\n#>   \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n#> [05] Skipping initial rows if needed\r\n#>   Positioned on line 1 starting: <<dataset,csv>>\r\n#> [06] Detect separator, quoting rule, and ncolumns\r\n#>   Detecting sep automatically ...\r\n#>   sep=','  with 2 lines of 2 fields using quote rule 0\r\n#>   sep=','  with 98 lines of 5 fields using quote rule 2\r\n#> Warning in fread(tf2, verbose = TRUE): Found and resolved improper quoting in\r\n#> first 100 rows. If the fields are not quoted (e.g. field separator does not\r\n#> appear within any field), try quote=\"\" to avoid this warning.\r\n#>   Detected 5 columns on line 3. This line is either column names or first data row. Line starts as: <<5.1,3.5,1.4,0.2,setosa>>\r\n#>   Quote rule picked = 2\r\n#>   fill=false and the most number of columns found is 5\r\n#> [07] Detect column types, good nrow estimate and whether first row is column names\r\n#>   Number of sampling jump points = 1 because (3658 bytes from row 1 to eof) \/ (2 * 2356 jump0size) == 0\r\n#>   Type codes (jump 000)    : 8888D  Quote rule 2\r\n#>   Type codes (jump 001)    : 8888D  Quote rule 2\r\n#> Types in 1st data row match types in 2nd data row but previous row has 6 fields. Taking previous row as column names.\r\n#> Warning in fread(tf2, verbose = TRUE): Detected 6 column names but the data has\r\n#> 5 columns. Filling rows automatically. Set fill=TRUE explicitly to avoid this\r\n#> warning.\r\n#>   =====\r\n#>   Sampled 147 rows (handled \\n inside quoted fields) at 2 jump points\r\n#>   Bytes from first data row on line 3 to the end of last row: 3635\r\n#>   Line length: mean=24.39 sd=1.97 min=19 max=27\r\n#>   Estimated number of rows: 3635 \/ 24.39 = 150\r\n#>   Initial alloc = 177 rows (150 + 18%) using bytes\/max(mean-2*sd,min) clamped between [1.1*estn, 2.0*estn]\r\n#>   =====\r\n#> [08] Assign column names\r\n#> [09] Apply user overrides on column types\r\n#>   After 0 type and 0 drop user overrides : 8888D1\r\n#> [10] Allocate memory for the datatable\r\n#>   Allocating 6 column slots (6 - 0 dropped) with 177 rows\r\n#> [11] Read the data\r\n#>   jumps=[0..1), chunk_size=1048576, total_size=3658\r\n#> Read 150 rows x 6 columns from 3.647KB (3735 bytes) file in 00:00.001 wall clock time\r\n#> [12] Finalizing the datatable\r\n#>   Type counts:\r\n#>          1 : bool8     '1'\r\n#>          4 : float64   '8'\r\n#>          1 : string    'D'\r\n#> =============================\r\n#>    0.000s (  7%) Memory map 0.000GB file\r\n#>    0.001s ( 85%) sep=',' ncol=6 and header detection\r\n#>    0.000s (  1%) Column type detection using 147 sample rows\r\n#>    0.000s (  1%) Allocation of 177 rows x 6 cols (0.000GB) of which 150 ( 85%) rows used\r\n#>    0.000s (  6%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 150 rows) using 1 threads\r\n#>    +    0.000s (  3%) Parse to row-major thread buffers (grown 0 times)\r\n#>    +    0.000s (  1%) Transpose\r\n#>    +    0.000s (  2%) Waiting\r\n#>    0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions\r\n#>    0.001s        Total\r\n#>       iris \"Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n#>      <num>         <num>       <num>        <num>      <char>  <lgcl>\r\n#>   1:   5.1           3.5         1.4          0.2      setosa      NA\r\n#>   2:   4.9           3.0         1.4          0.2      setosa      NA\r\n#>   3:   4.7           3.2         1.3          0.2      setosa      NA\r\n#>   4:   4.6           3.1         1.5          0.2      setosa      NA\r\n#>   5:   5.0           3.6         1.4          0.2      setosa      NA\r\n#>  ---                                                                 \r\n#> 146:   6.7           3.0         5.2          2.3   virginica      NA\r\n#> 147:   6.3           2.5         5.0          1.9   virginica      NA\r\n#> 148:   6.5           3.0         5.2          2.0   virginica      NA\r\n#> 149:   6.2           3.4         5.4          2.3   virginica      NA\r\n#> 150:   5.9           3.0         5.1          1.8  virginica\"      NA\r\nfread(tf2, nrow=1)  # correct output\r\n#>    dataset\r\n#>     <char>\r\n#> 1:    iris\r\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          csv\r\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <char>\r\n#> 1: Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species\\n5.1,3.5,1.4,0.2,setosa\\n4.9,3,1.4,0.2,setosa\\n4.7,3.2,1.3,0.2,setosa\\n4.6,3.1,1.5,0.2,setosa\\n5,3.6,1.4,0.2,setosa\\n5.4,3.9,1.7,0.4,setosa\\n4.6,3.4,1.4,0.3,setosa\\n5,3.4,1.5,0.2,setosa\\n4.4,2.9,1.4,0.2,setosa\\n4.9,3.1,1.5,0.1,setosa\\n5.4,3.7,1.5,0.2,setosa\\n4.8,3.4,1.6,0.2,setosa\\n4.8,3,1.4,0.1,setosa\\n4.3,3,1.1,0.1,setosa\\n5.8,4,1.2,0.2,setosa\\n5.7,4.4,1.5,0.4,setosa\\n5.4,3.9,1.3,0.4,setosa\\n5.1,3.5,1.4,0.3,setosa\\n5.7,3.8,1.7,0.3,setosa\\n5.1,3.8,1.5,0.3,setosa\\n5.4,3.4,1.7,0.2,setosa\\n5.1,3.7,1.5,0.4,setosa\\n4.6,3.6,1,0.2,setosa\\n5.1,3.3,1.7,0.5,setosa\\n4.8,3.4,1.9,0.2,setosa\\n5,3,1.6,0.2,setosa\\n5,3.4,1.6,0.4,setosa\\n5.2,3.5,1.5,0.2,setosa\\n5.2,3.4,1.4,0.2,setosa\\n4.7,3.2,1.6,0.2,setosa\\n4.8,3.1,1.6,0.2,setosa\\n5.4,3.4,1.5,0.4,setosa\\n5.2,4.1,1.5,0.1,setosa\\n5.5,4.2,1.4,0.2,setosa\\n4.9,3.1,1.5,0.2,setosa\\n5,3.2,1.2,0.2,setosa\\n5.5,3.5,1.3,0.2,setosa\\n4.9,3.6,1.4,0.1,setosa\\n4.4,3,1.3,0.2,setosa\\n5.1,3.4,1.5,0.2,setosa\\n5,3.5,1.3,0.3,setosa\\n4.5,2.3,1.3,0.3,setosa\\n4.4,3.2,1.3,0.2,setosa\\n5,3.5,1.6,0.6,setosa\\n5.1,3.8,1.9,0.4,setosa\\n4.8,3,1.4,0.3,setosa\\n5.1,3.8,1.6,0.2,setosa\\n4.6,3.2,1.4,0.2,setosa\\n5.3,3.7,1.5,0.2,setosa\\n5,3.3,1.4,0.2,setosa\\n7,3.2,4.7,1.4,versicolor\\n6.4,3.2,4.5,1.5,versicolor\\n6.9,3.1,4.9,1.5,versicolor\\n5.5,2.3,4,1.3,versicolor\\n6.5,2.8,4.6,1.5,versicolor\\n5.7,2.8,4.5,1.3,versicolor\\n6.3,3.3,4.7,1.6,versicolor\\n4.9,2.4,3.3,1,versicolor\\n6.6,2.9,4.6,1.3,versicolor\\n5.2,2.7,3.9,1.4,versicolor\\n5,2,3.5,1,versicolor\\n5.9,3,4.2,1.5,versicolor\\n6,2.2,4,1,versicolor\\n6.1,2.9,4.7,1.4,versicolor\\n5.6,2.9,3.6,1.3,versicolor\\n6.7,3.1,4.4,1.4,versicolor\\n5.6,3,4.5,1.5,versicolor\\n5.8,2.7,4.1,1,versicolor\\n6.2,2.2,4.5,1.5,versicolor\\n5.6,2.5,3.9,1.1,versicolor\\n5.9,3.2,4.8,1.8,versicolor\\n6.1,2.8,4,1.3,versicolor\\n6.3,2.5,4.9,1.5,versicolor\\n6.1,2.8,4.7,1.2,versicolor\\n6.4,2.9,4.3,1.3,versicolor\\n6.6,3,4.4,1.4,versicolor\\n6.8,2.8,4.8,1.4,versicolor\\n6.7,3,5,1.7,versicolor\\n6,2.9,4.5,1.5,versicolor\\n5.7,2.6,3.5,1,versicolor\\n5.5,2.4,3.8,1.1,versicolor\\n5.5,2.4,3.7,1,versicolor\\n5.8,2.7,3.9,1.2,versicolor\\n6,2.7,5.1,1.6,versicolor\\n5.4,3,4.5,1.5,versicolor\\n6,3.4,4.5,1.6,versicolor\\n6.7,3.1,4.7,1.5,versicolor\\n6.3,2.3,4.4,1.3,versicolor\\n5.6,3,4.1,1.3,versicolor\\n5.5,2.5,4,1.3,versicolor\\n5.5,2.6,4.4,1.2,versicolor\\n6.1,3,4.6,1.4,versicolor\\n5.8,2.6,4,1.2,versicolor\\n5,2.3,3.3,1,versicolor\\n5.6,2.7,4.2,1.3,versicolor\\n5.7,3,4.2,1.2,versicolor\\n5.7,2.9,4.2,1.3,versicolor\\n6.2,2.9,4.3,1.3,versicolor\\n5.1,2.5,3,1.1,versicolor\\n5.7,2.8,4.1,1.3,versicolor\\n6.3,3.3,6,2.5,virginica\\n5.8,2.7,5.1,1.9,virginica\\n7.1,3,5.9,2.1,virginica\\n6.3,2.9,5.6,1.8,virginica\\n6.5,3,5.8,2.2,virginica\\n7.6,3,6.6,2.1,virginica\\n4.9,2.5,4.5,1.7,virginica\\n7.3,2.9,6.3,1.8,virginica\\n6.7,2.5,5.8,1.8,virginica\\n7.2,3.6,6.1,2.5,virginica\\n6.5,3.2,5.1,2,virginica\\n6.4,2.7,5.3,1.9,virginica\\n6.8,3,5.5,2.1,virginica\\n5.7,2.5,5,2,virginica\\n5.8,2.8,5.1,2.4,virginica\\n6.4,3.2,5.3,2.3,virginica\\n6.5,3,5.5,1.8,virginica\\n7.7,3.8,6.7,2.2,virginica\\n7.7,2.6,6.9,2.3,virginica\\n6,2.2,5,1.5,virginica\\n6.9,3.2,5.7,2.3,virginica\\n5.6,2.8,4.9,2,virginica\\n7.7,2.8,6.7,2,virginica\\n6.3,2.7,4.9,1.8,virginica\\n6.7,3.3,5.7,2.1,virginica\\n7.2,3.2,6,1.8,virginica\\n6.2,2.8,4.8,1.8,virginica\\n6.1,3,4.9,1.8,virginica\\n6.4,2.8,5.6,2.1,virginica\\n7.2,3,5.8,1.6,virginica\\n7.4,2.8,6.1,1.9,virginica\\n7.9,3.8,6.4,2,virginica\\n6.4,2.8,5.6,2.2,virginica\\n6.3,2.8,5.1,1.5,virginica\\n6.1,2.6,5.6,1.4,virginica\\n7.7,3,6.1,2.3,virginica\\n6.3,3.4,5.6,2.4,virginica\\n6.4,3.1,5.5,1.8,virginica\\n6,3,4.8,1.8,virginica\\n6.9,3.1,5.4,2.1,virginica\\n6.7,3.1,5.6,2.4,virginica\\n6.9,3.1,5.1,2.3,virginica\\n5.8,2.7,5.1,1.9,virginica\\n6.8,3.2,5.9,2.3,virginica\\n6.7,3.3,5.7,2.5,virginica\\n6.7,3,5.2,2.3,virginica\\n6.3,2.5,5,1.9,virginica\\n6.5,3,5.2,2,virginica\\n6.2,3.4,5.4,2.3,virginica\\n5.9,3,5.1,1.8,virginica\r\n```\r\n\r\n\r\n<details style=\"margin-bottom:10px;\">\r\n<summary>\r\nSession info\r\n<\/summary>\r\n\r\n``` r\r\nsessioninfo::session_info()\r\n#> \u2500 Session info \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n#>  setting  value\r\n#>  version  R version 4.1.0 (2021-05-18)\r\n#>  os       CentOS Linux 7 (Core)\r\n#>  system   x86_64, linux-gnu\r\n#>  ui       X11\r\n#>  language en_US.UTF-8\r\n#>  collate  en_US.UTF-8\r\n#>  ctype    en_US.UTF-8\r\n#>  tz       Europe\/Berlin\r\n#>  date     2023-07-13\r\n#>  pandoc   2.19.2 @ \/usr\/lib\/rstudio-server\/bin\/quarto\/bin\/tools\/ (via rmarkdown)\r\n#> \r\n#> \u2500 Packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n#>  package     * version date (UTC) lib source\r\n#>  cli           3.6.1   2023-03-23 [1] CRAN (R 4.1.0)\r\n#>  data.table  * 1.14.9  2023-07-13 [1] local\r\n#>  digest        0.6.29  2021-12-01 [1] CRAN (R 4.1.0)\r\n#>  evaluate      0.14    2019-05-28 [1] CRAN (R 4.1.0)\r\n#>  fastmap       1.1.0   2021-01-25 [1] CRAN (R 4.1.0)\r\n#>  fs            1.5.2   2021-12-08 [1] CRAN (R 4.1.0)\r\n#>  glue          1.6.2   2022-02-24 [1] CRAN (R 4.1.0)\r\n#>  highr         0.9     2021-04-16 [1] CRAN (R 4.1.0)\r\n#>  htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.1.0)\r\n#>  knitr         1.37    2021-12-16 [1] CRAN (R 4.1.0)\r\n#>  lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.1.0)\r\n#>  magrittr      2.0.2   2022-01-26 [1] CRAN (R 4.1.0)\r\n#>  purrr         1.0.1   2023-01-10 [1] CRAN (R 4.1.0)\r\n#>  R.cache       0.16.0  2022-07-21 [1] CRAN (R 4.1.0)\r\n#>  R.methodsS3   1.8.1   2020-08-26 [1] CRAN (R 4.1.0)\r\n#>  R.oo          1.24.0  2020-08-26 [1] CRAN (R 4.1.0)\r\n#>  R.utils       2.11.0  2021-09-26 [1] CRAN (R 4.1.0)\r\n#>  reprex        2.0.1   2021-08-05 [1] CRAN (R 4.1.0)\r\n#>  rlang         1.1.1   2023-04-28 [1] CRAN (R 4.1.0)\r\n#>  rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.1.0)\r\n#>  rstudioapi    0.13    2020-11-12 [1] CRAN (R 4.1.0)\r\n#>  sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.1.0)\r\n#>  stringi       1.7.6   2021-11-29 [1] CRAN (R 4.1.0)\r\n#>  stringr       1.5.0   2022-12-02 [1] CRAN (R 4.1.0)\r\n#>  styler        1.10.1  2023-06-05 [1] CRAN (R 4.1.0)\r\n#>  vctrs         0.6.2   2023-04-19 [1] CRAN (R 4.1.0)\r\n#>  withr         2.5.0   2022-03-03 [1] CRAN (R 4.1.0)\r\n#>  xfun          0.39    2023-04-20 [1] CRAN (R 4.1.0)\r\n#>  yaml          2.3.4   2022-02-17 [1] CRAN (R 4.1.0)\r\n#> \r\n#>  [1] \/home\/gleixner\/R\/x86_64-pc-linux-gnu-library\/4.1.0d\r\n#>  [2] \/software\/r\/4.1.0\/lib64\/R\/library\r\n#> \r\n#> \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n```\r\n\r\n<\/details>\r\n\r\nIn above example, specifying nrow=1 allows data.table to correctly interpret and read the CSV but usually the correct number of rows is not know in advance.\r\n\r\nThere does not appear to be any provided means in data.table to turn off the automatic guessing of sep, quoting, and nrow. I've tried manually specifying sep=\",\" and\/or quoting and skip=0, but to no avail.\r\n\r\nA potential improvement could be an option to disable this automatic guessing feature. Alternatively, or additionally, data.table could provide a fallback mechanism that tries alternative guesses if the initial selected guess fails. Another proposal would be to prioritize guesses by number of characters instead of number of fields (I believe the latter to be the current strategy). \r\n\r\n\r\nFor context, the \"guesses\" I am referring to are:\r\n\r\n    #> Detecting sep automatically ...\r\n    #> sep=',' with 2 lines of 2 fields using quote rule 0\r\n    #> sep=',' with 98 lines of 5 fields using quote rule 2\r\n\r\n\r\nPS: I really hope I did not miss the original issue if this is a duplicate.","comments":[],"labels":["fread"]},{"title":"Convenience features for .SDcols patterns perl=TRUE","body":"Closes #5387 ","comments":["hi @iago-pssjd I implemented a PR which hopefully resolves your issue, can you please review?\r\n```r\r\n> DT = data.table(a=1:2, b=3, c='a')\r\n> DT[, .SD, .SDcols=patterns('^(?!W)a',perl=TRUE)]\r\n       a\r\n   <int>\r\n1:     1\r\n2:     2\r\n> not = function(regex, cols)grep(regex, cols, invert=TRUE)\r\n> DT[, .SD, .SDcols=not(\"[ac]\")]\r\n       b\r\n   <num>\r\n1:     3\r\n2:     3\r\n```","Hi @tdhock! Thanks for your work, I tested your branch and it works fine regarding the issue.","## [Codecov](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5663?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Comparison is base [(`2c1fac7`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/commit\/2c1fac7fabe85e2d63ad42b262ef6c51db114646?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.48% compared to head [(`83fa5d7`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5663?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.48%.\n\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@           Coverage Diff           @@\n##           master    #5663   +\/-   ##\n=======================================\n  Coverage   97.48%   97.48%           \n=======================================\n  Files          80       80           \n  Lines       14857    14859    +2     \n=======================================\n+ Hits        14484    14486    +2     \n  Misses        373      373           \n```\n\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5663?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n","hi @jangorecki @MichaelChirico I would like to include this fix in next release, and I believe that it is ready to merge into master, so could you please review and\/or merge if you agree that it is ready? Thanks!","Let's sort out #5937 first"],"labels":["reshape"]},{"title":"`[.data.table` is very slow with a single column ","body":"`#` [`Minimal reproducible example`](https:\/\/stackoverflow.com\/questions\/5963269\/how-to-make-a-great-r-reproducible-example); please be sure to set `verbose=TRUE` where possible!\r\n\r\n```R\r\nlibrary(bench)\r\nx <- data.frame(a = runif(10000), b= as.character(runif(10000)))\r\nindex <- runif(10000) <= 0.5\r\nmark(x[index, \"a\"], x[[\"a\"]][index])\r\n# A tibble: 2 \u00d7 13\r\n#    expression      min median `itr\/sec` mem_alloc `gc\/sec` n_itr  n_gc total_time\r\n#    <bch:expr>   <bch:> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\r\n# 1 \"x[index, \\\u2026 78.1\u00b5s 80.6\u00b5s    11587.    98.4KB     21.2  5469    10      472ms\r\n# 2 \"x[[\\\"a\\\"]]\u2026   71\u00b5s 73.2\u00b5s    13240.    98.4KB     23.5  6207    11      469ms\r\n# More or less the same !\r\nlibrary(data.table)\r\nx <- as.data.table(x)\r\nmark(x[index, a], x[[\"a\"]][index])\r\n# A tibble: 2 \u00d7 13\r\n#   expression     min  median `itr\/sec` mem_alloc `gc\/sec` n_itr  n_gc total_time\r\n#   <bch:expr> <bch:t> <bch:t>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\r\n# 1 \"x[index,\u2026 360.4\u00b5s 377.1\u00b5s     2403.   219.3KB     8.28  1161     4      483ms\r\n# 2 \"x[[\\\"a\\\"\u2026  71.4\u00b5s  73.9\u00b5s    12920.    98.4KB    23.9   5949    11      460ms\r\n# Five times slower !!!\r\n```\r\n\r\n`#` `Output of sessionInfo()`\r\n```R\r\nR version 4.1.2 (2021-11-01)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Ubuntu 22.04.2 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libblas.so.3\r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libopenblasp-r0.3.20.so\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              \r\n [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    \r\n [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   \r\n [7] LC_PAPER=es_ES.utf8        LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.2 bench_1.1.3      \r\n\r\nloaded via a namespace (and not attached):\r\n [1] matrixStats_0.61.0 fansi_1.0.2        utf8_1.2.2         irace_3.5.1.9000  \r\n [5] R6_2.5.1           lifecycle_1.0.3    magrittr_2.0.2     pillar_1.9.0      \r\n [9] profmem_0.6.0      rlang_1.0.6        cli_3.6.0          vctrs_0.5.2       \r\n[13] tools_4.1.2        glue_1.6.2         compiler_4.1.2     pkgconfig_2.0.3   \r\n[17] tibble_3.2.1\r\n``` ","comments":["But you aren't even benchmarking the same queries on `data.frame` and `data.table` since `x[index, a]` shouldn't eval in the first place on a `data.frame`.","> But you aren't even benchmarking the same queries on `data.frame` and `data.table` since `x[index, a]` shouldn't eval in the first place on a `data.frame`.\r\n\r\nThe first call to `bench()` is the one benchmarking `data.frame`, the second call evaluates `data.table`. But OK, just in case of doubt:\r\n```R\r\nlibrary(bench)\r\ndf <- data.frame(a = runif(10000), b= as.character(runif(10000)))\r\nindex <- runif(10000) <= 0.5\r\nlibrary(data.table)\r\ndt <- as.data.table(df)\r\nmark(\r\n  df[index, \"a\"],\r\n  df[[\"a\"]][index],\r\n  dt[index, a],\r\n  dt[[\"a\"]][index])\r\n```  \r\nResult:\r\n```R\r\n# A tibble: 4 \u00d7 13\r\n  expression     min  median `itr\/sec` mem_alloc `gc\/sec` n_itr  n_gc total_time\r\n  <bch:expr> <bch:t> <bch:t>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\r\n1 \"df[index\u2026  78.3\u00b5s  81.1\u00b5s    11541.    98.3KB    21.7   5319    10      461ms\r\n2 \"df[[\\\"a\\\u2026  71.3\u00b5s  73.6\u00b5s    13177.    98.3KB    23.6   6132    11      465ms\r\n3 \"dt[index\u2026 354.3\u00b5s 370.4\u00b5s     2597.   114.7KB     6.15  1267     3      488ms\r\n4 \"dt[[\\\"a\\\u2026  71.2\u00b5s    74\u00b5s    13165.    98.3KB    23.7   6115    11      464ms\r\n```\r\n\r\nDoes `data.table` have a benchmarking testsuite to track regressions? Is development frozen? There has been no activity since February and there are more than 1k issues opened and 131 PRs.","Does data.table have a benchmarking testsuite to track regressions? -> no but this is one of the goals in the next 2 years, we plan to hire someone to work on that full time.","I don't think there was a regression here anyway. Looking at the time units, I doubt it will ever get good attention from the team as we would have to subset columns in a loop tens of thousands time for the problem to be really noticeable, and that would be rather uncommon use case.","\r\n> > But you aren't even benchmarking the same queries on `data.frame` and `data.table` since `x[index, a]` shouldn't eval in the first place on a `data.frame`.\r\n> \r\n> The first call to `bench()` is the one benchmarking `data.frame`, the second call evaluates `data.table`. But OK, just in case of doubt:\r\n\r\nI'm not implying that the benchmarks are wrong, I'm just saying that `a != \"a\"`. If you would benchmark `dt[index, \"a\"]` then it would still be orders slower than the same call on a `data.frame` but it would also be faster than what you do namely comparing different calls.","> I'm not implying that the benchmarks are wrong, I'm just saying that `a != \"a\"`. If you would benchmark `dt[index, \"a\"]` then it would still be orders slower than the same call on a `data.frame` but it would also be faster than what you do namely comparing different calls.\r\n\r\nOK, following your suggestion:\r\n\r\n```R\r\nlibrary(bench)\r\ndf <- data.frame(a = runif(10000), b= as.character(runif(10000)))\r\nindex <- runif(10000) <= 0.5\r\nlibrary(data.table)\r\ndt <- as.data.table(df)\r\nmark(\r\n  df[index, \"a\"],\r\n  df[[\"a\"]][index],\r\n  dt[index, a],\r\n  dt[[\"a\"]][index],\r\n  as.list(dt[index, \"a\"])$a)\r\n```\r\n\r\nThe result is\r\n```R\r\n# A tibble: 5 \u00d7 13\r\n  expression     min  median `itr\/sec` mem_alloc `gc\/sec` n_itr  n_gc total_time\r\n  <bch:expr> <bch:t> <bch:t>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\r\n1 \"df[index\u2026  78.3\u00b5s  80.8\u00b5s    11308.    97.7KB    19.1   5339     9      472ms\r\n2 \"df[[\\\"a\\\u2026  71.5\u00b5s  73.5\u00b5s    13192.    97.7KB    23.7   6113    11      463ms\r\n3 \"dt[index\u2026   357\u00b5s   373\u00b5s     2629.   114.1KB     6.15  1283     3      488ms\r\n4 \"dt[[\\\"a\\\u2026  71.9\u00b5s  74.2\u00b5s    12873.    97.7KB    23.9   5928    11      461ms\r\n5 \"as.list(\u2026 250.2\u00b5s   262\u00b5s     3729.   129.9KB    10.4   1795     5      481ms\r\n```\r\n\r\nso slightly faster (which is surprising since this is not the recommended way in `data.table`'s documentation and tutorials), but still 4-5 times slower than a `data.frame`\r\n\r\n> I don't this there was a regression here anyway. Looking at the time units, I doubt it will ever get good attention from the team as we would have to subset columns in a loop tens of thousands time for the problem to be really noticeable, and that would be rather uncommon use case.\r\n\r\nIs it really an uncommon case to select just one column within a tight loop? The only reason that I noticed this is because `[.data.table` shows up in the profiles generated by `profvis` for my package (https:\/\/github.com\/MLopez-Ibanez\/irace\/tree\/datatable-clean), which does not happen for `[.data.frame`.\r\n \r\nI understand that the primary use of `data.table` may be interactive and without loops. However, given the breadth of packages using `data.table`, I will not be surprised if people are using `data.table` within tight loops. It could be possible to collect profile info in revdeps and filter it to keep only `data.table` functions  and see which functions are the most expensive ones. My bet would be that `[.data.table` is among the top ones (but there may be other slow and popular functions that I have not used).\r\n\r\nIn any case, given the fact that there were already 1K issues reported before this one, I do not expect a fix to this one anytime soon, but other users may find the information useful or contribute workarounds. I didn't know that `dt[index, \"a\"]` would be faster, so that is already useful to know.","**_If_** speed is really important here then you need to avoid method dispatch anyway. If you're only interested in the underlying column then you should be able to use `.subset2()` to get the column and then index. E.g.\r\n\r\n``` r\r\nlibrary(data.table)\r\ndf <- data.frame(a = runif(10000), b= as.character(runif(10000)))\r\nindex <- runif(10000) <= 0.5\r\ndt <- as.data.table(df)\r\nmicrobenchmark::microbenchmark(\r\n  df[index, \"a\"],\r\n  df[[\"a\"]][index],\r\n  .subset2(df, \"a\")[index],\r\n  dt[index, a],\r\n  dt[[\"a\"]][index],\r\n  as.list(dt[index, \"a\"])$a,\r\n  .subset2(dt, \"a\")[index]\r\n)\r\n#> Unit: microseconds\r\n#>                       expr     min       lq      mean   median       uq\r\n#>             df[index, \"a\"]  60.989  81.5590  94.47888 100.6775 105.0240\r\n#>           df[[\"a\"]][index]  55.149  65.9050  85.58422  93.3225  96.5160\r\n#>   .subset2(df, \"a\")[index]  49.975  81.0015  80.79162  85.2605  89.0560\r\n#>               dt[index, a] 251.248 275.0495 311.04374 304.2420 312.0940\r\n#>           dt[[\"a\"]][index]  55.672  68.5170  85.95876  94.0135  97.7035\r\n#>  as.list(dt[index, \"a\"])$a 172.928 217.2780 326.48526 226.8850 237.7520\r\n#>   .subset2(dt, \"a\")[index]  48.631  77.8495  79.47604  85.7895  88.7160\r\n#>       max neval\r\n#>   120.672   100\r\n#>   114.245   100\r\n#>   103.070   100\r\n#>   880.575   100\r\n#>   112.374   100\r\n#>  7082.201   100\r\n#>    98.199   100\r\n```\r\n\r\n<sup>Created on 2023-06-16 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n```","Maybe one could gain 10 microseconds per call but the confidence intervals appear to overlap and it is easier to write `dt[[\"a\"]][index]` than `.subset2(dt, \"a\")[index]`. \r\n\r\nThe issue remains that the \"natural\" approach of replacing `df[index, \"a\"]` with `dt[index, \"a\"]`, which is the intended use according to the documentation, will produce a slowdown of 200 microseconds per call on average (or x4 slower).","There is PR of mine which already speeds up DT[index], possibly will work with column selection as well. You can try it out.","Not sure yet what is implemented so far, but those are related... for column selection https:\/\/github.com\/Rdatatable\/data.table\/pull\/4488, and for row selection https:\/\/github.com\/Rdatatable\/data.table\/issues\/3736 and https:\/\/github.com\/Rdatatable\/data.table\/issues\/4485","This is rather a problem of optimizing tight loops. If there are many iterations then some care is needed. I documented that (amongst others things to be careful about in tight loops) in\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/21da019f63947c28f9cb93fdf8f275bd52461dd5\/man\/frollapply.Rd#L218-L225\r\n","Does data.table have a benchmarking testsuite to track regressions? -> is this a regression? has this code ever worked faster in the past?","I got this result\r\n![image](https:\/\/github.com\/Rdatatable\/data.table\/assets\/932850\/14445239-d3b8-42b4-a918-7955ef374ac0)\r\nwith this code\r\n```r\r\nlibrary(data.table)\r\natime.list <- atime::atime(\r\n  N=10^seq(3,8),\r\n  setup={\r\n    set.seed(1L)\r\n    df <- data.frame(\r\n      a = runif(N),\r\n      b = as.character(runif(N))\r\n    )\r\n    index <- runif(N) <= 0.5\r\n    dt <- as.data.table(df)\r\n  },\r\n  \"df[\"=df[index, \"a\"],\r\n  \"df[[\"=df[[\"a\"]][index],\r\n  \"dt[\"=dt[index, \"a\"],\r\n  \"dt[[\"=dt[[\"a\"]][index],\r\n  verbose=2)\r\nplot(atime.list)\r\n```","AFAIK it is not a regression. We could call it regression surely if we compare to data.table v1.0.0. Over years interface for processing and optimizing users input was only growing, so it was adding up. Question \"does it matter?\" is a matter of balance between common usage patterns and absolute time added. And here use case like that get definitely not a high priority.\r\n\r\nIt has been said multiple times in different places that it is better to vectorize your work on DT rather iteratively run many queries to DT, whenever possible. To avoid overhead of [. This is the cost of user friendliness that DT is packed with. There are some ideas, like `with=FALSE` to avoid such overhead, which IMO are step in good direction. The thing is that timing differences of atomic operations are so tiny that it is not getting higher priority.\r\nWhat would help is complete use case where user suffers at least multiple seconds from that. But then we probably will be able to present a better way which does not suffer from the overhead.\r\nTo really get it resolved at the root we need to move [ to C entirely, which IMO is not possible without making it more modular #852.","#4687 is the FR for continuous benchmarking"],"labels":["performance"]},{"title":"`[.data.table` is very slow with a single integer","body":"In this code:\r\n\r\n```R\r\nlibrary(data.table)\r\nparameters <- list(types = c(p1 = \"r\", p2 = \"r\", p3 = \"r\", dummy = \"c\"),\r\n                   digits = 4)\r\nn <- 10000\r\nnewConfigurations <- data.table(p1 = runif(n), p2 = runif(n), p3 = runif(n),\r\n                                dummy = sample(c(\"d1\", \"d2\"), n, replace=TRUE))\r\n\r\nrepair_sum2one <- function(configuration, parameters)\r\n{\r\n  isreal <- names(which(parameters$types[colnames(configuration)] == \"r\"))\r\n  digits <- parameters$digits[isreal]\r\n  c_real <- unlist(configuration[isreal])\r\n  c_real <- c_real \/ sum(c_real)\r\n  c_real[-1] <- round(c_real[-1], digits[-1])\r\n  c_real[1] <- 1 - sum(c_real[-1])\r\n  configuration[isreal] <- c_real\r\n  return(configuration)\r\n}\r\nj <- colnames(newConfigurations)\r\nfor (i in seq_len(nrow(newConfigurations)))\r\n      set(newConfigurations, i, j = j, value = repair_sum2one(as.data.frame(newConfigurations[i]), parameters))\r\n```\r\n\r\nMore than half the time is spent in `[.data.table`.  Even the function `repair_sum2one` is faster.\r\n\r\n_Originally posted by @MLopez-Ibanez in https:\/\/github.com\/Rdatatable\/data.table\/issues\/3735#issuecomment-1546753937_\r\n            ","comments":["AFAIR there is already PR that solves that, possibly authored by my person","which PR solves this? (would be useful in our efforts at performance testing)","https:\/\/github.com\/Rdatatable\/data.table\/pull\/4488\r\nPossibly"],"labels":["performance"]},{"title":"Using value.var = \".\" in a table with a column named \".\" gives wrong results","body":"```r\r\nDT = data.table(a = rep(1:3, 10), b = 2, rep_len(letters, 30))\r\ndcast(DT, a + b ~ ., fun.aggregate = length)[, dcast(.SD, a ~ b, value.var = \".\")]\r\n#    a 2\r\n# 1: 1 .\r\n# 2: 2 .\r\n# 3: 3 .\r\n\r\n# vs. renaming the column\r\n\r\ndcast(DT, a + b ~ ., fun.aggregate = length)[, x := .][, dcast(.SD, a ~ b, value.var = \"x\")]\r\n#    a  2\r\n# 1: 1 10\r\n# 2: 2 10\r\n# 3: 3 10\r\n```\r\n\r\nGlanced at `?dcast` and didn't see anything relevant, but it definitely feels like a bug anyway.","comments":[],"labels":["reshape"]},{"title":"Upcoming versions of base R eliminate the need for DT() functionality - consider eliminating?","body":">As an experimental feature the placeholder `_` can now also be used in the \u2018rhs\u2019 of a forward pipe `|>` expression as the first argument in an extraction call, such as `_$coef`. More generally, it can be used as the head of a chain of extractions, such as `_$coef[[2]]`.\r\n>\r\n> _[R-devel\/News, Mon, 13 Feb 2023](https:\/\/developer.r-project.org\/blosxom.cgi\/R-devel\/NEWS\/2023\/02\/13)_\r\n\r\nIn upcoming versions of R, syntax such as the following will be supported:\r\n\r\n```r\r\nas.data.table(mtcars) |> \r\n  _[am == 1] |> \r\n  _[, .(maxhp = max(hp)), by = .(cyl)]\r\n\r\n#    cyl maxhp\r\n# 1:   6   175\r\n# 2:   4   113\r\n# 3:   8   335\r\n```\r\n\r\nWith this in mind, I think it's worth reconsidering whether `DT()` should remain on the release roadmap.\r\n\r\nSome arguments in favor of scrapping `DT()` are as follows:\r\n\r\n- Base R functionality `dt |> _[...]` will address all use cases for `DT()`\r\n- Introducing an alternate syntax for calling `[.data.table` will be confusing for new users. Advertising this functionality also eliminates an opportunity to educate users about existing `dt[...][...]` chaining capabilities baked in by default\r\n- Exporting `DT()` will lead to namespace collision issues with the `DT::DT()` exported by the widely used [DT package](https:\/\/rstudio.github.io\/DT\/)\r\n- `DT()` acceptance of non `data.table` objects is likely to cause user confusion\r\n  - #5559 \r\n  - #5430 \r\n  - #5129 \r\n- Supporting `DT()` going forward will be a drain on valuable maintainer's time - [`DT()` Labeled Issues](https:\/\/github.com\/Rdatatable\/data.table\/issues?q=is%3Aissue+label%3ADT%28%29)\r\n- `DT()` has not yet been exported in a CRAN release so no there will no impacts to reverse dependencies and limited impact to users who have adopted this form\r\n\r\nThat being said, this is just my two cents as a satisfied user of `data.table` interested in the long term success of the package. Happy to hear counter arguments, and open to the idea that the broader community may see enough value to finish the push to support `DT()`. ","comments":["I agree it would be good to remove if possible, to avoid user confusion, and to save dev time.\r\n","given the number of headaches induced by what was supposed to be a \"simple\" wrapper, I would also be very happy to drop DT().\r\n\r\nthat said, I would pause on this issue until that syntax lands in a released version.","@markfairbanks care to elaborate? it will be important feedback.","I would love to have a `DT()` that would work on older versions of R (whether to v4.0.0 with the base pipe or with v3.4.0 with the magrittr pipe). It's good that they're adding the `_` placeholder for the base pipe but that won't help very much with backwards compatibility for users that don't always have access to the newest version of R.\r\n\r\nAnd as far as the current issues - every issue linked in the original comment goes back to the fact that `DT()` as currently constructed tries to work on data.frames. It seems much more logical to have `DT()` only work on data.tables instead of removing it entirely. Having it expand to data.frame capability can be added in a later version if people really want it (and then there would be more time to work out the kinks). Or if it's not something that's fixable it can just work on data.tables into the future.\r\n\r\nSomething like this:\r\n``` r\r\nlibrary(data.table)\r\n\r\nDT <- function(x, ...) {\r\n  if (!is.data.table(x)) {\r\n    stop(\"`x` must be a data.table\")\r\n  }\r\n  x[...]\r\n}\r\n\r\ndf <- data.table(x = 1:3, y = c(\"a\", \"a\", \"b\"))\r\n\r\ncopy(df) |>\r\n  DT(, double_x := x * 2) |>\r\n  DT()\r\n#>        x      y double_x\r\n#>    <int> <char>    <num>\r\n#> 1:     1      a        2\r\n#> 2:     2      a        4\r\n#> 3:     3      b        6\r\n```\r\n\r\nThere wouldn't be anything \"surprising\" with how it works because it would work exactly like `[.data.table` but work better with piping.",">that said, I would pause on this issue until that syntax lands in a released version.\r\n>@MichaelChirico\r\n\r\nAgree - but perhaps might be a reason to hold back `DT()` from CRAN releases in the interim? It looks like this change [made it into R 4.3 _(news item is included in changelog of release candidate R-latest.tar.gz)_](https:\/\/cran.r-project.org\/src\/base-prerelease\/), which has a [planned release date of April 21st, 2023](https:\/\/www.r-project.org\/), so there shouldn't be too long of a wait.\r\n\r\n>I would love to have a DT() that would work on older versions of R (whether to v4.0.0 with the base pipe or with v3.4.0 with the magrittr pipe).\r\n>@markfairbanks \r\n\r\nSince [backporting `|>` isn't an option](https:\/\/github.com\/r-lib\/backports\/issues\/69), the only portable solution is to use magrittr, which has always allowed `[` operations on the `.` placeholder. Does that address the desire here?\r\n\r\n```r\r\n## Works R > 3.4 using magrittr\r\nas.data.table(mtcars) %>%\r\n  .[am == 1] %>% \r\n  .[, .(maxhp = max(hp)), by = .(cyl)]\r\n\r\n```\r\n\r\n","I think it still makes more sense to have a functional `DT()` - a version that is designed to work with pipes without any extra thought or placeholders \ud83e\udd37\u200d\u2642\ufe0f In general the placeholder seems like a workaround, not integrated functionality.\r\n\r\n`DT()` also makes everything much more approachable for newer R users trying to use piping with `data.table` or `dplyr` users making the switch.","As we all got used to working with the dev version of `data.table()` I too had some use cases for `DT()` and grew reasonably fond of it -- in particular for one (only partially done) project where I translated some code 'from that other paradigm'.  \r\n\r\nSo with the caveat that I am not fully up to speed on any implementation headaches, my $0.02 are for shipping `DT()` as it is a nice usability improvement, especially for newbs.","I think that it would be useful to export `DT()` only when it will be mature enough to properly handle `data.frame` objects in general (`DT()` tested enough and all pending issues solved). Given the number of related issues, exporting `DT()` should not necessarily be considered for the upcoming data.table releases as it will take an important amount of time to solve all of them.\r\n\r\nI personally believe that the most important benefit of `DT()` is the fact that it can be used with data.frame objects in general and not only with data.tables. If only data.table object is considered, I think that the upcoming R version eliminates the need for `DT()`.\r\n\r\nBut I believe that `DT()` has one benefit that is not eliminated by the upcoming R version: it allows the use of data.table syntax on data.frame objects in general. I think that one of the main limitations of data.table so far is related to the fact that its syntax works only with data.table objects. So, solving the issues related to `DT()` will remove this limitation.\r\n\r\nWhile piping using the square brackets (repeatedly) was hard to follow and kept indenting the codes to the right, the upcoming R version literally solves this issue. I really like the fact that the square brackets are vertically aligned (with no additional indentation).\r\n\r\nIn conclusion, I would suggest to keep `DT()` only if it will also support data.frames in general (and not only data.tables); otherwise, I think that the upcoming R eliminates the need for `DT()`.\r\n","Whenever size is not a problem you can just alias DT=as.data.table and use [","another couple pieces of information that may be relevant to this discussion\r\n- There was at some point in the past another\/different DT, https:\/\/cloud.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-faq.html#DTremove1 \r\n- `DT=data.table(some.example.data)` is used frequently in documentation materials, so the new (old?) DT function would conflict with that (potentially confusing).","if we do keep DT, can someone (@markfairbanks or?) please volunteer to maintain it and review changes to it, by adding your name to the new\/proposed CODEOWNERS file? #5629 (currently DT is defined in data.table.R but maybe would be better to move it to a separate file, to make it clear who is responsible for it)","Anyway DT() is not going to land on CRAN as long as pending issues related to it are not being addressed. Putting it on CRAN having those issue would be a nightmare, and most likely new issues would be detected as well.\r\nFor new coming (non-patch) release what is currently planned is to un-export DT() as described in https:\/\/github.com\/Rdatatable\/data.table\/issues\/5472","FYI in the survey about 2\/3 of users indicated they'd find such a convenience function somewhat or very useful. (I've submitted a summary of results for publication on the data.table blog.)","One thing I don't like about the name DT is that it requires holding down the Shift key.\r\nAnd you'd type that a lot if you write pipe-heavy code.\r\n\r\nAn alternative I'd propose is `do`. Short and easy to type, no Shift needed, makes sense semantically as well.\r\nThe biggest drawback I can think of is that it's already a function in dplyr. But it was marked as superseded in v1 (and it's not like dplyr has never taken a name that was already in use :)).\r\n","I agree that something simple (and lowercase) would be useful. `do` makes some sense but I think the function should be clearly tied to `data.table` and should be something with minimal conflicts with other packages (especially `dplyr`). ","Since `dt` is already taken, maybe `td`? Probably too weird though.\r\n\r\nI don't quite get why the name should be 'clearly tied to data.table'. If the package includes e.g. `set`, why not `do`?","Yeah I know what you mean. I don't think it would have to be dt or td or anything with the exact same name. Just meant having some clear connection so it's easier for our users to find this function and know what it does. `do` I think is expressive in what it's doing but if there are conflicts with popular packages, we are just making it harder for adoption in this context. But maybe it's not too bad since do is being abandoned in dplyr. Can't think of another short word that would be as expressive as do. \n\n"],"labels":["DT()"]},{"title":"WISH\/BUG: Respect CPU resource limitations set by Linux CGroups to avoid CPU overuse and slowdown","body":"# Issue\r\n\r\n`data.table::getDTthreads()` is not agile to Linux CGroups settings.  If CGroups limits the number of CPU cores, then **data.table** will overuse the CPU resources is available to the R process.\r\n\r\n\r\nFor example, the ['Free' Posit Cloud plan](https:\/\/posit.cloud\/plans) gives you a single CPU core to play with.  They use CGroups v1 to limit the CPU resource.  Running the following from within their RStudio server reveals this:\r\n\r\n```r\r\n> total <- as.integer(readLines(\"\/sys\/fs\/cgroup\/cpu\/cpu.cfs_period_us\"))\r\n> total\r\n[1] 100000\r\n> quota <- as.integer(readLines(\"\/sys\/fs\/cgroup\/cpu\/cpu.cfs_quota_us\"))\r\n> quota\r\n[1] 100000\r\n> cores <- quota \/ total\r\n> cores\r\n[1] 1\r\n```\r\n\r\nA user on the 'Premium' plan has 4 CPUs to play with, so they would get `quota = 400000` and `cores = 4` above.\r\n\r\nThe defaults of **data.table** does _not_ pick this up:\r\n\r\n```r\r\n> data.table::getDTthreads(verbose = TRUE)\r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            16\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          16\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 8 threads with throttle==1024. See ?setDTthreads.\r\n[1] 8\r\n```\r\n\r\nThis means multi-threaded **data.table** tasks will overuse the CPU resources by 800%, which results in lots of overhead from context switching (unless there are other low-level mechanisms in **data.table** detecting this).  CPU overuse will slow down the performance.\r\n\r\nThe overuse problem becomes worse the more CPU cores the host has.  For example, the Posit Cloud instances currently runs with 16 vCPUs, but if they upgrade to say 64 vCPUs, the overuse will be 3200%.  On research HPC environments, it's now common to see 192 CPUs, and I'd expect this number to grow over time.\r\n\r\n\r\nFWIW, `parallelly::availableCores()` queries also CGroups\/CGroups v2, e.g.\r\n\r\n```r\r\n> parallelly:::availableCores()\r\ncgroups.cpuquota \r\n               1 \r\n\r\n> parallelly:::availableCores(which = \"all\")\r\n          system   cgroups.cpuset cgroups.cpuquota            nproc \r\n              16               16                1               16 \r\n```\r\n\r\n# Session info\r\n\r\n```r\r\n> sessionInfo()\r\nR version 4.2.3 (2023-03-15)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Ubuntu 20.04.5 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/atlas\/libblas.so.3.10.3\r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/atlas\/liblapack.so.3.10.3\r\n\r\nlocale:\r\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \r\n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \r\n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \r\n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.8\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.2.3 tools_4.2.3   \r\n```","comments":["similar to #5573 about using data.table on slurm cluster.\r\ncurrently we assume this kind of configuration should be handled by the user. For example, the user can set R_DATATABLE_NUM_THREADS environment variable.\r\nin terms of dev\/maintenance time, how many types of environment variables like this should we support? (SLURM, CGroups, ...?) how would we test each of them? given constraints on dev time, I would argue that it would be better to keep asking users to handle this.","> it would be better to keep asking users to handle this\n\nGiven that **data.table** is such a central infrastructure package used internally by many packages and pipelines, I wonder how many users even know they are using **data.table** yet know they need to configure the number threads it should use.\n\nFor the problem reported here, CGroups throttling, I believe there are lots of data.table instances out there running slower than a single-thread version would do, and this without anyone even noticing the problem. It's only the savvy user who would know that this could be a problem and that it should be fixed."],"labels":["openmp"]},{"title":"Numeric variable parsed as character when using comma decimals and separator in fread","body":"Quite new to fread and data.table. \r\nThe problem occurred when I had a dataset like quote1, which lead to var1 being interpreted as character, instead of numeric.\r\n\r\nManaged to isolate the issue in the following examples. There seem to be some kind of parsing issue when the first value is missing and it is followed by a date. It also seems related to  what separator is used. \r\n\r\n\r\n\r\n```r\r\nquote1 <- 'var1,var2\\n,2014-06-15\\n\"15,3\",2014-06-16'\r\ndata.table::fread(quote1, dec = ',', header = TRUE)\r\n\r\n# quote 1 output\r\n> data.table::fread(quote1, dec = ',', header = TRUE)\r\n   var1       var2\r\n1:      2014-06-15\r\n2: 15,3 2014-06-16\r\n\r\n\r\nquote2 <- 'var1,var2\\n\"15,3\",2014-06-15\\n\"15,3\",2014-06-16'\r\ndata.table::fread(quote2, dec = ',', header = TRUE)\r\n\r\n# quote 2 output\r\n> data.table::fread(quote2, dec = ',', header = TRUE)\r\n   var1       var2\r\n1: 15.3 2014-06-15\r\n2: 15.3 2014-06-16\r\n\r\nquote3 <- 'var1,var2\\n,\"2014-06-15\"\\n\"15,3\",\"2014-06-16\"'\r\ndata.table::fread(quote3, dec = ',', header = TRUE)\r\n\r\n# quote 3 output\r\n> data.table::fread(quote3, dec = ',', header = TRUE)\r\n   var1       var2\r\n1:   NA 2014-06-15\r\n2: 15.3 2014-06-16\r\n\r\nquote4 <- 'var1;var2\\n;2014-06-15\\n\"15,3\";2014-06-16'\r\ndata.table::fread(quote4, dec = ',', header = TRUE, sep = ';')\r\n\r\n# quote 4 output\r\n> data.table::fread(quote4, dec = ',', header = TRUE, sep = ';')\r\n   var1       var2\r\n1:   NA 2014-06-15\r\n2: 15.3 2014-06-16\r\n```\r\nTrying to use colClasses does not work either:\r\n```r\r\ndata.table::fread(quote1, dec = ',', header = TRUE, colClasses = c(\"var1\" = \"numeric\"))\r\n# output \r\n   var1       var2\r\n1:      2014-06-15\r\n2: 15,3 2014-06-16\r\n```\r\n","comments":[],"labels":["fread"]},{"title":"Unexpected behavior with fread when skip and nrows used and a  .csv file contains \"\\n\" within quotes","body":"Issue: ```fread``` with ```skip``` and ```nrows``` skips fewer rows than expected when .csv file contains \"\\n\" in a character variable.\r\n \r\nThis may be working as intended given the ```verbose==TRUE output````?\r\n\r\nHowever, this is not the result I expected \"\\n\" is within a string.\r\n\r\nI was working with chunks of a .csv file where the first \"\\n\" occurred on line 1,111,685 out of 153,009,891, so the issue was hard to find. It resulted in my chunks overlapping, giving mysterious duplicates. Given the file is 21.6 GB, pre-cleaning the .csv  of all instances of \"\\n\" with ```cmd``` would not be ideal. \r\n\r\nMinimum reproducible example:\r\n\r\n```\r\nlibrary(data.table)\r\nlibrary(vroom)\r\n\r\n# make data\r\nrow1<-c(1,2,3,4)\r\nrow2<-c(\"a\\na\",\"a\",\"a\",\"a\")\r\ndt<-data.table(cbind(row1,row2))\r\nfwrite(dt,\"temp.csv\")\r\n\r\n# try different values of skip\r\nfor(i in c(1,2,3)){\r\n  print(i)\r\n  fdf<-fread(\"temp.csv\",skip=i,nrows=2,header=FALSE)\r\n  vdf<-vroom(\"temp.csv\",skip=i,n_max=2,col_names=FALSE,delim=\",\",show_col_types = FALSE)\r\n  stopifnot(fdf$V1==vdf$X1)\r\n}\r\n```\r\n\r\nOutput with ```verbose=TRUE```\r\n\r\n```\r\n[1] 1\r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            8\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          8\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 4 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 4 threads (omp_get_max_threads()=8, nth=4)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  skip num lines = 1\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file temp.csv\r\n  File opened, size = 30 bytes.\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Skipped to line 2 in the file  Positioned on line 2 starting: <<1,\"a>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=','  with 2 lines of 2 fields using quote rule 0\r\n  Detected 2 columns on line 2. This line is either column names or first data row. Line starts as: <<1,\"a>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 2\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  'header' changed by user from 'auto' to false\r\n  Number of sampling jump points = 1 because nrow limit (2) supplied\r\n  Type codes (jump 000)    : 5C  Quote rule 0\r\n  All rows were sampled since file is small so we know nrow=2 exactly\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : 5C\r\n[10] Allocate memory for the datatable\r\n  Allocating 2 column slots (2 - 0 dropped) with 2 rows\r\n[11] Read the data\r\n  jumps=[0..1), chunk_size=1048576, total_size=19\r\nRead 2 rows x 2 columns from 30 bytes file in 00:00.000 wall clock time\r\n[12] Finalizing the datatable\r\n  Type counts:\r\n         1 : int32     '5'\r\n         1 : string    'C'\r\n=============================\r\n   0.000s ( 38%) Memory map 0.000GB file\r\n   0.000s ( 47%) sep=',' ncol=2 and header detection\r\n   0.000s (  4%) Column type detection using 2 sample rows\r\n   0.000s (  4%) Allocation of 2 rows x 2 cols (0.000GB) of which 2 (100%) rows used\r\n   0.000s (  6%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 2 rows) using 1 threads\r\n   +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)\r\n   +    0.000s (  0%) Transpose\r\n   +    0.000s (  6%) Waiting\r\n   0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions\r\n   0.000s        Total\r\n[1] 2                                                                                                               \r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            8\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          8\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 4 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 4 threads (omp_get_max_threads()=8, nth=4)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  skip num lines = 2\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file temp.csv\r\n  File opened, size = 30 bytes.\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Skipped to line 3 in the file  Positioned on line 3 starting: <<a\">>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=','  with 1 lines of 2 fields using quote rule 0\r\n  Detected 2 columns on line 4. This line is either column names or first data row. Line starts as: <<2,a>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 2\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  'header' changed by user from 'auto' to false\r\n  Number of sampling jump points = 1 because nrow limit (2) supplied\r\n  Type codes (jump 000)    : 5C  Quote rule 0\r\n  All rows were sampled since file is small so we know nrow=2 exactly\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : 5C\r\n[10] Allocate memory for the datatable\r\n  Allocating 2 column slots (2 - 0 dropped) with 2 rows\r\n[11] Read the data\r\n  jumps=[0..1), chunk_size=1048576, total_size=11\r\nRead 2 rows x 2 columns from 30 bytes file in 00:00.000 wall clock time\r\n[12] Finalizing the datatable\r\n  Type counts:\r\n         1 : int32     '5'\r\n         1 : string    'C'\r\n=============================\r\n   0.000s ( 34%) Memory map 0.000GB file\r\n   0.000s ( 48%) sep=',' ncol=2 and header detection\r\n   0.000s (  5%) Column type detection using 2 sample rows\r\n   0.000s (  4%) Allocation of 2 rows x 2 cols (0.000GB) of which 2 (100%) rows used\r\n   0.000s (  8%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 2 rows) using 1 threads\r\n   +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)\r\n   +    0.000s (  1%) Transpose\r\n   +    0.000s (  7%) Waiting\r\n   0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions\r\n   0.000s        Total\r\n[1] 3                                                                                                               \r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            8\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          8\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 4 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 4 threads (omp_get_max_threads()=8, nth=4)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  skip num lines = 3\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file temp.csv\r\n  File opened, size = 30 bytes.\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Skipped to line 4 in the file  Positioned on line 4 starting: <<2,a>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=','  with 2 lines of 2 fields using quote rule 0\r\n  Detected 2 columns on line 4. This line is either column names or first data row. Line starts as: <<2,a>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 2\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  'header' changed by user from 'auto' to false\r\n  Number of sampling jump points = 1 because nrow limit (2) supplied\r\n  Type codes (jump 000)    : 5C  Quote rule 0\r\n  All rows were sampled since file is small so we know nrow=2 exactly\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : 5C\r\n[10] Allocate memory for the datatable\r\n  Allocating 2 column slots (2 - 0 dropped) with 2 rows\r\n[11] Read the data\r\n  jumps=[0..1), chunk_size=1048576, total_size=11\r\nRead 2 rows x 2 columns from 30 bytes file in 00:00.000 wall clock time\r\n[12] Finalizing the datatable\r\n  Type counts:\r\n         1 : int32     '5'\r\n         1 : string    'C'\r\n=============================\r\n   0.000s ( 31%) Memory map 0.000GB file\r\n   0.000s ( 50%) sep=',' ncol=2 and header detection\r\n   0.000s (  8%) Column type detection using 2 sample rows\r\n   0.000s (  5%) Allocation of 2 rows x 2 cols (0.000GB) of which 2 (100%) rows used\r\n   0.000s (  7%) Reading 1 chunks (0 swept) of 1.000MB (each chunk 2 rows) using 1 threads\r\n   +    0.000s (  0%) Parse to row-major thread buffers (grown 0 times)\r\n   +    0.000s (  0%) Transpose\r\n   +    0.000s (  6%) Waiting\r\n   0.000s (  0%) Rereading 0 columns due to out-of-sample type exceptions\r\n   0.000s        Total\r\n```\r\n\r\nOutput of ```sessionInfo()```\r\n\r\n```\r\n> sessionInfo()\r\nR version 4.1.2 (2021-11-01)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Ubuntu 22.04.2 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libblas.so.3\r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/openblas-pthread\/libopenblasp-r0.3.20.so\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \r\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \r\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \r\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] vroom_1.6.1       data.table_1.14.8\r\n\r\nloaded via a namespace (and not attached):\r\n [1] fansi_1.0.4      withr_2.5.0      utf8_1.2.3       tzdb_0.3.0      \r\n [5] crayon_1.5.2     lifecycle_1.0.3  magrittr_2.0.3   pillar_1.8.1    \r\n [9] rlang_1.0.6      cli_3.6.0        rstudioapi_0.14  vctrs_0.5.2     \r\n[13] tools_4.1.2      bit64_4.0.5      glue_1.6.2       bit_4.0.5       \r\n[17] parallel_4.1.2   compiler_4.1.2   pkgconfig_2.0.3  tidyselect_1.2.0\r\n[21] tibble_3.1.8   \r\n```","comments":[],"labels":["fread"]},{"title":"`by` loops that should be possible fail because `dogroups` grows beyond 2**31","body":"```\r\n> data.table(i = 1:100)[, verbose = T, by = i, {message(\".GRP: \", .GRP); list(v = logical(1e7))}]\r\nDetected that j uses these columns: <none> \r\nFinding groups using forderv ... forder.c received 100 rows and 1 columns\r\n0.000s elapsed (0.000s cpu) \r\nFinding group sizes from the positions (can be avoided to save RAM) ... 0.000s elapsed (0.000s cpu) \r\nlapply optimization is on, j unchanged as '{'\r\nOld mean optimization is on, left j unchanged.\r\nMaking each group and running j (GForce FALSE) ... .GRP: 1\r\n.GRP: 2\r\ndogroups: growing from 10000000 to -2147483648 rows\r\nError in `[.data.table`(data.table(i = 1:100), , verbose = T, by = i,  : \r\n  negative length vectors are not allowed\r\n```\r\n\r\nI know that data.table still doesn't support long vectors (#3957), but this operation should only require 100 * 1e7 = 1e9 rows, which is well under 2**31 - 1 \u2248 2.1e9. And if the user really does ask for too many rows, the error message can surely be improved.\r\n\r\n# Output of `sessionInfo()`\r\n\r\n```\r\nR version 4.2.1 (2022-06-23)                                                    \r\nPlatform: x86_64-pc-linux-gnu (64-bit)                                          \r\nRunning under: Ubuntu 20.04.4 LTS                                               \r\n                                                                                \r\nMatrix products: default                                                        \r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/blas\/libblas.so.3.9.0                         \r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/lapack\/liblapack.so.3.9.0                            \r\n                                                                                       \r\nlocale:                                                                                       \r\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C                                                           \r\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8                                                           \r\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8                                                          \r\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                                                                        \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C                                                                   \r\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C                                                              \r\n\r\nattached base packages:                                 \r\n[1] stats     graphics  grDevices utils     datasets  methods   base                                             \r\n\r\nother attached packages:                                \r\n[1] data.table_1.14.8                                   \r\n\r\nloaded via a namespace (and not attached):                                                                       \r\n[1] compiler_4.2.1\r\n```","comments":["This is triggered by the internal estimate of rows of the result, which exceeds the 2**31-1 in your example. Subsequently, all column vectors are grown to that estimated length.\r\n\r\nApart from supporting long vectors which would include basically touching all C-files, we could try to improve that estimate","It might be easier and more reliable to just cap the estimate at 2**31 - 1.","Just capping the estimate at 2**31-1 unfortunately just kills the R process (if the cap is exceeded) which doesn't seem like an improvement to me","But the cap wouldn't be exceeded here, right? So wouldn't it make this code work? Better error messages for trying to make a data table that has too many rows would also be helpful, clearly, but that's supplementary.","In your example, it definitely works and gives the right output.\r\n\r\nBut slightly changing your example to `data.table(i = 1:220)[, verbose = T, by = i, {message(\".GRP: \", .GRP); list(v = logical(1e7))}]` kills the R process at group 147"],"labels":["longvec"]},{"title":"blank.lines.skip won't work when the top lines is empty","body":"I want to keep the total line number. Following code should return 3 rows, but it always return 1 row\r\n```\r\nfile2 <- tempfile()\r\nwriteLines(c(\"\", \"\", NA), file2)\r\ndata.table::fread(file2, blank.lines.skip = FALSE)\r\n```\r\n![image](https:\/\/user-images.githubusercontent.com\/19575010\/224280089-04f17979-b216-45aa-8c65-bded868d3aa7.png)\r\n","comments":["Add a reprex, and use `readLines()` to see the difference\r\n``` r\r\nfile <- tempfile()\r\nwriteLines(c(\"\", \"\", NA), file)\r\nreadLines(file)\r\n#> [1] \"\"   \"\"   \"NA\"\r\ndata.table::fread(file, blank.lines.skip = FALSE)\r\n#>        V1\r\n#>    <lgcl>\r\n#> 1:     NA\r\nsessionInfo()\r\n#> R version 4.3.1 (2023-06-16)\r\n#> Platform: x86_64-pc-linux-gnu (64-bit)\r\n#> Running under: Ubuntu 22.04.3 LTS\r\n#>\r\n#> Matrix products: default\r\n#> BLAS\/LAPACK: \/usr\/lib\/x86_64-linux-gnu\/libmkl_rt.so;  LAPACK version 3.8.0\r\n#>\r\n#> locale:\r\n#>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8\r\n#>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8\r\n#>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C\r\n#> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C\r\n#>\r\n#> time zone: Asia\/Shanghai\r\n#> tzcode source: system (glibc)\r\n#>\r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base\r\n#>\r\n#> loaded via a namespace (and not attached):\r\n#>  [1] digest_0.6.33     styler_1.10.1     fastmap_1.1.1     xfun_0.39\r\n#>  [5] magrittr_2.0.3    glue_1.6.2        R.utils_2.12.2    knitr_1.43\r\n#>  [9] htmltools_0.5.5   rmarkdown_2.23    lifecycle_1.0.3   cli_3.6.1\r\n#> [13] R.methodsS3_1.8.2 vctrs_0.6.3       reprex_2.0.2      data.table_1.14.9\r\n#> [17] withr_2.5.0       compiler_4.3.1    R.oo_1.25.0       R.cache_0.16.0\r\n#> [21] purrr_1.0.1       tools_4.3.1       evaluate_0.21     yaml_2.3.7\r\n#> [25] rlang_1.1.1       fs_1.6.3\r\n```\r\n\r\n<sup>Created on 2023-11-03 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n                        ","We should probably update the documentation that blank lines at the beginning of a file are always skipped until the first non-empty row is encountered. This is also mentioned by the verbose output (shortened it to highlight the important part).\r\n\r\n``` r\r\nlibrary(data.table)\r\nfile2 <- tempfile()\r\nwriteLines(c(\"\", \"\", \"a\"), file2)\r\nfread(file2, blank.lines.skip = FALSE, verbose=TRUE)\r\n#> [05] Skipping initial rows if needed\r\n#>   Positioned on line 3 starting: <<a>>\r\n```","It\u2019s not the document, we want to completely turn off the skipping of blank lines","I get that but changing the behavior of blank.lines.skip = FALSE would probably break a lot of existing code so I doubt we will do that.\r\n\r\nPossible changes I could imagine would be to introduce another argument or let blank.lines.skip accept other values to indicate you do not want to skip blank lines at the beginning."],"labels":["documentation","fread"]},{"title":"fread crash when using fill=TRUE option in a file with bad quotes","body":"fread crashes when trying to call: \r\n\r\n```\r\ndata.table::fread(\"testDoc.csv\",\r\n                         sep = \";\",\r\n                         quote = \"\\\"\",\r\n                         header = TRUE,\r\n                         fill = TRUE)\r\n```\r\n\r\non this file, that I reduced to a minimal example:  [testDoc.csv](https:\/\/github.com\/Rdatatable\/data.table\/files\/10933108\/testDoc.csv)\r\n\r\nIt seems to be due to the bad quotes in the last line, combined with fill = TRUE.\r\nIf the file is shorter, fread just returns an error without crashing, which is expected and not a problem. \r\n\r\nThe crash is consistently reproduced for me. I'm on windows 11, and tried it with data.table 1.14.0, as well as the latest 1.14.8, both on R.3.6.3 and R.4.2.2\r\n\r\nI think it might be related to the bug report: https:\/\/github.com\/Rdatatable\/data.table\/issues\/5110\r\n\r\n","comments":["Could you specify what you mean by \"crash\"? Does it segfault or simply stop reading the file?","I can report a similar problem. \r\n- Segfault.\r\n- If quote = \"\", it will solve the problem for the above reproducible example. For my example (not yet attached), it outputs a message such as \r\n```\r\nStopped early on line 246469. Expected 39 fields but found 40. Consider fill=TRUE and comment.char=.\r\n```\r\n\r\nwhich is slightly odd error message to begin with, because `fill = TRUE` is already specified, as well as the fact that `comment.char` is no longer an argument of `fread`?\r\n\r\nWhat's maddening about my case is that I can't actually find a bad quote in the said line, so I'm still trying to create a minimal reproducible example...\r\n\r\nIn any case, `readr::read_csv` didn't have this problem if this information benefits anyone.","Hi, sorry for the late reply, I had missed the notification.\r\nYes, it's an actual crash, the R process is killed (so segfault, or something equivalent). It doesn't just freeze or return.","I can't reproduce it on the current dev. You can update to current dev by using `data.table::update_dev_pkg()`\r\n","Thanks! That worked.\r\nA bit more details for people stumbling upon this in the future: using `data.table::update_dev_pkg()` got me to dev version 1.14.9. Now, using the same minimal example, I get the (very justified) warning: \r\n```\r\n\"Warning message:\r\nIn data.table::fread(\"testDoc.csv\", sep = \";\", quote = \"\\\"\", header = TRUE,  :\r\n  Found and resolved improper quoting out-of-sample. First healed line 102: <<\"860003110\";\"S.A.R.L. \\\\\\\"\\\"LA GIBAUDERIE\\\\\"\\\"\\\\\"\\\"\\\"\\\"\";\"CLINIQUE\";\"Nouvelle-Aquitaine\";NA;NA;NA;0;0;0;0;0;0;0;0>>. If the fields are not quoted (e.g. field separator does not appear within any field), try quote=\"\" to avoid this warning.\"\r\n```\r\n\r\nAnd the dataframe is correctly read."],"labels":["fread"]},{"title":"Consider Enabling GitHub Discussions","body":"I've been paying closer attention to this repo in the last year or so and it struck me that there are hundreds of open irrelevant issues relate more to technical support rather than filing bug reports or feature requests. \r\n\r\nSince all development for this project happens on GitHub, I think it would make sense to set the [GitHub Discussions](https:\/\/docs.github.com\/en\/discussions) tab. \r\n\r\nThis would allow for a reduction in the ever-increasing number of issues and better engagement from the community as it could all happen on the discussions tab.\r\n\r\nWhat are your thoughts?","comments":["If you could post some examples then it will help. Personally I prefer issues only, less places to keep track and less places to search when looking for something.","Few examples from R world would be [mlr3 ](https:\/\/github.com\/mlr-org\/mlr3\/discussions)and [quarto](https:\/\/github.com\/quarto-dev\/quarto-cli\/discussions). It's also possible to convert issues into discussions ([link](https:\/\/docs.github.com\/en\/discussions\/managing-discussions-for-your-community\/moderating-discussions#converting-an-issue-to-a-discussion)) and creating issues linked to discussions ([link](https:\/\/docs.github.com\/en\/discussions\/managing-discussions-for-your-community\/managing-discussions#converting-issues-based-on-labels)). \r\n\r\nI think that would help some discussions to resurface. To give an example, without reading https:\/\/github.com\/Rdatatable\/data.table\/issues\/5425 it would not occur to me that https:\/\/github.com\/Rdatatable\/data.table\/issues\/1523 was tracking all `print.data.table` enhancements. Because it was initially open in 2016, it got buried by new issues and it's got a poor visibility. \r\n\r\nI think that the discussions tab would help resurface these, as the most important discussion topics could be pinned without messing up the issues tab. This could encourage people who see some of these suggestions or ideas to contribute to the project. ","Another reason why I am against...\r\n\r\nActually I was as well against GH wiki, in favor of extra markdown documentation file inside the package code inst\/doc. GH wiki is fortunately accessible via git interface as well so it is less relevant.\r\n\r\n...is that we are locking ourselves into MS owned product, a company which history is not very bright. Ideally would be to keep everything in git, or git metadata (like wiki). IMO the less product specific lock-in feature that are not part of git protocol, the better. Of course we won't eliminate all.\r\nIn some way it is like managing dependencies, in this case, of a project rather than a code. The less we are dependent on a particular product to run our project the better.\r\n\r\nAs the suggestion is getting stale I would like to close it. Maybe more contributors to DT code could express their opinions, or upvote\/downvote comments here, so we know better if we want to proceed with this proposal."],"labels":["vote"]},{"title":"dcast outputs column name V1 for empty string","body":"Hi! I expected that `dcast(DT, .~column)` should always output a data table that has columns named `unique(paste(DT$column))` but here is a counter-example:\r\n```r\r\n> x=c(\"NA\",\"\",NA,\"foo\");DT=data.table(x);out=dcast(DT,.~x,length)\r\n> x\r\n[1] \"NA\"  \"\"    NA    \"foo\"\r\n> names(out)\r\n[1] \".\"   \"NA\"  \"V1\"  \"NA\"  \"foo\"\r\n> out\r\n   . NA V1 NA foo\r\n1: .  1  1  1   1\r\n```\r\nThe issue above is that the empty string value is mapped to the column name `V1`, which is confusing. I would propose to fix by changing that output column name to empty string, as below:\r\n```r\r\n> setnames(out,c(\".\",\"NA\",\"\",\"NA\",\"foo\"))\r\n> out\r\n   . NA   NA foo\r\n1: .  1 1  1   1\r\n```\r\n\r\nAnother thing I noticed is that both `NA` (missing value) and `\"NA\"` (string) map to two different columns (with the same name, \"NA\"), is that normal? (should there be a warning if dcast outputs columns with the same name?)","comments":["I guess the initial intention was to avoid duplicate names similar to `make.names(names, unique=TRUE)`, hence, I find it quite surprising that you can sneak in the double `\"NA\"`."],"labels":["reshape"]},{"title":"Export masks for all NSE-only constructs (currently at least '.', 'J', 'patterns')","body":"This subsumes #5277.\r\n\r\nWe export masks for `.N`, `.I`, `.GRP`, `.GRPI`, `.SD`, and `:=` to facilitate downstreams doing `@importFrom data.table .N` to hide `R CMD check` notes about undefined variables.\r\n\r\nWe should do the same for all the NSE constructs we interpret inside `[` \/ `melt()`. Currently, there's no export for `.`, `J`, and `patterns`, all of which could be masked as `function(...) NULL`.","comments":["Adding `measure()` to the list as indicated by @tdhock in #5277","wow that is a great idea"],"labels":["documentation","consistency","beginner-task"]},{"title":"Non-equi join showing columns in a not so perfect style","body":"In the latest version of dplyr, non-equi joins could be realized by:\r\n```\r\nlibrary(dplyr)\r\n\r\ntransactions <- tibble(\r\n  company = c(\"A\", \"A\", \"B\", \"B\"),\r\n  year = c(2019, 2020, 2021, 2023),\r\n  revenue = c(50, 4, 10, 12)\r\n)\r\ntransactions\r\n#> # A tibble: 4 \u00d7 3\r\n#>   company  year revenue\r\n#>   <chr>   <dbl>   <dbl>\r\n#> 1 A        2019      50\r\n#> 2 A        2020       4\r\n#> 3 B        2021      10\r\n#> 4 B        2023      12\r\n\r\ncompanies <- tibble(\r\n  id = c(\"A\", \"B\", \"B\"),\r\n  since = c(1973, 2009, 2022),\r\n  name = c(\"Patagonia\", \"RStudio\", \"Posit\")\r\n)\r\n\r\ncompanies\r\n#> # A tibble: 3 \u00d7 3\r\n#>   id    since name     \r\n#>   <chr> <dbl> <chr>    \r\n#> 1 A      1973 Patagonia\r\n#> 2 B      2009 RStudio  \r\n#> 3 B      2022 Posit\r\n\r\n\r\ntransactions |>\r\n  inner_join(companies, join_by(company == id, year >= since))\r\n#> # A tibble: 5 \u00d7 5\r\n#>   company  year revenue since name     \r\n#>   <chr>   <dbl>   <dbl> <dbl> <chr>    \r\n#> 1 A        2019      50  1973 Patagonia\r\n#> 2 A        2020       4  1973 Patagonia\r\n#> 3 B        2021      10  2009 RStudio  \r\n#> 4 B        2023      12  2009 RStudio  \r\n#> 5 B        2023      12  2022 Posit\r\n\r\n```\r\nUsing data.table, I yield:\r\n```\r\nlibrary(data.table)\r\n#> \r\n#> Attaching package: 'data.table'\r\n#> The following objects are masked from 'package:dplyr':\r\n#> \r\n#>     between, first, last\r\nsetDT(transactions)\r\nsetDT(companies)\r\ntransactions[companies, on = .(company == id, year >= since)]\r\n#>    company year revenue      name\r\n#> 1:       A 1973      50 Patagonia\r\n#> 2:       A 1973       4 Patagonia\r\n#> 3:       B 2009      10   RStudio\r\n#> 4:       B 2009      12   RStudio\r\n#> 5:       B 2022      12     Posit\r\n```\r\nThe operation is the same, but the results is not desirable as that of dplyr's, and dplyr also supports to keep all the columns using argument `keep = TRUE`. How can I get something similar in data.table?\r\n\r\nThanks.","comments":["You can do it like this: \r\n\r\n``` r\r\nlibrary(data.table)\r\n\r\n(transactions <- data.table(\r\n    company = c(\"A\", \"A\", \"B\", \"B\"),\r\n    year = c(2019, 2020, 2021, 2023),\r\n    revenue = c(50, 4, 10, 12)\r\n))\r\n#>    company  year revenue\r\n#>     <char> <num>   <num>\r\n#> 1:       A  2019      50\r\n#> 2:       A  2020       4\r\n#> 3:       B  2021      10\r\n#> 4:       B  2023      12\r\n\r\n(companies <- data.table(\r\n    id = c(\"A\", \"B\", \"B\"),\r\n    since = c(1973, 2009, 2022),\r\n    name = c(\"Patagonia\", \"RStudio\", \"Posit\")\r\n))\r\n#>        id since      name\r\n#>    <char> <num>    <char>\r\n#> 1:      A  1973 Patagonia\r\n#> 2:      B  2009   RStudio\r\n#> 3:      B  2022     Posit\r\n\r\ntransactions[\r\n    companies,\r\n    mget(c(names(transactions), \"since\")),\r\n    on = .(company == id, year >= since)\r\n]\r\n#>    company  year revenue since\r\n#>     <char> <num>   <num> <num>\r\n#> 1:       A  1973      50  1973\r\n#> 2:       A  1973       4  1973\r\n#> 3:       B  2009      10  2009\r\n#> 4:       B  2009      12  2009\r\n#> 5:       B  2022      12  2022\r\n```\r\n\r\n<sup>Created on 2023-02-15 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\nI based my answer on [this SO post](https:\/\/stackoverflow.com\/a\/41638655\/13158146) \r\n","there is a solution in #3093 which is in progress."],"labels":["non-equi joins"]},{"title":"when using bind_rows on a list of data.tables with keys, with either map_dfr or a do.call, the keys are not removed This is a problem, because the keys are not correct anymore, which means that later queries are using an index that is wrong. ","body":"\r\nnot sure if I should report this under data.table or dplyr, but since the issue is in bin_rows, I guess it is dplyr - but I think it's an issue for data.table users mostly  - and I think it is pretty serious, since it can completely mess up  data integrity, with no errors, no warnings, no way of knowing it happened except by external means, e.g. seeing that the data output  just can't be right - this happened to me last week and it made a pretty big mess of things. \r\n\r\n[here is the issue I filed under dplyr](https:\/\/github.com\/tidyverse\/dplyr\/issues\/6676)\r\n\r\n\r\n\r\n\r\n```\r\n\r\n\r\nlibrary(data.table)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\n\r\ndata <- data.table(var1=c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'), var2=c(rep(2016, 4), rep(2018, 4)), key=c('var1', 'var2'))\r\n\r\n\r\ntest1 <- lapply(c(2018, 2016), function(x) data[var2 %in% c(x-1, x)])\r\n\r\n\r\ntest2 <- do.call(bind_rows, test1)\r\n# wrong \r\ntest2[var1 %in% c('c', 'e'), .N]\r\n# right\r\nsum( test2$var1 %in% c('c', 'e') )\r\n\r\n\r\n```\r\n\r\nsame thing  happens w. map_dfr,that uses bind_rows under the hood. This is how I found the issue. It is very common in my workflow, and I suspect I am not the one.\r\n\r\n\r\ndoing the same with rbind removes the keys, which is the expected behaviour - as the index from the key are no longer valid.\r\n\r\n\r\n```\r\n\r\ntest2_rbind <- do.call(rbind, test1) \r\n# here it is fine - because the keys not there, giving a wrong picture of the structure of the DT\r\ntest2_rbind[var1 %in% c('c', 'e'), .N]\r\n\r\n# when removing the keys, everything is fine. \r\nsetkey(test2, NULL)\r\ntest2[var1 %in% c('c', 'e'), .N]\r\n\r\n\r\n```\r\n\r\n\r\n\r\ncuriosly, using split() instead of lapply circumvents the issue - but this is not a solution, since typically you need to apply a function to the data, - splitting it and then recombining it with split() makes no sense. But I am including it here for completeness.\r\n\r\n\r\n```\r\n\r\n\r\ntest1 <- split(data, data$var2)\r\ntest2 <- do.call(bind_rows, test1)\r\n# right \r\ntest2[var1 %in% c('c', 'e'), .N]\r\n\r\n```\r\n\r\nsessionInfo\r\n\r\n```\r\n\r\nR version 4.2.2 (2022-10-31)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Manjaro Linux\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/libblas.so.3.11.0\r\nLAPACK: \/usr\/lib\/liblapack.so.3.11.0\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_DK.UTF-8       LC_NUMERIC=C\r\n [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_DK.UTF-8\r\n [5] LC_MONETARY=en_DK.UTF-8    LC_MESSAGES=en_DK.UTF-8\r\n [7] LC_PAPER=en_DK.UTF-8       LC_NAME=C\r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C\r\n[11] LC_MEASUREMENT=en_DK.UTF-8 LC_IDENTIFICATION=C\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base\r\n\r\nother attached packages:\r\n[1] dplyr_1.0.10      data.table_1.14.6\r\n\r\nloaded via a namespace (and not attached):\r\n [1] fansi_1.0.4      assertthat_0.2.1 utf8_1.2.2       R6_2.5.1\r\n [5] DBI_1.1.3        lifecycle_1.0.3  magrittr_2.0.3   pillar_1.8.1\r\n [9] rlang_1.0.6      cli_3.6.0        vctrs_0.5.2      generics_0.1.3\r\n[13] glue_1.6.2       compiler_4.2.2   pkgconfig_2.0.3  tidyselect_1.2.0\r\n[17] tibble_3.1.8\r\n\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n","comments":["Similar to #5569","Well yes, except that in this case,  we DON'T want to preserve the key attribute, so it's a good thing rbindlist  doesn't do that in this case"],"labels":["rbindlist"]},{"title":"adapt uneven time series rolling window","body":"closes #3241\r\n\r\nIt is forked from https:\/\/github.com\/Rdatatable\/data.table\/tree\/frollapply branch, therefore should be rebased to master once #5575 will be merged. Till then it is probably best to look at the diff here: https:\/\/github.com\/Rdatatable\/data.table\/compare\/frollapply..adapt\r\n\r\n----\r\n\r\nAs well spotted by @DavisVaughan frolladapt only supports align=right for the moment.","comments":["@jangorecki is `frolladapt()` meant to only be used with right alignment? IIUC, it seems like `frolladapt()` produces adaptive widths that assume right alignment, so if you set `align = \"left\"` in the actual `frollmean()` call it doesn't seem right (at least to me).\r\n\r\nI see `align does not support \"center\"` in the docs of `?frollmean` but nothing about `\"left\"`\r\n\r\n``` r\r\nlibrary(data.table)\r\nlibrary(slider)\r\n\r\nx <- 1:5\r\ni <- c(1, 3, 5, 6, 8)\r\n\r\n# These look specific to right alignment? i.e.\r\n# - Range [0, 1], but first element so incomplete window\r\n# - Range [2, 3], only 3 is there so size 1 (but note this aligns right)\r\n# - Range [4, 5], only 5 is there so size 1\r\n# - Range [5, 6], 5 and 6 are there so size 2 (note this aligns right)\r\n# - Range [7, 8], only 8 is there so size 1\r\nfrolladapt(i, 2)\r\n#> [1] NA  1  1  2  1\r\n\r\nfrollmean(x, frolladapt(i, 2), adaptive = TRUE, align = \"right\")\r\n#> [1]  NA 2.0 3.0 3.5 5.0\r\nslide_index_mean(x, i, before = 1, complete = TRUE)\r\n#> [1]  NA 2.0 3.0 3.5 5.0\r\n\r\nfrollmean(x, frolladapt(i, 2), adaptive = TRUE, align = \"left\")\r\n#> [1]  NA 2.0 3.0 4.5 5.0\r\nslide_index_mean(x, i, after = 1, complete = TRUE)\r\n#> [1] 1.0 2.0 3.5 4.0  NA\r\n```\r\n\r\n<sup>Created on 2023-01-08 with [reprex v2.0.2.9000](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\n","Thank you. That's a good observation. I have overlooked align=left. There needs to be align argument to frolladapt, so the n argument can be prepared accordingly.","There are benchmarks made on top of this branch implemented in https:\/\/github.com\/duckdblabs\/db-benchmark\/pull\/9. @DorisAmoakohene may be interesed in it, steps to reproduce are at the end of the thread.","This branch is forked from frollapply branch but not from its head, therefore does not include all changes made in frollapply branch, missing commits: https:\/\/github.com\/Rdatatable\/data.table\/compare\/d09651710993b6bc7ab00fb1a7e53f9361bd6db8..21da019f63947c28f9cb93fdf8f275bd52461dd5\r\n\r\ndiff vs commit where it was forked from: https:\/\/github.com\/Rdatatable\/data.table\/compare\/d09651710993b6bc7ab00fb1a7e53f9361bd6db8..319c02788d32169e1c9f26bbb59dee0012eaa5e1"],"labels":["froll"]},{"title":"frollapply rewritten, supports by.column=F","body":"closes #4887\r\n\r\nIt is forked from https:\/\/github.com\/Rdatatable\/data.table\/tree\/frollmax branch, therefore should be rebased to master once #5441 will be merged. Till then it is probably best to look at the diff here: https:\/\/github.com\/Rdatatable\/data.table\/compare\/frollmax..frollapply","comments":["Exact diff when forked from frollmax branch https:\/\/github.com\/Rdatatable\/data.table\/compare\/a14b486e4978da65c469eb39f41c19018e91f846..21da019f63947c28f9cb93fdf8f275bd52461dd5","@jangorecki would you mind marking as draft any froll-related PR which should wait for review? I assume you have a sequence in mind for when they should be merged to avoid conflicts.","I think drafts make sense when new changes are expected, other then rebasing\/merging to master. This PR is ready for review as much as it can, so marking it as draft takes away the chance for extra eyes looking at it."],"labels":["froll"]},{"title":"data.table doesn't seem to be using multiple cores on Slurm cluster. How to troubleshoot?","body":"I'm using data.table on a SLURM cluster and for some reason it's having trouble using multiple cores on something as simple as fread, even though it's detecting them when loading the library. The file is a 46GB tab-delimited file in 4-column long format.\r\n\r\n```\r\nR version 4.1.2 (2021-11-01) -- \"Bird Hippie\"\r\nCopyright (C) 2021 The R Foundation for Statistical Computing\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\n\r\nR is free software and comes with ABSOLUTELY NO WARRANTY.\r\nYou are welcome to redistribute it under certain conditions.\r\nType 'license()' or 'licence()' for distribution details.\r\n\r\n  Natural language support but running in an English locale\r\n\r\nR is a collaborative project with many contributors.\r\nType 'contributors()' for more information and\r\n'citation()' on how to cite R or R packages in publications.\r\n\r\nType 'demo()' for some demos, 'help()' for on-line help, or\r\n'help.start()' for an HTML browser interface to help.\r\nType 'q()' to quit R.\r\n\r\n> library(data.table)\r\ndata.table 1.14.2 using 3 threads (see ?getDTthreads).  Latest news: r-datatable.com\r\n> options(datatable.verbose = TRUE)          \r\n> tmp <- fread(\"[..REDACTED..]\")\r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            6\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          6\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                6\r\n  RestoreAfterFork               true\r\n  data.table is using 3 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 3 threads (omp_get_max_threads()=6, nth=3)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file [..REDACTED..]\r\n  File opened, size = 45.65GB (49011968734 bytes).\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<chr1 10468   10469   0.0     P7740_237>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=0x9  with 100 lines of 5 fields using quote rule 0\r\n  Detected 5 columns on line 1. This line is either column names or first data row. Line starts as: <<chr1      10468   10469   0.0     P7740_237>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 5\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 100 because (49011968733 bytes from row 1 to eof) \/ (2 * 5114 jump0size) == 4791940\r\n  Type codes (jump 000)    : C557C  Quote rule 0\r\n  Type codes (jump 100)    : C557C  Quote rule 0\r\n  'header' determined to be false because there are some number columns and those columns do not have a string field at the top of them\r\n  =====\r\n  Sampled 10040 rows (handled \\n inside quoted fields) at 101 jump points\r\n  Bytes from first data row on line 1 to the end of last row: 49011968685\r\n  Line length: mean=57.97 sd=2.32 min=48 max=69\r\n  Estimated number of rows: 49011968685 \/ 57.97 = 845441913\r\n  Initial alloc = 929986104 rows (845441913 + 9%) using bytes\/max(mean-2*sd,min) clamped between [1.1*estn, 2.0*estn]\r\n  =====\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : C557C\r\n[10] Allocate memory for the datatable\r\n  Allocating 5 column slots (5 - 0 dropped) with 929986104 rows\r\n[11] Read the data\r\n  jumps=[0..46743), chunk_size=1048541, total_size=49011968733\r\n|--------------------------------------------------|\r\n|===================\r\n```\r\n\r\nWhen I ssh into the node, it's not even using all of the CPUs:\r\n\r\n```\r\ntop - 12:30:20 up 3 days, 15:38,  1 user,  load average: 8.58, 8.33, 7.57\r\nTasks:   4 total,   1 running,   3 sleeping,   0 stopped,   0 zombie\r\n%Cpu(s): 25.8 us,  0.5 sy,  0.0 ni, 72.9 id,  0.9 wa,  0.0 hi,  0.0 si,  0.0 st\r\nKiB Mem : 26357425+total, 19523254+free, 41698536 used, 26643172 buff\/cache\r\nKiB Swap:  4194300 total,  4194300 free,        0 used. 21920500+avail Mem \r\n\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                                      \r\n32464 billylau  20   0   74.4g  36.1g  17.4g S  31.6 14.4   1:28.06 R                                                                                                                                                            \r\n32189 billylau  20   0  116440   3760   1772 S   0.0  0.0   0:00.05 bash                                                                                                                                                         \r\n32509 billylau  20   0  116212   3492   1724 S   0.0  0.0   0:00.03 bash                                                                                                                                                         \r\n32697 billylau  20   0  161244   2108   1512 R   0.0  0.0   0:00.05 top                                                                                                                                                          \r\n```\r\n\r\nI can verify that when I use it on our personal workstations it is using multiple threads. How should I go about troubleshooting this? My guess is that SLURM\/R\/data.table are having some kind of weird interaction that is not provisioning the CPUs properly.\r\n\r\n`#` `Output of sessionInfo()`\r\n```> sessionInfo()\r\nR version 4.1.2 (2021-11-01)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: CentOS Linux 7 (Core)\r\n\r\nMatrix products: default\r\nBLAS\/LAPACK: \/share\/software\/user\/open\/openblas\/0.3.10\/lib\/libopenblas_haswellp-r0.3.10.so\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \r\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \r\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \r\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.1.2\r\n```\r\n","comments":["Data.table uses per default 50% of the available virtual cores. You can raise this limit, e.g., by setting `Sys.setenv(R_DATATABLE_NUM_PROCS_PERCENT=\"90\")`","There may be multiple steps in fread that may not be parallelized. So for example if your file has character columns then a lot of time will be spent single threaded. I suggest to try running forder (or frollmean algo=exact) in a loop and then observe `top`.","from the output it looks like data.table is using 3 threads out of 6 on that cluster node, so I'm not sure this is a problem with data.table, and you may consider closing the issue.\r\nWhen using SLURM you can tell data.table to use all SLURM CPUs  via\r\n```r\r\ndata.table::setDTthreads(as.integer(Sys.getenv(\"SLURM_JOB_CPUS_PER_NODE\", \"1\")))\r\n```","when using 3 threads, you would have at best 3x speedups relative to a single thread, but that would be only in an ideal case. related to #2687 we should add some docs to clarify how exactly openmp is used, so people can have realistic expectations of when speedups should happen.","in fread.c the only instance of pragma omp for I see is\r\n```c\r\n    #pragma omp for ordered schedule(dynamic) reduction(+:thRead,thPush)\r\n    for (int jump = jump0; jump < nJumps; jump++) {\r\n```\r\nbut I am not an expert on fread so I am not sure what exactly happens in this for loop, and if using several threads in this for loop should result in big speedups.","> When using SLURM you can tell data.table to use all SLURM CPUs via\r\n> \r\n> ```r\r\n> data.table::setDTthreads(as.integer(Sys.getenv(\"SLURM_JOB_CPUS_PER_NODE\", \"1\")))\r\n> ```\r\n\r\nNote that `SLURM_JOB_CPUS_PER_NODE` may hold multi-host values, e.g. `4,8` and `10,2(x3)`.  This depends on what parallel resources the Slurm job requested.  If you're interested in the number CPUs allotted on the _current_ machine, I think you want to use `SLURM_CPUS_ON_NODE` instead - that holds an integer scalar.\r\n\r\n "],"labels":["openmp"]},{"title":"I had expected rbindlist() to preserve attributes, but it doesn't.  It'd be a nice feature; or an important limitation to disclose in the documentation.","body":"`#` [`Minimal reproducible example`](https:\/\/stackoverflow.com\/questions\/5963269\/how-to-make-a-great-r-reproducible-example)\r\n``` r\r\nlibrary(data.table)\r\nx <- data.table(0)\r\nsetattr(x,\"xa\",0)\r\ny <- data.table(1)\r\nsetattr(y,\"ya\",1)\r\nz <- rbindlist(list(x,y))\r\nattributes(z)\r\n#> $names\r\n#> [1] \"V1\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $class\r\n#> [1] \"data.table\" \"data.frame\"\r\n#> \r\n#> $.internal.selfref\r\n#> <pointer: 0x00000239858df980>\r\n# I had expected attributes(z) to include at least \"xa\", or both \"xa\" and \"ya\".\r\n```\r\n\r\n<sup>Created on 2022-12-20 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n\r\n`#` `Output of sessionInfo()`\r\n``` r\r\nsessionInfo()\r\n#> R version 4.2.2 (2022-10-31 ucrt)\r\n#> Platform: x86_64-w64-mingw32\/x64 (64-bit)\r\n#> Running under: Windows 10 x64 (build 22621)\r\n#> \r\n#> Matrix products: default\r\n#> \r\n#> locale:\r\n#> [1] LC_COLLATE=English_New Zealand.utf8  LC_CTYPE=English_New Zealand.utf8   \r\n#> [3] LC_MONETARY=English_New Zealand.utf8 LC_NUMERIC=C                        \r\n#> [5] LC_TIME=English_New Zealand.utf8    \r\n#> \r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \r\n#> \r\n#> loaded via a namespace (and not attached):\r\n#>  [1] rstudioapi_0.14   knitr_1.41        magrittr_2.0.3    R.cache_0.16.0   \r\n#>  [5] rlang_1.0.6       fastmap_1.1.0     stringr_1.5.0     styler_1.8.1     \r\n#>  [9] highr_0.9         tools_4.2.2       xfun_0.34         R.oo_1.25.0      \r\n#> [13] cli_3.4.1         withr_2.5.0       htmltools_0.5.3   yaml_2.3.6       \r\n#> [17] digest_0.6.29     lifecycle_1.0.3   purrr_0.3.5       vctrs_0.5.0      \r\n#> [21] R.utils_2.12.2    fs_1.5.2          glue_1.6.2        evaluate_0.18    \r\n#> [25] rmarkdown_2.18    reprex_2.0.2      stringi_1.7.8     compiler_4.2.2   \r\n#> [29] R.methodsS3_1.8.2\r\n```\r\n\r\n<sup>Created on 2022-12-20 with [reprex v2.0.2](https:\/\/reprex.tidyverse.org)<\/sup>\r\n","comments":["To be fair the `base` equivalent does also not preserve all attributes.\r\n\r\n``` R\r\nx <- data.frame(a=0)\r\nattr(x, \"xa\") <- 1\r\ny <- data.frame(a=1)\r\nattr(y, \"ya\") <- 1\r\nz <- do.call(rbind, list(x,y))\r\nattributes(z)\r\n#> $names\r\n#> [1] \"a\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $xa\r\n#> [1] 1\r\n#> \r\n#> $class\r\n#> [1] \"data.frame\"\r\n```\r\n\r\nNeither does `dplyr::bind_rows()`\r\n``` r\r\nx <- data.frame(a=0)\r\ny <- data.frame(a=1)\r\nattr(x, \"xa\") <- 1\r\nattr(y, \"ya\") <- 1\r\nattributes(dplyr::bind_rows(x, y))\r\n#> $names\r\n#> [1] \"a\"\r\n#> \r\n#> $class\r\n#> [1] \"data.frame\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $xa\r\n#> [1] 1\r\n```","Indeed, I'd be happy if `rbindlist()` was similar to either `rbind()` or `dplyr::bind_rows()` with respect to the preservation (or destruction, or copying) of table-level attributes.  However I can't find any documentation of their intended behaviour!  Furthermore, I wouldn't be surprised if the modify-in-place reference semantics of `data.table` methods will make it impossible (or undesirable) to mimic the behaviour of either of those `data.frame` methods.  \r\n\r\nMy use-case for these methods is to add a row of experimental data to an existing table.  I now realise that this is *much* more easily done with `data.frame` methods than with `data.table` methods:\r\n\r\n``` r\r\nxdf <- data.frame(data=c(0))\r\nattr(xdf, \"xa\") <- 0\r\nxdf <- dplyr::bind_rows(xdf, c(data=1))\r\n# I expect attributes(xdf) to include \"xa\".  (PASS)\r\nattributes(xdf)\r\n#> $names\r\n#> [1] \"data\"\r\n#> \r\n#> $class\r\n#> [1] \"data.frame\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $xa\r\n#> [1] 0\r\n\r\nxdf <- data.frame(data=c(0))\r\nattr(xdf, \"xa\") <- 0\r\nxdf <- rbind(xdf, c(data=1))\r\n# I expect attributes(xdf) to include \"xa\".  (PASS)\r\nattributes(xdf)\r\n#> $names\r\n#> [1] \"data\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $xa\r\n#> [1] 0\r\n#> \r\n#> $class\r\n#> [1] \"data.frame\"\r\n\r\nlibrary(data.table)\r\nxdt <- data.table(data=0)\r\nsetattr(xdt, \"xa\", 0)\r\nxdt <- dplyr::bind_rows(xdt, c(data=1))\r\n# I expect class(xdt) to have \"data.table\" as its first element (PASS)\r\nclass(xdt)\r\n#> [1] \"data.table\" \"data.frame\"\r\n# I expect xdt to have an attribute \"xa\" (PASS)\r\nattributes(xdt)\r\n#> $names\r\n#> [1] \"data\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $class\r\n#> [1] \"data.table\" \"data.frame\"\r\n#> \r\n#> $.internal.selfref\r\n#> <pointer: 0x000002c36fbc4bb0>\r\n#> \r\n#> $xa\r\n#> [1] 0\r\n# I expect class(xdt) to have a valid .internal.selfref structure.  (FAIL)\r\ndata.table:::selfrefok(xdt)\r\n#> [1] 0\r\n\r\nlibrary(data.table)\r\nxdt <- data.table(data=0)\r\nsetattr(xdt, \"xa\", 0)\r\nxdt <- rbindlist(list(xdt, data.table(data=1)))\r\n# I expect class(xdt) to have a valid .internal.selfref structure.  (PASS)\r\ndata.table:::selfrefok(xdt)\r\n#> [1] 1\r\n# I expect attributes(xdt) to include \"xa\".  (FAIL)\r\nattributes(xdt)\r\n#> $names\r\n#> [1] \"data\"\r\n#> \r\n#> $row.names\r\n#> [1] 1 2\r\n#> \r\n#> $class\r\n#> [1] \"data.table\" \"data.frame\"\r\n#> \r\n#> $.internal.selfref\r\n#> <pointer: 0x000002c36fbc4bb0>\r\n```\r\n\r\nI'll leave it to the DT team to decide whether to continue treating this as a feature-request, or (as I'd now suggest) as a request to amend the [documentation of rbindlist()](https:\/\/www.rdocumentation.org\/packages\/data.table\/versions\/1.14.6\/topics\/rbindlist).  A sentence would suffice, I should think, to inform users that -- unlike `DF <- rbind(DF, ...)` and `DF <- dplyr::bind_rows(DF, ... )` -- the new `data.table` DT created by `DT <- rbindlist(l, ...)` has none of the attributes of the tables in its argument `l` aside from their column names.\r\n\r\nYour prompt response to my feature-request is very impressive, thanks!  I'm extremely grateful for the bazillions of hours of volunteer labour, over the decades, which is making my current coding experience in R *so* *so* *so* much more pleasant and productive than my only prior experience with stochastic experimentation -- which was way back in the early 1990s, using S (yeah I'm an old codger!  ;-)","Preserve attributes with `rbindlist` seems trickier than I initially thought, since we also store` key` and `indices` as attributes.\r\n\r\nThis is something that will definitely break a `data.table` when using `dplyr::bind_rows` together with a keyed `data.table` as first argument (similar to #5361)","Yeah, it'd be a big shift in semantics of `rbindlist`, if its first arg was redefined to be a DT that'll be modified by reference, followed by a list of DTs to be concatenated!   There must be many people (and codes) who are currently using `rbindlist` as a UNION ALL, i.e. are relying on it *not* to modify its first argument.   \r\n\r\nSo... maybe... it'd make sense to define a new method for DTs which (essentially) overloads `append` with the usual DT design pattern of modifying its first arg by reference and returning it invisibly?  Maybe call it `rappend`?   As with `rbindlist` I guess it'd be invalidating the keys, to avoid the performance overhead of rebuilding them."],"labels":["feature request","rbindlist"]},{"title":"i column not found in rolling join with by","body":"hi all! I'm not an expert in rolling joins, but I thought it should be possible to use arguments `by` and `roll` at the same time to do rolling join and summarization efficiently in one step. But I observed a \"not found\" error when I tried that. \r\nHere is a complete example:\r\n```r\r\nlibrary(data.table)\r\nX.vec = c(0.05, 0.15, 0.75, 0.95)\r\nY.vec = 10*X.vec\r\n(irregular = data.table(X=X.vec, Y=Y.vec, key=\"X\"))\r\ngrid.space = 0.2\r\noffset = grid.space\/2\r\ngrid = seq(offset, 1-offset, by=grid.space)\r\n(regular <- data.table(grid, X=grid, min=grid-offset, max=grid+offset, key=\"X\"))\r\n(joined = regular[irregular, roll=\"nearest\"])\r\njoined[, .(N=.N, mean.Y=mean(Y)), by=grid]\r\nirregular[regular, .(grid, N=.N, mean.Y=mean(Y)), on=.(X<max, X>min), by=.EACHI, nomatch=0L]\r\nregular[irregular, .(N=.N), roll=\"nearest\", by=grid]\r\nregular[irregular, .(N=.N, mean.Y=mean(Y)), roll=\"nearest\", by=grid]\r\n```\r\nThe goal is a rolling join between `regular` and `irregular`, with matching each `irregular$X` to the closest `regular$grid` value, then computing the mean of `irregular$Y` for each value of `regular$grid`.  \r\n```r\r\n> (irregular = data.table(X=X.vec, Y=Y.vec, key=\"X\"))\r\n      X   Y\r\n1: 0.05 0.5\r\n2: 0.15 1.5\r\n3: 0.75 7.5\r\n4: 0.95 9.5\r\n> (regular <- data.table(grid, X=grid, min=grid-offset, max=grid+offset, key=\"X\"))\r\n   grid   X min max\r\n1:  0.1 0.1 0.0 0.2\r\n2:  0.3 0.3 0.2 0.4\r\n3:  0.5 0.5 0.4 0.6\r\n4:  0.7 0.7 0.6 0.8\r\n5:  0.9 0.9 0.8 1.0\r\n```\r\nOne way to do that is by first materializing the `joined` table, then summarizing it:\r\n```r\r\n> (joined = regular[irregular, roll=\"nearest\"])\r\n   grid    X min max   Y\r\n1:  0.1 0.05 0.0 0.2 0.5\r\n2:  0.1 0.15 0.0 0.2 1.5\r\n3:  0.7 0.75 0.6 0.8 7.5\r\n4:  0.9 0.95 0.8 1.0 9.5\r\n> joined[, .(N=.N, mean.Y=mean(Y)), by=grid]\r\n   grid N mean.Y\r\n1:  0.1 2    1.0\r\n2:  0.7 1    7.5\r\n3:  0.9 1    9.5\r\n```\r\nThat works fine, but I thought there would be some more efficient way to do that (which could avoid allocating memory for the intermediate `joined` table). So I tried adding `by` to the same call as `roll` but I got an error:\r\n```r\r\n> regular[irregular, .(N=.N, mean.Y=mean(Y)), roll=\"nearest\", by=grid]\r\nError in `[.data.table`(regular, irregular, .(N = .N, mean.Y = mean(Y)),  : \r\n  object 'Y' not found\r\n```\r\nIs this a bug? In other words, when doing `DT[i,j,by,roll]`, shouldn't it be possible to use columns of `i` in `j`?\r\nOne thing that suggests that it should be possible is the code below, which shows that you can count the number of items in each `by` group:\r\n```r\r\n> regular[irregular, .(N=.N), roll=\"nearest\", by=grid]\r\n   grid N\r\n1:  0.1 2\r\n2:  0.7 1\r\n3:  0.9 1\r\n```\r\nFinally, another way of computing the result that I want, in one step (without the intermediate table) would be:\r\n```r\r\n> irregular[regular, .(grid, N=.N, mean.Y=mean(Y)), on=.(X<max, X>min), by=.EACHI, nomatch=0L]\r\n     X   X grid N mean.Y\r\n1: 0.2 0.0  0.1 2    1.0\r\n2: 0.8 0.6  0.7 1    7.5\r\n3: 1.0 0.8  0.9 1    9.5\r\n```","comments":[],"labels":["consistency","joins"]},{"title":"groupingsets() wrong scoping logic","body":"\r\n```R\r\nlibrary(data.table)\r\nirisDT <- data.table(iris)\r\n\r\nfoo = function(w) {\r\n  bv = \"Species\"\r\n  groupingsets(\r\n    irisDT, \r\n    j  = lapply(.SD, \\(y) sum(y > w)),\r\n    by = bv,\r\n    sets = as.list(bv),\r\n    .SDcols = c(\"Sepal.Length\", \"Petal.Length\")\r\n  ) |> print()\r\n  irisDT[, lapply(.SD, \\(y) sum(y > w)), .SDcols = c(\"Sepal.Length\", \"Petal.Length\"), by=bv]\r\n}\r\nfoo(5)\r\n# Error in ..FUN1(Sepal.Length) : object 'w' not found\r\n\r\nw = 4\r\nfoo(5)\r\n#       Species Sepal.Length Petal.Length\r\n#        <fctr>        <int>        <int>\r\n# 1:     setosa           50            0\r\n# 2: versicolor           50           34\r\n# 3:  virginica           50           50\r\n#       Species Sepal.Length Petal.Length\r\n#        <fctr>        <int>        <int>\r\n# 1:     setosa           22            0\r\n# 2: versicolor           47            1\r\n# 3:  virginica           49           41\r\n```\r\n\r\n\r\nI'm using data.table \u20181.14.7\u2019 (just updated with `data.table::update_dev_pkg()`) with R 4.2.1 on Windows 10.\r\n","comments":["I suspect this is coming from:\r\n\r\n```r\r\n  jj = if (!missing(jj)) \r\n    jj\r\n  else substitute(j)\r\n```\r\n\r\nfrom `data.table:::groupingsets.data.table`. According to `substitute`'s documentation:\r\n\r\n> Substitution takes place by examining each component of the parse tree as follows: If it is not a bound symbol in `env`, it is unchanged. If it is a promise object, i.e., a formal argument to a function or explicitly created using `delayedAssign()`, the expression slot of the promise replaces the symbol. If it is an ordinary variable, its value is substituted, unless `env` is `.GlobalEnv` in which case the symbol is left unchanged.\r\n\r\nBy which I think since `w` does not exist in `data.table:::groupingsets.data.table`, it is left unchanged. You seem to be able to solve it by using `substitute` in the call to `data.table:::groupingsets.data.table`, but as a `jj` argument.\r\n\r\n```r\r\nlibrary(data.table)\r\nirisDT <- data.table(iris)\r\n\r\nfoo = function(w) {\r\n  bv = \"Species\"\r\n  groupingsets(\r\n    irisDT, \r\n    jj  = substitute(lapply(.SD, \\(y) sum(y > w))),\r\n    by = bv,\r\n    sets = as.list(bv),\r\n    .SDcols = c(\"Sepal.Length\", \"Petal.Length\")\r\n  ) |> print()\r\n  irisDT[, lapply(.SD, \\(y) sum(y > w)), .SDcols = c(\"Sepal.Length\", \"Petal.Length\"), by=bv]\r\n}\r\nfoo(w=5)\r\n```","I would call it misuse of j arg.\r\nIsn't jj argument description good enough to explain this use cases? Or maybe adding examples could be useful? PR welcome \r\n\r\nThanks @avimallu for working example.","I'm not sure how that can be called a *misuse* of the `j` arg, since the documentation mentions that whatever is provided to the argument will be sent to the `j` of `data.table`. A typical call to `data.table` can use another variable defined outside of the set of columns in that `data.table` object, yeah?\r\n\r\nThe `jj` argument description is probably good enough from a *developer* perspective, but I haven't seen quoting being encouraged often online (unless you meant literal quotes `\"`, `'`), so it wasn't obvious to me how to use it until now (but that's also because I've looked it up before). One resource I came across is Hadley's Advanced R, specifically the section on *[metaprogramming](https:\/\/adv-r.hadley.nz\/metaprogramming.html)*.\r\n\r\nI'd love to file a PR with more examples on the use of the `jj` argument - could you provide a better source for quoting that is specific to base R?","R language manual, chapter 6.\r\n\r\nGreat about the PR. Ideally if it also convey that the use case described is a misuse of j and jj should be used instead."],"labels":["documentation","help-wanted","groupingsets"]},{"title":"Unclear class for argument and return value of DT","body":"While experimenting with a potential fix for #5286 this test of `DT` surfaced:\r\n```r\r\n  D = copy(mtcars)\r\n...\r\n  test(2212.02, EVAL(\"D |> DT(,.SD)\"), mtcars)\r\n```\r\n\r\n[This line](https:\/\/github.com\/Rdatatable\/data.table\/blob\/cb8aeff9453acec878e5ab8515cda0d302c943eb\/R\/data.table.R#L1335) in `[.data.table` :\r\n```r\r\nif (!is.data.table(ans)) setattr(ans, \"class\", c(\"data.table\",\"data.frame\"))  # DF |> DT(,.SD[...]) .SD should be data.table, test 2212.013\r\n```\r\nSeems to indicate that the desired behavior is to return a data.table.  However `ans` isn't the variable that is returned directly, [later on the code restores the class to data.frame before returning](https:\/\/github.com\/Rdatatable\/data.table\/blob\/cb8aeff9453acec878e5ab8515cda0d302c943eb\/R\/data.table.R#L1415-L1421):\r\n\r\nThe returned object has class data.frame but still has data.table attributes (namely `.internal.selfref`).\r\n\r\nBefore suggesting fixes I believe some design decisions need to be explicitly discussed:\r\n\r\n 1.  Currently `DT` operates freely on non-data-tables. The only discussion I could find of it is [this single comment](https:\/\/github.com\/Rdatatable\/data.table\/issues\/4872#issuecomment-844558253), and I'm not sure this is the best design. `[.data.table`, as the name implies, is a method of _data.table_. It might accidently work in some (even most) scenarios on data.frames, but is a hefty piece of code that was designed to use data.table attributes and trying to open it would mean an avalanche of bugs and a world of pain. In general, as a user I don't expect methods of one class to operate on another - on the contrary, I expect the class system to help me by isolating their respective functionalities.   \r\n In particular, rejecting non-data.table's would make #5286 (and probably others, both current and future) moot.\r\n 2. Inside `[.data.table` and other package methods I'd suggest using `setDF` or `setDT` instead of setting `class` directly - as `setDF` cleans up data.table-only attributes, that manual-class-set misses.\r\n","comments":["I hesitate to comment here, because I'm a newbie to `data.table`, to R's S3 class system, and to RStudio.  But... maybe my perspective will be helpful to the core devteam of this package?\r\n\r\nI wholeheartedly agree with #OfekShilon: I too \"... don't expect methods of one class to operate on another - on the contrary, I expect the class system to help me by isolating their respective functionalities.\"  \r\n\r\nI tentatively suggest that the `data.table` development effort adopt, as a long-term goal, that it become impossible to silently promote a `data.frame` object to a `data.table` object -- except by using `DT` and perhaps `fread()`.  I'm reluctantly including `fread()` here, because any future CRAN package which defines an `fread()` method will become hazardous for use in conjunction with `data.table`.  For backwards compatibility, all existing automagic promotions of `data.frame` objects to `data.table` objects should (I suppose!) be allowed with a warning.\r\n\r\nI'd also suggest the [Introduction to data.table vignette](https:\/\/cran.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-intro.html) be amended, so that it include a warning against manipulating `data.table` objects in interpretive environments which don't include the `data.table` library.   I'm moderately confident that such careless manipulation was how I had once managed to create a `data.table` object with a corrupted set of object-level attributes: my best guess is that I had done this by mistakenly invoking an undecorated `save()` method (via command shell in RStudio) on a `data.table` object that my codebase had created.  Fortunately -- and I'm *very* grateful for this -- my subsequent `load()` of this serialised object threw a warning which informed me of a dangling reference in its object-level attribute list. \r\n\r\nI hope it is clear that the above are only *suggestions* from a newbie to R who isn't your usual sort of newbie -- because I have extensive experience with other languages, I prefer to develop in a strongly-typed language, I am happy to \"hack\" throwaway code in a weakly-typed scripting language, and my only prior experience with statistical programming was way back in the early 1990s -- when I used S and emacs!   I'm *hugely* impressed with the current status of R.  After my initial foray into emacs\/ess, RStudio was a huge relief... although I'm still struggling with the basics e.g. I can't be bothered to figure out how to ensure that` data.table` is loaded by default in my project environment.  (Surely I shouldn't be hacking .Renviron but instead should be invoking some method in `usethis`... but what is it called?  I'm confident I could figure it out within a  half-hour, maybe even faster -- but instead I find myself writing this overlong comment ;-)  \r\n\r\nThanks to the lovely documentation by Hadley Wickham, and so many years of well-directed behind-the-scenes work by core R  developers over the years, I'm now making great progress on my [rev of vote_2.3.2](https:\/\/github.com\/cthombor\/SafeVote)!\r\n","@cthombor If I understand your intention correctly there is already an attempt to give something like \"a warning against manipulating data.table objects in interpretive environments which don't include the data.table library\" - check [cedta](https:\/\/github.com\/Rdatatable\/data.table\/blob\/master\/R\/cedta.R), which is checked in the beginning of most (all?) data.table method calls. \r\n\r\nThis is a different matter but I do feel there is some commonality - maybe the underlying bias is to try and help _everyone_ with _everything_ they try to do with data.tables. Which is very generous, but often leads to fragile design.\r\n\r\nAnyway you'd probably draw better attention for your problem if you open a new issue with concrete examples.  Can you give examples of silent promotions of data.frames to data.tables?\r\n","@OfekShilon thanks for your response.  I have no idea whether there are any silent promotions of data.frames to data.tables; but their *possible* existence is one of the (many!) gaps in my knowledge which makes it difficult for me to discover the root cause(s) of the invalid `.internal.selfref` warnings I have triggered while developing.  \r\n\r\nAnother gap in my knowledge: does `data.table` export a method for testing the `.internal.selfref` sentinel?  AFAIK the answer is \"no\".  [Section 5.13 of Writing R Extensions](https:\/\/cran.r-project.org\/doc\/manuals\/R-exts.html#External-pointers-and-weak-references) tells me that it's not something that can be written in R.  I'm asking because it'd not be very onerous for me to find a root cause for a warning -- and to develop an MRE if it turns out to be a novel cause -- by pepper-potting my code with `stopifnot(valid.internal.selfref(DT))` calls along the path to the operation on `DT` which produced the warning message.\r\n\r\nAnd yes I believe we're on the same page with the difficulties (both to users and to its devs!) of supporting use-cases for `data.table` that are outside what (I now, belatedly) understand to be its central set of use-cases -- the analysis of very large datasets of well-established provenance.  I am now almost certain that a `data.table` is not, and can never be, an appropriate structure for the collection and archival storage of experimental data.  The secondary factors of an experimental dataset must be reliably recorded and preserved in archival storage.  During analysis -- especially if there's a conveniently-accessible archival store of experimental datasets -- there's no strong data-integrity requirement on the secondary factors which have been copied into the object-level attributes of a `data.table` which contains a *copy* of the archival record.  That's my  current thinking anyway: that the tidyverse might \"someday\" include a data-collection package that's distinct from `data.table`.  Rather orthogonal really: for analysis of experimental data in the tidyverse you want the experimental units indexing the rows, and the primary factors indexing the columns.  Given R's column-major storage: for the collection of experimental data you want the primary factors indexing the rows and the experimental units indexing the columns!","@OfekShilon thanks for the pointer to [cedta](https:\/\/github.com\/Rdatatable\/data.table\/blob\/master\/R\/cedta.R).  However I won't expect every base method which could copy (or corrupt) a `data.table` to include a `cedta()` call; so the existence of `cedta()` doesn't reduce the hazard of my foolishly manipulating a `data.table` with a base method which copies it and therefore puts the integrity of my DT at risk.  But... returning to the issue at hand -- yes I strongly support your idea of removing (what I understand to be) the silent (even if only temporary!) promotion of a `data.frame` object to a `data.table` object.  \r\n\r\nThe [cheat sheet for DT](https:\/\/github.com\/rstudio\/cheatsheets\/blob\/main\/datatable.pdf) lists only two ways to create a `data.table`.   Converting from a `data.frame` is easy-as... so I guess the body of that `DT` test could \"easily\" be revised to `EVAL(\"D |> setDT() |> DT(,.SD)\"), mtcars)` but... I'm back to wondering about the feasibility of a transition in which the silent promotion you had identified in `[.data.table`, and of any other silent promotions which may exist in the data.table codebase, are deprecated with a warning.  \r\n\r\nWarnings can be very confusing to a newbie like me -- indeed quite distressing, if it's unclear what one can do to reliably avoid triggering such warnings in the future -- but I doubt any of my stumblings with .SD syntax and semantics would have included an attempt to use a data.frame as a value for .SD, so deprecating this particular promotion with a warning wouldn't have bothered me.\r\n\r\nHmmm... maybe a search for `is.data.table` in the `data.table` codebase would put a quick upper-bound on the number of its methods which can -- at least under some conditions -- silently promote an R object to a `data.table`? \r\n","@cthombor \r\n> Another gap in my knowledge: does `data.table` export a method for testing the `.internal.selfref` sentinel? AFAIK the answer is \"no\". [Section 5.13 of Writing R Extensions](https:\/\/cran.r-project.org\/doc\/manuals\/R-exts.html#External-pointers-and-weak-references) tells me that it's not something that can be written in R. I'm asking because it'd not be very onerous for me to find a root cause for a warning -- and to develop an MRE if it turns out to be a novel cause -- by pepper-potting my code with `stopifnot(valid.internal.selfref(DT))` calls along the path to the operation on `DT` which produced the warning message.\r\n\r\n\r\nYou can use the non-exported `data.table:::selfrefok`. \r\n`data.table` does many things that can't be done in vanilla R by invoking C code. The key one in this context is `address`.\r\n\r\n\r\n\r\n"],"labels":["DT()"]},{"title":"Inconsistent replacement of list column element with NULL in table with 1 row","body":"In base R data.frame we can replace an element of a list column with NULL via:\r\n```r\r\n> DF1=data.frame(L=I(list(\"A\")),i=1)\r\n> DF1$L=list(NULL)\r\n> DF1\r\n     L i\r\n1 NULL 1\r\n```\r\nHowever in data.table, doing that results in deleting the list column entirely:\r\n```r\r\n> DT1=data.table(L=list(\"A\"),i=1)\r\n> DT1$L=list(NULL)\r\n> DT1\r\n       i\r\n   <num>\r\n1:     1\r\n```\r\nRequest: can we make the above code do a replacement (like base R data.frame) instead of deleting the column?\r\n\r\nThe above issue is confusing because it is inconsistent with how data table handles replacement in tables with more than one row:\r\n```r\r\n> DT2=data.table(L=list(\"B\",\"C\"),i=1)\r\n> DT2$L <- list(NULL,NULL)\r\n> DT2\r\n        L     i\r\n   <list> <num>\r\n1:            1\r\n2:            1\r\n> DF2=data.frame(L=I(list(\"B\",\"C\")),i=1)\r\n> DF2$L <- list(NULL,NULL)\r\n> DF2\r\n     L i\r\n1 NULL 1\r\n2 NULL 1\r\n```\r\nActually, we can do the replacement (inconsistently) in data.table via\r\n```r\r\n> DT1=data.table(L=list(\"A\"),i=1)\r\n> DT1$L=list(list(NULL))\r\n> DT1\r\n        L     i\r\n   <list> <num>\r\n1:            1\r\n```\r\n","comments":["I can do this:\r\n\r\n```r\r\nlibrary(data.table)\r\nDT1=data.table(L=list(\"A\"),i=1)\r\nDT1[, `:=`(L = list(NULL))]\r\n```\r\n\r\nthat works, but oddly not\r\n\r\n```\r\nDT1[, L := list(NULL)]\r\n```\r\n\r\nwhich should be identical per `data.table` documentation."],"labels":["consistency"]},{"title":"revdep dtts test failure due to CsubsetDT rename","body":"With data.table from github master installed, I get the following test failure from revdep dtts:\r\n```\r\n* checking tests ...\r\n  Running 'tinytest.R'\r\n ERROR\r\nRunning the tests in 'tests\/tinytest.R' failed.\r\nLast 13 lines of output:\r\n  test_dtts.R...................   32 tests OK \r\n  test_dtts.R...................   32 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK \r\n  test_dtts.R...................   33 tests OK Error in .align_duration_cpp(x[[1]], sort(y), x, start, end, sopen, eopen,  : \r\n    function 'CsubsetDT' not provided by package 'data.table'\r\n  Calls: <Anonymous> ... .local -> data.table -> do.call -> .align_duration_cpp\r\n  Execution halted\r\n```\r\ngit bisect says this started happening after merging https:\/\/github.com\/Rdatatable\/data.table\/pull\/4753 which renamed an exported C function so the easy fix is for dtts to use the new name. Maintainer of dtts is @eddelbuettel who reviewed that PR so I assume he is aware of the rename and will do it when we submit the new version of data.table to CRAN.","comments":["I will take a closer look tomorrow.\r\n\r\nFrom a first look, there is a mismatch as `CsubsetDT` and `CsubsetVector` are exported in `src\/init.c` via `callMethods[]` but later `DT_subsetDT` is the function that is registered.  Those two need to align, but do not. \r\n\r\nFor example, just weeks ago I exported functions from [RcppSpdlog](https:\/\/github.com\/eddelbuettel\/rcppspdlog) and there we have (in Rcpp auuto-generated code, so a little verbose) the same functions in both `CallEntries[]` going into `R_registerRoutines()` as we have in the different `R_RegisterCCallable()` calls, see https:\/\/github.com\/eddelbuettel\/rcppspdlog\/blob\/master\/src\/RcppExports.cpp (and I have better\/simper human-written examples elsewhere).\r\n\r\nNow, `dtts` has been on CRAN unchanged for a while.  But it is possible that this was always unfinished business.  The patches by @lsilvest and myself had been dormant for some time.  It won't take much to clean this up.","How exactly are you triggering an error in `dtts`?  `R CMD check --as-cran` works fine for me in `dtts`.\r\n\r\nAlso running the `tinytest` file directly (via a trivial wrapper) shows no issue:\r\n\r\n```sh\r\nedd@rob:~\/git\/dtts(feature\/small_refactor)$ tt.r -f inst\/tinytest\/test_dtts.R \r\ntest_dtts.R...................   84 tests OK 0.9s\r\nAll ok, 84 results (0.9s)\r\nedd@rob:~\/git\/dtts(feature\/small_refactor)$ \r\n```\r\n\r\nOh, I see it with `master` in `data.table`.  That is odder as nothing appears to have changed and releases 0.14.0, 0.14.2, 0.14.4, and 0.14.6 all passed.","Here is a diff between 0.14.6 and master than may explain it:\r\n\r\n```diff\r\n void attribute_visible R_init_data_table(DllInfo *info)\r\n {\r\n-  \/\/ C exported routines, see ?cdt for details\r\n-  R_RegisterCCallable(\"data.table\", \"CsubsetDT\", (DL_FUNC) &subsetDT);\r\n+  \/\/ C exported routines\r\n+  \/\/ must be also listed in inst\/include\/datatableAPI.h\r\n+  \/\/ for end user documentation see ?cdt\r\n+  R_RegisterCCallable(\"data.table\", \"DT_subsetDT\", (DL_FUNC) &subsetDT);\r\n```","Exactly. Looks like after the change of name `dtts` will be dependent on the latest `data.table` version as `R_GetCCallable` calls `error` if it can't find the function we ask it to get.","And, for completeness, the same issue in `inst\/include\/datatableAPI.h`:\r\n\r\n```diff\r\nedd@rob:\/tmp\/data.table-1.14.6$ diff -ru inst\/include\/datatableAPI.h ~\/git\/data.table\/inst\/include\/datatableAPI.h \r\n--- inst\/include\/datatableAPI.h 2022-11-15 18:47:20.000000000 -0600\r\n+++ \/home\/edd\/git\/data.table\/inst\/include\/datatableAPI.h        2021-09-28 17:34:28.619110668 -0500\r\n@@ -21,11 +21,14 @@\r\n \/* provided the interface for the function exported in\r\n    ..\/src\/init.c via R_RegisterCCallable()             *\/\r\n \r\n+\/\/ subsetDT #3751\r\n inline SEXP attribute_hidden DT_subsetDT(SEXP x, SEXP rows, SEXP cols) {\r\n      static SEXP(*fun)(SEXP, SEXP, SEXP) =\r\n-       (SEXP(*)(SEXP,SEXP,SEXP)) R_GetCCallable(\"data.table\", \"CsubsetDT\");\r\n+       (SEXP(*)(SEXP,SEXP,SEXP)) R_GetCCallable(\"data.table\", \"DT_subsetDT\");\r\n      return fun(x,rows,cols);\r\n }\r\n+\/\/ forder #4015\r\n+\/\/ setalloccol alloccolwrapper setDT #4439\r\n \r\n \/* permit opt-in to redefine shorter identifiers *\/\r\n #if defined(DATATABLE_REMAP_API)\r\nedd@rob:\/tmp\/data.table-1.14.6$ \r\n```","So what appears to be happening here is that \r\n- your `master` branch is ahead of the release branches carrying 2.14.{0,2,4,6}, and has been for some time\r\n- the current `master` branch imposes a downstream renaming; we could do that but it has to be coordinated\r\n- the names at the `R_RegisterCCallable()` and `R_GetCCallable()` do not actually matter all that much: \r\n  - it is all inside a namespace, \r\n  - it is not directly user-facing\r\n- there are two functions ready to be exported so we may as well have a conversation about both","It is also point 2 under 'Notes' in the current [NEWS.md](https:\/\/github.com\/Rdatatable\/data.table\/blob\/master\/NEWS.md) and as such not a new issue but something that could probably be closed as a duplicate.","This issue should not block new DT CRAN submission (revdep will update after new DT appears on CRAN)."],"labels":["revdep"]},{"title":"revdeps new errors in rbindlist due to class mis-match","body":"revdep heuristicsmineR has the following new error when running examples if data.table from github master is installed,\r\n```\r\n* checking examples ... ERROR\r\nRunning examples in 'heuristicsmineR-Ex.R' failed\r\nThe error most likely occurred in:\r\n\r\n> ### Name: as.petrinet\r\n> ### Title: Converts the object to a Petrinet\r\n> ### Aliases: as.petrinet\r\n> \r\n> ### ** Examples\r\n> \r\n> data(L_heur_1)\r\n> cn <- causal_net(L_heur_1, threshold = .8)\r\nError in rbindlist(l, use.names, fill, idcol) : \r\n  Class attribute on column 7 of item 2 does not match with column 7 of item 1.\r\nCalls: causal_net ... merge -> merge.data.table -> rbind -> rbind -> rbindlist\r\nExecution halted\r\n```\r\ngit bisect says this started happening after merging https:\/\/github.com\/Rdatatable\/data.table\/pull\/5263\r\n@ben-schwen can you please have a look? (you were author of that PR)","comments":["A similar error, started happening after merging the same PR, in revdep table.express:\r\n```\r\n* checking tests ...\r\n  Running 'testthat.R'\r\n ERROR\r\nRunning the tests in 'tests\/testthat.R' failed.\r\nLast 13 lines of output:\r\n  \r\n  == Failed tests ================================================================\r\n  -- Error (???): Full join works. -----------------------------------------------\r\n  Error in `rbindlist(l, use.names, fill, idcol)`: Class attribute on column 4 of item 2 does not match with column 4 of item 1.\r\n  Backtrace:\r\n      x\r\n   1. +-base::merge(...)\r\n   2. \\-data.table::merge.data.table(...)\r\n   3.   \\-base::rbind(dt, y[missingyidx], use.names = FALSE, fill = TRUE)\r\n   4.     \\-data.table (local) rbind(deparse.level, ...)\r\n   5.       \\-data.table::rbindlist(l, use.names, fill, idcol)\r\n  \r\n  [ FAIL 1 | WARN 0 | SKIP 1 | PASS 614 ]\r\n  Error: Test failures\r\n  Execution halted\r\n```","Update: heuristicsmineR published a new version on CRAN (0.3.0, 2023-04-04), but actually this issue is still happening."],"labels":["revdep","rbindlist"]},{"title":"revdep FIESTA new failures to re-build vignettes","body":"revdep FIESTA uses data.table in vignettes, which fail to rebuild when data.table github master is installed.\r\n```\r\nQuitting from lines 220-224 (FIESTA_tutorial_SA.Rmd) \r\nError: processing vignette 'FIESTA_tutorial_SA.Rmd' failed with diagnostics:\r\nIncompatible join types: x.CONDID (integer) and i.DOMAIN (character)\r\n```\r\nand\r\n```\r\nQuitting from lines 218-224 (FIESTA_tutorial_MA.Rmd) \r\nError: processing vignette 'FIESTA_tutorial_MA.Rmd' failed with diagnostics:\r\nColumn or expression 1 of 'by' or 'keyby' is type 'NULL' which is not currently supported. \r\nIf you have a compelling use case, please add it to https:\/\/github.com\/Rdatatable\/data.table\/issues\/1597. \r\nAs a workaround, consider converting the column to a supported type, \r\ne.g. by=sapply(list_col, toString), whilst taking care to maintain distinctness in the process.\r\n```\r\ngit bisect says this error started happening after merging https:\/\/github.com\/Rdatatable\/data.table\/pull\/5084\r\nSeveral other revdeps broke after merging that same PR, and there is a proposed follow-up https:\/\/github.com\/Rdatatable\/data.table\/pull\/5133\r\nNot sure if it fixes this issue though.","comments":[],"labels":["revdep"]},{"title":"Installation into SQL Server Machine Learning on SQL Managed instance","body":"I have downloaded sqlmlutils from MS and installed into R Studio to load R modules into SQL Server Machine Language.\r\n\r\nOn SQL Server 2016 (R 3.5.2), the load of data.table ends up with with data.table 1.14.0 installed and available.\r\n\r\nOn SQL Managed Instance (in Azure) the load ends up with only data.table 1.12.8 installed.\r\n\r\nWe would like to standardise \/ update both environments to 1.14.0 but despite some significant reading and internet searching, nothing is looking like a solution.\r\n\r\nCan you help with steps to get data.table 1.14.0 installed onto SQL Managed instance (R 3.5.2)?","comments":["How you installed data.table exactly? What are R modules? Did you tried to reach MS SQL support?","Hi Allen, thanks for reporting but it's unlikely we'll be able to help you. I agree with Jan that checking with IT support is the appropriate route here.\r\n\r\nI'll leave it open for now in case anyone else following the issue tracker has a lot of Azure experience and can help, but will close the issue soon."],"labels":["install"]},{"title":"error with set() or := NULL to a list column item","body":"```R\r\nDT = data.table(A=1:3, B=list())\r\nDT\r\n      A      B\r\n   <int> <list>\r\n1:     1       \r\n2:     2       \r\n3:     3       \r\nDT[2, B:=letters]     # ok\r\nDT\r\n       A               B\r\n   <int>          <list>\r\n1:     1                \r\n2:     2 a,b,c,d,e,f,...      # ok\r\n3:     3                \r\nDT[2, B:=NULL]\r\nError in `[.data.table`(DT, 2, `:=`(B, NULL)) : \r\n  When deleting columns, i should not be provided\r\nset(DT, 2L, \"B\", NULL)\r\nError in set(DT, 2L, \"B\", NULL) : \r\n  When deleting columns, i should not be provided\r\n\r\n# Expected result: NULL in that list column item\r\nDT\r\n      A      B\r\n   <int> <list>\r\n1:     1       \r\n2:     2          \r\n3:     3       \r\n```\r\n\r\nWhen fixed can simplify `tables()` dd2134e9aad6fd9d432b1cc0a35ca9c33fcd5dca #5524","comments":["Hi @MichaelChirico I came across this Bug while browsing through the issue's section and wanted to work on it , but while reproducing this bug i came across some differences in output from the one mentioned in the issue.\r\n\r\n## My output\r\n```\r\nDT = data.table(A=1:3, B=list())\r\n> DT\r\n       A      B\r\n   <int> <list>\r\n1:     1 [NULL].     <-- *** default for the list is not empty anymore , it prints null instead***\r\n2:     2 [NULL]\r\n3:     3 [NULL]\r\n> DT[2, B:=letters]     \r\n> DT\r\n       A               B\r\n   <int>          <list>\r\n1:     1          [NULL]\r\n2:     2 a,b,c,d,e,f,...\r\n3:     3          [NULL]\r\n> DT[2, B:=NULL]\r\nError in `[.data.table`(DT, 2, `:=`(B, NULL)) : \r\n  When deleting columns, i should not be provided\r\n> DT\r\n       A               B\r\n   <int>          <list>\r\n1:     1          [NULL]\r\n2:     2 a,b,c,d,e,f,...\r\n3:     3          [NULL]\r\n> set(DT, 2L, \"B\", NULL)\r\nError in set(DT, 2L, \"B\", NULL) : \r\n  When deleting columns, i should not be provided.  < - - ***while the actual error is still the same .***\r\n> DT\r\n       A               B\r\n   <int>          <list>\r\n1:     1          [NULL]\r\n2:     2 a,b,c,d,e,f,...       <-- *** the 2nd item in the  column B doesn't disappear after the operations.***\r\n3:     3          [NULL]\r\n```\r\nand yeah i m confused about the last table mentioned in the issue, is it supposed to show the expected output or the output we got after doing all these operation","It's the expected output when the set operation succeeded instead of erroring.\r\n\r\nNote that we recently changed the way NULL elements are printed to distinct them from the empty string.\r\n\r\nChanges need to be made in `Cassign` which is one of the trickier C parts in our codebase.\r\n\r\nFWIW you also do not have to mention Michael in every issue, since he subscribed to all changes anyway IINM","> It's the expected output when the set operation succeeded instead of erroring.\r\n> \r\n> Note that we recently changed the way NULL elements are printed to distinct them from the empty string.\r\n> \r\n> Changes need to be made in `Cassign` which is one of the trickier C parts in our codebase.\r\n> \r\n> FWIW you also do not have to mention Michael in every issue, since he subscribed to all changes anyway IINM\r\n\r\nThanks, will keep that in mind.","Confirming what Ben said: the change from `` (nothing) to `[NULL]` is due to #4250, i.e., the actual data in the table is the same in both cases, but just prints differently.\r\n\r\nThe error is thrown here:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/15c127e99f8d6aab599c590d4aec346a850f1334\/src\/assign.c#L420\r\n\r\nSo indeed you'll need to edit the C code here to make the fix. That code is relatively complex, but should also be pretty well-tested, so a good strategy is to add a new test for the desired behavior, then try and make changes. If you do something wrong, you'll likely fail other tests, so if you have a change that passes all existing + your new test, it's likely correct."],"labels":["bug"]},{"title":"add some consistency checks of new tables() quick sizing approach","body":"\r\n> Yes good idea. With a caveat that if R-devel change object.size slightly it could break our test. Maybe find what object.size says now and match that but use that figure instead of the call. Then put the test that actually calls object.size somewhere off CRAN like maybe other.Rraw (by stretching how we think of other to include base). Or, just put it in other.Rraw only. But coverage, ok that's why in tests.Rraw too then.\r\n\r\n_Originally posted by @mattdowle in https:\/\/github.com\/Rdatatable\/data.table\/pull\/5524#discussion_r1020853792_\r\n      ","comments":["and this issue can serve as a reminder to review all the changes to `tables()` and comments in #5524 and its follow-ups. I wouldn't normally make changes to master like this but I was working in the move_ram_tests branch for #5520 whose nature is resolving tipping points in rss to reveal the next test ID. I figured it was better to at least commit changes to tables() separately than do them in that branch.  I expect some changes to tables() before release once #5520 is merged."],"labels":["tests"]},{"title":"Set compression level for gzipped output in fwrite","body":"Can we have functionality to choose compression level for gzipped CSVs in `fwrite()`?","comments":[],"labels":["fwrite"]},{"title":"rbindlist internal error when binding integer64 + character columns","body":"```r\r\nrbindlist(list(\r\n  data.table(bit64::as.integer64(1)),\r\n  data.table(\"1\")\r\n))\r\nError in rbindlist(list(data.table(bit64::as.integer64(1)), data.table(\"1\"))) : \r\n  Internal error: column 1 of result is determined to be integer64 but maxType=='character' != REALSXP\r\n```\r\n","comments":["This seems like a duplicate to #3911 which should already be addressed by #5446 but I don't have a laptop with me to check. ","the solution may be the same but that's not throwing an internal error, so leaving this open for now (feel free to mark as closed by the PR, I also can't check at the moment)"],"labels":["bug","bit64","rbindlist"]},{"title":"`frank` could support reverse ranking","body":"I try to work with dates using `frank` but find that the minus symbol seems not working:\r\n\r\n``` \r\nlibrary(data.table)\r\ndates <- c(\"02\/27\/92\", \"02\/27\/92\", \"01\/14\/92\", \"02\/28\/92\", \"02\/01\/92\")\r\nas.Date(dates, \"%m\/%d\/%y\") -> dates2\r\nfrank(dates2,ties.method = \"min\")\r\n#> [1] 3 3 1 5 2\r\nfrank(-dates2,ties.method = \"min\")\r\n#> Error in `-.Date`(dates2): unary - is not defined for \"Date\" objects\r\n```\r\n\r\nIs there a way to make it work?\r\n\r\n\r\nThanks.\r\n","comments":["It's not a problem with frank, error comes for - call. Make it work there and then use frank. You could possibly turn date to number, apply -, and turn it back to date.","I understand, and have done so. The issue is you can not rank date reversely using minus symbol, and frank does not provide a method for reverse ranking. Is it a necessity? This problem is open.","I know the `frankv` has a `order` parameter, maybe this could be applied to `frank` as well.","Yes, frank uses frankv, so just passing this new arg seems to be a small change."],"labels":["feature request"]},{"title":"Make GForce kick-in smarter","body":"A lot of `GForce` functions only show a speedup when the number of groups in `BY` is high enough and are even slower than their `base` equivalents when applied to a small number of groups.\r\n\r\nI would like to discuss whether it makes sense to turn off `GForce` optimization for smaller values of `.NGRP`\r\n\r\nI know that for a single execution of these statements a difference won't be noticeable but it still seems wrong to power up multithreading for no gain at all.","comments":["On a second thought this might be not a good idea since we have diverging functionalities between `base` and `data.table`, e.g. handling of `NA`, overflows etc.","Worth to check if gforce is slower or it is [ overhead.\r\nNot sure if gforce runs in parallel? Finding groups yet, but just aggregation possibly not?\r\nEven then we have throttle mechanism which switch it off for small input. Then it's good to check if it works as expected, possibly using verbose and see how many threads are being used, if such information is provided.\r\nThere is existing issue about GeForce being smarter, but it focuses on another aspects."],"labels":["performance","GForce"]},{"title":"rbindlist() with list of sf objects doesn't combine different geometries","body":"I've check the NEWS\/issues\/stack overflow but not seen any mentions of rbindlist() in connection with this issue.\r\n\r\nI want to make use of `data.table`'s amazing speed :) in combining sf dataframes as in this example here, [issue #798](https:\/\/github.com\/r-spatial\/sf\/issues\/798#issuecomment-405171212) \r\n\r\nHowever, when different geometries are involved, an error is returned on different class attributes of the geometry column in the sf dataframe. Here is a simple example:\r\n\r\n```\r\nlibrary(sf)\r\n\r\n# create a sf data frame with different geometries \r\nst <- st_as_sf(data.frame(\r\n  id = 1:2,\r\n  geometry = st_as_sfc(c(\"LINESTRING(10 5, 9 4, 8 3, 7 2, 6 1)\",\r\n                         \"MULTILINESTRING ((0 3, 0 4, 1 5, 2 5), (0.2 3, 0.2 4, 1 4.8, 2 4.8), (0 4.4, 0.6 5))\"))))\r\nst\r\n# split into list\r\nst.split <- split(st, ~id)\r\n\r\n# attempt to recombine with rbindlist() - error\r\nst2.dt <- data.table::rbindlist(st.split)\r\n\r\n# Error in data.table::rbindlist(st.split) : \r\n #  Class attribute on column 2 of item 2 does not match with column 2 of item 1.\r\n\r\n# rbind does work:\r\nst2.rbind <- do.call(rbind, st.split)\r\nst2.rbind\r\n\r\n# Is data table checking these classes?\r\nlapply(st.split, function(x) class(x$geometry))\r\n\r\n```\r\n\r\nA workaround can be to use `st_cast()` to unify geometries (eg to `LINESTRING` here, or more generally `GEOMETRYCOLLECTION`) but this is not alway practical.\r\nCan this check on classes be changed for sf objects so that different geometries can be combined? \r\n\r\n`#` `Output of sessionInfo()`\r\n\r\nR version 4.2.1 (2022-06-23 ucrt)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19044)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United Kingdom.utf8  LC_CTYPE=English_United Kingdom.utf8    LC_MONETARY=English_United Kingdom.utf8\r\n[4] LC_NUMERIC=C                            LC_TIME=English_United Kingdom.utf8    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] sf_1.0-8\r\n\r\nloaded via a namespace (and not attached):\r\n [1] Rcpp_1.0.9         rstudioapi_0.14    magrittr_2.0.3     units_0.8-0        tidyselect_1.1.2   R6_2.5.1           rlang_1.0.5       \r\n [8] fansi_1.0.3        dplyr_1.0.9        tools_4.2.1        grid_4.2.1         compare_0.2-6      data.table_1.14.2  KernSmooth_2.23-20\r\n[15] utf8_1.2.2         cli_3.3.0          e1071_1.7-11       DBI_1.1.3          class_7.3-20       assertthat_0.2.1   tibble_3.1.8      \r\n[22] lifecycle_1.0.1    purrr_0.3.4        vctrs_0.4.1        glue_1.6.2         proxy_0.4-27       compiler_4.2.1     pillar_1.8.1      \r\n[29] generics_0.1.3     classInt_0.4-7     pkgconfig_2.0.3   ","comments":["This looks like a duplicate to #3911 ","> This looks like a duplicate to #3911\r\n\r\nYes, it's the same issue with a different use case. I'd favour an ignore class \/ attributes option as well with rbindlist()\r\n","Actually here comes another thing into play:\r\n\r\n`sf` uses attributes and `rbindlist` does not retain them so it is also related to #5569"],"labels":["rbindlist"]},{"title":"encoding test 2194.7 not portable","body":"I am able to reproduce on clean `debian:testing` without setting up any locale.\r\n\r\nimage of debian + r-base + gcc + make:\r\n```sh\r\nsudo docker run -it --rm registry.gitlab.com\/jangorecki\/dockerfiles\/r-base-gcc\r\n```\r\n```\r\n  Sat Nov 25 10:50:59 2023  endian==little, sizeof(long double)==16, longdouble.digits==64, sizeof(pointer)==8, TZ=='UTC', Sys.timezone()=='UTC', Sys.getlocale()=='C', l10n_info()=='MBCS=FALSE; UTF-8=FALSE; Latin-1=FALSE; codeset=ANSI_X3.4-1968', getDTthreads()=='This installation of data.table has not been compiled with OpenMP support.; omp_get_num_procs()==1; R_DATATABLE_NUM_PROCS_PERCENT==unset (default 50); R_DATATABLE_NUM_THREADS==unset; R_DATATABLE_THROTTLE==unset (default 1024); omp_get_thread_limit()==1; omp_get_max_threads()==1; OMP_THREAD_LIMIT==unset; OMP_NUM_THREADS==unset; RestoreAfterFork==true; data.table is using 1 threads with throttle==1024. See ?setDTthreads.', zlib header files were not found when data.table was compiled\r\n  Error: 1 error(s) out of 9985. Search tests\/tests.Rraw for test number(s) 2194.7. Duration: 21.7s elapsed (21.5s cpu).\r\n  In addition: Warning message:\r\n  In readLines(testDir(\"issue_563_fread.txt\")) :\r\n    invalid input found on input connection '\/builds\/Rdatatable\/data.table\/bus\/test-rel-vanilla-lin\/data.table.Rcheck\/data.table\/tests\/issue_563_fread.txt'\r\n```\r\n\r\n\r\nThis is the only error there, and AFAIK we have many encoding related tests...\r\nso not sure if it is expected to install and configure locale just to pass this single test, or maybe improve test somehow. @shrektan any idea?","comments":[],"labels":["tests","encoding"]},{"title":"fread read compressed .bgz","body":"Closes #5461","comments":["# [Codecov](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\n> Merging [#5474](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) (9f92588) into [master](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/commit\/2f675318268ab483da16e4ad49e5d66c9ef7d3b4?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) (2f67531) will **increase** coverage by `0.00%`.\n> The diff coverage is `100.00%`.\n\n```diff\n@@           Coverage Diff           @@\n##           master    #5474   +\/-   ##\n=======================================\n  Coverage   99.51%   99.51%           \n=======================================\n  Files          78       80    +2     \n  Lines       14756    14770   +14     \n=======================================\n+ Hits        14684    14698   +14     \n  Misses         72       72           \n```\n\n\n| [Impacted Files](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) | Coverage \u0394 | |\n|---|---|---|\n| [src\/fread.c](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474\/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-c3JjL2ZyZWFkLmM=) | `99.41% <\u00f8> (\u00f8)` | |\n| [src\/init.c](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474\/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-c3JjL2luaXQuYw==) | `100.00% <\u00f8> (\u00f8)` | |\n| [R\/fread.R](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474\/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-Ui9mcmVhZC5S) | `100.00% <100.00%> (\u00f8)` | |\n| [R\/notin.R](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474\/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-Ui9ub3Rpbi5S) | `100.00% <100.00%> (\u00f8)` | |\n| [src\/negate.c](https:\/\/codecov.io\/gh\/Rdatatable\/data.table\/pull\/5474\/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable#diff-c3JjL25lZ2F0ZS5j) | `100.00% <100.00%> (\u00f8)` | |\n\n:mega: We\u2019re building smart automated test selection to slash your CI\/CD build times. [Learn more](https:\/\/about.codecov.io\/iterative-testing\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable)\n"],"labels":["fread"]},{"title":"fread randomly crashes Rstudio (server) when in a loop","body":"Using Rstudio server with R 4.1.0 and data.table 1.14.2.\r\n\r\nThis is a hard one to reproduce, but I have about 1000 tsv files that I am reading and appending together in a loop. Each file has ~150k lines and 23 columns. For some reason it randomly crashes R when it's somewhere in the middle of the loop, and also at random points. At first, I used rbindlist(lapply(files,fread) or files %>% map(fread) and when it crashed I thought there was something wrong with the files themselves. But when I made a loop like:\r\n\r\n```\r\nout <- list()\r\nfor(i in 1:length(files)) {\r\n  print(files[i])\r\n  out[[i]] <- fread(files[i])\r\n  print(\"done\")\r\n}\r\n```\r\n\r\nIt would crash randomly in the loop when I run this multiple times.\r\n\r\nConcatenating the file with bash seems to fix the issue, so I think there's something strange going on with fread.\r\n\r\n`#` `Output of sessionInfo()`\r\n\r\n```\r\nR version 4.1.0 (2021-05-18)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Ubuntu 20.04.3 LTS\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/x86_64-linux-gnu\/atlas\/libblas.so.3.10.3\r\nLAPACK: \/usr\/lib\/x86_64-linux-gnu\/atlas\/liblapack.so.3.10.3\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \r\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \r\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \r\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n [1] forcats_0.5.1     stringr_1.4.0     purrr_0.3.4       readr_2.1.2      \r\n [5] tidyr_1.2.0       tibble_3.1.3      ggplot2_3.3.5     tidyverse_1.3.1  \r\n [9] data.table_1.14.2 dplyr_1.0.7      \r\n\r\nloaded via a namespace (and not attached):\r\n [1] Rcpp_1.0.7       cellranger_1.1.0 pillar_1.6.2     compiler_4.1.0  \r\n [5] dbplyr_2.1.1     tools_4.1.0      jsonlite_1.7.2   lubridate_1.7.10\r\n [9] lifecycle_1.0.0  gtable_0.3.0     pkgconfig_2.0.3  rlang_0.4.11    \r\n[13] reprex_2.0.1     cli_3.0.1        rstudioapi_0.13  DBI_1.1.1       \r\n[17] haven_2.4.3      xml2_1.3.2       withr_2.4.2      httr_1.4.2      \r\n[21] fs_1.5.0         generics_0.1.0   vctrs_0.3.8      hms_1.1.0       \r\n[25] grid_4.1.0       tidyselect_1.1.1 glue_1.6.2       R6_2.5.1        \r\n[29] fansi_0.5.0      readxl_1.3.1     tzdb_0.1.2       modelr_0.1.8    \r\n[33] magrittr_2.0.1   backports_1.2.1  scales_1.1.1     ellipsis_0.3.2  \r\n[37] rvest_1.0.1      assertthat_0.2.1 colorspace_2.0-2 utf8_1.2.2      \r\n[41] stringi_1.7.3    munsell_0.5.0    broom_0.7.9      crayon_1.4.1   \r\n```\r\n","comments":["Is it reproducible in R? Console, not IDE.","Just did this twice on console with verbose=T. Here's the outputs. It says segfault but I have 1TB of memory on this machine... I've pasted the output for the loop segment where it crashed (first time it was the 200ish file, the second time it was the 130ish or so).\r\n\r\n```\r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            112\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          112\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 56 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 56 threads (omp_get_max_threads()=112, nth=56)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file [... redacted ...]\r\n  File opened, size = 25.22MB (26448169 bytes).\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<chrom        start   end     t-statistic     pv>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=0x9  with 100 lines of 24 fields using quote rule 0\r\n  Detected 24 columns on line 1. This line is either column names or first data row. Line starts as: <<chrom    start   end     t-statistic     pv>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 24\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 100 because (26448168 bytes from row 1 to eof) \/ (2 * 13694 jump0size) == 965\r\n  Type codes (jump 000)    : C55855575555555577777777  Quote rule 0\r\n  Type codes (jump 001)    : C55875775755775577777777  Quote rule 0\r\n  Type codes (jump 020)    : C55877775755775577777777  Quote rule 0\r\n  Type codes (jump 100)    : C55877775755775577777777  Quote rule 0\r\n  'header' determined to be true due to column 2 containing a string on row 1 and a lower type (int32) in the rest of the 10048 sample rows\r\n  =====\r\n  Sampled 10048 rows (handled \\n inside quoted fields) at 101 jump points\r\n  Bytes from first data row on line 2 to the end of last row: 26447980\r\n  Line length: mean=141.03 sd=6.02 min=124 max=224\r\n  Estimated number of rows: 26447980 \/ 141.03 = 187532\r\n  Initial alloc = 206285 rows (187532 + 9%) using bytes\/max(mean-2*sd,min) clamped between [1.1*estn, 2.0*estn]\r\n  =====\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : C55877775755775577777777\r\n[10] Allocate memory for the datatable\r\n  Allocating 24 column slots (24 - 0 dropped) with 206285 rows\r\n[11] Read the data\r\n  jumps=[0..25), chunk_size=1057919, total_size=26447980\r\n\r\n *** caught segfault ***\r\naddress 0x7f15880a1efe, cause 'invalid permissions'\r\n\r\nTraceback:\r\n 1: fread(files[i], verbose = T)\r\n\r\nPossible actions:\r\n1: abort (with core dump, if enabled)\r\n2: normal R exit\r\n3: exit R without saving workspace\r\n4: exit R saving workspace\r\n```\r\n\r\n```\r\n OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            112\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          112\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 56 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 56 threads (omp_get_max_threads()=112, nth=56)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file \/mnt\/ix1\/Projects\/M081_20210811_CREST_proc\/A01_CREST_sequencing_data\/bedgraph_filter_bigvalidation\/CREST.pvals_OOB_filter.big_validation.swap_valid_lowcount_138.pvals_corrected.txt\r\n  File opened, size = 26.00MB (27262748 bytes).\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<chrom        start   end     t-statistic     pv>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=0x9  with 100 lines of 24 fields using quote rule 0\r\n  Detected 24 columns on line 1. This line is either column names or first data row. Line starts as: <<chrom    start   end     t-statistic     pv>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 24\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 100 because (27262747 bytes from row 1 to eof) \/ (2 * 16087 jump0size) == 847\r\n  Type codes (jump 000)    : C55855575555555577777777  Quote rule 0\r\n  Type codes (jump 031)    : C55875775755775577777777  Quote rule 0\r\n  Type codes (jump 100)    : C55875775755775577777777  Quote rule 0\r\n  'header' determined to be true due to column 2 containing a string on row 1 and a lower type (int32) in the rest of the 10048 sample rows\r\n  =====\r\n  Sampled 10048 rows (handled \\n inside quoted fields) at 101 jump points\r\n  Bytes from first data row on line 2 to the end of last row: 27262559\r\n  Line length: mean=164.93 sd=5.72 min=148 max=250\r\n  Estimated number of rows: 27262559 \/ 164.93 = 165300\r\n  Initial alloc = 181830 rows (165300 + 10%) using bytes\/max(mean-2*sd,min) clamped between [1.1*estn, 2.0*estn]\r\n  =====\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : C55875775755775577777777\r\n[10] Allocate memory for the datatable\r\n  Allocating 24 column slots (24 - 0 dropped) with 181830 rows\r\n[11] Read the data\r\n  jumps=[0..25), chunk_size=1090502, total_size=27262559\r\n  2 out-of-sample type bumps: C55877777755775577777777\r\n  jumps=[0..25), chunk_size=1090502, total_size=27262559\r\nRead 165253 rows x 24 columns from 26.00MB (27262748 bytes) file in 00:00.043 wall clock time\r\n[12] Finalizing the datatable\r\n  Type counts:\r\n         6 : int32     '5'\r\n        16 : float64   '7'\r\n         1 : float64   '8'\r\n         1 : string    'C'\r\n=============================\r\n   0.000s (  0%) Memory map 0.025GB file\r\n   0.005s ( 12%) sep='\\t' ncol=24 and header detection\r\n   0.000s (  0%) Column type detection using 10048 sample rows\r\n   0.000s (  1%) Allocation of 181830 rows x 24 cols (0.027GB) of which 165253 ( 91%) rows used\r\n   0.037s ( 87%) Reading 25 chunks (0 swept) of 1.040MB (each chunk 6610 rows) using 25 threads\r\n   +    0.014s ( 33%) Parse to row-major thread buffers (grown 0 times)\r\n   +    0.004s ( 10%) Transpose\r\n   +    0.019s ( 44%) Waiting\r\n   0.019s ( 44%) Rereading 2 columns due to out-of-sample type exceptions\r\n   0.043s        Total\r\nColumn 6 (\"mean_case\") bumped from 'int32' to 'float64' due to <<1.66666666666667>> on row 1234\r\nColumn 9 (\"var_case\") bumped from 'int32' to 'float64' due to <<13.8888888888889>> on row 1234\r\n[1] \"done\"\r\n[1] \"\/mnt\/ix1\/Projects\/M081_20210811_CREST_proc\/A01_CREST_sequencing_data\/bedgraph_filter_bigvalidation\/CREST.pvals_OOB_filter.big_validation.swap_valid_lowcount_139.pvals_corrected.txt\"\r\n  OpenMP version (_OPENMP)       201511\r\n  omp_get_num_procs()            112\r\n  R_DATATABLE_NUM_PROCS_PERCENT  unset (default 50)\r\n  R_DATATABLE_NUM_THREADS        unset\r\n  R_DATATABLE_THROTTLE           unset (default 1024)\r\n  omp_get_thread_limit()         2147483647\r\n  omp_get_max_threads()          112\r\n  OMP_THREAD_LIMIT               unset\r\n  OMP_NUM_THREADS                unset\r\n  RestoreAfterFork               true\r\n  data.table is using 56 threads with throttle==1024. See ?setDTthreads.\r\nInput contains no \\n. Taking this to be a filename to open\r\n[01] Check arguments\r\n  Using 56 threads (omp_get_max_threads()=112, nth=56)\r\n  NAstrings = [<<NA>>]\r\n  None of the NAstrings look like numbers.\r\n  show progress = 1\r\n  0\/1 column will be read as integer\r\n[02] Opening the file\r\n  Opening file  [... redacted ...]\r\n  File opened, size = 21.50MB (22539193 bytes).\r\n  Memory mapped ok\r\n[03] Detect and skip BOM\r\n[04] Arrange mmap to be \\0 terminated\r\n  \\n has been found in the input and different lines can end with different line endings (e.g. mixed \\n and \\r\\n in one file). This is common and ideal.\r\n[05] Skipping initial rows if needed\r\n  Positioned on line 1 starting: <<chrom        start   end     t-statistic     pv>>\r\n[06] Detect separator, quoting rule, and ncolumns\r\n  Detecting sep automatically ...\r\n  sep=0x9  with 100 lines of 24 fields using quote rule 0\r\n  Detected 24 columns on line 1. This line is either column names or first data row. Line starts as: <<chrom    start   end     t-statistic     pv>>\r\n  Quote rule picked = 0\r\n  fill=false and the most number of columns found is 24\r\n[07] Detect column types, good nrow estimate and whether first row is column names\r\n  Number of sampling jump points = 100 because (22539192 bytes from row 1 to eof) \/ (2 * 14873 jump0size) == 757\r\n  Type codes (jump 000)    : C55855575555555577777777  Quote rule 0\r\n  Type codes (jump 028)    : C55875775755775577777777  Quote rule 0\r\n  Type codes (jump 100)    : C55875775755775577777777  Quote rule 0\r\n  'header' determined to be true due to column 2 containing a string on row 1 and a lower type (int32) in the rest of the 10048 sample rows\r\n  =====\r\n  Sampled 10048 rows (handled \\n inside quoted fields) at 101 jump points\r\n  Bytes from first data row on line 2 to the end of last row: 22539004\r\n  Line length: mean=152.98 sd=6.20 min=138 max=250\r\n  Estimated number of rows: 22539004 \/ 152.98 = 147338\r\n  Initial alloc = 162071 rows (147338 + 9%) using bytes\/max(mean-2*sd,min) clamped between [1.1*estn, 2.0*estn]\r\n  =====\r\n[08] Assign column names\r\n[09] Apply user overrides on column types\r\n  After 0 type and 0 drop user overrides : C55875775755775577777777\r\n[10] Allocate memory for the datatable\r\n  Allocating 24 column slots (24 - 0 dropped) with 162071 rows\r\n[11] Read the data\r\n  jumps=[0..21), chunk_size=1073285, total_size=22539004\r\n\r\n *** caught segfault ***\r\naddress 0x7f4e8dfa8efe, cause 'invalid permissions'\r\n\r\nTraceback:\r\n 1: fread(files[i], verbose = T)\r\n\r\nPossible actions:\r\n1: abort (with core dump, if enabled)\r\n2: normal R exit\r\n3: exit R without saving workspace\r\n4: exit R saving workspace\r\n```","Possible a memory leak, possible the same as #3292? There's a long-standing PR which is supposed to fix that memory leak, you might try installing from that branch if it solves your issue...","According to the number of threads this could also be #5077","Just want to bump this to say that the only way that I've worked around this was to set the number of threads to 1 and then set it back to default after the loop finishes. Definitely an odd bug.","Can you share one of the `tsv` where the segfault happens @billytcl? Reading it multiple (e.g. 1000) times might be a possibility to make it somewhat reproduceable."],"labels":["fread","segfault"]},{"title":"`DT` function returns the wrong class when `j` depends on an entry condition that is not satisfied for all groups in `by`.","body":"This is an inconsistency issue in the new function `DT`; it returns the wrong class when `j` depends on an entry condition that is not satisfied for all groups in `by`. \r\n\r\n\ttb = tibble::tibble(a = c(19, 27, 10, 25, 34), b = c(1, 3, 3, 1, 3))\r\n\r\n\t# A tibble: 5 x 2\r\n\t      a     b\r\n\t  <dbl> <dbl>\r\n\t1    19     1\r\n\t2    27     3\r\n\t3    10     3\r\n\t4    25     1\r\n\t5    34     3\r\n\r\n\ttb |> DT(, if(.N==2) .SD, by=b) |> class()               # returns right class\r\n\t[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\r\n\r\n\ttb |> DT(, if(.N==5) .SD, by=b) |> class()               # returns wrong class (if no group has 5 rows)\r\n\t[1] \"data.table\" \"data.frame\"\r\n\r\nNote that this is a general issue and also happens with other data frame extensions (panel data frame _pdata.frame_, etc.).\r\n","comments":[],"labels":["dev","DT()"]},{"title":"Add .bgz file decompression to data.table::fread() (compatible with .gz)","body":"Currently, .bgz files are read as plain text, which fails due to invalid characters. [.bgz files are compatible with gunzip](http:\/\/www.htslib.org\/doc\/bgzip.html), and have the same data header (0x1F 0x8B). renaming *.bgz files to *.gz files allows them to be decompressed normally.\r\n\r\nAdding .bgz to the list of files that can be decompressed by data.table::fread shouldn't require anything other than R.utils. I think adding `\".bgz\"` to the vector in this line:\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/R\/fread.R#L121\r\n\r\nAnd checking for `w<=2` on this line\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/R\/fread.R#L124\r\n\r\nWould allow fread to decompress .bgz files automatically. However, I haven't tested this.","comments":["Do you have any .bgz files you could share for testing?","Sure, here are three files containing the same (uncompressed) contents:\r\n\r\n[raw_data.txt](https:\/\/github.com\/Rdatatable\/data.table\/files\/9657186\/raw_data.txt)\r\n[raw_data.txt.gz](https:\/\/github.com\/Rdatatable\/data.table\/files\/9657188\/raw_data.txt.gz)\r\n[raw_data.txt.bgz.zip](https:\/\/github.com\/Rdatatable\/data.table\/files\/9657196\/raw_data.txt.bgz.zip)\r\n\r\nGithub doesn't allow .bgz file extensions, so I put the .bgz file in a .zip archive.\r\n\r\nmds5 sums are:\r\n\r\n```\r\n63b4ba65676e975cbe336c74ff489c3a  raw_data.txt\r\n3298f1a8a311b732701e2cfac830860b  raw_data.txt.bgz\r\nfed5bb8aff303ceb7ab1939b7d5e5005  raw_data.txt.gz\r\n```\r\n\r\nAnd the zcat output for all is:\r\n\r\n```\r\nsome file information to compress\r\narbitrary data 1\r\ndata line 2 also\r\n```"],"labels":["fread","help-wanted"]},{"title":"Calling data.table::DT cannot find `[.data.table` method when data.table is not loaded","body":"`DT` cannot find `[.data.table` method if called in the form `data.table::DT` when data.table is not loaded\r\n\r\n\tdata.table::DT(mtcars, 1:2)                                  # does not work\r\n\r\n\tError in `[.data.table`(x = mtcars, 1:2) : \r\n\t  could not find function \"[.data.table\"\r\n\r\n\r\n\tdata.table::`[.data.table`(mtcars, 1:2)                      # works\r\n\r\n\t  mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\n\t1  21   6  160 110  3.9 2.620 16.46  0  1    4    4\r\n\t2  21   6  160 110  3.9 2.875 17.02  0  1    4    4\r\n\r\n\r\nSession info\r\n\r\n\tR version 4.1.0 (2021-05-18)\r\n\tPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\n\tRunning under: Windows 10 x64 (build 19043)\r\n\r\n\tMatrix products: default\r\n\r\n\tlocale:\r\n\t[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252\r\n\t[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252    \r\n\r\n\tattached base packages:\r\n\t[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\n\tloaded via a namespace (and not attached):\r\n\t [1] fansi_1.0.3          utf8_1.2.2           crayon_1.5.1         lifecycle_1.0.1      magrittr_2.0.3      \r\n\t [6] pillar_1.7.0         bench_1.1.2          rlang_1.0.3          stringi_1.7.6        cli_3.2.0           \r\n\t[11] data.table_1.14.3    rstudioapi_0.13      vctrs_0.4.1          ellipsis_0.3.2       forcats_0.5.1       \r\n\t[16] tools_4.1.0          glue_1.6.2           hms_1.1.1            compiler_4.1.0       pkgconfig_2.0.3     \r\n\t[21] microbenchmark_1.4.9 haven_2.5.0          tibble_3.1.7        \r\n\r\n","comments":["Thanks for the report! I can reproduce.\r\n\r\nI am rusty on namespace requirements, but one approach to allow for this to work is to adjust this line\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/R\/data.table.R#L1974\r\n\r\nto\r\n\r\n```\r\nfun[[1L]] = str2lang(\"data.table::`[.data.table`\")\r\n```\r\nHere's a `reprex` demonstration of it which means that it was done in a new environment.  Note the `DT2()` function is a simplified version that skips checks in the goal of having a shorter example.\r\n``` r\r\nDT2 = function(x, ...) {  #4872\r\n  fun = match.call()\r\n  fun[[1L]] = str2lang(\"data.table::`[.data.table`\")   # hence now exporting [.data.table method otherwise R CMD check can't find it in tests 2212.*\r\n  ans = eval(fun, envir = parent.frame(), enclos = parent.frame())\r\n\r\n  ans\r\n}\r\nDT2(mtcars, 1:2)\r\n#>   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\n#> 1  21   6  160 110  3.9 2.620 16.46  0  1    4    4\r\n#> 2  21   6  160 110  3.9 2.875 17.02  0  1    4    4\r\n```\r\n_Edit_: it looks like `str2lang()` might be relatively new to R. These alternatives also work\r\n```\r\n# quote\r\nfun[[1L]] = quote(data.table::`[.data.table`)\r\n\r\n#parse(text =...)\r\nfun[[1L]] = parse(text = \"data.table::`[.data.table`\", keep.source = FALSE)[[1L]]\r\n```\r\n","This will not work for our dev helper function `cc` because it will refer to installed pkg rather than the one already in-dev","I noticed that the method was exported before the function. I switched the order of the two and the original issue was addressed (yes!). I can do a PR. @jangorecki is there a way to add a test? Specifically, can we test when the `data.table` package is not loaded within our test suite? Otherwise, I will do a PR that is simply the line switch.\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/NAMESPACE#L62-L63","No idea how to test that. Yes please do PR.","Sorry, I was mistaken. I accidently was loading the package after the build. My proposed fix does not work.\r\n\r\nAfter digging deeper, it seems like this `eval(..., envir = parent.frame(), enclos())` is what causes this. If we evaluated it as `eval(...)` then it works as expected. Unfortunately, #5129 needs those calls in order to function. \r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/R\/data.table.R#L1975-L1977"],"labels":["dev","DT()"]},{"title":"Pipe with DT() after join-update with addition double-counts","body":"This is a little hard to explain without code, so:\r\n```\r\nlibrary(data.table)\r\na <- data.table(id = letters[24:26], data_a = 1:3)\r\nb <- data.table(id = letters[24:26], data_b = 4:6)\r\na[b, data_a := data_a + data_b, on = 'id'] |> \r\n  DT(, data_a := data_a + 1L)\r\n```\r\nIncorrectly gives:\r\n```\r\n       id data_a\r\n   <char>  <int>\r\n1:      x     10\r\n2:      y     13\r\n3:      z     16\r\n```\r\nIt's apparently double-counting data_b.\r\nRemoving the pipe and using `][` instead works fine.\r\nUsing a separate `a[, data_a := data_a + 1L]` works fine.\r\n```\r\na\r\n       id  data\r\n   <char> <int>\r\n1:      x     6\r\n2:      y     8\r\n3:      z    10\r\n```\r\nAlso creating a new column to hold the sum works fine even with the pipe:\r\n```\r\na <- data.table(id = letters[24:26], data_a = 1:3)\r\nb <- data.table(id = letters[24:26], data_b = 4:6)\r\na[b, data := data_a + data_b, on = 'id'] |> \r\n  DT(, data := data + 1L) |> \r\n  DT(, data_a := NULL)\r\n       id  data\r\n   <char> <int>\r\n1:      x     6\r\n2:      y     8\r\n3:      z    10\r\n```\r\ndata.table::update_dev_pkg()\r\nR data.table package is up-to-date at c4a2085e35689a108d67dacb2f8261e4964d7e12 (1.14.3)","comments":["Thanks for the report!\r\n\r\nI can reproduce without a join:\r\n\r\n``` r\r\nlibrary(data.table)\r\na = data.table(id = 1:3, x = 1:3, z = 4:6)\r\nDT(a[, x:= x + z])\r\n\r\n#>       id     x     z\r\n#>    <int> <int> <int>\r\n#> 1:     1     9     4\r\n#> 2:     2    12     5\r\n#> 3:     3    15     6\r\n```\r\n\r\nThe cause is related to the call being done twice as determined with setting `options(datatable.verbose = TRUE)` which repeats the same message twice. In the `DT` function definition, we first determine if it is `is.data.table(x)` which would evaluate the expression within the call and then we explicitly `eval(...)` the expression. \r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/R\/data.table.R#L1967-L1975\r\n\r\nOne fix would be to always set the optimize level to 2 or less which means we would be able to skip checking the class of what is being evaluated. NSE might be an option to allow for simple cases like this but would add complication and might not be ideal. \r\n\r\nA last note is that it does work with the `magrittr` `%>%` pipe. My understanding is that the base R pipe does preprocessing so `f(x) |> g()` is directly interpreted and evaluated as as `g(f(x))` whereas `magrittr` would have an intermediate of `g(res_f_x)` which is why `%>%` does not cause a problem in this use case.","Hi , i was checking out this issue , i am not able to reproduce it .\r\n```\r\n a <- data.table(id = letters[24:26], data_a = 1:3)\r\n> b <- data.table(id = letters[24:26], data_b = 4:6)\r\n> a[b, data_a := data_a + data_b, on = 'id'] |> \r\n+   DT(, data_a := data_a + 1L)\r\nError in DT(a[b, `:=`(data_a, data_a + data_b), on = \"id\"], , `:=`(data_a,  : \r\n  could not find function \"DT\"\r\n```\r\nor \r\n```\r\n a = data.table(id = 1:3, x = 1:3, z = 4:6)\r\n> DT(a[, x:= x + z])\r\nError in DT(a[, `:=`(x, x + z)]) : could not find function \"DT\"\r\n```\r\nFunction DT has been removed or changed ??, so this issue should be closed now?","We unexported `DT()` in #5472 since there are still some issues with it. You can use it either by loading `data.table` via `cc()` (checkout the README in `.dev`) or by using explicit namespacing for unexported functions with `data.table:::DT`.\r\n\r\nSo the issue still needs to be taken care of but has no \"high priority\"","for now I would not spend time on it in light of #5621. future of DT() should be decided first."],"labels":["bug","dev","DT()"]},{"title":"Understanding the value of .I for non-matching rows when using .EACHI","body":"I join two data.tables, use `.I` in `j`, and `by = .EACHI`. When a row in `i` has no match to `x` the result is `0`. I wish to understand why this is the case.\r\n\r\nSome toy data:\r\n\r\n```\r\nd1 = data.table(v = c(\"A\", \"B\", \"C\", \"A\", \"C\"))\r\n\r\n# add column identical (value-wise) to .I\r\nd1[ , i := .I]\r\n\r\nd2 = data.table(v = c(\"D\", \"A\", \"G\", \"C\"))\r\n\r\nd1\r\n#    v i\r\n# 1: A 1\r\n# 2: B 2\r\n# 3: C 3\r\n# 4: A 4\r\n# 5: C 5\r\n\r\nd2\r\n#    v\r\n# 1: D\r\n# 2: A\r\n# 3: G\r\n3 4: C\r\n```\r\n\r\nJoin the two tables on 'v'. In `j`, call either \"i\" or `.I`. Use `by = .EACHI` (\"evaluates `j` for the groups in 'DT' that each row in `i` joins to\"). \r\n\r\nWhen `j` is \"i\" (which at least \"looks the same\" as `.I`), non-matched rows evaluates to `NA`. To me, this seems consistent with the default `nomatch` behaviour: \"When a row in `i` has no match to `x`, `nomatch=NA` (default) means `NA` is returned\":\r\n```\r\nd1[d2, on = .(v), i, by = .EACHI]\r\n#    v  i\r\n# 1: D NA # unmatched row in `i` evaluates to NA\r\n# 2: A  1\r\n# 3: A  4\r\n# 4: G NA # unmatched row in `i` evaluates to NA\r\n# 5: C  3\r\n# 6: C  5\r\n```\r\nOn the other hand, when `j` is `.I`, non-matched rows evaluates to `0`:\r\n\r\n```\r\nd1[d2, on = .(v), .I, by = .EACHI]\r\n#    v I\r\n# 1: D 0 # unmatched row in `i` evaluates to 0\r\n# 2: A 1\r\n# 3: A 4\r\n# 4: G 0 # unmatched row in `i` evaluates to 0\r\n# 5: C 3\r\n# 6: C 5\r\n``` \r\n\r\nFrom `?.I`:\r\n>While grouping, it holds for each item in the group, its row location in `x`\r\n\r\nHowever, I fail to find documentation on how unmatched rows in `i` evaluate to 0 when `j = .I`. Can someone help me understand this _seemingly_ inconsistent behaviour? \r\n\r\n------\r\n\r\nR version 4.2.1 (2022-06-23 ucrt)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19044)\r\n\r\nTried on:\r\ndata.table_1.14.2 &\r\ndata.table 1.14.3 IN DEVELOPMENT built 2022-07-20 18:26:12 UTC\r\n\r\n","comments":["Worth to try on devel as well just to ensure it haven't changed since 1.14.2","Thanks @jangorecki,  I forgot to include that I also attempted with devel version. Updated the post.","I just found a related open issue: [With by=.EACHI and unmatched i, can we set nomatch= to get .SD[0] instead of .SD[NA]?](https:\/\/github.com\/Rdatatable\/data.table\/issues\/3452), with a comment on the same result as here:\r\n\r\n>.I correctly (?) displays 0 for unmatched group"],"labels":["documentation","consistency"]},{"title":"Documentation bug in datatable-reshape vignette","body":"In the current `datatable-reshape` vignette in CRAN, https:\/\/cran.r-project.org\/web\/packages\/data.table\/vignettes\/datatable-reshape.html, in the section corresponding to\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/blob\/c4a2085e35689a108d67dacb2f8261e4964d7e12\/vignettes\/datatable-reshape.Rmd#L157-L174\r\n\r\nthe variables `dob` and `gender` become `IDate` format in `DT.c1`, so the types `character` and `integer` mentioned at point 2 (L174) seem to be wrong.","comments":[],"labels":["help-wanted"]},{"title":"Error message for unspecified join gives incorrect advice on how to do a natural join","body":"When I try to do a join `x[i]` on an unkeyed data.table `x`, I get an error message which tells me:\r\n\r\n> When i is a data.table (or character vector), the columns to join by must be specified using 'on=' argument (see ?data.table), by keying x (i.e. sorted, and, marked as sorted, see ?setkey), **_or by sharing column names between x and i (i.e., a natural join)._**\r\n\r\n(Incidentally, this error message has no line breaks, adding some might be good for readability in the terminal.)\r\n\r\nThis seems to me that this syntax should perform a natural join if column names are shared between `x` and `i`. However, this is not the case, and attempting to do so just produces the same error message (see example below). Digging into the `?data.table` docs, it appears that the keyword `on = .NATURAL` must be provided for a natural join. I think that should be indicated in the error message, perhaps:\r\n\r\n> ... or by sharing column names between x and i (i.e., a natural join) with the keyword 'on = .NATURAL'.\r\n\r\n# Minimal reproducible example\r\n\r\n```R\r\nx <- data.table(foo = c(1,2,3), bar = c(4,5,6))\r\ni <- data.table(baz = c(2,4,6), foo = c(1,2,3))\r\nx[i]\r\n# Error in `[.data.table`(x, i) : \r\n#  When i is a data.table (or character vector), the columns to join by must be specified using\r\n#  'on=' argument (see ?data.table), by keying x (i.e. sorted, and, marked as sorted, see ?setkey),\r\n#  or by sharing column names between x and i (i.e., a natural join). Keyed joins might have\r\n#  further speed benefits on very large data due to x being sorted in RAM.\r\n# [Line breaks added for readability]\r\nx[i, on=.NATURAL]\r\n#    foo bar baz\r\n# 1:   1   4   2\r\n# 2:   2   5   4\r\n# 3:   3   6   6\r\n```\r\n\r\n# Output of sessionInfo()\r\n\r\n```\r\nR version 4.2.1 (2022-06-23)\r\nPlatform: aarch64-apple-darwin20 (64-bit)\r\nRunning under: macOS Monterey 12.5.1\r\n\r\nMatrix products: default\r\nLAPACK: \/Library\/Frameworks\/R.framework\/Versions\/4.2-arm64\/Resources\/lib\/libRlapack.dylib\r\n\r\nlocale:\r\n[1] en_US.UTF-8\/en_US.UTF-8\/en_US.UTF-8\/C\/en_US.UTF-8\/en_US.UTF-8\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.2.1 tools_4.2.1   \r\n```\r\n","comments":[],"labels":["help-wanted"]},{"title":"`rbind` cannot handle different name encodings","body":"In the following case, the bind does not work:\r\n\r\n```R\r\nx <- data.table(A = 1, B = 2, C = 3)\r\ny <- copy(x)\r\nsetnames(x , c(\"\u00c4\", \"\u00d6\", \"\u00dc\"))\r\nsetnames(y , iconv(c(\"\u00c4\", \"\u00d6\", \"\u00dc\"), from = \"UTF-8\", to = \"latin1\"))\r\nEncoding(names(x))\r\nEncoding(names(y))\r\nrbind(x,y)\r\n```\r\n\r\nOutput:\r\n```\r\n[1] \"UTF-8\" \"UTF-8\" \"UTF-8\"\r\n[1] \"latin1\" \"latin1\" \"latin1\"\r\n\r\nError in rbindlist(l, use.names, fill, idcol) (rbindbug.R#7): Column 1 ['\u00c4'] of item 2 is missing in item 1. Use fill=TRUE to fill with NA (NULL for list columns), or use.names=FALSE to ignore column names.\r\nShow stack trace\r\n```\r\n\r\n```\r\n> sessionInfo()\r\nR version 4.1.2 (2021-11-01)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 22000)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252\r\n[3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C\r\n[5] LC_TIME=German_Germany.1252\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base\r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.1.2\r\n```\r\n","comments":["Well you could always use option `use.names = FALSE`, but I guess what you had in mind was that binding with `use.names` should work with different encodings.","Exactly, that's what I meant.  Otherwise you would also have to permanently consider the order of the columns."],"labels":["encoding","rbindlist"]},{"title":"rolling functions: adaptive left, frollmax, frollapply adaptive, partial","body":"scope:\r\n\r\n- [x] `adaptive && align=\"left\"` #5438 \r\n- [x] frollmax adaptive (exact) #5438 \r\n- [x] frollmax (fast, exact) #2778 \r\n- [x] give.names argument https:\/\/github.com\/Rdatatable\/data.table\/issues\/2778\r\n- [x] frollapply adaptive #5438\r\n- [x] de-duplicate logic (by adding helper function - `partial2adaptive`, `coerceK`, `ansSetMsg`, or redirecting frollapply to froll) of\r\n  - `align`\r\n  - early stopping for window bigger than x\r\n  - partial argument support\r\n  - left adaptive support\r\n  - warnings, errors and some messages\r\n  - k arg handling\r\n- [x] `algo=\"fast\"` support for `Inf` and `-Inf` (breaking change).\r\n- [x] rename hasNA to has.nf (has non-finite)\r\n- [x] partial argument #4968 \r\n- [x] documentation improvements #5306\r\n\r\nbenchmarks:\r\n\r\n- adaptive left aligned frollmax benchmark is in [NEWS.md](https:\/\/github.com\/Rdatatable\/data.table\/blob\/frollmax\/NEWS.md) file\r\n- below is a simple rolling max use case against very fast `roll` library\r\n```r\r\nlibrary(roll)\r\nlibrary(data.table)\r\nset.seed(108)\r\nx = rnorm(1e8)\r\nn = 1e5\r\nsystem.time(\r\n  a1 <- roll_max(x, n)\r\n)\r\n#   user  system elapsed \r\n#  1.593   0.148   1.741 \r\nsystem.time(\r\n  a2 <- frollmax(x, n)\r\n)\r\n#   user  system elapsed \r\n#  0.260   0.176   0.435 \r\nall.equal(a1, a2)\r\n#[1] TRUE\r\n```","comments":["@Rdatatable\/project-members any idea why codecov is not working?\r\n\r\nThis PR could possibly be ready for review.","> @Rdatatable\/project-members any idea why codecov is not working?\r\n> \r\n> This PR could possibly be ready for review.\r\n\r\nFWIW running `covr::codecov()` does also not work on my PC locally for this PR (after building + installing it) although it works for other PRs.\r\n\r\nThe local error message says `undefined symbol: wmax`","Thanks Ben for pointing out. I tried to running it locally, I am getting quite the same error as on CI here, so no mention about `wmax`. I also scanned `wmax` for potential problems but don't see anything there...","@brodieG in case you would like to look at roll funs manual\/implementation (as twitter suggests), it is probably best to look at this PR rather than master.","Excellent, thanks @jangorecki.","@jimhester Jim, could you by any chance, give a hint on debugging failure of covr as we experience here?\r\nFor me running locally gives same failure as Actions. Ben is getting another error, related to an inline function.","Running locally with `covr::package_coverage(quiet = FALSE, clean = FALSE)` gets me this linker error,\r\n\r\n```\r\nError: package or namespace load failed for \u2018data.table\u2019 in dyn.load(file, DLLpath = DLLpath, ...):\r\n unable to load shared object '\/private\/var\/folders\/v4\/gklzj0ms5mb3kp20s64y4dcc0000gn\/T\/Rtmp6gClID\/R_LIBS857536a04341\/data.table\/libs\/data_table.so':\r\n  dlopen(\/private\/var\/folders\/v4\/gklzj0ms5mb3kp20s64y4dcc0000gn\/T\/Rtmp6gClID\/R_LIBS857536a04341\/data.table\/libs\/data_table.so, 0x0006): symbol not found in flat namespace (_wmax)\r\n```\r\n\r\nI think you need to define the wmax method as `static inline` rather than just `inline`, as the linker is not picking it up currently.\r\n\r\nDoing that fixes the issue for me.","Thanks a lot Jim!","## [Codecov](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5441?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) Report\nAll modified and coverable lines are covered by tests :white_check_mark:\n> Comparison is base [(`5376881`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/commit\/537688106718b72e04ddf2859c3ec61a5aed2dc0?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.46% compared to head [(`fff2b08`)](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5441?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable) 97.52%.\n> Report is 95 commits behind head on master.\n\n\n<details><summary>Additional details and impacted files<\/summary>\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #5441      +\/-   ##\n==========================================\n+ Coverage   97.46%   97.52%   +0.05%     \n==========================================\n  Files          80       80              \n  Lines       14822    15117     +295     \n==========================================\n+ Hits        14447    14743     +296     \n+ Misses        375      374       -1     \n```\n\n\n\n<\/details>\n\n[:umbrella: View full report in Codecov by Sentry](https:\/\/app.codecov.io\/gh\/Rdatatable\/data.table\/pull\/5441?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).   \n:loudspeaker: Have feedback on the report? [Share it here](https:\/\/about.codecov.io\/codecov-pr-comment-feedback\/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=Rdatatable).\n","Test coverage is lacking. I think it make sense to turn macros into functions or just paste their bodies, otherwise they are not considered by codecov","Ready to review","Eager to see rolling functions with partial windows be released as well!  Will this functionality be available by group as well?\r\n\r\ne.g.  where `dt` is a data.table with a continuous variable `x` and a categorical variable `grp`\r\n\r\n```\r\ndt <- data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\n\r\ndt[, roll3partial := frollmean(x, 3, partial=TRUE), by = \"grp\"] \r\n## would result in \r\n    grp  x roll3partial                                                                                                                                                                                                                                                                                                                                  \r\n 1:   A  1           instead of NA, => 1                                                                                                                                                                                                                                                                                                                                  \r\n 2:   A  2           instead of NA => (1+2)\/2 => 1.5                                                                                                                                                                                                                                                                                                                                  \r\n 3:   A  3            2                                                                                                                                                                                                                                                                                                                                  \r\n 4:   A  4            3                                                                                                                                                                                                                                                                                                                                  \r\n 5:   A  5            4                                                                                                                                                                                                                                                                                                                                  \r\n 6:   A  6            5                                                                                                                                                                                                                                                                                                                                  \r\n 7:   A  7            6                                                                                                                                                                                                                                                                                                                                  \r\n 8:   A  8            7                                                                                                                                                                                                                                                                                                                                  \r\n 9:   A  9            8                                                                                                                                                                                                                                                                                                                                  \r\n10:   A 10            9                                                                                                                                                                                                                                                                                                                                  \r\n11:   B  1           instead of NA, => 1                                                                                                                                                                                                                                                                                                                                  \r\n12:   B  2           instead of NA, 1.5                                                                                                                                                                                                                                                                                                                                  \r\n13:   B  3            2                                                                                                                                                                                                                                                                                                                                  \r\n14:   B  4            3                                                                                                                                                                                                                                                                                                                                  \r\n15:   B  5            4                                                                                                                                                                                                                                                                                                                                  \r\n16:   B  6            5                                                                                                                                                                                                                                                                                                                                  \r\n17:   B  7            6                                                                                                                                                                                                                                                                                                                                  \r\n18:   B  8            7                                                                                                                                                                                                                                                                                                                                  \r\n19:   B  9            8                                                                                                                                                                                                                                                                                                                                  \r\n20:   B 10            9 \r\n```\r\n\r\nGiven the current examples, it is not clear to me how this could be done currently using _adaptive=TRUE_","@aaelony-aeg if that might help, here is how you could do it using _adaptive=TRUE_.\r\n\r\n    dt <- data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\n     \r\n    n = 3\r\n    dt[,    frollmean(x, c(seq_len(min(n, .N)), rep(n, max(0, .N-n))), adaptive=TRUE), grp]      # works\r\n    dt[1,   frollmean(x, c(seq_len(min(n, .N)), rep(n, max(0, .N-n))), adaptive=TRUE), grp]      # works\r\n    dt[1:2, frollmean(x, c(seq_len(min(n, .N)), rep(n, max(0, .N-n))), adaptive=TRUE), grp]      # works\r\n\r\n","> Will this functionality be available by group as well?\r\n\r\n@aaelony-aeg Everything is available by group. If you mean if it will be optimized by group, then the answer is no.\r\n\r\nUsing dev branch this is what I am getting already.\r\n```r\r\ndt = data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\ndt[, roll3partial := frollmean(x, 3, partial=TRUE), by = \"grp\"][]\r\n```\r\n```\r\n       grp     x roll3partial\r\n    <char> <int>        <num>\r\n 1:      A     1          1.0\r\n 2:      A     2          1.5\r\n 3:      A     3          2.0\r\n 4:      A     4          3.0\r\n 5:      A     5          4.0\r\n 6:      A     6          5.0\r\n 7:      A     7          6.0\r\n 8:      A     8          7.0\r\n 9:      A     9          8.0\r\n10:      A    10          9.0\r\n11:      B     1          1.0\r\n12:      B     2          1.5\r\n13:      B     3          2.0\r\n14:      B     4          3.0\r\n15:      B     5          4.0\r\n16:      B     6          5.0\r\n17:      B     7          6.0\r\n18:      B     8          7.0\r\n19:      B     9          8.0\r\n20:      B    10          9.0\r\n       grp     x roll3partial\r\n```\r\n\r\nBecause `frollmean` is parallel for vectorized input, if you have more groups you may want to split it instead. Probably it won't be much faster because frollmean is already so fast.\r\n```r\r\ndt = data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\ndt[, roll3partial := unlist(frollmean(split(x, grp), 3, partial=TRUE))][]\r\n```\r\n```\r\n       grp     x roll3partial\r\n    <char> <int>        <num>\r\n 1:      A     1          1.0\r\n 2:      A     2          1.5\r\n 3:      A     3          2.0\r\n 4:      A     4          3.0\r\n 5:      A     5          4.0\r\n 6:      A     6          5.0\r\n 7:      A     7          6.0\r\n 8:      A     8          7.0\r\n 9:      A     9          8.0\r\n10:      A    10          9.0\r\n11:      B     1          1.0\r\n12:      B     2          1.5\r\n13:      B     3          2.0\r\n14:      B     4          3.0\r\n15:      B     5          4.0\r\n16:      B     6          5.0\r\n17:      B     7          6.0\r\n18:      B     8          7.0\r\n19:      B     9          8.0\r\n20:      B    10          9.0\r\n       grp     x roll3partial\r\n```\r\n\r\nNote that partial is subefficient, as it turns out whole computation into adaptive computation. If input is large and speed matters, better to run non-partial computation, and then only fill out missing observations at the front using partial=TRUE.\r\n@Kamgang-B is a good example, but could be simplified even more, for complete example see: https:\/\/github.com\/Rdatatable\/data.table\/blob\/a14b486e4978da65c469eb39f41c19018e91f846\/man\/froll.Rd#L136-L147","> > Will this functionality be available by group as well?\r\n> \r\n> @aaelony-aeg Everything is available by group. If you mean if it will be optimized by group, then the answer is no.\r\n> \r\n> Using dev branch this is what I am getting already.\r\n> \r\n> ```r\r\n> dt = data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\n> dt[, roll3partial := frollmean(x, 3, partial=TRUE), by = \"grp\"][]\r\n> ```\r\n> \r\n> ```\r\n>        grp     x roll3partial\r\n>     <char> <int>        <num>\r\n>  1:      A     1          1.0\r\n>  2:      A     2          1.5\r\n>  3:      A     3          2.0\r\n>  4:      A     4          3.0\r\n>  5:      A     5          4.0\r\n>  6:      A     6          5.0\r\n>  7:      A     7          6.0\r\n>  8:      A     8          7.0\r\n>  9:      A     9          8.0\r\n> 10:      A    10          9.0\r\n> 11:      B     1          1.0\r\n> 12:      B     2          1.5\r\n> 13:      B     3          2.0\r\n> 14:      B     4          3.0\r\n> 15:      B     5          4.0\r\n> 16:      B     6          5.0\r\n> 17:      B     7          6.0\r\n> 18:      B     8          7.0\r\n> 19:      B     9          8.0\r\n> 20:      B    10          9.0\r\n>        grp     x roll3partial\r\n> ```\r\n> \r\n> Because `frollmean` is parallel for vectorized input, if you have more groups you may want to split it instead. Probably it won't be much faster because frollmean is already so fast.\r\n> \r\n> ```r\r\n> dt = data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\n> dt[, roll3partial := unlist(frollmean(split(x, grp), 3, partial=TRUE))][]\r\n> ```\r\n> \r\n> ```\r\n>        grp     x roll3partial\r\n>     <char> <int>        <num>\r\n>  1:      A     1          1.0\r\n>  2:      A     2          1.5\r\n>  3:      A     3          2.0\r\n>  4:      A     4          3.0\r\n>  5:      A     5          4.0\r\n>  6:      A     6          5.0\r\n>  7:      A     7          6.0\r\n>  8:      A     8          7.0\r\n>  9:      A     9          8.0\r\n> 10:      A    10          9.0\r\n> 11:      B     1          1.0\r\n> 12:      B     2          1.5\r\n> 13:      B     3          2.0\r\n> 14:      B     4          3.0\r\n> 15:      B     5          4.0\r\n> 16:      B     6          5.0\r\n> 17:      B     7          6.0\r\n> 18:      B     8          7.0\r\n> 19:      B     9          8.0\r\n> 20:      B    10          9.0\r\n>        grp     x roll3partial\r\n> ```\r\n> \r\n> Note that partial is subefficient, as it turns out whole computation into adaptive computation. If input is large and speed matters, better to run non-partial computation, and then only fill out missing observations at the front using partial=TRUE. @Kamgang-B is a good example, but could be simplified even more, for complete example see:\r\n> \r\n> https:\/\/github.com\/Rdatatable\/data.table\/blob\/a14b486e4978da65c469eb39f41c19018e91f846\/man\/froll.Rd#L136-L147\r\n\r\nThat's excellent. I'll switch to the dev branch then!\r\nThank-you\r\n\r\n","> ```r\r\n> dt[, roll3partial := frollmean(x, 3, partial=TRUE), by = \"grp\"][]\r\n> ```\r\n\r\nOne more thing... Could there also be a convenience method to compute the actual N that went into the window if it was partial? \r\n\r\nsomething akin to:\r\n\r\n```\r\ndt[, N_in_roll3partial := ifelse(is.na(roll3strict)==FALSE, 3, NA)]\r\ndt[is.na(roll3strict)==TRUE, N_in_roll3partial := seq_len(.N), by = \"grp\"][]\r\n       grp     x roll3partial roll3strict N_in_roll3partial\r\n    <char> <int>        <num>       <num>             <num>\r\n 1:      A     1          1.0          NA                 1\r\n 2:      A     2          1.5          NA                 2\r\n 3:      A     3          2.0           2                 3\r\n 4:      A     4          3.0           3                 3\r\n 5:      A     5          4.0           4                 3\r\n 6:      A     6          5.0           5                 3\r\n 7:      A     7          6.0           6                 3\r\n 8:      A     8          7.0           7                 3\r\n 9:      A     9          8.0           8                 3\r\n10:      A    10          9.0           9                 3\r\n11:      B     1          1.0          NA                 1\r\n12:      B     2          1.5          NA                 2\r\n13:      B     3          2.0           2                 3\r\n14:      B     4          3.0           3                 3\r\n15:      B     5          4.0           4                 3\r\n16:      B     6          5.0           5                 3\r\n17:      B     7          6.0           6                 3\r\n18:      B     8          7.0           7                 3\r\n19:      B     9          8.0           8                 3\r\n20:      B    10          9.0           9                 3\r\n       grp     x roll3partial roll3strict N_in_roll3partial\r\n```\r\n\r\n","For adaptive=FALSE it is always n-1. As in presented example above.","> dt <- data.table(grp = c(rep(\"A\", 10), rep(\"B\", 10)), x = c(1:10, 1:10))\r\n>  \r\n> n = 3\r\n> dt[,    frollmean(x, c(seq_len(min(n, .N)), rep(n, max(0, .N-n))), adaptive=TRUE), grp]\r\n\r\nThank-you.  IMHO, the above should be added to the documentation. \r\n\r\n`an = function(n, len) c(seq.int(n), rep(n, len-n));\r\nn = an(3, nrow(d));\r\nfrollmean(d, n, adaptive=TRUE)\r\n`\r\nwas unclear to me, but `dt[,    frollmean(x, c(seq_len(min(n, .N)), rep(n, max(0, .N-n))), adaptive=TRUE), grp]` clicked.","I don't see it is better explained than what is already in this PR (pasted above), where user don't even have to touch adaptive=TRUE.\r\none call partial=FALSE, and second call partial=TRUE using subset `seq.int(n-1)`.","> I don't see it is better explained than what is already in this PR (pasted above), where user don't even have to touch adaptive=TRUE. one call partial=FALSE, and second call partial=TRUE using subset `seq.int(n-1)`.\r\n\r\nAgree.  `partial = TRUE` or `partial = FALSE` is clear to me on its face (without having to study the documentation, examples, or code).","Another thought that is admittedly _scope creep_ outside the initial topic of this issue; but would [EWMA](https:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/trading-investing\/exponentially-weighted-moving-average-ewma\/) approaches, e.g. [ewma.R](https:\/\/github.com\/cran\/qcc\/blob\/master\/R\/ewma.R) be something to include while on the topic of enhancing rolling window functions in `data.table`?  Perhaps an option to _weight_ older observations might be something to consider?  (I can open\/create a separate issue if desirable)\r\n","Yes, please do. Providing minimal example and desired output.","\r\n\r\n\r\n\r\n> Yes, please do. Providing minimal example and desired output.\r\n\r\nI think the following may be sufficient taking guidance from [stackoverflow](https:\/\/stackoverflow.com\/questions\/65760795\/replicate-ewm-pandas-function-in-r) which mentions this [gist](https:\/\/gist.github.com\/assuncaolfi\/5581528021ac75247a4a1f1c0c3fe12f) .\r\n\r\nso, if I didn't goof converting `alpha` as an arg to `span` as an arg, following the [pandas ewm documentation](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.ewm.html):\r\n\r\n```\r\n\r\newma <- function(x, span) {\r\n  n     <- length(x)\r\n  alpha <- 2 \/ (span + 1)\r\n  sapply(\r\n    1:n,\r\n    function(i, x, alpha) {\r\n      y <- x[1:i]\r\n      m <- length(y)\r\n      weights <- (1 - alpha)^((m - 1):0)\r\n      ewma <- sum(weights * y) \/ sum(weights)\r\n    },\r\n    x = x,\r\n    alpha = alpha\r\n  )\r\n}\r\n\r\newmsd <- function(x, span) {\r\n  n <- length(x)\r\n  alpha <- 2 \/ (span + 1)\r\n  sapply(\r\n    1:n,\r\n    function(i, x, alpha) {\r\n      y <- x[1:i]\r\n      m <- length(y)\r\n      weights <- (1 - alpha)^((m - 1):0)\r\n      ewma <- sum(weights * y) \/ sum(weights)\r\n      bias <- sum(weights)^2 \/ (sum(weights)^2 - sum(weights^2))\r\n      ewmsd <- sqrt(bias * sum(weights * (y - ewma)^2) \/ sum(weights))\r\n    },\r\n    x = x,\r\n    alpha = alpha\r\n  )\r\n}\r\n\r\n\r\ndt <- fread(\"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/daily-total-female-births.csv\")\r\nnames(dt) <- tolower(names(dt))\r\ndt[, emwa_span_9 := ewma(births, span = 9)]\r\n```\r\n```\r\n          date births emwa_span_9\r\n  1: 1959-01-01     35    35.00000\r\n  2: 1959-01-02     32    33.33333\r\n  3: 1959-01-03     30    31.96721\r\n  4: 1959-01-04     31    31.63957\r\n  5: 1959-01-05     44    35.31652\r\n ---                              \r\n361: 1959-12-27     37    39.88377\r\n362: 1959-12-28     52    42.30702\r\n363: 1959-12-29     48    43.44561\r\n364: 1959-12-30     55    45.75649\r\n365: 1959-12-31     50    46.60519\r\n```\r\n","By yes, I meant, please fill out new issue with all details. This issue is unrelated to your question and it will be difficult to read by having off-topic discussion here.","> By yes, I meant, please fill out new issue with all details. This issue is unrelated to your question and it will be difficult to read by having off-topic discussion here.\r\n\r\n[done](https:\/\/github.com\/Rdatatable\/data.table\/issues\/5485).","Just a note to say, `partial=TRUE` is a great feature!\r\n\r\nWould be greatly appreciated if `partial=TRUE` functionality might be merged into master.","Exact diff before merging to master:\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/compare\/c4a2085e35689a108d67dacb2f8261e4964d7e12..a14b486e4978da65c469eb39f41c19018e91f846","I guess I'm already much too late but shouldn't `frollmin(x)` be the same as `-frollmax(-x)`?","> I guess I'm already much too late but shouldn't `frollmin(x)` be the same as `-frollmax(-x)`?\r\n\r\nVery good observation, I briefly checked how that would behave for +Inf\/-Inf and looks to work as well. Ultimately changing sign is a cost of two in-memory copies:\r\n```r\r\nx = rnorm(1e8)\r\nsystem.time(a1<- -frollmax(-x, 1e3))\r\n#   user  system elapsed \r\n#  0.934   0.514   1.451 \r\nsystem.time(a2<- frollmin(x, 1e3))\r\n#   user  system elapsed \r\n#  0.308   0.159   0.468 \r\nall.equal(a1,a2)\r\n#[1] TRUE\r\n```\r\nWe could make it only one time copy by changing sign in-place for a result object from rolling function, then we will save some part of the time lost here, but for the input object, I would prefer not to touch it in-place.","> We could make it only one time copy by changing sign in-place for a result object from rolling function, then we will save some part of the time lost here, but for the input object, I would prefer not to touch it in-place.\r\n\r\nInteresting that negating takes so much time. I wrote a simple negation function in R.\r\n```r\r\nx = rnorm(1e8)\r\nf = function(x) x\r\nsystem.time(a1<- -f(-x))\r\n#   user  system elapsed \r\n#  0.374   0.243   0.618 \r\nsystem.time(a2<- negate(f(negate(x, copy=TRUE)), copy=FALSE))\r\n#   user  system elapsed \r\n#  0.294   0.208   0.503 \r\n\r\nidentical(a1, a2)\r\n#[1] TRUE\r\n\r\n# also parallelized it with openmp\r\nsystem.time(a3<- negate_omp(f(negate_omp(x, copy=TRUE)), copy=FALSE))\r\n#   user  system elapsed \r\n#  0.688   0.288   0.172 \r\n\r\nidentical(a1, a3)\r\n#[1] TRUE\r\n```\r\n\r\nSo I guess the extra running time is worth the hassle of maintaining two very similar functions."],"labels":["froll"]},{"title":"fwrite ISO datetime64 without Z-zone marker","body":"Hi! There is a need to fwrite datetime64 in ISO format without Z at the end of the date.\r\n\r\nTell me, is there a way to ask data.table to record datetime without a timezone marker?\r\n\r\nIf not, please add...","comments":["Gentlemen, the question is quite important!\r\n\r\nI have in the dttm table for 2022.08.29T19.08.07. In the database I have already set the timezone I am interested in. And now when I put this file to sleep, I expect to get the same 2022.08.29T19.08.07, and not 2022.08.29T23.08.07 or from another time.\r\n\r\nThis Z - is very much obstruct! And it is deleted from the file for a very long time through a linux request. It's easier to write without it right away. Is it difficult to add such a parameter (or tell how to remove Z if it is already possible)?","try the dateTimeAs parameter","Yes, I need \"ISO\" without Z. And I don't see any settings in this parameter to exclude the letter Z from iSO!\r\n\r\nHow do I pass the condition in the parameter - do not write the letter Z at the end? (then the date will still remain in ISO format)","if you're asking for the timestamp to be written with a T but not Z, you'll have to point to some external standards. T\/Z are part of ISO8601:\r\n\r\nhttps:\/\/en.m.wikipedia.org\/wiki\/ISO_8601\r\n\r\nexcluding the Z would not conform to any standard I know of. I recommend you use strftime to convert to character manually to accomplish your goal."],"labels":["fwrite"]},{"title":"Adaptive, left-aligned rolling max","body":"I guess `frollmax` would be ideal here. \r\n\r\nI would like to perform a rolling max using an (adaptive) non-equi join in `data.table`. Basing myself on [this answer](https:\/\/stackoverflow.com\/a\/68978808\/5224236), I think I'm getting close, but not quite there.\r\n\r\nFor all rows of `ex` I need the `max(value[row:end_window])`\r\n\r\n    > ex\r\n                        time    value end_window row\r\n      1: 2022-03-14 08:20:02 13344.77        540   1\r\n      2: 2022-03-14 08:20:02 13343.52        541   2\r\n      3: 2022-03-14 08:20:03 13342.27        547   3\r\n      4: 2022-03-14 08:20:04 13343.27        541   4\r\n      5: 2022-03-14 08:20:04 13343.02        541   5\r\n\r\nI tried calculating `rollmax` like so:\r\n\r\n    res <-ex[ex,.(value,row,end_window),on=.(row>=row,row<=end_window)][\r\n      ,.(rollmax = max(value), end_window),by=.(row)]\r\n\r\nbut it isn't exactly what I expect. For `row 1` it is correct because `max(ex$value[1:540])` is indeed `13361.77`. \r\n\r\n    res\r\n       row  rollmax end_window\r\n    1:   1 13361.77        540\r\n    2:   1 13361.77        541\r\n    3:   1 13361.77        547\r\n    4:   1 13361.77        541\r\n\r\nHowever, for row 596 this is not the case:\r\n\r\n    > res <- res[order(row)]\r\n    > res[596]\r\n       row end_window  rollmax end_window row\r\n    1: 596        622 13327.42        622 596\r\n    > max(ex$value[596:622])\r\n    [1] 13328.67\r\n\r\nAny idea?\r\n\r\nData:\r\n\r\n```\r\n\r\n> dput(ex)\r\nstructure(list(time = structure(c(1647246002.24923, 1647246002.79761, \r\n1647246003.33618, 1647246004.39149, 1647246004.44229, 1647246005.04222, \r\n1647246005.61024, 1647246006.19858, 1647246006.74267, 1647246007.28133, \r\n1647246007.82673, 1647246007.86367, 1647246008.44411, 1647246008.98759, \r\n1647246009.02739, 1647246010.60053, 1647246011.15027, 1647246011.17698, \r\n1647246011.72366, 1647246012.79303, 1647246013.33196, 1647246013.87512, \r\n1647246015.96322, 1647246015.99679, 1647246016.53583, 1647246017.08083, \r\n1647246017.6256, 1647246018.17829, 1647246018.71606, 1647246019.25767, \r\n1647246019.79644, 1647246020.33468, 1647246020.8824, 1647246021.94552, \r\n1647246022.01591, 1647246023.6453, 1647246024.18459, 1647246024.72267, \r\n1647246025.26541, 1647246025.8108, 1647246026.36594, 1647246027.00413, \r\n1647246027.5502, 1647246028.08886, 1647246028.12655, 1647246028.19781, \r\n1647246029.25628, 1647246029.7948, 1647246029.82717, 1647246029.88239, \r\n1647246030.94222, 1647246030.98792, 1647246031.02387, 1647246031.56251, \r\n1647246032.10701, 1647246033.68004, 1647246034.21849, 1647246034.75698, \r\n1647246034.79206, 1647246035.33537, 1647246035.88073, 1647246036.41933, \r\n1647246037.47445, 1647246038.02743, 1647246038.56629, 1647246039.11172, \r\n1647246039.69961, 1647246040.24075, 1647246040.78262, 1647246040.81593, \r\n1647246040.84878, 1647246040.89548, 1647246041.44543, 1647246043.02019, \r\n1647246043.56719, 1647246044.11148, 1647246045.17907, 1647246045.73629, \r\n1647246046.2821, 1647246047.33841, 1647246048.92821, 1647246049.47257, \r\n1647246050.01503, 1647246050.04984, 1647246050.58807, 1647246051.13286, \r\n1647246051.6825, 1647246052.7956, 1647246053.49162, 1647246054.03274, \r\n1647246054.57456, 1647246055.632, 1647246055.66697, 1647246055.70239, \r\n1647246056.25411, 1647246056.79885, 1647246057.87931, 1647246058.42802, \r\n1647246060.52577, 1647246061.07338, 1647246061.61201, 1647246062.66817, \r\n1647246063.20656, 1647246063.76296, 1647246064.31383, 1647246064.87371, \r\n1647246065.41859, 1647246066.47316, 1647246067.52794, 1647246068.11098, \r\n1647246069.16574, 1647246069.7041, 1647246070.2422, 1647246070.27887, \r\n1647246070.31238, 1647246070.85178, 1647246071.4025, 1647246071.94053, \r\n1647246072.47854, 1647246073.02985, 1647246073.58003, 1647246074.63929, \r\n1647246075.18196, 1647246075.72753, 1647246076.7952, 1647246077.33347, \r\n1647246077.87198, 1647246078.41077, 1647246078.53224, 1647246079.11493, \r\n1647246079.14671, 1647246079.69738, 1647246080.23599, 1647246080.77401, \r\n1647246081.83168, 1647246082.89821, 1647246082.94748, 1647246083.50207, \r\n1647246084.56697, 1647246085.11225, 1647246085.14328, 1647246085.68186, \r\n1647246086.22483, 1647246086.76414, 1647246087.30244, 1647246087.84414, \r\n1647246087.90234, 1647246088.51317, 1647246089.06105, 1647246089.09262, \r\n1647246089.12905, 1647246089.66859, 1647246091.2575, 1647246091.80716, \r\n1647246092.34612, 1647246093.40056, 1647246093.43814, 1647246093.49506, \r\n1647246094.0503, 1647246094.59483, 1647246095.13323, 1647246095.67456, \r\n1647246096.21326, 1647246097.26845, 1647246097.80666, 1647246098.35339, \r\n1647246098.38786, 1647246098.92667, 1647246099.46508, 1647246099.49848, \r\n1647246100.03814, 1647246100.57699, 1647246101.1153, 1647246102.69753, \r\n1647246102.73281, 1647246103.30528, 1647246103.84384, 1647246104.38213, \r\n1647246104.92884, 1647246105.46755, 1647246106.53496, 1647246107.08357, \r\n1647246107.6341, 1647246108.18945, 1647246108.72779, 1647246109.27579, \r\n1647246109.30699, 1647246109.8461, 1647246111.42362, 1647246112.49641, \r\n1647246113.04461, 1647246113.08354, 1647246114.14973, 1647246114.68916, \r\n1647246115.74509, 1647246116.2965, 1647246117.35259, 1647246117.89085, \r\n1647246118.43221, 1647246118.98784, 1647246120.04347, 1647246120.58226, \r\n1647246121.11993, 1647246121.15373, 1647246121.69873, 1647246122.24513, \r\n1647246122.78392, 1647246122.82012, 1647246123.35951, 1647246124.43122, \r\n1647246124.97043, 1647246125.00463, 1647246126.06283, 1647246126.60107, \r\n1647246127.139, 1647246128.1969, 1647246128.7363, 1647246129.28432, \r\n1647246129.83505, 1647246130.38045, 1647246130.91879, 1647246131.45954, \r\n1647246131.99771, 1647246132.53656, 1647246133.60522, 1647246134.14361, \r\n1647246134.68677, 1647246135.76064, 1647246135.79411, 1647246136.33365, \r\n1647246136.87596, 1647246137.41375, 1647246138.46839, 1647246140.04516, \r\n1647246140.5832, 1647246141.12186, 1647246141.66057, 1647246141.69597, \r\n1647246142.24788, 1647246142.78726, 1647246143.32551, 1647246143.86371, \r\n1647246144.40198, 1647246144.9439, 1647246145.99847, 1647246147.05978, \r\n1647246147.59852, 1647246148.65275, 1647246149.19058, 1647246149.22625, \r\n1647246149.77296, 1647246150.92239, 1647246151.98347, 1647246152.52875, \r\n1647246153.59316, 1647246154.1348, 1647246154.69452, 1647246155.74972, \r\n1647246156.80442, 1647246157.34306, 1647246157.88178, 1647246158.93975, \r\n1647246158.97796, 1647246159.52411, 1647246160.06207, 1647246160.60031, \r\n1647246161.14835, 1647246161.737, 1647246162.27981, 1647246163.33446, \r\n1647246163.87274, 1647246163.90699, 1647246164.4459, 1647246164.984, \r\n1647246165.02719, 1647246165.08346, 1647246165.6332, 1647246166.68854, \r\n1647246167.22671, 1647246167.76482, 1647246168.31278, 1647246168.85157, \r\n1647246169.90642, 1647246169.94019, 1647246170.47815, 1647246171.02075, \r\n1647246171.12008, 1647246171.19541, 1647246171.73418, 1647246172.27306, \r\n1647246173.32804, 1647246174.3931, 1647246174.9318, 1647246175.46992, \r\n1647246176.00905, 1647246176.04586, 1647246176.59036, 1647246177.12817, \r\n1647246177.6664, 1647246178.20724, 1647246180.29625, 1647246180.84686, \r\n1647246181.39772, 1647246181.93643, 1647246182.99156, 1647246183.53319, \r\n1647246184.58817, 1647246185.12657, 1647246186.19437, 1647246186.22632, \r\n1647246186.79265, 1647246187.84722, 1647246188.38569, 1647246189.45762, \r\n1647246190.0031, 1647246190.55265, 1647246191.09596, 1647246191.13081, \r\n1647246191.71897, 1647246192.30443, 1647246192.33833, 1647246192.87677, \r\n1647246193.41573, 1647246193.95388, 1647246194.49243, 1647246195.03318, \r\n1647246195.58616, 1647246195.629, 1647246195.68133, 1647246196.7452, \r\n1647246196.78274, 1647246197.32815, 1647246198.383, 1647246198.92212, \r\n1647246199.47407, 1647246200.01366, 1647246201.09431, 1647246201.28339, \r\n1647246201.83275, 1647246201.88873, 1647246202.42749, 1647246202.96512, \r\n1647246203.50335, 1647246203.53717, 1647246204.60806, 1647246205.66341, \r\n1647246207.76851, 1647246208.31093, 1647246208.85274, 1647246209.39086, \r\n1647246209.92895, 1647246210.46752, 1647246211.00958, 1647246211.58626, \r\n1647246212.13684, 1647246212.6751, 1647246213.23354, 1647246213.77225, \r\n1647246214.31059, 1647246214.85187, 1647246215.39784, 1647246215.93649, \r\n1647246216.48652, 1647246216.51868, 1647246217.06782, 1647246217.61658, \r\n1647246218.15583, 1647246218.69413, 1647246219.2327, 1647246219.78182, \r\n1647246219.81398, 1647246219.87332, 1647246220.41379, 1647246220.99915, \r\n1647246221.54392, 1647246221.57975, 1647246222.13378, 1647246222.6731, \r\n1647246223.2105, 1647246223.74841, 1647246224.28625, 1647246225.34108, \r\n1647246225.37343, 1647246225.92474, 1647246226.46305, 1647246227.00108, \r\n1647246228.57557, 1647246230.68348, 1647246232.2717, 1647246232.42394, \r\n1647246232.48924, 1647246233.5488, 1647246234.60351, 1647246235.65857, \r\n1647246236.19704, 1647246236.73508, 1647246237.28333, 1647246237.82224, \r\n1647246238.36069, 1647246239.41507, 1647246239.44798, 1647246241.04188, \r\n1647246241.08999, 1647246241.63496, 1647246242.17391, 1647246242.71288, \r\n1647246243.27745, 1647246243.8328, 1647246244.37185, 1647246244.91051, \r\n1647246246.48473, 1647246247.03573, 1647246247.57391, 1647246248.11399, \r\n1647246248.65179, 1647246249.18901, 1647246249.72819, 1647246250.26673, \r\n1647246252.36213, 1647246252.90016, 1647246253.44222, 1647246253.97995, \r\n1647246254.55197, 1647246255.14833, 1647246255.18025, 1647246256.24064, \r\n1647246256.77879, 1647246257.3168, 1647246257.8549, 1647246259.42901, \r\n1647246259.96739, 1647246260.51821, 1647246261.06769, 1647246261.60631, \r\n1647246262.14409, 1647246262.68199, 1647246263.7364, 1647246264.27817, \r\n1647246264.8161, 1647246264.8493, 1647246265.38822, 1647246266.02156, \r\n1647246267.08953, 1647246267.13244, 1647246267.70627, 1647246268.2439, \r\n1647246269.30283, 1647246269.84823, 1647246270.38616, 1647246270.92416, \r\n1647246271.46258, 1647246272.01895, 1647246272.05282, 1647246273.11308, \r\n1647246273.65094, 1647246274.19252, 1647246274.22361, 1647246274.76141, \r\n1647246275.3023, 1647246275.90684, 1647246276.45273, 1647246276.48599, \r\n1647246277.0393, 1647246277.5776, 1647246278.64335, 1647246279.18532, \r\n1647246280.23975, 1647246280.7782, 1647246281.31639, 1647246281.85913, \r\n1647246282.39697, 1647246282.96604, 1647246282.99761, 1647246283.53741, \r\n1647246284.08358, 1647246284.62939, 1647246285.17108, 1647246285.70896, \r\n1647246286.31339, 1647246286.85253, 1647246287.90696, 1647246288.45875, \r\n1647246289.00068, 1647246290.58259, 1647246291.12135, 1647246291.65992, \r\n1647246292.20938, 1647246292.7532, 1647246293.29224, 1647246293.31763, \r\n1647246293.38285, 1647246294.45661, 1647246295.00258, 1647246296.05955, \r\n1647246296.08681, 1647246296.68747, 1647246297.22569, 1647246297.76365, \r\n1647246297.79615, 1647246298.33488, 1647246298.33488, 1647246299.26168, \r\n1647246300.3193, 1647246301.37476, 1647246301.9131, 1647246302.96969, \r\n1647246304.02832, 1647246304.57973, 1647246305.11874, 1647246305.66082, \r\n1647246306.19956, 1647246307.78313, 1647246308.39276, 1647246309.46614, \r\n1647246311.04522, 1647246311.59425, 1647246312.64913, 1647246313.20936, \r\n1647246313.75224, 1647246314.82892, 1647246315.37281, 1647246315.92568, \r\n1647246316.48708, 1647246317.03067, 1647246317.58859, 1647246318.64671, \r\n1647246319.19176, 1647246319.75091, 1647246320.31055, 1647246321.36516, \r\n1647246322.41996, 1647246322.96025, 1647246324.53452, 1647246325.58951, \r\n1647246326.13133, 1647246326.68027, 1647246327.22395, 1647246327.76302, \r\n1647246328.30463, 1647246328.84338, 1647246329.38136, 1647246329.93062, \r\n1647246329.97819, 1647246330.52056, 1647246331.06359, 1647246332.12599, \r\n1647246332.66487, 1647246333.20322, 1647246333.75134, 1647246334.30892, \r\n1647246334.85475, 1647246335.39292, 1647246335.93137, 1647246336.48062, \r\n1647246337.04778, 1647246337.6012, 1647246338.14782, 1647246338.69871, \r\n1647246338.738, 1647246339.29972, 1647246339.84796, 1647246340.38704, \r\n1647246340.93951, 1647246340.97152, 1647246341.5138, 1647246342.59254, \r\n1647246343.133, 1647246343.67222, 1647246344.72683, 1647246345.2765, \r\n1647246345.83496, 1647246346.37337, 1647246346.91159, 1647246347.96947, \r\n1647246348.5073, 1647246349.04528, 1647246349.07814, 1647246349.62146, \r\n1647246350.68969, 1647246351.35044, 1647246351.88839, 1647246352.44432, \r\n1647246352.98305, 1647246353.01189, 1647246353.55901, 1647246354.10541, \r\n1647246354.64892, 1647246355.18692, 1647246355.72845, 1647246356.79225, \r\n1647246357.84731, 1647246358.38574, 1647246358.94398, 1647246358.99016, \r\n1647246359.02776, 1647246360.60906, 1647246361.15149, 1647246361.69722, \r\n1647246362.23563, 1647246363.29111, 1647246363.82956, 1647246364.38134, \r\n1647246364.91961, 1647246365.4643, 1647246366.00604, 1647246366.54683, \r\n1647246367.10826, 1647246368.17288, 1647246368.71083, 1647246369.76524, \r\n1647246370.32348, 1647246370.35208, 1647246370.98812, 1647246371.09929, \r\n1647246371.13341, 1647246371.71661, 1647246372.25956, 1647246372.80556, \r\n1647246373.34373, 1647246373.88164, 1647246374.41965, 1647246375.99117\r\n), class = c(\"POSIXct\", \"POSIXt\"), tzone = \"UTC\"), value = c(13344.77, \r\n13343.52, 13342.27, 13343.27, 13343.02, 13343.27, 13343.27, 13342.27, \r\n13344.77, 13345.02, 13344.52, 13345.52, 13345.02, 13345.05, 13345.52, \r\n13345.27, 13345.52, 13345.77, 13346.27, 13346.52, 13345.52, 13345.02, \r\n13345.77, 13346.02, 13345.52, 13346.02, 13347.77, 13347.02, 13345.77, \r\n13346.77, 13347.27, 13347.52, 13349.02, 13349.02, 13349.27, 13350.77, \r\n13350.52, 13351.02, 13348.77, 13348.27, 13348.77, 13349.52, 13349.27, \r\n13348.52, 13347.77, 13348.27, 13348.77, 13350.77, 13350.27, 13351.27, \r\n13351.52, 13352.02, 13351.77, 13352.02, 13351.52, 13350.52, 13351.02, \r\n13351.52, 13351.77, 13351.27, 13351.52, 13351.27, 13351.27, 13351.77, \r\n13350.77, 13351.52, 13351.77, 13352.52, 13353.52, 13353.77, 13354.27, \r\n13355.77, 13356.27, 13355.52, 13354.02, 13353.77, 13353.52, 13355.27, \r\n13355.52, 13356.27, 13357.02, 13358.77, 13358.52, 13358.02, 13358.27, \r\n13358.02, 13357.77, 13355.77, 13356.02, 13356.27, 13356.02, 13356.27, \r\n13356.52, 13356.05, 13356.52, 13357.02, 13357.52, 13358.77, 13358.52, \r\n13357.77, 13358.27, 13358.02, 13359.77, 13360.27, 13360.52, 13359.02, \r\n13359.52, 13360.27, 13360.52, 13360.27, 13360.52, 13361.02, 13361.27, \r\n13361.52, 13361.77, 13360.27, 13360.02, 13359.52, 13359.27, 13358.52, \r\n13359.77, 13360.02, 13360.27, 13360.02, 13360.27, 13360.52, 13359.77, \r\n13360.02, 13359.52, 13359.27, 13358.77, 13359.77, 13359.52, 13359.27, \r\n13360.02, 13359.02, 13358.77, 13358.27, 13359.02, 13359.27, 13359.77, \r\n13360.27, 13359.77, 13359.52, 13359.02, 13358.52, 13359.02, 13359.52, \r\n13359.77, 13359.52, 13359.27, 13359.77, 13359.27, 13358.52, 13358.27, \r\n13356.02, 13356.02, 13356.02, 13357.27, 13359.27, 13359.02, 13359.52, \r\n13357.77, 13357.27, 13357.02, 13357.27, 13357.02, 13356.77, 13357.02, \r\n13356.02, 13355.02, 13355.77, 13356.27, 13357.27, 13357.52, 13357.27, \r\n13356.27, 13356.02, 13354.77, 13355.02, 13353.77, 13353.27, 13352.02, \r\n13350.52, 13350.02, 13349.77, 13350.02, 13350.77, 13348.77, 13347.77, \r\n13348.02, 13347.77, 13347.52, 13347.02, 13344.52, 13343.52, 13344.52, \r\n13344.27, 13344.02, 13343.52, 13343.77, 13344.02, 13344.52, 13345.02, \r\n13344.77, 13343.77, 13344.02, 13344.27, 13344.52, 13344.27, 13344.02, \r\n13343.77, 13344.02, 13344.27, 13343.77, 13344.02, 13343.27, 13343.02, \r\n13342.77, 13342.52, 13342.77, 13343.02, 13342.77, 13343.27, 13343.77, \r\n13342.27, 13342.52, 13342.27, 13342.52, 13341.77, 13342.02, 13341.77, \r\n13339.77, 13340.27, 13340.52, 13340.77, 13341.27, 13340.77, 13341.27, \r\n13341.02, 13341.52, 13341.77, 13342.27, 13341.77, 13342.27, 13342.52, \r\n13341.77, 13341.52, 13340.77, 13341.27, 13340.52, 13341.52, 13341.42, \r\n13343.42, 13343.02, 13341.92, 13340.67, 13341.17, 13342.42, 13343.52, \r\n13344.77, 13344.27, 13344.52, 13344.67, 13345.27, 13345.17, 13343.27, \r\n13342.92, 13342.42, 13343.27, 13344.17, 13343.92, 13342.67, 13343.02, \r\n13343.27, 13343.02, 13343.77, 13344.27, 13344.77, 13345.52, 13345.27, \r\n13346.02, 13345.92, 13346.02, 13345.17, 13345.42, 13345.17, 13345.27, \r\n13345.92, 13346.92, 13346.52, 13346.17, 13345.42, 13347.52, 13346.42, \r\n13346.52, 13347.42, 13346.67, 13346.77, 13347.77, 13349.52, 13350.77, \r\n13351.17, 13351.27, 13350.92, 13349.42, 13349.17, 13348.77, 13347.77, \r\n13347.42, 13348.27, 13347.92, 13348.27, 13348.02, 13347.02, 13346.17, \r\n13346.42, 13346.52, 13347.17, 13348.27, 13347.55, 13348.42, 13348.52, \r\n13348.92, 13347.77, 13347.67, 13347.92, 13347.17, 13347.02, 13347.17, \r\n13347.17, 13346.17, 13347.27, 13347.02, 13346.52, 13346.77, 13346.27, \r\n13345.17, 13344.92, 13345.02, 13343.92, 13344.27, 13344.42, 13344.27, \r\n13345.27, 13344.17, 13344.92, 13344.42, 13344.27, 13343.42, 13343.92, \r\n13342.77, 13340.92, 13340.67, 13341.42, 13340.92, 13341.27, 13340.8, \r\n13341.92, 13342.02, 13341.77, 13341.27, 13342.27, 13342.02, 13341.77, \r\n13342.52, 13343.02, 13342.52, 13342.27, 13341.77, 13342.02, 13341.77, \r\n13342.52, 13342.77, 13343.02, 13342.77, 13344.77, 13344.02, 13343.77, \r\n13345.02, 13345.52, 13344.27, 13344.52, 13345.77, 13346.52, 13346.02, \r\n13346.52, 13347.02, 13348.27, 13347.55, 13348.02, 13347.52, 13346.77, \r\n13346.52, 13344.02, 13343.27, 13342.27, 13342.02, 13342.52, 13342.27, \r\n13342.52, 13343.27, 13343.77, 13344.02, 13344.77, 13344.02, 13344.52, \r\n13343.02, 13344.02, 13344.27, 13344.27, 13345.27, 13344.52, 13344.77, \r\n13345.52, 13345.02, 13345.52, 13344.52, 13345.77, 13345.02, 13345.52, \r\n13345.27, 13345.02, 13345.52, 13345.27, 13345.02, 13344.52, 13344.02, \r\n13344.27, 13344.02, 13344.02, 13343.77, 13346.52, 13346.02, 13346.77, \r\n13347.52, 13347.27, 13347.02, 13346.52, 13346.27, 13346.77, 13346.27, \r\n13346.52, 13346.27, 13345.27, 13346.77, 13347.77, 13347.27, 13346.77, \r\n13347.27, 13347.02, 13348.52, 13348.67, 13348.17, 13346.67, 13345.42, \r\n13345.67, 13345.02, 13344.92, 13345.42, 13345.27, 13345.42, 13345.02, \r\n13344.42, 13344.52, 13344.92, 13345.27, 13344.92, 13344.17, 13343.92, \r\n13343.92, 13344.17, 13344.52, 13346.02, 13345.92, 13345.67, 13345.52, \r\n13345.42, 13345.67, 13346.17, 13345.67, 13346.17, 13346.67, 13348.05, \r\n13348.67, 13347.42, 13347.77, 13347.67, 13348.52, 13348.42, 13348.17, \r\n13348.27, 13348.92, 13348.67, 13348.52, 13349.27, 13349.17, 13348.77, \r\n13348.67, 13348.92, 13348.92, 13348.42, 13348.17, 13349.27, 13349.67, \r\n13348.27, 13347.77, 13347.92, 13348.77, 13350.17, 13351.02, 13350.67, \r\n13350.77, 13350.42, 13350.27, 13349.92, 13350.02, 13347.67, 13348.17, \r\n13346.02, 13345.42, 13345.52, 13344.92, 13345.17, 13346.02, 13346.17, \r\n13346.02, 13344.42, 13344.67, 13344.52, 13343.27, 13344.02, 13343.27, \r\n13343.02, 13341.77, 13340.02, 13341.02, 13341.27, 13339.77, 13339.27, \r\n13337.77, 13337.42, 13339.17, 13338.52, 13338.02, 13337.27, 13335.52, \r\n13334.27, 13334.62, 13334.27, 13334.37, 13333.77, 13333.12, 13332.92, \r\n13333.42, 13331.42, 13329.92, 13329.92, 13330.27, 13329.67, 13330.27, \r\n13329.77, 13331.02, 13331.27, 13332.02, 13330.77, 13332.02, 13331.52, \r\n13331.02, 13332.02, 13332.52, 13331.77, 13332.27, 13329.52, 13329.77, \r\n13329.52, 13329.27, 13329.02, 13329.52, 13329.77, 13330.02, 13329.77, \r\n13329.02, 13329.27, 13329.52, 13328.77, 13329.02, 13329.67, 13330.17, \r\n13329.02, 13329.52, 13329.02, 13328.17, 13329.02, 13328.67, 13327.42, \r\n13327.77, 13328.67, 13327.42, 13327.17, 13327.42, 13326.77, 13327.92, \r\n13327.52, 13326.92, 13326.92, 13327.42, 13325.92, 13326.42, 13325.42, \r\n13323.17, 13323.67, 13324.42, 13324.27, 13323.55, 13324.92, 13324.77, \r\n13324.02, 13323.17, 13323.02, 13322.52, 13322.27), end_window = c(540L, \r\n541L, 547L, 541L, 541L, 541L, 541L, 547L, 540L, 233L, 540L, 233L, \r\n233L, 233L, 233L, 233L, 233L, 233L, 233L, 233L, 233L, 233L, 233L, \r\n233L, 233L, 233L, 220L, 230L, 233L, 233L, 230L, 226L, 196L, 196L, \r\n196L, 195L, 195L, 195L, 196L, 218L, 196L, 196L, 196L, 217L, 220L, \r\n218L, 196L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, \r\n195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, \r\n194L, 190L, 190L, 189L, 184L, 184L, 185L, 189L, 190L, 190L, 185L, \r\n185L, 184L, 184L, 182L, 182L, 183L, 183L, 183L, 183L, 184L, 184L, \r\n184L, 184L, 184L, 184L, 184L, 184L, 184L, 183L, 182L, 182L, 183L, \r\n183L, 183L, 181L, 171L, 171L, 181L, 181L, 171L, 171L, 171L, 171L, \r\n171L, 156L, 156L, 156L, 171L, 179L, 181L, 181L, 182L, 181L, 179L, \r\n171L, 179L, 171L, 171L, 181L, 179L, 181L, 181L, 182L, 181L, 181L, \r\n181L, 179L, 181L, 182L, 183L, 181L, 181L, 181L, 171L, 181L, 181L, \r\n181L, 182L, 181L, 181L, 181L, 181L, 181L, 181L, 181L, 182L, 183L, \r\n184L, 184L, 184L, 183L, 181L, 181L, 181L, 183L, 183L, 184L, 183L, \r\n184L, 184L, 184L, 184L, 186L, 184L, 184L, 183L, 183L, 183L, 184L, \r\n184L, 189L, 186L, 190L, 190L, 195L, 195L, 195L, 195L, 195L, 195L, \r\n196L, 220L, 219L, 220L, 226L, 230L, 540L, 541L, 540L, 541L, 541L, \r\n541L, 541L, 541L, 540L, 233L, 540L, 541L, 541L, 541L, 540L, 541L, \r\n541L, 541L, 541L, 541L, 541L, 541L, 541L, 541L, 542L, 542L, 542L, \r\n541L, 542L, 541L, 541L, 547L, 542L, 547L, 542L, 547L, 547L, 547L, \r\n548L, 548L, 548L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, \r\n547L, 547L, 542L, 547L, 547L, 547L, 547L, 548L, 547L, 547L, 541L, \r\n541L, 547L, 547L, 547L, 546L, 541L, 540L, 541L, 540L, 540L, 536L, \r\n536L, 541L, 541L, 546L, 541L, 541L, 541L, 542L, 541L, 541L, 541L, \r\n541L, 541L, 540L, 536L, 536L, 353L, 354L, 353L, 536L, 536L, 536L, \r\n536L, 354L, 353L, 353L, 353L, 536L, 353L, 353L, 353L, 353L, 353L, \r\n353L, 353L, 341L, 338L, 338L, 316L, 338L, 341L, 341L, 350L, 353L, \r\n353L, 352L, 352L, 352L, 352L, 353L, 353L, 353L, 353L, 353L, 352L, \r\n353L, 352L, 350L, 350L, 353L, 353L, 352L, 353L, 353L, 353L, 353L, \r\n353L, 353L, 353L, 353L, 353L, 353L, 536L, 539L, 539L, 541L, 541L, \r\n540L, 541L, 536L, 541L, 539L, 540L, 541L, 541L, 541L, 542L, 547L, \r\n547L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, 547L, \r\n547L, 542L, 541L, 542L, 547L, 547L, 547L, 547L, 542L, 542L, 541L, \r\n542L, 540L, 541L, 541L, 539L, 536L, 541L, 540L, 536L, 536L, 536L, \r\n536L, 535L, 397L, 397L, 397L, 397L, 536L, 536L, 541L, 541L, 547L, \r\n547L, 542L, 547L, 542L, 541L, 541L, 541L, 540L, 541L, 540L, 541L, \r\n541L, 541L, 541L, 536L, 540L, 540L, 536L, 539L, 536L, 540L, 536L, \r\n539L, 536L, 536L, 539L, 536L, 536L, 539L, 540L, 541L, 541L, 541L, \r\n541L, 541L, 536L, 536L, 536L, 535L, 535L, 535L, 536L, 536L, 536L, \r\n536L, 536L, 536L, 536L, 536L, 535L, 535L, 536L, 535L, 535L, 531L, \r\n531L, 534L, 536L, 536L, 536L, 539L, 539L, 536L, 536L, 536L, 539L, \r\n540L, 540L, 539L, 536L, 539L, 541L, 541L, 541L, 541L, 540L, 536L, \r\n536L, 536L, 536L, 536L, 536L, 536L, 536L, 536L, 536L, 534L, 531L, \r\n535L, 535L, 535L, 531L, 531L, 534L, 534L, 531L, 531L, 531L, 531L, \r\n531L, 531L, 531L, 531L, 531L, 531L, 534L, 531L, 528L, 534L, 535L, \r\n535L, 531L, 523L, 521L, 521L, 521L, 523L, 523L, 528L, 523L, 535L, \r\n534L, 536L, 536L, 536L, 539L, 536L, 536L, 536L, 536L, 540L, 540L, \r\n540L, 541L, 541L, 541L, 541L, 547L, 548L, 547L, 547L, 548L, 552L, \r\n556L, 556L, 552L, 553L, 554L, 556L, 557L, 578L, 574L, 578L, 577L, \r\n593L, 596L, 596L, 593L, 608L, 611L, 611L, 611L, 611L, 611L, 611L, \r\n608L, 608L, 602L, 610L, 602L, 608L, 608L, 602L, 596L, 608L, 600L, \r\n611L, 611L, 611L, 611L, 611L, 611L, 611L, 611L, 611L, 611L, 611L, \r\n611L, 611L, 611L, 611L, 611L, 611L, 611L, 611L, 620L, 611L, 611L, \r\n622L, 621L, 611L, 622L, 623L, 622L, 623L, 621L, 622L, 623L, 623L, \r\n622L, 625L, 623L, 625L, 630L, 630L, 630L, 630L, 630L, 625L, 625L, \r\n630L, 630L, 630L, 638L, 638L), row = 1:622, rowid = 1:622), row.names = c(NA, \r\n-622L), class = c(\"data.table\", \"data.frame\"), .internal.selfref = <pointer: 0x0000026f4d0c1ef0>)\r\n\r\n```\r\n\r\nrelated: https:\/\/stackoverflow.com\/questions\/73403038\/r-data-table-rolling-max?noredirect=1#comment129630587_73403038","comments":["To summarize, what will address your problem nicely is:\r\n- frollmax\r\n- support `adaptive=TRUE && align!=\"right\"`\r\n\r\nfrollmax is already on our Todo list, but wasn't high priority. Filling this issue helps to prioritize.","Is rolling max the only function you need to use adaptively, or you think you may require arbitrary function to apply adaptively?","I pushed some initial drafts in https:\/\/github.com\/Rdatatable\/data.table\/tree\/frollmax not sure how I will be with time to complete this. Algo=exact for non-adaptive and adaptive could be already done, needs comprehensive testing. Algo=fast for non-adaptive is buggy. Algo=fast for adaptive does not yet exist.","I'll require arbitrary functions to work adaptively as well, but I think that optimized rolling min and max should come first in terms of value to the community. \r\nJust curious, what technical difficulty prevents the application of `align=right` in adaptive cases?\r\nThanks for your work and contribution","Align is implemented now as a post-processing shift on the results of `align=\"right\"`. It is implemented  that way to make code simpler and reduce maintenance burden. We don't have to push down `align` to the workhorse functions, and do extra shifts in deepest loops, or duplicate the loops for different align cases. To easy way is to duplicate all workhorse function 3 times (changing 2 new copies for align center\/left logic), but this obviously increase the amount of code to maintain.\r\nFollowing example will put some light on why it is not trivial to apply currently implemented technique to `adaptive=TRUE`.\r\nFunction `r2l` is basically what we use now (talking in C it is `memmove` followed by a small loop to fill out NAs at the end of the result).\r\n\r\n```r\r\nx = c(1,3,4,2,0)\r\nn = 3\r\nr = frollsum(x, n)\r\nl = frollsum(x, n, align=\"left\")\r\n\r\nr2l = function(r, n) shift(r, -n+1)\r\nstopifnot(all.equal(r2l(r, n), l))\r\n\r\n## results progressively computed by i iteration for adaptive=FALSE\r\n#i  right  left\r\n#1  .      8\r\n#2  ..     89\r\n#3  ..8    896\r\n#4  ..89   896.\r\n#5  ..896  896..\r\n\r\nan = c(3,2,2,3,2)\r\nar = frollsum(x, an, adaptive=TRUE)\r\n#al = frollsum(x, an, adaptive=TRUE, align=\"left\")\r\n\r\n## results progressively computed by i iteration for adaptive=TRUE\r\n#i  right  left\r\n#1  .      8\r\n#2  .4     87\r\n#3  .47    876\r\n#4  .479   876.\r\n#5  .4792  876..\r\n\r\nar2al = function(r, n) stop(\"fixme\")\r\n```\r\nThe problem is that for variable window width we cannot use current technique. Any ideas are welcome.","@gpierard is it feasible for your operation to sort reverse your data and then use `align=\"right\"`? It that would work, then I think it also make sense to improve documentation covering such use case.","In theory, it should. I'll take a look.","Is there a bug in the current `frollapply` ? \r\n\r\n```\r\nprice <- c(10:1)\r\nthresh_down <- c(6:15)\r\nfor(i in 1:length(prices)) {\r\n  tlist[[i]] <- as.integer(length(prices)-(i-1))\r\n}\r\nfrolls <- frollapply(price, n=tlist, align = \"right\", adaptive=TRUE, FUN= max)\r\n\r\n#Error in frollapply(price, n = tlist, align = \"right\", adaptive = TRUE,  : \r\n#  n must be integer\r\n\r\n```","```r\r\nfrollapply(x, n, FUN, ..., fill=NA, align=c(\"right\", \"left\", \"center\"))\r\n```\r\nfrollapply does not support `adaptive`.\r\n\r\nyou could test if that will work on `mean` or `sum`.\r\n","left aligned adaptive seems to work\r\n```r\r\nleftadaptivefrollsum = function(x, an) rev(frollsum(rev(x), rev(an), adaptive=TRUE))\r\nleftadaptivefrollsum(x, an)\r\n#[1]  8  7  6 NA NA\r\n```\r\nand will not be an expensive operation.\r\n~~I could possibly implement~~ [**implemented**] this pre and post processing inside data.table to make support for `align=\"left\"`","Benchmarking current frollmax adaptive (frollmax branch) vs fastest answer from SO shows another x30 speed up:\r\n```r\r\nlibrary(data.table)\r\nset.seed(108)\r\nex = data.table(value = cumsum(rnorm(1e6, 0.1)), end_window = 1:1e6 + sample(50:500, 1e6, TRUE), row = 1:1e6)[, end_window := pmin(end_window, .N)]\r\n# calc window size\r\nex[, \"w\" := pmin(end_window, max(row))-row+1L] ## pmin trims any end_window bigger than row to row\r\n\r\nf3 = function(ex) setnames(setcolorder(ex[ex, .(max(value)), on = .(row >= row, row <= end_window), .EACHI], c(1,3,2)), c(\"row\", \"rollmax\", \"end_window\"))\r\nf4 = function(ex) ex[, \"fm\" := frollmax(value, w, adaptive=TRUE, align=\"left\")]\r\n\r\nsystem.time(a3<-f3(ex))\r\n#   user  system elapsed \r\n#  3.788   0.007   3.648 \r\nsystem.time(a4<-f4(ex))\r\n#   user  system elapsed \r\n#  0.397   0.008   0.115 \r\nidentical(a3$rollmax, a4$fm)\r\n#[1] TRUE\r\n```\r\nSetting to 100% cpus pushes 0.115s down to 0.092s.\r\nAlso using `hasNA=FALSE` when you are sure there are no NAs reduces timing to 0.07s.\r\n\r\n@gpierard could you try this out on your real dataset and report timings (using `setDTthreads(0)` and `hasNA=FALSE` if there are no NAs)? Also `by=.EACHI` version if possible. Please check correctness as well.\r\nIf you are on windows you can install from binaries (no Rtools required) made by our CI\r\n```r\r\ninstall.packages(\"https:\/\/ci.appveyor.com\/api\/buildjobs\/0d88ro0bss71on0f\/artifacts\/data.table_1.14.3.zip\")\r\n```\r\nonce branch will be merged to master, installing will be even easier.","This function performs really well on [my real data](https:\/\/drive.google.com\/file\/d\/1_w7W_9SGkTjZltKXqCul-DXAULUaHRGB\/view?usp=sharing), computing in 45 seconds an operation that doesn't quite seem doable by the best join cited in the [stackoverflow discussion](https:\/\/stackoverflow.com\/a\/73408459\/5224236). \r\n\r\nI find that the performance of joins heavily depends on the data - ranging from a few seconds to at east an hour (never returning a result, like below). `frollmax`, on the other hand, maximizes both `nextdown` and its opposite in 45-46 secs. \r\n\r\n```r\r\n# join solution -----------------------------------------------------------\r\nbig <- fread('bigfull2_nextups.csv')\r\nbg <- big[,.(time, price, row, nextdown)]\r\n\r\nsetDTthreads(0)\r\nstartt <- Sys.time()\r\nres_join <- bg[bg, .(min(nextdown)), on = .(row >= row, row <= nextdown), .EACHI]\r\nendt <- Sys.time()\r\nendt-startt \r\n# doesn't finish (left for 15 - 20mins)\r\n\r\n# froll solution ----------------------------------------------------------\r\n#bg[, \"w\" := pmin(nextdown, max(row))-row+1L]\r\n\r\nany(bg[, nextdown >= .N ], na.rm = T) # FALSE : all window sizes are already within .N\r\nbg[, nextdown := ifelse(is.na(nextdown), 0, nextdown)][, opp_nextdown:=-nextdown] # setting NA's to 0 and reversing data in order to maximize.\r\n\r\nbg[, end_window := nextdown-row+1L] # since frollmax uses offsets rather than absolute indices\r\nbg[, end_window := pmax(end_window,0)] # ensuring no negative offset\r\n\r\nsetDTthreads(0)\r\nstartt <- Sys.time()\r\nres_froll <- bg[, \"fm\" := frollmax(opp_nextdown, end_window, adaptive=TRUE, align=\"left\", hasNA = F)]\r\nendt <- Sys.time()\r\nendt-startt # Time difference of 45.97646 \/ 46.22508 secs\r\n\r\nmin(bg$nextdown[496:531]) #[1] 520 OK\r\nmin(bg$nextdown[8406301:8406323]) #[1] 8406323 OK\r\n\r\n```\r\n`frollmax` is clearly superior to other methods for computing the maximum over an adaptive window. This constitutes a clear bonus and a much appreciated new feature for the immediate problem at hand. \r\n\r\nFor other problems such as [this SO question](https:\/\/stackoverflow.com\/q\/73378024\/5224236): finding first occurrences of a condition in a rolling, adaptive context, I rolled back to using `Rcpp` due to the variation in the performance of non-equi joins.\r\n\r\nI am really curious about how `frollapply` would be able to generalize the performance of `frollmax` for problems such as:\r\n- The above-mentioned problem of finding the first occurrence of a condition in the next variable-length window.\r\n- Returning the `index` of the row which maximizes a given column, which could be called `frollwhichmax` - or `frollmax(..., index=TRUE)` although I suspect that a join might be able to use the result from `frollmax` to get the corresponding index. \r\n\r\n**EDIT:** A join indeed is able to retrieve the index of the maximum values returned by `frollmax` in a couple of seconds.\r\n\r\n```r\r\nbig <- fread('bigfull3_rollmin.csv') # including `min_nextdown` computed above\r\nbig$fm <- NULL\r\n\r\nbjoin <- big[,.(time, price, row, nextdown, min_nextdown)]\r\nbjleft <- copy(bjoin)\r\nbjleft[,val:=nextdown]\r\nbjright <- copy(bjoin)\r\nbjright[,val:=min_nextdown]\r\n\r\n# bjleft <- bjleft[1:1000]\r\n# bjright <- bjright[1:1000]\r\n\r\nstartt <- Sys.time()\r\nprint(startt)\r\nres_whichmin_join <- bjleft[bjright, on = .(val, row >= row, row <= nextdown), .(result = x.row), mult='first']$result\r\nendt <- Sys.time() \r\nendt-startt # Time difference of 2.162442 secs\r\n\r\n# and for the final check:\r\n\r\nbig$max_idx <- res_whichmin_join \r\ncheck <- big$nextdown[big$max_idx] == big$min_nextdown\r\nall(check, na.rm = T) # TRUE\r\n\r\n\r\n```\r\n\r\n\r\n-----------------------------\r\n**INSTALLATION**\r\nInstalling the binary package returned `not available for this version of R` (I am on `R version 4.2.1 (2022-06-23 ucrt)`). I was able to install on Win10 by these steps:\r\n\r\n- Removed existing package with remove.packages()\r\n- ran RStudio as an admin\r\n- Installed by specifying the libpath:  `devtools::install_github('https:\/\/github.com\/Rdatatable\/data.table\/tree\/frollmax', lib=.libPaths()[2])`\r\n\r\n\r\n\r\n","I am glad to hear about so tremendous speed up `frollmax` gives. PR still needs polishing non-adaptive version (I focused on your use case first) before it can put for review by data.table team and contributors.\r\n\r\nAs for the new problem you described, let's not introduce another problem in this issue. You are welcome to fill out new FR, I just cannot guarantee we will decide to have it in scope (which is not only implementing those but also maintaining).","There is also now support for `adaptive` in `frollapply` (as well as `align=\"left\"`), which gives pretty descend speed up. Difference to alternatives is much bigger here than in non-adaptive version. I put some timings in NEWS.md entry: https:\/\/github.com\/Rdatatable\/data.table\/pull\/5441\/files#diff-51920e95310ebfbc1ae31709f3b95f89afffbf4f1a6e38e8b2b406e2fb6197ea","great, thanks again","It will be completed when it will be merged to master branch."],"labels":["feature request","froll"]},{"title":"natural anti-join (X[on=!Y])","body":"Thanks for this package! I only recently started using it, so apologies for any misunderstandings on my part. I'm often joining data, and have been using previously `merge` before, so I was happy to find that `X[on=Y]` allows natural joins with a concise syntax. However, it seems to me that natural anti-joins `X[on=!Y]` are not supported:\r\n\r\n`#` `Minimal reproducible example`\r\n```\r\nlibrary(data.table)\r\n\r\nset.seed(42)\r\nbig <- data.table( \r\n  id = LETTERS[2:11],\r\n  a = sample(101:105, 10, replace = TRUE),\r\n  b = sample(200:300, 10)\r\n)\r\nsmall <- data.table(\r\n  id = LETTERS[1:5],\r\n  y = sample(1:5, 5, replace = TRUE),\r\n  z = sample(10:20, 5) \r\n)\r\n\r\n# this works \r\nsmall[!big, on = .(id)]\r\n# A data frame: 1 \u00d7 3\r\n   id y  z\r\n1:  A 4 10\r\n\r\n\r\n# this also works \r\nsmall[!big, on=.NATURAL]   \r\nid y  z\r\n1:  A 4 10\r\n\r\n\r\n# this does not work\r\nsmall[on=!big]\r\nError in FUN(left) : invalid argument type\r\n```\r\n\r\nOutput of sessionInfo()\r\n```\r\nR version 4.2.1 (2022-06-23)\r\nPlatform: x86_64-pc-linux-gnu (64-bit)\r\nRunning under: Manjaro Linux\r\n\r\nMatrix products: default\r\nBLAS:   \/usr\/lib\/libblas.so.3.10.1\r\nLAPACK: \/usr\/lib\/liblapack.so.3.10.1\r\n\r\nlocale:\r\n [1] LC_CTYPE=en_US.utf8        LC_NUMERIC=C              \r\n [3] LC_TIME=nl_NL.UTF-8        LC_COLLATE=en_US.utf8     \r\n [5] LC_MONETARY=nl_NL.UTF-8    LC_MESSAGES=en_US.utf8    \r\n [7] LC_PAPER=nl_NL.UTF-8       LC_NAME=C                 \r\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n[11] LC_MEASUREMENT=nl_NL.UTF-8 LC_IDENTIFICATION=C       \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.2.1 tools_4.2.1\r\n```\r\n\r\n\r\nIs there something wrong with my approach, or is this feature not implemented?","comments":["on argument specifies columns to join by, having special keyword for natural join. I am not sure if we want to have a DT object passed into it. Also see for another interface proposed in https:\/\/github.com\/Rdatatable\/data.table\/pull\/4370\r\nEdit:\r\nUh ok, X[on=Y] already works..."],"labels":["feature request","non-equi joins","joins"]},{"title":"The new function DT() used for chaining\/piping drops `data.frame` rownames names","body":"I noticed that the new function `DT()` drops  row names when applied to `data.frame`.\r\n\r\n\tdf = mtcars[1:5, 1:2]\r\n\tdf\r\n\t\t\t   mpg cyl\r\n\tMazda RX4         21.0   6\r\n\tMazda RX4 Wag     21.0   6\r\n\tDatsun 710        22.8   4\r\n\tHornet 4 Drive    21.4   6\r\n\tHornet Sportabout 18.7   8\r\n\r\n\tdf |> \r\n\t  DT(1:4, .SD)\r\n\t  \r\n\t   mpg cyl\r\n\t1 21.0   6\r\n\t2 21.0   6\r\n\t3 22.8   4\r\n\t4 21.4   6\r\n\r\n","comments":["I assume that's intended. data.table doesn't support row names (for good reasons).","Maybe I was not clear enough, Note that the input here is not a `data.table` but a `data.frame` (as apposed to data.table which is its extension). `DT()` works with `data.frame`'s too (not only `data.table`s).\r\n\r\nI personally think that the row names should be kept too since the input and output in this case support row names, and in my example it has row names.\r\n\r\nThe reasons you refer to are likely those specific to `data.table`.","Because `DT()` supports build-in `data.frame` (as opposed to data.table), I think it should also handle it appropriately. "],"labels":["dev","DT()"]},{"title":"Add functionality for varying optimization level directly into test()","body":"We have a lot of code in tests that does rigamarole around tests for different optimization levels. we should take a look at how to consolidate this into the signature of test(), if possible.\r\n\r\ntest(optimization_levels = c(...)) looks like the simplest signature, but not sure it's general enough to capture what varies by level. Needs more investigation.","comments":["> Agree. One thing is that we test verbose output for specific strings to ensure optimization is on or off when we think it should be. Maybe just a tweak to test() would overcome that. Plus moving the 2 or 3 relatively longer running sections into benchmark.Rraw. Those could be left in test.Rraw with a reduced size so they're still tested for correctness. Nothing else springs to mind. It's at under 1 minute so doubling that should be ok on CRAN.\r\n\r\nwe should be able to accomplish this with key-value pairs of optimization level-expected output.","Also it would be nice if verbose=TRUE was automatically set by test() if output= was provided so we don't have to add verbose= into each query.  On the other hand sometimes we check that printed output is correct not the verbose output. Maybe adding verbose= argument to test() then. Seeing the string passed to verbose= should be clear the intention of checking the verbose output contains that string and not that we want verbose output from test() for some reason.","Related to #4305"],"labels":["internals","tests"]},{"title":"Grouped calculations of list columns are very slow","body":"Calculations get very slow when we create list columns. Consider these two calls:\r\n```\r\ndt <- data.table::data.table(x = sample(10000, 10000000, TRUE), y = sample(10000, 10000000, TRUE), z = sample(10000, 10000000, TRUE))\r\nsystem.time(dt[, .I, by = c(\"x\", \"y\")])\r\nsystem.time(dt[, .(.(.I)), by = c(\"x\", \"y\")])\r\n```\r\nThe second one should take about 3 times as long as the first one, which is not really reasonable (especially since the first call is not particularly fast, compared to more typical calculations).","comments":["Did verbose=TRUE printed anything interesting?","I think it is reasonable to expect it to be much slower. Consider that in the first case you call integer allocation once. In the second case you allocate once a list, and then for each group you allocate integer separately. Allocation is what in many situations slows down the code, therefore 3 times slower is not a big in such case IMO.\r\nAnyway before closing this issue it make sense to see your @vpetzel verbose=T output as well to ensure there are no unexpected things popping up.","@jangorecki I do not know how data table does these things, but I suppose it is along the lines of\r\n\u2192 For each group evaluate the entries of `j`. This probably forms some sort of list (somehow recycling must be handled?)\r\n\u2192 Concatenate all of these results into one vector (or list)\r\n\r\nSo I guess the integer allocation has to be done for each group no matter what. But instead we have one allocation of a list for each  group. So I suppose it is *understandable* why this is so much slower, but I would not say it is *reasonable*, because instead of calculating a list of lists of results and concatenating them we could simply calculate a list of the results and leave them be as they are.\r\n\r\nSo it is not unlikely that a simple optimization could be added in case one part of `j` starts by `list(...)` or `.(...)` to make this much faster.\r\n\r\nNote that this would potentially severely affect some things like split.data.table.\r\n\r\nAs requested here are the outputs with `verbose=TRUE`:\r\n```\r\ndt[, .I, by = c(\"x\", \"y\"), verbose = TRUE]\r\nDetected that j uses these columns: <none> \r\nFinding groups using forderv ... forder.c received 10000000 rows and 2 columns\r\n0.114s elapsed (0.516s cpu) \r\nFinding group sizes from the positions (can be avoided to save RAM) ... 0.053s elapsed (0.043s cpu) \r\nGetting back original order ... forder.c received a vector type 'integer' length 9514892\r\n1.129s elapsed (1.697s cpu) \r\nOptimization is on but left j unchanged (single plain symbol): '.I'\r\nMaking each group and running j (GForce FALSE) ... dogroups: growing from 9514892 to 11027017 rows\r\nWrote less rows (10000000) than allocated (11027017).\r\n\r\n  collecting discontiguous groups took 4.404s for 9514892 groups\r\n  eval(j) took 4.530s for 9514892 calls\r\n21.3s elapsed (12.8s cpu) \r\n[DATA]\r\n\r\ndt[, .(.(.I)), by = c(\"x\", \"y\"), verbose = TRUE]\r\nDetected that j uses these columns: <none> \r\nFinding groups using forderv ... forder.c received 10000000 rows and 2 columns\r\n0.109s elapsed (0.604s cpu) \r\nFinding group sizes from the positions (can be avoided to save RAM) ... 0.043s elapsed (0.077s cpu) \r\nGetting back original order ... forder.c received a vector type 'integer' length 9514892\r\n0.287s elapsed (0.898s cpu) \r\nlapply optimization is on, j unchanged as 'list(list(.I))'\r\nGForce is on, left j unchanged\r\nOld mean optimization is on, left j unchanged.\r\nMaking each group and running j (GForce FALSE) ... \r\n  collecting discontiguous groups took 4.531s for 9514892 groups\r\n  eval(j) took 7.113s for 9514892 calls\r\n[DATA]\r\n```","Thanks for verbose. What looks interesting is \"Getting back original order\", somehow it is much faster in a list case (0.28 vs 1.12). Could you try again, starting in a clean session, running list first, and then integer as a second query. Just to see if order of evaluation makes difference here.\r\n\r\n> I guess the integer allocation has to be done for each group no matter what.\r\n\r\nI am not sure how is in this case but it doesn't have to allocate for each group. It is perfectly fine to allocate only single integer vector once (length of answer's nrow), and then fill out its values.","@jangorecki I think this was just a random quirk, usually they take the same amount of time.\r\n\r\n> I am not sure how is in this case but it doesn't have to allocate for each group. It is perfectly fine to allocate only single integer vector once (length of answer's nrow), and then fill out its values.\r\n\r\nBut don\u2019t you need to calculate the result to know the length of the vector you need to allocate in this case? And if we want to evaluate `.I` in some expression I think it has to be allocated at least temporarily somewhere. And even if we do something like\r\n`system.time(dt[, .(c(.I, NULL)), by = c(\"x\", \"y\")])`\r\n(which should force an additional allocation plus a copy of the vector for each group) the time does only get slightly worse, not at all comparable to using `list`.","Such a big difference should not be treated as noise and should be investigated.\r\nI think list is faster there because we only rearrange pointers order, while for an integer, new allocation is made. Would be good to confirm that. WDYT @mattdowle ?\r\n\r\nIf you make \"j\" more complex than just .I then of course it cannot avoid allocation by group. But only for .I it is perfectly fine to allocate once and fill out values, although I don't if we have this kind of optimization Implemented.\r\n\r\nAnyway, we are left with investigating dogroups, for .I and list(.I) to, most likely, compare the number of allocations, as there are no computation there that would take time.","@jangorecki I did evaluate this a few more times (also with a new clean session) and the timing was always similar, so I\u2019m quite sure this was a one time spike (this can happen occasionally in R).\r\n\r\nEven if we make `j` more complex to force allocation the timing does increase only slightly.","I looked at the source and there is no optimization made for .I alone in j. It is possible to completely skip call to dogroups in such case, and implementing such a short circuit is fairly easy. Then difference will be even bigger comparing to list(.I).\r\nFor now, in this issue I think it make sense to focus on j=999L vs j=list(999L), as this (or any other, possibly not to small integer, to avoid R's potential constants) is the minimal overhead case to observe impact of wrapping results into a list column. Just commenting for now as I don't have machine on hand to try that out."],"labels":["performance"]},{"title":"setDT extremely slow for very wide input","body":"```r\r\nDT = replicate(1e7, 1, simplify = FALSE)\r\nsystem.time(setDT(DT))\r\n#    user  system elapsed \r\n#  23.043   0.216  23.261 \r\n```","comments":[],"labels":["performance"]},{"title":"[Feature Request \/ Question] Tibble-esqe DT Printing","body":"Hello,\r\n\r\nAs a long time user of `{data.table}`, I first want to appreciate the efforts you and your team have taken to build this package and maintain it. Your work is truly an exemplar of the best of what R has to offer, and I routinely look up to it as the touchstone to which I judge my own packages.\r\n\r\n---\r\n\r\nAs a former `{tibble}` user, I now use `{data.table}` everyday for my work for the speed and efficiency of handling large datasets. The _only_ feature which I truly miss about `tibble` is it's print function, which (imho) leaves a bit to be desired. Especially datasets with many columns, long strings, NA values, and numerics, the default print method for tibbles gives a better user experience. Here's a comparison from a recent dataset on my 18\" laptop:\r\n\r\n<img width=\"1119\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/6344050\/181682047-50bde39c-ed2c-477b-bb31-0ecc40bcf5c3.png\">\r\n\r\n(UX features: the left-vs-right align for char-vs-numerics, red-colored NA values, long col names trimmed)\r\n\r\nComparing this with our data.table print method, it results in a very large print; difficult to follow the items in the rows since I have to scroll up and down:\r\n\r\n<img width=\"1162\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/6344050\/181682070-4e3ba176-3180-46de-9900-affd582b2f1d.png\">\r\n<img width=\"1166\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/6344050\/181682116-83ea0ee9-efa7-4d86-b866-7275bfbb487d.png\">\r\n\r\n--\r\n\r\n1. Is this by design? Is there a performance reasoning behind keeping the print method simple?\r\n2. Are there any options & adjustments I could make to have the data.table print like a tibble?\r\n\r\nIf this isn't possible, would you consider this a feature request for an updated print method?\r\n\r\nCheers and thanks for your hard work on this amazing package!","comments":["For answering the 2nd question. You could overwrite the printing mechanism of data.table.\r\n\r\nThere is a nice gist on this topic from @krlmlr https:\/\/gist.github.com\/krlmlr\/35f56d625ea56ff098f965d7c6d5a382\r\n\r\n```r\r\nlibrary(data.table)\r\nlibrary(tibble)\r\n\r\nprint_data_table <- function(x, ...) {\r\n  # Adapted from data.table:::as.data.frame.data.table()\r\n  ans <- x\r\n  attr(ans, \"row.names\") <- .set_row_names(nrow(x))\r\n  attr(ans, \"class\") <- c(\"tbl\", \"data.frame\")\r\n  attr(ans, \"sorted\") <- NULL\r\n  attr(ans, \".internal.selfref\") <- NULL\r\n  print(ans)\r\n  invisible(x)\r\n}\r\n\r\nassignInNamespace(\"print.data.table\", print_data_table, asNamespace(\"data.table\"))\r\n\r\nx = as.data.table(mtcars)\r\nx\r\n#> # A data frame: 32 \u00d7 11\r\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\r\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\r\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\r\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\r\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\r\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\r\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\r\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\r\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\r\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\r\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\r\n#> # \u2026 with 22 more rows\r\n```","You can control the [max col length](https:\/\/github.com\/Rdatatable\/data.table\/blob\/68872b4ea8e27c7b29d064506caf8652a6cb0fd6\/R\/print.data.table.R#L231) using\r\n\r\n```r\r\noptions(datatable.prettyprint.char = 30L)\r\n```\r\n\r\nwhich is (for me) the main issue.\r\n\r\nhttps:\/\/github.com\/Rdatatable\/data.table\/issues\/1091","Fantastic @ben-schwen and @andrewrech . This is exactly what I need.\r\n\r\nIt would be great if we could incorporate that print method into the package, at least an an option via `options(...)`. That would be a feature request... if not, feel free to close out this issue.\r\n\r\nThanks y'all!","Talking about options, just to add that you can get the column types (`chr`, `int`, etc.) under the headers with:\r\n\r\n```r\r\noptions(datatable.print.class = TRUE, datatable.print.keys = TRUE)\r\n```\r\n\r\nYou can obviously add the above to your .Rprofile if you'd like the behaviour to persist over sessions. But note that the next release of data.table will [enable these options by default](https:\/\/github.com\/Rdatatable\/data.table\/pull\/5275). So another way of getting this to work is just to grab the latest dev version from GitHub (or [r-universe](https:\/\/rdatatable.r-universe.dev\/ui#package:data.table) if you're on Windows\/Mac and just want binaries). ","We have long-standing issue #1523 which tracks enhancements to printing. Some things you mention might be added as new options, but I do prefer data.tables defaults here.\r\n\r\nIn addition to the notes above, there's an option `trunc.cols` (controlled by option `datatable.print.trunc.cols`) that will help reduce the total width of printing, see `?print.data.table`.\r\n\r\n> Is there a performance reasoning behind keeping the print method simple?\r\n\r\nNot really. The main performance gain comes from subsetting the table to `rbind(head(x), tail(x))` -- once the \"inner\" rows of the table are ignored, most operations on the remaining table will be all but instantaneous. The real issue here is dependencies -- tibble printing uses pillar, which has a non-trivial dependency load:\r\n\r\nhttps:\/\/github.com\/r-lib\/pillar\/blob\/3f849a5e95eac06075985a4365cd5ab5bdcd18f5\/DESCRIPTION#L20-L49\r\n\r\nThe main innovation here is `fansi` to color terminal output. We might consider doing something similar with `fansi` as a Suggested dependency...","BTW and as it hasn't been mentioned: you can get colored output in an informal (== non-CRAN) way via the rather nice [colorout](https:\/\/github.com\/jalvesaq\/colorout) package I quite like and use.  Because it gets into R internals it can never be on CRAN but it is good, lightweight and zero (other) depends.  Works well with `data.table`.","Maybe stating the obvious here, but if you want to print a data.table exactly like a tibble and don't mind loading {tibble}, note that `as_tibble(dt)` won't copy the columns of `dt`, so just overwriting print.data.table to convert your data to a tibble first isn't all that expensive.\r\n\r\nPersonally the default settings in dev data.table do everything I'd want, including showing the keys (which tibble obviously doesn't support), but thought I'd mention this in case it's useful and non-obvious to someone else.\r\n\r\n``` r\r\nlibrary(tibble)\r\n#> Warning: package 'tibble' was built under R version 4.1.2\r\nlibrary(data.table)\r\n\r\ndt <- as.data.table(head(mtcars))\r\ntracemem(dt$mpg)\r\n#> [1] \"<0x7f99ed356ba8>\"\r\n\r\nprint_data_table <- function(x, ...) print(as_tibble(x), ...)\r\nassignInNamespace(\"print.data.table\", print_data_table, asNamespace(\"data.table\"))\r\n\r\ndt # note no messages from tracemem\r\n#> # A tibble: 6 \u00d7 11\r\n#>     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\r\n#>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n#> 1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\r\n#> 2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\r\n#> 3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\r\n#> 4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\r\n#> 5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\r\n#> 6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\r\n```\r\n\r\n<sup>Created on 2022-07-31 by the [reprex package](https:\/\/reprex.tidyverse.org) (v2.0.1)<\/sup>\r\n\r\n","One thing that caught me out when I switched to data.table is that strings with prefixed whitespace e.g. \" foo\" look very much like \"foo\" when in data.table. Tibbles surround strings containing whitespace with quotes, which is really useful in my opinion.\r\n\r\n```\r\ndata.table::data.table(a = c(\"foo\", \"bar\"), b= c(\" foo\", \"bar \"))\r\n#>         a      b\r\n#>    <char> <char>\r\n#> 1:    foo    foo\r\n#> 2:    bar   bar\r\ntibble::tibble(a = c(\"foo\", \"bar\"), b= c(\" foo\", \"bar \"))\r\n#> # A tibble: 2 \u00d7 2\r\n#>   a     b     \r\n#>   <chr> <chr> \r\n#> 1 foo   \" foo\"\r\n#> 2 bar   \"bar \"\r\n```\r\n\r\nIt would be really nice to have something like this in data.table as well","there's quite=TRUE which will surround all strings with quotes, but auto-quoting when leading\/trailing whitespace is detected shouldn't be too hard to support ","`quote = TRUE` will surround everything with quotes:\r\n\r\n```\r\nlibrary(data.table)\r\ndt = data.table(a = c(\"foo\", \"bar\"), b = c(\" foo\", \"bar \"), c = 1:2) \r\nprint(dt, quote = TRUE)\r\n#>         \"a\"      \"b\"     \"c\"\r\n#>    \"<char>\" \"<char>\" \"<int>\"\r\n#> 1:    \"foo\"   \" foo\"     \"1\"\r\n#> 2:    \"bar\"   \"bar \"     \"2\"\r\n```\r\n\r\nGood to hear that `tibble`'s way of doing it wouldn't be difficult to support!","Actually it is quite easy to make use of tibble printing for data tables, like this:\r\n```\r\n# Override format.data.table to format with pillar functions. Not exactly sure why we need this, as data.table does not provide format.\r\nformat.data.table <- pillar:::format.tbl\r\n# Override print.data.table to use pillar:::print_tbl. We need to copy the code so that format.data.table can be scoped\r\nprint.data.table <- function (x, width = NULL, ..., n_extra = NULL, n = NULL, max_extra_cols = NULL, max_footer_lines = NULL) \r\n{\r\n    if (!is.null(n_extra)) {\r\n        deprecate_soft(\"1.6.2\", \"pillar::print(n_extra = )\", \r\n            \"pillar::print(max_extra_cols = )\", user_env = caller_env(2))\r\n        if (is.null(max_extra_cols)) {\r\n            max_extra_cols <- n_extra\r\n        }\r\n    }\r\n    writeLines(format(x, width = width, ..., n = n, max_extra_cols = max_extra_cols, \r\n        max_footer_lines = max_footer_lines))\r\n    invisible(x)\r\n}\r\n\r\ndt <- data.table::as.data.table(mtcars)\r\n# Add tbl to class list (keep data.table first!)\r\nclass(dt) <- c(\"data.table\", \"tbl\", \"data.frame\")\r\n```\r\n\r\nThe only real challenges here is getting the methods accept a data.table.","Thanks for all the replies so far. Very insightful.\r\n\r\n@eutwt , I was hoping to put these lines in the `.Rprofile` at my project level. But that doesn't seem to work. Any idea why?\r\n\r\n> print_data_table <- function(x, ...) print(as_tibble(x), ...)\r\n> assignInNamespace(\"print.data.table\", print_data_table, asNamespace(\"data.table\"))\r\n\r\n\r\n"],"labels":["feature request","print"]},{"title":"split.data.table ignores sep= when running by=","body":"```r\r\nx=data.table::data.table(rep(1:2, each=5), 1:5, 1:10)\r\n\r\nnames(split(as.data.frame(x), list(x$V1, x$V2), sep = \"|\"))\r\n#  [1] \"1|1\" \"2|1\" \"1|2\" \"2|2\" \"1|3\" \"2|3\" \"1|4\" \"2|4\" \"1|5\" \"2|5\"\r\n\r\nnames(split(x, list(x$V1, x$V2), sep = \"|\"))\r\n# [1] \"1|1\" \"2|1\" \"1|2\" \"2|2\" \"1|3\" \"2|3\" \"1|4\" \"2|4\" \"1|5\" \"2|5\"\r\n\r\nnames(split(x, by = c(\"V1\", \"V2\"), sep = \"|\"))\r\n#  [1] \"1.1\" \"1.2\" \"1.3\" \"1.4\" \"1.5\" \"2.1\" \"2.2\" \"2.3\" \"2.4\" \"2.5\"\r\n```\r\n\r\nThis is basically as documented (since `...` are documented as associated with `f=`), so this is technically a feature request to support `sep=` with `by=`.","comments":[],"labels":["consistency"]},{"title":"feature request: fread should raise an error instead of a warning when reading a gzipped file that does not fit in temporary storage","body":"\r\nHi,\r\n\r\nThanks for developing this awesome package!\r\n\r\nfread has supported gzipped files for a while now and it usually works great\r\n#717\r\n\r\nRecently, I have been working with containers more often. These often have limited temporary storage. When reading a gzipped file with fread, I believe it first unzips it to temporary storage, and then reads the file from there into memory (I am aware of the `tmpdir` argument that controls this). If the file is very large, the temporary storage can fill up. fread, instead of raising an error, reads the truncated file and prints a cryptic warning (telling you that the file was truncated). This warning is easily overlooked when working non-interactively.\r\n\r\nI think it would be better if instead of reading a truncated file, fread would actually raise an error in these cases. The error could then be caught early on. Only reading part of the data often leads to downstream errors, which then harder to de-bug. I don't think many users want to read just the first x lines that happen to fit into their temporary storage at random.\r\n\r\nA flag could still force reading a truncated gzipped file, if necessary.\r\n\r\nthank you for your time\r\n\r\n","comments":[],"labels":["feature request"]},{"title":"Feature Request: Use of sep2=''","body":"Hello\r\n\r\nIs there any reason why ```sep2=''``` is not allowed in ```fwrite```? \r\n\r\nSeveral R packages are used to manipulate genomic data (such as SNP) and do a good job filtering out those with bad quality. However, when comes to performing genomic analyses to estimate \"Breeding Values\" for example, on large scale, we have to do that outside of R. A common layout when writing the SNP to a file is the following, where the first column is the individual identification and the second is the SNP (where all SNPs are collapsed without any space between them).\r\n\r\n```data.table``` is the fastest package I've benchmarked to read, write, and filter rows\/columns. Because of that, I wonder if there is any way to use ```sep2=''```, as shown in the following example.\r\n\r\n```\r\nlibrary(\"data.table\")\r\ngeno <- data.table(\r\n  IID = 1:10,\r\n  SNP = lapply(1:10, function(i) sample(0:2, 10, replace = TRUE))\r\n)\r\n```\r\n\r\nAccording to ```?fwrite```, ```sep2[2]``` must be a single character. Therefore I have to collapse the list, rather than use ```sep2```. Hence, if I try to run ```fwrite(geno, \"Geno.txt\", col.names = FALSE, sep = \" \", sep2 = c(\"\",\"\",\"\"))``` I get the following error message:\r\n\r\n```\r\nError in fwrite(geno, \"Geno.txt\", col.names = FALSE, row.names = FALSE,  : \r\n  is.character(sep2) && length(sep2) == 3L && nchar(sep2[2L]) ==  .... is not TRUE\r\n```\r\n\r\nI would like to have the following result, without having to collapse all values before writing it to a file.\r\n\r\n```\r\n1 2221210202\r\n2 0020010221\r\n3 1010022212\r\n4 0120121221\r\n5 1212211202\r\n6 2100002010\r\n7 1110011210\r\n8 1212012121\r\n9 2221121021\r\n10 1122220101\r\n```\r\n\r\nThank you.\r\n\r\n","comments":["Any progress?"],"labels":["feature request","help-wanted"]},{"title":"documentation of \":=\" (or maybe rather \"[\" actually)","body":"After creation of a data.table\r\n```\r\nlibrary(data.table)\r\na=fread(\"\r\nnr,a\r\n1,a\r\n2,b\r\n\")\r\na\r\n```\r\na new one is created the following way:\r\n```\r\nb <- a[]\r\nb\r\n```\r\nNB: I did not just assign `b <- a` but `b <- a[]`. According to the documentation (see below) I'd expect `b` to be independent from `a`, and changes not reflect back to `a`. On a first glance that seems to be the case:\r\n```\r\nb[1, 1] <- 4\r\nb\r\na\r\nall(b==a)\r\n```\r\nHowever when using `:=` to assign the new value that logic seems broken:\r\n```\r\nb <- a[]\r\n\r\nb\r\nb[1, nr:=4]\r\nb\r\na\r\nall(b==a)\r\n```\r\nWhat am I missing from the documentation, which says that the subsetting operation returns a _new_ `data.table`, which then in turn is updated by reference using the `:=` operator? \r\n\r\nThe effect is the same when creating `b` as `b <- a[TRUE]`. However when using `b <- a[1:.N]` then both objects seem to be separate entities as I'd expect.\r\n\r\nSee **Note** section of `:=`:\r\n\r\n> DT[a > 4, b := c] is different from DT[a > 4][, b := c]. The first expression updates (or adds) column b with the value c on those rows where a > 4 evaluates to TRUE. X is updated by reference, therefore no assignment needed.\r\n> \r\n> The second expression on the other hand updates a **new data.table that's returned by the subset operation**. Since the subsetted data.table is ephemeral (it is not assigned to a symbol), the result would be lost; unless the result is assigned, for example, as follows: ans <- DT[a > 4][, b := c]. \r\n> \r\n\r\n```\r\n> sessionInfo()\r\nR version 4.1.3 (2022-03-10)\r\nPlatform: i386-w64-mingw32\/i386 (32-bit)\r\nRunning under: Windows 10 x64 (build 18363)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252\r\n[3] LC_MONETARY=German_Germany.1252 LC_NUMERIC=C\r\n[5] LC_TIME=German_Germany.1252\r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base\r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.1.3\r\n```","comments":["Thank you for reporting. Related issue https:\/\/github.com\/Rdatatable\/data.table\/issues\/3215"],"labels":["documentation"]},{"title":"Differences in numeric representation between read.csv and data.table::fread","body":"Using data.table v1.14.3 and running the below code identifies 34 cases where ```fread``` and ```read.csv``` generate numbers with minor numeric differences. I'm fairly sure this is related to the now fixed issue #4461 (listed as bugfix 40 in the v1.14.3 changelog). Certainly, something happened between v1.14.2 and 1.14.3, as running with 1.14.2 identifies 9 additional cases of inconsistency that are not present following the update. I initially raised this on [stackoverflow \r\n](https:\/\/stackoverflow.com\/questions\/72649882\/differences-in-numeric-representation-between-read-csv-and-data-tablefread), but given it appears to be related to a previously fixed issue, I thought I'd raise it here too. I realise this isn't necessarily data.table's problem (and that the numbers aren't meaningfully different), but consistency between the two methods would be useful.\r\n\r\n```\r\nx <- (1:200000)\/1000000\r\ndf <- data.frame(x = x)\r\n\r\nfname <- tempfile()\r\nwrite.csv(df, fname)\r\nf1 <- read.csv(fname)\r\nf2 <- data.table::fread(fname, data.table=FALSE)\r\nnum_diffs <- which((f1[,2] - f2[,2]) != 0)\r\n\r\nnum_diffs\r\ndiff(num_diffs)\r\n\r\nprint(f1[num_diffs, 2], digits=18)\r\nprint(f2[num_diffs, 2], digits=18)\r\n```\r\n\r\nreturns: \r\n```\r\n> num_diffs\r\n [1]   2877   5754  11227  11508  22454  23016  23859  24421  32093  33217  44908  46032  47718  48842  60533  61657  64186  66434 137253 143997\r\n[21] 148382 148493 152878 159622 164007 164118 168503 175247 179632 179743 184128 190872 195257 195368 199753\r\n> diff(num_diffs)\r\n [1]  2877  5473   281 10946   562   843   562  7672  1124 11691  1124  1686  1124 11691  1124  2529  2248 70819  6744  4385   111  4385  6744  4385\r\n[25]   111  4385  6744  4385   111  4385  6744  4385   111  4385\r\n> print(f1[num_diffs, 2], digits=18)\r\n [1] 0.0028770000000000002 0.0057540000000000004 0.0112270000000000009 0.0115080000000000009 0.0224540000000000017 0.0230160000000000017\r\n [7] 0.0238589999999999983 0.0244209999999999983 0.0320929999999999965 0.0332169999999999965 0.0449080000000000035 0.0460320000000000035\r\n[13] 0.0477179999999999965 0.0488419999999999965 0.0605330000000000035 0.0616570000000000035 0.0641859999999999931 0.0664339999999999931\r\n[19] 0.1372530000000000139 0.1439969999999999861 0.1483820000000000139 0.1484929999999999861 0.1528780000000000139 0.1596219999999999861\r\n[25] 0.1640070000000000139 0.1641179999999999861 0.1685030000000000139 0.1752469999999999861 0.1796320000000000139 0.1797429999999999861\r\n[31] 0.1841280000000000139 0.1908719999999999861 0.1952570000000000139 0.1953679999999999861 0.1997530000000000139\r\n> print(f2[num_diffs, 2], digits=18)\r\n [1] 0.0028769999999999998 0.0057539999999999996 0.0112269999999999991 0.0115079999999999991 0.0224539999999999983 0.0230159999999999983\r\n [7] 0.0238590000000000017 0.0244210000000000017 0.0320930000000000035 0.0332170000000000035 0.0449079999999999965 0.0460319999999999965\r\n[13] 0.0477180000000000035 0.0488420000000000035 0.0605329999999999965 0.0616569999999999965 0.0641860000000000069 0.0664340000000000069\r\n[19] 0.1372529999999999861 0.1439970000000000139 0.1483819999999999861 0.1484930000000000139 0.1528779999999999861 0.1596220000000000139\r\n[25] 0.1640069999999999861 0.1641180000000000139 0.1685029999999999861 0.1752470000000000139 0.1796319999999999861 0.1797430000000000139\r\n[31] 0.1841279999999999861 0.1908720000000000139 0.1952569999999999861 0.1953680000000000139 0.1997529999999999861\r\n```","comments":["This is a duplicate of #5346. Since the issue seems to be platform specific, could you confirm that you are using Windows?","On an M1 Mac I get 62260 differences, the largest being 2.775558e-17","@jsocolar does that still happen when you install the current dev version?","@tlapak yes, on Windows. \r\n```> sessionInfo()\r\nR version 4.1.0 (2021-05-18)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19044)\r\n","When building with the latest version of rtool42 and using R 4.2.1 I can no longer reproduce the above example on the current dev version on Windows."],"labels":["duplicate"]},{"title":"Aliasing issue with `:=` affecting a different column","body":"The below triggers the bug for me (note the assignment to `col2` changing the value of `col1`!):\r\n\r\n```\r\nlibrary(data.table)\r\n\r\ncoalesce = function(x, ...) {\r\n  for (y in list(...)) {\r\n    idx = is.na(x)\r\n    x[idx] = if (length(y) != 1) y[idx] else y\r\n  }\r\n  x\r\n}\r\n\r\ndt = data.table(id=1:64, col1=0, col2=0)\r\nprint(dt[1, .(col1, col2)])\r\n#    col1 col2\r\n# 1:    0    0\r\ndt[, col1 := coalesce(col2, 111)]\r\ndt[, col2 := 999]\r\nprint(dt[1, .(col1, col2)])\r\n#    col1 col2\r\n# 1:  999  999\r\n```\r\n\r\nAnd my `sessionInfo()` output:\r\n```\r\nR version 4.0.5 (2021-03-31)\r\nPlatform: x86_64-w64-mingw32\/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19044)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252    LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      \r\n[5] LC_TIME=English_Australia.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods   base     \r\n\r\nother attached packages:\r\n[1] data.table_1.14.2\r\n\r\nloaded via a namespace (and not attached):\r\n[1] compiler_4.0.5 tools_4.0.5   \r\n```\r\n\r\nBasically it looks like col1 and col2 end up pointing at the same vector such that `:=` modifies them both; I'm guessing they are shared but the reference counts are off such that `:=` thinks it is safe to modify in-place. Not 100% clear to me if the actual underlying bug may be base R or data.table.\r\n\r\nWhen trying to put together a minimal repro, I noticed a few different changes that make this bug disappear:\r\n\r\n* Simply printing the data table between the col1 and col2 assignments makes the issue go away.\r\n\r\n* It only manifests where the number of rows is at least 64. Perhaps that is used as a threshold at which some sort of copy-on-write optimization logic is kicking in somewhere?\r\n\r\n* Also the problem seems to be related to the coalesce function used here, despite it not having any effect in this example. Eg replacing it with `coalesce = function(x, ...) x` avoids any issue. It seems as though base r is doing something weird with `[<-` with an all false logical subset; maybe the result is the same object but no longer marked as shared? Note that assigning to col1 after coalesce does not affect col2, only vice-versa. Alternatively returning `x[]` in coalesce bypasses the erroneous sharing by forcing a copy or bumping the ref count.\r\n\r\n","comments":["Thanks for reporting. You've found an interesting way to get R to return the result in an ALTREP wrapper. This is an optimisation where R avoids allocating memory (e.g. when you type 1:64 R doesn't actually allocate 64 integers immediately). In this case, R produces a wrapper that just points to the other column. As you have noticed, touching the column in any way gets this expanded and the effect disappears. As `data.table` circumvents many R mechanisms to achieve its efficiency, it goes to some lengths to catch these cases but currently misses this one.","Wow thanks @tlapak, incredibly quick diagnosis. Would have never expected ALTREP to be the issue either."],"labels":["bug"]},{"title":"Unexpected behavior in `.BY`","body":"Hi, \r\n\r\nwe are trying to extract the `.BY` information to a list object outside the current data.table. Unfortunately, we get unexpected results.\r\nAll elements of the list of length(by groups) contain the same value - the .BY information from the last group.\r\nWhen using .GRP instead, the list is populated with different values - the correct group indices. \r\n\r\nIs this expected behavior, and how can I get a list of correct by-group values be extracted?\r\n\r\nThanks!\r\n\r\n``` r\r\nlibrary(data.table)\r\n\r\ndt = data.table(l = sample(letters[1:5], 100, replace = TRUE), b = rnorm(100))\r\nsetkey(dt, l)\r\n\r\nbyg = list()\r\ngrp = list()\r\ndt[\r\n  , m := {\r\n    byg <<- append(byg, .BY)\r\n    grp <<- append(grp, .GRP)\r\n    mean(b)\r\n  }\r\n  , by = l\r\n]\r\n\r\nstr(byg)\r\n#> List of 5\r\n#>  $ l: chr \"e\"\r\n#>  $ l: chr \"e\"\r\n#>  $ l: chr \"e\"\r\n#>  $ l: chr \"e\"\r\n#>  $ l: chr \"e\"\r\nstr(grp)\r\n#> List of 5\r\n#>  $ : int 1\r\n#>  $ : int 2\r\n#>  $ : int 3\r\n#>  $ : int 4\r\n#>  $ : int 5\r\n\r\nsessionInfo()\r\n#> R version 4.2.0 (2022-04-22)\r\n#> Platform: x86_64-pc-linux-gnu (64-bit)\r\n#> Running under: Ubuntu 20.04.4 LTS\r\n#> \r\n#> Matrix products: default\r\n#> BLAS:   \/usr\/lib\/x86_64-linux-gnu\/blas\/libblas.so.3.9.0\r\n#> LAPACK: \/usr\/lib\/x86_64-linux-gnu\/lapack\/liblapack.so.3.9.0\r\n#> \r\n#> locale:\r\n#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \r\n#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \r\n#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \r\n#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \r\n#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \r\n#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \r\n#> \r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \r\n#> \r\n#> other attached packages:\r\n#> [1] data.table_1.14.2\r\n#> \r\n#> loaded via a namespace (and not attached):\r\n#>  [1] rstudioapi_0.13   knitr_1.39        magrittr_2.0.3    R.cache_0.15.0   \r\n#>  [5] rlang_1.0.2       fastmap_1.1.0     fansi_1.0.3       stringr_1.4.0    \r\n#>  [9] styler_1.7.0      highr_0.9         tools_4.2.0       xfun_0.31        \r\n#> [13] R.oo_1.24.0       utf8_1.2.2        cli_3.3.0         withr_2.5.0      \r\n#> [17] htmltools_0.5.2   ellipsis_0.3.2    yaml_2.3.5        digest_0.6.29    \r\n#> [21] tibble_3.1.7      lifecycle_1.0.1   crayon_1.5.1      purrr_0.3.4      \r\n#> [25] R.utils_2.11.0    vctrs_0.4.1       fs_1.5.2          glue_1.6.2       \r\n#> [29] evaluate_0.15     rmarkdown_2.14    reprex_2.0.1      stringi_1.7.6    \r\n#> [33] compiler_4.2.0    pillar_1.7.0      R.methodsS3_1.8.1 pkgconfig_2.0.3\r\n```\r\n\r\n<sup>Created on 2022-05-23 by the [reprex package](https:\/\/reprex.tidyverse.org) (v2.0.1)<\/sup>\r\n","comments":["Seems more to be an issue of the collecting procedure, since printing the values works as expected.\r\n\r\n```R\r\nlibrary(data.table)\r\ndt = data.table(l = sample(letters[1:5], 100, replace = TRUE), b = rnorm(100))\r\nsetkey(dt, l)\r\n\r\ndt[\r\n  , m := {\r\n    cat(\"BY:\", as.character(.BY), \"\\n\")\r\n    cat(\"GRP:\", as.character(.GRP), \"\\n\")\r\n    mean(b)\r\n  }\r\n  , by = l\r\n]\r\n#> BY: a \r\n#> GRP: 1 \r\n#> BY: b \r\n#> GRP: 2 \r\n#> BY: c \r\n#> GRP: 3 \r\n#> BY: d \r\n#> GRP: 4 \r\n#> BY: e \r\n#> GRP: 5\r\n```\r\n","It seems that `R`s lazy evaluation comes here into play. Forcing evaluation counters this.\r\n\r\n``` r\r\nlibrary(data.table)\r\n\r\ndt = data.table(l = sample(letters[1:5], 100, replace = TRUE), b = rnorm(100))\r\nsetkey(dt, l)\r\n\r\nbyg = list()\r\ngrp = list()\r\ndt[\r\n  , m := {\r\n    byg <<- append(byg, force(unlist(.BY)))\r\n    grp <<- append(grp, .GRP)\r\n    mean(b)\r\n  }\r\n  , by = l\r\n]\r\nstr(byg)\r\n#> List of 5\r\n#>  $ l: chr \"a\"\r\n#>  $ l: chr \"b\"\r\n#>  $ l: chr \"c\"\r\n#>  $ l: chr \"d\"\r\n#>  $ l: chr \"e\"\r\nstr(grp)\r\n#> List of 5\r\n#>  $ : int 1\r\n#>  $ : int 2\r\n#>  $ : int 3\r\n#>  $ : int 4\r\n#>  $ : int 5\r\n```","Thanks for your comments @ben-schwen .\r\n\r\nWhen using `unlist()`, forcing is not required. \r\nHowever, I haven't found a solution for getting back `.BY` without modifying it before (I do not want to unlist due to coercion). \r\nMy feeling is, that when sending the `.BY` to a function using some src code (e.g. `unlist`, `purrr::flatten`), it works. \r\n\r\nIs this expected? Any ideas for just returning the `.BY` lists?\r\n \r\n``` r\r\nlibrary(data.table)\r\n\r\ndt = data.table(\r\n  l = sample(letters[1:2], 100, replace = TRUE)\r\n  , n = sample(1L:2L, 100, replace = TRUE)\r\n  , b = rnorm(100)\r\n)\r\n\r\nkeys = c(\"l\", \"n\")\r\nsetkeyv(dt, keys)\r\n\r\nby_list = list()\r\nby_unlist = list()\r\nby_flatten = list()\r\ndt[\r\n  , m := {\r\n    by_list <<- append(by_list, list(.BY))\r\n    by_unlist <<- append(by_unlist, list(unlist(.BY)))\r\n    by_flatten <<- append(by_flatten, list(purrr::flatten(.BY)))\r\n    mean(b)\r\n  }\r\n  , by = keys\r\n]\r\n\r\nstr(by_list)\r\n#> List of 4\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 2\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 2\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 2\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 2\r\nstr(by_unlist)\r\n#> List of 4\r\n#>  $ : Named chr [1:2] \"a\" \"1\"\r\n#>   ..- attr(*, \"names\")= chr [1:2] \"l\" \"n\"\r\n#>  $ : Named chr [1:2] \"a\" \"2\"\r\n#>   ..- attr(*, \"names\")= chr [1:2] \"l\" \"n\"\r\n#>  $ : Named chr [1:2] \"b\" \"1\"\r\n#>   ..- attr(*, \"names\")= chr [1:2] \"l\" \"n\"\r\n#>  $ : Named chr [1:2] \"b\" \"2\"\r\n#>   ..- attr(*, \"names\")= chr [1:2] \"l\" \"n\"\r\nstr(by_flatten)\r\n#> List of 4\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"a\"\r\n#>   ..$ n: int 1\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"a\"\r\n#>   ..$ n: int 2\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 1\r\n#>  $ :List of 2\r\n#>   ..$ l: chr \"b\"\r\n#>   ..$ n: int 2\r\n```\r\n\r\n<sup>Created on 2022-05-27 by the [reprex package](https:\/\/reprex.tidyverse.org) (v2.0.1)<\/sup>\r\n\r\n\r\n","Please let me know, if you think this should better discussed on Stackoverflow. Thanks","If you take a look at the source of `dogropus` https:\/\/github.com\/Rdatatable\/data.table\/blob\/e9a323de01a17af70d5316016606fa8d35b25023\/src\/dogroups.c#L91-L104\r\nyou can see that BY is always directly assigned and overwritten for each group, so with assigning it to a list, you always assign the same memory pointer.\r\n\r\nThis is also confirmed by the following.\r\n```R\r\nlibrary(data.table)\r\ndt = data.table(l = sample(letters[1:5], 100, replace = TRUE), b = rnorm(100))\r\nsetkey(dt, l)\r\n\r\nby_list = list()\r\n# you should preallocate in R\r\n# by_list = vector(mode=\"list\", length(unique(dt$l)))\r\n\r\ndt[\r\n  , m := {\r\n    by_list <<- append(by_list, copy(.BY))\r\n    cat(address(.BY), \"\\n\")\r\n    mean(b)\r\n  }\r\n  , by = l\r\n]\r\n#> 0x56343a667930 \r\n#> 0x56343a667930 \r\n#> 0x56343a667930 \r\n#> 0x56343a667930 \r\n#> 0x56343a667930\r\nby_list\r\n#> $l\r\n#> [1] \"a\"\r\n#> \r\n#> $l\r\n#> [1] \"b\"\r\n#> \r\n#> $l\r\n#> [1] \"c\"\r\n#> \r\n#> $l\r\n#> [1] \"d\"\r\n#> \r\n#> $l\r\n#> [1] \"e\"\r\n```\r\nYou can achieve your wanted behavior with using `copy(.BY)`. Similar info is in the help of `?.BY` but it only mentions this behavior for `.SD`.\r\n\r\nThe working assignment without copying probably depends on whether R is internally copying or not. In previous R versions, it would always copy but R-core improved this a lot in the last years.\r\n\r\nAnyway, we should clarify this in the documentation. So I guess here is the right place to discuss it."],"labels":["documentation"]},{"title":"FR: Allow `patterns` to have the arguments of `grep`","body":"Beyond the 2 essential arguments of `grep`, most of the other would be also useful if they were included in `patterns`. In fact, any other except `value` could help to look for much other patterns. \r\n\r\nIn particular, it is needed for certain #regex which only work under `perl=TRUE`.\r\n\r\nSo I propose to modify\r\n```r\r\npatterns = function(..., cols=character(0L)) {\r\n  # if ... has no names, names(list(...)) will be \"\";\r\n  #   this assures they'll be NULL instead\r\n  L = list(...)\r\n  p = unlist(L, use.names = any(nzchar(names(L))))\r\n  if (!is.character(p))\r\n    stopf(\"Input patterns must be of type character.\")\r\n  matched = lapply(p, grep, cols)\r\n  # replace with lengths when R 3.2.0 dependency arrives\r\n  if (length(idx <- which(sapply(matched, length) == 0L)))\r\n    stopf('Pattern(s) not found: [%s]', brackify(p[idx]))\r\n  matched\r\n}\r\n```\r\nwith something like\r\n```r\r\npatterns = function(..., cols=character(0L), ignore.case = FALSE, perl = FALSE, fixed = FALSE, useBytes = FALSE, invert = FALSE) {\r\n  # if ... has no names, names(list(...)) will be \"\";\r\n  #   this assures they'll be NULL instead\r\n  L = list(...)\r\n  p = unlist(L, use.names = any(nzchar(names(L))))\r\n  if (!is.character(p))\r\n    stopf(\"Input patterns must be of type character.\")\r\n  matched = lapply(p, grep, cols, ignore.case = ignore.case, perl = perl, fixed = fixed, useBytes = useBytes, invert = invert)\r\n  # replace with lengths when R 3.2.0 dependency arrives\r\n  if (length(idx <- which(sapply(matched, length) == 0L)))\r\n    stopf('Pattern(s) not found: [%s]', brackify(p[idx]))\r\n  matched\r\n}\r\n```\r\n\r\nThanks!","comments":["Hi @iago-pssjd thanks for the feature request. That makes sense to support some of the same arguments as grep, for user convenience.\r\nCan you please give a simple use case which illustrates what you are trying to do?\r\n\r\nFor case insensitive matching (ignore.case argument of grep function) you can actually get equivalent functionality using the current data.table::patterns code by just changing your pattern, as described in the TRE docs, https:\/\/laurikari.net\/tre\/documentation\/regex-syntax\/#options\r\n```r\r\n> melt(data.table(iris[1,]),measure.vars=patterns(\"sepal\"))\r\nError: Pattern(s) not found: [[sepal]]\r\n> melt(data.table(iris[1,]),measure.vars=patterns(\"(?i)sepal\"))\r\n   Petal.Length Petal.Width Species     variable value\r\n          <num>       <num>  <fctr>       <fctr> <num>\r\n1:          1.4         0.2  setosa Sepal.Length   5.1\r\n2:          1.4         0.2  setosa  Sepal.Width   3.5\r\n```","Hi @tdhock \r\nYes I can give an example. It is the use of negative (and positive) lookahead (and lookbehind). Indeed, (even if this is not the best example, where I wouldn't need this kind of regex)\r\n```r\r\n> iris <- data.table(iris)\r\n> iris[, .SD, .SDcols = patterns(\"^Sepal\\\\.(?!W)\")]\r\nError in FUN(X[[i]], ...) : \r\n  invalid regular expression '^Sepal\\.(?!W)', reason 'Invalid regexp'\r\nIn addition: Warning message:\r\nIn FUN(X[[i]], ...) : TRE pattern compilation error 'Invalid regexp'\r\n> iris[, .SD, .SDcols = grep(\"^Sepal\\\\.(?!W)\", names(iris))]\r\nError in grep(\"^Sepal\\\\.(?!W)\", names(iris)) : \r\n  invalid regular expression '^Sepal\\.(?!W)', reason 'Invalid regexp'\r\nIn addition: Warning message:\r\nIn grep(\"^Sepal\\\\.(?!W)\", names(iris)) :\r\n  TRE pattern compilation error 'Invalid regexp'\r\n> iris[, .SD, .SDcols = grep(\"^Sepal\\\\.(?!W)\", names(iris), perl = TRUE)]\r\n     Sepal.Length\r\n  1:          5.1\r\n  2:          4.9\r\n  3:          4.7\r\n  4:          4.6\r\n  5:          5.0\r\n ---             \r\n146:          6.7\r\n147:          6.3\r\n148:          6.5\r\n149:          6.2\r\n150:          5.9\r\n```\r\nso It would be great to have the chance of using `iris[, .SD, .SDcols = patterns(\"^Sepal\\\\.(?!W)\", perl = TRUE)]`.\r\n\r\nThanks, and also for the link about TRE.","Also I would have expected this to work, \r\n```r\r\n> data.table(iris)[, .SD, .SDcols=function(cols)grepl(\"^Sepal\\\\.(?!W)\",cols,perl=TRUE)]\r\nErreur : When .SDcols is a function, it is applied to each column; the output of this function must be a non-missing boolean scalar signalling inclusion\/exclusion of the column. However, these conditions were not met for: [Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species]\r\n```\r\nThe reason I thought the above code should work is because patterns works by substituting column names into cols argument, `patterns = function (..., cols = character(0L))` but the current logic is \"if .SDcols is patterns, then substitute column names into cols argument of patterns and run it, else run the .SDcols function on each vector of data.\" Below is current code from data.table.R, and I would suggest changing first line from \"if .SDcols is patterns\" to \"if .SDcols is a function with argument named cols\" which would be true for patterns and my example above, but false for other use cases of .SDcols functions (is.numeric etc which have function name x rather than cols).\r\n```r\r\n            if (colsub %iscall% 'patterns') {\r\n              # each pattern gives a new filter condition, intersect the end result\r\n              .SDcols = Reduce(intersect, eval_with_cols(colsub, names_x))\r\n            } else {\r\n              .SDcols = eval(colsub, parent.frame(), parent.frame())\r\n              # allow filtering via function in .SDcols, #3950\r\n              if (is.function(.SDcols)) {\r\n                .SDcols = lapply(x, .SDcols)\r\n                if (any(idx <- vapply_1i(.SDcols, length) > 1L | vapply_1c(.SDcols, typeof) != 'logical' | vapply_1b(.SDcols, anyNA)))\r\n                  stopf(\"When .SDcols is a function, it is applied to each column; the output of this function must be a non-missing boolean scalar signalling inclusion\/exclusion of the column. However, these conditions were not met for: %s\", brackify(names(x)[idx]))\r\n```"],"labels":["feature request"]},{"title":"dcast: add argument to error on \"Aggregate function missing, defaulting to 'length'\"","body":"We all _love_ seeing `Aggregate function missing, defaulting to 'length'` as it usually indicates a bug (possibly upstream resulting in unexpected input). I really need this to be an error but can't even use `options(warn = 2)` because `dcast` only gives a message and not a warning.\r\n\r\nI suggest modifying `dcast.data.table`. One easy option would be a `strict` argument that defaults to `FALSE` but promotes the message to an error if set to `TRUE`. If we want to avoid additional arguments, we could have `fun.aggregate` accept `NA` as input with the same result.\r\n\r\nRight now the best way to catch this case appears to be `stopifnot(anyDuplicated(DT[, .(LHS, RHS)]) == 0L)` but I dislike littering my code with this. ","comments":["Upvoted. Any solution seems better than the current state. Even simply changing the 'message' to 'warning' would make debugging a lot easier if we've got a number of calls to dcast"],"labels":["feature request","reshape"]}]
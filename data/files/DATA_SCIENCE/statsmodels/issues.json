[{"title":"BUG: Fix unequal varcorrection in tukeyhsd","body":"- [ ] tests added \/ passed. \r\n- [x] code\/documentation is well formatted.  \r\n- [x] properly formatted commit message. See \r\n      [NumPy's guide](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/dev\/gitwash\/development_workflow.html#writing-the-commit-message). \r\n\r\n<details>\r\n\r\nlike tests in try_tukey_hsd.py\r\nWith the first dataset, results should be the same, as below.\r\n\r\n![image](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/56634448\/9b070938-dac8-4e8a-ac5e-eef711c00477)\r\n\r\n\r\n<\/details>\r\n","comments":["Great, thanks for finding this mistake.\r\n\r\nDo you have an example that uses this code path?\r\nI don't remember now where this is used, and it does not have unit test coverage"],"labels":["prio-high","type-bug","comp-stats"]},{"title":"BUG\/ENH: broken code in RLM._scale_est, scale callback option is missing","body":"`_scale_est` method has an else path that uses an undefined variable `scale.scale_est`. `scale` refers to the module robust.scale but there is no scale_est.\r\n\r\n```\r\n        else:\r\n            return scale.scale_est(self, resid) ** 2\r\n```\r\n\r\nMy guess is that this should be `return self.scale_est(self, resid) ** 2`\r\nwhich would allow scale_est to be a callable.\r\n\r\nObviously, we don't have a unit test for that path.\r\n\r\nThis is more an ENH than bug, because callable `scale_est` is not in the docstring of the fit method.\r\n\r\n```\r\n        scale_est : str or HuberScale()\r\n            'mad' or HuberScale()\r\n            Indicates the estimate to use for scaling the weights in the IRLS.\r\n            The default is 'mad' (median absolute deviation.  Other options are\r\n            'HuberScale' for Huber's proposal 2. Huber's proposal 2 has\r\n            optional keyword arguments d, tol, and maxiter for specifying the\r\n            tuning constant, the convergence tolerance, and the maximum number\r\n            of iterations. See statsmodels.robust.scale for more information.\r\n```\r\n\r\n\r\nMotivation:\r\nI'm looking at reusing RLM for S-estimator for regression.\r\n\r\nS-estimator for regression satisfies locally the moment conditions of RLM, M-estimator for the mean parameters.\r\nAdditionally, an S-estimator needs an M-scale based on a redescending rho function.\r\nI'm not sure if the `HuberScale` is the correct M-scale for the S-estimator.\r\nWe need a scale_est callback function, so that we can compute the scale with a user chosen norm\/rho for the M-scale.\r\nThe norm in the M-scale needs to be the same as the norm of the RLM mean parameter model. In the current RLM those two are separately chose. \r\n\r\nI guess, RLM can provide the local solution for the S-estimator for given start_params. However, the S-estimator needs a global solution that minimizes the scale, i.e. choose the local solution that has the smallest estimated M-scale.\r\n\r\nIf this works, the Fast-S or Det-S regression only needs to handle the global problem, i.e. choose several good or many random starting points and pick the best global solution.  \r\n\r\n\r\nMore generically:\r\nCurrent RLM when used with a redescending norm computes a local optimum chosen through starting values based on the OLS estimate.\r\nThere is currently no attempt at finding a \"good\" local optimum.\r\n(AFAIR, I have not seen a reference that would use the rho at the local optimum to chose between local optima. We need an extra criterion like minimizing M-scale across local optima in the case of S-estimators.)\r\n\r\nSomething to try:\r\nUse mad scale with redescending norm in RLM and do global optimization, e.g. those in scipy.optimize or by random initializations.\r\nThis should have a high breakdown point, but very low efficiency.\r\n(However, RLM.fit has currently only irls method and no scipy.optimizers)\r\n\r\n\r\n**update**\r\n\r\nAFAICS, HuberScale is a winsorized scale\r\nThis is the same as the rho function of TrimmedMean (called Talwar in Menenez et al 2021, and IIRC in SAS documentation)\r\ni.e. RLM currently has a M-scale based on redescending rho.\r\n(\r\nI have no idea what are the breakdown point and relative efficiency when we also use a redescending norm for mean parameters, and use global optimization.\r\n`scale_bias` correction term is `h` in HuberScale computed in `__init__` which would determine efficiency and breakdown point for given threshold `c` for an S-estimator with TrimmedMean norm, which might be close to LTS.\r\n)\r\n\r\nAside: (I don't remember which textbook mentioned this as comment.)\r\n\r\nThe scale_bias correction for M-scale and S-estimators is calibrated to achieve a specified breakdown point and implied efficiency for a given reference distribution.\r\nIf the actual distribution is different from the reference distribution, then the breakdown point will differ from the intended one. However, mean estimates are still consistent (assuming symmetric distribution, as usual)\r\nScale estimate will not be calibrated for the normal distribution, but will still be a valid (?) scale estimate.\r\n\r\n\r\n","comments":["checking HuberScale versus TrimmedMean\r\n\r\nsecond part is copied from `HuberScale.__init__`, dropping `df_resid \/ nobs` from `h`\r\nResults for `scale_bias` agrees with numerical expectation\/integration at high precision\r\n\r\n```\r\nstats.norm.expect(lambda x : norms.TrimmedMean(c=2.5).rho(x))\r\n0.4887799917273257\r\n\r\nd = 2.5\r\n(\r\nd ** 2\r\n+ (1 - d ** 2) * stats.norm.cdf(d)\r\n- 0.5\r\n- d \/ (np.sqrt(2 * np.pi)) * np.exp(-0.5 * d ** 2)\r\n)\r\n0.4887799917264034\r\n```\r\nIt looks like breakdown point is only around 0.15, with 90% relative efficiency\r\n```\r\n0.4887799917273257 \/ norms.TrimmedMean(c=2.5).rho(2.5)\r\n0.15640959735274423\r\n\r\n1 \/ var_normal_jump(norms.TrimmedMean(c=2.5))\r\n0.8999391668109229\r\n```\r\n\r\naside: comparing with TukeyBiweight, 91.7% efficiency at breakdown point 0.15\r\n`print(bp, eff, c_bp, b)`\r\n`0.15 0.917435422551397 4.096255394231153 0.4194827063691953`\r\n\r\nSo I think we can add a generic MScale class for user chosen norm (rho) with the same interface as HuberScale for compatibility in RLM. And with option to either tune for breakdown point or for efficiency.\r\n\r\n","and another piece again\r\nsingle objective function for joint optimization of loc parameter and scale\r\n\r\nHampel et all book p. 312 Remark 2 on Huber Proposal two\r\n\r\nobjective function:   `(rho((y - x * theta) \/ s) + b ) * s`\r\nfirst order condition for s: chi(r) - b = 0, moment condition for mean parameters psi(.) = 0\r\nwhere `chi(x) = r * psi(x) - rho(r) - b`\r\n(alternative is to use a different chi function in the moment condition for scale without having a matching objective function)\r\n\r\nquick check\r\nThe implied chi function for the scale of HuberT norm for mean parameters is the TrimmedMean rho function used in HuberScale, although with a different threshold parameter.\r\n```\r\nnorm = norms.HuberT()\r\nchi =(x * norm.psi(x) - norm.rho(x))\r\nrho_tm = norms.TrimmedMean(c=norm.t).rho(x)\r\nprint(np.max(np.abs(xhi - rho_tm)))\r\nplt.plot(x, xhi)\r\nplt.plot(x, rho_tm, '--')\r\n1.1102230246251565e-16\r\n...\r\n```\r\n\r\nI went through this joint optimization in the context of nonlinear models, Huber-Dutter (PR ???)\r\n\r\nSo we have potentially several options for scale method\r\n\r\n- fixed scale from preliminary estimator, e.g. MM-estimation or One-step (truncated M-estimator iterations)\r\n- separately defined moment conditions (that's what RLM currently does), no single objective function that includes scale estimate\r\n- Huber: single objective function, chi is implied by norm\r\n- S-estimator: scale function is rho function of norm for mean parameters\r\n\r\nFor the case of nonlinear mean function, I had tried to get a single objective function, but that is not needed.\r\n\r\nestimation algorithm that uses scipy optimizers could be:\r\n\r\n-  single objective function for both mean parameters and scale. Does not allow separately define moment conditions\r\n- use rootfinding algorithms, i.e. solve moment conditions directly without objective function\r\n- use current alternating optimization, This can be extended to using scipy.optimize similar to nonlinear least square, i.e. alternate between nonlinear mean estimation and nonlinear scale estimation. This treats scale as auxiliary parameter as in OLS\/WLS and GLM.\r\n\r\nIn the last case, we only have to replace `fit` in RLM by nonlinear optimization  (or use nonlinear WLS in irls) and we could reuse separate scale estimation.\r\nThe same would apply to S-estimation once we extend RLM and scale methods.\r\n\r\n**update**\r\nvariation on alternating optimization, mainly performance related\r\n\r\nAlternating optimization requires outer iteration loop. How many inner optimization iterations are we using.\r\n\r\n- joint iteration, only one-step updating of each parameter in the iteration loop\r\n  - example robust.scale.Huber only does one step updating of scale in iteration loop (AFAICS) \r\n- alternating full optimization: within each outer iteration we update each parameter to convergence\r\n  - current RLM fully updates scale, or at least has large maxiter in the scale estimation \r\n- k-step updating: in Fast-S and other Fast-XXX, inner optimization has small maxiter, e.g. 2 or 5, which can differ for mean parameters and scale or other parameters. (Several articles mention that scale computation is expensive, e.g. when using q_n scale)\r\n\r\nNote, convergence occurs in one-step for linear mean parameters, e.g. WLS or weighted average, for given scale.\r\n\r\nCurrent RLM has no option to reduce maxiter for scale estimation during optimization. It also does not inform the user about whether scale optimization in HuberScale itself has converged, although the RLM convergence requires that params don't change anymore (up to tolerance), which will require extra RLM iteration steps if scale has not converged yet.\r\n\r\nFor Fast-X this performance improvement are relevant because the number of random starts has to be large or very large.\r\nFor Det-X and single optimization runs will not be affected as much ","another question for which I don't know the answer yet.\r\n\r\nWhat is the scale used in cov_params for two stage estimators (high breakdown followed by high efficiency estimators) like MM-estimators?\r\nDo we use scale from first stage also for inference or do we reestimate the scale after using fixed scale during optimization?\r\n\r\ne.g. MM-estimators: M-estimator with fixed scale following a preliminary S-estimator.\r\n\r\n- we keep the initial scale\r\n- we reestimate using the initial S-estimator rho function\r\n- we reestimate using the second stage rho or psi function\r\n- we reestimate using a separately chosen rho or psi function\r\n- or just treat it as a generic M-estimator and use the sandwich for the given score\/hessian (moment condition and derivatives) treating initial scale which is fixed in second stage as \"nonrandom\".\r\n\r\nTheoretically they all estimate the same scale.\r\nAlso, it does not affect the parameter estimate, it affects only (small sample) inference.\r\n\r\nI guess I have to look more at the outlier robust inference literature for details and (small sample) behavior of different estimators.\r\n\r\n(AFAICS, efficiency discussion usually looks at mean parameter estimation. I don't remember seeing much on breakdown point efficiency of the scale estimate and of inference. However, I never looked much directly at the inference literature.)\r\n\r\nrelated:\r\nVan Aelst, Stefan, Gert Willems, and Ruben H. Zamar. 2013. \u201cRobust and Efficient Estimation of the Residual Scale in Linear Regression.\u201d Journal of Multivariate Analysis 116 (April): 278\u201396. https:\/\/doi.org\/10.1016\/j.jmva.2012.12.008.\r\n\r\nmaybe this is a starting point (I did not look at it):\r\nKoller, Manuel, and Werner A. Stahel. 2011. \u201cSharpening Wald-Type Inference in Robust Regression for Small Samples.\u201d Computational Statistics & Data Analysis 55 (8): 2504\u201315. https:\/\/doi.org\/10.1016\/j.csda.2011.02.014.\r\n302 citations in google scholar (citations might be mainly for their `norms`, weight functions)\r\n"],"labels":["type-bug","type-enh","comp-robust"]},{"title":"TST: add weights(0) == 1 unit test to test_norms_consistent","body":"see #8801 and #8808\r\n\r\nwe don't have a unit test in the consistency checks that rho is scaled so that weights(0) = 0\r\n\r\nwe can just add something similar to the rho check:\r\n\r\n```\r\n    # check location and u-shape of rho\r\n    assert_allclose(rho[4], 0, atol=1e-12)\r\n    assert np.all(np.diff(rho[4:]) >= 0)\r\n    assert np.all(np.diff(rho[:4]) <= 0)\r\n```\r\n\r\n(I'm waiting to get rid of my current PR checkouts before fixing this. Needs to be done before adding new norm classes.)\r\n","comments":[],"labels":["type-enh","comp-robust","type-test"]},{"title":"ENH: distance measures for two covariance, correlation or scatter matrices","body":"I'm running into various measure for the closeness of two covariance or scatter matrices.\r\n\r\nWe should get a function with a collection of those.\r\n\r\ncurrent usecase: checking how well a robust cov or scatter estimate estimates the true\/dgp covariance or how it compares to standard pearson covariance in the clean gaussian case.\r\n\r\nThe one I usually use is mse (Frobenious norm of difference cov1 - cov2 divided by nobs).\r\nOthers are Kullback-Leibler distance, or some on range of eigenvalues or trace of cov1^{-1} @ cov2.\r\nNote KL and some of the others are not \"norms\", e.g. symmetry may not hold.\r\n\r\nOther options should be to compare shapes only, i.e. scatter matrices with arbitrary scale (e.g. standardize scaling to satisfy trace or determinant constraint).\r\n\r\nhttps:\/\/stats.stackexchange.com\/questions\/14673\/measures-of-similarity-or-distance-between-two-covariance-matrices\r\n\r\ne.g. based on second answer on stats.stackexchange\r\nfrom https:\/\/www.researchgate.net\/publication\/4194743_Correlation_Matrix_Distance_a_Meaningful_Measure_for_Evaluation_of_Non-Stationary_MIMO_Channels equation (4)\r\n\r\nnot subtracting means shows smaller distance than using np.corrcoeff\r\napplied to x1, x2 being raveled covariance estimates.\r\n\r\nNote: corrdist is zero if two cov are the same except for a scale factor, i.e. should be fine for scatter and shape matrices.\r\n```\r\ndef corrdist(x1, x2):\r\n    \"\"\"correlation distance between two vectors\r\n\r\n    This correspond to one minus the correlation coefficient under the assumption that means are zero.\r\n    \"\"\"\r\n    if x1.ndim !=1 or x2.ndim !=1:\r\n        raise ValueError(\"data should be 1-dimensional\")\r\n    cross = x1.T @ x2 \r\n    s1 = (x1**2).sum(0)\r\n    s2 = (x1**2).sum(0)\r\n    cmd = 1 - cross \/ np.sqrt(s1 * s2)\r\n    return cmd\r\n```","comments":["Another reference in the stackexchange answers might also be interesting for looking at where the differences are\r\nhttps:\/\/bmcecolevol.biomedcentral.com\/articles\/10.1186\/1471-2148-12-222#CR21_2258\r\n\r\ncompares principal components based on own and foreign covariance eigenvalue decomposition, AFAICS based on brief look."],"labels":["comp-robust","comp-stats","comp-tools","comp-multivariate"]},{"title":"Constraining state-space model parameters between certain bounds ","body":"Hi,\r\n\r\nI am currently using statsmodels to make state-space models using statsmodels.tsa.statespace.structural.UnobservedComponents and fit these models to my data using maximum likelihood estimation. However, an unconstrained fit leads to overfitting for some of my time series that are very eratic. This occurs because one of the model variance parameters (either sigma2.trend or sigma2.cycle depending on the dataset) becomes very large, allowing for large fluctuations in the corresponding model component, resulting in overfitting. At the same time, the sigma2.irregular parameter, which can be seen as the variance of the noise, is set to almost zero.\r\n\r\nI want to constrain my sigma2.irregular parameter to take on values between certain bounds, but when using statsmodels' .fit_constrained functionality, it seems like you can only fix parameters to one given value. Is there any other way to constrain parameters to be somewhere between a minimum and maximum value?","comments":["There is no build-in way to do this, but you can define a subclass that implements bounds using parameter transformations.  Here is an example:\r\n\r\n```python\r\nimport statsmodels.api as sm\r\n\r\ndef transform_to_range(unconstrained, low, high):\r\n    constrained = (\r\n        1 \/ (1 + np.exp(-unconstrained))\r\n    ) * (high - low) + low\r\n\r\n    return constrained\r\n\r\ndef untransform_to_range(constrained, low, high):\r\n    x = (constrained - low) \/ (high - low)\r\n    unconstrained = np.log(x \/ (1 - x))\r\n\r\n    return unconstrained\r\n\r\nclass UCwithBounds(sm.tsa.UnobservedComponents):\r\n    def __init__(self, *args, bounds=None, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n\r\n        if bounds is None:\r\n            bounds = {}\r\n        self.bounds = bounds\r\n    \r\n    def transform_params(self, unconstrained):\r\n        # Perform the usual transformations\r\n        constrained = super().transform_params(unconstrained)\r\n\r\n        # Perform bounds transformations\r\n        param_names = list(self.param_names)\r\n        for key, (low, high) in self.bounds.items():\r\n            ix = param_names.index(key)\r\n            constrained[ix] = transform_to_range(unconstrained[ix], low, high)\r\n        return constrained\r\n\r\n    def untransform_params(self, constrained):\r\n        # Perform the usual reverse transformations\r\n        unconstrained = super().untransform_params(constrained)\r\n\r\n        # Perform bounds reverse transformations\r\n        param_names = list(self.param_names)\r\n        for key, (low, high) in self.bounds.items():\r\n            ix = param_names.index(key)\r\n            unconstrained[ix] = untransform_to_range(constrained[ix], low, high)\r\n        return unconstrained\r\n\r\n    @property\r\n    def start_params(self):\r\n        # Ensure that the starting parameters are not outside of the bounds\r\n        start_params = super().start_params\r\n        param_names = list(self.param_names)\r\n        for key, (low, high) in self.bounds.items():\r\n            # Use a pad so that we never set the starting parameters\r\n            # exactly on the bounds\r\n            pad = 0.1 * (high - low)\r\n            ix = param_names.index(key)\r\n            value = start_params[ix]\r\n            if value >= high:\r\n                start_params[ix] = high - pad\r\n            if value <= low:\r\n                start_params[ix] = low + pad\r\n        return start_params\r\n```\r\n\r\nand then you can use it like:\r\n\r\n```python\r\ndf = sm.datasets.macrodata.load_pandas().data\r\ndf.index = pd.period_range(start='1959Q1', end='2009Q3', freq='Q')\r\nmod = UCwithBounds(df['infl'], 'llevel', bounds={\r\n    'sigma2.irregular': (0.01, 5),\r\n    'sigma2.level': (0.2, 0.6)\r\n})\r\nres = mod.fit(disp=False)\r\nprint(res.summary())\r\n```","Using the code you provided, it seems the parameter will always takes on the minimum of the given bounds. \r\n\r\nI tested it with another dataset where I don't encounter the overfitting issue I mentioned earlier. An unconstrained fit to this dataset results in an estimated sigma2.irregular of 71.48. When I then provide constraints between 10 and 100 using UCwithBounds, I would expect to obtain the same sigma2.irregular of 71.48, but instead the estimated sigma2.irregular becomes 10.0014 (only very slightly greater than the minimum bound). \r\n\r\nThe same occurs regardless of what bounds I provide; the parameter always goes to the minimum bound. Is this caused by an error in the UCwithBounds class or am I doing something wrong in my implementation below?\r\n\r\n`model_settings = {\r\n\r\n    'level': True, \r\n    'stochastic_level': False, \r\n    'trend': True, \r\n    'stochastic_trend': True,\r\n    'irregular': True,\r\n    'use_exact_diffuse': True,\r\n    'freq_seasonal': [{'period': 12, 'harmonics': 1}],\r\n    'stochastic_seasonal': True,\r\n    'cycle': True,\r\n    'damped_cycle': False,\r\n    'stochastic_cycle': True,\r\n    'cycle_period_bounds': [5, 5.5], #SA tidal alias\r\n    'autoregressive': 0\r\n\r\n     }\r\n constraints = {'sigma2.irregular': (10,100)},\r\n\r\n model_unconstrained = UCwithBounds(data, **model_settings, bounds = {})\r\n model_constrained = UCwithBounds(data, **model_settings, bounds = constraints)\r\n\r\n fit_unconstrained = model_unconstrained.fit()\r\n fit_constrained = model_constrained.fit()\r\n\r\n print(fit_unconstrained.summary())\r\n print(fit_constrained.summary())\r\n`"],"labels":["comp-tsa-statespace","question"]},{"title":"ENH: outlier robust extreme value statistics and distributions","body":"parking some references, I did not look at them\r\n\r\ntopic: outlier robust estimation of highly skewed distributions.\r\nrelated issues, e.g. #8690 #9148 #9146 #9128\r\n\r\nMost standard theory for outlier robust estimation requires (approximate) symmetry of the distribution.\r\nExtreme value and lifetime models require highly asymmetric, skewed distribution.\r\nSo, this looks like a good literature to see what outlier robust approaches are used in the highly skewed cases.\r\n(e.g. minimum distance is used based on some abstracts #7412 #7440)\r\n(I'm more interested in generic approaches, we don't have EVT yet.\r\nLiterature search triggered by bias correction for m-estimators that are not fisher consistent in Dupuis and Morgenthaler in #9148)\r\n\r\nsome disadvantage: literature is largely for estimating distribution (constant) parameters and not for regression.\r\n\r\none possibly interesting topic: distinguishing heavy tailed \"observations\" and outliers\r\n\r\nAlfons, Andreas, Matthias Templ, and Peter Filzmoser. \u201cRobust Estimation of Economic Indicators from Survey Samples Based on Pareto Tail Modelling.\u201d Journal of the Royal Statistical Society Series C: Applied Statistics 62, no. 2 (March 1, 2013): 271\u201386. https:\/\/doi.org\/10.1111\/j.1467-9876.2012.01063.x.\r\nAndria, Joseph. \u201cA Computational Proposal for a Robust Estimation of the Pareto Tail Index: An Application to Emerging Markets.\u201d Applied Soft Computing 114 (January 1, 2022): 108048. https:\/\/doi.org\/10.1016\/j.asoc.2021.108048.\r\nBhattacharya, Shrijita, Francois Kamper, and Jan Beirlant. \u201cOutlier Detection Based on Extreme Value Theory and Applications.\u201d Scandinavian Journal of Statistics 50, no. 3 (2023): 1466\u20131502. https:\/\/doi.org\/10.1111\/sjos.12665.\r\nDupuis, D. J., and C. A. Field. \u201cRobust Estimation of Extremes.\u201d Canadian Journal of Statistics 26, no. 2 (1998): 199\u2013215. https:\/\/doi.org\/10.2307\/3315505.\r\nDupuis, Debbie J., and Stephan Morgenthaler. \u201cRobust Weighted Likelihood Estimators with an Application to Bivariate Extreme Value Problems.\u201d Canadian Journal of Statistics 30, no. 1 (2002): 17\u201336. https:\/\/doi.org\/10.2307\/3315863.\r\nDupuis, D.J. \u201cExceedances over High Thresholds: A Guide to Threshold Selection.\u201d Extremes 1, no. 3 (January 1, 1999): 251\u201361. https:\/\/doi.org\/10.1023\/A:1009914915709.\r\nFedotenkov, Igor. \u201cA Review of More than One Hundred Pareto-Tail Index Estimators.\u201d Statistica 80, no. 3 (2020): 245\u201399. https:\/\/doi.org\/10.6092\/issn.1973-2201\/9533.\r\nGomes, M. Ivette, Miranda Cristina, and Manuela Souto de Miranda. \u201cA Note on Robust Estimation of\u00a0the\u00a0Extremal Index.\u201d In Nonparametric Statistics, edited by Michele La Rocca, Brunero Liseo, and Luigi Salmaso, 213\u201325. Springer Proceedings in Mathematics & Statistics. Cham: Springer International Publishing, 2020. https:\/\/doi.org\/10.1007\/978-3-030-57306-5_20.\r\nHe, Xuming, and Wing K. Fung. \u201cMethod of Medians for Lifetime Data with Weibull Models.\u201d Statistics in Medicine 18, no. 15 (August 15, 1999): 1993\u20132009. [https:\/\/doi.org\/10.1002\/(SICI)1097-0258(19990815)18:15<1993::AID-SIM165>3.0.CO;2-H](https:\/\/doi.org\/10.1002\/(SICI)1097-0258(19990815)18:15%3C1993::AID-SIM165%3E3.0.CO;2-H).\r\nJu\u00e1rez, Sergio F., and William R. Schucany. \u201cRobust and Efficient Estimation for the Generalized Pareto Distribution.\u201d Extremes 7, no. 3 (September 1, 2004): 237\u201351. https:\/\/doi.org\/10.1007\/s10687-005-6475-6.\r\nLala Bouali, Dalal, Fatah Benatia, Brahim Brahimi, and Christophe Chesneau. \u201cRobust Estimator of Conditional Tail Expectation of Pareto-Type Distribution.\u201d Journal of Statistical Theory and Practice 15, no. 1 (January 3, 2021): 16. https:\/\/doi.org\/10.1007\/s42519-020-00153-0.\r\nMinkah, Richard, Tertius de Wet, and Abhik Ghosh. \u201cRobust Estimation of Pareto-Type Tail Index through an Exponential Regression Model.\u201d Communications in Statistics - Theory and Methods 52, no. 2 (January 17, 2023): 479\u201398. https:\/\/doi.org\/10.1080\/03610926.2021.1916530.\r\nPeng, Liang, and A.H. Welsh. \u201cRobust Estimation of the Generalized Pareto Distribution.\u201d Extremes 4, no. 1 (March 1, 2001): 53\u201365. https:\/\/doi.org\/10.1023\/A:1012233423407.\r\nVandewalle, B., J. Beirlant, A. Christmann, and M. Hubert. \u201cA Robust Estimator for the Tail Index of Pareto-Type Distributions.\u201d Computational Statistics & Data Analysis 51, no. 12 (August 15, 2007): 6252\u201368. https:\/\/doi.org\/10.1016\/j.csda.2007.01.003.\r\n","comments":[],"labels":["type-enh","comp-robust","comp-extremes"]},{"title":"ENH: make sure calling summary after remove data works if cached","body":"see https:\/\/github.com\/statsmodels\/statsmodels\/issues\/9147#issuecomment-1939106367\r\n\r\ncurrently `summary()` does not work for all models after remove_data has been called even when summary was called before remove_data\r\n\r\nunit tests for remove_data currently only check that `predict` works after remove_data, but not summary or any other results methods.\r\n\r\n- GLM, bug see #9147\r\n- OLS: diagnostics not cached, e.g. jarque_bera\r\n\r\n","comments":["related to unit tests\r\n\r\ncheck whether we can use base\/tests RemoveDataPickle as mixin class for regular unit test classes for results\r\ne.g. use in unit test classes in genmod, discrete and regression.\r\ninstead of adding more models in  statsmodels.base.tests.test_shrink_pickle  which currently only has the original models, OLS, WLS, GLM, RLM, Logit, Poisson, but not the more recently added models.\r\n\r\nsee also #4018"],"labels":["type-enh","comp-base"]},{"title":"BUG\/ENH: Saving a model with `remove_data=True` breaks `.summary()`","body":"It would be great if a model saved with the data removed would still be able to produce the summary statistics. Potentially it's worthwhile keeping the minimum amount of data in order to reproduce these stats? Especially when using the formula API in which the original data is still saved, see [here](https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8947) and [here](https:\/\/github.com\/statsmodels\/statsmodels\/issues\/7494). \r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\n\r\n# load \"growth curves pf pigs\" dataset\r\nmy_data = sm.datasets.get_rdataset(\"dietox\", \"geepack\").data\r\n\r\n# make GLM model\r\nmy_formula = \"Weight ~ Time\"\r\nmy_family = sm.families.Gaussian(link=sm.families.links.identity())\r\nmodel = smf.glm(formula=my_formula, data=my_data, family=my_family)\r\nresult = model.fit()\r\n\r\n# save model with \"remove_data=True\" \r\nresult.save('test_model.pickle', remove_data=True)\r\nnew_result = sm.load('test_model.pickle')\r\nprint(new_result.summary())\r\n```\r\n\r\nOutput:\r\n\r\n![image](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/12799126\/fdfb5b86-f124-4535-9202-1542c9c62640)\r\n\r\n","comments":["see for example #6887\r\n\r\nyou need to call `summary()` before remove_data, so that the summary statistics are computed and cached, computation of extra statistics is lazy but cached.\r\nAfterwards, calling summary again will use the cached values. \r\n\r\nThis behavior is intentional.\r\nremove_data removes all data-length arrays, and now new statistics can be computed. \r\nThis means that users can only call remove_data after they have computed everything they want that needs the data.\r\n","closing as duplicate\r\n\r\nthere should be unit tests for this for all core models, but likely not for more recently added models.","Thanks @josef-pkt for pointing that out.\r\n\r\nUnfortunately, that doesn't seem to work for the example above:\r\n\r\n```\r\nresult.summary() # works\r\nresult.save('test_model.pickle', remove_data=True)\r\nnew_result = sm.load('test_model.pickle')\r\nnew_result.summary() # fails\r\n``` \r\n\r\nSame for:\r\n\r\n```\r\nresult.summary() # works\r\nresult.remove_data()\r\nresult.summary() # fails\r\n```\r\n","Sounds like it needs another look. ","ok, that's a bug\r\n\r\nfails in computing pseudo_rsquared, which cannot be cached because it's a method with arguments, but it only needs to use cached attributes.\r\n\r\nProblem is\r\nllnull, llf is deleted (set to None) in cache\r\nThe same also happens if I use numpy arrays in GLM instead of the formula interface.\r\nAlso it's unrelated to pickling, remove_data alone has same effect.\r\nI thought we have a unit test that includes GLM\r\n\r\ncache after remove data\r\n```\r\nresult._cache\r\n{'null': None,\r\n 'llnull': None,\r\n 'mu': None,\r\n 'llf': None,\r\n 'deviance': None,\r\n 'pearson_chi2': None,\r\n 'bse': array([0.52072095, 0.07095736]),\r\n 'tvalues': array([30.16077478, 97.89963309]),\r\n 'pvalues': array([7.74789429e-200, 0.00000000e+000]),\r\n 'aic': None,\r\n 'bic_deviance': None,\r\n 'bic_llf': None,\r\n 'fittedvalues': None,\r\n 'null_deviance': None,\r\n 'resid_anscombe': None,\r\n 'resid_anscombe_scaled': None,\r\n 'resid_anscombe_unscaled': None,\r\n 'resid_deviance': None,\r\n 'resid_pearson': None,\r\n 'resid_response': None,\r\n 'resid_working': None,\r\n 'resid': None,\r\n 'wresid': None}\r\n```","bug was introduced in #4421\r\n\r\nllf, llnull are `@cache_readonly`\r\naic which is not nulled is `@cached_value`\r\n\r\n#4421 moved from a list for things to remove to removing things by default.\r\nAlso it looks like it evaluates cached attributes before remove_data, which does a lot of computation in remove_data.\r\nOriginal reason for adding remove data was only `predict` which does not need any extra results statistics.\r\n\r\nmaybe revert most of #4421 ?\r\nI'm not even sure what the current behavior is. It seems to have changed the behavior in a large way compared to what I had initially added.\r\n","OLS summary after remove_data also fails, but it fails in the diagnostics, `jarque_bera(self.wresid)`\r\n\r\nunit tests\r\nbase tests\r\ntest_remove_data_pickle  calls summary before remove_data but not after.\r\nSo, it looks like working summary after remove_data was never guaranteed generically by `base` tests.\r\n\r\nThe original unit tests only checked that `predict` works correctly after remove_data.\r\n","correction: I'm still not sure how this currently works.\r\nThe current code still uses the whitelist, #4421 added additional Noneing based on identity of cache decorator.\r\n\r\nIn OLS, there are no cached results statistics set to None.\r\n\r\nAFAICS:\r\nThe bug is in the interaction of #4421 and #6056\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/6056\/files#diff-caa0f13bb4c8b24512745dc6ef3d5eb4781a45610dd793ed488e0ece329b6759L142\r\n\r\n4421 introduced behavior of remove data depending on isinstance of specific cache decorator.\r\n6056 made all decorators aliases of the same (pandas) decorator.\r\n\r\nBut I still don't see why OLS is not affected. It uses the same decorator `@cache_readonly`\r\n\r\n**update**\r\nthis sounded familiar to me, looking for issue\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/7511#issuecomment-864187480\r\n7511 was a quickfix that removed triggering the lazy evaluation\r\nbut did not make any changes to the underlying decorator problem\r\n","@RoelVerbelen \r\njust as background information, motivation for summary after remove_data\r\n\r\nWhat is your usecase for summary from pickled model\/results?\r\n\r\nrelated issue for summary_col #7368\r\n\r\nI think we can add to the unit tests to verify that summary() works after remove_data if summary has been called before.\r\n\r\nRelated: given that it came up several times\r\nWe could add a `run_summary=False` or similar (`summary=False` ?) to remove_data and to pickle\/save.\r\nThis would at least provide an obvious, visible documentation about this behavior.\r\nWhether the default should be True is not clear. It's not backwards compatible in terms of performance, and might not be the correct default if the main usecase for pickling is still `predict`.\r\n","two points here\r\n\r\n- bugfix remove decorator alias and their usage in remove_data,   this is a bit more extensive than quickfix in #7511\r\n- ENH\/TST:  make summary after remove_data possible and verify with unit test \r\n\r\nI will open a separate issue for the ENH so this issue is just the bug fix\r\n","Hi @josef-pkt \r\n\r\nMy motivation for using `.summary()` after `.remove_data()` is that I'm setting up modelling pipelines where I build (many) GLMs in a python script, save the GLMs and then run quarto reports to evaluate these. \r\n\r\nWithin the model evaluation quarto reports, I'd like to print out the model summary as it contains a useful short summary of the model object and key statistics. \r\n\r\nThe data sets I'm working with are very large, so I'm looking for ways to reduce the size of the statsmodels saved objects. Currently they contain a lot of duplication as the data needs to be saved down alongside it for the formula API to work. That's not ideal. \r\n\r\nFYI, in case you're interested, I have the same modelling pipeline set-up in R and there I've managed to strongly reduce the size of glms\/gams (mgcv) by removing all \"nobs arrays\" and keeping only the first row of the model frame without affecting functionality. \r\n\r\n```\r\n#' Reduce glm object size (for saving purposes)\r\n#'\r\n#' @param object glm model\r\n#'\r\n#' @return glm model with certain components removed\r\nreduce_glm_size <- function(object) {\r\n  \r\n  object$residuals <- NULL\r\n  object$fitted.values <- NULL\r\n  object$effects <- NULL\r\n  object$qr$qr <- NULL\r\n  object$linear.predictors <- NULL\r\n  object$working.weights <- NULL\r\n  object$z <- NULL\r\n  object$wt <- NULL\r\n  object$weights <- NULL\r\n  object$prior.weights <- NULL\r\n  object$hat <- NULL\r\n  object$model <- object$model[1, ]\r\n  object$data <- NULL\r\n  object$offset <- NULL\r\n  \r\n  object\r\n}\r\n```"],"labels":["prio-high","type-bug","type-enh","comp-base","comp-genmod"]},{"title":"ENH: right truncated count models for oulier trimming","body":"related to robust regression, e.g. #3315 , https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8690#issuecomment-1913715516\r\n\r\nproblem is imposing fisher consistency.\r\n\r\none possibility would be to have a full MLE model that is right truncated at the trimming points. Then we should get consistent estimates for the underlying distribution parameter(s)\r\n\r\nfor example trim at `isf(alpha; theta)` where alpha is e.g. 0.025, 0.01 or similar.\r\nweight function then is indicator function for `(y : y < isf(alpha; theta))`\r\ntruncation point will differ across observations\r\n\r\n- need a preliminary estimator for parameter theta (mu in poisson case), e.g. robust initial estimate\r\n- iterate weighted\/truncated MLE to update theta.\r\n- drop outliers and reestimate truncated MLE\r\n\r\nfor predict, results we consider it as untruncated model, i.e. predicted mean=mu. This will simplify post-estimation by a large amount. \r\n(e.g. truncated mean does not have a closed form expression, and we would need to compute it through enumeration.)\r\n\r\nFirst target is for only small counts, i.e. we only need right truncation and not look at left outliers.\r\n\r\npossible theoretical problem: \r\nTrimming\/truncation is endogenous, data dependent through parameters and the support of the distribution depends on parameters.\r\nMLE would not apply, but effect should be \"negligible\" if we only truncate in the far tail.\r\n","comments":["parking a (semi-random) idea, but should be more general for robust GLM, poisson, logit,...\r\n\r\nThe fisher consistency term needs expectation of estimating equation w.r.t. the distribution. The distribution depends on the estimated distribution parameter for each observation, e.g. mu in Poisson.\r\nSo, I thought it will require slow computation if we need to compute consistency factor by numerical integration for each observation.\r\n\r\nThe current idea is to estimate this expectation at the aggregate predictive distribution, e.g. using pdf values that we already compute in, e.g., get_prediction with aggregate=True, predictive probs for count data models.\r\n\r\nE psi(y, x) dF(y, theta) dF(x), with estimated predicted distribution parameters theta f(theta | x) \r\n(F(theta|x) = indicator(predict(x)), i.e. theta is just one point for each x)\r\n\r\ndetail:\r\nWe will need to truncate the aggregate predictive distribution for some y. This will be difficult if the distribution is heavy tailed, i.e. tail weight cannot be ignored except at very far tail.\r\nHowever, redescending weights will be zero after some threshold c, so tail component will just be `c * sf(c) = c * (1 - cdf(c))`   and cdf(c) can be computed from the center y, or something similar if we trim\/down weight at both tails.\r\n\r\n\r\n(I need to get Ronchetti and similar for robust GLM again in more details)   \r\n"],"labels":["type-enh","comp-discrete","comp-robust"]},{"title":"`_fit_minimize` requires update of `no_hess` list: L-BFGS-B and TNC do not use Hessian information (hess)","body":"L-BFGS-B and TNC do not use Hessian information (hess). Therefore, these methods should be added to \r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/23faea30e30ff759c3c9d3f1d57864b2aa68c827\/statsmodels\/base\/optimizer.py#L341","comments":["yes lbfgs are missing but AFAICS we don't have a wrapper for fmin_tnc.\r\n\r\nDo you get any problems, exception, ... when running the current code?\r\nI don't understand why this doesn't cause problems when we run the unit tests.\r\n\r\n**update**\r\n`no_hess` is only used in `_fit_minimize` and we have few unit tests for it.\r\n\r\n","we should get at least a smoke test for all methods in `_fit_minimize`.\r\n","I do not have any problem when running the current code, only a warning. Sorry, I should have mentioned this. I just wanted to point this out, as I believe it can be confusing for users to get this warning when there shouldn't be one."],"labels":["type-bug","comp-base"]},{"title":"ENH design, tools: one-step approximation to params in iterative fixed point M-estimation","body":"(random find sorting through articles downloaded for robust in 2016)\r\n\r\nAelst, Stefan Van, and Gert Willems. \u201cFast and Robust Bootstrap for Multivariate Inference: The R Package FRB.\u201d Journal of Statistical Software 53, no. 3 (2013). https:\/\/doi.org\/10.18637\/jss.v053.i03.\r\nsection 3, mainly equ. 16\r\n\r\nM, MM, S estimators for outlier robust estimation work with a iterative fixed point algorithm, e.g. IRLS style.\r\nFor fast bootstrap, they compute a new parameter based on the fixed point iteration but corrected for a second order Taylor series expansion.\r\n\r\nThis issue is mainly to park the reference.\r\nIt might be useful to applications other than bootstrap. We use one-step `dparams` in related cases like outlier-influence and jackknife.\r\n\r\nMany outlier robust estimators are computed through fixed point iteration, and we don't have (yet) score and hessian (or estimating equations and their derivatives, for the case when we don't have one objective function for the M-estimator) to compute a newton-raphson or similar one-step approximations to params_new and dparams.\r\n\r\n","comments":[],"labels":["type-enh","comp-robust","topic-diagnostic","topic-post_estim"]},{"title":"ENH Roadmap outlier robust covariance or scatter matrix","body":"see\r\n#3221\r\n#8129\r\n\r\nMaronna and Yohai 2017 recommend MM-estimator for p < 15 and Rocke S-estimator for p >= 15.\r\nThey only look at affine equivariant estimators.\r\n#8129 does not include those and similar estimators.\r\nMany estimators in there are not affine equivariant or are only approximately affine equivariant (like OGK)\r\n\r\nHigher priority target would be to get MM and Rocke-S\r\n\r\n- `norm`, weight functions\r\n  - smoothed hard rejection (SHR in MY 2012)\r\n  - Rocke nonmonotonic weight function \r\n- S-estimator\r\n- MM estimator\r\n- starting cov or samples\r\n  - use `Det` as started in #8129, approximately affine equivariant\r\n  - Pena and Prieto 2007, KSD in MY 2012, \"kurtosis plus specific directions\", affine equivariant\r\n- ...","comments":["module(s) structure:\r\n\r\nThe robust covariance module will get large if I want to add additional robust cov estimators, e.g. detmcd, s-estimators, ...\r\n\r\nOne problem splitting it up will be to avoid circular imports, detmcd, dets, detmm will need to call the basic estimators like ogk and tyler.\r\nThat is, if I put those in a separate file, then I need to import from robust.covariance, but the detxxx should have public access through robust.covariance.\r\nThe tool, common helper functions could be outsourced.\r\n`_cov_starting` was (AFAIU) initially intended as preparation for DetXXX, but now it includes extras. It needs ogk and others, and could go to a `det` module as helper function if we use it only for that.\r\nWe might later add the extra starting points from pena and prieto 2007, which I guess is not a short function either.\r\n\r\nAside: one starting cov that is still missing is cov of tanh transformed data. That will only be as starting cov (only use is mahlanobis ranking of observations) and not a full cov function as those in stats.covariance. It will only be a helper\/tool function.\r\n\r\nOne option would be to make robust.covariance into an `api` module, i.e. only imports but no included code, then I could also outsource tyler and ogk without circular imports.\r\nAnother option is to make the required imports lazy, i.e. import inside a function that needs it. We better still outsource the helper functions, so we only need imports inside cov_starting. The relevant shared tool functions are reweight and consistency factor functions.\r\n\r\nDetMM also needs to import the M-estimators, currently that is `_cov_iter`. same for S-estimators (DetS) AFAIU.\r\nAlso DetMM needs a starting estimate which frequently in the literature is the S-estimator.\r\n\r\none possibility. \r\n- tools: reweight, consistency factors\/adjustments\r\n- basic cov: M, tyler, ogk, t-cov, imports tools\r\n- det cov, can include semi-det cov Pena\/Prieto: imports tools and basic cov\r\n- most likely not needed in statsmodels: FastXXX, but those could go into the det_cov module\r\n\r\nopen: regularized version for large k_vars \/ nobs ?  go with the non-regularized versions ? \r\n(currently there is only tyler_regularized, and I have not looked at the literature for other regularized outlier robust cov)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","to square or not to square?\r\n\r\nShould we use Mahalanobis distance (square root) or squared Mahalanobis statistic?\r\nAnd how do we define the corresponding `norm` function, rho, psi, weights?\r\n\r\nThe literature is not consistent, and even more so across estimators.\r\ne.g. Tyler and all my current code use squared Maha distance. \r\n\r\nMost of the original literature for S-, MM-, .... estimators use square-root maha and the standard `norms` (as defined for regression).\r\nMaronna\/Yohai\/.. including text book use squared mahalanobis distance with redefined norms (take out the square)\r\nAdditionally, they change the norm to have a fixed threshold, e.g. in bisquare. They also drop one norm parameter from the Rocke norm (without explaining why).\r\n\r\nrelated ambiguity: some articles\/books include a correction factor as separate term in norm functions, e.g. rho(resid \/ s \/ c) instead of just rho(resid \/ s)\r\nThis makes the definition of consistency factor ambiguous, e.g.\r\nE rho(d) = b  versus E rho(d \/ c) = b'      where b' compensates for c so those two are equivalent.\r\n\r\nThe Maronna\/Yohai\/... book (using R in latest edition) would be good, but no proofs and very stingy on justification for the statements, besides the differences in definitions.\r\n\r\nThe Belgian and Italian robust authors have package in matlab which is GPL compatible but not BSD compatible (some European public license) (at least based on comments in articles)\r\n\r\n**update** after reading around a bit\r\nFor S-estimators, I will use square root Maha distance in the `norm` functions. All original articles use that in rho or weights.\r\nThis could be changed after we have a working version to avoid the redundant square-root - square detour.\r\n\r\n**update 2**\r\n\r\nradial distribution in elliptically symmetric distributions depends on square maha.\r\nIf we take sqrt maha, then we need to correct for it in the computation of fisher consistency term `b`.\r\ne.g. for bisquare `norm = norms.TukeyBiweight(c=c)`\r\n\r\nsquared maha under mv normal distributed data is chi2.\r\nsqrt maha is chi distributed\r\nThe following has explicit formula, then it's numerical integration \r\n\r\nFor mv t-distribution, the squared maha is F distributed. I don't know of any distribution for sqrt of F random variable.\r\nThis means generically we need squared maha for radial distribution and corresponding sqrt correction for `norm` rho function, as below with `stats.chi2(p).expect(func)`\r\n\r\nI used p=1 to compare with regression case. see https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8129#issuecomment-1937024357\r\n\r\n```\r\ncdf = stats.chi2.cdf\r\np = 1\r\nc2 = c**2\r\nb_p = (p \/ 2 * cdf(c2, p+2) - \r\n       p * (p + 2) \/ 2 \/ c2 * cdf(c2, p + 4) + \r\n       p * (p + 2) * (p + 4) \/ 6 \/ c2**2 * cdf(c2, p + 6) +\r\n       c2 \/ 6 * (1 - cdf(c2, p))\r\n      )\r\nb_p\r\n0.43684963008360306\r\n\r\nfunc = lambda x: norm(np.sqrt(x))\r\nstats.chi(p).expect(norm), stats.chi2(p).expect(norm), stats.chi2(p).expect(func)\r\n(0.43684963023076195, 0.6617870376575539, 0.43684963005117056)\r\n```\r\n\r\n\r\n"],"labels":["type-enh","comp-robust"]},{"title":"ENH: outlier robust regression, OLS, IV2SLS, based on outlier robust covariance\/moment matrices","body":"once we have robust covariance computation as in #8129, we can compute linear regression models including IV based on those cov.\r\n\r\nI don't find right now the reference for using  MCD\/MVE and similar for OLS\r\n\r\nfor IV:\r\n\r\nCohen Freue, Gabriela V., Hernan Ortiz-Molina, and Ruben H. Zamar. \u201cA Natural Robustification of the Ordinary Instrumental Variables Estimator.\u201d Biometrics 69, no. 3 (September 1, 2013): 641\u201350. https:\/\/doi.org\/10.1111\/biom.12043.\r\n\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/3901#issuecomment-1913180626 OLS based on summary statistics\r\n","comments":[],"labels":["type-enh","comp-regression","comp-robust"]},{"title":"ENH: nonlinear robust estimation, curve fitting, application to distribution parameter estimation","body":"\r\nvague idea\r\n\r\nI have robust or non-robust to outliers (squared) mahalanobis distance.\r\nFor elliptical distributions this distance, i.e. radial function, has a given distribution.\r\nIn the normal case it is chi-square, in the t distribution case it is a scaled F distribution. We don't have a collection of these radial distribution functions for various elliptical distributions.\r\n\r\nrobust multivariate methods usually use normal reference case with chi2 for calibrating the scale, #8129.\r\nMore general we could try to fit possible candidate distributions like chi2, F, and estimate the distribution parameters. Those parameters will reflect scale and kurtosis, tail heaviness, e.g. degrees of freedom in F.\r\n(Similar in qq-plot we can estimate loc and scale by fitting a line in the plot. AFAIR we have the option of using RLM for it, or I used RLM only in examples.)\r\n\r\nEstimation could fit some distance measure for quantiles or cdf, but that is in general nonlinear in distribution parameters.\r\nFor the outlier application we need an outlier robust estimation procedure, non-linear RLM, truncation, ...\r\n\r\nE.g. I want a quick check whether multivariate-t distribution is a good approximation for the radial component of the data and which degrees of freedom would fit best.\r\n\r\nrelated topic goodness-of-fit, minimum distance estimators #7412\r\n ","comments":[],"labels":["type-enh","comp-robust","comp-stats"]},{"title":"boolean columns in dataframe fails","body":"#### Describe the bug\r\n\r\nNot sure if this is a bug or intended. I've tried searching issues but couldn't find a similar issue.\r\n\r\nWhen passing a dataframe as `exog` with `bool` columns an exception occurs because\r\n`pandas` does not cast to `float` with `np.ndarray` when there are `float` and `bool` columns, but defaults to `object` instead. A mix of `float` and `int` columns gets properly cast.\r\n\r\n\r\n\r\n\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nimport pandas as pd\r\nimport statsmodels.api as sm\r\nimport numpy as np\r\n\r\nrng = np.random.default_rng(0)\r\n\r\n# only float and int\r\nX = pd.DataFrame({'A':rng.standard_normal(5),\r\n                  'B':np.array([1,2,3,2,1], int)})\r\nY = rng.standard_normal(5)\r\nsm.OLS(Y, X).fit() # works\r\n\r\n# add a bool column\r\nX['C'] = np.array([True,False,False,True,True],bool)\r\nsm.OLS(Y, X).fit() # does not work\r\n\r\n```\r\n<details>\r\n\r\nI think changing the `np.asarray` call in https:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/statsmodels\/base\/data.py#L507 to `np.asarray(exog, dtype=float)` would simply resolve this.\r\n\r\nOne could say that it's a bug in pandas (as it had decided not to cast as float in the `np.ndasarray` call), but I presume they have a rationale for that choice. In the statsmodels code, `np.asarray` will already potentially involve casting some columns so casting them explicitly to `float` seems not a huge deal.\r\n\r\nCases where you would not want to do this perhaps are cases where the original `pd.DataFrame` was large and all `int` or `bool` perhaps. Not sure how prevalent such cases are.\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nI would expect no exception raised here because it is clear how to cast such dataframes to design matrices for use in `statsmodels`.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.11.4.final.0\r\nOS: Darwin 22.4.0 Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3\/RELEASE_ARM64_T6000 arm64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.14.0 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.24.2 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/numpy)\r\nscipy: 1.11.1 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/scipy)\r\npandas: 1.5.3 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.7.2 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/matplotlib)\r\nInstalled osx event loop hook.\r\n    backend: MacOSX \r\ncvxopt: Not installed\r\njoblib: 1.3.1 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 8.14.0 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/IPython)\r\n    jinja2: 3.1.2 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/jinja2)\r\nsphinx: 5.3.0 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/sphinx)\r\n    pygments: 2.15.1 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/pygments)\r\npytest: 7.4.0 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/pytest)\r\nvirtualenv: 20.24.3 (\/Users\/jonathantaylor\/anaconda3\/envs\/islp_freeze_311\/lib\/python3.11\/site-packages\/virtualenv)\r\n\r\n\r\n<\/details>\r\n","comments":["currently it's intentional.  Any exog that np.asarray converts to a non-numeric dtype is the problem of the user to handle.\r\n\r\nIt's still open how much we can and will support for automatic dtype conversion from pandas.\r\n(There are related issues, but I'm not looking for them now)\r\n"],"labels":["type-enh","comp-base","pandas-integration"]},{"title":"ENH: allow cov_type in specification tests in stats.diagnostic","body":"lm-test that are based on auxiliary regression can use cov_type to make the specification test robust to (co)variance and other misspecification.\r\n\r\nsee https:\/\/github.com\/statsmodels\/statsmodels\/issues\/9121#issuecomment-1892389971\r\n\r\nfirst type of cases is where we can directly include cov_type as argument and use it directly in the auxiliary regression.","comments":[],"labels":["type-enh","comp-regression","topic-diagnostic","prio-elev","topic-post_estim"]},{"title":"linear_reset can't be set with \"HAC\" and maxlags, KeyErrors","body":"#### Describe the bug\r\nHello, thanks for your time. We've tried several things but it still doesn't work even with the newest version of Statsmodels. We think this might be a bug. Issue is: we were trying to use \"statsmodels.stats.diagnostic.linear_reset\" with HAC as coviance type and maxlags as the key word args, but we got KeyErrors.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n```\r\n# Import modules\r\nimport numpy as np\r\nimport statsmodels.api as sm\r\nfrom statsmodels.stats.diagnostic import linear_reset\r\n\r\n# Generate some random data\r\nnp.random.seed(42)\r\nx = np.random.randn(100)\r\ny = 2 * x + np.random.randn(100)\r\n\r\n# Fit a linear regression model\r\nmodel = sm.OLS(y, sm.add_constant(x))\r\n# results = model.fit()\r\n\r\n# Perform linear reset test\r\n## opt 1\r\nreset_test = linear_reset(results, power=2, test_type='fitted', cov_type='HAC', cov_kwargs={'maxlags': 2})\r\n\r\n## opt 2\r\n# test = linear_reset(results, cov_type='HAC', cov_kwds={'maxlags': 2})\r\n\r\n## opt 3\r\n# kwargs = {'maxlags': 2}\r\n# test = linear_reset(results, cov_type='HAC', **kwargs)\r\n\r\n## opt 4\r\n# test = linear_reset(results, cov_type='HAC', maxlags=2)\r\n\r\n```\r\n```python\r\n# Your code here that produces the bug\r\n# This example should be self-contained, and so not rely on external data.\r\n# It should run in a fresh ipython session, and so include all relevant imports.\r\n```\r\n<details>\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nFile <command-2970740754718372>:19\r\n     16 model = sm.OLS(y, x).fit(cov_type='HAC', cov_kwds={'maxlags': 2})\r\n     18 # Perform linear reset test\r\n---> 19 reset_test = linear_reset(model, power=2, test_type='fitted', cov_type='HAC', cov_kwargs={'maxlags': 2})\r\n     21 # # Perform the linear reset test with HAC covariance and maxlags=2\r\n     22 # test = linear_reset(results, cov_type='HAC', cov_kwds={'maxlags': 2})\r\n     23 \r\n   (...)\r\n     33 \r\n     34 # Print the test results\r\n     35 print(reset_test.summary())\r\n\r\nFile \/databricks\/python\/lib\/python3.9\/site-packages\/pandas\/util\/_decorators.py:207, in deprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper(*args, **kwargs)\r\n    205     else:\r\n    206         kwargs[new_arg_name] = new_arg_value\r\n--> 207 return func(*args, **kwargs)\r\n\r\nFile \/local_disk0\/.ephemeral_nfs\/cluster_libraries\/python\/lib\/python3.9\/site-packages\/statsmodels\/stats\/diagnostic.py:1098, in linear_reset(res, power, test_type, use_f, cov_type, cov_kwargs)\r\n   1096 mod = mod_class(res.model.data.endog, aug_exog)\r\n   1097 cov_kwargs = {} if cov_kwargs is None else cov_kwargs\r\n-> 1098 res = mod.fit(cov_type=cov_type, cov_kwargs=cov_kwargs)\r\n   1099 nrestr = aug_exog.shape[1] - exog.shape[1]\r\n   1100 nparams = aug_exog.shape[1]\r\n\r\nFile \/local_disk0\/.ephemeral_nfs\/cluster_libraries\/python\/lib\/python3.9\/site-packages\/statsmodels\/regression\/linear_model.py:373, in RegressionModel.fit(self, method, cov_type, cov_kwds, use_t, **kwargs)\r\n    370     self.df_resid = self.nobs - self.rank\r\n    372 if isinstance(self, OLS):\r\n--> 373     lfit = OLSResults(\r\n    374         self, beta,\r\n    375         normalized_cov_params=self.normalized_cov_params,\r\n    376         cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\r\n    377 else:\r\n    378     lfit = RegressionResults(\r\n    379         self, beta,\r\n    380         normalized_cov_params=self.normalized_cov_params,\r\n    381         cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t,\r\n    382         **kwargs)\r\n\r\nFile \/local_disk0\/.ephemeral_nfs\/cluster_libraries\/python\/lib\/python3.9\/site-packages\/statsmodels\/regression\/linear_model.py:1653, in RegressionResults.__init__(self, model, params, normalized_cov_params, scale, cov_type, cov_kwds, use_t, **kwargs)\r\n   1651             use_t = use_t_2\r\n   1652         # TODO: warn or not?\r\n-> 1653     self.get_robustcov_results(cov_type=cov_type, use_self=True,\r\n   1654                                use_t=use_t, **cov_kwds)\r\n   1655 for key in kwargs:\r\n   1656     setattr(self, key, kwargs[key])\r\n\r\nFile \/local_disk0\/.ephemeral_nfs\/cluster_libraries\/python\/lib\/python3.9\/site-packages\/statsmodels\/regression\/linear_model.py:2570, in RegressionResults.get_robustcov_results(self, cov_type, use_t, **kwargs)\r\n   2567     res.cov_params_default = getattr(self, 'cov_' + cov_type.upper())\r\n   2568 elif cov_type.lower() == 'hac':\r\n   2569     # TODO: check if required, default in cov_hac_simple\r\n-> 2570     maxlags = kwargs['maxlags']\r\n   2571     res.cov_kwds['maxlags'] = maxlags\r\n   2572     weights_func = kwargs.get('weights_func', sw.weights_bartlett)\r\nKeyError: 'maxlags'\r\n\r\n**Note**: As you can see, there are many issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates.\r\n\r\n**Note**: Please be sure you are using the latest released version of `statsmodels`, or a recent build of `main`. If your problem has been fixed in an unreleased version, you might be able to use `main` until a new release occurs. \r\n\r\n**Note**: If you are using a released version, have you verified that the bug exists in the main branch of this repository? It helps the limited resources if we know problems exist in the current main branch so that they do not need to check whether the code sample produces a bug in the next release.\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\nWe expect that the code would produce the p-value from linear RESET test with \"HAC\" as the covariance estimator.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\n0.14.1\r\n<\/details>\r\n","comments":["**update** this does not apply here, new issue #9122\r\n\r\n\r\nrobust cov_type is not implemented for, AFAIR, any of the diagnostic and specification tests.\r\n\r\nMain reason: When I implemented those I only had references (and other packages for unit tests) for the standard version with basic OLS in the auxiliary regression.\r\n\r\nAt the time I was not sure about the theory and did not find explicit references for using `cov_type`s, although it's mentioned e.g. by Wooldridge that we can just use a specification robust cov_type in the auxiliary regression.\r\n\r\nAlso for some LM specification tests, we use uncentered rsquared as computational shortcut and not a standard wald test, i.e. it hardcodes the nonrobust hypothesis test.\r\n\r\nFor some specification\/diagnostic tests we might be able to just forward the cov_type information to the auxiliary regression,\r\neven if we cannot write unit test to verify against other packages.\r\nIn other cases it might be easier to use generic conditional moment test and newer score_test\/lm_test which AFAIR already allow for cov_type (although currently with some restrictions).\r\n\r\nI add elevated priority to issue to look at this.\r\nI worked a lot on misspecification robust (HC, ...) CMT, score\/lm test in the last years (but mostly for GLM and discrete), and should be able to get back to this. \r\n \r\nNote: using wald test with augmented `exog` is easy to perform by just defining a new model.\r\ne.g. one simple case for poisson https:\/\/gist.github.com\/josef-pkt\/1fdcc5c57cd63824fb3130d013e30103\r\n\r\nAside: \r\nThe older literature in econometrics was emphasizing auxiliary regression because it was relatively easy to use existing models from statistical packages for this. It also relied in many cases on an OPG version which is much easier to compute but has worse small sample properties (mainly relevant for nonlinear or non-gaussian models)\r\nFor statsmodels it's easier and provides more options to implement these tests based on generic theory and generic methods.\r\n\r\n\r\n","my mistake\r\nthe reset test in `diagnostic` was written by @bashtage and has cov_type keywords. ","I found the bug\r\n\r\nthe reset test uses cov_kwargs, but the model.fit use cov_kwds\r\nthe correct code in the reset function should be\r\n`res = mod.fit(cov_type=cov_type, cov_kwds=cov_kwargs)`\r\n\r\n(I don't like the word kwargs, and prefer `xxx_kwds` for named options)\r\n"],"labels":["type-bug","comp-regression","topic-diagnostic"]},{"title":"RuntimeWarning in zt_ind_solve_power function : invalid value encountered in sqrt","body":"#### Describe the bug\r\n\r\nSample size optimization in statsmodels 0.13.5 ends up in RunTimeWarning as nobs and nobs1 become negative values under square root. The optimization algorithm should not allow negative sample sizes nobs or nobs1 either and should correctly convert to positive integers.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\neffect_size = -0.0008095274776159337\r\npower = 0.8\r\nalpha error = 0.1\r\nratio = 1.0146909910259767,\r\nalternative = 'larger'\r\n\r\nsample_size = zt_ind_solve_power(\r\n    effect_size = effect_size,\r\n    power = power,\r\n    alpha = alpha_error,\r\n    sample_size_ration = sample_size_ration,\r\n    alternative = alternative\r\n)\r\n\r\n```\r\nAdapting power.py file to print nobs and nobs1 from power function it is clear that optimization end up with negative values for sample sizes: \r\n```\r\neffect_size: -0.0008095274776159337, power: 0.8, alpha_error: 0.1, ratio: 1.0146909910259767, alternative: larger\r\nnobs: 1.0072919326543943, nobs1: 2.0, ddof: 0\r\nnobs: 25.182298316359862, nobs1: 50.0, ddof: 0\r\nnobs: 25.182298316359862, nobs1: 50.0, ddof: 0\r\nnobs: 25.182298316359862, nobs1: 50.0, ddof: 0\r\nnobs: 1.0072919326543943, nobs1: 2.0, ddof: 0\r\nnobs: [5.03645966], nobs1: [10.], ddof: 0\r\nnobs: [5.03645966], nobs1: [10.], ddof: 0\r\nnobs: [5.03645966], nobs1: [10.], ddof: 0\r\nnobs: [5.03645974], nobs1: [10.00000015], ddof: 0\r\nnobs: [-498.60950666], nobs1: [-990.], ddof: 0\r\n```\r\n\r\n#### Expected Output\r\nThe solution of sample size should converge to positive int or issue the ConvergenceWarning but not allow negative sample size numbers in optimization.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.7.3.final.0\r\nOS: Linux 4.18.0-372.16.1.0.1.el8_6.x86_64 #1 SMP Wed Jul 13 11:58:23 PDT 2022 x86_64\r\nbyteorder: little\r\nLC_ALL: en_US.UTF-8\r\nLANG: en_US.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.5 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.21.6 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/numpy)\r\nscipy: 1.7.3 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/scipy)\r\npandas: 1.3.5 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.5.3 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/matplotlib)\r\n    backend: module:\/\/matplotlib_inline.backend_inline \r\ncvxopt: Not installed\r\njoblib: 1.3.2 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 7.34.0 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/IPython)\r\n    jinja2: Not installed\r\nsphinx: Not installed\r\n    pygments: 2.15.1 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/pygments)\r\npytest: 7.4.0 (\/home\/alfred_sasko\/.virtualenvs\/abtest\/lib\/python3.7\/site-packages\/pytest)\r\nvirtualenv: Not installed\r\n```\r\n\r\n<\/details>\r\n","comments":["Is  `sample_size_ration` labeled `ratio` above?\r\n","In the corrected example, on the main branch, I see\r\n\r\n```python\r\neffect_size = -0.0008095274776159337\r\npower = 0.8\r\nalpha_error = 0.1\r\nratio = 1.0146909910259767\r\nalternative = 'larger'\r\n\r\nsample_size = zt_ind_solve_power(\r\neffect_size = effect_size,\r\npower = power,\r\nalpha = alpha_error,\r\nratio=ratio,\r\nalternative = alternative\r\n)\r\n```\r\n```plaintext\r\nC:\\git\\statsmodels\\statsmodels\\stats\\power.py:133: RuntimeWarning: invalid value encountered in sqrt\r\n  pow_ = stats.norm.sf(crit - d*np.sqrt(nobs)\/sigma)\r\nC:\\git\\statsmodels\\statsmodels\\stats\\power.py:525: ConvergenceWarning:\r\nFailed to converge on a solution.\r\n\r\n  warnings.warn(convergence_doc, ConvergenceWarning)\r\n```","@bashtage yes indeed it may be convergence error but let me look at my original code I have it emebeeded in try except statement cathing this convergence warning. Will send you the original code.","sample size computation does not always have a positive finite solution.\r\n\r\nFor example, if the hypothesis goes the wrong direction (as in the example), then there is no sample size that satisfies both power and significance level (alpha) requirements.\r\n\r\nThe power is always defined (for positive finite sample size), but power could be smaller than alpha, under the null hypothesis power is equal to alpha. (The latter is used in unit tests as verification, AFAIR)\r\n\r\ncurrently it raises if effect_size=0 in the rootfinding\r\n\r\n```\r\n...\\statsmodels\\statsmodels\\stats\\power.py in solve_power(self, **kwds)\r\n    453                 return kwds['power']\r\n    454             else:\r\n--> 455                 raise ValueError('Cannot detect an effect-size of 0. Try changing your effect-size.')\r\n```\r\n\r\nIt might be possible to raise also on the impossible cases where effect_size goes the wrong direction compared to null hypothesis `alternative`, but it's a bit more difficult to detect because it depends on \"smaller\" and \"larger\" and may not apply to hypothesis that are squares or absolute values.  (e.g. standard F- or chi2-tests which are usually one sided).\r\n\r\nI'm marking this issue as ENH to check whether we can raise an exception instead of relying on convergence failure.\r\n \r\n"],"labels":["type-enh","comp-stats"]},{"title":"Error estimating mean in rainbow test","body":"The function `linear_rainbow` has an option to either order cases by a certain reference case order, or alternatively to calculate Mahalanobis distances to the center of the data and then build the central fit from the `frac` fraction of cases that have the smallest Mahalanobis distances, as was proposed in the original paper by Jessica Utts (1982).  In the latter option, cases in the raw data are not assumed to be sorted. However, the present code will just take the n\/2 th case as the center of the data: \r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/6614975f4468abba15ae6894db276cfe2e29ba63\/statsmodels\/stats\/diagnostic.py#L1229\r\n\r\nThis is obviously incorrect. The location estimate required in Mahalanobis distances is the mutlivariate mean, not the n\/2 th case in the data. I suggest to modify this line to `center_vec = np.mean(exog,axis=0)` and then use `center_vec` instead of `center_obs` going forward.  ","comments":["Thanks for finding this.\r\n\r\nI agree that there is a problem with the current code.\r\nAFAICS, there is no test coverage for the Mahalanobis distance case (based on brief code search).\r\n\r\nI don't know or remember why there is a try-except with using mean in the except case.\r\n\r\nR uses the mean https:\/\/search.r-project.org\/CRAN\/refmans\/lmtest\/html\/raintest.html\r\n\r\nAlternatively to using the mean, using median might be better, and more in the spirit of using central observations as in the univariate case.\r\nAlso we could use a robust cov, mahalanobis distance when they become available #8129\r\n\r\n\r\n","asides:\r\n\r\nI don't have the Utts article on my computer and not in my Zotero list. So, I don't know what reference I used for this.\r\n(The mahalanobis extension was not added by me, so I guess I never looked at that part in details)\r\n\r\nrelationship to outlier robust and influence diagnostics.\r\nAFAICS, This test does not only reject the model for missing nonlinearities but also for outlier-influence problems.\r\nIt's a good idea to have a diagnostic test similar to robust estimators. The corresponding robust estimator would be LTS (least trimmed squares) where the regression is only based on a subset of exog that should be outlier free.\r\n\r\nI guess we can extend the basic idea to other (nonlinear) models like GLM. If there is no specific test statistic, then we could use a variable addition test that separates parameter estimates for central and tail observations (defined by mahalanobis distance).\r\n ","Thanks for acknowledging the issue. \r\n\r\nWhile I like your remarks on robustness, I think that we should at first focus on having a correct and reliable test in the classical case (no outliers). The rainbow test is based on F statistics that hold true under normal distribution assumptions for the linear model.  When using a robust estimator, this inference is no longer correct (and it is much more complicated to derive accurate distributional properties). I therefore suggest to go with the mean (univariate and multivariate) and defer a robust version to further research. That research could even lead to a publication. \r\n\r\nAnother suggestion is to expand the `linear_rainbow` function such that it also works for Ridge regression fits (`fit_regularized(method='elastic_net', L1_wt=0)`), since there are no inference issues in that case. \r\n\r\nThe original paper by Jessica Utts is available free of charge at: https:\/\/www.researchgate.net\/publication\/232819080_The_Rainbow_Test_for_Lack_of_Fit_in_Regression","independently of fixing the problem with rainbow test implementation\r\n\r\nI found an article that included the rainbow test in a monte carlo for nonlinear specification (nonlinearity also in y, so a bit different)\r\n\r\nrainbow test \r\ncompared to other specification tests, rainbow test is not very powerful under normality.\r\nIf the error distribution is non-normal, then rainbow test is liberal and overrejects quite a bit.\r\n\r\nreset test was doing pretty well and is robust to non-normality in their case, but maybe because it's a larger misspecification that also affects nonlinearity in endog\/link.\r\n\r\nGodfrey, L. G., Michael McAleer, and C. R. McKenzie. \u201cVariable Addition and Lagrange Multiplier Tests for Linear and Logarithmic Regression Models.\u201d The Review of Economics and Statistics 70, no. 3 (1988): 492\u2013503. https:\/\/doi.org\/10.2307\/1926788.\r\n\r\n"],"labels":["type-bug","comp-stats","topic-diagnostic","prio-elev"]},{"title":"Save the offset name in `GLM` and results wrapper","body":"#### Is your feature request related to a problem? Please describe\r\nPost model training, it is helpful to know which variable was used as the offset. This aids in post model analysis and deployment.\r\n\r\nThe offset array is saved and can be accessed after saving the model, but the name of the offset variable is lost when it is a pandas series. The series is [converted to a np.array](https:\/\/github.com\/statsmodels\/statsmodels\/blob\/ab10165eb897729b50e703b4ea831ae712b53585\/statsmodels\/genmod\/generalized_linear_model.py#L315-L316) which removed the name. Current state, it is difficult to tell which variable may have been used as an offset without tracking it outside the model.\r\n\r\nExample use case: Sharing a saved model with a peer. They inspect it to determine what variable was used as the offset in training.\r\n\r\nThe same may apply to the `var_weights` and `freq_weights` for GLM.\r\n\r\n#### Describe the solution you'd like\r\nThe model has access on `__init__` to the name of the offset if it is a pandas series. A way to save the offset array's name if it is a series would be wonderful.\r\n\r\nSimilar to how the endog and exog names can be used in the model summary.\r\n\r\nHere's a few ideas I had for how to implement this. Happy to hear if there's a better option.\r\n\r\n1. Add an `offset_name` property for GLM\r\n    - Similar to the base models [`endog_names`\/`exog_names`](https:\/\/github.com\/statsmodels\/statsmodels\/blob\/ab10165eb897729b50e703b4ea831ae712b53585\/statsmodels\/base\/model.py#L235-L247)\r\n    - Simple to implement\r\n2. Add it to the `model.data` so it's handled by [`PandasData`](https:\/\/github.com\/statsmodels\/statsmodels\/blob\/ab10165eb897729b50e703b4ea831ae712b53585\/statsmodels\/base\/data.py#L498)\r\n    - The name could be added back to the offset when making the results wrapper (at least I think that's how it works)\r\n    - I could use some guidance on how to implement this if it is the preferred approach\r\n    - I think it has something to do with the data attrs but it's a bit hard to track down\r\n3. Do not convert to a numpy array if it is a series\r\n    - One could use `model.offset.name` to get at the variable name \r\n    - Doesn't line up with how the rest of the code works, it expects numpy arrays\r\n    - Likely not a good option\r\n4. User adds `offset_name` attribute to the model class before saving it.\r\n    - Seems like a bad idea, would like support in statsmodels\r\n\r\n#### Describe alternatives you have considered\r\nCurrent workaround is saving the offset name in a separate file, which is not ideal.\r\n\r\n#### Additional context\r\nHappy to work on a PR for this.\r\n","comments":["I think currently `1.` is the only option. `2.` would be good but currently the extra arrays are not going through the endog\/exog `model.data` handling (at least not in most cases.\r\n\r\nWe could add a helper function that can be added to the `__init__` as replacement for np.asarray which does asarray plus return additionally the name of the variable if it is available.\r\nThis could also be applied to other extra data like exposure and the various weights.\r\n\r\nCurrent extra data like offset, exposure, weights are 1dim. \r\nFor flexibility the helper function could check for and distinguish 1dim and 2dim. In the later case, return individual column names instead of the Series name.\r\n\r\nThe same as in GLM also applies to discrete models and likely to some other models.\r\n\r\n"],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"DOC: no docstrings describes diagnostic part of regression `summary`","body":"I don't find a docstring that includes description and defintion summary info.\r\n\r\ncurrent point: I was not sure whether kurtosis is fisher or pearson, i.e. 0 or 3 in normal case.\r\nIt is pearson, normal = 3, after looking at the code in `jarque_bera`.\r\n","comments":[],"labels":["comp-docs","comp-regression"]},{"title":"ENH: (outlier) robust multivariate regression","body":"(I found several articles that I printed at around 2016)\r\n\r\n- robust, RLM is only for univariate endog\r\n- MultivariateLS has been added in #8919\r\n- a first set of robust covariance estimators is in PR #8129, but it mostly assumes mean is known\r\n\r\nThe target is to add something like a multivariate RLM equivalent, and possibly other robust and resistant estimators (which are also still missing for univariate endog case).\r\n\r\n`statsmodels.robust.multivariate`\r\nthis could also be the location for constant mean models, i.e. joint estimation of mean\/center and covariance.\r\n\r\nAFAIR, I did not read much of the robust multivariate regression literature, but more on joint mean-cov estimation.\r\n\r\n\r\nReferences (added to zotero between 2016-09-29 and 1016-10-02):\r\n\r\nAgull\u00f3, Jose, Christophe Croux, and Stefan Van Aelst. \u201cThe Multivariate Least-Trimmed Squares Estimator.\u201d Journal of Multivariate Analysis 99, no. 3 (March 1, 2008): 311\u201338. https:\/\/doi.org\/10.1016\/j.jmva.2006.06.005.\r\n\r\nBen, Marta Garc\u00eda, Elena Mart\u00ednez, and V\u00edctor J. Yohai. \u201cRobust Estimation for the Multivariate Linear Model Based on a \u03c4-Scale.\u201d Journal of Multivariate Analysis 97, no. 7 (August 1, 2006): 1600\u20131622. https:\/\/doi.org\/10.1016\/j.jmva.2005.08.007.\r\n\r\nGnanadesikan, R., and J. R. Kettenring. \u201cRobust Estimates, Residuals, and Outlier Detection with Multiresponse Data.\u201d Biometrics 28, no. 1 (1972): 81\u2013124. https:\/\/doi.org\/10.2307\/2528963.\r\n\r\nHubert, Mia, Tim Verdonck, and \u00d6zlem Yorulmaz. \u201cFast Robust SUR with Economical and Actuarial Applications.\u201d Statistical Analysis and Data Mining: The ASA Data Science Journal, May 1, 2016, n\/a-n\/a. https:\/\/doi.org\/10.1002\/sam.11313.\r\n\r\nJung, Kang-Mo. \u201cMultivariate Least-Trimmed Squares Regression Estimator.\u201d Computational Statistics & Data Analysis 48, no. 2 (February 1, 2005): 307\u201316. https:\/\/doi.org\/10.1016\/j.csda.2004.01.008.\r\n\r\nKent, John T., and David E. Tyler. \u201cConstrained M-Estimation for Multivariate Location and Scatter.\u201d The Annals of Statistics 24, no. 3 (June 1996): 1346\u201370. https:\/\/doi.org\/10.1214\/aos\/1032526973.\r\n\r\nKoenker, Roger, and Stephen Portnoy. \u201cM Estimation of Multivariate Regressions.\u201d Journal of the American Statistical Association 85, no. 412 (1990): 1060\u201368. https:\/\/doi.org\/10.2307\/2289602.\r\n\r\nKudraszow, Nadia L., and Ricardo A. Maronna. \u201cEstimates of MM Type for the Multivariate Linear Model.\u201d Journal of Multivariate Analysis 102, no. 9 (October 2011): 1280\u201392. https:\/\/doi.org\/10.1016\/j.jmva.2011.04.011.\r\n\r\nLopuhaa, Hendrik P. \u201cAsymptotics of Reweighted Estimators of Multivariate Location and Scatter.\u201d The Annals of Statistics 27, no. 5 (1999): 1638\u201365.\r\n\r\nLopuha\u00e4, Hendrik P. \u201cMultivariate \u03c4-Estimators for Location and Scatter.\u201d The Canadian Journal of Statistics \/ La Revue Canadienne de Statistique 19, no. 3 (1991): 307\u201321. https:\/\/doi.org\/10.2307\/3315396.\r\n\r\nMuler, Nora, and V\u00b4ictor J. Yohai. \u201cRobust Estimation for Vector Autoregressive Models.\u201d Computational Statistics & Data Analysis, Special issue on Robust Analysis of Complex Data, 65 (September 2013): 68\u201379. https:\/\/doi.org\/10.1016\/j.csda.2012.02.011.\r\n\r\nRousseeuw, Peter J., Stefan Van Aelst, Katrien Van Driessen, and Jose Agull\u00f3. \u201cRobust Multivariate Regression.\u201d Technometrics 46, no. 3 (2004): 293\u2013305.\r\n","comments":[],"labels":["type-enh","comp-robust","comp-multivariate"]},{"title":"WIP\/ENH: add stats for complex valued random variables","body":"see \r\n#9064\r\n#3528\r\n\r\nThe main references that I used for this are in first comment of #9064.\r\n\r\ncurrently mainly tools to convert random variable and cov between 3 representations (to park those formulas)\r\neventual target is OLS, MLE and hypothesis tests (wald) when params are complex valued.\r\n\r\n","comments":["style check fails, whitespace errors, e.g. with empty lines\r\n\r\nNew files: statsmodels\/stats\/_complex.py statsmodels\/stats\/tests\/test_complex.py\r\nstatsmodels\/stats\/_complex.py:149:5: E303 too many blank lines (2)\r\nstatsmodels\/stats\/_complex.py:158:19: E251 unexpected spaces around keyword \/ parameter equals\r\nstatsmodels\/stats\/_complex.py:185:13: E221 multiple spaces before operator\r\nstatsmodels\/stats\/_complex.py:219:9: E303 too many blank lines (2)\r\nstatsmodels\/stats\/_complex.py:246:5: E303 too many blank lines (2)\r\nstatsmodels\/stats\/tests\/test_complex.py:18:1: E302 expected 2 blank lines, found 1\r\n\r\n"],"labels":["type-enh","comp-regression","comp-stats"]},{"title":"VECM calculation calls matrix inverse directly which is not numerically stable; a proposal for a better solution","body":"#### Describe the bug\r\n\r\nThere is vast literature in numerical linear algebra that advise against inverting the matrix directly unless you have a very good reason. Specifically in vecm.py, the _sij function calls matrix multiplication 6 times, and matrix inversion twice. This is not optimal for numerically stability, which is crucial for statistical computing applications. \r\n\r\nMy proposed improvement calls matrix multiplication 3 times, and SVD twice, and no matrix inversion needs to be called. Here is the code that produces exact same result.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\nstatsmodels\/statsmodels\/tree\/main\/statsmodels\/tsa\/vector_ar\/vecm.py\r\nline 416\r\n```python\r\ndef _sij(delta_x, delta_y_1_T, y_lag1):\r\n    \"\"\"Returns matrices and eigenvalues and -vectors used for parameter\r\n    estimation and the calculation of a models loglikelihood.\r\n\r\n    Parameters\r\n    ----------\r\n    delta_x : ndarray (k_ar_diff*neqs x nobs)\r\n        (dimensions assuming no deterministic terms are given)\r\n    delta_y_1_T : ndarray (neqs x nobs)\r\n        :math:`(y_1, \\\\ldots, y_T) - (y_0, \\\\ldots, y_{T-1})`\r\n    y_lag1 : ndarray (neqs x nobs)\r\n        (dimensions assuming no deterministic terms are given)\r\n        :math:`(y_0, \\\\ldots, y_{T-1})`\r\n\r\n    Returns\r\n    -------\r\n    result : tuple\r\n        A tuple of five ndarrays as well as eigenvalues and -vectors of a\r\n        certain (matrix) product of some of the returned ndarrays.\r\n        (See pp. 294-295 in [1]_ for more information on\r\n        :math:`S_0, S_1, \\\\lambda_i, \\\\v_i` for\r\n        :math:`i \\\\in \\\\{1, \\\\dots, K\\\\}`.)\r\n\r\n    References\r\n    ----------\r\n    .. [1] L\u00fctkepohl, H. 2005. *New Introduction to Multiple Time Series Analysis*. Springer.\r\n    \"\"\"\r\n    nobs = y_lag1.shape[1]\r\n    r0, r1 = _r_matrices(delta_y_1_T, y_lag1, delta_x)\r\n    ``` existing code\r\n    # s00 = np.dot(r0, r0.T) \/ nobs\r\n    # s01 = np.dot(r0, r1.T) \/ nobs\r\n    # s10 = s01.T\r\n    # s11 = np.dot(r1, r1.T) \/ nobs\r\n    # s11_ = inv(_mat_sqrt(s11))\r\n    # # p. 295:\r\n    # s01_s11_ = np.dot(s01, s11_)\r\n    # eig = np.linalg.eig(s01_s11_.T @ inv(s00) @ s01_s11_)\r\n    ```\r\n    # start of proposed improvement\r\n    u0, _, v0h = svd(r0.T, full_matrices=False)\r\n    u1, _, v1h = svd(r1.T, full_matrices=False)\r\n    R = u0.T @ u1 @ v1h\r\n    eig = np.linalg.eig(R.T @ R)\r\n    # end of proposed improvement\r\n    lambd = eig[0]\r\n    v = eig[1]\r\n    # reorder eig_vals to make them decreasing (and order eig_vecs accordingly)\r\n    lambd_order = np.argsort(lambd)[::-1]\r\n    lambd = lambd[lambd_order]\r\n    v = v[:, lambd_order]\r\n    return s00, s01, s10, s11, s11_, lambd, v\r\n```\r\nWhy it works:\r\n\r\nMy proposed solution only requires 4 lines of code. The derivation is as follows:\r\nWe are calculating the eigenvalues and eigenvectors of\r\n```\r\ns01_s11_.T @ inv(s00) @ s01_s11_\r\n```\r\nWe will replace r0 and r1 by their SVD in this expression. We see that the diagonal matrix of singular values cancel out. The numerical instability of inverse comes from ill-conditioned matrix where we take the inverse of singular values that are very close to 0, and taking the inverse of values very close to zero make the result blow up.\r\n\r\n","comments":["can the last `eig = np.linalg.eig(R.T @ R)` also be replaced by an svd(R) ?\r\n\r\nI'm not able to tell how this works from just reading through it, but we try to get this in before the next release.\r\n\r\nThanks for the improvement and code review."],"labels":["comp-tsa","Performance","prio-elev"]},{"title":"ENH: S-estimator and CM-estimator for outlier robust estimation","body":"just parking references to read more carefully for implementation\r\nI don't find an issue specifically for S-estimator.\r\n\r\nrelated #8997 MM-estimator\r\n\r\nDo we base the estimation on an objective function or on just the estimating equations?\r\nFor the former we need the nonlinear version of RLM to optimize w.r.t. beta and sigma. e.g. #3261\r\nFor the latter we either need explicit iteration (as in IRLS) alternating between beta and sigma iterations or rootfinding based M-estimators. #5674 #7436\r\n\r\n\r\nArslan, O. \u201cA Simple Test to Identify Good Solutions of Redescending M Estimating Equations for Regression.\u201d In Developments in Robust Statistics, edited by Rudolf Dutter, Peter Filzmoser, Ursula Gather, and Peter J. Rousseeuw, 50\u201361. Heidelberg: Physica-Verlag HD, 2003. https:\/\/doi.org\/10.1007\/978-3-642-57338-5_4.\r\n\r\nArslan, O., O. Edlund, and H. Ekblom. \u201cAlgorithms to Compute CM- and S-Estimates for Regression.\u201d Metrika 55, no. 1 (April 1, 2002): 37\u201351. https:\/\/doi.org\/10.1007\/s001840200185.\r\n\r\n\u2014\u2014\u2014. \u201cAlgorithms to Compute CM- and S-Estimates for Regression.\u201d In Developments in Robust Statistics, edited by Rudolf Dutter, Peter Filzmoser, Ursula Gather, and Peter J. Rousseeuw, 62\u201376. Heidelberg: Physica-Verlag HD, 2003. https:\/\/doi.org\/10.1007\/978-3-642-57338-5_5.\r\n\r\nThe last two references seem to be the same article.\r\n","comments":["tuning and consistency parameters are a pain\r\n\r\nS-estimators for location-scatter in R or other packages compute the tuckey biweight threshold parameter from the desired break down point. However, the main articles and books do not describe in details how this is done.\r\n\r\nA reference that seems to explain the details is\r\nRiani, Marco, Andrea Cerioli, and Francesca Torti. \u201cOn Consistency Factors and Efficiency of Robust  S-Estimators.\u201d TEST 23, no. 2 (June 1, 2014): 356\u201387. https:\/\/doi.org\/10.1007\/s11749-014-0357-7.\r\ntheorem 4.1 p. 370 looks like has the formula (requires function inversion, root finding) for the multivariate location-scatter case.\r\n\r\nOther than the two factors for tuning and consistency, the S-estimation seems to be relatively straight forward.\r\nAfter #8129, we should be able to do DetS instead of FastS, i.e. deterministic initial samples instead of a large number of random sampling to find the correct local optimum for (almost) global optimum.\r\n\r\n\r\n"],"labels":["type-enh","comp-robust"]},{"title":"ENH: make diagnostic and specification tests outlier robust or resistant","body":"another general theme\r\n\r\nMake diagnostic measures and specification tests outlier robust, which can be especially sensitive if we need higher moments.\r\nThe only issue we have, AFAIR, is jarque-bera #8286\r\n\r\n(This is a bit of a companion to making specification test misspecification robust)\r\n\r\n\r\njust parking two references (found by chance, I did not look specifically for this topic)\r\n\r\nAlih, Ekele, and Hong Choon Ong. \u201cAn Outlier-Resistant Test for Heteroscedasticity in Linear Models.\u201d Journal of Applied Statistics 42, no. 8 (August 3, 2015): 1617\u201334. https:\/\/doi.org\/10.1080\/02664763.2015.1004623.\r\n\r\nBerenguer-Rico, Vanessa, and Ines Wilms. \u201cHeteroscedasticity Testing after Outlier Removal.\u201d Econometric Reviews 40, no. 1 (January 2, 2021): 51\u201385. https:\/\/doi.org\/10.1080\/07474938.2020.1735749.\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["topic-diagnostic","topic-post_estim"]},{"title":"ENH\/DOC  WLS when weights are not var_weights (e.g. survey weighting)","body":"Reading Li and Valliant\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/9067#issuecomment-1823299955\r\n\r\nThey have a case where the model implied cov_params for WLS params is\r\n\r\ncov(beta) = sigma^2 A^{-1} X' W^2 X A^{-1} where A = X' W X\r\n\r\nwhy is W squared in the inner part of the sandwich?\r\n\r\nFrom GLS\/WLS:\r\n\r\nparams = (X' W X)^{-1} X' W y\r\nsandwich cov_params cov(beta) = A^{-1} [X' W V W X] A^{-1}  if V = cov(u) (nobs, nobs)\r\n\r\n- If W is correctly specified var_weights, then W = V^{-1}, then sandwich simplifies to cov(beta) = sigma^2 A^{-1} = sigma^2 (X' W X)^{-1} \r\n   This is default nonrobust cov_params of WLS \r\n- If W is not correctly specified var_weights, then\r\n  - general case: We need the (HC) sandwich\r\n  - special case: V = I, i.i.d., homoscedastic variance but W is some weighting we use, then\r\n     cov(beta) = sigma^2 A^{-1} X' W^2 X A^{-1}\r\n\r\n(see also Green chapter on heteroscedasticity, ch. 11 in 5th edition)\r\n\r\n\r\nConclusions:\r\n- WLS cov_params is incorrect if weights are not correctly specified inverse variances.\r\n- generic sandwich form (Huber, White, GMM, HC) provides correct cov_params\r\n- We might want to impose additional assumptions on cov(momend_conditions) and avoid robustness to unspecified misspecification\/heteroscedasticity., e.g. the last case above.\r\n\r\nThe last one is currently not available in WLS or GLM with some weights.\r\n\r\nrelated\r\n#7861\r\n#6794 \r\n#1379 RLM cov_params is not HC\r\n#5327 cov_type for Theil\/Ridge, maybe related\r\n#1074 modelled but misspecified heteroscedasticity WLS+HC\r\nand others\r\n\r\nIt's not yet clear to me what this implies for different models and cases.\r\nIPW, survey weighting: poisson with some weights but assuming correctly specified distribution ?\r\n\r\n\r\n\r\n ","comments":["more general estimating equations but more specific to survey\/calibration weighting\r\n\r\nBinder, David A., and Zdenek Patak. \u201cUse of Estimating Functions for Estimation from Complex Surveys.\u201d Journal of the American Statistical Association 89, no. 427 (1994): 1035\u201343. https:\/\/doi.org\/10.2307\/2290931.\r\n\r\nWu, Changbao, and Mary E. Thompson. Sampling Theory and Practice. ICSA Book Series in Statistics. Cham: Springer International Publishing, 2020. https:\/\/doi.org\/10.1007\/978-3-030-44246-0.\r\n\r\nsection 7.2 Survey Weighted Estimating Equations looks interesting (based on very brief look)\r\nAFAICS, the inner part of the sandwich cov(moms) depends on the sampling scheme of the survey design.\r\n\r\nrelated\r\nit looks like \"calibration weighting\" is the term for weighting towards a target population\r\nfor example\r\nWu, Changbao, and Wilson W. Lu. \u201cCalibration Weighting Methods for Complex Surveys.\u201d International Statistical Review 84, no. 1 (2016): 79\u201398. https:\/\/doi.org\/10.1111\/insr.12097.\r\n\r\n#3759 and maybe other issues\r\nnote: \r\nUsing weights in regression computes target average even if there is unmodelled heterogeneity.\r\nsimilar to semi-\/non-parametric identification of ATE, ...\r\nUsing weights in predict based on sample-estimated model assumes model is correctly specified for target population\/exog\r\n\r\n","another possible application:\r\n\r\nclass weights in machine learning\r\ne.g. increase weight of rare events in sample.\r\nThis needs corrected cov_params (which the machine learning packages don't care about)\r\n","ok that gives enough background to add another weight class to GLM, ...\r\np_weights, prob_weights ?\r\n\r\nfreq_weights and var_weights are part of the likelihood model, so MLE is well defined.\r\npweights are not appropriate with MLE, we need an M-estimator with sandwich form of cov_params, but maybe not the generic HC, ... We have a very specific deviation from the score function in the estimating equation, that we can take into account without, for example, giving up on the assumption of correctly specified likelihood.\r\n\r\n#4234\r\n\r\n**update**\r\nWe should get a more generic name for those weights.\r\nInterpretation is that they just multiply the estimating equations for an M-estimator.\r\nee_weights (estimating equations)\r\ng_weights (generic)\r\nest_weights (estimation)\r\na_weights (imitating Stata)\r\nw_weights (weighting weights :)\r\n...\r\n?"],"labels":["type-enh","comp-genmod","comp-regression","topic-covtype","topic-weights"]},{"title":"Can statsmodels.MNLogit handle scenarios with only two categories?","body":"#### Is your feature request related to a problem? Please describe\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\n\r\nCan statsmodels.MNLogit handle scenarios with only two categories, or does it automatically convert to sm.Logit in cases where there are exactly two categories?\r\nIf statsmodels.MNLogit encounters a scenario with only two categories, it will essentially behave like the standard logistic regression, and the model will automatically simplify to sm.Logit.\r\n\r\nMy dataset contains instances with only two categories and others with more than two categories. I would like to know if statsmodels.MNLogit has the functionality to handle both scenarios. I appreciate your assistance.\r\n\r\n\r\n#### Describe the solution you'd like\r\nA clear and concise description of what you want to happen.\r\n\r\nIf statsmodels.MNLogit encounters a scenario with only two categories, it will essentially behave like the standard logistic regression, and the model will automatically simplify to sm.Logit.\r\n\r\n\r\n#### Describe alternatives you have considered\r\nA clear and concise description of any alternative solutions or features you have considered.\r\n\r\n#### Additional context\r\nAdd any other context about the feature request here.","comments":["Did you try?\r\n\r\nIt should handle automatically the case when endog is binary, but I never looked at that.\r\nA problem could be if there are any computations that convert something to 1-dim that is 2-dim in the general case.\r\n\r\nIt needs a test case to compare all results with Logit.\r\n(I did that for OrderedModel, but never for MNLogit, AFAIR.)\r\n"],"labels":["comp-discrete"]},{"title":"ENH: combined model checks, diagnostics, outlier influence, multicollinearity","body":"support semi-automatic model checking, diagnostics, data problems, ...\r\n\r\n#6381, #1908 sanity checks, for model\/data (pre-estimation checks), not post-estimation results.\r\n#1126 White's specification test\r\n#7834 diagnostic classes, get_diagnostic\r\n\r\n\"\r\nEven with a vast arsenal of diagnostics, it is very hard to write down rules that can be used to guide a data analysis. So much is really subjective and subtle.\r\n\"\r\nin concluding remarks of\r\nWelsch, Roy E. \u201c[Influential Observations, High Leverage Points, and Outliers in Linear Regression]: Comment.\u201d Statistical Science 1, no. 3 (August 1986): 403\u20135. https:\/\/doi.org\/10.1214\/ss\/1177013625.\r\n\r\nHe is referring to an MIT software project that tried to have support for automatic \"decision making\". \r\nBased on the low number of citation of the referenced paper by Oldford and Peters and of follow-up papers, this never became popular.\r\n\r\nOne tool we could add is an expanded version of the regression summary table 3.\r\ne.g. a new set of classes ModelChecks that attaches and delegates to the influence, Diagnostic, Multicollinearity, ... classes and provides an overall summary.\r\nFor outlier influence we would need explicit thresholds to issue alerts.\r\n\r\nI guess we should add an option for \"data relationship\" type like \"sequential\", \"cross-section\", \"grouped\" to have the appropriate diagnostics,\r\ne.g. ols summary combines sequential autocorrelation tests and cross-section tests which will not apply to all types of data.\r\n\r\nFor example, we could warn users \r\nif they use a nonrobust cov_type but have large heteroscedasticity or residual correlation or excess dispersion. IM ratio could choose sandwich according to the data structure.\r\nor if some outlier influence measures like cooks distance are very high for some observations.\r\n\r\nThis will only become relevant once we have the Diagnostic classes and corresponding specification test for the different models.\r\n\r\n\r\n\r\n\r\n","comments":["interesting case as motivation\r\nhttps:\/\/stats.stackexchange.com\/questions\/637898\/why-do-my-bootstrapped-coefficient-standard-errors-differ-from-statsmodels-stand\r\n\r\nlarge sample, few parameters asymptotic results should be good if underlying assumptions hold.\r\nIf asymptotic results don't look right, then we need a quick check what might be wrong, i.e. which assumptions are too strongly violated, either for parameter estimation or for inference.\r\n(my first guess in the stackexchange question was heteroscedasticity, the second guess is influential points)\r\n\r\n(Aside: we don't have spatial correlation robust standard errors. AFAIR, I played with it but did not merge anything for it.)"],"labels":["type-enh","topic-diagnostic","topic-post_estim"]},{"title":"ENH: WLS outlier influence","body":"https:\/\/github.com\/statsmodels\/statsmodels\/blob\/5598e1034a76a515fa0d71e28bdb8d1c32d9ecd7\/statsmodels\/stats\/outliers_influence.py#L517C36-L517C36\r\n\r\nIs it planned to include weighted Studentized residuals to WLS summary table?\r\n(The paper https:\/\/doi.org\/10.1007\/BF03014491 contains some formulas.)","comments":["Thanks for the reference\r\nIt's very useful to finally have a reference explicitly for the WLS case\r\n\r\nrelated issue #4268  (which became more focused on GLM than WLS, so I keep this issue open for focusing on WLS)\r\n\r\nThis should be target for 0.15 together with related issues like #9009\r\n\r\n\r\n","The article is a bit disappointing, missing equ 3.2 to 3.4\r\nH matrix is not a hat matrix, does not include the final weight w, i.e. in most places h has to be multiplied by w.\r\nThesis is not accessible online, AFAICS, so we cannot check the derivation.\r\n\r\nkind of interesting: the example at the end uses weights from RLM with huber norm.\r\n\r\nI found another branch of the literature that uses influence outlier for WLS in survey sample analysis\r\ne.g.\r\nLi, Jianzhu, and Richard Valliant. \u201cLinear Regression Influence Diagnostics for Unclustered Survey Data.\u201d Journal of Official Statistics 27, no. 1 (2011): 99\u2013119.\r\n\r\nhttps:\/\/www.scb.se\/contentassets\/ca21efb41fee47d293bbee5bf7be7fb3\/linear-regression-influence-diagnostics-for-unclustered-survey-data.pdf\r\n\r\n(brief check, they define hat matrix with W at the end, i.e. the projection matrix for prediction and not the wendog\/wexog version\r\n\r\n"],"labels":["comp-regression","topic-diagnostic","prio-elev","topic-post_estim"]},{"title":"ENH: statistics and distribution for complex valued random variables","body":"related to #3528 but more general or different focus\r\n\r\nWe could add statistics and models for complex random variables with almost the usual coverage (mean, cov, params, ...).\r\n\r\nI did not find much on standard statistics, standard errors, cov_params, asymptotic distribution for hypothesis tests.\r\nHowever, complex normal distributions are defined through their bivariate representation of real and imaginary parts.\r\nSo, we can use all the standard statistics and hypothesis for real and imaginary parts.\r\n\r\nIf random variable is second order circular (alias proper), then this imposes restrictions on the covariance of the real and imaginary parts. That covariance has to be constructed specifically for those `proper` cases. (This case can also be directly computed with \"standard\" complex computation.)\r\nIf complex random variable is improper, then it corresponds to a bivariate normal distribution without restrictions on cov (except positive semidefinite)\r\n\r\nwhat we need:\r\n\r\n- functions that work directly for `proper` complex random variables\r\n- functions, classes for bivariate random variables, e.g. MultivariateOLS\r\n- inference for both single hypothesis (on one real or imaginary part of parameters), e.g. use cov_params for [p.real, p.imag] and joint hypothesis for complex parameter (i.e. using joint distribution of real and complex parts)\r\n- wrapper function\/class\r\n  - internally convert to whichever representation is convenient\r\n  - double `results` for both complex parameters and bivariate representation of parameters, methods to specify hypothesis on complex parameter and methods for hypothesis on [real, imag] parts of complex parameters. \r\n- tools:\r\n  - conversion between different representations (complex, bivariate-real, extended complex)\r\n  - descriptive statistics, e.g. cov, pcov\r\n  - specific functions, e.g. test for second order circularity, Taylor estimate of complex scatter\r\n- specific models, OLSComplexCircular, OLSComplexImproper and MultivariateLS (multivariate complex endog) versions\r\n  - models corresponding to nonlinear LS might be more frequently used than linear models \r\n- multivariate statistics (maybe eventually)\r\n  - independent component analysis ICA seems to be popular\r\n  - CCA, e.g. used for canonical correlation for x and x.conj()\r\n  - hypothesis tests for patterned cov, (and pcov ?)\r\n  - ... ?\r\n\r\ndefinitions\r\n- circular: distribution is rotation invariant, i.e. assumption on full distribution, all moments (original definition)\r\n- second order circular (proper): assumption on second moments (pseudo-cov is zero), no assumption on higher moments\r\n\r\ndisadvantage\r\nI guess this will not have a large user base. Signal processing literature does not have much traditional statistics (including inference on mean parameters), and econometrics in signal processing is more time series analysis and forecasting.\r\n\r\nmain references (not complete)\r\n\r\nAdali, T\u00fclay, Peter J. Schreier, and Louis L. Scharf. \u201cComplex-Valued Signal Processing: The Proper Way to Deal With Impropriety.\u201d IEEE Transactions on Signal Processing 59, no. 11 (November 2011): 5101\u201325. https:\/\/doi.org\/10.1109\/TSP.2011.2162954.\r\n\r\nOllila, E., and V. Koivunen. \u201cGeneralized Complex Elliptical Distributions.\u201d In Processing Workshop Proceedings, 2004 Sensor Array and Multichannel Signal, 460\u201364, 2004. https:\/\/doi.org\/10.1109\/SAM.2004.1502990.\r\n\r\nOllila, Esa. \u201cOn the Circularity of a Complex Random Variable.\u201d IEEE Signal Processing Letters 15 (2008): 841\u201344. https:\/\/doi.org\/10.1109\/LSP.2008.2005050.\r\n\r\nOllila, Esa, Jan Eriksson, and Visa Koivunen. \u201cComplex Elliptically Symmetric Random Variables\u2014Generation, Characterization, and Circularity Tests.\u201d IEEE Transactions on Signal Processing 59, no. 1 (January 2011): 58\u201369. https:\/\/doi.org\/10.1109\/TSP.2010.2083655.\r\n\r\nOllila, Esa, Visa Koivunen, and H. Vincent Poor. \u201cComplex-Valued Signal Processing \u2014 Essential Models, Tools and Statistics.\u201d In 2011 Information Theory and Applications Workshop, 1\u201310, 2011. https:\/\/doi.org\/10.1109\/ITA.2011.5743596.\r\n\r\nOllila, Esa, David E. Tyler, Visa Koivunen, and H. Vincent Poor. \u201cComplex Elliptically Symmetric Distributions: Survey, New Results and Applications.\u201d IEEE Transactions on Signal Processing 60, no. 11 (November 2012): 5597\u20135625. https:\/\/doi.org\/10.1109\/TSP.2012.2212433.\r\n\r\nPicinbono, B. \u201cOn Circularity.\u201d IEEE Transactions on Signal Processing 42, no. 12 (December 1994): 3473\u201382. https:\/\/doi.org\/10.1109\/78.340781.\r\n\r\n\u2014\u2014\u2014. \u201cSecond-Order Complex Random Vectors and Normal Distributions.\u201d IEEE Transactions on Signal Processing 44, no. 10 (October 1996): 2637\u201340. https:\/\/doi.org\/10.1109\/78.539051.\r\n\r\nPicinbono, B., and P. Bondon. \u201cSecond-Order Statistics of Complex Signals.\u201d IEEE Transactions on Signal Processing 45, no. 2 (February 1997): 411\u201320. https:\/\/doi.org\/10.1109\/78.554305.\r\n","comments":["finally, I found some literature on cov_params (I have not looked at details yet.)\r\n\r\nThe signal processing literature for cov_params is under the term \"Cramer-Rao Bound\"\r\ni.e. cov_params for MLE (Fisher information matrix, expected OPG)\r\n\r\nFortunati et al 2016 includes sandwich form for misspecified likelihood and M-estimators but only for proper complex r.v.\r\nOllila et al 2008 includes RLB for improper complex r.v.\r\nFortunati 2017 also includes misspecified non-circular\/improper models. section 5 example for misspecified circularity when estimating a simple\/not-wide linear model.\r\n\r\nFortunati, Stefano. \u201cMisspecified Cram\u00e9r-Rao Bounds for Complex Unconstrained and Constrained Parameters.\u201d In 2017 25th European Signal Processing Conference (EUSIPCO), 1644\u201348, 2017. https:\/\/doi.org\/10.23919\/EUSIPCO.2017.8081488.\r\n\r\nFortunati, Stefano, Fulvio Gini, and Maria S. Greco. \u201cThe Misspecified Cramer-Rao Bound and Its Application to Scatter Matrix Estimation in Complex Elliptically Symmetric Distributions.\u201d IEEE Transactions on Signal Processing 64, no. 9 (May 2016): 2387\u201399. https:\/\/doi.org\/10.1109\/TSP.2016.2526961.\r\n\r\nOllila, Esa, Visa Koivunen, and Jan Eriksson. \u201cOn the Cram\u00e9r-Rao Bound for the Constrained and Unconstrained Complex Parameters.\u201d In 2008 5th IEEE Sensor Array and Multichannel Signal Processing Workshop, 414\u201318, 2008. https:\/\/doi.org\/10.1109\/SAM.2008.4606902.\r\n","detail for constrained cov estimation\r\n\r\nOllila et al 2012, theorem 6 explanation, proof relies on equivalence between real bivariate and complex representation, however it is for the second order circular case. (So I got confused)\r\ndata has length 2*nobs\r\n\r\nIt looks like they are stacking the bivariate real vector, z = x + j y,  stacked is v = [[x, y], [-y, x]].\r\nthen estimate cov or scatter using Tyler's scatter estimate.  \r\nSmall differences to the empirical covariance (estimated from complex, which ignores pseudo-cov)\r\n\r\n```\r\ny2 = np.vstack((y, np.column_stack((- y[:, -2:], y[:, :2]))))\r\ny2.mean(0)\r\narray([-0.00071712, -0.00750972, -0.00359257,  0.00438998])\r\n\r\n# implied cov and pcov\r\nc2 = np.cov(y2.T, ddof=0)\r\ncov_from_rvec(c2)\r\n(array([[1.97897467+0.j        , 0.00516927-0.00691401j],\r\n        [0.00516927+0.00691401j, 2.00438595+0.j        ]]),\r\n array([[ 1.23923196e-05+0.j        , -2.11567142e-05-0.00691401j],\r\n        [-2.11567142e-05+0.00691401j, -3.71239976e-05+0.j        ]]))\r\n\r\n# sample estimates from complex variables\r\ncovx, pcovx\r\n(array([[1.97896125+0.j        , 0.00517966-0.00694414j],\r\n        [0.00517966+0.00694414j, 2.00431028+0.j        ]]),\r\n array([[0.00404063-0.00518913j, 0.00051496+0.00378722j],\r\n        [0.00051496+0.00378722j, 0.01313544+0.00295734j]]))\r\n\r\nnp.cov(x, rowvar=False, ddof=0)\r\narray([[1.97896125+0.j        , 0.00517966-0.00694414j],\r\n       [0.00517966+0.00694414j, 2.00431028+0.j        ]])\r\n```\r\n\r\nnobs is 50000, so there might also be precision issues when computing statistics in different ways \r\n\r\nThis vertical stacking looks like an interesting useful tool to impose equal cov across sub-matrices.\r\n\r\nOllila et al only look at Tyler's or M estimation of scattermatrix for second order circular case.\r\nHowever,\r\n- if we use the original bivariate real data cov([x, y]), then we should get the M-estimator for the unrestricted cov_real and from it the implied M-estimator for cov and pcov of complex variable\r\n- we can compare this to separately computing M-estimator of cov and pcov from the complex variables.\r\n\r\nextending this to non-gaussian scatter matrix requires Tyler\/M-estimation in PR #8129\r\nOllila et al have weight functions for Tyler's M-estimator for several elliptically circular distributions. The PR only includes weight function for t-distribution, AFAIR\r\n\r\n\r\n "],"labels":["type-enh","comp-regression","comp-stats"]},{"title":"ENH: GLM check for valid endog","body":"https:\/\/stats.stackexchange.com\/questions\/630757\/glm-invalid-value-encountered-in-log-special-gammaln\r\n\r\nuses GLM with Binomial family\r\nendog is a count variable but no n_trials is specified.\r\nResults look wrong, but only a warning in gammaln is issued.\r\n\r\nMy guess is that somewhere in the computation we don't check that the terms in the computation are in the valid range.\r\n\r\nrelated issue: inappropriate models like log-binomial\r\nThose are for \"foreign\" links or variance functions that don't impose the correct domain.\r\nBut, I think that does not affect checking for valid endog in required range (QMLE can have incorrect endog, e.g. continuous instead of discrete, but I guess it still has to satisfy bound constraints for support of distribution [0, 1], [0, n_trials], R+, R++.\r\n\r\ncode from the question, with added imports:\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\n\r\n# creating the Numpy array \r\narray = np.array([[4., 441, 25, 4, 17], \r\n                  [10, 444, 49, 8, 9], \r\n                  [3, 483, 12, 2, 38],\r\n                  [2,447,10,1,43],\r\n                  [4,423,22,3,19],\r\n                  [3,19,19,3,1],\r\n                  [18,445,111,17,4],\r\n                  [7,423,42,7,10],\r\n                  [9,426,53,8,8],\r\n                  [12,378,54,8,7],\r\n                  [36,450,225,36,2],]) \r\n   \r\n# creating a list of column names \r\ncolumn_values = ['CONST', 'ACTUAL_PRODTIME', 'TIME_LOGO', 'COST_PPL_LOGO', 'LOGO'] \r\n  \r\n# creating the dataframe \r\ndf = pd.DataFrame(data = array,   \r\n                  columns = column_values) \r\n\r\nmodel = smf.glm(\r\n                formula = \"CONST ~ ACTUAL_PRODTIME + TIME_LOGO + COST_PPL_LOGO + LOGO\",\r\n                data = df, \r\n                family = sm.families.Binomial())\r\n# Fit the model\r\nresult = model.fit()\r\n\r\n# Display and interpret results\r\nprint(result.summary())\r\n```","comments":[],"labels":["type-bug","type-enh","comp-genmod","prio-elev"]},{"title":"BUG: regression summary() computes diagnostic statistics even if slim=True","body":"use case #3528\r\ncomplex data in OLS raises exception in `summary()`. It fails in omni_normtest.\r\n\r\nIf I use `.summary(slim=True)`, then that still raises at the same place because the diagnostics are computed even though we don't include them in the summary.\r\n\r\n","comments":[],"labels":["type-bug","comp-regression"]},{"title":"ENH: option to flip plot axes, vertical error bars in tukeyhsd plot_simultaneous","body":"plot_simultaneous has horizontal error bars. pairs are on y-axis, diff values are on x axis.\r\n\r\nrequest for flipping x and y axis:\r\nhttps:\/\/stackoverflow.com\/questions\/77409496\/flipping-the-x-y-axis-in-diagram-resulting-from-plot-simultaneous-in-a-tukey\r\n\r\nwe might have other similar functions that might benefit for the same option\r\ne.g.  the standard plot in meta-analysis also has horizontal error bars.\r\n\r\n","comments":[],"labels":["type-enh","comp-graphics","comp-stats"]},{"title":"Panel Vector Autoregression","body":"Hi, I saw statsmodel currently has VAR\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.vector_ar.var_model.VAR.html\r\n\r\nBut, I didn't find VAR for panel dataset.\r\nCould you please consider add the panel var?\r\n\r\nThank you.","comments":["We don't have anyone actively working on panel VAR models, so it is unlikely that this model would be implemented in Statsmodels in the near future, unless you have an interest in contributing it."],"labels":["comp-tsa"]},{"title":"ENH: GMM special cases: exponential mean function (log-link) and quadratic or power variance function","body":"looking by chance at Basu, Rathouz 2005 again https:\/\/github.com\/statsmodels\/statsmodels\/issues\/624#issuecomment-1494917639\r\n\r\nand a bit at\r\nManning, Willard G., Anirban Basu, and John Mullahy. \u201cGeneralized Modeling Approaches to Risk Adjustment of Skewed Outcomes Data.\u201d Journal of Health Economics 24, no. 3 (May 1, 2005): 465\u201388. https:\/\/doi.org\/10.1016\/j.jhealeco.2004.09.011.\r\n\r\nBasu Rathouz have 3 types of moment conditions, wrt. mean, link and variance parameters.\r\nIt does not necessarily need a quasi-likelihood function, e.g. variance function is `a0 + a1 mu + a2 mu**2` or `a * mu**p)\r\n(we might have some other issues with variance functions that also include generalized Poisson GPP, NBP variance.)\r\nBR use box-cox link for mean function.\r\n\r\n This should be reasonably easy to implement as generic GMM. We need a model with the moment conditions.\r\nWe don't need the full multi-link version of models if we use hardcoded parameterized link functions.\r\n\r\nFor GMM start_params we could take Poisson, exp mean and var = mu\r\n\r\n","comments":[],"labels":["type-enh","comp-genmod","comp-discrete","comp-othermod"]},{"title":"Recursive errors with BetaModel","body":"#### Describe the bug\r\n\r\n[A clear and concise description of what the bug is. This should explain **why** the current behaviour is a problem and why the expected output is a better solution.]\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nfrom statsmodels.othermod import betareg\r\n\r\nbeta_glm = betareg.BetaModel(Y_dum_f1.astype(float), X_dum.astype(float))\r\nbeta_result = beta_glm.fit()\r\nbeta_result.summary()\r\n```\r\n<details>\r\nWith same Y and X, I ran a couple of GLM models in statsmodels.api, but with BetaModel I constantly face Error message that is blank:\r\n\r\nThe expected output is a summary table of the model result. \r\nMy data has a total of 7776 observations, endog is a 1-d array and exog is 7776*10 (10 variables).\r\n<img width=\"823\" alt=\"Screenshot 2023-10-30 at 12 17 57\u202fPM\" src=\"https:\/\/github.com\/statsmodels\/statsmodels\/assets\/102279867\/9467e0a8-1af7-4b06-b114-3ccd97928784\">\r\n","comments":["The failing assert means that your endog y data is not inside the (0, 1) interval.\r\nNote, open interval, 0 and 1 are not included.\r\n\r\nThis assert could be converted to a proper input check with a intentional raise and an informative error message (similar to Logit, Probit).\r\n\r\nWhat are the values in your `Y_dum_f1.astype(float)` array?\r\n","I'm not sure I checked 0, 1 observations for Beta model.\r\n\r\nAFAIR: for some alpha and beta parameter values, the pdf at 0 and 1 is finite. I'm not sure whether we can loosen the restriction if we detect those cases. \r\nIn other cases the pdf(0) and pdf(1) go to infinity.\r\n\r\nAnother option would be to automatically clip the endog to be in the interior of (0, 1) interval. But I don't remember how numerically stable pdf, score and similar are in the 0 and 1 neighborhoods.\r\n\r\n","Hi thank you, my engog ranges between 0 and 1, they are probabilities. Now I tried rerunning by deleting the \"asssert\" part of the code, I see the following result:\r\n<img width=\"598\" alt=\"Screenshot 2023-10-30 at 12 41 53\u202fPM\"\r\n\r\nWhich I didn't expect as an ideal outcome... src=\"https:\/\/github.com\/statsmodels\/statsmodels\/assets\/102279867\/feb91f51-c161-4d6f-8bda-d6a60002e48f\"\r\n","The beta model is not well defined if there are 0 or 1 endog values.\r\nIn your example it looks like the parameter estimates are ok, but the Hessian computation fails, most likely some division by zero.\r\n\r\nAs workaround you can np.clip endog to stay away from 0 and 1.\r\n\r\nIf 0 and 1 endog observations are \"by design\", then the beta model is not appropriate for those observations and those could be dropped.\r\nThere are 0, 1 inflated Beta models in the literature, but I have not implemented any of those yet. #7918 #9013 \r\n(in the simple cases a continuous-discrete mixture can be estimated by splitting the data into separate parts for the continuous and discrete cases\r\n\r\nRelated:\r\nIf endog are observed relative frequencies for given number of experiments, then GLM Binomial can handle 0 and one frequencies with var_weights. The numerical restriction in GLM Binomial is that the expected mean, i.e. estimated probabilities stay away from 0 and 1, but observations can be at the boundary points.\r\n","score_hessian_factor computes expression like `np.log(y \/ (1. - y))` which results in +\/- inf at the boundaries.\r\nThe method includes clipping for expressions for expected params, e.g. alpha and beta (implied by mu and variance), but no clipping for y.\r\n\r\nClipping of y needs to be intentional and not for numerical reasons, i.e. checks and possible workarounds need to be in `model.__init__`. \r\n\r\nClipping in the parameters is mainly to facilitate optimization. I don't know whether there is a check for the Beta regression similar to perfect separation\/prediction in binary models, e.g. warn or raise if alpha or beta go to zero, or predicted mean goes to zero or one for some observations.\r\n\r\n\r\n \r\n","I guess what should be done here first is to raise an informative Exception if not 0 < y < 1, i.e. in interior of unit interval."],"labels":["type-enh","comp-othermod"]},{"title":"BUG: Adjust eval environment for stack depth","body":"- [x] closes #9037\r\n- [x] tests added \/ passed. \r\n- [x] properly formatted commit message. See \r\n      [NumPy's guide](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/dev\/gitwash\/development_workflow.html#writing-the-commit-message). \r\n\r\nThis PR fixes the formula environment for the conditional logit and Poisson, the proportional hazards model, GEE, and QIF.\r\n","comments":["Overall PR looks good.  Do you know if other formulas suffer, or is this the full extent fo the problem?","There are a few linting issues that need to be fixed.  You can ignore MICE errors (needs a deep dive).\r\n","The problem appears in several other `from_formula`s, but gets more complicated because multiple formulas are used. For example, `BetaModel` takes both the main formula and an optional `exog_precision_formula`. That second formula isn't parsed with an eval environment, so I expect that it would similarly grab the environment from a statsmodels file rather than the user's environment.\r\n\r\nI was trying to sidestep those cases for my first contribution (sorry about the linting error) but I just noticed `GEE.from_formula` has an optional second formula too. If you don't mind a longer PR, I can extend the fix to those other implementations and their other formulas.\r\n\r\nWhat do you think about putting `eval_env` into the method signatures directly? I'm not sure if it would have to be a keyword-only argument to avoid breaking possible uses."],"labels":["prio-high","type-bug","comp-formula"]},{"title":"ENH: measurement error with calibration sample, repeat measurements","body":"#3187 assumes known measurement error (co)variances\r\n\r\nanother strand of literature (and an R package) looks at simultaneously estimating the measurement error or it's variances by using more information from either and internal subsample or an external sample.\r\n\r\nI have not looked much at the details, one reference for now.\r\nThe commonly used term is \"calibration\".\r\nSome parts are similar to IV, but not with the same full sample (either subsample or separate sample)\r\n(We might run into #8773 when reusing GMM or similar horizontal stacking)\r\n\r\nFreedman, Laurence S., Douglas Midthune, Raymond J. Carroll, and Victor Kipnis. \u201cA Comparison of Regression Calibration, Moment Reconstruction and Imputation for Adjusting for Covariate Measurement Error in Regression.\u201d Statistics in Medicine 27, no. 25 (November 10, 2008): 5195\u20135216. https:\/\/doi.org\/10.1002\/sim.3361.\r\n\r\n(This is currently not on my near term roadmap)","comments":[],"labels":["type-enh"]},{"title":"ENH: misclassification, measurement error for categorical, binary variables","body":"related to #3187 measurement error models\r\n\r\nWhat if respondents to a survey don't answer with or remember the \"true\" answer to a binary or multiple choice question?\r\n\r\nchapters 2 and 3 for simple `stats.proportions` \r\n\r\nBuonaccorsi, John P. Measurement Error: Models, Methods, and Applications. New York: Chapman and Hall\/CRC, 2010. https:\/\/doi.org\/10.1201\/9781420066586.\r\n\r\nrelated problem for measurement errors\/misclassifiction of regressors ","comments":["related\r\nmeasurement error depending on true value - What difference does it make to the models and estimators?\r\ne.g. bias answer to what is \"politically correct\" or sounds better."],"labels":["type-enh","comp-discrete","comp-stats"]},{"title":"ENH: control function IV with multinomial treatment","body":"related\r\n#8756 categorical, multinomial, multiple treatment with conditional independence `treatment` effect\r\n#7689 general issue for control function approach using IV for endogenous treatment.\r\n\r\nThe basic model extension that we need here is to use MNLogit or other categorical endog models (OrderedModel ?) as first stage for residual inclusion or control function methods.\r\n\r\nI never used MNLogit for something similar (e.g. as moment conditions in GMM). I don't know if MNLogit has all the pieces.\r\n\r\n- residuals, pearson, generalized ?\r\n- score_obs and moment conditions\r\n\r\ne.g.\r\nGeraci, Andrea, Daniele Fabbri, and Chiara Monfardini. \u201cTesting Exogeneity of Multinomial Regressors in Count Data Models: Does Two-Stage Residual Inclusion Work?\u201d Journal of Econometric Methods 7, no. 1 (January 1, 2018). https:\/\/doi.org\/10.1515\/jem-2014-0019.\r\n(earliest versions in my Zotero, working papers in 2014 and 2015)\r\n\r\n\r\nrelated: \r\nMNLogit as outcome model with endogenous regressors. What's the status of score_test in MNLogit? AFAIR, I never tried.\r\n\r\n","comments":[],"labels":["type-enh","comp-causal"]},{"title":"ENH: MNLogit identification with categorical variables","body":"https:\/\/stackoverflow.com\/questions\/77352027\/mnlogit-summary-return-a-array-with-a-lot-of-nan\r\n\r\nMy guess would be that if we have categorical variables, then we need observations for each endog\/exog-cat cell.\r\nBut I'm not sure what identification condition we need.\r\n\r\npossible workaround:\r\nadd option `hessian_pinv=True` to use pinv for hessian that is only semi-definite but not positive definite.\r\n\r\n","comments":["This might be a separation case for nonexistence of MLE\r\n\r\nCook, Scott J., John Niehaus, and Samantha Zuhlke. \u201cA Warning on Separation in Multinomial Logistic Models.\u201d Research & Politics 5, no. 2 (April 1, 2018): 2053168018769510. https:\/\/doi.org\/10.1177\/2053168018769510.\r\n\r\n\"standard\" solution is Firth penalization, but other penalization might be better (in some cases, analogous to Logit)\r\n\r\nBeiser-McGrath, Liam F. \u201cSeparation and Rare Events.\u201d Political Science Research and Methods 10, no. 2 (April 2022): 428\u201337. https:\/\/doi.org\/10.1017\/psrm.2020.46.\r\n\r\naside:\r\nI never tried whether the PenalizedMixin works with MNLogit, my usual example cases were GLM, Poisson and Logit.\r\n"],"labels":["type-enh","comp-discrete","topic-covtype"]},{"title":"ENH: diagnostic plots with errors in variables, instrumental variables, measurement error","body":"problem that I was not aware of as generic issue\r\n\r\nIn diagnostic plots like residual plots, CPR, ..., we want to have no (or known) pattern of the graph if the null model holds. This requires that there is no correlation between the variables on x and y axis under the null. Deviations can be interpreted as violation of null model.\r\n\r\nProblem with errors in variables can be that \"residual\" are correlated with the plot-x-variable.\r\nThis means we need to choose the variables on the axis so that they are (approximately) orthogonal.\r\n(similar to orthogonality conditions in specification testing, CMT, ...)\r\n\r\nissue triggered by measurement error, where we should use the \"predicted exog\" and measurement error corrected residuals.\r\n\r\nCarroll, R. J., and C. H. Spiegelman. \u201cDiagnostics for Nonlinearity and Heteroscedasticity in Errors-in-Variables Regression.\u201d Technometrics 34, no. 2 (May 1, 1992): 186\u201396. https:\/\/doi.org\/10.1080\/00401706.1992.10484907.\r\n\r\nRomeo, Giovanni, John P. Buonaccorsi, and Magne Thoresen. \u201cDetecting and Correcting for Heteroscedasticity in the Presence of Measurement Error.\u201d Communications in Statistics - Simulation and Computation 0, no. 0 (2023): 1\u201317. https:\/\/doi.org\/10.1080\/03610918.2023.2190061.\r\narticle only for simple regression\r\n\r\nbrief search for IV\r\nhttps:\/\/socialsciences.mcmaster.ca\/jfox\/Courses\/SORA-TABA\/slides-mixed-models-ivs.pdf\r\nsecond part has diagnostics for IV2SLS, with references\r\n(using 2nd stage regressors, predicted by instruments, sounds ok)\r\nR ivreg package has vignette with various diagnostics for 2SLS https:\/\/cran.r-project.org\/web\/packages\/ivreg\/vignettes\/Diagnostics-for-2SLS-Regression.html\r\n\r\nafter residual inclusion, control function:\r\nbrief google search finds nothing on specification testing (except exogeneity) or diagnostic plots.\r\nFor residual plot with respect to endogenous regression we might need to add the control function\/residual, i.e. use predicted regressor.\r\nIn GLM we should also be able to use working residuals for \"linear predictor residuals\" in diagnostic plots. (needs reference search)\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-regression","comp-graphics","topic-diagnostic"]},{"title":"ENH: TOST for quantiles, tolerance interval","body":"mainly parking a reference\r\n(found looking for Bland-Altman agreement comparisons)\r\n\r\nGwowen Shieh (2020) Assessing Agreement Between Two Methods of\r\nQuantitative Measurements: Exact Test Procedure and Sample Size Calculation, Statistics in\r\nBiopharmaceutical Research, 12:3, 352-359, DOI: 10.1080\/19466315.2019.1677495\r\n\r\nI didn't try to understand the details.\r\nlooks at TOST test for the equivalence range of a normal quantile, one-sample interpreted as the diff of paired 2 samples.\r\nsmall quantiles indicates agreement between 2 paired measuring methods.\r\n\r\nrelated to tolerance intervals, i.e. get confidence interval for quantiles or for region between 2 quantiles with required coverage\/content.\r\n\r\n (AFAIR, Shieh's articles were useful for other stats)","comments":[],"labels":["type-enh","comp-stats"]},{"title":"ENH: Berkson measurement error, missing unobserved random effect, WLS","body":"(trying to summarize basic setup for Berkson measurement error, not verified with references yet)\r\n\r\nx is know, observed or fixed (e.g. a choice variable of agent)\r\ntrue value of exog that affects endog y has an extra term, x* = x + v\r\ncausal structure for endog\r\ny ~ x* b + e\r\n\r\ncausal effect of choice variable x:\r\ny ~ (x + v) b + e = x b + (v b + e) = x b + u, where u = v b + e\r\n\r\nAssume cov(x, v) = 0, cov(x, e) = 0, or independence\r\nthen cov(x, u) = 0 and OLS is consistent but not efficient (unless var(u_i) is constant)\r\nThis fails if V(v | x) != V(v), does it? We can still have E(v | x) = 0.\r\n(check again what assumptions we need. I'm not sure right now.)\r\n\r\nUsing weight when non-constant error variances, s2v_i, s2e_i\r\nthen weights for WLS should be based on s2e_i + b**2 s2v_i or s2e_i + b' Vv_i b   (Vv = cov of vector v).\r\nRequires iterative WLS to update weights with beta estimates, (similar to HetWLS\/FGLS)\r\n\r\nFor nonlinear models, GLM, discrete:\r\nthe independent measurement error in exog creates unobserved heterogeneity and adds a mixing distribution component to the conditional distribution F(y | x). \r\nWe have related issues for GLM\/discrete with other reasons for unobserved random effects. e.g. Poisson with normal mixing distribution.\r\n\r\n\r\npossible to implement:\r\nfor linear model we just need the iterative WLS to get the model often used for error-in-variables in some fields like chemometrics.\r\ncov_params: WLS (weights taken as given), fully parametric (weights are function of params) or sandwich?\r\n \r\n\r\nthoughts on equation error:\r\n- y-measurement error, no equation error, `e_i` given, `scale=1` fixed\r\n- split equation error multiplicative: scale is estimated by WLS, implicitly multiplies variance defined by weights.\r\n- y-measurement error, equation error on y: `e_i = ve_i + q` equation error is additive in residual e and u., `scale=1` fixed, ve_i known sigma_q needs to be estimated (mainly guessing here)\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-regression"]},{"title":"ENH: measurement error: corrected score function in GLM\/LEF with canonical link","body":"related: linear case #3187\r\n\r\nfor GLM with canonical link, it is possible to correct the score for covariate measurement errors.\r\nWe could add versions of Poisson, Logit, Gaussian, and not for GLM (unless we special case GLM when link is canonical)\r\n\r\nThis will need optimization based on moment conditions, estimating equations (I have not seen an artificial objective function so far) \r\nCorrected score function is nonlinear and not of the single index form, e.g. extra quadratic term in params\/beta.\r\n\r\ngood overview chapter 8, Gaussian and Poisson case in section 8,3,2 p. 142\r\nYi, Grace Y., Aurore Delaigle, and Paul Gustafson, eds. Handbook of Measurement Error Models. New York: Chapman and Hall\/CRC, 2021. https:\/\/doi.org\/10.1201\/9781315101279.\r\n\r\n\r\nAside:\r\nextra term in corrected score looks a bit like characteristic function approach to endogenous regressor case.\r\nHowever, we don't have an estimate of the individual error term, instead we have known variances of the covariate (measurement) error.\r\n  ","comments":[],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"BUG: GLM scale is not updated when computing null model","body":"bug or feature ?\r\n\r\nGLMResults.null computes null model estimate, but only returns fittedvalues\r\nGLMResults.llnull computes with null fittedvalues, but uses scale from full model\r\n\r\nhttps:\/\/stats.stackexchange.com\/questions\/628309\/python-statsmodels-glm-log-likelihood-of-null-model\r\n\r\n**update**\r\n\r\nllnull does not seem to be unit tested (except Poisson)\r\nmain unit tests are for `null_deviance`\r\nIt looks like null_deviance does not take scale into account at all (scale is fixed at 1.)\r\n\r\nThere are some unit tests for pseudo-rsquared\/prsquared, but I don't see which models are covered by it.\r\n\r\n(I never really looked at this for families that have scale != 1)\r\n\r\n","comments":["I have added llnull myself in #1714\r\nI don't see any comments about the implementation, and I was focusing mainly on Poisson, Binomial, scale=1 cases\r\n\r\nIt looks like Stata glm does not have llnull and related.\r\n\r\nI don't find any direct literature for llnull in GLM gamma or similar.\r\nReferences for pseudo-rsquared in GLM seem to NOT adjust the scale\/dispersion coefficient for the null model.\r\nThis looks related to the frequent comment in the GLM literature to keep scale\/dispersion fixed or at the full model when comparing models with different\/nested exog.\r\n\r\nBased on stackoverflow\/stackexchange question it looks like statistical packages don't have pseudo-R2 for GLM gamma and similar, or no pseudo-R2 at all. (and most questions are for logistic\/logit).\r\nMost pseudo-R2 seem to be deviance based, not based on loglike definition. (but I guess scale in deviance remains a similar open question)\r\n\r\nIt's still open what we should do in GLM.\r\nHaving both would be messy because llnull is a cached attribute and we cannot add keyword option.\r\n\r\nAside: gamma pearson scale can be far from the mle scale #7525\r\n\r\n(We still don't have multi-link model framework to add a have full GammaModel that would have clean MLE properties)\r\n\r\n\r\n**update**\r\nllnull is not in `summary`\r\nonly `Pseudo R-squ. (CS)` is in `summary`\r\nno test statistic for comparison to null model is in `summary`, e.g. no p-value for joint hypothesis that all slope coefficients are zero.\r\n\r\nMost likely llnull in GLM is used by very few users.\r\n\r\nWe have some methods\/cached attributes with `_scaled` post-fix, e.g. llf_scaled. Similarly we could add ll_null options.\r\n\r\nWhile looking at some references\/websites for analysis of deviance, I also saw the distinction deviance versus deviance_scaled.\r\ndeviance_scaled seems to correspond to loglike difference while deviance multiplies by scale\/dispersion coefficient. Standard recommendation seems to be to use scale of largest model (e.g. https:\/\/stat.ethz.ch\/R-manual\/R-patched\/library\/stats\/html\/anova.glm.html\r\n\"The dispersion estimate will be taken from the largest model, using the value returned by summary.glm.\"\r\n\r\n**update2**\r\nfor poisson QMLE with excess-dispersion\r\n\"It is crucial to use the full-model dispersion parameter for both scaled deviances, as the regressor-dependent fit is to be measured, and not that depending on the dispersion parameter\"\r\nin \r\nHeinzl, Harald, and Martina Mittlb\u00f6ck. \u201cPseudo R-Squared Measures for Poisson Regression Models with over- or Underdispersion.\u201d Computational Statistics & Data Analysis, Special Issue in Honour of Stan Azen: a Birthday Celebration, 44, no. 1 (October 28, 2003): 253\u201371. https:\/\/doi.org\/10.1016\/S0167-9473(03)00062-8.\r\n\r\n","more on scale in R2\r\n\r\nColin Cameron, A., and Frank A. G. Windmeijer. \u201cAn R-Squared Measure of Goodness of Fit for Some Common Nonlinear Regression Models.\u201d Journal of Econometrics 77, no. 2 (April 1, 1997): 329\u201342. https:\/\/doi.org\/10.1016\/S0304-4076(96)01818-0.\r\n\r\nhave a discussion\r\nR2_LRT  uses scale (implicitly assumes MLE for mean and scale)  likelihood-ratio definition\r\nR2_KL scale cancels in definition if scale is multiplicative (gaussian, gamma), based on Kullback-Leibler divergence and looks similar to deviance (with dispersion coefficient removed from loglike)\r\n\r\nThey have KL formulas in a table for all our families except Tweedie, so we could add it to the family classes.\r\nNeed to check whether or how much it differs from the deviance definition.\r\n","GLM prsquared was added in #7682 (links to original PR and issue)\r\n\r\nUCLA list of pseudo R2 with example Logit\r\nhttps:\/\/stats.oarc.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\r\n\r\nDoes not include GLM specific pR2 like those based on deviance\r\nno mention of scale\r\nDoes not explicitly mention that McFadden pR2 only applies to discrete endog.\r\n "],"labels":["type-bug","comp-genmod"]},{"title":"ENH: conformalized quantile conditional prediction intervals for count data, poisson","body":"this is a special case of #9005 but for discrete data\r\n\r\nquick experiments with 2-sample poisson (constant + dummy), assuming correctly specified model:\r\n\r\nIf means are 5 and 7.5 than discreteness for upper tail ppf(0.9) is large.\r\nIf I use ppf(0.9) from get_distribution on new sample for the upper threshold, then the tail prob (relative frequency) is only 0.03.\r\nWhen I use ppf(0.9) - 1, then the tail rel. frequency is 0.13.\r\nThis means that the ppf(0.9) threshold produces valid, but conservative intervals. We could do only better by either randomizing or shifting the threshold only for a subsample.\r\n\r\nIf I increase poisson means to 50 and 75, then ppf(0.9) has can have incorrect coverage and ppf(0.9) + d or - d will produce better coverage.\r\n\r\nMy calibration sample is nobs=100, and results vary quite a bit, i.e. threshold correction `d` will be pretty random.\r\n(estimation sample has nobs=200). (I initially set the random.seed and then dropped it)\r\n\r\nsome thoughts,\r\n\r\n- results might change with continuous exog, I expect better coverage on average, but discretens remain conditionally.\r\n- results will change with misspecified model, e.g. excess dispersion where poisson.ppf is not valid and adjustment is important.\r\n- with small means, the discreteness might overwhelm additional uncertainty that is not included in ppf.\r\n- threshold correction `d` only needs to use integer values because observations are discrete (check d in range(-dlow, +dupp) with expanding range if needed)\r\n- definition of threshold: threshold point is considered to be in prediction interval, \"outlying observations\" are > or <.\r\n  I used `yn > qupp - d, (yn > qupp - d).mean()` where qupp is predicted ppf(0.9).\r\n- alternative: use multiplicative d instead of additive d, then all ceil(ppf(q) + d * m) would be the discretized thresholds (see reference that compares conformalized quantile regression methods using different calibration score functions)\r\n\r\n \r\n","comments":[],"labels":["type-enh","comp-discrete","topic-predict"]},{"title":"DOC: PoissonResults is not in the docs","body":"PoissonResults is missing in discretemode.rst\r\nno SeeAlso link from the Poisson class, e.g. fit\r\n\r\nstrange, PoissonResults existed forever\r\n\r\nalso\r\ndiscretemode.rst base class list at end of file is missing some results classes like CountResults.\r\nbut CountResults is included in list of specific results classes\r\nNegativeBinomialPResults  seems also to be missing\r\n\r\nI guess all count subclasses have their own results class.\r\n\r\n","comments":["I bumped into this again when doing sphinx doc search for PoissonResults get_distribution\r\n\r\nanswer in https:\/\/stackoverflow.com\/questions\/49416697\/statsmodel-poisson-prediction-return-floats-instead-of-whole-numbers\/49418794#49418794 needs update","related: Parameters and attributes for discrete Results classes is wrong and incomplete\r\ndefined in `_discrete_results_docs`\r\n`Parameters` section has base model  arguments, but discrete results classes take mle_fit returned from super fit.\r\n\r\nAlso attribute list looks incomplete and might need adjustment for DiscreteResults subclasses\r\n\r\nsee comments to answer in\r\nhttps:\/\/stackoverflow.com\/questions\/29868937\/how-to-get-the-coefficients-from-an-mle-logit-regression\/77839730#77839730\r\n"],"labels":["type-bug","comp-docs","comp-discrete","prio-elev"]},{"title":"ENH\/DOC: GMM, 2 linear equations, supply and demand","body":"Do we have an example of a simple model with 2 linear simultaneous equations estimated by GMM?\r\ne.g standard market equilibrium with quantity and price, supply and demand functions\r\n\r\n(I don't remember the details.)\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/77107115\/choices-for-optimization-method-with-generalized-method-of-moments-gmm-in-pyth\r\n\r\nhttps:\/\/ocw.mit.edu\/courses\/14-382-econometrics-spring-2017\/resources\/mit14_382s17_lec3\/\r\n\r\nrelated:\r\nShould we support multi-equation (IV) GMM in a more convenient way?\r\nComputational shortcuts for linear system using linear algebra instead of generic nonlinear optimization?\r\nor leave linear cases to `linearmodels`?","comments":[],"labels":["type-enh","comp-docs","comp-regression"]},{"title":"Pickle with a model using fit_constrained","body":"#### Describe the bug\r\nLoading a model in a pickle if the model was fit using fit_constrained and formula cause an AttributeError: 'numpy.ndarray' object has no attribute 'join'\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nfrom statsmodels.formula.api import glm\r\nfrom statsmodels.genmod import families\r\nimport pandas as pd\r\nimport numpy as np\r\nimport scipy.special\r\nrng = np.random.default_rng(seed=42)\r\n\r\n# Generate data\r\nN = 100\r\nX = rng.normal(size=N)\r\nY = rng.normal(size=N)\r\na = 0.5\r\np = scipy.special.expit(a*X+(1-a)*Y)\r\nobs = [rng.choice([0,1], p=[q, 1-q]) for q in p]\r\n\r\ndf = pd.DataFrame({\"X\":X, \"Y\":Y, \"obs\":obs})\r\n\r\n# Fit model\r\nmod = glm(\"obs ~ X + Y - 1\", data=df, family=families.Binomial()).fit_constrained(\"X + Y = 1\")\r\n\r\n# Save in pickle\r\nfichier = \"sm_model.pickle\"\r\nmod.save(fichier)\r\n\r\n# Load from pickle\r\nmod = sm.load_pickle(fichier)\r\n```\r\n<details>\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nAttributeError: 'numpy.ndarray' object has no attribute 'join'\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.8.16.final.0\r\nOS: Darwin 22.6.0 Darwin Kernel Version 22.6.0: Wed Jul  5 22:17:35 PDT 2023; root:xnu-8796.141.3~6\/RELEASE_ARM64_T8112 arm64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: fr_FR.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.14.0 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.24.4 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/numpy)\r\nscipy: 1.10.1 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/scipy)\r\npandas: 1.5.3 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.7.1 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/matplotlib)\r\nInstalled osx event loop hook.\r\n    backend: MacOSX\r\ncvxopt: Not installed\r\njoblib: 1.2.0 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 8.12.0 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/IPython)\r\n    jinja2: Not installed\r\nsphinx: Not installed\r\n    pygments: 2.15.1 (\/Users\/guillaume\/miniconda3\/lib\/python3.8\/site-packages\/pygments)\r\npytest: Not installed\r\nvirtualenv: Not installed\r\n\r\n<\/details>\r\n","comments":["Thanks for reporting and the example.\r\n\r\nDoes pickling work without using formulas?\r\n\r\nI need to check (tomorrow?).\r\nWe made some changes in how constraints are saved to enable pickling.\r\nBut I don't remember whether we checked many cases for whether pickling works.\r\n\r\nchecking: \r\nno unit tests for pickling after fit_constrained using formulas\r\n#6521\r\n","Using \r\nmod = glm(\"obs ~ X + Y - 1\", data=df, family=families.Binomial()).fit_constrained(([1.0, 1.0], 1))\r\ngives the same error.\r\n","without using formulas: pickling works\r\n\r\n```\r\n# Fit model\r\nmod = GLM(df[\"obs\"], df[[\"X\", \"Y\"]], family=families.Binomial()).fit_constrained(\"X + Y = 1\")\r\n\u200b\r\n# Save in pickle\r\nfichier = \"sm_model.pickle\"\r\nmod.save(fichier)\r\n\u200b\r\n# Load from pickle\r\nmod = sm.load_pickle(fichier)\r\n\r\nprint(mod.summary())\r\n                 Generalized Linear Model Regression Results                  \r\n==============================================================================\r\nDep. Variable:                    obs   No. Observations:                  100\r\nModel:                            GLM   Df Residuals:                       99\r\nModel Family:                Binomial   Df Model:                            0\r\nLink Function:                  Logit   Scale:                          1.0000\r\nMethod:                          IRLS   Log-Likelihood:                -82.329\r\nDate:                Sat, 07 Oct 2023   Deviance:                       164.66\r\nTime:                        14:55:21   Pearson chi2:                     144.\r\nNo. Iterations:                     1   Pseudo R-squ. (CS):            -0.3162\r\nCovariance Type:            nonrobust                                         \r\n==============================================================================\r\n                 coef    std err          z      P>|z|      [0.025      0.975]\r\n------------------------------------------------------------------------------\r\nX              0.8843      0.179      4.938      0.000       0.533       1.235\r\nY              0.1157      0.179      0.646      0.518      -0.235       0.467\r\n==============================================================================\r\n\r\nModel has been estimated subject to linear equality constraints.\r\n```\r\n\r\nso there is a bug specific to formulas after fit_constrained.\r\n\r\npickling works, but unpickling raises the exception in `statsmodels\\base\\data.py in __setstate__`\r\n\r\n"],"labels":["type-bug","comp-base","comp-genmod","prio-elev"]},{"title":"ENH: quantile regression for discrete, count data","body":"I'm not sure how difficult it is to implement any of those.\r\n(motivated by conformal quantile regression https:\/\/github.com\/statsmodels\/statsmodels\/issues\/9005#issuecomment-1735680098 )\r\nI think it would be also useful as diagnostic tool for parametric count regression when we are interested in the entire predictive distributions or some quantiles of it.\r\n\r\nCarcaiso, Viviana, and Leonardo Grilli. \u201cQuantile Regression for Count Data: Jittering versus Regression Coefficients Modelling in the Analysis of Credits Earned by University Students after Remote Teaching.\u201d Statistical Methods & Applications, October 12, 2022. https:\/\/doi.org\/10.1007\/s10260-022-00661-2.\r\n\r\nFrumento, Paolo, and Matteo Bottai. \u201cParametric Modeling of Quantile Regression Coefficient Functions.\u201d Biometrics 72, no. 1 (2016): 74\u201384. https:\/\/doi.org\/10.1111\/biom.12410.\r\n\r\nGeraci, Marco, and Alessio Farcomeni. \u201cMid-Quantile Regression for Discrete Responses.\u201d Statistical Methods in Medical Research 31, no. 5 (May 1, 2022): 821\u201338. https:\/\/doi.org\/10.1177\/09622802211060525.\r\n\r\nMachado, Jos\u00e9 A. F, and J. M. C. Santos Silva. \u201cQuantiles for Counts.\u201d Journal of the American Statistical Association 100, no. 472 (December 1, 2005): 1226\u201337. https:\/\/doi.org\/10.1198\/016214505000000330.\r\n","comments":[],"labels":["type-enh","comp-discrete"]},{"title":"ENH\/reference: Zero-Inflated Nonnegative Continuous Data","body":"mainly parking a reference:\r\n\r\nLiu, Lei, Ya-Chen Tina Shih, Robert L. Strawderman, Daowen Zhang, Bankole A. Johnson, and Haitao Chai. \u201cStatistical Analysis of Zero-Inflated Nonnegative Continuous Data: A Review.\u201d Statistical Science 34, no. 2 (May 2019): 253\u201379. https:\/\/doi.org\/10.1214\/18-STS681.\r\nhttps:\/\/github.com\/joyfulstones\/zero-inflated-continuous  examples using SAS\r\n\r\ncompares Tobit with 2-part models\r\n\r\n2-part models are similar to count models with a mixture of a mass-point distribution and a distribution for values (weakly) larger than the threshold, in this case the distribution is for non-negative continuous data, e.g. log-normal or gamma.\r\nThis is also similar to Tweedie.\r\n\r\nThe interesting cases are when we cannot estimate the two parts separately (which we do right now in hurdle count models).\r\nExample: if there is correlation  (and tools will be similar to sample selection models with unobserved heterogeneity and\/or endogeneity, I guess)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-othermod"]},{"title":"REF\/ENH: jackknife - what should we add?","body":"I'm reading up a bit on jackknife, mainly for looo residuals and looo changes in params. #9008\r\n(The main reason that I never looked at it more carefully is that we can use bootstrap  as more general version of jackknife when we are willing to use repeated full estimation or cross-validation for other use cases. However, in many cases jackknife can be used without repeated estimation loops.)\r\n\r\nsome possible uses\r\n\r\n- bias correction for parameter estimates\r\n  - I don't know whether it's worth it \r\n- jackknife cov_params\r\n  - very close to White sandwich cov_type, e.g. Weber 1986  difference in OLS is only small sample correction n \/ (n - k)\r\n  - #8461 cluster robust cov_type and jackknife with leave one groupcluster out  (I did not look at references for leave one group out)\r\n  - We can stick to White sandwiches and look only at jackknife if it adds anything to that.\r\n- other usages (main target for now)\r\n  - diagnostic (looo)\r\n  - conformal prediction intervals #9005 \r\n\r\nWeber, N. C. \u201cThe Jackknife and Heteroskedasticity: Consistent Variance Estimation for Regression Models.\u201d Economics Letters 20, no. 2 (January 1, 1986): 161\u201363. https:\/\/doi.org\/10.1016\/0165-1765(86)90165-5.\r\nWeber has bias corrected cov_params at the end, which IIRC is HC3  (residuals are corrected by diag of hatmatrix)\r\n","comments":[],"labels":["type-enh","comp-base","topic-diagnostic"]},{"title":"ENH: Ridge regression, jackknife and looo change in params","body":"just parking a note for computational shortcut\r\n\r\nKhuran et al 2014 top of page 5252  for standard parameterization of generalized Ridge\r\n\r\nMost of the jackknife-Ridge literature uses canonical parameterization, i.e. Ridge penalty is on orthogonalized parameters (eigenvalues\/eigenvectors of x'x).\r\nNyquist section 2.1 refers to Woodbury, (i.e. Sherman-Morrison) formula. But as far as I could check, it does not use the property that the moment and penalization matrices are diagonal matrices. \r\n\r\n\r\nNyquist, Hans. \u201cApplications of the Jackknife Procedure in Ridge Regression.\u201d Computational Statistics & Data Analysis 6, no. 2 (March 1, 1988): 177\u201383. https:\/\/doi.org\/10.1016\/0167-9473(88)90048-5.\r\n\r\nKhurana, Mansi, Yogendra P. Chaubey, and Shalini Chandra. \u201cJackknifing the Ridge Regression Estimator: A Revisit.\u201d Communications in Statistics - Theory and Methods 43, no. 24 (December 17, 2014): 5249\u201362. https:\/\/doi.org\/10.1080\/03610926.2012.729640.\r\n\r\n\r\nrelated #9008 looo, jackknife residuals","comments":["to bias reduction for Ridge:\r\n\r\nI always thought bias reduction in Ridge is useless, because we want to have a biased regression to minimize MSE.\r\n\r\nNyquist p. 181: GRE and weighted jackknifed ridge estimator are equivalent in terms of bias, variance, mse if both are evaluated at their respective optimal (minimal MSE) ridge factors (penalization weight).\r\n\r\n"],"labels":["comp-regression","topic-diagnostic","topic-penalization"]},{"title":"ENH: OLS Influence, more statistics without explicit looo loop","body":"question on mailing list\r\n\r\n\"\r\nI am wondering why the computation times of the dfbeta and dfbetas influence statistics are extremely large. The reason seems to be that the implementation uses results form leave-one-out regression loops, requiring N auxilliary regressions (where N is the number of observations of the data set). In the classical monography by Belsley et al. it is shown that these statistics can be computed extremely fast, even without running OLS regressions.\r\n\r\nWhy are the algorithms presented in the book not implemented?\r\n\"\r\n\r\nrelated: Closed form studentized residuals external #373\r\n \r\n","comments":["looo for scale: BKW equ (2.8) p. 14"],"labels":["type-enh","Performance","topic-diagnostic"]},{"title":"ENH: outlier, influence  add looo, jackknife residuals","body":"motivated by https:\/\/github.com\/statsmodels\/statsmodels\/issues\/9005#issuecomment-1729932721\r\n\r\nWe don't have explicit looo, jackknife residuals (resid_response).\r\nI'm not sure whether we already have all the computation for it, for other statistics.\r\n\r\nlooo residuals look like a useful tool.\r\n\r\nIn the GLM, discrete cases we could also add other residuals based on looo, e.g. resid_pearson.\r\nother predictive statistics, looo-fittedvalues, looo-implied-var and similar might also be useful.\r\n(model.predict has \"which\" option and takes params as argument, but likely not a 2dim params vectorized by rows)\r\n\r\n\r\n","comments":["OLSInfluence has `dffits` which is studentized change in fittedvalues, but no `dffit` without studenization\r\n\r\nMLEInfluence has `d_fittedvalues`, one-step approximation to change in fittedvalues (based on derivative for slope of fittedvalues\/mean function, not finite difference) dy\/dparams * dparams\r\n(diff and derivative are the same in linear models)\r\n\r\n`d_fittedvalues_i = yhat_i - yhat_{i, -i}   (approx)`\r\n`resid_{i, -i} = y_i - yhat_{i, -i} =  (y_i - yhat_i) + (yhat_i - yhat_{i, -i}) = resid_i + d_fittedvalues_i`\r\n\r\nalternatively we can compute difference to predicted value at distorted params\r\nfittedvalues_{i, -i} = predict(params + dparams)\r\n","for plotting:\r\n\r\n- fittedvalues_{i, -i} versus fittedvalues_i\r\n\r\nfor overdispersed poisson, we could try to get looo-scale, i.e. mean of pearson residuals with looo\r\nin poisson: residuals**2 \/ fittedvalues using looo\/jackknife values"],"labels":["type-enh","topic-diagnostic"]},{"title":"Statsmodels ARIMA, reconstructing model prediction with exogenous variable","body":"I'm building an ARIMA model using statsmodels.tsa.arima.model, version 0.14.0. I'm having issues manually reproducing the output of the predict method on the training dataset when including exogenous variables, the difference is a million times larger than when manually reproducing a pure AR model. \r\n\r\nBased on this  [link](https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_sarimax_faq.html#Reconstructing-residuals,-fitted-values-and-forecasts-in-SARIMAX-and-ARIMA) I've tried to reproduce the functional form of the ARIMA equation. Assuming I have an ARIMA(1,1,0) model with an exogenous variable, would the predict method use an ARIMA of the form \r\n\r\n$y_{t+1}=y_t+\\beta X_{t+1}+\\alpha(y_t-y_{t-1}-\\beta X_t)$ \r\n\r\nwhere $y_t$ is my endogenous variable, $X_t$ is the exogenous variable, $\\alpha$  and $\\beta$ are the autoregressive and exogenous coefficient returned by  `.params`? ","comments":["[`SARIMAX`](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.statespace.sarimax.SARIMAX.html) uses a regression with ARIMA errors approach to `exog`. So an ARIMA(1, 1, 0) would be:\r\n\r\n$$y_t = \\beta x_t+ z_t$$\r\n\r\n$$\\Delta z_t = \\rho \\Delta z_{t-1} + \\varepsilon_t$$\r\n\r\n"],"labels":["comp-tsa-statespace","question"]},{"title":"ENH: conformal, distribution-free, nonparametric prediction intervals","body":"This looks like a recent hot topic mainly for machine learning.\r\n\r\nBasic idea: use calibration data, separate from estimation\/training data, to estimate quantiles and prediction sets or intervals for new observations.\r\n\r\nsimplest case: In regression with additive residual, we can add\/subtract quantile of calibration residuals to the get conformal prediction interval (several alternatives of splitting data, ex-ante split, jacknife\/looo, k-fold crossvalidation, ...)\r\n \r\nhttps:\/\/mapie.readthedocs.io\/en\/latest\/theoretical_description_regression.html    mapie seems to be the main python package for this\r\n\r\nLei, Jing, Max G\u2019Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. \u201cDistribution-Free Predictive Inference for Regression.\u201d Journal of the American Statistical Association 113, no. 523 (July 3, 2018): 1094\u20131111. https:\/\/doi.org\/10.1080\/01621459.2017.1307116.\r\n\r\nlarge number of references (to much to figure out for now what would be useful for statsmodels)\r\n\r\n\r\nlimitation:\r\nIt looks like the coverage \"guarantee\" is only for average, marginal coverage (over sample of exog, similar to ATE), but there is no distribution-free \"guarantee\" for the coverage conditional on a specific exog value.\r\n\r\nFoygel Barber, Rina, Emmanuel J Cand\u00e8s, Aaditya Ramdas, and Ryan J Tibshirani. \u201cThe Limits of Distribution-Free Conditional Predictive Inference.\u201d Information and Inference: A Journal of the IMA 10, no. 2 (June 15, 2021): 455\u201382. https:\/\/doi.org\/10.1093\/imaiai\/iaaa017.\r\n\r\nRelated: \r\nI saw some articles (abstracts) that include heteroscedasticity (var(exog)).\r\nMore literature looks at time series, i.e. without independence.\r\n\r\nCount data:\r\nI did not find any references for conformal prediction interval with count data (based on google scholar search)\r\nThis might be related to the above \"limits\" of conformal prediction intervals that we only have coverage with respect to marginal distribution.\r\nI guess that the main problem to get prediction interval conditional on x would be to find a ranking statistic (prediction \"score\" function) that is distribution-free, e.g. does not depend on higher moments that depend on exog through e.g. mean.\r\n\r\nI guess we can still get conditional (on x) prediction intervals under stronger assumptions, e.g.\r\nAssume regression model with additive i.i.d. residuals. That would still be non-parametric for the residual distribution and better than the current prediction interval based on normal or t distributed residuals.\r\n\r\nAside: There is a literature for adaptive local conformal prediction (or something like that) but I did not even skim those.\r\n\r\n\r\nRelated: diagnostics for this ?\r\nmaybe\r\nhomogeneity, i.i.d. assumption  (e.g. subsample homogeneity like Hosmer-Lemeshow type of test)\r\nDoes calibration set exog reflect population or sample exog?\r\nquantile regression on residual (to see whether quantiles do not depend on exog) \r\nCompare conformal prediction interval with parametric interval to see whether the latter is \"good enough\".\r\n\r\nclarification:\r\nin (general) regression setting (y, x) is assumed to be i.i.d. and coverage statement is over probability space of (y, x) and not for conditional (y | x)\r\n\r\nso far we want the much stronger conditional prediction interval for y | x\r\n","comments":["This reference might be useful: \r\nmentions PIT (probability integral transform) in the abstract\r\n\r\nChernozhukov, Victor, Kaspar W\u00fcthrich, and Yinchu Zhu. \u201cDistributional Conformal Prediction.\u201d Proceedings of the National Academy of Sciences 118, no. 48 (November 30, 2021): e2107794118. https:\/\/doi.org\/10.1073\/pnas.2107794118.\r\n\r\nAside:\r\nfor basic control charts we assume constant parameters (mean, ...).\r\nIf this is the case, then i.i.d. assumption works and we can use standard conformal prediction interval for control limits.\r\n(That's similar to in-sample nonparametric quantiles of residuals, but in this case on calibration sample).\r\n\r\n\r\nAside 2: conformal prediction for subsets of observations (no references, just analogy)\r\nFor count diagnostic and results interpretation, I split the sample by a binary variable (e.g. gender) to check whether the predictive distribution varies by the levels of the categorical variable.\r\n(maybe this does not really help for conformal prediction intervals with count data, except possibly as diagnostic. If the (categorical) variable has an effect on the mean, then it will also affect the conditional prediction interval limits.)\r\n","another possibility:\r\n\r\njacknife+ in MAPIE: looo loop residuals\r\nwe can get them from the outlier-influence classes.\r\nFor linear models, we can get them directly, \r\nFor nonlinear models we have one-step approximation to d_params as faster version (although, as above, our discrete nonlinear models don't have iid error term, residuals)\r\n\r\n\r\nBarber, Rina Foygel, Emmanuel J. Cand\u00e8s, Aaditya Ramdas, and Ryan J. Tibshirani. \u201cPredictive Inference with the Jackknife+.\u201d The Annals of Statistics 49, no. 1 (February 2021): 486\u2013507. https:\/\/doi.org\/10.1214\/20-AOS1965.\r\n\r\n\r\n\r\n\r\n","link to treatment effect: individual (conditional) treatment effect ITE versus average (marginal) effect ATE, ATT\r\n\r\nLei, Lihua, and Emmanuel J. Cand\u00e8s. \u201cConformal Inference of Counterfactuals and Individual Treatment Effects.\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology 83, no. 5 (November 1, 2021): 911\u201338. https:\/\/doi.org\/10.1111\/rssb.12445.\r\n\r\nYin, Mingzhang, Claudia Shi, Yixin Wang, and David M. Blei. \u201cConformal Sensitivity Analysis for Individual Treatment Effects.\u201d Journal of the American Statistical Association 0, no. 0 (2022): 1\u201314. https:\/\/doi.org\/10.1080\/01621459.2022.2102503.\r\n","calibrated conditional prediction intervals:\r\n\r\nLei, Jing, Max G\u2019Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. \u201cDistribution-Free Predictive Inference for Regression.\u201d Journal of the American Statistical Association 113, no. 523 (July 3, 2018): 1094\u20131111.\r\n(reference in initial comment above)\r\n\r\nsection 5.2 locally weighted conformal inference\r\nlooks at heteroscedasticity, scale is function of x\r\nuses scaled, (pearson) residuals as score function with scale function estimated by MAD.\r\nThis provides prediction intervals that vary with x, but still have the average (over sample and new y, x probability space)\r\n\r\nThis means we can get better conditional prediction intervals (y | x) but still have correct average coverage.  (*1)\r\nWe can calibrate a prediction interval function for correct average coverage, even if we don't have the \"true\" score function and \"true\" shape of prediction intervals for conditional prediction (y|x).\r\nBut we can get the correct intervals if the underlying model specification and distributional assumption is correct.\r\n\r\ne.g. GLM, poisson with large(r) mean\r\nuse scaled residuals (pearson residuals divided by `scale`, i.e. including excess dispersion, or empirical variance function) in the calibration score function. This would correct for variance as function of mean, but not for higher (asymmetric?) moments (like skew).\r\n\r\nThis would be like making prediction intervals robust to model misspecification, and still have good properties if model is correctly specified.\r\n \r\nrelated: asymmetry, skew\r\nBarber, Rina Foygel, Emmanuel J. Cand\u00e8s, Aaditya Ramdas, and Ryan J. Tibshirani. \u201cPredictive Inference with the Jackknife+.\u201d The Annals of Statistics 49, no. 1 (February 2021): 486\u2013507.\r\n(reference in 3rd comment)\r\n\r\nAppendix A: asymmetric prediction intervals with jackknife+ and cv+\r\nuses separate lower and upper limits\/quantiles for prediction interval instead of those based on absolute values of residuals.\r\n\r\n\r\ntransformation of endog \r\nalso Barber et al 2021\r\nsection 7.2 application to real data\r\nuses log(1 + y) transformation because both datasets are highly skewed.\r\nusing endog nonlinear transformation and endpoint transformation for intervals looses E(y | x) as prediction target, but we might still get good quantiles (median and interval limits) in the prediction.\r\n\r\n\r\n**update**\r\n(*1)\r\nWe establish ...\r\n1) Asymptotic conditional validity under consistent estimation\r\nof the conditional CDF\r\n2) Unconditional validity under model misspecification:\r\n\u2022 Finite-sample validity with iid (or exchangeable) data\r\n\u2022 Asymptotic validity with time series data\r\n\r\nin Chernozhukov et al 2021 \"Distributional conformal prediction\"","getting closer to useful for conditional prediction intervals  (based on partial skimming)\r\n\r\nquantiles and predictive distribution Fhat(y | x)\r\n\r\nRomano et al conformalized QR needs predicted quantile function (this could be implied by predictive distribution instead of quantile regression.\r\nadjusts estimated quantile function to have average coverage in calibration sample\r\nreference found through https:\/\/mindfulmodeler.substack.com\/p\/week-3-conformal-prediction-for-regression#%C2%A7conformalized-quantile-regression\r\n\r\nChernozhukov et al use PIT which sounds the same\r\n\r\nCandes et al have similar applied to conditional survival distribution\r\n\r\nChernozhukov, Victor, Kaspar W\u00fcthrich, and Yinchu Zhu. \u201cDistributional Conformal Prediction.\u201d Proceedings of the National Academy of Sciences 118, no. 48 (November 30, 2021): e2107794118. https:\/\/doi.org\/10.1073\/pnas.2107794118.\r\n\r\nRomano, Yaniv, Evan Patterson, and Emmanuel Candes. \u201cConformalized Quantile Regression.\u201d In Advances in Neural Information Processing Systems, Vol. 32. Curran Associates, Inc., 2019. https:\/\/proceedings.neurips.cc\/paper_files\/paper\/2019\/hash\/5103c3584b063c431bd1268e9b5e76fb-Abstract.html.\r\n\r\nCand\u00e8s, Emmanuel, Lihua Lei, and Zhimei Ren. \u201cConformalized Survival Analysis.\u201d Journal of the Royal Statistical Society Series B: Statistical Methodology 85, no. 1 (February 1, 2023): 24\u201345. https:\/\/doi.org\/10.1093\/jrsssb\/qkac004.\r\n\r\n\r\nrelated: \r\nwe have several issues and some code already using PIT for diagnostics (using estimation sample), e.g. \r\n#7153 quantile residuals\r\n#7873 add which=\"cdf\" to predict\r\n\r\nnote: predicted cdf, quantiles ignore estimation uncertainty (confint for cdf, ppf not or only partially available)\r\ncalibration with conformal prediction would improve coverage of prediction intervals to take account of modelling uncertainty and distributional misspecification.\r\n\r\n#6979 tolerance intervals\r\n","related by topic but separate literature (AFAIR)\r\n\r\ncalibration of prediction to new dataset\r\nbrief search of issues only finds it for the binary endog case, e.g. #6430\r\n(recalibrate predicted conditional probability of an endog event, AFAIR)\r\n","rough plan (AFAIU so far)\r\n\r\n- (additive) residual based\r\n   - jackknife, jackknife+, looo residuals #9008\r\n- nonlinear, count, ...\r\n  - PIT, ... \r\n- infrastructure\r\n  - separate calibration sample, predict\/get_distribution, calibrated prediction interval\r\n  - jackknife\/looo statistics (params, residuals) as intermediate step.\r\n  - what do we need to store, attach to some class used for prediction?  Where, in which class do we store it? API?\r\n    mainly calibration data and intermediate results to compute new (conformal) prediction intervals. \r\n\r\nWhat's the overlap with other packages like mapie?\r\nWhat can statsmodels do that they cannot? e.g. integrated jackknife+ without explicit loop, PIT\r\n\r\n\r\n**update** possible API\r\nresults method `get_conformal_predictor` \r\ntakes calibration dataset (unless method is jackknife) and options for method (e.g. mean, quantiles, ....) and sub-options within main conformalizing method.\r\nreturns an instance of a class that hold the calibration values (interval limit corrections) and delegates prediction (and get_distribution) tasks to the results and model instances.\r\n\r\nFirst step only for `which=\"mean\"`, but we could add conformalized prediction intervals also to other `which` available in predict and get_prediction.  \r\n(Maybe `which`, when available, needs to be a `get_confomalized_predictor` argument, so it is fixed in the calibrated predict class.)\r\n","detail: tail probabilities\r\n\r\nChernozhukov et al 2021 \"Distributional conformal prediction\"\r\nhave \"optimal DCP\" prediction intervals, which are essentially `minlike`  (for skewed distributions)\r\n\r\nTheir calibration score function uses absolute value of pit values F(x) which are uniform in [0,1] if F(x) is consistently estimated ([0, 0.5] for absolute values.\r\nscore function is `|x - 0.5|` in base DCP and with shifted \"center\" in optimal DCP\r\n\r\nThey do not mention or look at asymmetric case directly.\r\ni.e. separate score function for lower and upper limits of prediction interval.\r\n\r\nMy guess:\r\nFor equal tail (alpha\/2) prediction intervals it would be better to use two separate, \"asymmetric\" intervals instead of absolute value score function.\r\nUnder correct specification we have a consistent estimate of F(y|x) and rank\/pit values are uniformly distributed, but under misspecification that will not be the case and separate equal tail interval limits should be better.\r\nrelated: one-sided prediction intervals (which I have not seen yet in the references that I skimmed)\r\ntopic: equal tail versus minlike\/shortest interval for confidence intervals in statsmodels.stats.\r\n\r\naside:\r\nChernozhukov et al assume continuous distribution.\r\nMy guess is that it extends to discrete distribution, but we only get weak inequalities in coverage. `Coverage >= 1-alpha` but often it will be `>` because of discreteness.\r\n\r\n\r\n**update**\r\nRomano et al (2019) Conformalized quantile regression\r\nTheorem 2 has separate lower and upper limit in calibration correction.\r\ni.e. 2 one-sided quantiles\/tails.\r\n","another application: prediction interval in underdetermined models, parameters not identified\r\ne.g. k_params > nobs  and estimation by pinv, penalized or feature selection (e.g. sure independence screening)\r\n\r\nIf parameters are not identified, we do not get inference on parameters (cov_params, ...).\r\nHowever, all parameters in optimal set lead to the same prediction, and same predictive distribution (if we don't have or pick nuisance parameters like `scale`)\r\nUsing conformal prediction we can still get `valid` prediction intervals. Using conformalized distributional prediction, we can also get approximate conditional prediction intervals.\r\n\r\nproblem: scale estimation\r\nIn sample residuals are zero and in-sample `scale` is zero. Instead we can use out-of-sample residuals, e.g. either jackknife or calibration set, to estimate `scale` for predictive distribution.\r\n(in discrete, one-parameter family like poisson we don't have scale as nuisance or extra distribution parameter.)\r\n\r\nThis gets closer to machine learning and the popularity of conformal prediction there.\r\n\r\nThis means we should make e.g. jackknife\/looo residuals and scale (based on one-step approximation or linear dparams) a more prominent feature in the results classes.\r\n\r\n"],"labels":["type-enh","comp-regression","topic-predict"]},{"title":"Maint: nonempty `statsmodels\\__init__.py`","body":"`statsmodels\\__init__.py` now loads pandas and patsy, and all their non-lazy dependencies \r\n\r\nthe reason  is a pandas 2.x compat fix in patsy\r\n`from statsmodels.compat.patsy import monkey_patch_cat_dtype`\r\n\r\n\r\n(I only looked at it because `import statsmodels` took much longer than expected)\r\n\r\nNot a big problem (avoiding pandas and patsy is mainly useful in statsmodels.stats and a few other places)\r\nAnd I don't see much of an alternative solution. We could put it in `statsmodels.base.model` or `data`, but then we might monkeypatch on each import.\r\n\r\nMaybe fix shows up eventually in patsy, or it becomes obsolete if we use formulaic.\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["maintenance"]},{"title":"BLD: Move to Meson","body":"#### Is your feature request related to a problem? Please describe\r\nPython 3.12 no longer supports distutils, so statsmodels cannot be built for 3.12.  The solution is to floolw SciPy\/NumPy\/pandas and move to meson.\r\n\r\n#### Describe the solution you'd like\r\nSwitch build to meson.\r\n\r\n#### Describe alternatives you have considered\r\nNot aware of any.\r\n\r\n","comments":["Do you know what we need to do for this?\r\n\r\nI have not looked at the build system in a long time.\r\nAFAICS based on a file search, we only use distutils through cython.\r\n","Pretty sure we also depend on it through NumPy, which also doesn't have distutil support for Python 3.12 since NumPy depended on the native distutils.","I only see `from Cython.Distutils import build_ext` in setup.py  (otherwise \"distutils\" only shows up in C files and azure yml)\r\n\r\nyou have already removed direct dependency on numpy distutils in #8266\r\n\r\nSo it looks to me, if Cython\/Distutils\/build_ext.py  does not use python's or numpy's distutils anymore, then we also do not depend on it even indirectly.\r\n\r\nGiven that everyone switched to meson build, it should be also useful for statsmodels to switch, but we might not **have** to switch","It does seem that statsmodels can install using 3.12 now, so this isn't that urgent.  It can definitely wait for the next major release.\r\n","very good, glad it does not have to become prio-high","Side note: The Arch Linux package is broken, the reason seems to be that numpy is build with Meson: https:\/\/bugs.archlinux.org\/task\/79787","Guessing the only affects 3.12?","Other builds should be using oldest superted numpy. ","Did this affect the musllinux wheels?","Does arch use musl?","> Guessing the only affects 3.12?\r\n\r\nNo, Python is 3.11.5\r\n\r\n\r\n> Other builds should be using oldest superted numpy.\r\n\r\nArch Linux is rolling release, all Packages are build from source using the most recent version (currently 1.26.0) \r\n\r\n\r\n> Does arch use musl?\r\n\r\nno, standard is glibc\r\n","Seems to be arch-specific.  I tested the wheels on python 3.12 rc3 against the full set of wheels and there is no missing symbol.\r\n","@bashtage: many thanks, I'll report if we figure out the reason for the Arch behavior","fixed by changing numpy build: https:\/\/gitlab.archlinux.org\/archlinux\/packaging\/packages\/python-numpy\/-\/commit\/c3f5ad3e28d81c4f8c6d172a105a45d985f52980"],"labels":["build"]},{"title":"ENH: fourier polynomial, splines for seasonal pattern with irregular periodicity","body":"(semi-random thought seeing comments of users that want annual seasonality with daily data)\r\n\r\nyears and months don't have all the same number of periods. Assuming fixed periodicity (frequency, number of periods in a cycle) is not really appropriate.\r\nAnother example would be working week with unequal number of days if there are holidays\/non-working days.\r\n\r\nIn this case, periodicity is \"strictly exogenous\".\r\nMissing values would be similar but exogeneity is an assumption.\r\n\r\nIn GAM and similar splines, we don't need points on a (time) grid. We can convert to continuous, unequal spaced grid, eg. normalize days of the year to (max is) 365, and days can be real numbers, or skip periods if we have holidays, exogenously missing periods.\r\n\r\ntsa.deterministic only allows for fixed periodicity `period`, AFAICS.\r\n\r\naside: for constructing forecast periods we could require a `period_base`, e.g. assuming next year has 365 days.\r\n\r\nrelated\r\n\"analyzing seasonal time series with GAM\"\r\nhttps:\/\/petolau.github.io\/Analyzing-double-seasonal-time-series-with-GAM-in-R\/\r\n\r\n(I might have looked at examples when checking season splines in GAM)\r\n","comments":["The Fourier deterministic code can handle any spacing (including irregular) for a fixed period.\r\n\r\n```python\r\nfrom statsmodels.datasets import sunspots\r\nfrom statsmodels.tsa.deterministic import Fourier\r\n\r\ndata = sunspots.load_pandas().data\r\nfourier_gen = Fourier(11, order=2)\r\nfourier_gen.in_sample([0.5,1.0,10,10.5,11])\r\n```\r\n\r\n```plaintext\r\n      sin(1,11)  cos(1,11)  sin(2,11)  cos(2,11)\r\n0.5    0.000000   1.000000   0.000000   1.000000\r\n1.0    0.540641   0.841254   0.909632   0.415415\r\n10.0   0.909632   0.415415   0.755750  -0.654861\r\n10.5   0.989821  -0.142315  -0.281733  -0.959493\r\n11.0   0.755750  -0.654861  -0.989821  -0.142315\r\n```","AFAIU from reading the code:\r\n\r\nFourier still assumes that all observations are on an equal spaced grid, independent of the `index`\r\n`terms = self._get_terms(np.arange(nobs) \/ self._period)` in `in_sample`\r\n\r\nMaybe that makes sense (assuming that for example days are equal spaced), but I don't think it fully captures in-sample seasonality.\r\nSomething like the end of the monthly cycle is at the 30th, 2*30th, ... observations and not on calendar day like Feb 28.\r\nThat is, we predict a regular spaced cycle independently of the actual number of periods in a given cycle.\r\n(In a continuous time version the predicted or model cycle would not end at the predefined cycle end.)\r\n\r\n\r\n\r\n"],"labels":["comp-tsa","type-enh","comp-gam"]},{"title":"MM estimators","body":"In [this](https:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/examples\/notebooks\/robust_models_1.ipynb) example notebook, it says that this package currently doesn't have MM estimators, but that they are being worked on. I'm just wondering if there has been any progress on implementing this and if so where I can find it as I'm interested in using an MM-estimator in python.","comments":["Nothing was finished. It stalled in old pull requests of mine.\r\nMM-estimators need a resistant, high breakdown preliminary estimator of the scale or residual variance which I never finished AFAIR.","related stackoverflow question https:\/\/stackoverflow.com\/questions\/77067606\/how-to-implement-an-s-estimator-for-an-mm-estimator-in-python","PR #8125 has MM-estimator using LTS as starting estimator\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/8125\/files#diff-f07ba5610b73d402c49aa6901be71d0863ce7b25ec45381dcab69666adf25b27R589"],"labels":["type-enh","comp-robust"]},{"title":"BUG: SARIMAX Extend with constant exog doesn't work","body":"#### Describe the bug\r\n\r\nSimilar to this issue: https:\/\/github.com\/statsmodels\/statsmodels\/issues\/7019\r\n\r\nIt seems the above bug is solved, but when you try to extend a SARIMAX model with constant exog values, it throws an error\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nfrom statsmodels.tsa.arima.model import ARIMA\r\nimport numpy as np\r\n\r\n# A normal training demo\r\n\r\n# Old data\r\nold_data = np.random.rand(12*10, 1)\r\nold_exo = np.random.rand(10, 1)\r\nold_exo = np.repeat(old_exo, 12).reshape(-1, 1)\r\n\r\n# Model fit with the available data\r\nm = ARIMA(old_data, exog=old_exo)\r\nres = m.fit()\r\n\r\n# Some new data arrives\r\nnew_data = np.random.rand(12, 1)\r\nnew_exo = np.repeat(np.random.rand(1, 1), 12).reshape(-1, 1)\r\n\r\n# The following line should not fail, but it does\r\nres.extend(new_data, exog=new_exo)\r\n\r\n```\r\n\r\nWe get  `ValueError: A constant trend was included in the model specification, but the `exog` data already contains a column of constants.`\r\n\r\n#### Expected Output\r\n\r\nIt should just extend the model without checking if exog is constant or not, as exog changes every \"h\" periods. When I try to extend the model after doing \"h\" periods where exog doesn't change, we get the error above.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.10.11.final.0\r\nOS: Linux 5.14.0-70.30.1.el9_0.x86_64 #1 SMP PREEMPT Thu Nov 3 20:29:04 UTC 2022 x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.14.0 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.23.5 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/numpy)\r\nscipy: 1.11.1 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/scipy)\r\npandas: 2.0.3 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.7.1 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/matplotlib)\r\n    backend: module:\/\/matplotlib_inline.backend_inline \r\ncvxopt: Not installed\r\njoblib: 1.3.1 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 8.14.0 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/IPython)\r\n    jinja2: 3.1.2 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/jinja2)\r\nsphinx: Not installed\r\n    pygments: 2.15.1 (\/mnt\/lustre\/ludwig\/lqb955\/conda_envs\/air_forecast\/lib\/python3.10\/site-packages\/pygments)\r\npytest: Not installed\r\nvirtualenv: Not installed\r\n\r\n\r\n<\/details>\r\n","comments":["Thanks very much for posting this, definitely a bug with `ARIMA.extend`."],"labels":["type-bug","comp-tsa-statespace"]},{"title":"ENH: Treatment effect for binary endpoint, general outcome models","body":"mainly parking a link\r\n\r\nhttps:\/\/github.com\/pzivich\/zEpid\/blob\/master\/zepid\/causal\/doublyrobust\/AIPW.py\r\n\r\nzepid: has binary treatment and binary outcome.\r\nour current `treatment_effect` only allows for linear outcome (OLS)\r\n\r\n","comments":["another Zivich piece of code\r\n\r\nhttps:\/\/github.com\/pzivich\/publications-code\/blob\/master\/MissingDataEpi\/ipop_missing.py#L34\r\n\r\ng-computation in cross-section model (no panel data, repeated observations) is just regression adjustment.\r\ncode is just a few lines using GLM-Binomial for binary endpoint.\r\n\r\nrelated:\r\nsome articles with Zivich as co-author have 3 models, e.g. treatment, censoring or missing and outcome\r\n","Extensions to binary outcomes should be straightforward. IPW is the easiest since it should require no change on your part. The weighted mean works for any outcome, since IPW is nonparametric for the outcome process. For your IPW implementation, it isn't clear to me from the docs whether you are using the Horwitz-Thompson or the Hajek.\r\n\r\nRA should only require using logistic regression rather than a linear model. While making that modification, you may want to also consider adding support for generic GLM specifications. In `AIPW` I also allow the use of Poisson regression for the outcome.\r\n\r\nWhen using parametric models for the nuisance models (propensity score, outcome mechanism), weighted-regression AIPW (`aipw_wls`) and targeted maximum likelihood estimation (TMLE) should not be meaningfully different (see 4.6 of [Tran et al.](https:\/\/pubmed.ncbi.nlm.nih.gov\/30811344\/)). So, unless you allow for machine learning for nuisance model estimation, I wouldn't worry too much about offering TMLE. \r\n\r\nEdit: I replicate the Hernan and Robins 2023 book with zEpid [here](https:\/\/github.com\/pzivich\/Python-for-Epidemiologists\/tree\/master\/4_Hernan-Robins) and with delicatessen [here](https:\/\/deli.readthedocs.io\/en\/latest\/Examples\/Hernan-Robins-2023.html). The IPW sections demonstrate continuous and binary outcomes. You may also want to consider adding support for missing outcome data in the future as well\r\n"],"labels":["type-enh","comp-treatment","comp-causal"]},{"title":"ENH\/Design: models with several optional objective\/loglike functions","body":"partially related: #2949\r\n\r\nbase LikelihoodModel uses hardcoded method names for loglike, score, ... for nonlinear optimization.\r\n\r\nCan we implement a model that has several possible, user chosen objective functions?\r\nexample: covariance analysis should have option for mle, ls, wls as objective function. All of those require nonlinear optimization in general.\r\n\r\nproblems:\r\n\r\n- need a switch for selecting appropriate method, loglike, score, ... (e.g. NB assigns method)\r\n- possible incorrect state if results instance computes extras after model has changed (repeated calls to fit)\r\n  - This could be avoided if method choice is in `model.__init__`, but it would in general be different `fit` method.\r\n  - More messy to implement would be to add method selection keyword to relevant model methods (loglike, ...). Results classes would have to transmit keyword. (similarly, we added `scale` keyword to some loglike, ... methods, GLM)\r\n\r\nAlternative:\r\nCreate separate subclasses for each `fit` estimation method.\r\n\r\n\r\n\r\n","comments":[],"labels":["comp-base","design"]},{"title":"Adding leverage statistics for linear regression","body":"#### Is your feature request related to a problem?\r\nIn applied linear regression it is useful to compute the leverage(influence) of a single observation on the overall model fit.\r\nThis can help identify outliers and further assess the model fit.\r\nR provides a cook's distance function amongst other functions to compute the influence of observations.\r\n\r\n#### Describe the solution you'd like\r\nI think adding a Cook's distance formula and leverage statistics output for a linear model fit with the statsmodels package would be a great addition to the package. I could help create an initial design for this.\r\n\r\n#### Describe alternatives you have considered\r\nA clear and concise description of any alternative solutions or features you have considered.\r\n\r\n#### Additional context\r\nAdd any other context about the feature request here.","comments":["those are available for OLS, get_influence and for MLE models.\r\nAFAIR, no explicit version for WLS\/GLS  (I guess we don't have unit tests for `Influence` class using `wresid`, `wexog`, ...)"],"labels":["type-enh","comp-regression"]},{"title":"MAINT: failing style check: E721 do not compare types","body":"example `if type(corr1) == np.ndarray:`\r\nshould either use `is` or `isinstance`.\r\nMost likely we want `is` in those cases, i.e. rule out subclasses of ndarray (which might not have compatible behavior)\r\n\r\nin statsmodels\/tools\/data the `is` comparison does not fail the style check\r\n`type(endog) is np.ndarray`\r\n\r\n\r\n```\r\nstatsmodels\/genmod\/generalized_estimating_equations.py:2408:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/genmod\/generalized_estimating_equations.py:2415:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/regression\/mixed_linear_model.py:2951:8: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/stats\/correlation_tools.py:638:8: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/stats\/correlation_tools.py:650:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/stats\/correlation_tools.py:660:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/regime_switching\/markov_switching.py:412:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/regime_switching\/markov_switching.py:414:18: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/kalman_filter.py:1270:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/mlemodel.py:2234:12: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/mlemodel.py:5033:20: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/sarimax.py:1018:34: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/sarimax.py:1020:21: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:396:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:449:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:458:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:396:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:449:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tests\/test_tools.py:458:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tools.py:839:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tools.py:875:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tools.py:1081:8: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tools.py:1360:8: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/statespace\/tools.py:1417:16: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nstatsmodels\/tsa\/vector_ar\/tests\/JMulTi_results\/parse_jmulti_vecm_output.py:83:8: E721 do not compare types, for exact checks use `is` \/ `is not`, for instance checks use `isinstance()`\r\nPreviously passing files failed linting.\r\n```","comments":["Hello @josef-pkt, rule E721 has evolved, now they only recommend using `isinstance`. Do you think we can use `isinstance` for all these cases? I can send a PR if the change is sensible.\r\n\r\n```\r\nstatsmodels\/graphics\/regressionplots.py:1194:8: E721 Do not compare types, use `isinstance()`\r\n```\r\n\r\nRationale:\r\n- https:\/\/docs.astral.sh\/ruff\/rules\/type-comparison\/\r\n- https:\/\/www.flake8rules.com\/rules\/E721.html"],"labels":["maintenance"]},{"title":"Structural Equation Modelling (CB-SEM & PLS-SEM)","body":"#### Is your feature request related to a problem? Please describe\r\nI've seen past issue (https:\/\/github.com\/statsmodels\/statsmodels\/issues\/6400#issuecomment-823453344) that mentioned about semopy as package that might be included in statsmodels.\r\n\r\n- [semopy](https:\/\/gitlab.com\/georgy.m\/semopy) is a nice CB-SEM package comes with MIT license, but it is stored on GitLab instead of GitHub which I think will be less likely to be contributed.\r\n\r\n- There is also a package called [plspm-python](https:\/\/github.com\/GoogleCloudPlatform\/plspm-python) for PLS-SEM but it comes with GPL-3 license.\r\n\r\n#### Describe the solution you'd like\r\nStatsmodels is a well-maintained Python package that comes without GPL license, which are advantages for open sourcing data analysis GUI. In management research, especially marketing, SEM is commonly used, but we must utilize commercial software like SmartPLS. I would like to ask whether there is a plan to support SEM inside statsmodels?","comments":["in short: it's on the wishlist but not on my roadmap.\r\n\r\nSEM is also one of the popular methods in fields like psychology, but still missing in statsmodels. It's an important methods to add.\r\n\r\nHowever, SEM is a pretty large topic and I don't have any background in it, except for checking specific methods that are also useful in non-SEM applications. So, it would be time consuming work for me to understand enough to implement or maintain it. My own current priority is on non-gaussian, nonlinear models (GLM, counts, other distributions) while standard SEM heavily relies on linear and gaussian (or elliptical) assumptions.\r\n\r\nIf someone provides a pull request and knows which parts of SEM we should add, then I can help, review and merge the PR. (That requires a lot less time than figuring it out myself.)\r\n","Aside: \r\nsystem of linear equations is supported by the linearmodels package. But that comes from the econometrics tradition and has different underlying stochastic assumptions than SEM, AFAIU.","I'm not sure where to start if I wanted to contribute in this feature, but these are the components that are usually involved in SEM:\r\n\r\n**Variance-Based SEM (PLS-SEM) [1]:**\r\n- Measurement Model (indicator loadings, internal consistency reliability [rhoA], convergent validity [AVE], discriminant validity [HTMT])\r\n- Structural Model (multicollinearity, path coefficient size, p-value, r2, f2, q2)\r\n- Mediation Analysis (path coefficient size, p value)\r\n\r\n**Covariance-Based SEM (CB-SEM) [2]:**\r\n- Measurement Model (indicator loadings, goodness of fit [df, chi2, rmsea, nfi, nnfi, cfi], construct reliability, AVE)\r\n- Structural Model (path coefficient size, p-value)\r\n- Mediation Analysis (path coefficient size, p value)\r\n\r\n**References:**\r\n1. Hair, J., Ringle, C., Danks, N., Hult, T., Sarstedt, M., & Ray, S. (2021). _Partial Least Squares Structural Equation Modeling (PLS-SEM) Using R_. Cham: Springer Nature. https:\/\/link.springer.com\/book\/10.1007\/978-3-030-80519-7\r\n2. Malhotra, N. K.  (2019). _Marketing Research: An Applied Orientation, Global Edition,  7th Edition_. Harlow:Pearson. https:\/\/bookshelf.vitalsource.com\/books\/9781292265919","Thanks for the list and references.\r\n\r\nI terms of supporting SEM, one part that always discourages me when looking at this literature is that we need some support for specifying path diagrams or causal paths. It looks like there are python packages for it, but I never looked at those details.\r\n\r\nRelated: we have a PR for SEM style mediation analysis #8733\r\nIt will be merged for the next release. Currently the merge is delayed because I'm not sure about the location for it in statsmodels. statsmodels will get a new statsmodels.causal directory that should be the location for traditional and \"modern\" causal or treatment effect analysis.\r\n\r\nAFAICS, I never looked at PLS-SEM. I looked partially at CB-SEM for covariance analysis (without the restrictions imposed by structural models.)\r\n\r\n","I looked a bit at semopy.\r\n\r\nIt looks pretty good and supports the extras for SEM, e.g. SEM-model specification and graphs.\r\nIt looks too large to replicate or include in statsmodels.\r\n\r\nIt looks like it would be a good package if it hade more than one developer\/maintainer. At least it needs maintenance for changes in dependencies (e.g. pandas, sklearn in a gitlab PR). \r\nTheir unit tests look like their are not extensive, but run after the maintenance PR with 3 deprecation warnings coming from scipy and indirectly through numdifftools which in my version has 2 numpy\/scipy deprecation warnings.\r\n\r\n\r\n```\r\n...\\semopy\\semopy>pygount --format=summary semopy\r\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503 Language               \u2503 Files \u2503     % \u2503 Code \u2503     % \u2503 Comment \u2503    % \u2503\r\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2529\r\n\u2502 Python                 \u2502    55 \u2502  67.1 \u2502 6547 \u2502  50.1 \u2502    5602 \u2502 42.8 \u2502\r\n\u2502 HTML                   \u2502     1 \u2502   1.2 \u2502  145 \u2502  93.5 \u2502       0 \u2502  0.0 \u2502\r\n\u2502 Text only              \u2502     3 \u2502   3.7 \u2502   22 \u2502 100.0 \u2502       0 \u2502  0.0 \u2502\r\n\u2502 CSS+Lasso              \u2502     1 \u2502   1.2 \u2502    2 \u2502  28.6 \u2502       5 \u2502 71.4 \u2502\r\n\u2502 JavaScript+Genshi Text \u2502     1 \u2502   1.2 \u2502    1 \u2502  14.3 \u2502       6 \u2502 85.7 \u2502\r\n\u2502 __unknown__            \u2502    15 \u2502  18.3 \u2502    0 \u2502   0.0 \u2502       0 \u2502  0.0 \u2502\r\n\u2502 __duplicate__          \u2502     1 \u2502   1.2 \u2502    0 \u2502   0.0 \u2502       0 \u2502  0.0 \u2502\r\n\u2502 __binary__             \u2502     5 \u2502   6.1 \u2502    0 \u2502   0.0 \u2502       0 \u2502  0.0 \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 Sum                    \u2502    82 \u2502 100.0 \u2502 6717 \u2502  50.6 \u2502    5613 \u2502 42.3 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n```\r\n\r\nAside:\r\ndifference in model specification compared to simultaneous equation models is that it needs additional specification for latent variables and variance\/covariance restrictions.\r\n(measurement equation and latent variables similar to measurement error models or statespace models.) "],"labels":["type-enh","comp-multivariate","comp-causal"]},{"title":"REF\/Design: deprecate positional use of keyword parameters","body":"https:\/\/github.com\/scipy\/scipy\/pull\/18843\r\n\r\nundecided","comments":["There's much more to this than a PR (I think you know, but to be sure); here's some links:\r\nhttps:\/\/scikit-learn-enhancement-proposals.readthedocs.io\/en\/latest\/slep009\/proposal.html\r\nhttps:\/\/github.com\/scipy\/scipy\/issues\/14714\r\n","Definitely the right thing to do.  "],"labels":["design","type-refactor"]},{"title":"ENH: Add Gwet's AC1","body":"This implements Gwet's AC_1 (for two and three or more raters) and adds the relevant tests.","comments":["Thanks for the PR. Sounds interesting but I never heard of it (which is not surprising given my weak background in this area) and I will need to briefly read up on it, e.g. https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2215016123002108\r\n\r\nI'm currently on vacation and will look at it early August.\r\nprio-elevated as a reminder to myself, or ping me if I forget about the issue","@josef-pkt Is there anything I can do to make this a bit easier to review?"],"labels":["type-enh","comp-stats","prio-elev"]},{"title":"ols() : Row order of result table do not fit the formula","body":"#### Is your feature request related to a problem? Please describe\r\nI use `statsmodels.formula.api.ols()` to do linear regression. The ordering of rows in the resulting table is not the same as the variables in the formula.\r\n\r\nAs an example formula: `'scoreBeeintraechtigung ~ scale(Age) + scoreInformiertheit + scoreNegEinstellung + ZuHeisseTageN + C(AngenehmInWohnung, Treatment(reference=\"Ja\"))'`\r\n\r\nThe result looks like this\r\n```\r\n                                                      coef     SE      t      p              CI95\r\nIntercept                                           4.2932  0.847  5.070  0.000     (2.63, 5.956)\r\nC(AngenehmInWohnung, Treatment(reference=\"Ja\"))...  1.6363  0.378  4.324  0.000     (0.893, 2.38)\r\nscale(Age)                                         -0.7743  0.176 -4.407  0.000  (-1.119, -0.429)\r\nscoreInformiertheit                                -0.2697  0.117 -2.312  0.021  (-0.499, -0.041)\r\nscoreNegEinstellung                                 0.6791  0.081  8.434  0.000    (0.521, 0.837)\r\nZuHeisseTageN                                       0.0969  0.019  5.001  0.000    (0.059, 0.135)\r\n```\r\n#### Describe the solution you'd like\r\nThe ordering should be the same as in the formula.\r\n","comments":["There is not much that we can do here. Patsy handles the formula and has no option to change the ordering of terms. \r\nAFAIR, categorical variables are moved to the beginning of the term list (not sure I remember correctly what the ordering pattern is)\r\n\r\nThe only thing we could do is provide a summary or summary dataframe that lets the user provide a reordered index.\r\nBecause statsmodels is not parsing the formula string, only patsy would know the original term order in the formula.\r\n\r\nReordering the model and results attributes would be too fragile, there are too many things that could go wrong or would be a lot of work to implement.\r\n","I don't know if switching to formulaic would make a difference for ordering or ordering options.","Thanks for clearing this up. Than I have to do the ordering [myself](https:\/\/codeberg.org\/buhtz\/buhtzology).","We can leave it open.\r\nI think we should get the params_table with option for ordering.\r\nThat might actually be easy, e.g. we might be able to reuse results `t_test` with row permutation of identity matrix"],"labels":["type-enh","comp-base","comp-formula"]},{"title":"Dynamic Factor Models forecast output ","body":"Hi, \r\n\r\nI am using the `DynamicFactorMQ` class to implement a dynamic factor model. In general all good except when I am trying to use the forecast method\r\n\r\nHere is for example my data that I feed into the `DynamicFactorMQ` \r\n\r\n```\r\n# Construct the dynamic factor model\r\nreturns.index = pd.to_datetime(returns.index)\r\nreturns.index = pd.DatetimeIndex(returns.index).to_period('D')\r\n\r\n\r\ndescription  3i Group      ASML    Abbvie  ...     VINCI      Visa  Waste Management\r\ndate                                       ...                                      \r\n2023-07-03  -0.005387  0.012597  0.002078  ...  0.006463  0.002063         -0.011648\r\n2023-07-04  -0.013784  0.000000  0.000000  ... -0.001738  0.000000          0.000000\r\n2023-07-05  -0.009395 -0.024813  0.019332  ... -0.015511  0.006219         -0.002042\r\n2023-07-06  -0.010870 -0.022860 -0.002689  ... -0.031466 -0.002380          0.001812\r\n2023-07-07   0.004626 -0.003675 -0.012750  ...  0.006873 -0.010173         -0.007878\r\n\r\n```\r\n\r\nAnd estimated this model following the same steps as [here](https:\/\/www.chadfulton.com\/topics\/statespace_large_dynamic_factor_models.html#:~:text=This%20notebook%20describes%20working%20with%20these%20models%20in,of%20the%20%E2%80%9Cnews%E2%80%9D%20from%20updated%20datasets%208%20References)\r\n```\r\n\r\nmodel = sm.tsa.DynamicFactorMQ(returns, factors=factors,\r\n                               factor_orders=factor_orders,\r\n                               factor_multiplicities=factor_multiplicities)\r\n\r\nresults = model.fit(disp=10)\r\n\r\n```\r\n\r\nAs you can see my dataframe ends in '2023-07-07'. Yet, when I use the `forecast` method I get an unexpected outcome. \r\n\r\nFor instance using:\r\n```\r\n\r\n# Create point forecasts, 3 steps ahead\r\npoint_forecasts = results.forecast(steps=3)\r\n\r\n# Print the forecasts for the first 5 observed variables\r\nprint(point_forecasts.T.head())\r\n```\r\n\r\nI get \r\n```\r\n\r\n              2018-03-15  2018-03-16  2018-03-17\r\n3i Group        -0.007512   -0.005220   -0.005549\r\nASML            -0.003086   -0.003375   -0.003909\r\nAbbvie          -0.002512   -0.002067   -0.002270\r\nAlbemarle       -0.004013   -0.005110   -0.004677\r\nAlphabet Inc.   -0.002557   -0.002535   -0.003110\r\n```\r\n\r\nThat is the forecast is for `2018-03-15` onwards, while I was expecting `2023-07-07` onwards. \r\n\r\nPlease note that my Data were daily observations. The outcome was the expected one, only when I resampled to monthly. I see this as a bug. \r\n\r\n","comments":["Thanks for reporting this. It sounds like it might be the same as #6247. Does your daily data have gaps in it? e.g. the index goes from 2018-03-14 to 2018-03-16 or something similar at some point?\r\n\r\nWe need to at least raise an exception in these cases instead of returning forecasts with the wrong dates, although ideally we would just fix the behavior so that non-full indexes would be properly supported.\r\n\r\nIf the problem is gaps in the index, a workaround is to reindex the dataset to have a full index (so there would be NaN values for the missing indexes, but having NaN values is not a problem for the model).","Yes, indeed has gaps. "],"labels":["type-bug","comp-tsa-statespace"]},{"title":"ENH: Leybourne-McCabe stationarity test","body":"Hi SM - I started looking at stationarity again and forgot that I have a nice implementation of Leybourne-McCabe stationarity test (https:\/\/github.com\/JimVaranelli\/Leybourne-McCabe) which I'm happy to PR if there is any interest. AFAIK, Matlab is the only other implementation which was used for unit test verification. For usability I added an empirical autolag procedure as well as allowing for the possibility of zero lags. MC-derived CVs are also provided. Further details are in the readme.","comments":["Sounds interesting, but I have no information about it (and I'm currently on vacation)\r\n\r\nquick search\r\nCaner, Mehmet, and Lutz Kilian. \"Size distortions of tests of the null hypothesis of stationarity: evidence and implications for the PPP debate.\" Journal of International Money and Finance 20, no. 5 (2001): 639-657.\r\nhttps:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0261560601000110\r\n\r\nabstract sound pretty negative on stationarity tests (including KPSS)\r\nThat's not a reason not to add it, but it would be good to have more information.\r\n","Hi @josef-pkt - just got back myself! Yes the Caner study is well known but like most if not all size\/power studies wrt stationarity\/unit-root tests, there is simply no way to evaluate performance for all given DGPs. Of course asymptotically, the LM tests performance is exactly the same as KPSS (very similar to ADF unit-root test and it various derivatives). One of the aims of my research is to better understand the performance of these tests in small samples which typically arise in financial series. No matter if you are using stationarity or unit-root tests, there are always large size distortions as order approaches unity. One aspect I've been looking at is using an ensemble of tests with a mix of stat\/UR tests which is why I have all of those implementations.","As a point of interest, here are the MC-derived asymptotic distributions for ADF & KPSS (constant model). As counterpoint, using the same DGPs as the asymptotic studies, the stationarity tests actually perform better than the UR counterparts when limiting sample size to 25. Here are the size figures for 5% significance: ADF = 6.9%, DFGLS = 16.6%, KPSS\/LM = 4.8%, PP = 6.5%. These numbers change significantly depending upon chosen DGP (for example ARFIMA with d=0.5).\r\n![adf c](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/39306965\/e8bebd98-f1ad-4457-9a69-3d7cacb4de92)\r\n![kpss c](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/39306965\/f129e58d-d337-4737-b0c7-fb5ac8259639)\r\n","FYI - just issued pull request #9039 for this\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/9039"],"labels":["comp-tsa","type-enh"]},{"title":"Logit.fit(full_output=0) \u2192 TypeError: 'NoneType' object is not subscriptable \u2192 missing \"mle_retvals\"","body":"Hello,\r\n\r\nrecently updated to latest sm release (0.14) and newly sm.Logit(Y, X).fit(full_output=0) produces error:\r\n-> 4430         self.converged = mlefit.mle_retvals[\"converged\"]\r\nTypeError: 'NoneType' object is not subscriptable\r\n\r\nBy removing param (default value 1) or setting it as 1 -- sm.Logit(Y, X).fit(full_output=1)  -- then fit produces standard result as previous versions of sm.\r\n\r\nI assume that removing \"mle_retvals\" by specifing param full_output=0 in the fit is the issue and not desired behavior (didn't produce error in prev. versions).\r\n\r\nBest,\r\nMichal","comments":["I'm not sure in which direction this should be fixed, \r\n- don't allow `full_output=0` or \r\n- make `mle_retvals` not available.\r\n\r\nI though our solution what the former when we had this issue several years ago.\r\nFor example, we need the `converged` information, and I don't think it's useful to have to set it to nan. The cost for full_output should be very small, because it just changes what is returned by scipy.optimize but not what is computed. (AFAIR, I have not looked at the code in scipy.optimize in a long time.)\r\n\r\n"],"labels":["type-bug","comp-base","comp-discrete"]},{"title":"0.14.0: pytest is failing in `statsmodels\/treatment\/tests\/test_teffects.py` unit","body":"I'm packaging your module as an rpm package so I'm using the typical PEP517 based build, install and test cycle used on building packages from non-root account.\r\n- `python3 -sBm build -w --no-isolation`\r\n- because I'm calling `build` with `--no-isolation` I'm using during all processes only locally installed modules\r\n- install .whl file in <\/install\/prefix> using 'installer` module\r\n- run pytest with $PYTHONPATH pointing to sitearch and sitelib inside <\/install\/prefix>\r\n- build is performed in env which is *`cut off from access to the public network`* (pytest is executed with `-m \"not network\"`)\r\n\r\nHere is pytest output:\r\n<details>\r\n\r\n```console\r\n+ PYTHONPATH=\/home\/tkloczko\/rpmbuild\/BUILDROOT\/python-statsmodels-0.14.0-2.fc35.x86_64\/usr\/lib64\/python3.8\/site-packages:\/home\/tkloczko\/rpmbuild\/BUILDROOT\/python-statsmodels-0.14.0-2.fc35.x86_64\/usr\/lib\/python3.8\/site-packages\r\n+ \/usr\/bin\/pytest -ra -m 'not network' statsmodels --import-mode=importlib\r\n==================================================================================== test session starts ====================================================================================\r\nplatform linux -- Python 3.8.17, pytest-7.4.0, pluggy-1.0.0\r\nrootdir: \/home\/tkloczko\/rpmbuild\/BUILD\/statsmodels-0.14.0\r\nconfigfile: setup.cfg\r\ncollected 18241 items \/ 1 error\r\n\r\n========================================================================================== ERRORS ===========================================================================================\r\n_______________________________________________________________ ERROR collecting statsmodels\/treatment\/tests\/test_teffects.py _______________________________________________________________\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/runner.py:341: in from_call\r\n    result: Optional[TResult] = func()\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/runner.py:372: in <lambda>\r\n    call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/python.py:536: in collect\r\n    self._inject_setup_module_fixture()\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/python.py:550: in _inject_setup_module_fixture\r\n    self.obj, (\"setUpModule\", \"setup_module\")\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/python.py:315: in obj\r\n    self._obj = obj = self._getobj()\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/python.py:533: in _getobj\r\n    return self._importtestmodule()\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/python.py:622: in _importtestmodule\r\n    mod = import_path(self.path, mode=importmode, root=self.config.rootpath)\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/pathlib.py:538: in import_path\r\n    spec.loader.exec_module(mod)  # type: ignore[union-attr]\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/assertion\/rewrite.py:178: in exec_module\r\n    exec(co, module.__dict__)\r\nstatsmodels\/treatment\/tests\/test_teffects.py:31: in <module>\r\n    res_probit = Probit.from_formula(formula, dta_cat).fit()\r\n..\/..\/BUILDROOT\/python-statsmodels-0.14.0-2.fc35.x86_64\/usr\/lib64\/python3.8\/site-packages\/statsmodels\/base\/model.py:203: in from_formula\r\n    tmp = handle_formula_data(data, None, formula, depth=eval_env,\r\n..\/..\/BUILDROOT\/python-statsmodels-0.14.0-2.fc35.x86_64\/usr\/lib64\/python3.8\/site-packages\/statsmodels\/formula\/formulatools.py:63: in handle_formula_data\r\n    result = dmatrices(formula, Y, depth, return_type='dataframe',\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/highlevel.py:309: in dmatrices\r\n    (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/highlevel.py:164: in _do_highlevel_design\r\n    design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/highlevel.py:62: in _try_incr_builders\r\n    formula_like = ModelDesc.from_formula(formula_like)\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/desc.py:164: in from_formula\r\n    tree = parse_formula(tree_or_string)\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/parse_formula.py:146: in parse_formula\r\n    tree = infix_parse(_tokenize_formula(code, operator_strings),\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/infix_parser.py:210: in infix_parse\r\n    for token in token_source:\r\n\/usr\/lib\/python3.8\/site-packages\/patsy\/parse_formula.py:89: in _tokenize_formula\r\n    for pytype, token_string, origin in it:\r\nE   TypeError: next expected at least 1 argument, got 0\r\n===================================================================================== warnings summary ======================================================================================\r\n..\/..\/..\/..\/..\/usr\/lib\/python3.8\/site-packages\/jupyter_client\/connect.py:20\r\n  \/usr\/lib\/python3.8\/site-packages\/jupyter_client\/connect.py:20: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\r\n  given by the platformdirs library.  To remove this warning and\r\n  see the appropriate new directories, set the environment variable\r\n  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\r\n  The use of platformdirs will be the default in `jupyter_core` v6\r\n    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\r\n\r\n-- Docs: https:\/\/docs.pytest.org\/en\/stable\/how-to\/capture-warnings.html\r\n================================================================================== short test summary info ==================================================================================\r\nERROR statsmodels\/treatment\/tests\/test_teffects.py - TypeError: next expected at least 1 argument, got 0\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\n=============================================================================== 1 warning, 1 error in 16.20s ================================================================================\r\n```\r\n<\/details>\r\n\r\nHere is list of installed modules in build env\r\n<details>\r\n\r\n```console\r\nPackage                       Version\r\n----------------------------- -------\r\nalabaster                     0.7.13\r\nasttokens                     2.2.1\r\nattrs                         23.1.0\r\nBabel                         2.12.1\r\nbackcall                      0.2.0\r\nbeautifulsoup4                4.12.2\r\nbleach                        6.0.0\r\nbuild                         0.10.0\r\ncharset-normalizer            3.1.0\r\ncontourpy                     1.0.7\r\ncycler                        0.11.0\r\nCython                        0.29.35\r\ndecorator                     5.1.1\r\ndefusedxml                    0.7.1\r\ndistro                        1.8.0\r\ndocutils                      0.19\r\nexceptiongroup                1.1.1\r\nexecuting                     1.2.0\r\nfastjsonschema                2.16.3\r\nfonttools                     4.40.0\r\ngpg                           1.20.0\r\nhtml5lib                      1.1\r\nidna                          3.4\r\nimagesize                     1.4.1\r\nimportlib-metadata            6.7.0\r\nimportlib-resources           5.12.0\r\niniconfig                     2.0.0\r\ninstaller                     0.7.0\r\nipython                       8.12.0\r\njedi                          0.18.2\r\nJinja2                        3.1.2\r\njsonschema                    4.17.3\r\njupyter_client                8.3.0\r\njupyter_core                  5.3.1\r\njupyterlab-pygments           0.1.2\r\nkiwisolver                    1.4.4\r\nlibcomps                      0.1.19\r\nMarkupSafe                    2.1.2\r\nmatplotlib                    3.6.3\r\nmatplotlib-inline             0.1.6\r\nmistune                       3.0.1\r\nnbclient                      0.8.0\r\nnbconvert                     7.6.0\r\nnbformat                      5.9.0\r\nnbsphinx                      0.9.2\r\nnumpy                         1.24.3\r\nnumpydoc                      1.5.0\r\nolefile                       0.46\r\npackaging                     23.1\r\npandas                        2.0.2\r\npandocfilters                 1.5.0\r\nparso                         0.8.3\r\npatsy                         0.5.3\r\npexpect                       4.8.0\r\npickleshare                   0.7.5\r\nPillow                        9.5.0\r\npkgutil_resolve_name          1.3.10\r\nplatformdirs                  3.8.0\r\npluggy                        1.0.0\r\nprompt-toolkit                3.0.38\r\nptyprocess                    0.7.0\r\npure-eval                     0.2.2\r\nPygments                      2.15.1\r\npyparsing                     3.1.0\r\npyproject_hooks               1.0.0\r\npyrsistent                    0.19.3\r\npytest                        7.4.0\r\npython-dateutil               2.8.2\r\npytz                          2023.2\r\nPyYAML                        6.0\r\npyzmq                         24.0.1\r\nrequests                      2.31.0\r\nSciPy                         1.8.1\r\nsetuptools                    68.0.0\r\nsetuptools-scm                7.1.0\r\nsix                           1.16.0\r\nsnowballstemmer               2.2.0\r\nsoupsieve                     2.4.1\r\nSphinx                        6.2.1\r\nsphinxcontrib-applehelp       1.0.4\r\nsphinxcontrib-devhelp         1.0.2\r\nsphinxcontrib-htmlhelp        2.0.0\r\nsphinxcontrib-jsmath          1.0.1\r\nsphinxcontrib-qthelp          1.0.3\r\nsphinxcontrib-serializinghtml 1.1.5\r\nstack-data                    0.6.2\r\ntinycss2                      1.2.1\r\ntomli                         2.0.1\r\ntornado                       6.3.2\r\ntraitlets                     5.9.0\r\ntyping_extensions             4.6.3\r\nurllib3                       1.26.15\r\nwcwidth                       0.2.6\r\nwebencodings                  0.5.1\r\nwheel                         0.40.0\r\nzipp                          3.15.0\r\n```\r\n<\/details>\r\n","comments":["There are a lot of things that aren't really supported.  The only ways we support running the tests are either in the source tree or using\r\n\r\n```\r\npython -c \"import statsmodels;statsmodels.test(exit=True)\"\r\n```\r\n\r\nNot that I think this could cause this error.  Is the version of patsy you are using patched in any way?","could there be a problem with pytest   7.4.0 ?\r\nAFAICS, our azure testing uses older versions.","I'm not sure however it can be as well something which needs to be changed in test suite to be able use it with new pytest ..","Can you downgrade pytest?\r\n\r\nI wouldn't trust a `.0` release and wouldn't want to spend time on hunting bugs that might possibly in pytest.\r\n\r\nIf the error persists in later versions, or someone figures out how to avoid it, then we can fix the current unit test. ","Looking on https:\/\/github.com\/pytest-dev\/pytest\/compare\/7.4.0...7.4.x I don't see any already committed fix which may affect testing `statsmodels`. Additionally ..\r\n```console\r\n[tkloczko@pers-jacek SPECS]$ ls -1 python-*.spec | wc -l; grep ^%pytest python-*.spec | wc -l\r\n1171\r\n1154\r\n```\r\nWhich meas that I have currently 1171 python modules packaged as rpm packages and 1154 are using pytest on testing.\r\nI want only to say that I'm not aware of any bug in pytest affecting any of those test suites.","It's for sure a pytest bug.\r\nIt looks like it's messing with the module code and does not exec it correctly.\r\n\r\n```\r\n\/usr\/lib\/python3.8\/site-packages\/_pytest\/assertion\/rewrite.py:178: in exec_module\r\n    exec(co, module.__dict__)\r\n```\r\n\r\n(I don't like `exec` and both pytest and patsy are using it. No idea where this goes wrong)\r\n\r\nHowever, I guess the solution is simply to put the two lines in a function or inside the `setup_class`\r\n```\r\nformula = 'mbsmoke_ ~ mmarried_ + mage + mage2 + fbaby_ + medu'\r\nres_probit = Probit.from_formula(formula, dta_cat).fit()\r\n```","OK I'll try to open issue ticket against `pytest` \ud83d\udc4d "],"labels":["type-test","maintenance"]},{"title":"statsmodels.tools.sm_exceptions.X13Error: ERROR: Number of user-defined X elements= 166 not equal to a multiple of the number of columns= 4","body":"My test data has two columns `M0 `and `exog` (you may download the X13 software and test data from [this link](https:\/\/www.dropbox.com\/s\/v4adf86g2rqzan5\/X13_data.zip?dl=0) ).\r\n\r\n```\r\nimport statsmodels.api as sm\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\n\r\nX13PATH = r'D:\\data\\X13'\r\nos.chdir(X13PATH)\r\nos.environ['X13PATH'] = X13PATH\r\nprint(os.environ['X13PATH'])\r\n\r\nfile_path = '..\/test_data\/X13_test_data.xlsx'\r\nsheet_name = 'Sheet1'\r\n\r\ndf = pd.read_excel(file_path, sheet_name=sheet_name, index_col='date')\r\nprint(df.head(5))\r\n\r\ndf.index = pd.to_datetime(df.index)\r\n# df = df.rename(columns={'festival_factor': 'exog', 'M0': 'endog'})\r\nres = sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df['exog'], x12path=X13PATH, print_stdout=True)\r\n# res = sm.tsa.x13_arima_analysis(endog=df['M0'], x12path=X13PATH, print_stdout=True)\r\nprint(res.seasadj)\r\nprint(res.plot)\r\n```\r\nOut:\r\n\r\n```\r\n                  M0      exog\r\ndate                          \r\n2009-07-31  34239.30  0.000000\r\n2009-08-31  34406.62  0.000000\r\n2009-09-30  36787.89  0.333333\r\n2009-10-31  35730.23  0.516129\r\n2009-11-30  36343.86  0.000000\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\data\\X13_test.py\", line 19, in <module>\r\n    res = sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df['exog'], x12path=X13PATH, print_stdout=True)\r\n  File \"C:\\Users\\LSTM\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py\", line 210, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py\", line 518, in x13_arima_analysis\r\n    _check_errors(errors)\r\n  File \"D:\\Program Files\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py\", line 201, in _check_errors\r\n    raise X13Error(errors)\r\nstatsmodels.tools.sm_exceptions.X13Error: ERROR: Number of user-defined X elements= 166\r\n        not equal to a multiple of the number of columns=  4.\r\n```\r\nI use `sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df['exog'], x12path=X13PATH, print_stdout=True)` to perform X13 ARIMA on `M0 `For seasonal adjustment, the `exog `column is used as an exogenous variable, but an error occurs: `statsmodels.tools.sm_exceptions.X13Error: ERROR: Number of user-defined X elements= 166 not equal to a multiple of the number of columns= 4.`\r\n\r\nMy analysis found that the main cause of the error is caused by `exog=df['exog']`, because `sm.tsa.x13_arima_analysis(endog=df['M0'], x12path=X13PATH, print_stdout=True)` can run normally.","comments":["(I'm on vacation and cannot try out anything)\r\n\r\nMaybe `exog` needs to be a `DataFrame` and not a `Series`.\r\nyou could try\r\n`sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df[['exog']], x12path=X13PATH, print_stdout=True)`","Thanks for helping me to deal with this issue. After I changed code to `sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df[['exog']], x12path=X13PATH, print_stdout=True)`, it generates a new error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\EconomicTSProcessor\\X13_test.py\", line 35, in <module>\r\n    res = sm.tsa.x13_arima_analysis(endog=df['M0'], exog=df[['exog']], x12path=X13PATH, print_stdout=True)\r\n  File \"C:\\Users\\LSTM\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py\", line 210, in wrapper\r\nb''\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py\", line 518, in x13_arima_analysis\r\n    _check_errors(errors)\r\n  File \"D:\\Program Files\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py\", line 201, in _check_errors\r\n    raise X13Error(errors)\r\nstatsmodels.tools.sm_exceptions.X13Error: ERROR: forecasts end date, 2024.Apr, must end on or before \r\n        user-defined regression variables end date, 2023.Apr.\r\n```","I have not looked at the details. However\r\n\r\nThe error message indicates that `exog` is not available for the forecast horizon.\r\nTo forecast with `exog`, the values of the regression variable needs to be available for the forecast periods. Because `exog` are not forecast by the model, the user needs to provide those values for the forecast horizon.\r\nThat's for example easy if those are calendar events, like season, holidays, ..., but needs to be predicted outside of the model for other `exog`.\r\n\r\nTo check, you could limit the endog to shorten the sample period and use the extra exog periods for the forecast periods.","Thank you. Would you mind testing it with the data and code I provided in the link at the beginning of the question? I took the first 10 rows of `df` for testing (`df = df.head(10)`), and an error occurred:` statsmodels.tools.sm_exceptions.X13Error: ERROR: Series to be modeled and\/or seasonally adjusted must have at least 3 complete years of data .` If I take the first 36 rows for testing, the result is as follows: `statsmodels.tools.sm_exceptions.X13Error: ERROR: forecasts end date, 2013.Jun, must end on or before user-defined regression variables end date, 2012.Jun.`\r\nMy data `df `is like this, I want to seasonally adjust `M0 `using `X13 ` method while using `exog` from `df` as exogenous variable::\r\n\r\n ```\r\n                  M0 exog\r\ndate\r\n2009-07-31 34239.30 0.000000\r\n2009-08-31 34406.62 0.000000\r\n2009-09-30 36787.89 0.333333\r\n2009-10-31 35730.23 0.516129\r\n2009-11-30 36343.86 0.000000\r\n...\r\n```"],"labels":["comp-tsa","type-bug"]},{"title":"ENH: GAM combining different penalizations, splines","body":"We don't have an option to combine two different spline types in GAM\r\nhttps:\/\/stackoverflow.com\/questions\/76541891\/use-both-cyclic-and-b-spline-smoothers-in-statsmodels-gam\r\n\r\nWe had this question once before, soon after I had merged GAM.\r\n\r\nRelated we don't have a simple way to add different penalties, Penalty classes, either for overlapping or non-overlapping params.","comments":[],"labels":["type-enh","topic-penalization","comp-gam"]},{"title":"mnlogit does not have confidence intervals for predictions (available in .get_prediction() for other models in statsmodels)","body":"When I tried to call result.get_prediction() after fitting result = smf.mnlogit(\"dep_var ~ indep_var\", data = data).fit() I got an error message that \"MultinomialResults object has no attribute 'get_prediction'\". Looking at the documentation for MultinomialResults (https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.discrete.discrete_model.MultinomialResults.html), we see that it says that get_prediction() is not implemented for Multinomial. \r\n\r\nIf I instead use result.predict(), I get only point predictions and not standard errors and confidence intervals.\r\n\r\nWhile for the regular logit there is workaround in statsmodels whereby we can fit result = smf.glm(\"dep_var ~ indep_var\", data = data, family = sm.families.Binomial()).fit() instead of fitting result = smf.logit((\"dep_var ~ indep_var\", data = data).fit() and use result.get_prediction() with the results of the glm estimation, multinomial logit is not available via glm in statsmodels. \r\n\r\n Since including measures of uncertainty when making predictions is important, I think .get_prediction() should be implemented for mnlogit in statsmodels. \r\n","comments":["I did not implement get_prediction for MNLogit yet.\r\nAnything there is will be inherited and might not be appropriate.\r\n\r\nI'm not sure yet how to do that, because prediction is multivariate, and does not match the current get_prediction pattern.\r\nSimilar problem for OrderedModel.\r\n\r\nAll other discrete models including `Logit` should have get_prediction in 0.14 release.\r\n\r\nI guess we will add get_prediction for MNLogit in the next few months.\r\nI'm working currently on MultivariateLS which also has multivariate `predict`, but I have not looked at get_prediction for it yet. (#8919 )\r\n\r\n(There might be a workaround with current 0.14 or main code, but I need to look. `_test_wald_nonlinear` has the generic delta method for prediction and might work for MNlogit.predict.)\r\n\r\n","I had already opened an issue for this #7888"],"labels":["type-enh","comp-discrete","topic-predict","prio-elev","topic-post_estim"]},{"title":"ENH\/SUMM: outlier-influence, diagnostic for multi-endog, multivariate models","body":"We need to collect what post-estimation results we can add for multi-endog and multivariate endog models, and how we can implement reusable methods.\r\n(I'm working currently without references, just basics for MultivariateLS\r\n\r\nmain property: \r\nendog and predict are multivariate, ndim=2, balanced with no missing values in any of the endogs.\r\n\r\n**MultivariateLS**\r\n\r\n- outlier-influence (mainly multivariate measures, measures for individual endog are as in OLS\/WLS)\r\n  - resid_distance: mahalanobis distance for each observation of multivariate resid\r\n  - hat_matrix_diag: common exog, so works analogous to OLS\/WLS\r\n  - cook's distance, loglikeobs displacement ?\r\n  - plots ? \r\n- diagnostics\r\n  - multivariate normality (I have never committed draft versions somewhere (?)) \r\n- ...\r\n\r\n**Multinomial, Ordered**\r\n\r\nI should have some issues for this, but currently don't find them.\r\n1-dim resid measure for ordered is \"stochastically larger\" resid.\r\n\r\n**future models**\r\n\r\ncopula models\r\nother multivariate, multi-endog models\r\nother linear\/gaussian models, e.g. SUR\r\nGMM multi-equation models\r\n\r\nnot clear: should this include 2-stage models like instrumental variables, sample selection or bivariate Probit, ... (full MLE models)\r\n\r\n","comments":[],"labels":["type-enh","comp-base","comp-stats","topic-diagnostic","comp-multivariate","topic-post_estim"]},{"title":"TST\/Maint: test failures in MICE, pandas compat","body":"most likely problem with pandas compatibility\r\n\r\n```\r\nFAILED statsmodels\/imputation\/tests\/test_mice.py::TestMICEData::test_default\r\nFAILED statsmodels\/imputation\/tests\/test_mice.py::TestMICEData::test_pertmeth\r\nFAILED statsmodels\/imputation\/tests\/test_mice.py::TestMICEData::test_set_imputer\r\n```\r\n\r\ntest failure are in cycle_order\r\n\r\n```\r\n>       assert_equal(imp_data._cycle_order, ['x5', 'x3', 'x4', 'y', 'x2', 'x1'])\r\nE       AssertionError: \r\nE       Items are not equal:\r\nE       item=1\r\nE       \r\nE        ACTUAL: 'x4'\r\nE        DESIRED: 'x3'\r\n```\r\npandas version: pandas-2.0.2","comments":["Strange, in latest run in only failed in `pre` testing, earlier it also failed in one of the non-pre environments\r\nfrom last merge aglebov\/plot-acf-funcs\r\nhttps:\/\/dev.azure.com\/statsmodels\/statsmodels-testing\/_build\/results?buildId=5269&view=logs&j=e5ec9234-c220-5b28-4136-a9c82efad0a5&t=314f1936-9e6b-5410-c4ea-c53498aa77ae&l=742","maybe not a good test case\r\nboth, x3 and x4 have 10 observations with nan, i.e. a tie\r\n\r\n`MiceData.__init__` uses default argsort, which is quicksort, which AFAICS is not stable\r\n```\r\n        nmiss = np.asarray(nmiss)\r\n        ii = np.argsort(nmiss)\r\n        ii = ii[sum(nmiss == 0):]\r\n        self._cycle_order = [vnames[i] for i in ii]\r\n```"],"labels":["type-bug","comp-imputation","maintenance","backport"]},{"title":"ENH\/REF: LikelihoodModelResults, propagate init kwargs to super","body":"see https:\/\/github.com\/statsmodels\/statsmodels\/issues\/4679#issuecomment-877352386\r\n\r\nI just saw again that LikelihoodModelResults does not propagate extra kwargs and they are not attached to the results instance.\r\n(current dev case MultivariateLS)\r\n\r\nIn many models, we have to explicitly attach attributes after creating the results instance in `fit`.\r\nIt would be cleaner if we could use them as results `__init__` argument.\r\n\r\nOne possibility is to use a more explicit `attach_kwds` (or similar name) instead of kwargs. This would make it easier to check for invalid kwargs, currently extra kwargs are ignored and are swallowed by Likel.ihoodModelResults.\r\n\r\nNote, I guess there will remain some cases where we change attributes after creating the results instance, in cases where we need to override the LikelihoodModelResults defaults.\r\n\r\n(I'm not changing this in my current work on MultivariateLS, because it affects many models and needs more general checking.)\r\n\r\n","comments":["aside: base LikelihoodModel and Model do also not attach `df_resid` to results instance.\r\nThis is done by subclasses, e.g. by DiscreteResults."],"labels":["type-enh","comp-base","type-refactor"]},{"title":"Design\/ENH: seaborn as optional dependency ?","body":"seaborn has some nice graphics.\r\nThe last time that I looked at the seaborn code it looks like it's too nested and structured to copy just individual parts for our use.\r\n(I'm not sure it's worth it. I write only simple statistics plot, e.g. diagnostic plots, without fancy plot elements.)\r\n\r\nexample pair plot grids\r\nhttps:\/\/stackoverflow.com\/questions\/48139899\/correlation-matrix-plot-with-coefficients-on-one-side-scatterplots-on-another\r\nhttps:\/\/seaborn.pydata.org\/tutorial\/axis_grids.html#plotting-pairwise-data-relationships\r\n\r\n\r\nA long time ago I wrote statsmodels.graphics.plot_grids.scatter_ellipse but we don't have much in this area\r\n\r\npossible application\r\nacf\/ccf plot grids https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8783#issuecomment-1589685756\r\nVAR acorr_plot is also only very basic","comments":[],"labels":["type-enh","comp-graphics"]},{"title":"WIP    ENH: add offset (and exposure) to discrete margeff derivatives","body":"#8833\r\n\r\ntrial version for adding offset and exposure to margeff derivatives.\r\n\r\nThis is not possible as a quickfix.\r\n`get_margeff` and margeff code will need changes in addition to those in derivatives to support offset and exposure.\r\nI had stopped then partway through refactoring an adding this.\r\n\r\nIt should be possible to use those derivatives with NonlinearDeltaCov, wald_nonlinear, or (future) get_prediction_nonlinear\r\nThen we would be able to write unit test and examples for them.\r\n\r\n(postponed, opening PR so I don't loose track of it)","comments":[],"labels":["type-enh","comp-discrete"]},{"title":"MAINT: scipy interp2d is deprecated\/removed","body":"\r\nhttps:\/\/scipy.github.io\/devdocs\/notebooks\/interp_transition_guide.html\r\n\r\nfile search only finds one usage of it\r\n`statsmodels.stats.tabledist.TableDist.poly2d`\r\n\r\nTableDist is used in `_lilliefors.py`, but poly2d method is not used anywhere AFAICS\r\n","comments":[],"labels":["comp-stats","maintenance"]},{"title":"MAINT: test_var still uses record arrays and views on it","body":"\r\n```\r\ndef get_macrodata():\r\n    data = macrodata.load_pandas().data[[\"realgdp\", \"realcons\", \"realinv\"]]\r\n    data = data.to_records(index=False)\r\n    nd = data.view((float, 3), type=np.ndarray)\r\n    nd = np.diff(np.log(nd), axis=0)\r\n    return nd.ravel().view(data.dtype, type=np.ndarray)\r\n```\r\n\r\nalso: is rpy2 in use or is this stale code (in test_var.py)\r\n\r\n```\r\ndef generate_var():  # FIXME: make a test?\r\n    import pandas.rpy.common as prp\r\n    from rpy2.robjects import r\r\n    ...\r\n```","comments":[],"labels":["maintenance","comp-tsa-vector-ar"]},{"title":"ENH: manova type constraint mv_test with L, M, for MNLogit, LikelihoodModel","body":"I think we can use patsy's linear constraint functions to create L and M restriction matrices for mv_test applied to MNLogit and similar.\r\ne.g. \"y1=y2\" to compare parameters of equation 1 and equation 2.\r\n\r\n`wald_test_terms` currently does not work for 2-dim params as in MNLogit.\r\nBut it would apply to creating the M (or is it L) matrices.\r\n\r\nThe wald_test_terms constraints would only be for the exog parameters. To get the full constraint matrix for ravelled\/stacked params, we need the kronecker product of either eye kron M or something like L cron M. \r\n(I don't remember right now. I need to check which is which.)\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/8905#issuecomment-1583081922\r\n","comments":["based on reading the code\r\nconverting string constraints to constraint matrices L or M (using patsy) is already implemented, but my current examples do not work. \r\nAlso, I don't see unit tests for it. All hypotheses in test_multivariate_ols are defined by numpy constraint matrices.\r\n\r\nEither it's tricky to use which I have not figured out yet, or, more likely, there are bugs.\r\n\r\n**update**\r\nThere is a unit test `test_specify_L_M_by_string` in test_multivariate_ols.py\r\nSo not clear why it doesn't work for what I tried.\r\nI was trying to only specify the M matrix, with default L matrix.\r\n\r\n**update 2**\r\nI had mixed up the constraints. L is for exog, M is for endog. I had it reversed.\r\nIt works with correct definition of constraints.\r\n\r\nL, M, C are not informative names\r\nLex, Mend ? "],"labels":["type-enh","comp-base","comp-discrete","comp-multivariate"]},{"title":"FAQ-D, design: multivariate endog, layout, shape overview","body":"(I'm trying to get an overview of our multivariate, multi-endog models to see if what shape, layout multi-equation attributes have and whether they are consistent across models)\r\n\r\nsee also \r\n- #3137\r\n- #8575\r\n- ...\r\n\r\ncurrent models with 2-dim `params`: \r\nMNLogit, MANOVA\/_MultivariateOLS, VAR, VECM, statespace VARMAX, ...?\r\n\r\n- 2-dim params, bse\r\n  - MNLogit: `(K, J - 1)`  exog in rows, equations\/choices in columns\r\n  - VAR: `(df_model, self.neqs)`\r\n  - MultivariateOLS: (k_exog, k_endgo),   `params = pinv_x.dot(y)`\r\n- ravel, reshape\r\n  - MNLogit\r\n    - to 1-dim: `params = mnfit.params.reshape(self.K, -1, order='F')` (in fit)\r\n    - to 2dim:  `bse.reshape(self.params.shape, order='F')`, `params = params.reshape(self.K, -1, order='F')` (in fit)\r\n  - VAR (also uses 3-dim or list of 2-dim, by lags): \r\n    - to 1dim: `cov_params = np.kron(np.linalg.inv(z.T @ z), self.sigma_u)`\r\n    - to 2dim:  `stderr.reshape((self.df_model, self.neqs), order=\"C\")`\r\n   - base.LikelihoodModel.t_test and wald_test: `params = self.params.ravel()` (is this consistent with MNLogit?)\r\n- params names\r\n  - base.LikelihoodModel.t_test and wald_test `names = self.model.data.cov_names`\r\n  - see cov_names https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8907#issuecomment-1584812877\r\n  - cov in MNLogit has order \"F\", in VAR has order \"C\" (AFAICS)\r\n- summary()\r\n  - ??? todo \r\n- predict\r\n- ...","comments":["partially related: vec, unvec, .... function for cov, correlation tools #4143\r\nnote: `vec` is order=\"F\""],"labels":["design","FAQ","comp-multivariate"]},{"title":"DOC\/ENH: doc examples for multivariate mv_test","body":"I don't see any example for how to use mv_test for other hypotheses than default MANOVA.\r\nThis needs examples in docstrings and\/or notebooks.\r\ne.g. \r\nrow versus column of params (cross-equation or cross-variables)\r\nL versus M constraints on columns versus rows\r\nwhich is which?\r\n\r\n\r\nSeveral examples are in unit tests.\r\nhttps:\/\/stackoverflow.com\/questions\/76336133\/multivariate-linear-hypothesis-testing-using-statsmodels-in-python","comments":[],"labels":["comp-docs","comp-multivariate"]},{"title":"ENH\/BUG: vectorized NonlinearDeltaCov and get_wald_nonlinear","body":"It looks like NonlinearDeltaCov and `_get_wald_nonlinear` work for vectorized (2d) functions, but predicted and se have transposed shapes.\r\n\r\nmargeff does not have inference `se` if `at=\"all\"`  (used with atexog=exog array with more than one row)\r\n\r\nusing example from statsmodels.stats.tests.test_deltacov.test_deltacov_margeff\r\nwith either Poisson or GLM-Poisson (#8889)\r\n\r\n\r\n```\r\ndef f(p):\r\n    ex = res_poi.model.exog[:5]\r\n    fv = res_poi.model._derivative_exog(p, ex)\r\n    return np.squeeze(fv)\r\n\r\nnlp = NonlinearDeltaCov(f, res_poi.params, res_poi.cov_params())\r\n\r\nnlpm = res_poi._get_wald_nonlinear(f)\r\n\r\nnlp.predicted().shape, nlp.se_vectorized().shape\r\n((5, 10), (10, 5))\r\nnlpm.predicted().shape, nlpm.se_vectorized().shape\r\n((5, 10), (10, 5))\r\n```","comments":["AFAICS\r\ncurrent unit tests in \"statsmodels\\discrete\\tests\\test_predict.py\" for `which=\"prob\"` has only cases for 1-d returns, either exog is one observation or `average=True`.\r\n\r\nif there were unit test for multi-return case, then it should have raised an error.\r\ne.g. vectorized, multi-observation `ex`\r\n\r\n```\r\npred = res_poi0.get_prediction(ex, which=\"prob\", y_values=np.arange(2))\r\npred.predicted.shape, pred.se.shape\r\n((20190, 2), (2, 20190))\r\n```\r\n\r\nAFAICS (preliminary checking)\r\nthose results are correct (except for transposed se) and correspond to loop over observations\/rows.\r\n\r\ni.e. vectorized with common cov_params\r\nsee #8890 for more general support of vectorized or multivariate cases\r\n","wald_test would need 3-dim cov of constraints\r\ne.g. joint test that all elements in a row are equal to zero.\r\ngradient is 3-dim\r\n\r\nAFAICS, we would need dot product for the last two axis.\r\ntranspose reverses all axes, I think we need to swap the last two axes instead, i.e. transpose last two axes\r\nnumpy linalg functions are vectorized, so the main problem is transpose, making sure we get the correct axis for matrix algebra and for vectorization. (AFAIR, we don't use much vectorized linalg yet, outside of possibly tsa)\r\n\r\nor maybe `grad` numdiff produces the wrong shape (axis order) for this.\r\n\r\n\r\n```\r\nnlp.wald_test(0)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp\\ipykernel_33812\\1373430243.py in <module>\r\n----> 1 nlp.wald_test(0)\r\n\r\n...\\statsmodels\\statsmodels\\stats\\_delta_method.py in wald_test(self, value)\r\n    140         # TODO: add use_t option or not?\r\n    141         m = self.predicted()\r\n--> 142         v = self.cov()\r\n    143         df_constraints = np.size(m)\r\n    144         diff = m - value\r\n\r\n...\\statsmodels\\statsmodels\\stats\\_delta_method.py in cov(self)\r\n     97         \"\"\"\r\n     98         g = self.grad()\r\n---> 99         covar = np.dot(np.dot(g, self.cov_params), g.T)\r\n    100         return covar\r\n    101 \r\n\r\n<__array_function__ internals> in dot(*args, **kwargs)\r\n\r\nValueError: shapes (10,6,10) and (10,6,10) not aligned: 10 (dim 2) != 6 (dim 1)\r\n\r\nnlp.grad().shape\r\n(10, 6, 10)\r\n```\r\n\r\napplication get_prediction which=\"prob\"\r\n\r\nwald_test is the chisquare test for each row, if value is observed frequency, then wald_test would be a chisquare gof test for difference between expected and observed frequency.\r\nThis would not make much sense on the observation level where we only have one observation in observed frequency. It might be useful a outlier diagnostic.\r\nHosmer-Lemeshow type test would be if we have aggregation over groups, a partition of observations.\r\nA similar application would be forecast evaluation or control chart outlier detection, or when observations represent means. In the latter case, we would need var_weights, representing the subsample size captured by one observation. (e.g, control charts with unequal batch sizes)\r\n\r\nMaybe I wait with this until a have a more useful application.\r\nThe application I started with this now was margeff at several exog, `_derivative_exog(p, ex)`. \r\nThat's useful for `se_vectorized`\r\nwald_test could be checking whether some subset of margeff is zero, but again, testing on averages instead of single observations would be more appropriate.\r\nStill need test for linear combination, e.g. diff between two margeff is zero.\r\n\r\n","aside: NonlinearDeltaCov does not cache anything. All \"attributes\" are methods and require `()`.\r\nDo we currently return a NonlinearDeltaCov instance to the user in public functions or methods?\r\nI guess not.\r\nget_prediction delegates to NonlinearDeltaCov, but returns `PredictionResultsDelta` to user.\r\n\r\nPredictionResultsDelta has the same shape mismatch problems as NonlinearDeltaCov for 2-dim return of the nonlinear prediction function (e.g. vectorized which=\"prob\")\r\nHowever, that should be easy to fix because it has no `wald_test`, i.e. it only has single hypothesis t_test, AFAICS.","tranpose of `var` if ndim=2 added in #8889\r\nno specific unit tests AFAIR"],"labels":["comp-genmod","comp-discrete","topic-predict","topic-post_estim"]},{"title":"ENH\/REF\/BUG: negative df_resid with freq_weights, negative scale","body":"first example from\r\nhttps:\/\/stackoverflow.com\/questions\/76292524\/different-weighted-linear-regression-results-for-r-and-python-bug-or-just-diffe\r\n\r\nI was trying to use (var) weights as freq_weights but weights are small so that df_resid is negative.\r\nThis results in negative scale for GLM gaussian with pearson scale, and nan bse and inference in summary.\r\nThe same nan bse happens with `cov_type=\"HC0\"`, but I have not looked at why that happens.\r\n\r\nI guess this cannot happen with var_weights and pinv, because df_model is based on rank which cannot be larger than nobs.\r\n\r\nWhat do we do in this case?\r\nraise, warn ?\r\n\r\nRelated:\r\nAFAIR, we don't explicitly rule out negative weights. Those might raise an exception in some computation, but not intentionally.\r\n \r\n\r\n","comments":[],"labels":["type-bug","type-enh","comp-genmod","topic-weights"]},{"title":"ENH: hypothesis tests based on cov, hypothesis\/contrast class","body":"I have commented on this several times, but don't find a specific issue.\r\n\r\nCurrently wald tests and associated cov_params are integrated into the results classes. They cannot be reused outside of models.\r\n\r\nWe need a class that can compute hypothesis tests and other inference like conf_int based on mean of effects and their cov.\r\nThis will provide t_test and wald_test for, e.g., postestimation results, for which we compute the expected value and a (possibly vectorized) multivariate cov_effect.\r\n\r\nAFAIR, there are applications for comparing predictions.\r\n\r\nNonlinearDeltaCov is similar already, but it is for a given nonlinear, multivalued or vectorized function, and targeted to the current usecase of nonlinear wald tests and get_prediction.\r\n\r\nWhat we need is a class (maybe Mixin class) that includes the functionality of LikelihoodModelResults t_test and wald_test, and conf_int, i.e. for linear constraints on effects that might be based on a nonlinear, multivalued function.\r\n\r\nNot clear yet, vectorization:\r\nAFAICS, NonlinearDeltaCov, has vectorized or joint hypothesis but not both, and only for equal to value hypotheses but not for arbitrary linear constraints (contrasts) like testing that two `effects` are equal.\r\nt_test and wald_test are similar in that the first is vectorized for individual hypothesis (based on `se` and not on joint cov) and the second handles joint hypothesis based on cov.\r\n\r\nFull vectorization would have a 2dim array of effects, rows are independent cases, columns represent a multivariate effect.\r\nIf cov(_effect) is common to all rows, then vectorization should be relatively straightforward (last axis is multivariate).\r\nIf covariance differs across rows, then it gets messy, more memory and joint hypothesis tests require vectorized linear algebra (vectorized quadratic forms).\r\n\r\n\r\nrelated example\r\nTreatmentEffectResults subclasses ContrastResults which essentially presents `t_test` results.\r\n(however, \"effects\" in this case are linear dependent, i.e. we have 3 vectorized hypothesis on 2 underlying effects (ATE and POM0))\r\n\r\npossible usecase to check:\r\nget_prediction with multi-valued prediction, e.g. `which=\"prob\"`\r\n\r\nGMM with moment conditions for extra effects, and we want an inference class for the extra effects (base on estimated expected value an submatrix of cov_params)\r\n\r\nlinear constrast of two predictions: current `_wald_nonlinear` can compute effect and hypothesis for one effect, eg. difference between the two predictions, but does not compute the joint cov. \r\n(This sounds a bit similar to treatment effect case, but for predictions and not for all extra moment conditions.)\r\n\r\n\r\nMaybe related\r\nvectorization of score test, i.e. we have \"joint\" versus \"separate\" option, but it computes the hypothesis test directly (like t_test and wald_test).\r\n\r\n\r\nAside: `paired` versus `independent`\r\nDoes correlation of \"effects\" correspond to \"paired\" version?  (for linear constrasts) (might require more assumptions on specification)\r\ne.g. #8784 evaluating out of sample predictions\r\nExtra in-sample effects in GMM are or may be correlated.\r\nrisk ratio: is it average of ratio or ratio of average  #2215  but that also has nonlinear effect E f(x, y) != f(E x, E y)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["comp-base","comp-stats","topic-predict","topic-diagnostic"]},{"title":"ENH: NegativeBinomial nbinom link, inverse_deriv2 is missing, check domain restrictions, clipping","body":"I was trying out an example with canonical nb link for NegativeBinomial family.\r\n\r\nIt converges for a simple 2-sample problem.\r\nBut fails in convergence or has almost inf cov_params in a more generic case with continuous exog.\r\n\r\ninverse_deriv2 is missing and should not be difficult to add\r\n\r\nformulas in NegativeBinomial link seem to be correct, manual checking and we have consistency unit tests \r\n\r\nA bit weird: linear predictor needs to be negative\r\nThe link function produces negative return.\r\n\r\nWorth a try: \r\nsee if we can get better starting values.\r\nShould we clip some more?\r\n#2208 issue for domain violations with some links\r\n\r\nmotivation:\r\nI was trying to see whether canonical link in negative binomial provides us with interesting features.\r\n\r\naside:\r\nWith negativebinomial link, the mean function depends on the dispersion parameter alpha, i.e. we have a parameterized link.\r\nSo, the mean parameters cannot be interpreted independently of the dispersion parameter with this link.\r\n \r\n","comments":[],"labels":["type-enh","comp-genmod"]},{"title":"DOC: check QMLE for GLM with var_weights","body":"https:\/\/stackoverflow.com\/questions\/76191135\/why-do-i-get-different-results-when-i-want-to-find-the-aic-of-my-model-in-python\/76202275#76202275\r\n\r\nexamples, relationship to check:\r\nI think the dispersion interpretation of `var_weights` should imply that we can estimate dispersed families like NegativeBinomial or BetaBinomial using the undispersed family with var_weights to account for excess dispersion.\r\n\r\njust an idea, needs checking, but it would make a good example in the docs.","comments":[],"labels":["comp-docs","comp-genmod"]},{"title":"ENH\/Design: too much inheritance, list for ok or for unsupported features","body":"Stata has some `ok` flags if some features are not allowed for a model.\r\n\r\nUntil now I was not convinced that we need it.\r\n\r\nHowever, if\/when we add more models as subclasses, then we might want to prohibit some post estimation methods by default.\r\ne.g. when we add subclasses of models based on mixins\r\nPenalizedXxx\r\nMixedXxx\r\n...\r\n\r\nThe current pattern assumes that some inherited feature either works generically or raises an exceptions somewhere. However, the inherited generic methods might not be generic enough to produce the correct results if they don't raise an exception.\r\n\r\nThis will be a bit easier once we have \"generic\" testing of post-estimation features that I started for recent reviews of get_distribution, get_influence, score_test, ....\r\n \r\n\r\nThis affects currently mainly discrete, especially count models. That's were we have a larger variety of models and the largest number of submodels.\r\n","comments":["implementation detail\r\n\r\nWe need to make sure that the flag is not inherited also. \r\nThe new default should be that a subclass gets the \"not permitted\" flags.\r\n\r\n(This might be similar to the `__init__` kwargs check where we condition that we are exactly in the current class.)","IMO this is a very bad idea.  The right way is to refactor the class structure, rather than invent a custom way to black list methods.  Worse then refactoring, but better than an OK list would be to simply have them raise NotImplementedError or some other custom error.","I never liked it much and it can get messy.\r\n\r\nHowever, we get potentially a large number of variations of Poisson as subclasses that reuse either CountResults or PoisssonResults. \r\nFor example we can have Mixin modifiers for most of the basic models.\r\nCurrently we have explicit submodel of count models for Poisson: zero-inflated, truncated, censored and hurdle.\r\nWe should get Penalized, Mixed versions and then models with endogeneity that have Poisson as outcome models.\r\n\r\nAn alternative, that would leave the super classes alone, would be to add a corresponding ResultsMixin that overrides all generic post-estimation methods with NotImplementedError.\r\nThat sounds more localized and easier to implement.\r\n"],"labels":["type-enh","design","comp-discrete"]},{"title":"ENH: variable addition autoregression test using score_tes","body":"Just a thought while looking at #8854\r\n\r\nWe could include past endog as regressors for a score_test that there is no autoregressive, dynamic effect.\r\n\r\nThis could be done with current score_test, except that we need to truncate initial observations, because past endog (or linpred) will not have the full nobs length. (The first observation(s) will not have past endog values)\r\n\r\nIf we drop initial observations with missing exog_extra, then the sample changes compared to the estimation sample. The score for the included parameters will not be exactly zero, and score_test (using all moment conditions as we do now) will have an additional term of dropping the score_obs of the dropped initial observation(s).\r\nThat will not be relevant in large sample, but could have non-negligible effect in small samples.\r\n\r\nScore_test under the no dynamic effect null should be correct, even with endogenous, pre-determined exog_extra, similar to score_test (pseudo-wald) test for endogeneity with null that regressor is not endogenous.\r\n\r\nAlternative: \r\nusers can drop initial observations in the model estimate and then use exog_extra with the full length of the estimation sample.\r\n\r\nAlternative 2:\r\nimpute the missing initial observations.\r\ne.g. impute `exog_extra[:k] = 0` which will have no contribution to the score_obs, (and AFAICS, have zero covariance  of score_obs_extra with included score_obs in contribution to cov(score))","comments":[],"labels":["type-enh","comp-base","comp-discrete","topic-diagnostic"]},{"title":"ENH: score_factor, generalized residual for ordered model","body":"ordered model has (private) `score_obs_` and score_factor computation w.r.t. to latent function, but is missing the explicit derivatives w.r.t. thresholds.\r\n\r\nuse cases for generalized residuals, score_factor for latent function\r\n\r\n- control function of ordered endogenous regressor\r\n- residual diagnostic\r\n- score_test for variable addition, I think this should also work, when we have score_obs for thresholds (or all params), possibly by numdiff.\r\n\r\nOne likely implementation issue:\r\nDo we need score_factor for all possible values of the endog variable? For residuals we do.\r\nCurrently, loglike and score, and `score_obs_`, compute only the contribution for the observed endog values (saves computation and memory) .\r\n\r\nreference for residual diagnostic (also has an explanation of generalized residuals as \"score_factor\":\r\n\r\nIannario, Maria, and Anna Clara Monti. \u201cGeneralized Residuals and Outlier Detection for Ordinal Data with Challenging Data Structures.\u201d Statistical Methods & Applications, February 28, 2023. https:\/\/doi.org\/10.1007\/s10260-023-00686-1.\r\n","comments":[],"labels":["type-enh","comp-discrete","topic-diagnostic"]},{"title":"DOC\/ENH\/REF:  review score_test","body":"docstring for score_test is not clear enough\r\n(Rereading it, I don't understand, remember some of the details)\r\n\r\ne.g. cov_type\r\nIt should be made clearer that this keyword is only used to override the cov_type defined in `fit`.\r\nAFAIR, I had added this for when we can have different small sample correction in Wald and score_tests for each of the sandwiches.\r\n\r\nreturn is tuple of numbers, returning a HolderTuple would be better\r\n\r\n`hypotheses=\"separate\"`: no unit tests AFAICS, \r\nreturns also the score_test for included variables which have statistic (approx) equal to zero.  It's confusing for testing zero constraints with exog_extra, but likely it's needed for other constraints on params (e.g. linear constraints).\r\nAlso, it looks like it's a \"partial\" test, robust to \"other\" nonzero alternative. (reference?)\r\n\r\nUsing an exog_extra that is already included raises SingularMatrix exception if hypotheses=\"joint\". \r\nBut if hypothesis is \"separate\", then the extra condition also shows up with statistic equal to zero.\r\n\r\ndocstring has no reference\r\nWe need at least reference to \"sandwich\" form of score-test\r\n\r\nNotes section: \r\nThe description in \"Status\" sounds too negative. What, besides \"separate\" is not verified. `cov_type` is waiting for enhancement.\r\n\r\nThe description of cov_type HC0 only mentions covariance matrix of the score. It's still the sandwich form, isn't it?\r\n\r\nAside: What's the status with k_extra? Does score_test support hypothesis for mean parameters in models that have extra params?\r\nIt looks like discrete test_predict.py has basic unit tests for NBP and GPP.\r\nZI count model and models in truncated inherit score_test, but I guess it will not work for those. \r\nBetaModel BetaResults does not have score_test. However it has unit test using the generic score_test function. Those only check that two versions of using score_test produce the same\/equivalent results, no outside verification.","comments":["Also I think I should \"publish\" the notebook that I used to run the MonteCarlo for score_test with nonrobust cov_type for misspecified models.\r\nThat would be helpful for users to better understand what cov_type does and why it's important also for score_tests.\r\n\r\n(Still unclear whether we want to add any MonteCarlo experiments to the example notebooks. Those can be time consuming.)\r\n","(initially posted to wrong issue #8853)\r\n\r\nThe story behind `score_test(exog_extra=..., hypotheses-\"separate\")`\r\n\r\nsuppose we have two exog_extra that are (strongly) correlated. One of them is really missing.\r\n\r\n- score_test joint will reject\r\n- separate score_tests on each extra will reject\r\n- score_test with hypotheses-\"separate\" might not reject either, in the presence of one the other one is essentially redundant. The \"better\" one will be rejected more (smaller p-value) (on average, repeated sampling).\r\n\r\nwald test can show the same pattern including either one or both extra variables.\r\n\r\nHowever, In nonlinear models like GLM, discrete, the weights\/variance will adjust if variables are actually added as in wald tests.\r\nSo, Wald test will be more informative than score_test if the extra effect of the added variable on variance and shape is helpful.\r\n(Score test for variable addition is often conservative.)\r\n\r\nI was playing with example adding exog_extra and exog_extra**2 which has the above pattern, or has it in weakened form. i.e. pvalues move in the direction of the pattern but do not necessarily cross the rejection threshold of 0.05.\r\n\r\n\r\nThe cases that I have seen in the literature are more for different diagnostics, e.g.\r\n- test no correlation while being robust to possible heteroscedasticity\r\n- test homoscedasticity while being robust to possible correlation\r\n\r\nor something similar.\r\n","score_test with ZeroInflatedPoisson fails with\r\n`AttributeError: 'ZeroInflatedPoisson' object has no attribute 'score_factor'`\r\n\r\nIIrc, k_extra handling in score_test for NBP, GPP, BetaModel assumes that mean parameters for mean exog are at the front of params, and score_test is only for exog_extra in the mean function.\r\nHowever, ZI and Hurdle have the Zero model first in params, and for score_factor (if those are column ordered in the same way as params.\r\n","Looks like score_test is running into problems with inconsistent score_factor return if there is more than one score_factor.\r\n\r\nGPP and NBP score_factor returns tuple, \r\nbut score_test seems to expect 2-dim array\r\nmost likely some refactoring victim\r\n\r\n**update, correction**\r\n\r\nNBP and GPP have unit tests for score_test\r\n\r\nHowever, the proper path in score_test is for the case that exog_extra is a tuple. It looks like that works correctly.\r\nIf exog_extra is not a tuple, then the code path in score_test assumes 1-d or 2-d array is returned by score_factor. That path is wrong for the two score factor case, and raises AttributeError tuple has no ndim.\r\n\r\nAlso, NBP and BetaModel have variable addition score_test for the dispersion\/precision distribution parameter.\r\nSo this works and is checked already.\r\n\r\nHowever, docstring does not mention anything about the tuple requirement for exog_extra in score_test.\r\nI only saw it after looking at the unit tests.\r\nAlso a informative exception would be helpful.\r\n \r\nAlso, we could catch a single exog_extra array (no tuple) for the case k_extra > 0, and convert it to a tuple before calling base score_test helper function, assuming that a single exog_extra array (1-d or 2-d) applies to the mean parameter and variable addition in the mean function.\r\n"],"labels":["comp-docs","comp-genmod","comp-discrete","topic-diagnostic"]},{"title":"ENH: outlier influence xy_outlier plot","body":"I don't find a SUMM or roadmap issue for outlier-influence\r\n\r\nJust a thought: add an xy-outlier plot with mahalanobis distance of exog on horizontal and some residual, y-outlier on vertical axis.\r\n\r\nThis works if we don't have the standard horizontal measure, and is more explicit that implicitly redefining attributes.\r\nAnd it's interesting in it's own.\r\n\r\n(in some cases we might have x-outliers that are not influential if they receive very low weight e.g. in IRLS\/WLS)\r\n\r\nIannario, Maria, and Anna Clara Monti. \u201cGeneralized Residuals and Outlier Detection for Ordinal Data with Challenging Data Structures.\u201d Statistical Methods & Applications, February 28, 2023. https:\/\/doi.org\/10.1007\/s10260-023-00686-1.\r\n","comments":[],"labels":["type-enh","topic-diagnostic"]},{"title":"BUG\/REF  _ConditionalModel.fit ignores kwargs, cov_type","body":"I looked by chance again at https:\/\/stats.stackexchange.com\/questions\/526459\/how-to-correct-conditional-poisson-standard-errors-for-over-dispersion\r\n\r\nIn the example the user uses `cov_type` in `fit` but it does not do anything.\r\nkwargs are swallowed, i.e. fit has kwargs but those are not used not transmitted to super().fit\r\n\r\nto the topic: \r\nI don't know whether cov_type, or excess-dispersion scale correction makes sense in conditional models. \r\nI guess that we need the Poisson or Logit (canonical link) assumption and cannot have QMLE. \r\nBut maybe we only need the  assumption on mean function and moment condition i.e. it might work for exp mean function with multiplicative random effect. (similar to GMMPoisson). (I never looked at that for conditional models.)\r\nThen we need at least HC cov_type for  non-negative continuous endog.\r\nquick check: Stata xtpoisson (FE conditional poisson) allows vce(robust) but not cluster robust.\r\n\r\naside: Currently sandwich cov would not be available because there is no score_obs. The score and loglike are computed based on groups and not based on observations.\r\nThere is score_grp, which could be used for \"between\" robustness correction for cov(score) \r\n","comments":["two references for conditional Poisson with over-dispersion and conditional or fixed effect negative binomial\r\n\r\nAW criticize HHG that their model for conditional negbin does not remove fixed effect in *standard* negbin with fixed effect in mean.\r\nAW beginning of section 5:\r\nThe add overdispersion correction to standard errors of \"fixed-effects Poisson\" which, AFAICS, refers to the conditional poisson version. i.e. simple cov_type correction for scale as in GLM with scale option \"x2\" or \"dev\" for scale. \r\nHowever, standard errors are still much smaller than NB2 with fixed effect dummies.\r\n\r\nAllison, Paul D., and Richard P. Waterman. \u201cFixed-Effects Negative Binomial Regression Models.\u201d Sociological Methodology 32 (2002): 247\u201365.\r\n\r\nHausman, Jerry, Bronwyn H. Hall, and Zvi Griliches. \u201cEconometric Models for Count Data with an Application to the Patents-R & D Relationship.\u201d Econometrica 52, no. 4 (July 1984): 909. https:\/\/doi.org\/10.2307\/1911191.\r\n"],"labels":["type-bug","comp-discrete"]},{"title":"REF: offset, exposure as attributes with None","body":"Currently, discrete models (at least some) delete or do not attach attribute if model arguments are None (default).\r\n\r\nThis requires hasattr and getattr workarounds, e.g.\r\n`offset=getattr(self, \"offset\", None),`\r\n\r\nIt would be easier and cleaner to always have the attribute, with None\r\n\r\nNone might be better than no-op offset=0 and exposure=1, to save on redundant computation. \r\nI'm not sure we do save the redundant computation and don't just set them to no-op values internally, e.g. we do the latter in predict.\r\n \r\n#3810 general issue for review and refactor of discrete\r\n","comments":[],"labels":["comp-genmod","comp-discrete","type-refactor"]},{"title":"TST\/BUG\/ENH: status discrete post-estimation","body":"checking discrete post estimation, including support for offset\r\nI'm writing generic checks that catch any errors to see what the current status is\r\n\r\nspecific issues:\r\n#8842","comments":["current results  (on a branch that has already some related changes)\r\n\r\n- all: \r\n  - no offset, exposure in get_margeff\r\n  - correctly warn for unavailable `__init__` kwarg \"junk\", \r\n    but will not warn or raise on offset or exposure if not supported\r\n- binary: \r\n  - no diagnostic\r\n- NegativeBinomial\r\n  - get_influence raises AttributeError, missing methods\r\n- zi\r\n  - get_margeff raises NotImplementedError\r\n- truncated\r\n  - offset, params change, but look wrong \r\n  - get_distribution raises ???, not available\r\n  - get_influence raises AttributeError, missing methods\r\n- hurdle \r\n  - offset ignored, params unchanged \r\n  - get_distribution raises ???, not available\r\n  - get_margeff raises IndexError\r\n  - get_influence raises NotImplementedError\r\n\r\nNote: when get_distribution is not available, then the model is not the dgp. \r\nThe dgp and its params are from an artificial model that does not correspond to any available discrete count model.\r\n\r\n```\r\n Logit {}\r\n    converged: True True\r\n    [-0.5  1.   0.5] params dgp\r\n    [-0.31126047  0.81765306  0.36472772] params\r\n    [-1.31126047  0.81765306  0.36472772] params offset\r\n    diff offset [2.22044605e-16 2.77555756e-16] [0.]\r\n    margeff: [0.20082789 0.08958262]\r\n    diagnostic: <class 'AttributeError'>\r\n    influence, d_params: [-9.68431536e-07  1.84944972e-06  1.19247563e-06]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n Probit {}\r\n    converged: True True\r\n    [-0.5  1.   0.5] params dgp\r\n    [-0.40374754  0.96297421  0.48035518] params\r\n    [-1.40374754  0.96297421  0.48035518] params offset\r\n    diff offset [-2.22044605e-16 -1.66533454e-16] [0.]\r\n    margeff: [0.36879235 0.18396268]\r\n    diagnostic: <class 'AttributeError'>\r\n    influence, d_params: [-4.95050683e-07  1.09523390e-06  6.94996101e-07]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n Poisson {}\r\n    converged: True True\r\n    [-0.5  1.   0.5] params dgp\r\n    [-0.5966774   1.13118825  0.52350121] params\r\n    [-1.5966774   1.13118825  0.52350121] params offset\r\n    diff offset [0. 0.] [0.]\r\n    margeff: [1.16059915 0.53711225]\r\n    probs: [0.37900174 0.34784721 0.17806409 0.06729852]\r\n    influence, d_params: [ 1.69115336e-07 -7.76804770e-07 -4.68508754e-07]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n NegativeBinomialP {}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.31618406  0.72974376  0.30881968  0.43147222] params\r\n    [-1.31618406  0.72974376  0.30881968  0.43147222] params offset\r\n    diff offset [-1.01030295e-14 -5.93969318e-15  4.16333634e-15] [6.21724894e-15]\r\n    margeff: [0.78626126 0.33273727]\r\n    probs: [0.42093709 0.29841222 0.15514412 0.07166227]\r\n    influence, d_params: [-2.22137987e-05  4.02871982e-05  3.34716988e-05 -2.22276746e-05]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n NegativeBinomialP {'p': 1}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.57952774  1.16892861  0.59503904  0.53041619] params\r\n    [-1.57952774  1.16892861  0.59503904  0.53041619] params offset\r\n    diff offset [0.00000000e+00 1.11022302e-16 5.55111512e-16] [-1.11022302e-16]\r\n    margeff: [1.24140263 0.63193169]\r\n    probs: [0.44305561 0.28076235 0.14757327 0.07116025]\r\n    influence, d_params: [-2.16329935e-05  2.99191722e-05  1.73466543e-05  1.56512976e-05]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n GeneralizedPoisson {}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.56598103  1.16520341  0.73247363  0.51134242] params\r\n    [-1.56598103  1.16520341  0.73247363  0.51134242] params offset\r\n    diff offset [-2.44249065e-15 -2.33146835e-15 -6.66133815e-16] [8.8817842e-16]\r\n    margeff: [1.23977644 0.77935194]\r\n    probs: [0.50549998 0.23834199 0.1188035  0.06180264]\r\n    influence, d_params: [-2.82552672e-05  3.29757999e-05  3.23055631e-05  7.36910215e-06]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n GeneralizedPoisson {'p': 2}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.52750618  1.10474728  0.64012619  0.50776928] params\r\n    [-1.52750618  1.10474728  0.64012619  0.50776928] params offset\r\n    diff offset [-1.12132525e-13 -7.39408534e-14 -8.88178420e-16] [8.65973959e-14]\r\n    margeff: [1.18313292 0.68554535]\r\n    probs: [0.51079444 0.2394377  0.11496893 0.05858472]\r\n    influence, d_params: [-2.68000461e-05  3.73070951e-05  1.79121985e-05  2.57691297e-07]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n NegativeBinomial {}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.31617035  0.72972289  0.30881246  0.43152336] params\r\n    [-1.31617035  0.72972289  0.30881246  0.43152336] params offset\r\n    diff offset [-1.99840144e-15  4.44089210e-16 -1.34336986e-14] [1.77635684e-15]\r\n    margeff: [0.78624009 0.33273006]\r\n    probs: [0.42094323 0.29840677 0.15514123 0.07166205]\r\n    influence: <class 'AttributeError'>\r\n'NegativeBinomial' object has no attribute '_deriv_score_obs_dendog'\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n NegativeBinomial {'loglike_method': 'nb1'}\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [-0.57950484  1.16887499  0.59501294  0.53036895] params\r\n    [-1.57950484  1.16887499  0.59501294  0.53036895] params offset\r\n    diff offset [ 0.00000000e+00 -2.22044605e-16  2.22044605e-16] [0.]\r\n    margeff: [1.24133448 0.63189826]\r\n    probs: [0.44305155 0.28076801 0.14757537 0.07115987]\r\n    influence: <class 'AttributeError'>\r\n'NegativeBinomial' object has no attribute '_deriv_score_obs_dendog'\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n NegativeBinomial {'loglike_method': 'geometric'}\r\n    converged: True True\r\n    [-0.5  1.   0.5] params dgp\r\n    [-0.51819179  1.06461757  0.5931414 ] params\r\n    [-1.51819179  1.06461757  0.5931414 ] params offset\r\n    diff offset [ 0.00000000e+00 -1.11022302e-16] [0.]\r\n    margeff: [1.12614734 0.62742213]\r\n    probs: [0.49654351 0.24483869 0.12323113 0.06325887]\r\n    influence: <class 'AttributeError'>\r\n'NegativeBinomial' object has no attribute '_deriv_score_obs_dendog'\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n ZeroInflatedPoisson {}\r\n    converged: True True\r\n    [-0.75 -0.5   1.    0.5 ] params dgp\r\n    [-0.7942453  -0.55652403  1.12264903  0.57558373] params\r\n    [-0.79427785 -1.55657337  1.1227167   0.57563021] params offset\r\n    diff offset [ 3.25538043e-05 -6.76708261e-05 -4.64791426e-05] [4.93408885e-05]\r\n    margeff: <class 'NotImplementedError'>\r\n    probs: [0.56366648 0.24078743 0.12653367 0.04861377]\r\n    influence, d_params: [ 1.39575005e-05  4.49962610e-05 -7.97035311e-05 -5.08659136e-05]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n ZeroInflatedGeneralizedPoisson {}\r\n...\\statsmodels\\statsmodels\\stats\\outliers_influence.py:427: UserWarning: hat matrix is not available, missing derivatives\r\n  warnings.warn(\"hat matrix is not available, missing derivatives\",\r\n    converged: True True\r\n    [-0.75 -0.5   1.    0.5   0.5 ] params dgp\r\n    [-0.26947956 -0.39672661  1.15422183  0.59248101  0.34866514] params\r\n    [-0.27021339 -1.39706875  1.15437011  0.59262855  0.34889582] params offset\r\n    diff offset [ 0.00073382 -0.00014828 -0.00014754 -0.00023069] [0.00034213]\r\n    margeff: <class 'NotImplementedError'>\r\n    probs: [0.67801824 0.14908797 0.07904535 0.04162461]\r\n    influence, d_params: [ 2.40411877e-04  9.45021972e-05 -3.57851368e-05 -4.87998063e-05\r\n -6.86492437e-05]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n ZeroInflatedGeneralizedPoisson {'p': 1}\r\n...\\statsmodels\\statsmodels\\stats\\outliers_influence.py:427: UserWarning: hat matrix is not available, missing derivatives\r\n  warnings.warn(\"hat matrix is not available, missing derivatives\",\r\n    converged: True True\r\n    [-0.75 -0.5   1.    0.5   0.5 ] params dgp\r\n    [-0.58190225 -0.56325951  1.24864666  0.7293058   0.53186791] params\r\n    [-0.58191079 -1.56324607  1.24863529  0.72928222  0.53186047] params offset\r\n    diff offset [8.54043607e-06 1.13678704e-05 2.35718592e-05 7.44461309e-06] [-1.34383216e-05]\r\n    margeff: <class 'NotImplementedError'>\r\n    probs: [0.67551169 0.15184288 0.0776705  0.04145741]\r\n    influence, d_params: [ 4.62596224e-05  2.41652399e-05 -1.35756675e-05 -9.57939526e-06\r\n  1.77695193e-06]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n ZeroInflatedNegativeBinomialP {}\r\n    converged: True True\r\n    [-0.75 -0.5   1.    0.5   0.5 ] params dgp\r\n    [-0.65822119 -0.47749001  1.0664021   0.58177084  0.47133374] params\r\n    [-0.65831203 -1.47739902  1.06622963  0.58162408  0.4713868 ] params offset\r\n    diff offset [ 9.08356844e-05  1.72465108e-04  1.46756256e-04 -5.30645896e-05] [-9.09978455e-05]\r\n    margeff: <class 'NotImplementedError'>\r\n...\\statsmodels\\statsmodels\\stats\\outliers_influence.py:427: UserWarning: hat matrix is not available, missing derivatives\r\n  warnings.warn(\"hat matrix is not available, missing derivatives\",\r\n...\\statsmodels\\statsmodels\\stats\\outliers_influence.py:427: UserWarning: hat matrix is not available, missing derivatives\r\n  warnings.warn(\"hat matrix is not available, missing derivatives\",\r\n    probs: [0.62087513 0.19143822 0.10004643 0.04769744]\r\n    influence, d_params: [ 1.24321015e-04 -1.86469659e-04  3.97367361e-04  3.34267773e-04\r\n -9.43838257e-05]\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n ZeroInflatedNegativeBinomialP {'p': 1}\r\n    converged: True True\r\n    [-0.75 -0.5   1.    0.5   0.5 ] params dgp\r\n    [-0.43308536 -0.39777319  1.08202364  0.6022051   0.42148571] params\r\n    [-0.43302846 -1.39780353  1.08210625  0.60226649  0.42145852] params offset\r\n    diff offset [-5.69073757e-05 -8.26096732e-05 -6.13878541e-05  2.71952194e-05] [3.03397413e-05]\r\n    margeff: <class 'NotImplementedError'>\r\n    probs: [0.62475635 0.18029692 0.10308613 0.05127988]\r\n    influence, d_params: [-3.34641048e-05  1.19599370e-04 -2.23950383e-04 -1.81430611e-04\r\n  2.11387510e-05]\r\n...\\statsmodels\\statsmodels\\stats\\outliers_influence.py:427: UserWarning: hat matrix is not available, missing derivatives\r\n  warnings.warn(\"hat matrix is not available, missing derivatives\",\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n TruncatedLFPoisson {}\r\n    get_distribution is not available\r\n    converged: True True\r\n    [-0.5  1.   0.5] params dgp\r\n    [ 1.41163652 -0.0423928  -0.10059286] params\r\n    [ 0.30897288 -0.04978527 -0.11813807] params offset\r\n    diff offset [0.00739246 0.0175452 ] [0.10266364]\r\n    margeff: [-0.17354205 -0.41179375]\r\n    probs: [0.         0.07406181 0.14807889 0.19770954]\r\n    influence: <class 'AttributeError'>\r\n'TruncatedLFPoisson' object has no attribute '_deriv_score_obs_dendog'\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n TruncatedLFNegativeBinomialP {}\r\n    get_distribution is not available\r\n    converged: True True\r\n    [-0.5  1.   0.5  0.5] params dgp\r\n    [ 1.21134289 -0.05735151 -0.13288405  0.80684029] params\r\n    [-0.62701513 -0.07427696 -0.17076708  2.68094417] params offset\r\n    diff offset [ 0.01692545  0.03788302 -1.87410388] [0.83835802]\r\n    margeff: [-0.23479929 -0.54403244]\r\n    probs: [0.         0.22773816 0.18464067 0.14439617]\r\n    influence: <class 'AttributeError'>\r\n'TruncatedLFNegativeBinomialP' object has no attribute '_deriv_score_obs_dendog'\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n\r\n HurdleCountModel {}\r\n    get_distribution is not available\r\n    converged: [True, True] [True, True]\r\n    [-0.75 -0.5   1.    0.5   0.5 ] params dgp\r\n    [ 0.54813772 -0.20437021 -0.11837038  1.41163652 -0.0423928  -0.10059286] params\r\n    [ 0.54813772 -0.20437021 -0.11837038  1.41163652 -0.0423928  -0.10059286] params offset\r\n    diff offset [0. 0. 0. 0. 0.] [-1.]\r\n    margeff: <class 'IndexError'>\r\n    probs: [0.20999788 0.0585124  0.11698469 0.1561894 ]\r\n    influence: <class 'NotImplementedError'>\r\n\r\n    kwargs init: [ValueWarning(\"unknown kwargs ['junk']\")]\r\n```"],"labels":["comp-discrete","type-test","topic-post_estim"]},{"title":"BUG: offset in truncated model raises valueerror","body":"using offset in TruncatedLFPoisson raises\r\n`ValueError: offset is not the same length as endog`\r\n\r\nsame should be in other truncated models and with exposure\r\n\r\n```\r\n        self.model_main = Poisson(self.endog, self.exog,\r\n                                  exposure=exposure,\r\n                                  offset=offset)\r\n```\r\nshould use self.exposure and self.offset which are the reduced arrays of nontruncated observations.\r\n\r\nThe truncated countmodels do not require that we get only nontruncated observations in endog.","comments":["no \"offset\" or \"exposure\" in test_truncated_model.py\"","actually the ValueError is good, The truncated models do not support offset, exposure in loglike and score.\r\nparameter estimates are wrong\r\n\r\nIn standalone usage (without hurdle), users could supply data without truncated observations.\r\nIn that case offset and exposure would have the correct size and no error would be raised.\r\n","similar:\r\nHurdleCountModel has offset and exposure arguments, but they are not used at all, they are not transmitted to the submodels.\r\nsimilar for `_RCensored`, offset is argument but not transmitted to model_main, Poisson, NBP, GPP\r\n\r\nAPI design:\r\ncurrently exog is the same in both submodels.\r\nWhen we support offset and exposure, then internal consistency would require that those also apply to both models.\r\nHowever, then we have an ambiguity with separate exog and offsets when `offset_zero = None`. \r\n\r\nIs the None default, \"use the same offset in both submodels\", or does it mean \"no offset in the zero model\".\r\n\r\nWe could make it conditional on `if exog_zero is None`. If true, then same exog, offset, exposure applies to same model.\r\nIf false, then offset and exposure are treated independently of main model, i.e. `offset_zero = None` means `offset_zero=0`.\r\n\r\n","related: ZeroInflation agreement is low comparing with offset=ones \r\n(reduces const estimate, but leaves slope parameters unchanged)\r\n\r\nother discrete models have much better agreement.\r\n\r\n```\r\n ZeroInflatedPoisson\r\n    [-0.7942453  -0.55652403  1.12264903  0.57558373] [-0.75 -0.5   1.    0.5 ] True\r\n    [-0.79427785 -1.55657337  1.1227167   0.57563021] [-0.75 -0.5   1.    0.5 ] True\r\n    diff offset [ 3.25538043e-05 -6.76708261e-05 -4.64791426e-05] [4.93408885e-05]\r\n```\r\n\r\nIs this a bug with small effect, a loss in precision, or a too large convergence tolerance?\r\n\r\nZIPoisson with nonzero offset has unit tests against Stata.\r\nand I don't see any problems when reading the code.\r\n\r\n**update**\r\nIt's convergence tolerance.\r\nWhen I make gtol (for default bfgs) more strict, params agree at around 1e-10 or 1e-11\r\n\r\n```\r\nres = model(y2, x, **kwds).fit(gtol=1e-8, disp=disp)\r\nres_offset = model(y2, x, offset=np.ones(nobs), **kwds).fit(gtol=1e-8, disp=disp)\r\n```\r\n\r\nAlso if I switch to method=\"newton\" (which is default for basic discrete models) agreement of params is at around 1e-12\r\n","If I mess up CountModel offset if not None, then the following unit tests fail.\r\nAll models `(*)` that don't show up here, do not have unit tests for offset.\r\n`(*)`  subclasses of CountModel that use self.offset from superclass in `__init__`  (some might still store offset directly)\r\n\r\n```\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1a::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1a::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1b::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1b::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1c::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained1c::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonNoConstrained::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonNoConstrained::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2a::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2a::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2b::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2b::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2c::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestPoissonConstrained2c::test_basic_method\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_constrained.py::TestGLMPoissonConstrained1b::test_compare_glm_poisson\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_count_model.py::TestZeroInflatedModel_offset::test_params\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_count_model.py::TestZeroInflatedModel_offset::test_conf_int\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_sandwich_cov.py::TestNegbinCluExposure::test_basic\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\discrete\\tests\\test_sandwich_cov.py::TestNegbinCluExposureFit::test_basic\r\n```"],"labels":["prio-high","type-bug","comp-discrete"]},{"title":"FAQ-D\/Maint\/ENH: sphinx directive versionadded, versionchanged","body":"https:\/\/www.sphinx-doc.org\/en\/master\/usage\/restructuredtext\/directives.html#directive-versionadded\r\n\r\nThis would be very helpful to add.\r\n(I often need to check PRs, blame or release notes to see when parts were added to statsmodels)\r\n\r\n```\r\n    Notes\r\n    -----\r\n    .. versionadded:: 0.14.0\r\n       some text if needed\r\n\r\n    .. versionchanged:: 2.5\r\n       The *spam* parameter.\r\n```","comments":[],"labels":["type-enh","comp-docs","maintenance"]},{"title":"BUG: discrete CountModel does not support offset, exposure in `_derivative_exog`","body":"**update**\r\nI had already opened to issues for that a long time ago\r\n#1818 #2164\r\n\r\ndiscrete CountModel has no offset exposure in `derivative_exog` and use `predict(params, exog)`\r\n```\r\n_derivative_exog(self, params, exog=None, transform=\"dydx\",\r\n                         dummy_idx=None, count_idx=None)\r\n```\r\n\r\n**update** `_derivative_predict` same problem missing offset and exposure\r\n\r\nIgnoring exposure could be justified by that margins are computed for one unit of exposure. But we might want to see exposure weighted average.\r\n**update** not a bug, it's documented in get_margeff\r\n`When using after Poisson, returns the expected number of events per  period, assuming that the model is loglinear.`\r\nThis means we cannot change the default, or it breaks backwards compatibility.\r\n\r\nIgnoring nonzero offset is a bug and cannot be justified.\r\ni.e. margeff results will change if the user had specified offset in the model after fixing this\r\n\r\n`BinaryModel. _derivative_exog` already includes `offset`\r\n**update** still a bug for \"ey\" which requires a second call to predict which doesn't have offset\r\n\r\n`GenericZeroInflated` method raises NotImplementedError\r\nno method in HurdleCountModel, and truncated models, maybe it's incorrectly inherited from CountModel\r\n\r\nMaybe additional bugs: \r\nHurdleCountResults does not overwrite any `get_xxx` results methods. Are inherited working but incorrectly?\r\n\r\n\r\n","comments":["Maybe this should still be fixed before the final 0.14 release.\r\n\r\n(more generic unit test for checking results methods, including inherited methods ?)","HurdleCountModel `get_margeff` raises IndexError, so we don't get incorrect results back\r\n\r\n```\r\nstatsmodels\\statsmodels\\discrete\\discrete_margins.py in get_margeff(self, at, method, atexog, dummy, count)\r\n    728                 # if eyex, then effects is truncated to be without extra params\r\n    729                 effects_idx = effects_idx[:len(effects)]\r\n--> 730                 self.margeff_cov = margeff_cov[effects_idx][:, effects_idx]\r\n    731                 self.margeff_se = margeff_se[effects_idx]\r\n    732                 self.margeff = effects[effects_idx]\r\n\r\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 5 but corresponding boolean dimension is 3\r\n```","most likely bugs:\r\n\r\nTruncateLFxxx does not raise in get_margeff, which is completely inherited. \r\nThey are likely wrong, or they are for untruncated prediction, `exp` mean function.\r\n","Fixing offset and exposure (or other extra arrays) also requires rewriting the margins classes in `statsmodels.discrete.discrete_margins`.\r\nWe need to add the extra predict arrays as arguments to all methods.\r\nfor example in all calls to the model derivative methods\r\n`effects = model._derivative_exog(params, exog, method,  dummy_idx, count_idx)`\r\nThose calls are also in the helper functions like `_margeff_cov_params_count`.\r\n\r\nThe only quicker fix (short of refactoring margins) is to raise a NotImplementedError \r\n`if getattr(self.model, \"offset\", None is not None`\r\n\r\nRelated:\r\nThe same missing offset in the model derivative functions will show up in get_prediction with margins derivative as target function, as I had used it in my examples and unit tests.\r\n\r\nIf I raise in results.get_margeff, then I could fix the model derivative functions to use offset (and possibly exposure) for use with get_prediction.\r\n\r\n","aside R package margins also ignores offset and prints some internal warning messages\r\n\r\n```\r\n1: In dydx.default(X[[i]], ...) :\r\n  Class of variable, offset, is unrecognized. Returning NA.\r\n```\r\n\r\nfrom vignette example, adding offset\r\n```\r\n> x <- glm(am ~ cyl + hp * wt, data = mtcars, family = binomial(link=\"probit\"), offset = cyl)\r\n> summary(margins(x, type = \"response\"))\r\n```"],"labels":["prio-high","type-bug","comp-discrete","topic-post_estim"]},{"title":"ENH: summary for multiple models","body":"We need helper class that returns str, repr, to_latex, ... for summaries of several models.\r\nThis would essentially just concatenate the formatted output, e.g. strings for printing.\r\nMaybe some subheaders or other adjustments to title to indicate the role of the model. \r\n\r\nUsecase 2-stage models: first stage model(s), second stage outcome model, and GMM model for combined cov_params computation.\r\nIn this case all models have the correct cov_params. (outcome model cov_params are corrected for first stage.)\r\n#8812, #7689\r\n\r\nalternative: \r\nresults method with option for which summary to return. The user will get only one summary at a time.\r\n\r\nWith the helper class, the user could just call something like `summary_combined`\r\n\r\nThis only applies if submodels are \"correct\" models, not if they are internal auxiliary models that don't have corrected attributes like cov_params.\r\n","comments":[],"labels":["type-enh","comp-io","comp-causal"]},{"title":"ENH: tools: computing in blocks for matrix algebra, cov for multivariateLS and multi-equation or multi-stage models","body":"Computing a sandwich cov_params for MultivariateLS and multi-equation models can be memory intensive.\r\n\r\nOne way to avoid larger arrays is to compute in blocks.\r\n`linearmodels` has some utility functions for computing a cov in blocks.\r\nhttps:\/\/github.com\/bashtage\/linearmodels\/blob\/main\/linearmodels\/system\/_utility.py\r\n\r\ncurrent use case IV and control functions for GMM #7689 #8812\r\nThe easiest which I currently use is to horizontally stack all moment conditions, score_obs and use GMM.\r\nThis works well if we have only e.g. two equation or two models plus maybe some extra moments as in treatment effect.\r\n\r\nHowever, if there are several endogenous explanatory variables and first stage is MultivariateLS, then using the full, stacked mom_cond will become memory intensive.\r\n\r\nFor multivariateLS with all identical exog, it will avoid keeping several copies of exog in memory if we loop over the cross equation cov(moments) and build cov in blocks.\r\n\r\nrelated computing the sandwiches after we have cov and hessian blocks\r\n#8803 which assumptions about information matrix equality\r\n#7814 and similar for using inverse of block matrices\r\n#8421 computing hessian in blocks or for extra moment conditions\r\n","comments":[],"labels":["type-enh","comp-tools","comp-multivariate","comp-causal"]},{"title":"ENH: control function for endogenous switching regession","body":"In endogenous switching regression the parameters change depending on the \"treatment\", i.e. exog interacts with \"treatment\".\r\n\r\nIf switching is \"endogenous\" we can have correlation of noise terms in first and second stage model.\r\nThis can be estimated by two-stage control function method but we have to get the appropriate control function for each regime.\r\n\r\nIt looks like that we can share most of the code with Two-stage control function, but we need to see what \"residual\" inclusion we need for these cases.\r\nThe advantage of switching regression compared to sample selection is that we observe the same observations in the first and second model.\r\n\r\nI didn't check the details yet.\r\nI looked through Chiburis and Lokshin 2007 which have a 2-stage method with ordered Probit. control function is diff in density divided by diff in cdf form normal, (similar to inverse Mills hazard ratio, but defined on diff for ordered segments.)\r\n\r\nMurtazashvili, Irina, and Jeffrey M. Wooldridge. \u201cA Control Function Approach to Estimating Switching Regression Models with Endogenous Explanatory Variables and Endogenous Switching.\u201d Journal of Econometrics, Endogeneity Problems in Econometrics, 190, no. 2 (February 1, 2016): 252\u201366. https:\/\/doi.org\/10.1016\/j.jeconom.2015.06.014.\r\n\r\nChiburis, Richard, and Michael Lokshin. \u201cMaximum Likelihood and Two-Step Estimation of an Ordered-Probit Selection Model.\u201d The Stata Journal 7, no. 2 (June 1, 2007): 167\u201382. https:\/\/doi.org\/10.1177\/1536867X0700700202.\r\n\r\nLokshin, Michael, and Zurab Sajaia. \u201cMaximum Likelihood Estimation of Endogenous Switching Regression Models.\u201d The Stata Journal 4, no. 3 (August 1, 2004): 282\u201389. https:\/\/doi.org\/10.1177\/1536867X0400400306.\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-causal"]},{"title":"ENH: tools discrete approximation to empirical distribution","body":"Integrating a function at many points with respect to the empirical distribution requires a large amount of computation.\r\n\r\nIs there a good way to get a discrete approximation with few points to an empirical distribution?\r\n\r\nexample usecase\r\nsmearing estimates for expectation of nonlinear function with empirical noise inside the function.\r\ncase of it is for marginal expected value (average structural function) in IV or control function approach to endogenous explanatory variables #8812.\r\nterm \"smearing\" came from prediction if endog is a nonlinear transformation #2791\r\n\r\nWe don't need good precision in tails, we only want to compute some expectations like mean of a nonlinear function with full support (no indicator function for tail probabilities).\r\n\r\nrelated\r\nfor mixed models with random effects we can use either gauss-hermite quadrature for normal distribution or finite class random effect (where random effect is assumed to be a discrete, multinomial distribution).\r\nOr use halton or other quasi-random numbers for a specified mixture distribution family.\r\n\r\nAquick search found some articles, but I didn't check whether they can be used for this\r\n\r\nBarbiero, Alessandro, and Asmerilda Hitaj. \u201cDiscrete Approximations of Continuous Probability Distributions Obtained by Minimizing Cram\u00e9r-von Mises-Type Distances.\u201d Statistical Papers, September 23, 2022. https:\/\/doi.org\/10.1007\/s00362-022-01356-2.\r\n\r\nKeefer, Donald L., and Samuel E. Bodily. \u201cThree-Point Approximations for Continuous Random Variables.\u201d Management Science 29, no. 5 (1983): 595\u2013609.\r\n\r\nMiller, Allen C., and Thomas R. Rice. \u201cDiscrete Approximations of Probability Distributions.\u201d Management Science 29, no. 3 (1983): 352\u201362.\r\n","comments":["alternative to using directly the discrete distribution would be to use a vectorized integration of arrays from scipy, trapezoid or simpson.\r\n","A possible simple candidate, which can be refined to odd number of support points\r\n\r\nbracketed median BMd3, BMd5\r\nsplit in 3 or 5 or ... equal weight intervals (percentiles) and use median of interval as support point\r\n(described in first paragraph on p.9) \r\n\r\nHammond, Robert K., and J. Eric Bickel. \u201cReexamining Discrete Approximations to Continuous Distributions.\u201d Decision Analysis 10, no. 1 (March 2013): 6\u201325. https:\/\/doi.org\/10.1287\/deca.1120.0260.\r\n"],"labels":["comp-tools","topic-predict","comp-causal"]},{"title":"ENH: add GMMIVCF control function for endogenous explanatory variable","body":"getting started with #7689\r\nendogenous regressors in nonlinear models\r\n\r\ncurrently GLM, and only one endogenous regressor\r\n\r\nthis add `statsmodels.causal` directory","comments":["only 36 lines of code to implement control function for all GLM families, the advantage of reusable parts \ud83d\udc4d \r\n\r\nIf we allow for only one endogenous regressor, then we can easily add a family option for the 1st stage.\r\nI don't know yet if I will find another package to write a unit test against.\r\nIf we add inverse Mills ratio (hazard rate) to GLM-probit, then we can verify against methods for standard bivariate normal latent variables.\r\n(*) see below\r\n\r\nAdding `controlf_kwds` to `__init__` for options, like which \"resid\"???\r\n\r\nmultiple endogenous regressors:\r\nbesides MultivariateLS, we only have the option of adding several univariate estimators (vertical stacking will not add a correlation like in GLS). MultivariateLS params will also be the same as separate OLS, as long as we use the same instruments.\r\nIn this case, first stage parameter estimation will not take cross-correlation into account. \r\nHowever, the GMM cov_params will still take into account the correlation in cov(moments) across all first stage estimates.\r\n\r\n**update**\r\n(*)\r\n\r\nIn the test case we only have a binary 1st stage regressor (i.e. 2 sample case).\r\nIn this case, predicted probabilities are independent of link and also the same as linear probability model (OLS).\r\n\r\nSo using Logit as a first stage instead of OLS does not change the results of the second stage (last 3 parameters in test case).\r\n\r\n```\r\nres2.params - res.params\r\narray([-1.41142239e+00,  1.52702446e+00,  1.08890674e-12, -1.94577687e-12,\r\n        1.94377847e-12])\r\n\r\nres2.bse - res.bse\r\narray([ 1.87557947e-01,  2.56702075e-01,  1.78288273e-10, -2.90577562e-11,\r\n        7.99499356e-11])\r\n```\r\n\r\nsame if I use Probit as a first stage, GLM-binomial with probit link\r\nI'm a bit surprised that bse is also unchanged. I thought that maybe the cov(moments) would change and through that the cov_params, especially for non-canonical link.\r\n\r\n\r\n```\r\nres2.params - res.params\r\narray([-9.55567914e-01,  7.64154961e-01,  4.32986980e-14, -7.37188088e-14,\r\n        7.37188088e-14])\r\n\r\nres2.bse - res.bse\r\narray([ 9.26798281e-02,  1.27372806e-01,  2.76773049e-11, -3.19284266e-10,\r\n       -2.87747937e-10])\r\n```","CodeQL complains about `slice(...)`  is unhashable.  I silenced them as false positives. slice is used to index arrays","returning the second stage outcome model results, attached to res_gmm\r\n\r\ninside `fit`\r\n```\r\n        res_out = res_outcome._results  # res_outcome is wrapped results\r\n        res_out._cache = {}\r\n        k_out = len(res_outcome.params)\r\n        cov2 = res_gmm.cov_params()[-k_out:, -k_out:]\r\n        res_out.cov_params_default = cov2\r\n        res_out.normalized_cov_params = None\r\n```\r\n\r\nThis has the standard GLM model attached, predict is the standard GLM predict.\r\nThis treats the control function as just another `exog` variable.\r\n\r\nWe can compute conditional expectation for given unobserved random term (control function values), e.g. set them to zero for prediction ignoring the random effect.\r\nFor marginal expectation E(y | x) we would still need to integrate out the random effect. (ASF, average structural function in Wooldridge base on Blundell and Powell).\r\nWe need to either add something to predict of the 2nd stage model, or add additional methods to the second stage results instance.\r\n\r\nOne possibility to make this cleaner:\r\n\r\nAdd `results_class` attribute to GLM, and return a subclass of GLMResults that appropriately adjusts normalized cov_params, cov_params_default and adds control function specific methods (maybe with results mixin).\r\n","aside to the test case\r\nThe control function takes on only 4 values, because both eev and instrument are binary.\r\nThis will not be the case if any of those are continuous variables.\r\n\r\n```\r\nuni, uni_idx, uni_counts = np.unique(res_out.model.exog[:, -1], return_inverse=True, return_counts=True)\r\nuni, uni_counts\r\n(array([-0.69148936, -0.23584906,  0.30851064,  0.76415094]),\r\n array([29, 81, 65, 25], dtype=int64))\r\n```"],"labels":["type-enh","comp-causal"]},{"title":"ENH: array api, using computational backend other than numpy","body":"Other computational backends provide api similar to numpy, and computation can be dispatched in a compatible way.\r\n\r\nThis is a possibility to investigate, initially mainly for parts that are outside full models because those contain too much surrounding code that would need to change, e.g. base.data.\r\n\r\nmaybe that would also work for dask, e.g. #8629\r\n\r\n2 sklearn PRs:\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/22554\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/25956\r\n\r\nOne possibility to strip `base.data` handling is to create a model subclass that does not call `super().__init__`. But it would still need to add non-data initialization like df_xxx.\r\n","comments":[],"labels":["type-enh"]},{"title":"ENH: more robust norms, and supporting helper functions","body":"We could add more robust norms\r\nand helper functions like computing relative efficiency at normal distribution.\r\n\r\nMenezes et al has a long list of norms\r\n\r\nsee also https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8801#issuecomment-1510076913 and following\r\n\r\nMenezes, D. Q. F. de, D. M. Prata, A. R. Secchi, and J. C. Pinto. \u201cA Review on Robust M-Estimators for Regression Analysis.\u201d Computers & Chemical Engineering 147 (April 1, 2021): 107254. https:\/\/doi.org\/10.1016\/j.compchemeng.2021.107254.\r\n\r\nI think we don't need many similar norms. I guess they will not make much difference in the estimate.\r\nHowever, there should be a few that are useful and not too difficult to code.\r\ne.g. R normbase has more, but those require either numerical integration for the rho function or \"rational approximation\" to it.\r\nAlso Menezes et al looks like a good reference for the current norms. (Bughunting would have been easier with having it beforehand.)\r\n\r\naside:\r\nFor M-estimation based on moment conditions, we don't actually need the rho function (except that the current default convergence criterion is deviance which uses rho) \r\n","comments":["I want to add new norm similar to \"Asan\" in Menezes et al.\r\n(original articles for it are not very informative about parameterization)\r\n\r\npsi(x) = x * (1 - (x\/c)**4)**2\r\nweights(x) =  (1 - (x\/c)**4)**2\r\n\r\nsimilar to tukey bisquare, but with power 4 inside.\r\nsuggested name \"quartic\", referring to power 4.\r\nThis flattens the weight function around zero, so that points in the neighborhood of zero have larger weight than in bisquare.\r\n\r\n**update** https:\/\/en.wikipedia.org\/wiki\/Kernel_(statistics) uses \"quartic\" as first name for biweight. \\<end update\\>\r\n\r\nNote: there are similar norms and kernel functions, like tri-weight and tri-cube.\r\nHowever, increasing the outer power, e.g. x * (1 - (x\/c)**2)**3, does not flatten the function around zero, it mainly changes the slope part.\r\n\r\nplot of weights function with inner power 4 and outer power 2, 3, and 4:\r\nusing c=2\r\n\r\n![image](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/440735\/9e139017-ed38-4ec1-b46f-7311b7723914)\r\n\r\n**update**\r\n\r\nAn plot that shows that quartic has a larger interval close to least squares than bisquare, for the same rejection threshold c\r\n\r\n![image](https:\/\/github.com\/statsmodels\/statsmodels\/assets\/440735\/834b585b-db73-441c-a5d0-cbe862b53141)\r\n"],"labels":["type-enh","comp-robust"]},{"title":"DOC: docstrings in robust.norms, improve, reorganize","body":"The docstrings for methods in robust norm have the formulas in the `Returns` section.\r\n\r\nWould be better before Parameter section as extended description.\r\nAlso converting to `math` will look better if we can get table formatting of equations (align `for z ...` in piecewise functions)\r\n\r\n","comments":["I would be willing to work on this issue!","That would be great.\r\nIt's mostly rearranging the docstring and adding the math directive, and checking that docstring renders correctly.","Sounds good. I also noticed when I was looking at this code that there are some classes that are untested. Would you like me to attempt to add unit tests to the file test_norms.py?","which classes are untested?\r\n\r\nI just added unit tests for norms a few days ago, and there shouldn't be gaps left.","I saw some comments that said \"TODO: this is untested\" on the trimmedmean class, but I do see that there is an instance of trimmedmean in the test_norms.py file.","I forgot to look for outdated code comments when I added the unit tests.\r\nthose should be removed","What is the status on this issue, I'm willing to work on it if no one has picked it up.","There has not been any work done on this, AFAIR.\r\npull request would be welcome.\r\n","Hello! I wanted to mention that I'm working on this along with @sudo-CompSciGod. We should hopefully be able to get a pull request ready soon!"],"labels":["comp-docs","comp-robust","good first issue"]},{"title":"ENH: comparing params or effects across nested nonlinear models, Logit, ...","body":"I browsed several articles on this topic but just to get a basic overview\r\n\r\nAn omitted variable inside a nonlinear mean or inverse link function adds extra noise, so that variance of the true linear predictor increases. The additional noise shifts the mean (Jensen's inequality), similar to issue with control functions.\r\n\r\nIn binary models like Logit or Probit, the coefficients\/params are scaled, i.e. scale dependent. In nested models, extra terms will capture additional random fluctuation, change the underlying scale (variance of linear term), which then implies a different scaling of the parameters.\r\n\r\nSeveral articles show methods to work around this.\r\nThe main alternative is to look at \"margins\", marginal or partial effects instead of looking directly at params.\r\n\r\nTwo references, but AFAIR I skimmed some more\r\n\r\nWilliams, Richard, and Abigail Jorgensen. \u201cComparing Logit & Probit Coefficients between Nested Models.\u201d Social Science Research 109 (January 1, 2023): 102802. https:\/\/doi.org\/10.1016\/j.ssresearch.2022.102802.\r\n\r\nMize, Trenton D., Long Doan, and J. Scott Long. \u201cA General Framework for Comparing Predictions and Marginal Effects across Models.\u201d Sociological Methodology 49, no. 1 (August 1, 2019): 152\u201389. https:\/\/doi.org\/10.1177\/0081175019852763.\r\n\r\n(aside we don't have a generic \"post-estimation\" label, the main related label is diagnostic.)","comments":[],"labels":["type-enh","comp-genmod","comp-discrete","topic-diagnostic"]},{"title":"ENH: cov_type for nonlinear two-stage models","body":"Mainly parking a issue and references. I was just skimming parts.\r\n\r\nIn treatment effect we use GMM for cov_params which is a robust cov_type.\r\nIf we want nonrobust cov_type, then we need to skip some of the robust computations.\r\n\r\nBased on my skimming there are different versions of the two-stage cov_params, either nonrobust (correct specification) or robust (sandwiches for some misspecification):\r\n\r\n- Murphy-Topel seems to be OPG (*)\r\n- another version uses hessian\r\n- Terza article directly simplifies the computation exploiting the information matrix equality \r\n\r\nI don't know what (our) heckman uses.\r\n\r\nHole, Arne Risa. \u201cCalculating Murphy\u2013Topel Variance Estimates in Stata: A Simplified Procedure.\u201d The Stata Journal 6, no. 4 (November 1, 2006): 521\u201329. https:\/\/doi.org\/10.1177\/1536867X0600600405.\r\n\r\nPalmer, Tom M, Michael V Holmes, Brendan J Keating, and Nuala A Sheehan. \u201cCorrecting the Standard Errors of 2-Stage Residual Inclusion Estimators for Mendelian Randomization Studies.\u201d American Journal of Epidemiology 186, no. 9 (November 1, 2017): 1104\u201314. https:\/\/doi.org\/10.1093\/aje\/kwx175.\r\n\r\nTerza, Joseph V. \u201cSimpler Standard Errors for Two-Stage Optimization Estimators.\u201d The Stata Journal 16, no. 2 (June 1, 2016): 368\u201385. https:\/\/doi.org\/10.1177\/1536867X1601600206.\r\n\r\nNewey, Whitney K. \u201cA Method of Moments Interpretation of Sequential Estimators.\u201d Economics Letters 14, no. 2 (January 1, 1984): 201\u20136. https:\/\/doi.org\/10.1016\/0165-1765(84)90083-1.\r\nformula for method of moments, exactly identified GMM, using sandwiches for all parts.\r\n\r\n(not clear to me yet what we need)\r\n\r\n(*)\r\n**update**\r\nMurphy, Topel, section 5.1 two-step MLE\r\nequ. (29)  assumes and specifies information matrix equality, R_i is name for both.\r\nIn the following, they use `R_i`  and so do not specify whether OPG or hessian is used in their final formula equ. (34)\r\n(AFAICS, in equ (33) R still refers to negative hessian, i.e. second derivatives, and omega in equ (30) and (31) is cov(score), i.e. using R for opg.)\r\n\r\n\r\n ","comments":["We need helper functions, at least for Newey\/GMM and Murphy and Topel\r\n\r\nThere should be some overlap with statsmodels.stats._diagnostic_other, e.g conditional_moment_test_generic\r\n\r\nActually, I'm not sure we really need to use it. It's mainly computational if we have a large number of moment conditions.\r\nWe can just use the appropriate submatrix\/block of the joint cov_params instead of using partitioned matrix inverse.\r\n(one large matrix inverse instead of many computations with smaller matrices)\r\n"],"labels":["type-enh","topic-covtype","comp-causal"]},{"title":"ENH: specification test, test two non-nested nonlinear models, GLM, discrete ","body":"We have specification test for non-nested linear models, but not for nonlinear models like GLM and discrete models.\r\n\r\nrandom read (while taking a break)\r\n\r\nFahrmeir, Ludwig, and Gerhard Tutz. Multivariate Statistical Modelling Based on Generalized Linear Models. Springer Series in Statistics. New York, NY: Springer, 2001. https:\/\/doi.org\/10.1007\/978-1-4757-3454-6.\r\n\r\nsection 4.3.3. p. 167 Tests for non-nested hypotheses\r\nand especially subsection Generalized Wald and Score Tests.\r\n\r\nI need to check references for the details.\r\n\r\nmaybe\r\nsomething similar to reset test, adding a linear predictor of an alternative model, either wald or variable addition score test.\r\n(I guess we don't need a two-stage cov_params because under the null the added term is zero.\r\nTheory might be similar to residual inclusion score test for endogeneity)\r\n\r\n","comments":[],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"BUG: Allow string type for groups in NominalGEE","body":"The groups argument may be of any type.","comments":[],"labels":["type-enh","comp-genmod"]},{"title":"REF\/Maint: use pandas to_numpy instead of np.asarray","body":"It looks like pandas is by default not dtype compatible with numpy anymore  (if there have been `<nan>` in the dataframe or series.)\r\n\r\nasarray by default converts now in many cases to object arrays which are in most cases useless to statsmodels.\r\n\r\ncurrent base.data.PandasData uses asarray\r\n```\r\ndef _convert_endog_exog(self, endog, exog=None):\r\n        #TODO: remove this when we handle dtype systematically\r\n        endog = np.asarray(endog)\r\n        exog = exog if exog is None else np.asarray(exog)\r\n```\r\n\r\npandas docs are not clear enough to me about what's going on and what to do.\r\n(and I don't have pandas 2.0)\r\n\r\nhttps:\/\/hackmd.io\/@mntOORP3TCesJJyvg-IdFQ\/BJJrTQQMj\r\nsounds like `to_numpy` should work as long as there are no nan in the series, or dataframe\r\n\r\n`df.dropna().to_numpy()`  as default ?\r\n\r\ndtype conversion (from nullable to regular) will most likely require copying the data, so we don't get views on the underlying pandas data anymore.\r\n\r\n\r\n","comments":["to_numpy did not work, it's still a 'object' array\r\n\r\n```\r\nstatsmodels\/stats\/descriptivestats.py:398: in _mode\r\n    mode_res = stats.mode(ser.dropna().to_numpy(), keepdims=True)\r\n\/opt\/hostedtoolcache\/Python\/3.11.3\/x64\/lib\/python3.11\/site-packages\/scipy\/stats\/_axis_nan_policy.py:521: in axis_nan_policy_wrapper\r\n    res = hypotest_fun_out(*samples, **kwds)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\na = array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35,\r\n       37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69,\r\n       71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99],\r\n      dtype=object)\r\n```\r\n\r\npandas sucks :(","see fix in #8797 for possible solution"],"labels":["pandas-integration","type-refactor","maintenance"]},{"title":"TST: check and add unit tests for one parameter models","body":"see https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8780#issuecomment-1507017425\r\n\r\nAFAIK, we have almost no unit tests if len(params) is 1.\r\n\r\nThe smallest exog that I usually use has 2 columns, e.g. for 2 independent sample case.\r\n\r\nThere might be problems and bugs mainly in generic code, as in the approx_fprime case.\r\n","comments":["I would be willing to work on this issue if it is still available!","It's still available.\r\nHowever, it's not really clear where and how to add those, because we want some kind of parametrized unit tests or reuse existing unit tests.\r\n\r\nFor example,\r\nI think at test like this https:\/\/github.com\/josef-pkt\/statsmodels\/blob\/main\/statsmodels\/discrete\/tests\/test_predict.py#L348\r\ncould include smoke or shape tests for all `which` in `get_prediction(which=xxx)`\r\nWhere available model specific  `which` would have to be collected, for example in a dictionary, then we could just run the tests for all `which` in a loop.\r\n\r\naside: hurdle and truncated models are missing in that parametrized list of models.\r\n\r\n  ","You don't need to be assigned to an issue to open a pull request. We usually don't assign."],"labels":["type-test"]},{"title":"ENH: sandwich cov_type code without symmetry of hessian","body":"I never checked whether the generic sandwich code (based on score_obs\/momcond and hessian\/momcond_deriv is correct for asymmetric hessians.\r\nDo we use the correct transposition of the (inverse) hessian?\r\n\r\nI guess there are no unit test for that.\r\n\r\nWe get asymmetric \"hessian\" in GMM, conditional moment tests, GEE, RLM , ..., if those are based on moment conditions that do not come from a single objective function (like loglike).\r\ne.g. M-estimator that start with moment conditions and not we a single objective function. (nonlinear robust model with separately specified moment condition for variance) \r\nAlso two-stage models have asymmetric derivatives of score_obs, Murphy\/Topel and general versions of it (*)\r\n\r\nCurrent GMM does not yet reuse the generic cov_type code. \r\nCurrent conditional moment tests (CMT) also don't use generic cov_type code.\r\nscore_test uses cov_params and symmetric hessian based on the unrestricted loglikelihood but the cov of the null hypothesis restrictions is based on the CMT code. \r\n\r\n\r\n(*)\r\nZero-inflated models are not two-stage model, there is one loglikelihood function for both components.\r\nHurdle model is currently implemented with two separate models, but parameters are independent across submodels, hessian and cov(score) are block-diagonal.\r\n","comments":["code looks correct, just needs checking that there are unit tests\r\n`statsmodels\\stats\\tests\\test_sandwich.py` (which does not have many unit test) does not fail when I remove the transpose. All (or almost all) applications including their unit tests will be for symmetric hessian.\r\n\r\nall sandwiches are computed using `_HCCM2` which has the correct transpose\r\n\r\n```\r\ndef _HCCM2(hessian_inv, scale):\r\n\r\n    if scale.ndim == 1:\r\n        scale = scale[:,None]\r\n\r\n    xxi = hessian_inv\r\n    H = np.dot(np.dot(xxi, scale), xxi.T)\r\n    return H\r\n```"],"labels":["comp-base","comp-stats","type-test","topic-covtype"]},{"title":"ENH: generic helper method for in-sample average effect with moment correlation","body":"main related issue #7168\r\nthis issue is for an implementation for parts of it\r\n\r\nsimilar to margeff and get_prediction\r\n\r\nIf we want to compute an in-sample average effect, then we should include the correlation of the individual effect with the moment conditions, score_obs,, of the model.\r\n\r\nWe do this in treatment effect through GMM, but we can add this more generically for individual models.\r\nAnother case where we already have this is test_chisquare_prob in count diagnostic. Currently, that uses only OPG instead of derivatives of moment conditions.\r\n\r\nProposal is a new helper function similar to get_margeff and `get_prediction(..., average=True, agg_weights=...)`, but instead of just using the delta method, we take correlation of effect and score_obs into account.\r\nWe could add it as an option to the currrent get_margeff, get_prediction, ... methods, but even then we still need the generic helper function to support it.\r\n\r\nThis is only for in-sample or a subsample of it (sample with weights in {0, 1}), so the out of sample part of get_prediction would not apply.\r\n\r\nAs a complication: cov_type\r\nAFAIU, we would have to go through the sandwich robust cov_type computation for the joined moment conditions (effect, score_obs)\r\nbased on what cov_type was specified in `fit`.\r\nThis will be easy to add, if we have the concatenated hessian, current cov_type allow generic (score_obs, hessian) as argument.\r\nCurrent test_chisquare_prob does not use the cov_type, only OPG, i.e. a nonrobust cov_type, is available.\r\nDelta method as in current get_margeff and get_prediction always uses the cov_params which is based on the selected cov_type.\r\n\r\nseveral related issues for pieces of this and for specific cases for which we could reuse this\r\n#8767\r\n#8745 \r\n#8421\r\n...\r\n\r\nI think we will need to support more than one average effect, a 1-dim multivariate effect for each observation, so we get the joint covariance of those effects and can run hypotheses test on them. (similar to ATE, POM1, POM2)\r\n\r\nAside: we still need more generic joint effects, joint covariance for wald tests and for get_prediction.\r\nWe still don't have separate helper methods for testing parameter restrictions given only params and cov_params (and use_t, df_resid), It's build into LikelihoodModelResults (cov_params and wald test methods). ContrastResults is separate and can be reused.\r\n\r\n","comments":[],"labels":["comp-base","comp-causal","topic-post_estim"]},{"title":"ENH: out-of-sample prediction, forecast comparison, multiplicity correction (paired t-test)","body":"**edit** I found an earlier issue #2291 closing that as duplicate with less information)\r\n\r\nIn a twitter message, a user of statsmodels mentioned using tukey-hsd to compare models on test data (out-of-sample)\r\n\r\ntukey-hsd is inappropriate for this because it assumes samples are independent.\r\n\r\n\r\nrelated #8216 suest, when we take into account that models are estimated on the same data, and compare in-sample statistics.\r\n\r\nAFAICS\r\nThe Diebold-Mariano forecast comparison test, tests the difference of prediction errors. This would be similar to a \"paired\" t-test (for two correlated samples), while tukey-hsd is based on standard independent sample t-test.\r\n\r\nSo one possibilty might be to use a paired t-test and correct for multiple testing using one of the p-value correction methods.\r\n\r\nI never looked closely at Diebold-Mariano or similar forecast comparison tests. \r\n\r\nAn updated perspective on the DM test:\r\n\r\nDiebold, Francis X. \u201cComparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use and Abuse of Diebold\u2013Mariano Tests.\u201d Journal of Business & Economic Statistics 33, no. 1 (January 2, 2015): 1\u20131. https:\/\/doi.org\/10.1080\/07350015.2014.983236.\r\n\r\n\r\npairwise paired t-test sound like a useful function for statsmodels. \r\nIt does not specify and estimate the correlation, so parametric p-value correction as in tukey-hsd will not be available. The p-value correction (for I guess positive correlated samples) should still be valid.\r\n\r\n\r\nOne difference between time series and cross-section data should be that in the latter case we do not have to worry about serial correlation, it's assumed away. We don't need HAC or similar robust standard errors for serially correlated predictions or prediction errors.\r\n\r\naside: quick look at articles citing Diebold 2015\r\nintersection-union test for multiple forecast comparisons\r\nSpreng, Lars, and Giovanni Urga. \u201cCombining P-Values for Multivariate Predictive Ability Testing.\u201d Journal of Business & Economic Statistics 0, no. 0 (April 19, 2022): 1\u201313. https:\/\/doi.org\/10.1080\/07350015.2022.2067545.\r\n\r\n","comments":["brief look at \r\n\r\nClark, Todd E., and Kenneth D. West. \u201cApproximately Normal Tests for Equal Predictive Accuracy in Nested Models.\u201d Journal of Econometrics, 50th Anniversary Econometric Institute, 138, no. 1 (May 1, 2007): 291\u2013311. https:\/\/doi.org\/10.1016\/j.jeconom.2006.05.023.\r\n\r\nthey use an adjustment to forecast MSE to account for noise added by redundant covariates.\r\nnon-standard distribution with recommended thresholds\r\n\r\n\r\na related idea:\r\nuse SUEST for combined cov_params to directly test out-of-sample forecast\/prediction eval_measure taking parameter uncertainty into account. #8216\r\nThen, forecast comparison would be just a `get_prediction` case for a prediction function (which) that is nonlinear in parameters. However, `eval_measures` also need the out-of-sample endog. #7971\r\nWe need eval_measures as a function of params, so we can use NonlinearDeltaCov `_wald_test_nonlinear`.\r\n\r\n(Also still missing: predict for two or more statistics with joint cov #3721)\r\n\r\n\r\n\r\n"],"labels":["type-enh","comp-stats","topic-predict"]},{"title":"BUG: linear RegressionModel.get_distribution is only for OLS, no weights\/heteroscedasticity","body":"AFAICS, looking at the code.\r\n\r\nfor WLS we should interpret weights as var_weights and use heteroscedastic scale in the distribution.\r\nI have not yet thought about the GLS case, maybe NotImplementedError. We don't have \"anti-whiten\" or coloring for generic GLS\r\n\r\nNot tried out yet.","comments":["also we most likely don't have unit tests for var_weights in GLM get_distribution\r\n\r\nbut should be correct, e.g. gaussian family uses `scale_n = scale \/ var_weights`, and then sqrt for scipy \"scale\""],"labels":["type-bug","comp-regression"]},{"title":"ENH: implementation multi-equation GMM with missing parts, sample selection","body":"currently GMM requires that we have moment conditions for all observations.\r\n\r\nHow can we handle the case when one equation uses only a subsample of a second equation, e.g. as in sample selection models?\r\n\r\nspecific case: what's the GMM equivalent of heckman sample selection model and how can we fit into GMM implementation?\r\n\r\nWe could aggregate the individual moment equations over different subsamples, but we also need the cov(mom_cond) for which we don't have a complete sample.\r\n\r\nrandom thought in context of #8745 \r\n\r\nmore general,\r\nHow can we use GMM if we want to model missing not at random or not completely at random, i.e. missingness is endogenous.?\r\n\r\n","comments":["aside:\r\nIf we want to predict POM for missing values, then we still need the covariates for the outcome model even for the \"missing\" observations. In this case only the outcome variable is missing, but not the explanatory variables.\r\n\r\nThis is not required in general, we estimate outcome model only for available observations (but with selection correction).\r\n\r\nSeems to be the same problem with all the treatment effect literature. What if POM depends on variables that are only observed for the treated?  Regression adjustment and doubly robust estimators will not be available.\r\n\r\nIf we treat different treatment plus treatment specific covariate as different treatments, then we don't have a overlap (I think).\r\nMaybe there is\r\ncore treatment is buying\/owning a car\r\noutcome is number of accidents\r\ncovariate if car is commuting time (low or high)\r\ncombined treatments: car + commuting time (no car, car with low commuting time, car with high commuting time)\r\n??? \r\n\r\nbut if you don't own a car, then you also cannot cause a car accident :), but you can be hit by a car.\r\n\r\n\r\n"],"labels":["comp-base","comp-causal"]},{"title":"ENH: GAM Mixin, penalized splines for other models than GLM","body":"I guess it would not be too difficult to split out penalized splines from GAM so that it can be used with other models that are not in the GLM families.\r\n\r\nGAM already uses the penalization mixin, so we mainly need the spline and penalization parts for the model and the extra post-estimation methods for the results classes.\r\n\r\n#7128\r\n\r\n\r\napplication could be where we want to add a spline as extra term\r\ne.g. control functions in #8745 to make the statistics more semi-parametric\r\n\r\nbut initial candidates would be the count models in discrete. That needs adjustment to GAM for the extra params (NBP, GPP or multipart models like hurdle or zero-inflated.)","comments":[],"labels":["type-enh","comp-discrete","topic-penalization","comp-causal"]},{"title":"ENH: recursive bivariate binary choice model, bivariate Probit and Copulas","body":"similar to #8765 but when \"treatment\" and outcome are binary.\r\n\r\nrecursive, triangular\r\ny1 = 1(\u03b1 y2 + x1 \u03b2 + \u03b51 \u2265 0)\r\ny2 = 1(x2 \u03b22 + \u03b52 \u2265 0)\r\n\u03b51, \u03b52 are correlated\r\n\r\nfull, parametric MLE but with copulas, so we have flexibility in correlation and CDF-link\r\n\r\nbivariate Probit is in Greene\r\n\r\ntwo articles for using copulas\r\n\r\nHasebe, Takuya. \u201cMarginal Effects of a Bivariate Binary Choice Model.\u201d Economics Letters 121, no. 2 (November 1, 2013): 298\u2013301. https:\/\/doi.org\/10.1016\/j.econlet.2013.08.028.\r\n\r\nWinkelmann, Rainer. \u201cCopula Bivariate Probit Models: With an Application to Medical Expenditures.\u201d Health Economics 21, no. 12 (2012): 1444\u201355. https:\/\/doi.org\/10.1002\/hec.1801.\r\n\r\n\r\nThis looks like the simplest case to get started with copulas for discrete random variables, especially in the regression model context.\r\n\r\n\r\n**update**\r\n\r\nan article that shows identification in this model\r\n\r\nHan, Sukjin, and Edward J. Vytlacil. \u201cIdentification in a Generalization of Bivariate Probit Models with Dummy Endogenous Regressors.\u201d Journal of Econometrics 199, no. 1 (July 1, 2017): 63\u201373. https:\/\/doi.org\/10.1016\/j.jeconom.2017.04.001.\r\n\r\n","comments":["Marra and Radice have R packages with copulas for bivariate binary and copula sample selection models\r\n\r\nMarra, Giampiero, and Rosalba Radice. \u201cA Joint Regression Modeling Framework for Analyzing Bivariate Binary Data in R:\u201d Dependence Modeling 5, no. 1 (December 20, 2017): 268\u201394. https:\/\/doi.org\/10.1515\/demo-2017-0016.\r\n"],"labels":["comp-distributions","type-enh","comp-causal"]},{"title":"ENH: (cluster) robust standard errors in treatment effects ATE","body":"(random thought. I didn't look at any details.)\r\n\r\nCan we add cov_type choices to TreatmentEffect estimation?\r\nI would be available through the GMM options. \r\n\r\ne.g. cluster robust for cluster randomized sampling\r\nin analogy to clustered survey sampling\r\n\r\nquick search:\r\n\r\nAbadie, Alberto, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. \u201cWhen Should You Adjust Standard Errors for Clustering?*.\u201d The Quarterly Journal of Economics 138, no. 1 (February 1, 2023): 1\u201335. https:\/\/doi.org\/10.1093\/qje\/qjac038.\r\n\r\n(surprise: The supporting code is in Python using sklearn)\r\n\r\nand google scholar finds articles on cluster randomized trials.\r\n","comments":[],"labels":["type-enh","topic-covtype","comp-treatment"]},{"title":"ENH\/design: variance of average effect with extra term, random or fixed exog","body":"\r\nWhen we have an average nonlinear effect  g_bar = average(g_i)  where g_i = g_i(params, exog) is a nonlinear function, then we compute the variance of g_bar using the delta method. \r\n\r\nit is possible to add the sample variance of g_i as an extra variance term to the variance of g_bar.\r\n\r\nI just saw this in Liu et al https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8762#issuecomment-1488877320 #7193\r\nand I was puzzled about their variance for average effects\\.\r\n\r\nsome articles on background\r\n\r\nGreene, Dowd, Norton use only the delta method term, working conditional on the exog in the sample (similar to fixed exog bootstrap)\r\nTerza argues in favor of including the extra sample variance of the effect with the interpretation that it comes from random sampling of exog from a population distribution.\r\n\r\nGreene, William H., Bryan E. Dowd, and Edward C. Norton. \u201cResponse to \u2018Inference Using Sample Means of Parametric Nonlinear Data Transformations.\u2019\u201d Health Services Research 51, no. 3 (2016): 1114\u201316. https:\/\/doi.org\/10.1111\/1475-6773.12493.\r\n\r\nTerza, Joseph V. \u201cInference Using Sample Means of Parametric Nonlinear Data Transformations.\u201d Health Services Research 51, no. 3 (June 2016): 1109\u201313. https:\/\/doi.org\/10.1111\/1475-6773.12494.\r\n\r\nDowd, Bryan E., William H. Greene, and Edward C. Norton. \u201cComputation of Standard Errors.\u201d Health Services Research 49, no. 2 (2014): 731\u201350. https:\/\/doi.org\/10.1111\/1475-6773.12122.\r\n\r\nI think so far we only include the delta method term (following Stata and other packages)\r\n\r\nI'm not sure how GMM in treatment effect for ATE, POM is treating this. We still use a sandwich covariance that includes the variance of the moment conditions (including definition of ate and pom as moment condition).\r\n(This might be some explanation for what puzzled me in treatment effect when some things were different in my and Stata's GMM implementation of it that did not match up with what I had expected when I checked other ways to compute it. I didn't investigate, I just wanted to match Stata results.)\r\n\r\n\r\nmaybe partially related:\r\nmeta-analysis: fixed scale versus WLS scale, usually dressed up as fixed effects versus random effects.\r\n\r\nSimilar issue: \r\nFixed design matrix or population distribution of exog in power and sample size computation.\r\nSo far I have mainly looked at fixed sample design (except for nobs or nobs-ratios). In the regression setting I have now seen also the case where randomness of sample exog based on a population distribution of exog is taken into account when computing (unconditional) power.\r\n\r\nmain question: Do we need to add an option for including the second term?\r\ne.g. \r\nget_prediction has an aggregate option but only includes delta method term. same for margins, AFAIU.\r\nextra average effects that we will want to include as post-estimation result, e.g. risk ratio, risk difference in binary models\r\n\r\nand document appropriately which version is implicitly or explicitly used.\r\ne.g. what does (nonparametrically identified) causal effect literature use?\r\n\r\n\r\n \r\n\r\n","comments":["update\r\nfootnote in Terza article refers to problem 12.17 of Wooldridge ([2010]\r\nThe problem uses an M-estimator with standard sandwich form.\r\nThe variance of the average effect includes the extra term with sample var of individual effects. It takes randomness of individual effects into account.\r\nThe proof is in the solution manual.\r\n\r\nI guess the same applies to GMM.\r\n\r\nPart c) of the question refers to evaluating the average effect out of sample, i.e. \"predict\" exog are the exog used in the estimation. In this case, the extra term is just adding the variance of the \"out-of-sample individual effects\"\r\nIn part a) the extra moment condition is correlated with the model moment conditions, and the extra term is not just the simple  variance of individual effects.\r\n\r\nMy guess this is out-of-sample prediction versus in-sample correlated moment equations. But in both cases, wooldridge takes the extra randomness\/heterogeneity in the individual effects into account. (where heterogeneity comes form different exog across individuals)\r\n\r\nmaybe another related issue\r\nConditioning on sample exog, treats the sample as the population, similar to survey method when we \"sample\" the entire population. The mean or total of a population does not have a variance, it's just a number.\r\nUnconditional, we have random exog drawn from a super population, and we want to compute the average effect for the superpopulation.\r\n(But my knowledge of survey methods is \"vague\".)\r\n\r\nWe have related issues for average effects for out-of-sample (predict) versus in-sample (GMM).\r\n#8389, ....\r\nBut that should be a bit orthogonal to whether we compute averages conditional on (given) exogs or unconditional.\r\n\r\nAFAIU, ATE is always unconditional, ATT, AT for subpopulation is for the subpopulation (exog) distribution but not conditioning on the specific exog values.\r\n\r\nreading the statements in the problem 12.17 more carefully, it states (with different notation)\r\nexog is random variable exog w, and\r\nwe want to estimate unconditional effect `E(g(w, params))`  (expectation both w.r.t w and params )\r\n\r\nGreene, Stata and, so far, statsmodels have `E(g(w, params) | W)`\r\n\r\nand another update\r\nconsequence for predict:\r\n\r\nvar(average_effect for a subpopulation given by x) = var(g_i) + delta method\r\n\r\nhowever, predict exog might not be the correct sample to compute var(g_i). It will only be correct of the exog are a random draw from the appropriate exog population.\r\nThat is we would have to estimate var(g_i) \"in-sample\" and use it as a parameter in predict.\r\n\r\nThis is similar to variance for prediction interval (se_obs) where we have an additional var term for the individual randomness.\r\n\r\ne.g. in a control chart, we would have to take into account what the individual heterogeneity would be in a new control sample, but how large the heterogeneity is has been estimated with a first stage training sample.\r\n(an example might be to control some nonlinear statistic that is averaged over several observation points.)\r\n","naming problem\r\n\r\n- `margeff` marginal or partial effect only takes cov_params into account (\"overall\" or average=True)\r\n- treatment effect, ATE, average causal effect, partial effect in Wooldridge style, ... takes (in-sample) heterogeneity into account (unconditional, sample population average.\r\n\r\nBoth are average partial effects.\r\nDo we use two different names? which? \r\nHow do we distinguish the two cases?  (Users might not be aware of the distinction, as I wasn't)\r\nCan we rely on that the two effects are computed in different places, function, methods so that it's \"obvious\"?\r\n\r\nalso:\r\nThe first corresponds to `predict`, the second to extra (in-sample) moment condition (GMM) (or effect parameter estimate).\r\n\r\n"],"labels":["type-enh","comp-base","comp-discrete","comp-stats","topic-post_estim"]},{"title":"ENH: sample selection with copulas","body":"This sounds like another interesting and useful application to get started with copula modelling\r\nrelated #8032 sample selection without gaussian assumption\r\n\r\nSmith, Murray D. \u201cModelling Sample Selection Using Archimedean Copulas.\u201d The Econometrics Journal 6, no. 1 (June 1, 2003): 99\u2013123. https:\/\/doi.org\/10.1111\/1368-423X.00101.\r\n\r\nLiu, Ruixuan, and Zhengfei Yu. \u201cSample Selection Models with Monotone Control Functions.\u201d Journal of Econometrics 226, no. 2 (February 1, 2022): 321\u201342. https:\/\/doi.org\/10.1016\/j.jeconom.2021.01.010.\r\n\r\nWojty\u015b, Ma\u0142gorzata, Giampiero Marra, and Rosalba Radice. \u201cCopula Based Generalized Additive Models for Location, Scale and Shape with Non-Random Sample Selection.\u201d Computational Statistics & Data Analysis 127 (November 1, 2018): 1\u201314. https:\/\/doi.org\/10.1016\/j.csda.2018.05.001.\r\n\r\n\r\naside:\r\nSample selection models (heckman and similar) have correlated unobserved heterogeneity in the linear predictor, latent index function.\r\nThe marginal distribution is then not e.g. Poisson anymore, it's a poisson mixture with overdispersion.\r\n\r\nThe standard bivariate copula model specifies the marginal distribution and correlation directly, e.g. the marginal distribution would still be Poisson. \r\nHowever, we are interested in the conditional and marginal mean which would still need to depend on and vary with the correlation.\r\n\r\n(It's not clear to me whether the second version can be used to model endogenous explanatory variables, i.e. distribution parameter of the second conditional distribution depends directly on the first.\r\nSounds more like as if we want to model the reduced form, i.e. copula\/correlation determines distribution conditional on the first random variable, and regression model includes only exogenous variables.\r\n)\r\n\r\ntools: need to finish code for conditional distribution in copulas and copula distributions, pdf(y1 | y0) and cdf(y1 | y0)\r\n","comments":[],"labels":["comp-distributions","type-enh","comp-causal"]},{"title":"ENH: predicted mean, conditional expectation in nonlinear models with random term in linpred","body":"e.g Logit( x b + e) where e is an unobserved random term\r\nJensen's inequality bites again.\r\n\r\nE(y | x) is not Logit(x b)  (or expit ?)\r\n\r\nWooldridge on control function explains average structural function, ASF, that marginalizes\/integrates out the `e` random variable, implemented by averaging over the sample `e`.\r\nThis looks similar to the \"smearing\" mean adjustment when endog is a nonlinearly, e.g. log, transformed y. #6611\r\n\r\nWooldridge, Jeffrey M. \u201cControl Function Methods in Applied Econometrics.\u201d The Journal of Human Resources 50, no. 2 (2015): 420\u201345.\r\n\r\nAFAIU, the same applies to random\/mixed effects nonlinear models, GLM, discrete.\r\nAFAIR, I ran into this before in the random effect context, or as cause of overdispersion.\r\n\r\nWhat should predict, get_prediction return by default?\r\nWhat other statistics do we need as option for predict?\r\n\r\nIn the simple case of nonlinear transformation, the interpretation is in terms of median, quantiles only need monotonicity but no curvature adjustment. That's the interpretation that I prefer for nonlinear models, asymmetric distributions that don't have a mean parameterization, e.g. AFT accelerated failure time models for survival.\r\n\r\nIn tsa with log transformed endog, ignoring the mean correction often produces better forecasts than the smearing estimate, AFAIR some literature\r\n\r\n\r\nRelated:\r\nIn models like Probit, the additional randomness also changes the scale of the distribution of the latent random variable.\r\nThe estimated parameters are scaled parameters. b \/ s\r\nIn those cases it is important to look at marginal, partial effects. d E(y | x) \/ d x\r\n(similar applies to heteroscedastic Logit, Probit. However, there we are fully parametric.)\r\n\r\n\r\nsee also https:\/\/github.com\/statsmodels\/statsmodels\/issues\/7719#issuecomment-1476843407\r\n","comments":["example for fully parametric model with endogeneity (not control function)\r\n\r\nendogenous participation in ZIP, ZINB and hurdle models,\r\nerror terms in zero model and count model are correlated, correlated bivariate normal random terms in linear predictors.\r\n\r\nboth predicted mean and marginal\/partial effects integrate out the error term, similar to random effect in Poisson.\r\n(MixedMixin has some tools for integration like this, gauss-hermite quadrature, of each observation, not a random effect in a cluster or panel. I had used this for an overdispersed poisson model, poisson-normal mixture or poisson-gamma mixture as in NB)\r\n\r\nLiu, Xueyan, Wencong Chen, Tian Chen, Hui Zhang, and Bo Zhang. \u201cMarginal Effects and Incremental Effects in Two-Part Models for Endogenous Healthcare Utilization in Health Services Research.\u201d Health Services and Outcomes Research Methodology 20, no. 2 (September 1, 2020): 111\u201339. https:\/\/doi.org\/10.1007\/s10742-020-00211-x.\r\n\r\n"],"labels":["type-enh","comp-genmod","comp-discrete","comp-causal"]},{"title":"ENH: Are there any options to allow something like a GLS or WLS in the Ordered Models?","body":"It seems that remedies for heteroscedasticity, such as modelling for the variances of the error using WLS or GLS, are not supported in the ordinal model module. Are there any solutions like `oglmx` package in R described by the following paper here?\r\nhttps:\/\/cran.r-project.org\/web\/packages\/oglmx\/vignettes\/oglmxVignette.pdf\r\nIt would be helpful if this could be incorporated into Python since this technique is also used by some studies like Hausman, Lo and MacKinlay (1992) and section 3.3 of *The Econometrics of Financial Markets*, Campbell, Lo and MacKinlay (1997). I would be more than grateful if someone could propose a solution to something like this in Python. I am also willing to submit a PR if necessary.","comments":["Thanks for the reference. I had not seen this before. I had only looked at other extensions of the ordered, ordinal models.\r\n\r\nBased on the likelihood function in the vignette, this is similar to heteroscedastic Logit or Probit.\r\nI don't find a specific issue for that https:\/\/github.com\/statsmodels\/statsmodels\/issues\/2179#issuecomment-1280023699 #2730\r\n\r\nTwo implementation problems in general (for full MLE)\r\n- models with heteroscedasticity requires a multi-link model #7793\r\n- is not linear in parameters in the index, linear predictor. (I don't see an PR for my draft version for nonlinear GLM, code in gist linked to in #2179)\r\n\r\n(multi-link models has stalled. I delayed for now, I had tried last year to get a numfocus small development grant for it but it was rejected)\r\n\r\nHowever,\r\nOrderedModel currently still relies much on generic code, it subclasses GenericLikelihoodModel and still uses numerical derivative because of the nonlinear transformation of thresholds. And we don't have yet the post-estimation extension like score_test, get_influence and similar that exploit the single link with linear predictor structure.  \r\nSo, I guess it would be relatively easy to add heteroscedasticity.\r\n\r\nWLS with estimated weights (HetGLS):\r\nI don't know if a two-stage or iterative version using var_weights would work.\r\nIn WLS\/GLM this relies on (expected, asymptotic) orthogonality of mean and var, i.e. we can take var_weights as fixed even if they have been estimated.\r\nI don't know what theoretical properties the ordered model has with respect to estimated heteroscedasticity. Since it modifies the latent mean function, mean and variance might not be orthogonal. In that case we could not take var_weights as fixed for inference, standard errors. \r\n\r\n\r\n\r\n","Stata has hetoprobit https:\/\/www.stata.com\/manuals\/rhetoprobit.pdf\r\n\r\nWilliams includes ordered logit in oglm https:\/\/www3.nd.edu\/~rwilliam\/oglm\/oglm_Stata.pdf\r\n(I only paid attention to his hetlogit or heteroscedastic binary models)\r\nWilliams has several articles and the main author for Stata packages in this area.\r\n\r\nR oglmx vignette does not seem to have a reference to an article specific to oglm.\r\n\r\nGreen, Hensher Primer has sections on heteroscedasticity for the various models\r\n`7.4 Heterogeneous scaling (heteroscedasticity) of random utility` for standard ordered model\r\nwith additional references\r\n\r\n","aside:\r\nmarginal or partial effect d E(y | x) \/ dx would have to take into account if the same x shows up in both mean and variance functions, because heteroscedasticity affects the mean function, I guess.\r\n","Thank you very much for your timely reply! It seems that `oglmx` in R and `hetoprobita` in Stata are exactly what I want. Would be willing to propose a simple solution based on current framework of `OrdinalModel` (as a subclass of `GenericLikelihoodModel`) since the estimation only requires a simple adjustment on the MLE. However, issues like statistical inference and marginal effects seem somewhat complex, and I am also trying to conduct further researches on them.","That would be great if you can work on this.\r\nMaybe starting as a subclass of OrderedModel will be the easiest.\r\n\r\nCurrently BetaModel is the only two-link models with modelled heteroscedasticity https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.othermod.betareg.BetaModel.html\r\nThat should work for the pattern for how to include a second link and second exog.\r\n\r\nWald inference for a full MLE model is inherited. This should work automatically once we have the hessian\/cov_params.\r\n\r\nsome details:\r\ndf_resid, df_model is a bit complicated in GenericLikelihoodModel and is not very robust for different cases. That needs most likely model specific adjustment.\r\n\r\nNot clear to me:\r\nCan the exog in the variance function include a constant?\r\nAFAIU, hetlogit and similar only need relative heteroscedasticity (like relative weights), the overall scale is not identified and fixed at 1. If it cannot include a constant for identification, then we need to use the same\/similar code that checks this for the current exog also for the second exog.\r\n\r\nI never looked at the details for that.\r\n","The advantage here is that I had looked several times at hetlogit and hetprobit and know the background pretty well, even though I did not pay a lot of attention to implementation details.\r\n\r\n`discrete` Logit and Probit have the disadvantage that the single link, linear predictor assumption is build into many methods. So, extending those will be much more work than extending OrderedModel.","Another implementation issue\r\n\r\nEventually OrderedModelHet cannot be a subclass of OrderedModel because we can more easily add extensions to the model without heteroscedasticity.\r\n\r\nOne possibility is to reverse the subclassing, making OrderedModelHet the superclass of OrderedModel. \r\n(The same happened when we started out with statsmodels and had OLS as the superclass of GLS and WLS, until we reversed the hierarchy to have proper inheritance from general to specific classes.)\r\n\r\nAnother possibility would be to create a common base class for both, but it looks to me that that makes less sense.\r\n","another possible issues (I just realized)\r\n\r\nWe might loose that monotonicity in the latent variable holds for all parameters, AFAICS.\r\nI guess it's similar to the generalized ordered model #8444, that the nonlinearity in the index\/linpred function might make integration limits for some exog and parameters decreasing.\r\nI have not looked at references yet, whether they point this out.\r\n\r\n`d\/dt ((a t)\/exp(b t)) = a e^(-b t) (1 - b t)`\r\n(wolfram alpha)\r\n\r\nI don't see monotonicity problems for this mentioned in Greene, Hensher Primer\r\n","Thank you for your kind reply and discussion! I have gone through section 7.4 of Green, Hensher Primer which discusses exactly this problem. AFAIC, The main concerns here to implement the OrderedModelHet seems to be the degrees of freedom, the marginal effects, and the monotonicity assumption.\r\n1. For df_model and df_resid. Currently I am not very sure what the degrees of freedom should be. It is likely that they should be redefined during initialization of the OrderedModelHet class since the GenericLikelihoodModel class does not account for extra variables. For example, if there are $k_1$ variables as the explanatory variables of the mean, and $k_2$ variables as those of the variance, what should the df_model be? Should it be $k_1+k_2+1$ (suppose that a constant is allowed in the variance explanation equation) or something else?\r\nBesides, the df problem also reminds of a possible chi-square test for the model with heteroscedasticity and a plain Ordered Model (also mentioned in section 7.4 of Green, Hensher Primer). This can also be implemented in this model.\r\n2. For marginal effects, it seems that the `score_obs_` is some method to compute the marginal effect for the OrderedModel? (not yet checked) The OrderedModelHet should contain something like that to compute marginal effects, or AME, etc., and this issue is also addressed in section 7.4 of Green, Hensher Primer.\r\n3. Monotonicity problems are something truly worth noting. In Green, Hensher Primer they suggested using a hierarchical model proposed by Greene (2007a) (see formula (7.31) in the book), but the book seems to use the original form for further estimation. Maybe I should implement a simple form by simply adjusting the MLE (since it is maybe a common choice) and further blend this model by allowing a hierarchical form? I am also unfamiliar with this hierarchical model and I am trying to get into it.","to 3:\r\nbut I don't see any explicit mentioning of the monotonicity problem in section 7.4. The long discussion of it is in for generalized logit\/probit in 7.2.. Equ. (7.31) still has the same pattern with variance in the denominator, it only enforces monotonicity in the numerator.\r\nMy guess is that non-monotonicity for heteroscedastic version will be at most a small problem with \"strange\" data.\r\n\r\nto 3:\r\ncurrently margins for OrderedModel is not yet available #8443. It needs some derivative methods to work.\r\nThe basic requirements for a model are loglike, predict and cov_params for wald inference. The rest can be added later.\r\n\r\n The extra here that we do not have in any existing models is that the mean (conditional expectation) also depends on the second link, i.e. on variance. So we will need some way for the user to indicate variables that are common in both exog.\r\n\r\nto 1: df_model should count only slope parameters, i.e. parameters that are not in the null model which has constants only.\r\nI think this is exog.shape(1) + exog_var.shape(1)\r\ndf_resid will be nobs minus number of all parameters (len(params))\r\nAFAIR, the simplest will be to start with setting df_model and df_resid at the end of `__init__`.\r\n\r\nTo no constant in exog_var.\r\nI'm pretty sure we cannot have a constant in the second exog. But that can be imposed after we have a working model.\r\nMy check for the ordered model was to add a constant and then check that hessian is singular up to approximation and numerical noise. (Theoretically, we should not be able to invert the hessian in the unidentified or singular case, but because of numerical derivatives, the approximation error might still produce a positive definite hessian.)\r\n\r\nAside:\r\nI have not seen, or did not pay attention to Greene's HOPIT model. \r\nI thought of doing something like that for the generalized ordered model, before now seeing a reference for it.\r\nThe disadvantage is that it does not directly nest the standard model without choice specific parameters besides threshold.\r\nIt changes the functional for of those exog to incremental exponential instead of piecewise linear.\r\nHowever, that's not directly related to adding heteroscedasticity.\r\n \r\n\r\n","Thank you for your correction. I may first complete this OrderedModelHet class as a subclass of OrderedModel (which is convenient at first to complete part of my research), and then I will consider your suggestions that OrderedModelHet should be the superclass before committing a PR.\r\nBesides, for the constant in the second exogs, I checked from the Vignette in `oglmx` package in R. This package actually ALLOWS a constant in the second exog (btw, it also allows constants in the main ordered model to determine the thresholds as some further assumptions). I am not sure about it, and there are also other forms of the second regression other than a exponential form (for example, in section 3.3 of *The Econometrics of Financial Markets*, Campbell uses an augmented linear model which sets the constant term at 1 and all the coefficients as a squared form to ensure that the regression gives non-negative results). The regression takes the form as $$\\sigma_m^2=1+\\sum_{i=1}^L \\gamma_{im}^2 X_{im}$$ Anyway, the constant term can be allowed\/disallowed by some programming check.\r\nAs for HOPIT model, it seems that they indeed do not nest with the standard model. This enhancement may be added by some extra classes instead of being a superclass of OrderedModelHet or OrderedModel.","For the constant in the current OrderedModel\r\n\r\nIn our parameterization the lowest threshold is also estimated, and then it becomes the implicit constant in the model and `exog` cannot have another constant.\r\nThe alternative parameterization is to set the lowest threshold to zero, and require a constant in the `exog`.\r\n\r\nI had chosen the first parameterization before realizing the effect on a constant in exog.\r\n\r\nI doubt that there is an easy different parameterization that allows constant in the exog_var.\r\n**update** top of page 9 in oglmx vignette: if constant in var function is allowed, then two thresholds need to be fixed. scale can estimated because of the restricted thresholds.\r\n(that looks to messy to add to the model)\r\n\r\nIn a version for WLS, I have seen the recommendation to use `exp( beta * log(x))`, i.e. taking log of x before using exp (log-link).\r\n\r\nThe form of\r\n\r\n> all the coefficients as a squared form to ensure that the regression gives non-negative results\r\n\r\nwill not enforce a non-negative variance if x can be negative.\r\nAdditionally, gamma = 0 is at the boundary of the parameter space which makes inference non-standard.\r\n","Yes, I forgot to mention that the formula was applied to some non-negative market index data in that book, thus this \"linear\" form could be applied to model the variances. I reconsider this problem carefully and agree that there should be no constant in the second regression if the link function takes the form of some exponential function. The guidance in `hetoprobit` package in Stata also prohibits the usage of a constant (on page 4 of https:\/\/www.stata.com\/manuals\/rhetoprobit.pdf). I think we may impose a constraint on that during implementation."],"labels":["type-enh","comp-discrete"]},{"title":"BUG\/REF ? kwargs in RegressionModel.fit OLS, ...","body":"It looks like we can still have misspelled, incorrect keywords in `fit` of linear regression models.\r\nhttps:\/\/stackoverflow.com\/questions\/75842157\/results-summary-reporting-z-instead-of-t-value-for-multiple-ols-regression-using\r\n\r\nAny `kwargs` are just added as attributes to the results instance.\r\n\r\nI don't know or don't remember what this is used for, maybe some subclasses use it.\r\n\r\nIf subclasses use it, then we have to make up a list of allowed keywords similar to what I did for `__init__` in models.","comments":["The kwargs have been added 13 years ago, but I don't see a specific reason\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/commit\/b6eaa4c4df6fb4db25e47ab8a5731e27737aaa1b#diff-4038c404a4188bbadc3f50dd9db62c486e4806cacda9df176ea7e8a2b53388f0","unit test fails if I raise a ValueError `if kwargs` in\r\n\r\n```\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_gretl.py::TestGLSARGretl::test_all\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_gretl.py::test_GLSARlag\r\nFAILED ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_regression.py::test_regularized_predict\r\nERROR ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_stata.py::TestGLSARCorc::test_params_table\r\nERROR ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_stata.py::TestGLSARCorc::test_predicted\r\nERROR ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_stata.py::TestGLSARCorc::test_rho\r\nERROR ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_stata.py::TestGLSARCorc::test_glsar_arima\r\nERROR ..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_glsar_stata.py::TestGLSARCorc::test_glsar_iter0\r\n```\r\n\r\nSo we are using the kwargs.\r\n\r\nexample GLSAR.iterative_fit uses it to attach extra attributes to results instance\r\n\r\n`results = self.fit(history=history, **kwargs)`\r\n\r\none possible solution is to add a more explicit `results_kwds` to `fit`\r\n","for fit_regularized unit test\r\n\r\nIt looks like the generic code always uses maxiter as fit keyword.\r\nmaxiter does not apply or do anything in OLS, WLS, and GLSAR uses maxiter only in `iterative_fit`\r\n\r\n```\r\n..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\tests\\test_regression.py:1468:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\linear_model.py:848: in fit_regularized\r\n    rslt = OLS(self.wendog, self.wexog).fit_regularized(\r\n..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\regression\\linear_model.py:1130: in fit_regularized\r\n    return fit_elasticnet(self, method=method,\r\n..\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\base\\elastic_net.py:236: in fit_elasticnet\r\n    rslt = model1.fit(maxiter=0)\r\n```"],"labels":["type-bug","comp-regression"]},{"title":"ENH: penalization (inverse) proportional to t-values","body":"(A semi-random thought, no reference)\r\n\r\nRidge and Lasso penalize all parameter values in the same way (L1 or L2)\r\nWith SCAD, we don't penalize large parameters.\r\n\r\nHowever, I think it would be better not to penalize \"significant\" parameters.\r\nI don't remember what the optimal parameter specific penalization factor is in (generalized) Ridge, but there should be something like this dependence.\r\n\r\nWith fixed \"prior\", the prior will be already less important for parameters or directions for which the data contains a large amount of information. However, we might want to reduce penalization for \"significant\" parameters even more.\r\n\r\nexample\r\nhigh multicollinearity, but not singular.\r\nWe could use the initial estimate, compute t-values and then reduce the penalization `weight` for those parameters with large t-values.\r\n\r\nmaybe:\r\nuse SVD of initial cov_params and penalize in direction of eigenvectors with small eigenvalues.\r\nin Ridge with 2 almost identical regressors: don't penalize the total effect (sum of the two), but penalize the split.\r\n(this might be the same as reparameterizing (x1, x2) to (x1 + x2, x2 - x1) and penalizing individual parameters in the latter case.)\r\nRelated: this might make Ridge closer to invariant w.r.t. linear transformation (B X) (I don't remember what that invariance is called.)\r\n\r\nchoice of pen_weight:\r\n\r\nIn general we could have different penalization weights for each parameter and then choose some optimal criterion, GCV, IC, cross-validation. However, that is high dimensional hyper-parameter search.\r\nInstead, what we have is fixed `weights` that are multiplied by a scalar `pen_weight`. In that case chooses good `weights` will improve performance.\r\n\r\ncomputational aside\r\n\r\nIn OLS cov_params is just proportional to inv(x.T x), so eigenvalues will be related to singular values of x directly.\r\nAFAIR, plain Ridge penalizes eigenvalues\/eigenvector with something like 1 \/ (lambda + eigenvalue). Here we would have different lambda for each eigenvector depending on the size of the eigenvalue, i.e. just a different function of eigenvalues.\r\n\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","topic-penalization"]},{"title":"ENH: treatment effect with multiple treatments, multinomial propensity score","body":"(mainly a reminder to look at this. I downloaded several articles in statistics, medical statistics, ...)\r\nSUMM issue #2443 with reference to Cattaneo and Stata\r\n\r\n3 references, but I don't have an overview, and did not read much.\r\n\r\nwe need \r\n- a multinomial or similar model for propensity scores, and then \r\n- potential outcomes for pairwise comparisons or comparison to a reference treatment.\r\n\r\nImai, van Dyk is cited for general definition including multinomail\/ordinal and continuous treatment\r\nan older article of Guido Imbens is also often cited in this literature\r\n\r\nImai, Kosuke, and David A van Dyk. \u201cCausal Inference With General Treatment Regimes.\u201d Journal of the American Statistical Association 99, no. 467 (September 1, 2004): 854\u201366. https:\/\/doi.org\/10.1198\/016214504000001187.\r\n\r\nLi, Fan. \u201cPropensity Score Weighting for Causal Inference with Multiple Treatments.\u201d The Annals of Applied Statistics 13, no. 4 (December 2019): 2389\u20132415. https:\/\/doi.org\/10.1214\/19-AOAS1282.\r\n\r\nLopez, Michael J., and Roee Gutman. \u201cEstimation of Causal Effects with Multiple Treatments: A Review and New Ideas.\u201d Statistical Science 32, no. 3 (2017): 432\u201354.\r\n\r\n","comments":["a recent article with Newey as one of the authors\r\nbased on a brief look it might be to theoretical to be usable for us (mainly identification result)\r\n\r\nNewey, W K, and S Stouli. \u201cHeterogeneous Coefficients, Control Variables and Identification of Multiple Treatment Effects.\u201d Biometrika 109, no. 3 (September 1, 2022): 865\u201372. https:\/\/doi.org\/10.1093\/biomet\/asab060.\r\n","and related matching, but we don't have matching yet\r\n\r\nR package to an article\r\nhttps:\/\/github.com\/shuyang1987\/multilevelMatching","Stata teffects has option to use mlogit (same as our MNLogit) as treatment model.\r\n"],"labels":["type-enh","comp-treatment"]},{"title":"ENH: power_approximate: collection of simple formulas for large number of tests","body":"a possible idea for power and sample size computation\r\n\r\n\"breadth instead of depth\"\r\n\r\nInstead of getting good power computation for individual tests, we could add a large collection of simplified power and sample size computation for a large number of hypothesis tests.\r\n\r\nI looked several times at books like Chow et al, but did not like them much.\r\nThose power computation are simplified, e.g. proportion with same var or std under null and alternative. Those are simple asymptotic approximations.\r\nHowever, they would be easy to implement, and power and sample size computation could be added for a large number of hypothesis tests that we don't have yet. PASS\/NCSS referenced those in several cases.\r\n\r\n\r\nChow, Shein-Chung, Jun Shao, Hansheng Wang, and Yuliya Lokhnygina. Sample Size Calculations in Clinical Research. 3rd ed. New York: Chapman and Hall\/CRC, 2017. https:\/\/doi.org\/10.1201\/9781315183084.\r\n\r\nThe plan would be to add the collection but separately from the \"more serious\" individual power and sample size functions, e.g. put them in a new module `power_approximate`.\r\n\r\nThe advantage is that we would quickly get things for different designs and cases\r\none sample, 2 sample parallel, cross-over (paired)\r\ncluster randomized trials, group sequential trials\r\nNCSS\/PASS has a collection of function similarly named to the list of tests in Chow et al.\r\n\r\nAlso,\r\nTo compute the power or sample size we need almost all computation that are used in the hypothesis test.\r\nSo we might get those almost for free. We would have to split out intermediate computations either in helper functions or put them in classes.\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"ENH: characteristic function inversion, pdf, cdf","body":"I'm not sure I will ever get back to this.\r\nI gave up on most of this kind of work because it's difficult to get it to work for regression model outside of additive error, loc-scale distributions and models.\r\nSame applied to orthogonal polynomial approximation to distribution functions. \r\n\r\nparking a MIT licensed matlab package, found via a partial translation to julia\r\nhttps:\/\/github.com\/witkovsky\/CharFunTool\r\none interesting detail: choice of bounds for integration interval. AFAIR, I never had an automatic choice for this.\r\n`integration_interval` https:\/\/gitlab.com\/tom.plaa\/CharacteristicInvFourier.jl\/-\/blob\/main\/src\/CharacteristicInvFourier.jl#L30 \r\n\r\nrelated: fft used for discrete distributions #3556\r\n\r\nI don't find \"fft\" in sandbox.distributions. It looks like I never committed my code when I was working on this.\r\n\r\nI find 2 related threads in the mailing list:\r\nhttps:\/\/groups.google.com\/g\/pystatsmodels\/c\/jObbnAxykf0\/m\/ESe7uG-GQy0J\r\nhttps:\/\/groups.google.com\/g\/pystatsmodels\/c\/5aKGPCaQzAw\/m\/CjFXcgm7_LsJ\r\n","comments":["maybe it's still worth spending some time on this for pvalues.\r\nI recently ran into sum of beta (?) distributed random variables in multivariate linear model, manova tests.\r\nSum of chi2 random variables #3363\r\n\r\nBoth have approximation in a few terms using other distributions like F or chi2 distribution.\r\n"],"labels":["comp-distributions","type-enh"]},{"title":"I wrote a customized state space class but could not figure out the error. Please help.","body":"Statsmodels - 0.14.0.dev535\r\n@ChadFulton Could you please help me with this error? Thanks!\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp\/ipykernel_46580\/847275192.py in <module>\r\n----> 1 two_factor_fit = two_factor_mod.fit()\r\n\r\n~\\Anaconda3\\envs\\new_base\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py in fit(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\r\n    726             else:\r\n    727                 func = self.smooth\r\n--> 728             res = func(mlefit.params, transformed=False, includes_fixed=False,\r\n    729                        cov_type=cov_type, cov_kwds=cov_kwds)\r\n    730 \r\n\r\n~\\Anaconda3\\envs\\new_base\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py in smooth(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\r\n    887 \r\n    888         # Wrap in a results object\r\n--> 889         return self._wrap_results(params, result, return_ssm, cov_type,\r\n    890                                   cov_kwds, results_class,\r\n    891                                   results_wrapper_class)\r\n\r\n~\\Anaconda3\\envs\\new_base\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py in _wrap_results(self, params, result, return_raw, cov_type, cov_kwds, results_class, wrapper_class)\r\n    786                 wrapper_class = self._res_classes['fit'][1]\r\n    787 \r\n--> 788             res = results_class(self, params, result, **result_kwargs)\r\n    789             result = wrapper_class(res)\r\n    790         return result\r\n\r\n~\\Anaconda3\\envs\\new_base\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py in __init__(self, model, params, results, cov_type, cov_kwds, **kwargs)\r\n   2316         self.param_names = [\r\n   2317             '%s (fixed)' % name if name in self.fixed_params else name\r\n-> 2318             for name in (self.data.param_names or [])]\r\n   2319 \r\n   2320         # Save the state space representation output\r\n\r\n~\\Anaconda3\\envs\\new_base\\lib\\site-packages\\statsmodels\\base\\data.py in param_names(self)\r\n    354     def param_names(self):\r\n    355         # for handling names of 'extra' parameters in summary, etc.\r\n--> 356         return self._param_names or self.xnames\r\n    357 \r\n    358     @param_names.setter\r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() \r\n```","comments":["Hard to tell what exactly is going on here. Have you set the `_param_names` object to something? It expects the parameter names to be strings. This error can show up if `_param_names` are numeric, but there are a number of other potential problems too.","Hi @ChadFulton . Thank you so much for your reply.\r\n\r\nThis is what I got by calling the property 'param_names'\r\n<img width=\"420\" alt=\"Picture1\" src=\"https:\/\/user-images.githubusercontent.com\/43833244\/227040446-f895a0e3-2824-43a2-9cc3-53fc95e32929.png\">\r\n","I think I figured out why. I assigned a NumpyArray to param_names. I changed to a python list and the error went away.","Thanks for following up. We should update the state space setup to make `param_names` compatible with numpy arrays."],"labels":["comp-tsa-statespace","question"]},{"title":"\"update\" function in sm.tsa.statespace.MLEModel","body":"Hi @ChadFulton,\r\n\r\nI have a quick question for the \"update\" function - def update(self, params, **kwargs):\r\n\r\nThere is a term in the observation intercept vector ($d_t$) called 't' in my code and it changes for each row of the endog data.\r\nYou can think of 't' is the time index which starts from 0 (for the first row of the dataframe), 't' =  0 + $\\delta_t$ for the second row, 't' = 0 + 2 $\\delta_2$ for the third row, ..., etc. $\\delta_t$ is a fixed number (one week which is 1\/52). Could you please advise me how to accomplish this? That would be much appreciated! Thank you!","comments":["Sure, so it sounds like you want a time trend in the observation intercept.  Let's say that the $\\delta_t$ is the first element of your `params` array, then you would want something like:\r\n\r\n```python\r\ndef update(self, params, **kwargs):\r\n    # Handle any base class requirements, such as parameter transformations\r\n    params = super().update(params, **kwargs)\r\n\r\n    # Extract the parameter of interest\r\n    delta_t = params[0]\r\n\r\n    # Update the observation intercept\r\n    self['obs_intercept'] = np.arange(self.nobs) * delta_t\r\n```\r\n\r\nDoes that answer your question?"],"labels":["comp-tsa-statespace","question"]},{"title":"ENH\/Review: oaxaca-blinder, decompose 2 sample mean in diff exog, diff params effect","body":"I never looked at oaxaca-blinger before\r\n\r\nuses print statement in Results instead of returning string\r\nhttps:\/\/stackoverflow.com\/questions\/71490289\/explained-and-unexplained-effects-dont-add-up-to-gap-in-oaxaca-blinder-decompos\r\n\r\nextension to nonlinear models, Logit, Probit, ...\r\nSinning, Mathias, Markus Hahn, and Thomas K. Bauer. \u201cThe Blinder\u2013Oaxaca Decomposition for Nonlinear Regression Models.\u201d The Stata Journal 8, no. 4 (December 1, 2008): 480\u201392. https:\/\/doi.org\/10.1177\/1536867X0800800402.\r\n\r\nI ran into this while looking at references for mediation effect and interaction effects for Logit, Probit, nonlinear models\r\n\r\noaxaca-blinder sounds a bit like regression adjustment in treatment effect: 2 groups\/treatments and separate OLS regression by group.\r\n\r\nbrief google scholar search for recent articles, around 6700 since 2019\r\n\r\nGuo, Kevin, and Guillaume Basse. \u201cThe Generalized Oaxaca-Blinder Estimator.\u201d Journal of the American Statistical Association 0, no. 0 (June 16, 2021): 1\u201313. https:\/\/doi.org\/10.1080\/01621459.2021.1941053.\r\n\r\nRahimi, Ebrahim, and Seyed Saeed Hashemi Nazari. \u201cA Detailed Explanation and Graphical Representation of the Blinder-Oaxaca Decomposition Method with Its Application in Health Inequalities.\u201d Emerging Themes in Epidemiology 18, no. 1 (August 6, 2021): 12. https:\/\/doi.org\/10.1186\/s12982-021-00100-9.\r\n\r\nI guess we get some (code) overlap with other methods, decomposition of ATE in regression adjustment?\r\npartially related #8737 #8216\r\n\r\n","comments":["also we should find a descriptive name, oaxaca-blinder is not informative (except for literature search)\r\n\r\nI have no idea what would be a good name.\r\n\r\n(Based on quick look at the definition of the decomposition, it looks a bit like Laspeyre, Paasche indices)\r\n"],"labels":["type-enh","comp-stats","type-refactor","comp-treatment"]},{"title":"SUMM\/ENH: (roadmap) margins and get_prediction with generated exog","body":"We are still far away supporting `margins` as in Stata or margins\/lsmeans\/emmeans in R.\r\n\r\nThe main problem is getting the information to be able to construct multi-column exog and compute it's effects, e.g. derivatives w.r.t. original variable combined over several columns.\r\n(I briefly looked at formulaic, but it's likely even worse than patsy for getting the required information out of it.)\r\n\r\nwhat's possible\r\n\r\n- DOC, notebook: \"Do it yourself marginal effects.\" I have most of it using `_wald_nonlinear`.\r\n- helper functions for constructing exog by user\r\n- add public versions of predict_nonlinear, wald_nonlinear based ib delta method.\r\n\r\nmaybe worth a try\r\n\r\n- black box derivatives for dy\/dx, try numerical derivatives with predict \"transform\" (as generic as using python `eval`)\r\n- similar, can we support derivatives if we have user provided function\r\n\r\npossible\r\n\r\n- support some cases that we can figure out, e.g. categorical without interaction similar to t_test_pairwise\r\n\r\ndifficult (not my kind of work)\r\n\r\n- can we use our own stateful transforms, e.g. polynomials, similar to what we do in GAM?\r\n- improve formulaic, e.g. \r\n  - it has formula derivative w.r.t. x using sympy, but it does not work for categorical or categorical-continuous interaction.\r\n  - more stateful transforms with extra methods like derivatives that we need.\r\n\r\nmany open issues","comments":[],"labels":["comp-genmod","comp-discrete","topic-predict","roadmap"]},{"title":"ENH (roadmap) \"causal\"","body":"I'm starting to like `statsmodels.causal` as umbrella folder for anything related to some kind of endogeneity of regressors, both traditional methods and Rubin tradition of ATE.\r\n\r\ncurrent\r\n\r\n- `treatment` treatment effect\r\n- mediation currently in `stats`\r\n- sandbox or unit tests for IV2SLS, IVGMM, PoissonGMM\r\n- `heckman` in PR (hybrid model class combines MLE and 2-stage)\r\n\r\nTodo (likely not to difficult)\r\n\r\n- control function (2 stage)\r\n  - continuous endogenous regressor, ivpoisson\r\n  - generic control function, I guess we can just use any two models\r\n- `treatment` for non-linear outcome models, GLM, discrete\r\n- `etreatment` (Stata) similar to treatment but with IV, \r\n- endogenous missing values, dropout (not completely at random) ???\r\n- IVGMM ??? other models than Poisson ? LEF ???\r\n- endogenous selection models, heckman, ...\r\n  - parametric: probit + correlated unobserved random component in outcome model, \r\n    linear and nonlinear, e.g. count outcome\r\n  - semi- or nonparametric, e.g. 2stage with series expansion of 1st residual\/control functions (Newey), not a structural model.\r\n- ...\r\n\r\nTBC\r\n\r\nI guess dominant implementation will be 2-stage where main model results returned is outcome\/final model. like 2-stage heckman.\r\nJoint fully specified MLE would be separate, e.g. heckman MLE ???\r\nWe get a separate category for that, e.g. fully parametric 2-part models (2 submodels) with MLE, possibly with copula for dependence modelling?\r\n","comments":["`causal` will need some statement that it includes both\r\n\r\n- models for average causal effects that are non parametrically identified, e.g. `treatment`, `etreatment`\r\n- fully parametric models e.g. bivariate or triangular MLE with assumed distributions. (heckman, heckpoisson, hurdle count with endogenous participation\/correlated random effects.)\r\n\r\nand as compromise\r\n- semi-parametric methods, parametric models for at least some parts with flexible functional form (Newey selection model)\r\n  e.g. control function methods with series expansion or other expansion of control function (which might not be consistent with any standard fully parametric functional form). (maybe R has a package with copulas and penalized splines)\r\n- QMLE with nonparametric properties\/identification for some average statistics, (I'm not sure about the details yet, Wooldridge, maybe GLM with canonical link), needs the extras for computing average marginal effect correctly.\r\n\r\nWhere does system of linear equation, SEM, simultaneous equation models fit in? \r\nUnder normality assumptions it's fully parametric, but it might also be semi-parametric like OLS, e.g. only need assumption on mean function and correlation pattern or causal structure.\r\n\r\n\r\naside: measurement error models. where do we put those?  \r\nThis has similar structure to models with endogeneity or unobserved error correlated with some x (\"exog\")\r\n \r\n\r\n\r\n\r\n\r\n","not clear what the module structure inside \"causal\" should be\r\n- by type of outcome variable, linear, discrete, count, ...\r\n- by estimator, full MLE, IV\/control function, ...\r\n- by \"endogeneity\" problem: endogenous regressor, sample selection, unobserved heterogeneity, mediation, conditional independence (?)\r\n\r\nand there is a group of estimators that just focuses on treatment effect, POM, ATE as current `treatment`\r\n\r\ncode and computations will overlap across each of the categorizations.\r\nWhat is the appropriate class hierarchy?\r\n\r\ne.g. \r\ncontrol function as 2-part models with focus on outcome model and 1st stage model as auxiliary model.\r\nsomething like IV2SLS or heckman\/heckprobit without the full MLE, but as generic version.\r\nwith arbitrary endogenous regressors ?\r\n\r\nfull MLE for selection models with arbitrary outcome model and numerical integration, and binary treatment model.\r\n\r\n...\r\n\r\n","some of this will be a pain to unit test\r\nR does not have a lot on control function estimation, a few special method packages for endogenous variabls.\r\ntask view econometrics is very short in IV section\r\n\r\nthe closest I found for control function, residual inclusion is \r\nhttps:\/\/search.r-project.org\/CRAN\/refmans\/OneSampleMR\/html\/tsri.html\r\nThey use GMM to estimate.\r\nIf I can verify this case in a unit test, then changing the control function should also be correct,  assuming we have the correct derivatives for functions of first stage residuals that are included as control functions.\r\n\r\nStata seems to have a mix of different methods, and seems to be very \"eclectic\".\r\nivprobit, .... etreatment, eregress, ...\r\n(I don't have an overview for which methods they use in which function.)\r\n\r\n**update** \r\nAFAICS, eregress, eprobit, ... (`erm`) are all MLE based on unobserved correlated multivariate normal random error in the linear latent variables.\r\n\r\n\r\nAnother option for unit test is to find articles with cases to replicate.\r\nIn the worst case, we use Monte Carlo check that it \"works\".\r\n","aside https:\/\/cran.r-project.org\/web\/views\/CausalInference.html\r\nhas a lot more, but not so much on IV"],"labels":["type-enh","roadmap","comp-causal"]},{"title":"DOC: unstructured covariance is missing in GEE docs","body":"merged in #7059\r\nrequested in #7005\r\n\r\nI was trying to see how difficult it would be to replicate multivariate model with unstructered within covariance. (for #8722)\r\n`Unstructured` is already implemented but not in docs. \r\n\r\nalso not in https:\/\/www.statsmodels.org\/stable\/gee.html#dependence-structures :\r\n\r\nStationary\r\nEquivalence\r\nNominalIndependence\r\n\r\ncorrection to usage:\r\nGEE is more like panel data, common parameters across groups and group specific exog.\r\nMultivariate linear model like MNLogit is group specific params and common exog.\r\n\r\n","comments":[],"labels":["comp-docs","comp-genmod"]},{"title":"ENH: mv_test, manova tests for MNLogit","body":"We can use the restriction matrices of mv_test used in multivariate manova, _MultivariateOLS for MNLogit.\r\n\r\nHowever, the test statistics like Wilks' lambda are derived under normality assumptions. \r\nWilks' is \"exact\" LR-test, AFAIU, with distribution based on ratio of Wishart matrices.\r\n\r\nInstead we can use standard wald test in MNLogit and use mv_test mainly to create the appropriate constraint matrices for `L B M = C` hypothesis.\r\nOne needed tool is converting L, M and C to `vec` constraints, some kronecker product in MNLogit, 2-dim params without parameter restrictions imposed.\r\n\r\n\r\n\r\n ","comments":[],"labels":["type-enh","comp-discrete"]},{"title":"ENH: path analysis and directed acyclic graphs DAG","body":"\"DAGs and Control Variables - How to select control variables for causal inference using Directed Acyclic Graphs\"\r\nhttps:\/\/towardsdatascience.com\/controls-b63dc69e3d8c\r\n\r\nmaybe we can support something here.\r\nWe don't want to include or become a path package, but some support to eg. generate restrictions in system of equations, recursive systems, multi endog models could be useful.\r\n\r\n`SVAR` without the `VAR` ?\r\nor mediator, moderator effects #8733\r\n\r\n(related to \"causality\")\r\n\r\n","comments":["Imbens, Guido W. \u201cPotential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics.\u201d Journal of Economic Literature 58, no. 4 (December 2020): 1129\u201379. https:\/\/doi.org\/10.1257\/jel.20191597.\r\n\r\n(too long to read, but looks interesting)"],"labels":["type-enh"]},{"title":"ENH: Instrumental Variable Estimation of Causal Risk Ratios","body":"parking a reference for binary outcome \r\nmainly as motivation and overview what we might want to add.\r\n\r\ncompares different estimators including GMM (ivpoisson)\r\n\r\nPalmer, Tom M., Jonathan A. C. Sterne, Roger M. Harbord, Debbie A. Lawlor, Nuala A. Sheehan, Sha Meng, Raquel Granell, George Davey Smith, and Vanessa Didelez. \u201cInstrumental Variable Estimation of Causal Risk Ratios and Causal Odds Ratios in Mendelian Randomization Analyses.\u201d American Journal of Epidemiology 173, no. 12 (June 15, 2011): 1392\u20131403. https:\/\/doi.org\/10.1093\/aje\/kwr026.\r\n\r\nfound by chance looking at talk slides by Palmer https:\/\/remlapmot.github.io\/talk\/2010_iscb\/\r\nHe has several talk slides using GMM or comparing with it https:\/\/remlapmot.github.io\/talk\/2011_stata\/\r\n","comments":[],"labels":["type-enh"]},{"title":"ENH: out-of-sample predict for regression adjustment (treatment, ATE) ","body":"comparing predictions (potential outcomes and differences) across different models, similar to regression adjustment in `treatment` but out-of-sample.\r\n\r\nThis is similar to `margins` for a binary exog, but there we only have one model and need to compare predicted means for two different exog arrays.\r\nIn regression adjustment type models, we have two different models, one model per predicted potential outcome.\r\n\r\nI guess to get the standard errors, we need the joint cov_params of the two models.\r\n\r\nunclear: If the models where estimated on two different subsamples, then the cov_params would just be block-diagonal, assuming independence of observations or at least of subsamples.\r\nDoes this mean we don't need the full GMM version for joint inference?\r\nIs this equivalent to full interaction of treatment dummy with exog.\r\n\r\nIPW version will be more difficult, I don't have a guess (except using the full GMM cov_params as used in `treatment`)\r\nI guess probability of treatment would not be relevant for prediction, because we want to compute potential outcomes for each treatment choice. Similar to computing expected benefits for choice by expected utility maximization.\r\nHowever, if the decision maker is not the one making the choice, then we would need also the IPW adjustment and need to predict also the probability that treatment is choses or accepted. E.g. expected benefit or cost unconditional on treatment choice.\r\n\r\nPredict for individual observations still has strong parametric assumptions, regression adjustment is not doubly robust, and we are not averaging over unobserved heterogeneity.\r\n\r\n(no reference, Just a thought while browsing various treatment type literature)\r\n\r\nrelated issues: predict, get_prediction and margins","comments":[],"labels":["type-enh","comp-treatment","topic-predict"]},{"title":"BUG: cov_params is positive definite even though exog is singular","body":"Mainly an observation for now. I don't know yet what we could do.\r\n\r\nI'm trying out an OrderedModel with singular exog (one column duplicated)\r\ncov_params is positive definite, no error, now warning for hessian invertibility\r\nbut exog is (near) singular\r\n\r\nIn this case the user does not get any warning about the collinearity problem, although `bse` are large for the affected params.\r\n\r\nMy guess is that the finite diff approximation used for hessian in OrderedModel makes hessian positive definite in this example.\r\nIt might be not (or less of) a problem in models that have hessian_factor that multiplies (outer product of) exog_i.\r\n(I'm working in a messy notebook with several things based on https:\/\/stackoverflow.com\/questions\/75559325\/llr-pvalue-extract-from-orderedresults-statsmodels  with extra random columns in exog, ex_orderedmodel.ipynb)\r\n\r\n```\r\nnp.linalg.eigvals(res_prob.cov_params())\r\narray([0.2441063 , 0.02922286, 0.02428714, 0.00033394, 0.00546712])\r\n\r\nnp.linalg.eigvals(mod.exog.T @ mod.exog)\r\narray([3.68102909e+03, 7.74216134e+02, 5.31613486e+01, 4.62111681e+01,\r\n       2.91358409e-14])\r\n```","comments":[],"labels":["comp-base","corner-case","topic-diagnostic"]},{"title":"ENH\/BUG: allow fit keyword ridge_factor in newton","body":"`model.fit(method=\"newton\", ridge_factor= 1e-5)`\r\n\r\nissues a warning about unused kwarg, and ridge_factor is not transmitted to the newton optimizer.\r\nI wanted to try out fit with a large ridge factor to force convergence (to a possibly regularized solution)\r\n\r\nI'm not sure whether this ever worked or was a refactoring victim.\r\n\r\nAFAICS, it's because ridge_factor is explicit keyword and not in `kwargs` in signature, base.optimizer\r\n```\r\n        def _fit_newton(f, score, start_params, fargs, kwargs, disp=True,\r\n                maxiter=100, callback=None, retall=False,\r\n                full_output=True, hess=None, ridge_factor=1e-10)\r\n```","comments":[],"labels":["type-bug","comp-base"]},{"title":"ENH: df_resid for exog with structural zeros","body":"What is the appropriate df_resid for models where exog has structural zeros?\r\n\r\nexample SUR and MANOVA where exog in \"long\" version is a diagonal block matrix, e.g. eye kronecker exog.\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/8722#issuecomment-1464226913\r\nA similar case would be if we vertically stack equations or models, e.g. combine several OLS into one big OLS.\r\n\r\nEven if we don't support automatic choices of df_resid, we would need a user option to set or adjust df_resid.\r\nI had already added `df_resid_inference` for similar cases.\r\n\r\nI guess this is related to within and between (repeated measures) anova which I never understood, or looked in details.\r\n\r\nRelated discussion: df or small sample correction in cluster robust cov_type.\r\n\r\nlikely related #6564\r\nIssue for general degrees of freedom computation (effective df of exog, not because of penalization)\r\n(It was too difficult to go through without very strong motivation)\r\n","comments":[],"labels":["type-enh","comp-base","comp-regression"]},{"title":"BUG: SARIMAX with time-varying coefficients works when model is (1,1,0) or (0,1,1), but raises an error when model set to (0,1,0) ","body":"Call used:\r\n\r\n```\r\nSARIMAX(endog=endog, exog=exog, order=order\r\n    seasonal_order= (0,0,0,4),\r\n    trend= 'n',\r\nenforce_stationarity=True,\r\nenforce_invertibility=True,\r\ntime_varying_regression=True,\r\nconcentrate_scale = False,\r\nmeasurement_error = False,\r\nmle_regression = False)\r\n```\r\nEndog has 6 covariates. \r\n\r\nWorks when order = (1,1,0) (however state.1 is all zero); also works for order = (0,1,1).  However when using order=(1,1,0) I get the following error:\r\n\r\n```\r\n    524     self.ssm._time_invariant = False\r\n    526 # Initialize the fixed components of the statespace model\r\n--> 527 self.ssm['design'] = self.initial_design\r\n    528 self.ssm['state_intercept'] = self.initial_state_intercept\r\n    529 self.ssm['transition'] = self.initial_transition\r\n\r\nFile C:\\Developer\\venv\\lab\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:420, in Representation.__setitem__(self, key, value)\r\n    417     if key not in self.shapes:\r\n    418         raise IndexError('\"%s\" is an invalid state space matrix name'\r\n    419                          % key)\r\n--> 420     setattr(self, key, value)\r\n    421 # If it's a tuple (with a string as the first element) then we must be\r\n    422 # setting a slice of a matrix\r\n    423 elif _type is tuple:\r\n\r\nFile C:\\Developer\\venv\\lab\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:54, in MatrixWrapper.__set__(self, obj, value)\r\n     51 shape = obj.shapes[self.attribute]\r\n     53 if len(shape) == 3:\r\n---> 54     value = self._set_matrix(obj, value, shape)\r\n     55 else:\r\n     56     value = self._set_vector(obj, value, shape)\r\n\r\nFile C:\\Developer\\venv\\lab\\lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:68, in MatrixWrapper._set_matrix(self, obj, value, shape)\r\n     65     value = value[None, :]\r\n     67 # Enforce that the matrix is appropriate size\r\n---> 68 validate_matrix_shape(\r\n     69     self.name, value.shape, shape[0], shape[1], obj.nobs\r\n     70 )\r\n     72 # Expand time-invariant matrix\r\n     73 if value.ndim == 2:\r\n\r\nFile C:\\Developer\\venv\\lab\\lib\\site-packages\\statsmodels\\tsa\\statespace\\tools.py:1477, in validate_matrix_shape(name, shape, nrows, ncols, nobs)\r\n   1474     raise ValueError('Invalid dimensions for %s matrix: requires %d'\r\n   1475                      ' rows, got %d' % (name, nrows, shape[0]))\r\n   1476 if not shape[1] == ncols:\r\n-> 1477     raise ValueError('Invalid dimensions for %s matrix: requires %d'\r\n   1478                      ' columns, got %d' % (name, ncols, shape[1]))\r\n   1480 # If we do not yet know `nobs`, do not allow time-varying arrays\r\n   1481 if nobs is None and not (ndim == 2 or shape[-1] == 1):\r\n\r\nValueError: Invalid dimensions for design matrix: requires 7 columns, got 6\r\n\r\n```","comments":["Thanks for reporting this!","Here's a minimum reproducible example:\r\n\r\n```python\r\nimport numpy as np\r\nimport statsmodels.api as sm\r\n\r\nnp.random.seed(1234)\r\nendog = np.random.normal(size=100)\r\nexog = np.random.normal(size=(100, 1))\r\n\r\nmod = sm.tsa.SARIMAX(endog=endog, exog=exog, order=(0, 1, 0), trend='n',\r\n                     time_varying_regression=True, mle_regression=False)\r\n\r\nres = mod.fit(disp=False)\r\n```"],"labels":["type-bug","comp-tsa-statespace"]},{"title":"ENH: enhance _MultivariateOLS, MANOVA, code duplication, ","body":"I thought MANOVA is using `_MultivariateOLS`.\r\nHowever, it looks like they share code, helper functions, but manova doesn't reuse the _MultivariateOLS class.\r\nThere is also quite a bit of code duplication.\r\n\r\nI was looking for access to the _MultivariateOLS instance in the MANOVA and it's test result instances, but it's not available.\r\n\r\n_MultivariateOLS does not have a summary implemented, which makes it difficult to get a quick overview of results.\r\n\r\ncontext #8713 trying to figure out usage and problems with multi-way manova.\r\n\r\n\r\nbased on an example: _MultivariateOLS runs an identical test to MANOVA\r\n\r\n```\r\nformula = 'PC1 + PC2 + PC3 + PC4 ~ C(Genotype, Helmert) * C(Temp, Helmert) * C(Time, Helmert)'\r\nmod = _MultivariateOLS.from_formula(formula, data=p_df)\r\nres = mod.fit()\r\ntt = res.mv_test()\r\n```\r\nbut `res` does not have any of the usual results attributes and methods, not even `params`\r\n\r\n```\r\n[i for i in dir(res) if not i.startswith(\"__\")]\r\n['_fittedmod',\r\n 'design_info',\r\n 'endog_names',\r\n 'exog_names',\r\n 'mv_test',\r\n 'summary']\r\n```\r\n","comments":["I'm trying to figure out more generally what we need for Multivariate linear model.\r\n\r\n- mv_test with eigenvalue based tests: it looks like the multivariate linear model only supports this.\r\n  However, it works also for a single restriction on one or two parameters, but likely not for arbitrary linear restrictions.\r\n  e.g. https:\/\/stats.stackexchange.com\/questions\/526672\/linear-hypothesis-test-for-multivariate-linear-model-mlm-object-in-r \r\n- wald_test for individual parameter\r\n  - need cov_params, AFAICS for cross-equation restriction we need the full GLS cov_params for flattened params. For within equation restrictions we would only need the var of each residual as `scale` \r\n  - actual hypothesis test methods would be inherited as in MNLogit \r\n- summary for model results is missing\r\n- robust cov_types, not clear, e.g. for #4121\r\n  I might have found more references but did not look at those yet, e.g. HC  \r\n- ...\r\n\r\nDo the inferential results differ from OLS with cluster robust standard errors?\r\nThe params will be the same, and I guess cov_params will be the same or similar (except for df, small sample corrections)\r\nWhat are the \"rank\" conditions between MultivariateOLS and OLS with cluster robust standard errors. \r\n\r\nMultivariateOLS might be a misnomer if we add GLS inference. ie. only params and within inference are equivalent to OLS.\r\n\"MultivariateLinearModel\" which might mean a likelihoodmodel, gaussian or quasi-gaussian\r\nmaybe \"MultivariateLS\"\r\n\r\nDo  we need a \"blown up\", memory inefficient version as reference, using kronecker product exog?\r\nIt would not be a memory problem in small samples as in experimental data.\r\nBut, I think we get into the SUR case if we allow for restrictions or penalization (#7255) of individual parameters.\r\n\r\nWhat about GMM equivalent model?\r\nWould not be to difficult with horizontal stacking of moment conditions, and robust cov_types would be inherited.\r\n\r\nNote: this is all for balanced groups\/panel case, i.e. same number of obs for each equation.\r\n  \r\naside:\r\nnice proof of equivalence of within cov_params is identical between single equation OLS and GLS\r\nhttps:\/\/economics.stackexchange.com\/questions\/45753\/seemingly-unrelated-regression-estimation-equivalent-to-ols-standard-errors\r\nHowever, it does not look at cross-equation cov, cov(beta_i, beta_j) for i != j\r\n","Inference in Multivariate linear model \"MultivariateGLS\"?\r\nsame regressors for each endog.\r\n\r\nthis article looks looks useful, includes the eigenvalue based tests Rao, ...\r\nand standard Wald on raveled params\r\nfor row-column hypothesis as in MANOVA\r\n\r\nStewart, Kenneth G. \u201cExact Testing in Multivariate Regression.\u201d Econometric Reviews 16, no. 3 (January 1, 1997): 321\u201352. https:\/\/doi.org\/10.1080\/07474939708800390.\r\n\r\na quick try comparing t_test with mv_test\r\nmvGLS t_test with mv_test: test statistic t**2 and F are very close, however not identical.\r\nmvGLS t_test with single equation OLS t_test: test statistics, tvalues are identical but p-values are only close if use_t=True.\r\n\r\nproblem is how to define consistent df_resid\r\n\r\nfor _multivariateOLS, I used  `res.df_resid = nobs * k_groups - res.params.size` (corresponding to long form of OLS\/GLS)\r\nsingle equation uses nobs - k_vars\r\naside: k_groups - k_params is negative in the example, k_params = k_groups * k_vars\r\n\r\n`stats` analogue would be 2 or k paired, correlated samples. What's the df for t-test? \r\nIt should be nobs - 1 if we just t-test the observationwise (pair) diff \r\n\r\n**update** \r\n`df_resid = nobs - k_vars` looks better\r\njustification would be that params are equivalent to single equation regression\r\n\r\nThe mv_test have df_denom (df_resid) that are neither of the two above.\r\n In small sample Roy's greatest root test differs quite a bit from the other three, both in p-values and df_num, df_denom (for multi-parameter joint hypothesis)\r\n\r\naside: multivariate L B M hypothesis only allows for within equation hypotheses, AFAICS, but joint over all or several equations.\r\n*wrong* M can do multi-equation comparison. The only restriction is that hypothesis are on a rectangular block of params.\r\n\r\n","aside: Roy's greatest root\r\n\r\ndf is not the same as in Steward 1997, it uses the max(p, q)\r\n\r\nquote\r\n\"where r=max(p, q) is an upper bound on F that yields a lower bound on the significance level. Degrees of freedom are r for the numerator and v - r + q for the denominator. \"\r\nwhere \"Let v be the error degrees of freedom\"\r\n\r\nhttps:\/\/documentation.sas.com\/doc\/en\/pgmsascdc\/9.4_3.4\/statug\/statug_introreg_sect038.htm#statug_introreg002005\r\n\r\n```\r\n    sigma = results.loc[\"Roy's greatest root\", 'Value']\r\n    r = np.max([p, q])\r\n    df1 = r\r\n    df2 = v - r + q\r\n    F = df2 \/ df1 * sigma\r\n```","aside:\r\nI should add the analogue to wald_test_terms to MANOVA, MultivariateGLS\r\nspecifically all terms that involve a factor are zero under null\r\n\r\ncurrent MANOVA is type 3, i.e. main factor is tested in the model that also includes interaction terms\r\n\r\nhttp:\/\/users.stat.umn.edu\/~helwig\/notes\/aov2-Notes.pdf for univariate anova\r\np. 57 type 2 anova tests main effect in the model without interaction effect  (section for unbalanced anova)\r\nthis is different from testing that both main and interaction effects are zero in full model.\r\n\r\n","back to the roots\r\n\r\nBerndt, Ernst R., and N. Eugene Savin. \u201cConflict among Criteria for Testing Hypotheses in the Multivariate Linear Regression Model.\u201d Econometrica 45, no. 5 (1977): 1263\u201377. https:\/\/doi.org\/10.2307\/1914072.\r\n\r\nOne application for multivariate models are cost and consumption share estimation.\r\nThis should get us closer to one of the original demands for multivariate regression in compositional analysis #3560\r\n(related MNLogit does not handle fractional data, AFAIK)\r\n\r\n"],"labels":["type-enh","type-refactor","comp-multivariate"]},{"title":"SUMM: singular exog (again)","body":"mainly for linear models, but some might also affect other models.\r\n\r\nI'm looking at singular exog again in context of pivoting QR.\r\nI don't have an overview of open issues and possible design.\r\n\r\nrelated\r\n- positive definite hessian in optimization #3099\r\n  -  AFAIR, no check if hessian is computed in fit and skip_hessian is in optimization \r\n- discussion on not raising if singular #3824\r\n- a FAQ issue for ill-conditioning #6381\r\n- asking for dropping instead of pinv #8670\r\n- PR for pivoting QR #6935\r\n- a similar issue to this one #7695\r\n- old issue for Logit, this should now raise #3367\r\n- MixedLM singular, I'm not familiar with this #6140\r\n\r\nto do, to check:\r\n\r\n- cutoffs, thresholds: OLS, ... need an option in fit for `rcond` in pinv #2628\r\n  - it looks like matrix rank of cov_param can be much smaller than rank of exog. I guess because of squaring\r\n  - if we use different linalg computations, then we need to check that matrix rank definitions, i.e. thresholds, are consistent, e.g. pinv rcond versus np.linalg.matrix_rank. Our rank attribute in OLS might not be the same as the  pinv rank and not the same as the rank of cov_params. #1662 #3994\r\n  - need to report\/warn rank of cov_params if not full rank. But constrained estimation, `fit_constrained, has intentionally reduced rank cov_params, variance is zero of constraints  #1717\r\n  - pinv threshold in OLS is too small, many collinear cases slip through.\r\n- Where is the function that I wrote to drop variables sequentially based on QR? I cannot find it right now.\r\n  https:\/\/github.com\/statsmodels\/statsmodels\/pull\/2380\/files#diff-6c9338f4faac471ff45da570aa82c563661ada8aff75afe24e70f60ae9b0ea85R586 there is also an iterative version somewhere (not mine) \r\n- scipy.linalg pivoting QR looks pretty \"random\", that is in repeated runs with different random samples, which variable is dropped varies.\r\n- empty cells, columns with only zeros (this is more urgent) patsy does not check whether cells are empty when creating the design matrix for categorical interactions. AFAIR pinv works to set those params to zero. but df for anova_lm and wald tests can be wrong. #4137 #5398 #8506  #8512\r\n- postestimation, hypothesis tests with singular cov_params. Not clear, not systematically checked, (maybe relevant #4798) \r\n- still no \"estimable\" checks and helper functions. #6271\r\n- `fit_collinear` #8580\r\n- ...\r\n\r\n","comments":[],"labels":["type-bug","type-enh","design","comp-regression"]},{"title":"Results of n-way MANOVA with interactions differ significantly from results from R's stats::manova","body":"(I'm not sure if this is a bug or a feature request - sorry for false alarm if this was expected!)\r\n\r\nI get different results when performing n-way MANOVA with interactions when using this implementation and R.\r\n\r\nLet's use data from the file I attached as an example. \r\n[metals.csv](https:\/\/github.com\/statsmodels\/statsmodels\/files\/10882245\/metals.csv)\r\n\r\nIn R I would use the following formula:\r\n\r\n```R\r\nm_data = read.csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10882245\/metals.csv')\r\nmetal_manova = manova(cbind(C1, C2) ~ extractants * metals, data=m_data)\r\nsummary(metal_manova, test='Wilks')\r\n```\r\nThis yields the following result:\r\n\r\n| |Df |Wilks|approx F|num Df|den Df |Pr(>F)|\r\n|---|---|---|---|---|--- |---|\r\nextractants|9|0.0039340|164.38|18|198|< 2.2e-16 ***|\r\nmetals|4|0.0019440|536.58|8|198|< 2.2e-16 ***|\r\nextractants:metals|36|0.0005648|112.97|72|198|< 2.2e-16 ***|\r\nResiduals|100|\r\n\r\n\r\nUsing the following steps:\r\n```python\r\nfrom statsmodels.multivariate.manova import MANOVA\r\nimport pandas as pd\r\nm_df = pd.read_csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10882245\/metals.csv')\r\nm_man = MANOVA.from_formula('C1 + C2 ~ extractants * metals', data=m_df)\r\nm_res = m_man.mv_test().summary_frame\r\nm_res[m_res.index.get_level_values(1) == 'Wilks\\' lambda']\r\n```\r\n\r\nI get the following result:\r\n|Effect|Statistic|Value|Num DF|Den DF|F Value|Pr > F|\r\n|---|---|---|---|---|---|---|                                 \r\n|Intercept|Wilks' lambda|0.999777|2|99.0|0.011051|0.989011|\r\n|extractants|Wilks' lambda|0.808417|18|198.0|1.23418|0.236848|\r\n|metals|Wilks' lambda|0.942882|8|198.0|0.738631|0.657287|\r\n|extractants:metals|Wilks' lambda|0.000565|72|198.0|112.965548|0.0|\r\n\r\ni.e. the F values are completely different in these two implementations (and thus the Pr > F is also different). Why is it the case? The results are the same for statsmodels'and R's implementations when there is no interaction between the independent variables - only the analysis with interactions yield different results. Can I specify some parameters that would make the results the same as in R? \r\n\r\nI'm working with version 0.13.5 in WSL2. Python version 3.10.6.\r\n","comments":["I don't know what causes those very large differences.\r\n\r\nHere is a long issue on difference between statsmodels and SPSS #6464\r\n\r\nIn unbalanced anovas, both encoding of categorical and type of anova can have a large effect on the results. But I'm not really familiar with most of the MANOVA details or variants.\r\n\r\nOne guess would be that R stats manova computes a type 1 (?) anova, where the main effects are significant on their own but not when including the interaction effect. ","Can you add the R results without interaction effect?\r\n","Sure, it would be:\r\n\r\n```R\r\nm_data = read.csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10882245\/metals.csv')\r\nmetal_manova = manova(cbind(C1, C2) ~ extractants + metals, data=m_data)\r\nsummary(metal_manova, test='Wilks')\r\n```\r\n\r\n| |Df|Wilks|approx F|num Df|den Df|Pr(>F)|\r\n|---|---|---|---|---|---|---|\r\n|extractants|9|0.61729|4.0918|18|270|1.441e-07 ***|\r\n|metals|4|0.40943|18.9952|8|270|< 2.2e-16 ***|\r\n|Residuals|136|\r\n\r\nIn statsmodels the results are as follows:\r\n|Effect|Statistic|Value|Num DF|Den DF|F Value|Pr > F|\r\n|---|---|---|---|---|---|---|\r\n|Intercept|Wilks' lambda|0.973636|2|135.0|1.827725|0.164735|\r\n|extractants|Wilks' lambda|0.617288|18|270.0|4.091826|0.0|\r\n|metals|Wilks' lambda|0.409431|8|270.0|18.995228|0.0|\r\n","I don't really understand what's going on. The data looks balance with 3 observations per cell\r\n\r\nIf I use Helmert coding, I get very different answers, same as R at floating point precision\r\n\r\n```\r\nm_man = MANOVA.from_formula('C1 + C2 ~ C(extractants, Helmert) * C(metals, Helmert)', data=m_df)\r\nm_res = m_man.mv_test().summary_frame\r\nres = m_res[m_res.index.get_level_values(1) == 'Wilks\\' lambda']\r\nprint(res)\r\n                                                             Value Num DF  \\\r\nEffect                                     Statistic                        \r\nIntercept                                  Wilks' lambda  0.005816      2   \r\nC(extractants, Helmert)                    Wilks' lambda  0.003934     18   \r\nC(metals, Helmert)                         Wilks' lambda  0.001944      8   \r\nC(extractants, Helmert):C(metals, Helmert) Wilks' lambda  0.000565     72   \r\n\r\n                                                         Den DF     F Value  \\\r\nEffect                                     Statistic                          \r\nIntercept                                  Wilks' lambda   99.0  8461.53262   \r\nC(extractants, Helmert)                    Wilks' lambda  198.0  164.377752   \r\nC(metals, Helmert)                         Wilks' lambda  198.0  536.584806   \r\nC(extractants, Helmert):C(metals, Helmert) Wilks' lambda  198.0  112.965548   \r\n\r\n                                                         Pr > F  \r\nEffect                                     Statistic             \r\nIntercept                                  Wilks' lambda    0.0  \r\nC(extractants, Helmert)                    Wilks' lambda    0.0  \r\nC(metals, Helmert)                         Wilks' lambda    0.0  \r\nC(extractants, Helmert):C(metals, Helmert) Wilks' lambda    0.0  \r\n\r\nres[\"Pr > F\"].to_numpy()\r\narray([2.2327275071851898e-111, 1.7172312697863343e-108,\r\n       9.280908812934959e-130, 1.484331159350747e-129], dtype=object)\r\n```\r\n\r\n```\r\nstat = res[\"Value\"].to_numpy(dtype=float)\r\n\r\nstat[1:] - [0.0039340182601693471, 0.0019440456693583979, 0.0005647834453735598]\r\narray([2.96724451e-15, 1.53501344e-15, 4.54605971e-16])\r\n```","That's it.\r\nR stats manova uses by default the Helmert encoding.\r\nI got the same result in R before and after setting the contrast option with helmert.\r\n\r\nWith helmert coding the design matrix is orthogonal\r\n\r\n```\r\nnp.max(np.abs(np.corrcoef(m_man.exog[:, 1:].T) - np.eye(m_man.exog.shape[1] - 1)))\r\n2.220446049250313e-16\r\n```\r\n\r\nI guess we should add explaining Helmert coding  to a docstring or docs\r\n","That indeed seems to solve the issue for this data, but I encounter an error when I try to use Helmert contrasts to my actual test data:\r\n[example.csv](https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv)\r\n\r\nWhen I use:\r\n```python\r\nimport pandas as pd\r\nfrom statsmodels.multivariate.manova import MANOVA\r\np_df = pd.read_csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv', index_col=0)\r\nformula = 'PC1 + PC2 + PC3 + PC4 ~ C(Genotype, Helmert) * C(Temp, Helmert) * C(Time, Helmert)'\r\nfit = MANOVA.from_formula(formula, data=p_df)\r\nfit.mv_test().summary_frame\r\n```\r\nthe model is created correctly using MANOVA.from_formula, but calling fit.mv_test() results in ValueError: zero-size array to reduction operation maximum which has no identity. \r\n\r\nI don't know what to think here, since I can perform the analysis in R without any issues:\r\n```R\r\ne_data = read.csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv', row.names = 1)\r\nm_e = manova(cbind(PC1, PC2, PC3, PC4) ~ Genotype * Temp * Time, data=e_data)\r\nm_e$contrasts\r\nsummary(m_e, test='Wilks')\r\n```\r\nm_e$contrasts also shows that R uses 'contr.treatment', not 'contr.helmert' in this case. Weird.","using the debugger, I found that it breaks when testing the intercept. I don't know why or why it is doing that\r\n\r\nHowever it doesn't happen with patsy's standard coding for categorical\r\n\r\nThis is what I get if I remove the explicit intercept\r\n\r\n```\r\np_df = pd.read_csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv', index_col=0)\r\nformula = 'PC1 + PC2 + PC3 + PC4 ~ C(Genotype, Helmert) * C(Temp, Helmert) * C(Time, Helmert) - 1'\r\nfit = MANOVA.from_formula(formula, data=p_df)\r\nm_res = fit.mv_test().summary_frame\r\npd.set_option('display.width', 1000)\r\nprint(m_res[m_res.index.get_level_values(1) == 'Wilks\\' lambda'])\r\n                                                                     Value Num DF    Den DF   F Value    Pr > F\r\nEffect                                             Statistic                                                   \r\nC(Genotype, Helmert)                               Wilks' lambda  0.537404     12  55.85228  1.231318  0.285792\r\nC(Temp, Helmert)                                   Wilks' lambda   0.70032      4      21.0  2.246571  0.098472\r\nC(Time, Helmert)                                   Wilks' lambda   0.98168      4      21.0  0.097974  0.981956\r\nC(Genotype, Helmert):C(Temp, Helmert)              Wilks' lambda  0.466618      8      42.0   2.43561  0.029155\r\nC(Genotype, Helmert):C(Time, Helmert)              Wilks' lambda  0.453839      8      42.0   2.54307  0.023473\r\nC(Temp, Helmert):C(Time, Helmert)                  Wilks' lambda  0.806734      4      21.0  1.257722  0.317694\r\nC(Genotype, Helmert):C(Temp, Helmert):C(Time, H... Wilks' lambda  0.378451      8      42.0  3.284039  0.005347\r\n```","The bug happens because all eigenvalues are close to zero (below 1e-8 threshold) and the test matrices are empty when testing the intercept.\r\n\r\nMy guess is that for MANOVA we can skip testing the intercept.\r\nMore generally, we need to handle the edge case with empty test matrices, or figure out why all eigenvalues are zero\r\n(Aside: I don't remember why intermediate results are complex, maybe because of eigenvalues of asymmetric matrix)\r\n\r\nfrom debugger session using the latest example as test case.\r\n\r\n```\r\n> ...\\statsmodels\\statsmodels\\multivariate\\multivariate_ols.py(360)_multivariate_test()\r\n-> stat_table = multivariate_stats(eigv2, p, q, df_resid)\r\n(Pdb) p name\r\n'Intercept'\r\n(Pdb) hypo\r\n['Intercept', array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), None]\r\n(Pdb) hypotheses\r\n[['Intercept', array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), None], ['C(Genotype, Helmert)', array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), None], ['C(Temp, Helmert)', array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]), None], ['C(Time, Helmert)', array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]), None], ['C(Genotype, Helmert):C(Temp, Helmert)', array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]), None], ['C(Genotype, Helmert):C(Time, Helmert)', array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]]), None], ['C(Temp, Helmert):C(Time, Helmert)', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]), None], ['C(Genotype, Helmert):C(Temp, Helmert):C(Time, Helmert)', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]), None]]\r\n(Pdb) eigv1\r\n*** NameError: name 'eigv1' is not defined\r\n(Pdb) eigv2\r\narray([-7.48674631e-46-1.36665658e-45j, -7.48674631e-46+1.36665658e-45j,\r\n        0.00000000e+00+0.00000000e+00j,  2.80694850e-29+0.00000000e+00j])\r\n```","This is what I get if I change the code to skip testing the intercept:\r\n\r\n````\r\np_df = pd.read_csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv', index_col=0)\r\nformula = 'PC1 + PC2 + PC3 + PC4 ~ C(Genotype, Helmert) * C(Temp, Helmert) * C(Time, Helmert)'\r\nfit = MANOVA.from_formula(formula, data=p_df)\r\nm_res = fit.mv_test().summary_frame\r\npd.set_option('display.width', 1000)\r\nprint(m_res[m_res.index.get_level_values(1) == 'Wilks\\' lambda'])\r\n                                                                     Value Num DF Den DF   F Value    Pr > F\r\nEffect                                             Statistic                                                \r\nC(Genotype, Helmert)                               Wilks' lambda  0.537404      8   42.0  1.911578  0.083575\r\nC(Temp, Helmert)                                   Wilks' lambda   0.70032      4   21.0  2.246571  0.098472\r\nC(Time, Helmert)                                   Wilks' lambda   0.98168      4   21.0  0.097974  0.981956\r\nC(Genotype, Helmert):C(Temp, Helmert)              Wilks' lambda  0.466618      8   42.0   2.43561  0.029155\r\nC(Genotype, Helmert):C(Time, Helmert)              Wilks' lambda  0.453839      8   42.0   2.54307  0.023473\r\nC(Temp, Helmert):C(Time, Helmert)                  Wilks' lambda  0.806734      4   21.0  1.257722  0.317694\r\nC(Genotype, Helmert):C(Temp, Helmert):C(Time, H... Wilks' lambda  0.378451      8   42.0  3.284039  0.005347\r\n```","The source of the problem is that endogs in the last example have mean zero. (around 1e-18)\r\n\r\nThe estimated intercept are\r\n`array([-2.73761049e-18,  1.00830802e-17,  6.45100293e-18,  5.63785130e-18])`\r\n\r\nSo all hypothesis test computation are essentially on zeros, so there will not be nonzero eigenvalues. \r\n(i.e. it's mostly floating point noise, but we ignore eigenvalues below 1e-8 threshold)\r\n\r\naside:\r\nparams are the first element of the `_fittedmod` attribute\r\n`params, df_resid, inv_cov, sscpr = mod._fittedmod`\r\n ",">  This is what I get if I change the code to skip testing the intercept:\r\n\r\nThese results are consistent with what I get with R's MANOVA. Since you marked resolving this issue as a milestone for 0.15, I guess I'll just use rpy2 to implement what I need into the software I'm developing and switch to statsmodels once it gives me the same results. Thanks for helping me!\r\n\r\nShould I close the issue for now or leave it open for you? ","leave it open. It will stay open as a FAQ. (After some time I won't remember what the issue between statsmodels and R is.)\r\n(issue #8722 is for enhancements and refactoring, not for the problem with empty matrices)\r\n\r\nI will fix the exception with the intercept for 0.14, but maybe just with an option to skip the intercept test. I guess we don't skip it by default.\r\nI don't know what we should return in case of empty test matrices in general, other than raising a more explicit exception.","for the record: my initial unit test example was\r\n\r\n```\r\ndef test_interaction():\r\n    p_df = pd.read_csv('https:\/\/github.com\/statsmodels\/statsmodels\/files\/10897687\/example.csv', index_col=0)\r\n    formula = 'PC1 + PC2 + PC3 + PC4 ~ C(Genotype, Helmert) * C(Temp, Helmert) * C(Time, Helmert)'\r\n    fit = MANOVA.from_formula(formula, data=p_df)\r\n    fit.mv_test(skip_intercept_test=True).summary_frame\r\n```\r\n\r\nI have it replaced with an example with random numbers."],"labels":["type-bug","FAQ","comp-multivariate"]},{"title":"ENH\/SUMM (roadmap) Power and sample size computation","body":"several open issues and missing extensions\r\ngeneric issues\r\n- #8162\r\n- #8206\r\n\r\n81 open issues for `is:issue is:open power sample size `\r\n\r\n### open issues and todos\r\n\r\n- better docs: #8647 #4085 \r\n- power classes #8652\r\n- option for \"exact\" power in discrete rates, proportions, ..., actual power (when different from requested minimum power)\r\n  - sawtooth #2731,\r\n- add (excess) dispersion option to power computation for proportion, rates\r\n- proportion, one and two sample, one sample requires #6721\r\n  - non-zero null #8251\r\n  - match power to test method #8675\r\n  - other tests: equivalence \r\n- rates, more general #8138\r\n  - #8303 \r\n- power_approximate #8755 \r\n- regression\r\n  - linear, FTestRegressionPower #8646 #8403\r\n  - GLM ?\r\n- generic, based on test statistic, noncentrality, ...\r\n- power for confidence interval\r\n- gof\r\n- other tests, hypothesis test we have or have not, ???\r\n  - paired, correlated 2 sample,  ... e.g. #2743\r\n  - tests for variance\/dispersion\r\n- ...","comments":["comment https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8652#issuecomment-1419663977\r\nincludes links to docs of other packages GPower, R, Stata, NCSS\/PASS"],"labels":["type-enh","comp-stats","roadmap"]},{"title":"Confusion with se_mean and standard deviation","body":"I would like to clarify that I understood conception of se_mean and standard deviation in statsmodels correctly. Could you help me with this? \r\n\r\nIn documentation **statsmodels.tsa.base.prediction.PredictionResults.se_mean** we have description that se_mean is the standard deviation of the predicted mean. At the same time in **Release 0.8.0** there is a passage that get_forecast provides standard errors. As far as I know standard deviation and standard errors of mean are not the same things. \r\n\r\nAs a rookie in statistics I found in wiki that std is a variation in measurements, while the standard error of the mean is a probabilistic statement about how the sample size will provide a better bound on estimates of the population mean, in light of the central limit theorem. However, standard error can be described as an estimation of that standard deviation. Does it mean that in **statespace.sarimax** we estimate possible future values of standard deviations and the model outputs std which depends on the number of time series point (more additional time points, better prediction of std)? \r\n\r\nI build SARIMAX model and want to construct \u0441onfidence interval as a variation in measurements for forecast. Is it possible to use mean_se for this or I need to convert these values to std by multiplying SE by sqrt(n)? And does n equal the number of data points in time series before forecasting? \r\n\r\nThank you for yor reply in advance!","comments":["For a simple state space model (of which SARIMAX is a special case), we have:\r\n\r\n$$y_t = Z \\alpha_t + \\varepsilon_t, \\varepsilon_t \\sim N(0, H)$$\r\n\r\n$$\\alpha_t = T \\alpha_{t-1} + \\zeta_t, \\zeta_t \\sim N(0, Q)$$\r\n\r\nHere we will assume that the matrices $Z, H, T, Q$ are known. (Actually, the estimated parameters of the model are in those matrices, but the state space model prediction results standard errors and confidence intervals supported by Statsmodels never account for parameter uncertainty, so we can ignore that for now).\r\n\r\nBy default, `get_prediction` (or `get_forecast`) gives one-step-ahead predictions of $y_t$, so that:\r\n\r\n- `PredictionResults.predicted_mean` = $E[y_t | y_{t-1}, y_{t-2}, \\dots]$\r\n- `PredictionResults.se_mean` = $StdDev[y_t | y_{t-1}, y_{t-2}, \\dots]$\r\n\r\n(Aside: @josef-pkt pointed out that this actually doesn't match the intended\/typical Statsmodels usage of the `_mean` suffix, which I believe would be intended to capture e.g. $E[Z \\alpha_t | y_{t-1}, y_{t-2}, \\dots]$.  But things are a little bit different in state space models, because (a) many models do not have a $\\varepsilon_t$ term anyway, e.g. the SARIMAX model, and (b) you can always rewrite any state-space model such that it doesn't have a $\\varepsilon_t$ term, by placing that term into the state vector $\\alpha$).\r\n\r\nI'm not sure if that answers your question or not, but please feel free to follow up.","\r\n- `PredictionResults.predicted_mean` = $E[y_t | y_{t-1}, y_{t-2}, \\dots]$\r\n\r\n\"mean\" here sounds fine, it's a conditional expectation of y\r\n\r\n- `PredictionResults.se_mean` = $StdDev[y_t | y_{t-1}, y_{t-2}, \\dots]$\r\n\r\nIn OLS I used `se_obs` for similar (which includes parameter uncertainty plus residual standard deviation), corresponding to prediction interval.\r\n\r\nse_mean would be the uncertainty of the conditional expectations (coming from parameter uncertainty)\r\n`y_hat` = $E[y_t | y_{t-1}, y_{t-2}, \\dots]$\r\n`se_mean = std(y_hat | ...)`\r\n\r\naside:\r\nIn the newer prediction results class I use only `se` because get_prediction for discrete models can predict other statistics than `mean`. Outside of tsa and linear models, we don't have prediction intervals and `se_obs` yet.","Thanks @josef-pkt!"],"labels":["comp-tsa-statespace","question"]},{"title":"ENH\/REF: update stata export file","body":"supersedes #1716\r\n\r\nI'm using the ado file with Stata 14\r\n\r\ntrying to write a docstring\r\n\r\n```\r\nsyntax [namelist(min=1)], [RESName(str)] SAVing(str) [ Format(str) APPend REPlace NOEst]\r\n\r\nnamelist : list of matrices to be included in output\r\nRESName(str) : name used as prefix for results and matrix names\r\nSAVing(str) : output filename, name of python module\r\nFormat(str) : format for numbers in output\r\noptions :\r\n    APPend : append results to existing output file\r\n    REPlace : replace content of existing output file or create new output file\r\n    NOEst : do not include the estimation results dictionary `ereturn`\r\n```","comments":["my latest usage\r\n\r\ntreatment effect, export only r(table)\r\nthis does not include loading and preparing the data, which I did in the interpreter directly\r\n\r\n```\r\nset type double\r\ncd \"...\\stata_work\"\r\nrun ...\\stata_work\\estmat2nparray.ado\r\n\r\n\r\nquietly teffects ra (bweight prenatal1 mmarried mage fbaby) (mbsmoke), aequations\r\nquietly return list\r\nmatrix table = r(table)'\r\n* estmat2nparray table, resname(\"ra\") saving(\"results_teffects.py\") format(\"%16.0g\") noest replace \/* append replace *\/\r\n\r\nquietly teffects ra (bweight prenatal1 mmarried mage fbaby) (mbsmoke), atet\r\nquietly return list\r\nmatrix table_t = r(table)'\r\nestmat2nparray table table_t, resname(\"ra\") saving(\"results_teffects.py\") format(\"%16.0g\") noest replace \/* append replace *\/\r\n\r\n\r\nquietly teffects ipw (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), aequations\r\nquietly return list\r\nmatrix table = r(table)'\r\n* estmat2nparray table, resname(\"ipw\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly teffects ipw (bweight) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), atet\r\nquietly return list\r\nmatrix table_t = r(table)'\r\nestmat2nparray table table_t, resname(\"ipw\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly teffects aipw (bweight prenatal1 mmarried mage fbaby) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), aequations\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"aipw\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\n\r\nquietly teffects aipw (bweight prenatal1 mmarried mage fbaby) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), wnls aequations\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"aipw_wls\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\n\r\nquietly teffects ipwra (bweight prenatal1 mmarried mage fbaby) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), aequations\r\nquietly return list\r\nmatrix table = r(table)'\r\n* estmat2nparray table, resname(\"ipwra\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly teffects ipwra (bweight prenatal1 mmarried mage fbaby) (mbsmoke mmarried c.mage##c.mage fbaby medu, probit), atet\r\nquietly return list\r\nmatrix table_t = r(table)'\r\nestmat2nparray table table_t, resname(\"ipwra\") saving(\"results_teffects.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n```\r\n","another recent usage\r\n\r\ntruncated model, export estimation results and margins, predict\r\n\r\n```\r\nset type double\r\ncd \"...\\stata_work\"\r\nrun ...\\stata_work\\estmat2nparray.ado\r\n\r\nclear\r\ninsheet using ...\\statsmodels\\statsmodels\\sandbox\\regression\\tests\\racd10data_with_transformed.csv, delimiter(\",\")\r\n\r\n\r\n\r\nquietly tpoisson docvis aget totchr if docvis > 0\r\n\r\nmatrix cov = e(V)\r\nsvmat cov, names(cov)\r\nereturn display\r\nmatrix params_table = r(table)'\r\nsvmat params_table, names(params_table)\r\n\r\nestat ic\r\nquietly return list\r\nmatrix icr = r(S)\r\nsvmat icr, names(icr)\r\n\r\nestmat2nparray params_table cov icr, resname(\"trunc_poisson\") saving(\"results_truncated_st.py\") format(\"%16.0g\") replace\r\n\r\nmargins, predict(cm)\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson.margins_cm\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nmargins\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson.margins_means\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nmargins, atmeans\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson.margins_atmeans\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(cpr(1)) predict(cpr(2)) predict(cpr(3)) predict(cpr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson.margins_cpr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(pr(0)) predict(pr(1)) predict(pr(2)) predict(pr(3)) predict(pr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson.margins_pr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\n\r\n\/* next truncated negative binomial\r\n*\/\r\n\r\nclear matrix\r\nmatrix drop _all\r\ndrop cov* icr* params_table*\r\n\r\nquietly tnbreg docvis aget totchr if docvis > 0\r\n\r\nmatrix cov = e(V)\r\nsvmat cov, names(cov)\r\nereturn display\r\nmatrix params_table = r(table)'\r\nsvmat params_table, names(params_table)\r\n\r\nestat ic\r\nquietly return list\r\nmatrix icr = r(S)\r\nsvmat icr, names(icr)\r\n\r\nestmat2nparray params_table cov icr, resname(\"trunc_negbin\") saving(\"results_truncated_st.py\") format(\"%16.0g\") append\r\n\r\nquietly margins, predict(cm)\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin.margins_cm\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margins\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin.margins_means\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margins, atmeans\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin.margins_atmeans\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(cpr(1)) predict(cpr(2)) predict(cpr(3)) predict(cpr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin.margins_cpr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(pr(0)) predict(pr(1)) predict(pr(2)) predict(pr(3)) predict(pr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin.margins_pr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\n\r\n\/*\r\n*\/\r\nclear matrix\r\nmatrix drop _all\r\ndrop cov* icr* params_table*\r\n\r\n\r\nquietly tpoisson docvis aget totchr if docvis > 1, ll(1)\r\n\r\nmatrix cov = e(V)\r\nsvmat cov, names(cov)\r\nereturn display\r\nmatrix params_table = r(table)'\r\nsvmat params_table, names(params_table)\r\n\r\nestat ic\r\nquietly return list\r\nmatrix icr = r(S)\r\nsvmat icr, names(icr)\r\n\r\nestmat2nparray params_table cov icr, resname(\"trunc_poisson1\") saving(\"results_truncated_st.py\") format(\"%16.0g\") append\r\n\r\nmargins, predict(cm)\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson1.margins_cm\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nmargins\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson1.margins_means\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nmargins, atmeans\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson1.margins_atmeans\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(cpr(2)) predict(cpr(3)) predict(cpr(4)) predict(cpr(5, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson1.margins_cpr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(pr(0)) predict(pr(1)) predict(pr(2)) predict(pr(3)) predict(pr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_poisson1.margins_pr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\n\r\n\/* next truncated negative binomial\r\n*\/\r\n\r\nclear matrix\r\nmatrix drop _all\r\ndrop cov* icr* params_table*\r\n\r\nquietly tnbreg docvis aget totchr if docvis > 1, ll(1)\r\n\r\nmatrix cov = e(V)\r\nsvmat cov, names(cov)\r\nereturn display\r\nmatrix params_table = r(table)'\r\nsvmat params_table, names(params_table)\r\n\r\nestat ic\r\nquietly return list\r\nmatrix icr = r(S)\r\nsvmat icr, names(icr)\r\n\r\nestmat2nparray params_table cov icr, resname(\"trunc_negbin1\") saving(\"results_truncated_st.py\") format(\"%16.0g\") append\r\n\r\nquietly margins, predict(cm)\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin1.margins_cm\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margins\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin1.margins_means\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margins, atmeans\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin1.margins_atmeans\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(cpr(2)) predict(cpr(3)) predict(cpr(4)) predict(cpr(5, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin1.margins_cpr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n\r\nquietly margin, predict(pr(0)) predict(pr(1)) predict(pr(2)) predict(pr(3)) predict(pr(4, .))\r\nquietly return list\r\nmatrix table = r(table)'\r\nestmat2nparray table, resname(\"trunc_negbin1.margins_pr\") saving(\"results_truncated_st.py\") format(\"%16.0g\") noest append \/*replace *\/\r\n```","here is an export of a minimal estimation result (from 2017)\r\n\r\n```\r\n\r\nimport delimited \"...\\statsmodels\\datasets\\randhie\\randhie.csv\", clear\r\n\r\ntempname filename\r\nlocal filename = \"results_genpoisson.py\"\r\n\r\ngnpoisson mdvis lncoins idp lpi fmde physlm disea hlthg hlthf hlthp\r\n\/* boiler plate, add matrices if needed *\/\r\ntempname cov\r\ntempname params_table\r\nmatrix cov = e(V)\r\nmatrix params_table = r(table)'\r\nestat ic\r\nmatrix infocrit = r(S)\r\nestmat2nparray params_table cov infocrit, saving(`filename') format(\"%16.0g\") resname(\"gp2_mdvis\") replace\r\n\/*------------------*\/\r\n```"],"labels":["maintenance"]},{"title":"ENH: outlier detection, skewed, zero-inflated data","body":"\r\nThe Templ et al article uses several methods for outlier detection.\r\nA bit ad-hoc. uses Box-Cox transformation of data. Methods are not distribution specific.\r\nMight be good to get an overview of what we might want to have, e.g. ogk does well for multivariate x-outlier identification.\r\nIt might also be useful to compare with distribution specific outliers, e.g. outlier-influence post-estimation for Poisson, .... \r\n\r\nTempl, M., J. Gussenbauer, and P. Filzmoser. \u201cEvaluation of Robust Outlier Detection Methods for Zero-Inflated Complex Data.\u201d Journal of Applied Statistics 47, no. 7 (May 18, 2020): 1144\u201367. https:\/\/doi.org\/10.1080\/02664763.2019.1671961.\r\n\r\nYang, Jun, Min Xie, and Thong Ngee Goh. \u201cOutlier Identification and Robust Parameter Estimation in a Zero-Inflated Poisson Model.\u201d Journal of Applied Statistics, October 27, 2010. https:\/\/doi.org\/10.1080\/02664760903456426.\r\n","comments":["semi-random idea\r\n\r\noutlier detection under full distributional assumption, giving up on QMLE and robustness to distributional misspecification or keeping distribution unspecified.\r\n\r\nFor robust methods in GLM, Poisson, Logit we also have to mostly give up on QMLE. The correction factor for Fisher consistency are computed based on a specified reference distribution. (Similarly, AFAIR, truncation and censored model like Tobit are not distribution free (no QMLE), with possible workaround if we assume symmetry.) \r\n\r\nMy initial idea was to look for distributions that remove skewness, so trimming would be symmetric in the transformed random variable and we can use robust methods calibrated for symmetric\/normal distributions.\r\nHowever comments in stackexchange mainly recommend cdf\/pit transformation (e.g. we could use normal scores) \r\nhttps:\/\/stats.stackexchange.com\/questions\/210846\/transformation-from-skewed-to-symmetric-distribution\r\n\r\nIf make the assumption that we have correctly specified distribution, then we can apply the predictive distribution that we already have in `get_distribution`. We mainly need to add support for outlier detection,, i.e. `predict(which=\"cdf\")` and look at both tails.\r\n(related would be minimum distance estimation, where we estimate parameters by minimizing some distance between data and cdf. I looked at it for a while but did not decide on implementing anything specific.)\r\n\r\nSome of the effect of \"bad\" outliers, i.e. observations that are x and y outliers could be removed by trimming or weighting of influential points, base on (robust) mahalanobis distance of exog.\r\n\r\n\r\n\r\n"],"labels":["type-enh","comp-robust","topic-diagnostic"]},{"title":"ENH: model equivalence testing","body":"reversing the null hypothesis in model comparisons and gof tests.\r\n\r\nnot a very popular topic, but should be more used\r\nMain practical problem is how to specify insignificant \"effect size\" margin for model comparisons and gof.\r\n\r\nexample where it would work is FTestRegressionPower, i.e. Cohen's f2 effect size that can be related to (partial) R-square. In these cases we have a interpretable scaled noncentrality nc\/nobs.\r\n\r\nIndependently of how to define equivalence margin, we could just add the functions.\r\nThe target would be to extend equivalence testing from one and two sample functions to models, model comparisons and diagnostic and specification tests.\r\n\r\nThese equivalence tests would be targeted to specific statistics, e.g. gof in general, directional misspecification, params, predictive test (?), ... depending on the test statistic that is used.\r\n\r\ne.g.\r\n- show that interaction effect is close to zero (equivalent models with and without interaction effect)\r\n- show that some statistics are equivalent for models with different link functions.\r\n- ...\r\n\r\n\r\n\r\n","comments":["two econometrics articles for equivalence testing\r\n\r\nLavergne, Pascal. \u201cModel Equivalence Tests in a Parametric Framework.\u201d Journal of Econometrics 178 (January 1, 2014): 414\u201325. https:\/\/doi.org\/10.1016\/j.jeconom.2013.05.007.\r\n\r\nKim, Jae H., and Andrew P. Robinson. \u201cInterval-Based Hypothesis Testing and Its Applications to Economics and Finance.\u201d Econometrics 7, no. 2 (June 2019): 21. https:\/\/doi.org\/10.3390\/econometrics7020021.\r\n\r\n"],"labels":["type-enh","comp-stats","topic-diagnostic"]},{"title":"ENH: out-of-sample moment conditions, score, GMM, MLE","body":"looks interesting for diagnostics\r\n\r\nout of sample or forecast moment conditions, and it's asymptotic distribution\r\n\r\nHoffman, Dennis, and Adrian Pagan. \u201cPRACTITIONERS CORNER: Post-Sample Prediction Tests for Generalized Method of Moments Estimators.\u201d Oxford Bulletin of Economics and Statistics 51, no. 3 (1989): 333\u201343. https:\/\/doi.org\/10.1111\/j.1468-0084.1989.mp51003007.x.\r\n\r\n\r\nWe still have the problem that loglike and score can only be evaluated at the model data, endog, exog, i.e. no predictive, out-of-sample statistics. Similarly for GMM, momcond are only in-sample.\r\nIn GLM we could use freq_weights to only estimate on a subsample. \r\n","comments":[],"labels":["type-enh","comp-base","topic-diagnostic"]},{"title":"ENH: Firth penalized Poisson","body":"This article has explicit expression for `score` of Firth penalized poisson, based on IRLS\r\n\r\nmotivation is separation in Poisson #1512\r\n\r\nMondol, Momenul Haque, M. Shafiqur Rahman, and Wasimul Bari. \u201cA Penalized Likelihood Approach for Dealing with Separation in Count Data Regression Model.\u201d Communications in Statistics - Simulation and Computation 0, no. 0 (March 30, 2022): 1\u201315. https:\/\/doi.org\/10.1080\/03610918.2022.2057541.\r\n","comments":["Firth Logit also has diag_hatmatrix in the penalization term, no need to compute penalization from hessian\r\n\r\nfor example, referenced by Mondol et al\r\nHeinze, G, and M. Schemper. 2002. A solution to the problem of separation in logistic regression. Statistics in\r\nMedicine 21 (16):2409\u201319. doi:10.1002\/sim.1047."],"labels":["type-enh","comp-genmod","comp-discrete","topic-penalization"]},{"title":"ENH: recentering cluster robust cov_type in GMM, fixed number of cluster asymptotics","body":"mainly parking a reference\r\n\r\n\r\nHwang, Jungbin. \u201cSimple and Trustworthy Cluster-Robust GMM Inference.\u201d Journal of Econometrics 222, no. 2 (June 1, 2021): 993\u20131023. https:\/\/doi.org\/10.1016\/j.jeconom.2020.07.048.\r\n\r\ncov_params based on centered and uncentered moment conditions are not asymptotically equivalent if number of clusters is fixed.\r\nin overidentified GMM where moment conditions are not zero at optimum.\r\n\r\nArticle is too heavy to quickly figure out. I did not find how the centering \"constant\" is defined. \r\n\r\n(I recently saw other articles for centering issue in GMM. ???)\r\n\r\n","comments":[],"labels":["type-enh","comp-base","reference"]},{"title":"ENH\/MAINT add typing, mypy or similar","body":"I don't like typing annotation in the code, it's ugly and makes the code difficult to read.\r\n\r\nHowever, numpy has stub files `.pyi` for all modules. Those would be appropriate also for statsmodels.\r\nI tried to create stub files with mypy's stubgen, but those are using `Any` for all parameters.\r\n\r\nI started with statsmodels.stats because that has relatively simple functions.\r\nThe model modules will need classes or class hierarchies as types.\r\n\r\npandas:\r\nhttps:\/\/github.com\/pandas-dev\/pandas\/blob\/main\/pandas\/_typing.py\r\n\r\nnumpy\r\nhas only a few public types in numpy.typing\r\nmore details in private https:\/\/github.com\/numpy\/numpy\/tree\/main\/numpy\/_typing and individual pyi files\r\n\r\nsklearn discussion\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16705#issuecomment-859378575\r\n","comments":["#8152 PR that add some typing support to (mainly) tsa"],"labels":["type-enh","maintenance"]},{"title":"SUMM\/ENH: (roadmap) penalization","body":"(penalization label has currently 75 open issues) \r\n\r\n### models with penalized objective functions\r\n\r\n- penalization classes\r\n...\r\n...\r\n\r\n- Firth and similar\r\n  - data dependent penalty\r\n  - double penalization Firth + L2\r\n  - data augmentation \r\n  - alternative penalization for logit\/binomial (F2 ? references ?)\r\n- ...\r\n\r\n### stats\r\n\r\n- penalized cov\r\n- ...\r\nTBC","comments":[],"labels":["type-enh","topic-penalization","roadmap"]},{"title":"ENH: logit, binomial double penalization Firth and (generalized) Ridge (L2)","body":"(mainly to park references)\r\n\r\nGao, Sujuan, and Jianzhao Shen. \u201cAsymptotic Properties of a Double Penalized Maximum Likelihood Estimator in Logistic Regression.\u201d Statistics & Probability Letters 77, no. 9 (May 1, 2007): 925\u201330. https:\/\/doi.org\/10.1016\/j.spl.2007.01.004.\r\n\r\nShen, Jianzhao, and Sujuan Gao. \u201cA Solution to Separation and Multicollinearity in Multiple Logistic Regression.\u201d Journal of Data Science 6, no. 4 (August 4, 2022): 515\u201331. https:\/\/doi.org\/10.6339\/JDS.2008.06(4).395.\r\n","comments":[],"labels":["type-enh","comp-genmod","comp-discrete","topic-penalization"]},{"title":"WIP\/ENH: add option null_prop_var in samplesize_proportions_2indep_onetail","body":"closes #8675\r\n\r\nThis will not be in 0.14\r\n\r\nNo real theoretical justification, and might need other options in the future.\r\n\r\nfor example\r\nWang Chow use the same var or std under null and alternative in sample size formula.\r\nMy guess is that this corresponds to a wald test for proportions, which does not have good small sample properties.\r\n\r\nWang, Hansheng, and Shein-Chung Chow. \u201cSample Size Calculation for Comparing Proportions.\u201d In Wiley Encyclopedia of Clinical Trials. John Wiley & Sons, Ltd, 2007. https:\/\/doi.org\/10.1002\/9780471462422.eoct005.\r\n\r\nopposite direction\r\nsome articles add continuity correction to match more closely Fisher's exact test, which is conservative in small samples.\r\ne.g.\r\nlast paragraph in note section with two references\r\nhttps:\/\/search.r-project.org\/CRAN\/refmans\/EnvStats\/html\/propTestPower.html\r\n\r\nanother article, Ury and Fleiss use the same var under null and alternative but add continuity correction\r\nUry, Hans K., and Joseph L. Fleiss. \u201cOn Approximate Sample Sizes for Comparing Two Independent Proportions with the Use of Yates\u2019 Correction.\u201d Biometrics 36, no. 2 (1980): 347\u201351. https:\/\/doi.org\/10.2307\/2529991.\r\n","comments":["I think I will add this to 0.14\r\n\r\n\"pooled\" can be changed to \"score\" or similar in a backwards compatible way (as alias) when we add power for non-zero null.\r\n","I still cannot think of a (theoretical) justification for using either prop1 or prop2 for the null std.\r\n\r\nR also has only the pooled version, no option for changing it.\r\nResults match our current results.\r\nhttps:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/power.prop.test.html\r\n\r\ne.g.\r\n```\r\n> power.prop.test(p1=0.03, p2=0.02, power=0.8, strict=FALSE)\r\n\r\n     Two-sample comparison of proportions power calculation \r\n\r\n              n = 3825.15\r\n             p1 = 0.03\r\n             p2 = 0.02\r\n      sig.level = 0.05\r\n          power = 0.8\r\n    alternative = two.sided\r\n\r\nNOTE: n is number in *each* group\r\n```\r\n","One possible justification could be as a bound, i.e. we take the worst of the three options.\r\nThe variance is larger as we get closer to prop=0.5.\r\n\r\nhere\r\np2 = 0.02\r\np1 = p2 + diff = 0.02 + 0.01 = 0.03\r\n\r\n```\r\nn1 = sm.stats.samplesize_proportions_2indep_onetail(0.01, 0.02, 0.8, null_var=\"prop1\")\r\nn2 = sm.stats.samplesize_proportions_2indep_onetail(0.01, 0.02, 0.8, null_var=\"prop2\")\r\nnp = sm.stats.samplesize_proportions_2indep_onetail(0.01, 0.02, 0.8, null_var=\"pooled\")\r\n\u200b\r\nn1, n2, np \r\n(4337.0730837641095, 3292.2657351312073, 3825.1497221026225)\r\n```"],"labels":["type-enh","comp-stats"]},{"title":"ENH: add option for null_var or null_prop to samplesize_proportions_2indep_onetail ","body":"https:\/\/stats.stackexchange.com\/questions\/605466\/how-to-get-python-statsmodels-to-match-evan-millers-famous-ab-test-sample-size\/605540#605540\r\n\r\nI'm not sure what the name of the options should be.","comments":["\"pooled\" is actually the estimated prob under the null of equal proportion if the true DGP is the one specified by the alternative.\r\n\r\nusing prop2 or prop1 for var_null might not correspond to any hypothesis test method, unless the hypothesis specified a fixed level\/value of prop2 under the null, e.g. H0: prob1 = prob2 = 0.05.\r\n\r\nThe generalization of \"pooled\" if there is a nonzero null diff (null margin as in noninferiority testing) would be the estimate under the null hypothesis restriction, eg. p2 + p1 - value = 0. We would need a more generic name for the option.\r\nThis might differ under wald versus score test.\r\n\r\nWe need some MonteCarlo studies if we want to match up the power and sample size computation directly with the hypothesis test `method`.\r\n\r\nAside: \r\n\"pooled\" uses the props specified by the alternative. Those are expected values under the alternative. \r\nIn the actual hypothesis test we use the sample estimates so var_null is a random variable. I don't think using the expected variance is \"exact\" (or equal to the one we would get with simulations) because of the nonlinearities.\r\nPASS has some \"averaged power\" but, AFAIR, that refers to a distribution of props under the alternative (and not sampling variation of variance estimate, although using random props also changes the implied var.)\r\n\r\n"],"labels":["type-enh","comp-stats"]},{"title":"OLS P value not consistent with t value ","body":"After running a simple univariate linear regression using OLS I obtain the following model summary\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/62406542\/218344585-07ad7c81-b68d-4505-a1dd-c7c48030b909.png)\r\n\r\nThe t value is consistent with the t value I calculate manually when I use the R-squared value given. However a |t| of 2.76 in no way corresponds to a p value of 0.010. I am not able to replicate the exact value obtained from model.pvalues using any number of degrees of freedom.  \r\n\r\nThe code I use to create this model is:\r\n\r\n`model = sm.OLS(list(DF[ind]),sm.add_constant(list(DF[de]))).fit()`\r\n\r\nwhere \r\n\r\n`list(DF[ind])` = [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1]\r\n\r\nand \r\n\r\n`list(DF[de])` = [7087.9, 5820.3, 5726.5, 5092.200000000001, 6391.8, 8094.1, 6491.1, 6852.700000000001, 7096.9, 7336.2, 6315.3, 5800.7, 6961.6, 6389.700000000001, 7977.0, 7199.799999999999, 7672.700000000001, 5165.8, 5243.7, 6628.2, 6309.6, 8531.7, 5336.9, 6744.6, 6522.7, 7255.200000000001, 6494.5, 4757.799999999999, 3726.5, 7140.3, 5000.799999999999]\r\n\r\nThe exact p value it gives is 0.009916977768826793 where according to that t-statistic it should be 0.11065255490834112.\r\n\r\nThe closest I can get to the given p value is using a DF of 10 which gives 0.010067636348152975 even though it seems as though the DF should just be 1, and given the coefficient of -0.002 it should certainly not be significantly different than 0.\r\n\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.11.1.final.0\r\nstatsmodels\r\n===========\r\nInstalled: 0.13.5 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\statsmodels)\r\nRequired Dependencies\r\n=====================\r\ncython: Not installed\r\nnumpy: 1.24.1 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\numpy)\r\nscipy: 1.10.0 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\scipy)\r\npandas: 1.5.2 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\pandas)\r\n    dateutil: 2.8.2 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\dateutil)\r\npatsy: 0.5.3 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\patsy)\r\nOptional Dependencies\r\n=====================\r\nmatplotlib: 3.6.3 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\matplotlib)\r\n    backend: module:\/\/backend_interagg \r\ncvxopt: Not installed\r\njoblib: 1.2.0 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\joblib)\r\nDeveloper Tools\r\n================\r\nIPython: Not installed\r\n    jinja2: 3.1.2 (C:\\Users\\gavin\\PycharmProjects\\FTDNS\\venv\\Lib\\site-packages\\jinja2)\r\nsphinx: Not installed\r\n    pygments: Not installed\r\npytest: Not installed\r\nvirtualenv: Not installed\r\n\r\n","comments":["looks correct to me, and it's heavily unit tested against R and Stata\r\n\r\n ```\r\nstats.t.sf(2.76, 29) * 2\r\n0.009913185201085792\r\n```"],"labels":["type-invalid"]},{"title":"DescrStatsW.quantile() function produce inconsistent results ","body":"#### Describe the bug\r\n\r\nThe quantile() function returns different arrays depending on whether the `probs ` are specified by a manually typed array or an array generated by `np.linspace()`\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nfrom statsmodels.stats.weightstats import DescrStatsW\r\nimport numpy as np\r\n\r\nwq = DescrStatsW(np.linspace(1,25,25), weights=[10]*25)\r\nprint(wq.quantile(probs = np.linspace(0, 1.0, 6)))\r\nprint(wq.quantile(probs = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])))\r\n```\r\n<details>\r\n\r\nWhen using `np.linspace(0, 1.0, 6)` to specify the probs, the function returns unexpected results: the 60% quantile is returned as 16.0 rather than 15.5. \r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\np\r\n0.0     1.0\r\n0.2     5.5\r\n0.4    10.5\r\n0.6    15.5\r\n0.8    20.5\r\n1.0    25.0\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\np\r\n0.0     1.0\r\n0.2     5.5\r\n0.4    10.5\r\n0.6    16.0\r\n0.8    20.5\r\n1.0    25.0\r\ndtype: float64\r\np\r\n0.0     1.0\r\n0.2     5.5\r\n0.4    10.5\r\n0.6    15.5\r\n0.8    20.5\r\n1.0    25.0\r\ndtype: float64\r\n\r\n<\/details>\r\n","comments":["I guess this is numpy behavior\r\nAFAICS, the computation is directly based on np.nanpercentile\r\n\r\n```\r\nnp.linspace(0, 1.0, 6) - np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])\r\narray([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.11022302e-16,\r\n       0.00000000e+00, 0.00000000e+00])\r\n```\r\n\r\nEquality checks with floats are numerically fragile.\r\n\r\nFor continuous variables ties happen with zero Lebesque measure.\r\nFor int valued arrays, I use a slack in functions like histogram to force that values are not exactly at the bin boundaries.\r\n\r\nI don't see anything that we could do here\r\n\r\n\r\n"],"labels":["comp-stats","won't fix"]},{"title":"ENH: test_proportions_2indep, method = 'logit-smoothed' is not vectorized","body":"see #8332\r\n\r\ncurrently raises ValueError in this case.","comments":[],"labels":["type-enh","comp-stats"]},{"title":"REF\/TST: check rootfinder in power classes","body":"I wrote the rootfinder for solve_power in the power classes a long time ago.\r\nscipy behavior has changed and new solvers became available.\r\n\r\nThe rootfinder would need a check for scipy compatibility and possible improvements.\r\n\r\nThere is currently no specific problem or bug with it.\r\n\r\nHowever, I just saw a warning running the unit tests\r\n\r\n```\r\nstatsmodels\/stats\/tests\/test_power.py::test_power_solver_warn\r\n  ...\\statsmodels\\stats\\power.py:108: RuntimeWarning: invalid value encountered in sqrt\r\n    pow_ = stats.norm.sf(crit - d*np.sqrt(nobs)\/sigma)\r\n```\r\n\r\nand the unit test has the following comments\r\n```\r\n# TODO: can something useful be made from this?\r\n@pytest.mark.xfail(reason='Known failure on modern SciPy >= 0.10', strict=True)\r\ndef test_power_solver_warn():\r\n    # messing up the solver to trigger warning\r\n    # I wrote this with scipy 0.9,\r\n    # convergence behavior of scipy 0.11 is different,\r\n    # fails at a different case, but is successful where it failed before\r\n```","comments":[],"labels":["comp-stats","comp-tools","type-refactor"]},{"title":"SUMM\/ENH\/REF: statsmodels 1.0","body":"Statsmodels 1.0 is still far in the future\r\n\r\nSome thoughts on wishlist and requirements\r\n\r\n- consistent returns of function, e.g. HolderTuple\r\n- consistent methods and attributes across models\r\n  - for and with consistent post-estimation\r\n  - we might still need categories of models, eg. MLE versus moment estimators, OLS\/WLS, GLM and GMM \r\n- flake and pep8 (but no \"black\"ening of code that I maintain.\r\n  - related: code patterns. LGTM\/CodeQL has a large number of false positives. Do we want to simplify some code to make the \"bot\" happy?  \r\n- stable frameworky classes, \r\n  - GenericLikelihoodModel (or alternative new class)\r\n  - GMM\r\n  - Multilink and multipart models (generic class still missing)\r\n- missing core methods and models, especially those that need to be based on new pattern and structure\r\n  - generic multilink models\r\n  - generic multipart models (not clear what we need), as in ZI, hurdle models, Copula models, generic truncated\/censored or inflated models. (several submodels)\r\n  - multi-endog and multivariate models, copula regression models would be multi-endog and multi-part\r\n  - endogeneity and two step models, IV \r\n  - there is some overlap in the latter 3 model categories in either data handling, model structure or results structure.\r\n- code cleanup\r\n  - public, and commonly used methods (models, functions): those should be ok after refactoring for consistency, unit test coverage is ok\r\n  - public methods with unused (?) extensions: need more unit test and verify methods. This affects mainly inherited methods in new classes and methods that come from combining different options. e.g. var_weights and freq_weights in GLM are not supported for all cov_types and post-estimation results.\r\n  - public models or functions not advertised. Some parts look public but are not in docs and might have insufficient unit tests.\r\n  - private methods. In some case we don't clearly distinguish between private and public methods. Either privatize some or make them stable (stable api, sufficient unit test coverage). This applies to many helper and tool functions that were created for internal support but have a public name.\r\n  - unused code: remove it unless there is a strong reason to keep it. e.g. sandbox. \r\n    (This will be a lot of work if we want to publish sandbox code that is 70% or 80% finished. e.g. I don't want to throw away sandbox.distributions unless superseded by scipy or newer statsmodels code.)\r\n- tsa\r\n  - no idea \r\n- ...\r\n\r\nI think we can do most over several pre-1.0 releases so that 1.0 would not have a large backwards compatibility impact.\r\n\r\none example for CodeQL problem and inconsistent returns: \r\nWe have several functions where the number of returns depends on an option. CodeQL cannot figure out in all cases what the appropriate tuple length is, and then reports false positives. If we always return a HolderTuple or similar, then that would not be the case. (This is in functions that were written a long time ago.)\r\n","comments":[],"labels":["type-enh","comp-base","design","type-refactor"]},{"title":"BUG\/REF: OLS, WLS hessian_factor unused keyword arguments","body":"(found in random browsing of docs,code)\r\n\r\nhessian_factor in linear model has scale and observed keywords, both are unused.\r\nDocstring of method looks like it was copied from GLM.\r\n\r\nThe scaled hessian_factor would need an estimate of scale, which we don't have.\r\ni.e. this is least squares and not MLE.\r\n\r\nNot sure what we need to change besides docstring.\r\nwe might need the keywords for consistency with GLM. We need to check where it's used.\r\n\r\n","comments":[],"labels":["type-bug","comp-regression"]},{"title":"ENH\/REF\/SUMM: enhance, refactor power classes","body":"triggered by #8646 and #8651\r\nsee also #8159 for power classes without effect size\r\nrelated issues: ...\r\n\r\nThe semi-generic power classes were written initially based on effect sizes and packages GPower and R pwr.\r\nDesign decisions were based on those packages with the generic structure it is often not \"obvious\" how to use those.\r\n\r\nTarget is to make them more directly usable and extend them to new cases, with possibly test specific power classes.\r\n\r\nThis should be based more on NCSS\/PASS and new Stata pss, than on the previous packages.\r\nThe more recent power function, especially for cases where var\/std differs between null and alternative as in rates and proportions, where heavily based on the NCSS\/PASS docs.\r\n\r\nAdditionally, I want \r\n- power classes or\/and effect sizes based on test statistics, tstat, fstat, ..., with normalized noncentrality, nc \/ nobs.\r\n  - main difference is e.g. using std of test statistic instead of std of population as in Cohen's d family. \r\n- ...\r\n\r\nnot clear yet\r\n- options not yet included, e.g. unequal var in t_test\r\n- more general: power computation that are specification robust, \r\n   e.g. robust cov_type, excess dispersion in poisson, kurtosis in variance hypothesis tests\r\n- ...\r\n\r\nspecific todos\r\n\r\n- review existing power classes for hidden assumptions especially what special cases they are designed for\r\n  - FTestPower, see comments in #8646 \r\n  - TTestIndPower: assumes equal var, and cohen's d effect size\r\n- maybe distinguish more clearly between keyword we can solve for and keywords that define setting or hypothesis test. For example we will likely need a `method` argument if we make classes for recently added power functions like those for rates and proportions.\r\n- power classes for recently added power functions, rates, proportions, variances, ...\r\n- power classes for TOST and other hypothesis tests that are not point hypothesis\r\n- ...\r\n- ...\r\n\r\nI guess (not checked again): The basic power classes for one sample TTestPower can be used for generic case if std in effect size is the std of the (unstandardized) test statistic.\r\nWhy is there currently no NormalPower class? We only have NormalIndPower with same equal var assumption as TTestIndPower.\r\n**update**\r\nNormalIndPower can be used for one sample test if ratio=0\r\n```\r\n            ``ratio`` can be set to zero in order to get the power for a\r\n            one sample test.\r\n```\r\nIt wouldn't cost much to add a specific NormalPower class.\r\nAside: \r\nNormalIndPower has an option in the `__init__` `self.ddof = ddof` instead of as method keyword.\r\nIt's the only power class with an `__init__` method","comments":["some overview references\r\n\r\nhttps:\/\/stats.oarc.ucla.edu\/other\/mult-pkg\/faq\/general\/effect-size-power\/faqhow-is-effect-size-used-in-power-analysis\/\r\nnote, in the last part square for f are missing\r\n`noncentrality coefficient lambda = N*f = 60*.369^2 = 60*.136 = 8.17`\r\nshould be\r\n`noncentrality coefficient lambda = N*f^2 = 60*.369^2 = 60*.136 = 8.17`\r\nalso they use `f^2` for regression and `f` for oneway anova in effect size definition\r\n\r\nGPower manual has much better coverage of cases than when I wrote power classes\r\nhttps:\/\/www.psychologie.hhu.de\/fileadmin\/redaktion\/Fakultaeten\/Mathematisch-Naturwissenschaftliche_Fakultaet\/Psychologie\/AAP\/gpower\/GPowerManual.pdf\r\n\r\nrelated Stata `esize` for various effect size estimates,  also `estat esize` after regression anova\r\nhttps:\/\/www.stata.com\/manuals\/resize.pdf\r\neffect size overview page https:\/\/www.stata.com\/features\/overview\/effect-sizes\/\r\n\r\npartially related meta-analysis effect size https:\/\/www.stata.com\/manuals\/metametaesize.pdf\r\n\r\nNCSS\/PASS several doc chapter on regression including linear, poisson and binomial, and GEE\r\nhttps:\/\/www.ncss.com\/software\/pass\/pass-documentation\/#Regression\r\nexample Logit, power has different std under null and alternative, sample size formula on p. 3\r\nhttps:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Tests_for_the_Odds_Ratio_in_Logistic_Regression_with_One_Binary_X_and_Other_Xs-Wald_Test.pdf\r\nHowever, for a normally distributed exog, the sample size formula does not have different std under null and alternative\r\nhttps:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Tests_for_the_Odds_Ratio_in_Logistic_Regression_with_One_Normal_X_and_Other_Xs-Wald_Test.pdf\r\n\r\nhttps:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Multiple_Regression.pdf\r\nhttps:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Poisson_Regression.pdf\r\n\r\nanother interesting one WMW rankorder statistic, stochastically larger as in brunner-munzel\r\nhttps:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Tests_for_Two_Ordered_Categorical_Variables-Non-Proportional_Odds.pdf\r\nI never looked at this AFAIR.\r\nreferences in their docs\r\nMachin, D., Campbell, M., Tan, S.B., and Tan, S.H. 2018. Sample Size Tables for Clinical Studies, 4th Edition.\r\n John Wiley & Sons. Hoboken, NJ.\r\nZhao, Y.D., Rahardja, D. Qu, Y. 2008. 'Sample size calculation for the Wilcoxon-Mann-Whitney test adjusting for\r\n ties.' Statistics in Medicine, 27, 462-468.\r\n\r\nin R\r\nhttp:\/\/users.stat.umn.edu\/~helwig\/notes\/espa-Notes.pdf\r\n   70 pages of slides, good overview, first part is effect size measure, multiple regression starting on p. 63, \r\nHe also has rcode and slides on other topics http:\/\/users.stat.umn.edu\/~helwig\/teaching.html\r\n(e.g. inference for multivariate means might be interesting.)\r\n\r\npartially related: effect size in meta-analysis https:\/\/bookdown.org\/MathiasHarrer\/Doing_Meta_Analysis_in_R\/es-calc.html\r\nlow in formulas, references to text book\r\nLipsey, Mark W, and David B Wilson. 2001. Practical Meta-Analysis. SAGE.\r\n\r\nslides for GPower https:\/\/med.und.edu\/research\/daccota\/_files\/pdfs\/berdc_resource_pdfs\/sample_size_gpower_module.pdf\r\nmostly point-and-click instructions\r\noverview tables of hypothesis tests on p. 10 and 11\r\n\r\n","(random thought)\r\n\r\nt-test with unequal variance (Welch, HC):\r\nI think we need var_ratio as fixed keyword argument in power function.\r\nIt's possible to use the effect size with unequal variance, but if nobs-ratio changes, then this effect size will also change.\r\nSimilar to current computation of std correction for nobs1, nobs2 inside TTestIndPower class. \r\n`nobs = 1.\/ (1. \/ nobs1 + 1. \/ nobs2)`\r\nbut in the unequal case, we will need to include var_ratio.\r\nThe optimal nobs_ratio should then depend on the var_ratio.\r\n\r\n","semi-random idea\r\n\r\nuse keyword value \"required\" in solve_power, e.g. base rate or proportion for poisson, binomial\r\n\r\n`PoissonPower().solve_power(nobs=None, ..., base_rate=\"required\", ...)`\r\nalternative would be to set it in `__init__`\r\n`PoissonPower(base_rate=None).solve_power(nobs=None, ...,)`\r\n\r\nThe first is more flexible if we want to loop over a required keyword, and power method itself will be vectorized in the required keyword in many cases, but not in string keywords like `method` and `compare`.\r\n\r\nName of class would more likely be `Poisson2indepPower`, `PoissonRegressionPower`... for different sampling cases or models.\r\n","not clear to me or I don't remember\r\n\r\nHow are effect size and power defined for `alternative=\"smaller\"` ?\r\nDoes solve_power look for a negative effect size?\r\n\r\ne.g. hypothesis tests with inherent heteroscedasticity:\r\n\r\nHow does it work if we use \"ratio\" or \"diff\" as (raw) effect size for Binomial and Poisson power?\r\nMinimum detectable effect can be greater or smaller 1 or 0 resp, but only one side will have power > alpha with one-sided alternative. \r\nWith two-sided alternative, there will be a difference between \"negative\" and \"positive\" effect when std and var depend on the mean values (GLM type). \r\nIf we don't add a keyword option for which side we should compute, then we would have to compute both sides in some cases.\r\n\r\nIn cases other than solving for minimum detectable effect, there should not be a problem because users specify all rates or proportions under the alternative.\r\n\r\nAFAICS, TestTTPowerTwoSx for two sample t-test do not include unit tests for alternative=\"smaller\" (two-sided and larger are included)\r\nWhat happens if effect size is negative? \r\n\r\nHowever, docstring for effect size of TTestIndPower says  \"`effect_size` has to be positive.\"\r\n\r\nAlso, currently we don't have a null_value (for a margin under the null) in the power classes. In t-test it can be subsumed under the effect size (AFAIU, AFAIR), but not for full poisson, binomial tests. (inferiority, superiority)\r\n\r\nAlso, no power classes yet for any TOST, AFAICS. (need to check what interface R powerTOST package is using, I never looked at it AFAIR)\r\n\r\n\r\n"],"labels":["type-enh","comp-stats"]},{"title":"BUG\/REF\/SUMM: problem I mix up df_num and df_denom very often","body":"AFAICS, FTestPower has df_num and df_denom reversed\r\n\r\nNote:\r\nratio = numerator \/ denominator\r\n\r\nscipy F distribution  has dfn and dfd as shape parameters\r\n\r\nin testing constraints\r\ndf_num = df_constraints = number of constraints\r\ndf_denom = df_resid of full model, nobs - k\r\n\r\nTBC","comments":["so far it looks like in statsmodels.stats it's only the F power functions that have it reversed\r\n\r\nin oneway:\r\n `anova_oneway has it reversed in the docstring returns, but not in the code\r\n`df = (df_denom, df_num) : tuple of floats`\r\nanova_generic returns HolderTuple with attribute `df=(df_num, df_denom)`\r\n\r\nin power:\r\nhelper function `ftest_anova_power` uses reversed arg sequence\r\n```\r\n    crit = stats.f.isf(alpha, df_denom, df_num)\r\n    pow_ = stats.ncf.sf(crit, df_denom, df_num, effect_size**2 * nobs)\r\n```\r\nand same in `ftest_power` function\r\n```\r\n    crit = stats.f.isf(alpha, df_denom, df_num)\r\n    pow_ = stats.ncf.sf(crit, df_denom, df_num, nc)\r\n```\r\n\r\n`ftest_anova_power` has no bug, incorrect naming is only inside function, args are in terms of nobs and k_groups\r\n\r\nhowever `ftest_power` and the corresponding `FTestPower` methods have the \"naming bug\" with df_num, df_denom  as keywords in signature\r\n\r\nThe `ftest_power` function uses only positional args, so we can change it without backwards compat problems. Except for users that use keywords for positional. I switch to using df1, df2, so using df_num or df_denom as keyword will raise an exception.\r\nNot really, problem is that the backwards compatible arg sequence would be `df2, df1`."],"labels":["type-bug","comp-stats"]},{"title":"SUMM\/DOC: functions and classes not in docs","body":"We don't have a central summary issue for things, functions and classes, that are left out of the docs, or those are spread out over issues and comments.\r\n(I don't have an overview.)\r\nSome might be left out intentionally, many might have been left out by accident.\r\nWe should make a note of those, and might require checking of the api and unit test coverage of those functions and classes.\r\n\r\nNote, private methods and functions are mostly left out intentionally.\r\n\r\n- [ ] impute_ros #7961\r\n- [ ] ...\r\n\r\nAside: I had written scripts\/notebooks to crawl the modules for content. (mostly notebooks for specific cases and not reusable and runnable scripts)\r\nI had used it to compare with `__all__` in modules and with `api` modules.\r\nWe need something similar for comparing module content with doc toc content.\r\n\r\n\r\n\r\n","comments":["another curious observation:\r\n\r\ncodecov has 128960 tracked lines\r\nopenhub has 227,174 python code lines\r\nwhy the large discrepancy?\r\n\r\nhttps:\/\/app.codecov.io\/gh\/statsmodels\/statsmodels\r\nhttps:\/\/www.openhub.net\/p\/statsmodels\/analyses\/latest\/languages_summary\r\n\r\nAFAICS, codecov does not include results directories in test directories, but that shouldn't be anywhere near the 100,000 missing lines.\r\n"],"labels":["comp-docs","maintenance"]},{"title":"DOC: improve docs for power classes","body":"power functions and classes are difficult to use because there is not clear description of which effect size is required.\r\n\r\ntodo:\r\n\r\n- add notebook for standard cases, several issues for missing notebooks (1 PR)\r\n- add to docstrings for effect size which effect size is required for the power function https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8646#issuecomment-1416132114\r\n- examples in docstrings, or related functions (e.g. oneway has power example in effectsize function)\r\n\r\n#8162 incomplete overview issue, what's available\r\n\r\nrelated\r\na stats.stackexchange answer for effectsize in FTestAnovaPower\r\nhttps:\/\/stats.stackexchange.com\/questions\/544027\/different-results-for-anova-power-calculation-by-r-and-statsmodels\/544144#544144\r\n\r\n\r\n","comments":[],"labels":["comp-docs","comp-stats","prio-elev"]},{"title":"ENH: add force_uniform to BernsteinDistribution from_data","body":"If I want an empirical Berstein copula, then it should have uniform margins.\r\n\r\nBernsteinDistribution from_data currently does not have and option to impose uniform margins\r\n`force_uniform` is option in distribution tools `approx_copula_pdf`\r\n\r\nexample: using rvs from a copula to generate data\r\nrvs is sample drawn from gaussian copula k_dim=3\r\n\r\n\r\n```\r\nfrom statsmodels.distributions.bernstein import BernsteinDistribution\r\nimport statsmodels.distributions.tools as smdt\r\n\r\nbern = BernsteinDistribution.from_data(rvs, 4)\r\npdfb = smdt.cdf2prob_grid(bern.cdf_grid, prepend=None)\r\nfreq = smdt.frequencies_fromdata(rvs, 4, use_ranks=True) \/ nobs\r\n\r\npdfb.sum(0).sum(1), freq.sum(0).sum(1)\r\n(array([0.2492, 0.2544, 0.2538, 0.2426]), array([0.25, 0.25, 0.25, 0.25]))\r\n\r\nbern_m2 = bern.get_marginal(2)\r\nnp.diff(bern_m2.cdf_grid)\r\narray([0.2598, 0.2506, 0.2468, 0.2428])\r\n```\r\n","comments":[],"labels":["comp-distributions","type-enh"]},{"title":"ENH: correlation, concordance for discrete distributions, pearson, spearman, kendall","body":"I want to know what the 3 correlation coefficients are for a bivariate multinomial distribution given by probabilities.\r\nNeed some helper functions for this.\r\n\r\nusecase: checking correlation, rho, tau of discretized copula in PR #8642\r\nactually I have histogram copula, but continuous case might be difficult, too much numerical integration (for stepwise constant pdf)\r\n\r\nsome references, I did not look at them\r\n\r\nBouezmarni, Taoufik, Mhamed Mesfioui, and Abdelouahid Tajar. \u201cOn Concordance Measures for Discrete Data and Dependence Properties of Poisson Model.\u201d Journal of Probability and Statistics 2009 (January 27, 2010): e895742. https:\/\/doi.org\/10.1155\/2009\/895742.\r\n\r\nMesfioui, Mhamed, and Jean-Fran\u00e7ois Quessy. \u201cConcordance Measures for Multivariate Non-Continuous Random Vectors.\u201d Journal of Multivariate Analysis 101, no. 10 (November 1, 2010): 2398\u20132410. https:\/\/doi.org\/10.1016\/j.jmva.2010.06.011.\r\n\r\nPerrone, Elisa, Edwin R. van den Heuvel, and Zhuozhao Zhan. \u201cKendall\u2019s Tau Estimator for Bivariate Zero-Inflated Count Data.\u201d arXiv, August 5, 2022. https:\/\/doi.org\/10.48550\/arXiv.2208.03155.\r\n","comments":["Mesfioui, Quessy  2010 intro\r\n\"\r\nNelsen [7] defined multivariate concordance\r\nas the probability of concordance among all d(d \u2212 1)\/2 possible pairs. Then one can show that the concordance between X1\r\nand X2 is\r\nQ(H1, H2) = P(X1 < X2) + P(X2 < X1),\r\n\"\r\n\r\nI have this in brunner munzel, stochastic larger for multinomial (but only for two independent rv).\r\nHere we want it for correlated rv, i.e. r x k contingency table, bivariate multinomial\r\n"],"labels":["comp-distributions","type-enh","comp-stats"]},{"title":"Adding scipy.sparse.linalg.svd as an option for OLS","body":"#### Is your feature request related to a problem? Please describe\r\nWhile doing regressions with large matrices, using standard svd from numpy uses too much RAM. From this [post](https:\/\/fa.bianp.net\/blog\/2012\/singular-value-decomposition-in-scipy\/) using scipy.sparse.linalg.svds can also be a solution that uses less RAM. \r\n \r\n#### Describe the solution you'd like\r\nAdd an option to choose the svd algorithm as an optional parameter when using OLS.\r\n\r\n#### Describe alternatives you have considered\r\nI am not aware of any other optimizations. Feel free to suggest other ways to improve RAM usage.\r\n\r\n#### Additional context\r\nI can't do OLS with a 120k datapoints with 8GB RAM.","comments":[],"labels":["type-enh","comp-regression"]},{"title":"ENH: meta-analysis for risk difference","body":"(parking a reference)\r\n\r\nThis article looks like it has a good overview of methods for meta-analysis of risk difference, based on a very brief look at the article.\r\n\r\nGuo, Juanru, Mengli Xiao, Haitao Chu, and Lifeng Lin. \u201cMeta-Analysis Methods for Risk Difference: A Comparison of Different Models.\u201d Statistical Methods in Medical Research 32, no. 1 (January 1, 2023): 3\u201321. https:\/\/doi.org\/10.1177\/09622802221125913.\r\n\r\nrelated issues\r\n#7184 2x2xk stratified proportions\r\n#6585 basic meta-analysis issues\r\n","comments":["related OR or RR, or RD: which is \"portable\", independent of baseline risk\r\n\r\nseries of articles or comments in Debates and Controversy\r\nlast section in https:\/\/www.sciencedirect.com\/journal\/journal-of-clinical-epidemiology\/vol\/142\/suppl\/C\r\n\r\n(I only read parts of a few abstracts)\r\nIt looks like nothing is \"portable\" in general, \r\nalternatives ???  last article seems to have suggestions\r\n(my guess if baseline risk varies a lot across studies, then assuming any of those comparisons are constant is shaky, \r\nI guess we just get some kind of weighted average treatment effect.\r\n)  \r\n\r\nDoi, Suhail A., Luis Furuya-Kanamori, Chang Xu, Tawanda Chivese, Lifeng Lin, Omran A. H. Musa, George Hindy, Lukman Thalib, and Frank E. Harrell. \u201cThe Odds Ratio Is \u2018Portable\u2019 across Baseline Risk but Not the Relative Risk: Time to Do Away with the Log Link in Binomial Regression.\u201d Journal of Clinical Epidemiology 142 (February 1, 2022): 288\u201393. https:\/\/doi.org\/10.1016\/j.jclinepi.2021.08.003.\r\n\r\nDoi, Suhail A., Luis Furuya-Kanamori, Chang Xu, Lifeng Lin, Tawanda Chivese, and Lukman Thalib. \u201cControversy and Debate: Questionable Utility of the Relative Risk in Clinical Research: Paper 1: A Call for Change to Practice.\u201d Journal of Clinical Epidemiology 142 (February 1, 2022): 271\u201379. https:\/\/doi.org\/10.1016\/j.jclinepi.2020.08.019.\r\n\r\nWells, George A. \u201cCommentary on Controversy and Debate 4 Paper Series: Questionable Utility of the Relative Risk in Clinical Research.\u201d Journal of Clinical Epidemiology 142 (February 1, 2022): 268\u201370. https:\/\/doi.org\/10.1016\/j.jclinepi.2021.09.016.\r\n\r\nXiao, Mengli, Yong Chen, Stephen R Cole, Richard F MacLehose, David B Richardson, and Haitao Chu. \u201cControversy and Debate: Questionable Utility of the Relative Risk in Clinical Research: Paper 2: Is the Odds Ratio \u2018Portable\u2019 in Meta-Analysis? Time to Consider Bivariate Generalized Linear Mixed Model.\u201d Journal of Clinical Epidemiology 142 (February 1, 2022): 280\u201387. https:\/\/doi.org\/10.1016\/j.jclinepi.2021.08.004.\r\n\r\nXiao, Mengli, Haitao Chu, Stephen R. Cole, Yong Chen, Richard F. MacLehose, David B. Richardson, and Sander Greenland. \u201cControversy and Debate\u202f: Questionable Utility of the Relative Risk in Clinical Research: Paper 4\u202f:Odds Ratios Are Far from \u2018Portable\u2019 \u2014 A Call to Use Realistic Models for Effect Variation in Meta-Analysis.\u201d Journal of Clinical Epidemiology 142 (February 1, 2022): 294\u2013304. https:\/\/doi.org\/10.1016\/j.jclinepi.2021.08.002.\r\n","I looked by chance briefly at the debate again.\r\n\r\nCan we get a measure, plots about whether OR or RR is more \"stable\" across baseline risk?\r\n\r\nWhich parameter or statistic is constant depends on the link function, logit, log or identity (for risk difference).\r\nCan we use some diagnostics on link functions to check this?\r\nIs there a link function that encompasses all 3 links?\r\nSimplest test is RESET test for link misspecification, maybe interacted with binary treatment regressor.\r\nAnother possibility is LR test for non-nested models (Vuong)\r\n\r\nrelated: marginal effects that depend on the baseline\/fittedvalue, e.g. for interaction effect https:\/\/github.com\/statsmodels\/statsmodels\/issues\/5387#issuecomment-1476985078\r\n\r\nGreene, William. \u201cTesting Hypotheses about Interaction Terms in Nonlinear Models.\u201d Economics Letters 107, no. 2 (May 1, 2010): 291\u201396. https:\/\/doi.org\/10.1016\/j.econlet.2010.02.014.\r\n\r\nBuis, Maarten L. \u201cStata Tip 87: Interpretation of Interactions in Nonlinear Models.\u201d The Stata Journal 10, no. 2 (July 2010): 305\u20138. https:\/\/doi.org\/10.1177\/1536867X1001000211.\r\n\r\nanother possibility\r\ntry to get nonparametric effects OR, RR, RD, and compare stability across subgroups.\r\n\r\n"],"labels":["type-enh","comp-stats"]},{"title":"ENH: rvs simulate random variables uniform on simplex","body":"used, e.g., for simulating multivariate archimedean copulas\r\nfound in some copula article (???)\r\n\r\nneed to use exponential\r\n\r\n```\r\ndef runif_in_simplex(n):\r\n  ''' Return uniformly random vector in the n-simplex '''\r\n\r\n  k = np.random.exponential(scale=1.0, size=n)\r\n  return k \/ sum(k)\r\n```\r\n\r\nhttps:\/\/stackoverflow.com\/a\/67202070\/333700\r\n\r\nmaybe for some `statsmodels.distribution._tools` module with misc functions (for internal reuse or as recipes if inlined)\r\n\r\nalso available with dirichlet all params = 1\r\nhttps:\/\/math.stackexchange.com\/questions\/32618\/uniform-distribution-on-a-simplex-via-i-i-d-random-variables\r\nnumpy has a dirichlet in np.random\r\n\r\n\r\n","comments":["FYI: If you implement a uniform sampler for a standard simplex, you might want to experiment a bit with both the exponential-based version and the Dirichlet-based version.  For large samples, I've found that using the Dirichlet distribution is somewhat faster.  For smaller samples, the exponential-based version is faster.  Also, for an exponential-based sampler, the axis along which the samples are stored can make a nontrivial difference in the performance.\r\n\r\nE.g.\r\n\r\n```\r\ndef uniform_simplex1(n, size, rng):\r\n    p = rng.exponential(size=(size, n))\r\n    p \/= p.sum(axis=-1, keepdims=True)\r\n    return p\r\n\r\ndef uniform_simplex2(n, size, rng):\r\n    p = rng.exponential(size=(n, size))\r\n    p \/= p.sum(axis=0)\r\n    return p.T\r\n```\r\n\r\nTo generate 25000 samples, `rng.dirichlet` is fastest.  Also note that the second version of the exponential-based generator is faster than the first.\r\n\r\n```\r\nIn [80]: rng = np.random.default_rng()\r\n\r\nIn [81]: %timeit uniform_simplex1(5, 25000, rng)\r\n1.24 ms \u00b1 1.76 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [82]: %timeit uniform_simplex2(5, 25000, rng)\r\n786 \u00b5s \u00b1 198 ns per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\nIn [83]: %timeit rng.dirichlet(np.ones(5), size=25000)\r\n655 \u00b5s \u00b1 13.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\r\n\r\n```\r\n\r\nBut for just 3 samples, using `exponential` and normalizing is faster:\r\n\r\n```\r\nIn [86]: %timeit uniform_simplex1(5, 3, rng)\r\n3.18 \u00b5s \u00b1 7.75 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\nIn [87]: %timeit uniform_simplex2(5, 3, rng)\r\n3.1 \u00b5s \u00b1 4.75 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n\r\nIn [88]: %timeit rng.dirichlet(np.ones(5), size=3)\r\n7.65 \u00b5s \u00b1 165 ns per loop (mean \u00b1 std. dev. of 7 runs, 100,000 loops each)\r\n```\r\n\r\nThis caught my eye because I had been experimenting with uniformly sampling from a simplex a few months ago, and in one of my small experimental repos, [`numerand`](https:\/\/github.com\/WarrenWeckesser\/numerand), I added code to sample uniformly from a set of nonoverlapping simplices.  I used the Dirichlet-based method there, mainly because I wasn't aware of the exponential-based method.\r\n"],"labels":["comp-distributions","type-enh"]},{"title":"DOC:  section for \"generic\" topics, fit, post-estimation, ...","body":"Stata has description for generic, model independent topics.\r\n\r\nWe should have something similar for features that are common to many models.\r\n\r\nThe docs have a background section with just general comments and description of basic structure\r\nhttps:\/\/www.statsmodels.org\/dev\/user-guide.html#background\r\n\r\nWe could add a separate section, or we add it to background section.\r\n\r\npossible candidates are `fit`, `cov_type` and various post-estimation topics\r\n\r\n\r\nAside:\r\nIt would also be good to add a section \"Watch out. Frequent beginner mistakes\"\r\ne.g. add_constant, GLM is one-parameter family (e.g. negbin does not estimate dispersion parameter)\r\nexog is not transformed\/changed, e.g. bad scaling, (near) singularity\r\n","comments":[],"labels":["type-enh","comp-docs"]},{"title":"ENH: `fit` docstrings, generic cov_type docstring template","body":"generic fit methods are still missing description for `cov_type` options\r\n\r\ndefine it as a separate string that can be used in templated docstrings\r\n\r\nwe also need a pattern to add model specific cov_types to the fit docstrings\r\n","comments":[],"labels":["type-enh","comp-docs","comp-base","prio-elev"]},{"title":"ENH\/SUMM: extensions to copulas, missing parts","body":"(need a list of parts missing or that we or I want to have)\r\n\r\n\r\n- extend archimedean to k_dim > 2, see #8631, \r\n  - [x] pdf k_dim <= 4 #8633\r\n  - [ ] pdf #8649\r\n  - [x] rvs, #8642\r\n  - [ ] subset, marginal conditional\r\n  - [ ] ...\r\n- [ ] extreme value\r\n   - [ ] rvs\r\n   - [ ] conditional u2 | u1 \r\n   - [ ] ...\r\n- [ ] Bernstein, advertise, examples, usage\r\n- [ ] diagnostics for multivariate copulas e.g. (*1)\r\n- [ ] corner cases, methods not defined at boundaries, needs checking, unit tests, raising exceptions(?)\r\n  - [x] #8662 \r\n  - [ ] cdf at a zero in archimedean\r\n  - [ ] limit of copula args towards independence  (e.g. R switches to independence copula by default at independence arg)\r\n  - [ ] negative correlation if k_dim=2, check, enforce valid range of copula args.\r\n- [ ] ...\r\n\r\ngeneral formulas, generic implementations\r\ne.g. \r\n`sf` survival copula\r\nnumerical derivatives as fallback\r\nconditional, and marginal subset formulas\r\n\r\n*1 \r\nMarius Hofert & Martin M\u00e4chler (2014) A Graphical Goodness-of-Fit Test for\r\nDependence Models in Higher Dimensions, Journal of Computational and Graphical Statistics,\r\n23:3, 700-716, DOI: 10.1080\/10618600.2013.812518\r\nhttps:\/\/doi.org\/10.1080\/10618600.2013.812518\r\n\r\nand section in their book for copulas in R\r\n","comments":["example with conditional distribution F(y2 | y1)\r\nhttps:\/\/stats.stackexchange.com\/questions\/308775\/conditional-expectation-of-two-identical-marginal-normal-random-variables\/308831#308831","aside: python copulas package switched to incompatible license on Dec 21, 2022\r\nhttps:\/\/github.com\/sdv-dev\/Copulas\/commit\/8cc4336d113ea4008ccaab16c7c500b4a2a33d91#diff-c693279643b8cd5d248172d9c22cb7cf4ed163a3c98c8a3f69c2717edd3eacb7\r\n\r\nno more code sharing\r\n"],"labels":["comp-distributions","type-enh","roadmap"]},{"title":"Copula density not correct for d>2","body":"It seems that the pdf of some Archimedian Copulas are incorrect for d>2. They do not throw an error or warning in this case. The formulas look generic for any dimension at first but are restricted to d=2. At least for Gumbel and Clayton.\r\nI compared with this: https:\/\/arxiv.org\/pdf\/1207.1708.pdf\r\nFor Frank there is a dim check (\"pdf is currently only available for bivariate copula\").\r\n\r\n```\r\nfrom statsmodels.distributions.copula.archimedean import GumbelCopula\r\nc=GumbelCopula()\r\nc.pdf([.7,.7,.7],20)\r\n> array([3.97063356e-08])\r\n```\r\nFor example in this case, a high value for the density would be expected.\r\n\r\nAlso it would be good to know the source where the formulas have been taken from.\r\n","comments":["formulas were taken from several books and articles, main books AFAIR, Nelson and two books by Joe\r\nsome docstrings have references, and code comments in a few places\r\nThe formulas for Gumbel and Clayton might have been copied from an older (license compatible) python package.\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/4046#issuecomment-758121868\r\nmentions Nelson\r\narchimedean module mentions Joe 2014 and Joe 1997\r\n\r\nI don't remember about the explicit pdf formulas in Gumbel and Clayton archimedean (nor whether I or tupui wrote the code)\r\n\r\nI had read the referenced article, but didn't implement a way to compute higher dimensional derivatives for archimedean copulas.\r\nNote, the Swiss authors use `psi` as generator while we use `phi`, where one is the inverse function of the other.\r\n \r\n\r\n\r\n\r\n","AFAICS, we only have unit test for bivariate Gumbel\r\nIt looks like all copula unit tests are only for bivariate case, dim=2","Looks like a bug, thanks for reporting\r\n\r\nvery high agreement to R copula package for dim=2, completely different for dim=3\r\nIt's not clear to me what's the problem, the formula should be independent of dim. ???\r\n\r\n```\r\nfrom statsmodels.distributions.copula.archimedean import GumbelCopula\r\nc=GumbelCopula()\r\nc.pdf([.7,.7,.7],20)  # should be 469.1509239922922 from R copula\r\n# > array([3.97063356e-08])\r\narray([3.97063356e-08])\r\n \r\npdfval = c.pdf([.7,.7],20)\r\npdfval[0], 19.82764498336631, pdfval[0] - 19.82764498336631\r\n(19.82764498336638, 19.82764498336631, 7.105427357601002e-14)\r\n```","setting k_dim = 3 issues a warning\r\nThe warning is standard UserWarning and warns only once. Maybe we want to use a statsmodels warning class.\r\n\r\n```\r\nc=GumbelCopula(k_dim=3)\r\n...\\statsmodels\\distributions\\copula\\copulas.py:269: UserWarning: copulas for more than 2 dimension is untested\r\n  warnings.warn(\"copulas for more than 2 dimension is untested\")\r\n```","I have not found a source for the gumbel and clayton pdf formulas yet.\r\nThe formulas in the arxiv paper and an earlier article differs from those implemented.\r\nOur formulas might drop a term that is irrelevant for dim=2 or hardcode some part for dim=2.\r\n(I had assumed the formula is for any k_dim, since we just use prod and sum accumulation.\r\n\r\npage 138 in\r\nHofert, M., M\u00e4chler, M., and McNeil, A. J. (2012), Likelihood inference for Archimedean\r\ncopulas in high dimensions under known margins, Journal of Multivariate Analysis,\r\n110, 133\u2013150, doi:10.1016\/j.jmva.2012.02.019.\r\n","Ok, \r\nAs long as we don't have the appropriate extension to k_dim > 2, we should raise an exception.\r\nThe warning occurs in `__init__`,  The cdf is generic for any k_dim in archimedean, so still useful\r\n\r\nSo we can raise in methods if the evaluation point `u` does not have `u.shape(-1) == self.k_dim`\r\ncurrently we only use `u = np.atleast_2d(u)` but no checks in pdf and other methods.\r\n\r\n`logpdf` of superclass ArchimedeanCopula already does this, it raises explicitly for dim >2, independently of k_dim\r\n\r\n```\r\n        if u.shape[-1] > 2:\r\n            msg = \"pdf is currently only available for bivariate copula\"\r\n            raise ValueError(msg)\r\n```","Reference\r\n\r\nJoe 2014, Dependence Modeling with Copulas, has explicit formula for density of bivariate Gumbel copula, page 172, but no density for multivariate extensions (section 4.8.2)\r\n\r\nsame for Clayton\r\nbivariate density p. 168 is the same as the formula in our code\r\nJoe's full name for \"clayton\" is Bivariate Mardia-Takahasi-Clayton-Cook-Johnson","It's worse with clayton and gumbel\r\n**correction** cdf for gumbel for k_dim=3 is correct, formula does not have k_dim in it\r\n\r\nEven cdf is only correct for k_dim=2\r\ncdf of those two copulas use explicit expressions and not the generic generator formula. Those formulas are hardcoded for dim=2\r\n\r\nFrank has `dim` in the explicit cdf formula, but `dim = u.shape[-1]` independent of `self.k_dim`.\r\ni.e. we are not restricting `u` to be consistent with `self.k_dim` (Initially I did not have a k_dim in `__init__`)\r\n\r\n","Aside:\r\nExtremevalue copula will implicitly raise if `u` is not for bivariate because of tuple unpacking.\r\n`u1, u2 = np.asarray(u).T`\r\n\r\nto unit tests:\r\n\r\nmy test for archimedean `test_copulas` checks the generic generator construction, but it's also only for bivariate cases in `cop_list`\r\n\r\n","another aside:\r\narchimedean generator classes in `transforms` don't have docstrings.\r\nnot clear is `deriv` the derivative of phi or psi (`evaluate` is phi, `inverse` is psi), but AFAICS, Hofert et al have derivative of psi.\r\n\r\nbased on ArchimedeanCopula.pdf implementation it's phi_deriv\r\nbut AFAIU my old code, the last part computes the second derivative of the inverse function (psi) from the derivatives of phi\r\n        pdfv *= phi_d2(cdfv, *args)\r\n        pdfv \/= phi_d1(cdfv, *args)**3\r\n\r\nin analogy to link classes, we need deriv_inverse or deriv2_inverse\r\nFor pdf we need 1st derivative of phi, and k_dim'th derivative of the inverse function psi. \r\n\r\n(IIRC, Before I stopped working on copulas, I was trying to get numerical derivatives out of mathematica, or Wolfram alpha)\r\n\r\n","Wouldn't it be possible to implement the density formulas of Hofert et al for the multivariate pdf?\r\n\r\nside note:\r\nI looked at the package \"pycopula\" they use a numeric approach for n>2 but I am only getting complex numbers.","> Wouldn't it be possible to implement the density formulas of Hofert et al for the multivariate pdf?\r\n\r\nYes, that would be best. But it's work that someone needs to do.\r\nAdding copulas was already a lot of work and I ran out of time and patience to work on more extensions.\r\n\r\n> I looked at the package \"pycopula\" they use a numeric approach for n>2 but I am only getting complex numbers.\r\n\r\nI saw this, but I worried that numerical approximations for higher order might not be very accurate. And the Hofert et al makes it unnecessary for the standard archimedean.\r\n\r\n(Another thing missing are conditional distributions of second dimension or random variable given the first. I had added it only to Frank) ","looking again at the special function used by Hofert et al for derivatives\r\n\r\npolylogarithm Li doesn't look so bad, It's not available in scipy.special, it could be computed from Hurwitz zeta function.\r\nHowever, AFAICS we only need it for negative integers where it is a finite sum with Sterling second kind numbers.\r\nhttps:\/\/en.wikipedia.org\/wiki\/Polylogarithm#Particular_values\r\n\r\nSterling numbers are also not in scipy.special.\r\nAFAICS, we can precompute them for given k_dim, independently of evaluation points of pdf or generator function.\r\n I have not checked yet which indexing we need or whether we can vectorize the required Sterling number computation.","parking this here for now\r\n\r\nSterling code is from Rosetta Code converted to class for cache\r\npolylog Li code formulas from Wikipedia\r\n(I also have a brute force polynomial series function for Li, but finding truncation point is non-trivial, and not needed for negative integer values)\r\n\r\nn = k_dim, but Li argument is -n (defined for first argument is negative integer)\r\n\r\n```\r\nclass Sterling1():\r\n    \r\n    def __init__(self):\r\n        self._cache = {}\r\n    \r\n    def __call__(self, n, k):\r\n        key = str(n) + \",\" + str(k)\r\n\r\n        if key in self._cache.keys():\r\n            return self._cache[key]\r\n        if n == k == 0:\r\n            return 1\r\n        if n > 0 and k == 0:\r\n            return 0\r\n        if k > n:\r\n            return 0\r\n        result = sterling1(n - 1, k - 1) + (n - 1) * sterling1(n - 1, k)\r\n        self._cache[key] = result\r\n        return result\r\n    \r\n    def clear_cache(self):\r\n        self._cache = {}\r\n\r\nsterling1 = Sterling1()\r\n\r\n\r\nclass Sterling2():\r\n    \r\n    def __init__(self):\r\n        self._cache = {}\r\n    \r\n    def __call__(self, n, k):\r\n        key = str(n) + \",\" + str(k)\r\n\r\n        if key in self._cache.keys():\r\n            return self._cache[key]\r\n        if n == k == 0:\r\n            return 1\r\n        if (n > 0 and k == 0) or (n == 0 and k > 0):\r\n            return 0\r\n        if n == k:\r\n            return 1\r\n        if k > n:\r\n            return 0\r\n        result = k * sterling2(n - 1, k) + sterling2(n - 1, k - 1)\r\n        computed[key] = result\r\n        return result\r\n\r\n    def clear_cache(self):\r\n        self._cache = {}\r\n        \r\nsterling2 = Sterling2()\r\n\r\n\r\ndef li3(z):\r\n    return z * (1 + 4 * z + z**2) \/ (1 - z)**4\r\n\r\n\r\ndef li4(z):\r\n    return z * (1 + z) * (1 + 10 * z + z**2) \/ (1 - z)**5\r\n\r\n\r\ndef lin(n, z):\r\n    \"\"\"Polylogarithm for negative integer\r\n    \"\"\"\r\n    if np.size(z) > 1:\r\n        z = np.array(z)[..., None]\r\n        # raise ValueError(\"currently only for scalar z\")\r\n        \r\n    k = np.arange(n+1)\r\n    st2 = np.array([sterling2(n + 1, ki + 1) for ki in k]) \r\n    res = (-1)**(n+1) * np.sum(factorial(k) * st2 * (-1 \/ (1 - z))**(k+1), axis=-1)\r\n    return res\r\n```","@Tobi0 \r\nby the way, Which dimension do you need for the copula?\r\n\r\nI went back to Wolfram alpha, and I can get at least a few analytical derivatives of generators.","I opened PR #8633 for bugfixes and extensions for k_dim > 2","> Which dimension do you need for the copula?\r\n\r\nat least 20 ...\r\n\r\n\r\nI think I can implement generic densities.\r\nIt's probably easier if you merge your PR before. \r\nWhere should I put the sterling code?","gumbel pdf for d=2 looks correct, it passes and passed the unit tests. Unit tests for cdf, pdf for d=2 were already pretty good.\r\nI didn't check why the formulas in the references differ.\r\n\r\nIt would be great if you can implement the densities for general k_dim.\r\nNow we have several `special` functions in copulas, so using a new `_special.py` module in copulas is better as location to collect those, and maybe at some point we can use scipy functions instead.\r\n\r\nWhen we add the general pdf to the individual archimedean copulas, then sterling numbers can be cached for fixed k_dim.\r\nI'm not sure whether we can cache the sterling numbers for the generator `transforms` because they are currently stateless (no stored attributes). I have not checked that yet.\r\n\r\nI'm soon finished with my PR and will merge it. Mainly a final check on raising exceptions for not supported cases.\r\n","brief github search for copula in python\r\n\r\nMIT licensed, multivariate rvs for archimedean, arbitrary n_dim\r\nhttps:\/\/github.com\/maximenc\/pycop\/blob\/master\/pycop\/simulation.py\r\n\r\nI guess, we mainly need to extend the sum, product over more than 2-dim in the current computation\r\n\r\nincludes Joe as additional copula\r\n","> parking this here for now\r\n> \r\n> Sterling code is from Rosetta Code converted to class for cache polylog Li code formulas from Wikipedia (I also have a brute force polynomial series function for Li, but finding truncation point is non-trivial, and not needed for negative integer values)\r\n> \r\n> n = k_dim, but Li argument is -n (defined for first argument is negative integer)\r\n> \r\n> ```\r\n> class Sterling1():\r\n>     \r\n>     def __init__(self):\r\n>         self._cache = {}\r\n>     \r\n>     def __call__(self, n, k):\r\n>         key = str(n) + \",\" + str(k)\r\n> \r\n>         if key in self._cache.keys():\r\n>             return self._cache[key]\r\n>         if n == k == 0:\r\n>             return 1\r\n>         if n > 0 and k == 0:\r\n>             return 0\r\n>         if k > n:\r\n>             return 0\r\n>         result = sterling1(n - 1, k - 1) + (n - 1) * sterling1(n - 1, k)\r\n>         self._cache[key] = result\r\n>         return result\r\n>     \r\n>     def clear_cache(self):\r\n>         self._cache = {}\r\n> \r\n> sterling1 = Sterling1()\r\n> \r\n> \r\n> class Sterling2():\r\n>     \r\n>     def __init__(self):\r\n>         self._cache = {}\r\n>     \r\n>     def __call__(self, n, k):\r\n>         key = str(n) + \",\" + str(k)\r\n> \r\n>         if key in self._cache.keys():\r\n>             return self._cache[key]\r\n>         if n == k == 0:\r\n>             return 1\r\n>         if (n > 0 and k == 0) or (n == 0 and k > 0):\r\n>             return 0\r\n>         if n == k:\r\n>             return 1\r\n>         if k > n:\r\n>             return 0\r\n>         result = k * sterling2(n - 1, k) + sterling2(n - 1, k - 1)\r\n>         computed[key] = result\r\n>         return result\r\n> \r\n>     def clear_cache(self):\r\n>         self._cache = {}\r\n>         \r\n> sterling2 = Sterling2()\r\n> \r\n> \r\n> def li3(z):\r\n>     return z * (1 + 4 * z + z**2) \/ (1 - z)**4\r\n> \r\n> \r\n> def li4(z):\r\n>     return z * (1 + z) * (1 + 10 * z + z**2) \/ (1 - z)**5\r\n> \r\n> \r\n> def lin(n, z):\r\n>     \"\"\"Polylogarithm for negative integer\r\n>     \"\"\"\r\n>     if np.size(z) > 1:\r\n>         z = np.array(z)[..., None]\r\n>         # raise ValueError(\"currently only for scalar z\")\r\n>         \r\n>     k = np.arange(n+1)\r\n>     st2 = np.array([sterling2(n + 1, ki + 1) for ki in k]) \r\n>     res = (-1)**(n+1) * np.sum(factorial(k) * st2 * (-1 \/ (1 - z))**(k+1), axis=-1)\r\n>     return res\r\n> ```\r\n\r\n@josef-pkt the stirling2 code in scipy can be used in place of what you have here (once it is merged) ~~it should be faster in most scenarios b\/c the recursive computation from Rosetta code does not cache sub-problems whereas the dp approach in the scipy.special pr will handle that easily. Also,~~\r\n it works on numpy arrays and supports all the usual broadcasting. \r\n\r\nOne question we're debating on the PR is whether negative values for stirling2 (of n or k) should be a 0 or a `Nan`. if you'd like to weigh in as a likely future user of the code that would be helpful. \r\n\r\nhttps:\/\/github.com\/scipy\/scipy\/pull\/18103 \r\n\r\n`stirling1` is going to be a separate pr but we'll get to that one too. ","@rlucas7 \r\nI'm not an expert in special functions. So, I don't have any opinion on details or support outside of our use case.\r\n\r\nA brief search finds an extension of sterling numbers to negative integer values, at least for both n, k negative and possibly for other cases. https:\/\/en.wikipedia.org\/wiki\/Stirling_number#Stirling_numbers_with_negative_integral_values\r\nI would return nan instead of zero if either k or n or both are strictly negative to avoid committing on any extension of the function that might not be \"appropriate\", future proof, supported by theory.\r\n"],"labels":["comp-distributions","type-bug-wrong"]},{"title":"ENH\/REF: refactor multitest to focus on pvalue correction","body":"related issue #8619\r\n\r\nWe should update `multitest` to focus on the pvalue correction and not on rejection decisions as in original articles.\r\n\r\nThe interface, options and returns will change, so it's better to create a new function.\r\nWe cannot change returns in a backwards compatible way.\r\n\r\ntwo tasks\r\n\r\n- return HolderTuple with corrected pvalues as main return and extra info, e.g. for fdr, or bonferroni and sidak\r\n   two stage fdr whould return more history besides alphastar, e.g. ri and ntest0 \r\n- streamline code to remove code for rejection decision\r\n- Enhancement: \r\n  - add \"prior\" null proportion, true fraction to (one-stage) fdr, and return prior-adjusted corrected pvalues.\r\n\r\n\r\nto streamlining code:\r\nfdr twostage needs to call fdrcorrection function only once, iteration only needs to update ri and ntest0 by adjusting alpha instead of adjusting corrected p-values\r\n\r\n\r\n","comments":["possible new addition (not just a small change)\r\n\r\nother estimation methods for \"prior\" null fraction could be included as option to fdrcorrection methods\r\ncp4p package has extra null option based on raw pvalues\r\nhttps:\/\/search.r-project.org\/CRAN\/refmans\/cp4p\/html\/adjust.p.html\r\nhttps:\/\/search.r-project.org\/CRAN\/refmans\/cp4p\/html\/estim.pi0.html\r\n\r\nour `local_fdr` function is based on zscores and not on p-values."],"labels":["type-enh","comp-stats","type-refactor","prio-elev"]},{"title":"DOC: review notebook weights in GLM","body":"notebook for weights in GLM seems to have an outdated section\r\n\r\nhttps:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/glm_weights.html#aggregated-data:-exposure-and-var_weights\r\n\r\n`Note: LR test agrees with original observations, pearson_chi2 differs and has the wrong sign.`\r\nplus investigation afterwards\r\n\r\nbased on a quick look pearson_chi2 is correct and agrees across equivalent representation with or without weights.\r\n(need to go through notebook to check)\r\n\r\n","comments":[],"labels":["comp-docs","comp-genmod"]},{"title":"ENH: GEE add use_t option for cov_params","body":"GEE does not have use_t option, use_t = False is hardcoded in summary, no fit option\r\n\r\nMonte Carlo comparison. they use both t and z\/normal based tests and pvalues\r\nt-test can be conservative with some of the bias corrections and small number of clusters\r\n\r\nAlso we might add more options for small sample adjusted, bias reduced cov_types\r\nrelated to variants of cluster correlation robust sandwiches, CR3, ...\r\n\r\nLi, Peng, and David T. Redden. \u201cSmall Sample Performance of Bias-Corrected Sandwich Estimators for Cluster-Randomized Trials with Binary Outcomes.\u201d Statistics in Medicine 34, no. 2 (2015): 281\u201396. https:\/\/doi.org\/10.1002\/sim.6344\r\n\r\nreference found in \r\nHuang, Shuang, Mallorie H Fiero, and Melanie L Bell. \u201cGeneralized Estimating Equations in Cluster Randomized Trials with a Small Number of Clusters: Review of Practice and Simulation Study.\u201d Clinical Trials 13, no. 4 (August 1, 2016): 445\u201349. https:\/\/doi.org\/10.1177\/1740774516643498.\r\n\r\nalso compares several small sample corrections\r\n","comments":[],"labels":["type-enh","comp-genmod","topic-covtype"]},{"title":"ENH: hc3 cov_type for GMM, GMMIVNonlinear","body":"this article has pseudo-hatmatrix for small sample corrections for HC2, HC3, ...\r\n\r\nmoment condition `z (y - g(x, beta))` i.e. nonlinear IV, but not for generic nonlinear GMM\r\n\r\nLin, Eric S., and Ta-Sheng Chou. \u201cFinite-Sample Refinement of GMM Approach to Nonlinear Models under Heteroskedasticity of Unknown Form.\u201d Econometric Reviews 37, no. 1 (January 2, 2018): 1\u201328. https:\/\/doi.org\/10.1080\/07474938.2014.999499.\r\n\r\nI did not find any HC3 equivalent for generic GMM, and nothing for more general non-gaussian MLE.\r\n(Poisson, GLM use the IRLS, WLS hatmatrix)\r\n","comments":[],"labels":["type-enh","comp-regression","topic-covtype"]},{"title":"ENH: cluster bootstrap using weights","body":"parking a reference for a useful approach to cluster bootstrap, \"resampling\" clusters\r\n\r\nInstead of actually resampling clusters which would result in different nobs in replication samples, we can use weights by cluster.\r\n\r\nWithout computational shortcut, we can just use freq_weights in GLM or cluster weights in GEE and compute full model for each replication sample.\r\nI don't know how much we gain in computational efficiency if we just adjust weights in a single model instance. We might only safe on model `__init__` setup cost.\r\n\r\nCheng, Guang, Zhuqing Yu, and Jianhua Z. Huang. \u201cThe Cluster Bootstrap Consistency in Generalized Estimating Equations.\u201d Journal of Multivariate Analysis 115 (March 1, 2013): 33\u201347. https:\/\/doi.org\/10.1016\/j.jmva.2012.09.003.\r\n\r\n\r\n","comments":["another one\r\n\r\nbalanced bootstrap\r\nEnsure that each individual\/cluster is sampled equally often in total for fixed number of bootstrap replications.\r\neg. create n_repl copies of indices or \"observations\", random shuffle and then select segments.\r\n\r\nGleason, John R. \u201cAlgorithms for Balanced Bootstrap Simulations.\u201d The American Statistician 42, no. 4 (1988): 263\u201366. https:\/\/doi.org\/10.2307\/2685134.\r\n\r\nGraham, R. L., D. V. Hinkley, P. W. M. John, and S. Shi. \u201cBalanced Design of Bootstrap Simulations.\u201d Journal of the Royal Statistical Society: Series B (Methodological) 52, no. 1 (1990): 185\u2013202. https:\/\/doi.org\/10.1111\/j.2517-6161.1990.tb01781.x.\r\n"],"labels":["type-enh","comp-base","comp-genmod","topic-covtype"]},{"title":"ENH: GEE for distributions that are not LEF\/GLM-families","body":"parking references\r\n\r\nwe already have several issues for variation on GEE\/GLM including variance modeling #2898, #5674\r\n\r\nHere the extension is to use \"standard\" GEE but with underlying score functions that are not coming from a LEF family, e.g. zero-inflated count models.\r\n\r\nHow do we reuse the pattern for GEE estimation with working correlation for score functions or moment conditions that do not come from GLM (mean, variance functions)?\r\nI don't know what the theoretical properties are. Non-LEF, non-GLM do not have the consistency properties robust to misspecification. \r\n\r\nHall, Daniel B, and Zhengang Zhang. \u201cMarginal Models for Zero Inflated Clustered Data.\u201d Statistical Modelling 4, no. 3 (October 1, 2004): 161\u201380. https:\/\/doi.org\/10.1191\/1471082X04st076oa.\r\n\r\nKong, Maiying, Sheng Xu, Steven M. Levy, and Somnath Datta. \u201cGEE Type Inference for Clustered Zero-Inflated Negative Binomial Regression with Application to Dental Caries.\u201d Computational Statistics & Data Analysis 85 (May 1, 2015): 54\u201366. https:\/\/doi.org\/10.1016\/j.csda.2014.11.014.\r\n","comments":[],"labels":["type-enh","comp-genmod","comp-othermod"]},{"title":"Feature Request: Addition of non-parametric test based on Chebyshev's Inequality","body":"#### Is your feature request related to a problem? Please describe\r\nWhenever I need to perform a non-parametric test using Chebyshev's Inequality, I have to write the equation myself. It would be good to have it as a part of statsmodel\r\n\r\n#### Describe the solution you'd like\r\nA function which takes in a distribution(array), a scalar value to be used for the test, the type of test(one tailed\/two tailed), confidence and returns whether the scalar value is within the limits\r\n\r\n#### Describe alternatives you have considered\r\nCurrently I have to write the code myself so I don't have any other alternative, would be better to have it as a part of statsmodel\r\n\r\n#### Additional context\r\nIf anyone thinks this will be helpful, do let me know, I can raise a pull request.","comments":["Can you provide an example and reference?\r\n\r\nIt\u2019s not clear to me what hypothesis test this is or to what the inequality is applied ","Hi @josef-pkt ,\r\nSimilar to the student's t-test, Chebyshev's inequality can also be used to define Confidence intervals which can be used to perform significance\/hypothesis tests. However, it is much more conservative than the student's t-test. The reason for this is that **Chebyshev's inequality doesn't assume the distribution to be normal**.\r\nGiven below is the inequality,\r\n\r\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $P(|X-\u00b5| \u2265 k\u03c3) \u2264 1\/k^2$\r\n\r\nhere, $P$ is the probability\r\n&emsp;&emsp; $X$ is the random variable\r\n&emsp;&emsp; $\u00b5$ is the mean or expected value of the distribution\r\n&emsp;&emsp; $\u03c3$ is the standard deviation of the distribution\r\n&emsp;&emsp; $k$ is an integer which can be used to find the probability of a value at $k$ standard deviations\r\n\r\nA few papers where this inequality is used are mentioned below,\r\n* [https:\/\/doi.org\/10.1109\/ICSPCC.2013.6663961](url), Credibility test for blind processing results of sinusoid using Chebyshev's Inequality\r\n* [https:\/\/doi.org\/10.1002\/for.3980080207](url), An examination of the accuracy of judgemental confidence intervals in time series forecasting\r\n* [https:\/\/doi.org\/10.1111\/j.1467-9876.2004.00428.x](URL), Chebyshev's inequality for nonparametric testing with small N and \u03b1 in microarray research\r\n* [https:\/\/doi.org\/10.23919\/FRUCT48808.2020.9087459](URL), Stream Data Preprocessing: Outlier Detection Based on the Chebyshev Inequality with Applications\r\n\r\nThere are many more papers where you can find this inequality being used especially when the distribution is not normal or if one wants to take a conservative approach to define the confidence intervals.\r\n\r\nI hope this gives a better context as to where it can be used. Do let me know if this can be a feature to the non-parametric tests of statsmodel.","I'm waiting with this for scipy https:\/\/github.com\/scipy\/scipy\/issues\/17690\r\n\r\nWe can add it to statsmodels if scipy does not include it.\r\nIf scipy includes it, then we can add it if we have more context where this fits in.\r\ne.g. simplest version as function based on summary statistic (stat, var or std) that can be used standalone or reused by function from data. (similar to what we have as `generic` functions from summary statistics).\r\n\r\nAside:\r\nmaybe add or make it usable with other variance estimates, eg. robust MAD, ...\r\n\r\n\r\n"],"labels":["type-enh","comp-stats","comp-nonparametric"]},{"title":"statsmodels.tsa.x13.x13_arima_analysis & Change Specifications","body":"#### Is your feature request related to a problem? Please describe\r\nI want to use \"X-13ARIMA-SEATS Seasonal Adjustment Program\" via Python. I know statsmodels.tsa.x13.x13_arima_analysis is the Python wrapper.\r\n\r\nUsing statsmodels, is it possible to provide the inputs (specification) files into the Seasonal adjustment procedure? \r\nEg, I want to add\/drop certain outliers or regressors. How do I go about doing it in  statsmodels?\r\n\r\nThank  you\r\n\r\n#### Describe the solution you'd like\r\nA clear and concise description of what you want to happen.\r\n\r\n#### Describe alternatives you have considered\r\nA clear and concise description of any alternative solutions or features you have considered.\r\n\r\n#### Additional context\r\nAdd any other context about the feature request here.","comments":["Do you have an example? Do you know which spec file x13 option that is?\r\n\r\nAFAICS, the spec file definition in x13 for seasaonal adjustment only allows to specify the type of outliers to handle, but not specific dates.\r\nhttps:\/\/www2.census.gov\/software\/x-13arima-seats\/x13as\/windows\/documentation\/gettingstartedx13-winx13.pdf?\r\n\r\nThere seems to be an option to include outlier dummies in the arima model. page 9\r\n","It appears to me that the Python wrapper:\nstatsmodels.tsa.x13.x13_arima_analysis\n\nhas no way to read in the spec file, is that correct?\n\nThank you\n\nOn Mon, 26 Dec 2022, 12:36 am Josef Perktold, ***@***.***>\nwrote:\n\n> Do you have an example? Do you know which spec file x13 option that is?\n>\n> AFAICS, the spec file definition in x13 for seasaonal adjustment only\n> allows to specify the type of outliers to handle, but not specific dates.\n>\n> https:\/\/www2.census.gov\/software\/x-13arima-seats\/x13as\/windows\/documentation\/gettingstartedx13-winx13.pdf\n> ?\n>\n> There seems to be an option to include outlier dummies in the arima model.\n> page 9\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8595#issuecomment-1364707159>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ADDYA4CHEAXG3PIU3GRZFFLWPBZYNANCNFSM6AAAAAATI3X5LI>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n"],"labels":["comp-tsa"]},{"title":"ENH: penalized mixin: custom penalization with model properties","body":"(semi-random idea while browsing SAS docs for logistic and firth penalization)\r\n\r\nrelated to Firth penalization #3561\r\n\r\nreference\r\nHeinze, G., and Schemper, M. (2002). \u201cA Solution to the Problem of Separation in Logistic Regression.\u201d Statistics in Medicine 21:2409\u20132419.\r\n\r\npenalized loglikelihood is\r\nlog L*(beta)  = log L*(beta) + 1\/2 log | I(beta)|\r\nwhere I(beta) is the information matrix evaluated at beta\r\n\r\nThat means we need the penalization class to have access to the model so we can reuse the hessian (OIM or EIM)\r\nThis might currently be tricky or impossible because we define the penalization instance before creating a penalized model instance.\r\n\r\nOne possibility may be to assign a `_model` attribute to the penalty instance after creating the model. This makes it a bit circulate\r\nThe penalized model instance holds the penalty instance which holds the penalized model instance as attributes.\r\nI guess the circularity cannot be avoided unless we use extra args in each method of the penalty class.\r\n\r\nNote we need the hessian\/EIM of the original model and not the penalized hessian of the penalized model.\r\n\r\n\r\n \r\n\r\n\r\n","comments":["@josef-pkt is there any quick hack to get this working for Firth regression?\r\nI'm trying to run Firth regression but I need some l-bfgs optimizer because Newton-Raphson does not converge:\r\nhttps:\/\/github.com\/jzluo\/firthlogist\/issues\/17\r\n(The same solution helped with sm.Logit already.)","I have not tried it out yet.\r\nIt's a bit tricky, but my guess is that it should work as a hack that modifies the model instance.\r\n\r\ne.g. the penalty as in https:\/\/github.com\/jzluo\/firthlogist\/blob\/master\/firthlogist\/firthlogist.py#L318\r\nwould need to call `model.hessian(params)` to get the (observed) information matrix (GLM also has EIM)\r\n\r\nI'm not sure about some details, e.g. taking derivatives\r\n`statsmodels.base._penalties.Penalty.deriv` Is it a required method or are there numerical derivative default methods in the model or mixin.\r\nAFAICS, all the current penalty classes have analytical `deriv`.\r\n\r\nAside:\r\n`exp` (in Logit, Poisson) easily overflows if exog x has large values. Trying with rescaled exog often helps in those cases.\r\n( x > 80 or x > 100 already causes optimization problems in some examples that I had used for Poisson unit tests) \r\n\r\n\r\n**update** some details on current implementation\r\n\r\n- access to unpenalized loglike, score, hessian: needs super call\r\n  `hess = super(PenalizedMixin, self).hessian(params, **kwds)` in PenalizedMixin\r\n- Penalty.deriv and Penalty.deriv2 seems to be required, called by score_obs and hessian methods of PenalizedMixin\r\n  PenalizedMixin has `hessian_numdiff` method but it's not used by generic calls, I guess I had added it for checking or unit tests.\r\n- ...\r\n\r\nto access unpenalized hessian:\r\nIf subclasses or model class want to access unpenalized loglike, score, hessian, then the super call needs to jump to the class inherited above the Mixin class.\r\nMaybe we should add `loglike_base` (or `loglike_unpenalized`) methods (and similar for score_obs, hessian) to the PenalizedMixin class so that we don't need to jump `super` classes. ","I never looked at the numerical details or properties of the Firth penalty\r\n\r\nDoesn't the penalty logdet break if information matrix is (near) singular?\r\nThis might need some additional protection for (near) singular information matrix or hessian.\r\n","> I never looked at the numerical details or properties of the Firth penalty\r\n> \r\n> Doesn't the penalty logdet break if information matrix is (near) singular? This might need some additional protection for (near) singular information matrix or hessian.\r\n\r\nI (kind of) ran into this issue that I got a negative determinant -> log not possible:\r\nhttps:\/\/github.com\/jzluo\/firthlogist\/pull\/18\r\nMy solution was to just set the penalty to 0 in such cases. No idea if that's a good idea..."],"labels":["type-enh","comp-genmod","comp-discrete","topic-penalization"]},{"title":"ENH: model averaging for GLM, discrete, nonlinear models, MLE","body":"(parking some references)\r\n\r\npreviously I only looked at model averaging in linear models. There is also a literature for Poisson, discrete and other nonlinear models.\r\n\r\nZhang, Xinyu, and Chu-An Liu 2022 target prediction, models can be from different families, e.g. Logit and Poisson\r\n\r\nZhang, Xinyu, Dalei Yu, Guohua Zou, and Hua Liang. 2016 require one family, averaging is over different models for linear predictor (including or excluding some regressors). Aggregation weights minimize penalised likelihood similar to information criteria (AIC, BIC) using average k_params (weighted sum over included models) as effective number of parameters. \r\n\r\nI didn't read more than a tiny bit of skimming.\r\n\r\nFeng, Yang, Qingfeng Liu, Qingsong Yao, and Guoqing Zhao. \u201cModel Averaging for Nonlinear Regression Models.\u201d Journal of Business & Economic Statistics 40, no. 2 (April 3, 2022): 785\u201398. https:\/\/doi.org\/10.1080\/07350015.2020.1870477.\r\n\r\nSun, Yuying, Yongmiao Hong, Shouyang Wang, and Xinyu Zhang. \u201cPenalized Time-Varying Model Averaging.\u201d Journal of Econometrics, December 2, 2022. https:\/\/doi.org\/10.1016\/j.jeconom.2022.09.007.\r\n\r\nZhang, Xinyu, and Chu-An Liu. \u201cModel Averaging Prediction by K-Fold Cross-Validation.\u201d Journal of Econometrics, May 16, 2022. https:\/\/doi.org\/10.1016\/j.jeconom.2022.04.007.\r\n\r\nZhang, Xinyu, Dalei Yu, Guohua Zou, and Hua Liang. \u201cOptimal Model Averaging Estimation for Generalized Linear Models and Generalized Linear Mixed-Effects Models.\u201d Journal of the American Statistical Association 111, no. 516 (October 1, 2016): 1775\u201390. https:\/\/doi.org\/10.1080\/01621459.2015.1115762.\r\n\r\nZhou, Jianhong, Alan T. K. Wan, and Dalei Yu. \u201cFrequentist Model Averaging for Zero-Inflated Poisson Regression Models.\u201d Statistical Analysis and Data Mining: The ASA Data Science Journal 15, no. 6 (2022): 679\u201391. https:\/\/doi.org\/10.1002\/sam.11598.\r\n\r\nZou, Jiahui, Wendun Wang, Xinyu Zhang, and Guohua Zou. \u201cOptimal Model Averaging for Divergent-Dimensional Poisson Regressions.\u201d Econometric Reviews 41, no. 7 (August 9, 2022): 775\u2013805. https:\/\/doi.org\/10.1080\/07474938.2022.2047508.\r\n","comments":["not directly for this\r\n\r\naveraging over Ridge penalization parameters in linear model\r\n\r\nI did not check how useful their averaging is. However, they have some good formulas for LOOO computation for standard Ridge. I guess it's similar to GCV but extended to Ridge estimator.\r\n\r\nZhao, Shangwei, Jun Liao, and Dalei Yu. \u201cModel Averaging Estimator in Ridge Regression and Its Large Sample Properties.\u201d Statistical Papers 61, no. 4 (August 1, 2020): 1719\u201339. https:\/\/doi.org\/10.1007\/s00362-018-1002-4.\r\n"],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"ENH: confint for probplots, qqplot","body":"two related SUMM issues #2183 and #3981\r\n(I didn't find a specific issue for confint, although I looked for those several times. AFAIR I never found reference numbers for unit tests)\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/74874235\/confidence-interval-in-normal-q-q-plot-using-statsmodels\/74874450\r\npinguin has confints for qqplot\r\n\r\nI don't know how they define those or what method they use. It looks a bit like prediction intervals.\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-graphics"]},{"title":"ENH\/TST: postestimation after fit_constrained, fit_zeros, fit_collinear","body":"I never checked which postestimation results work after imposing constraints, i.e. cov_params is rank reduced.\r\n\r\n- wald test should be fine, under the assumption that constrained params have bse=0\r\n- wald test with robust cov_type, I don't know\r\n- score_test: uses score_obs of full, unconstrained model, i.e. tests whether imposed constraints hold in the data\r\n- get_prediction should be fine, it should not be affected by singular cov_params\r\n- get_distribution: not affected, predict is not affected\r\n- get_influence ???\r\n- get diagnostic ???\r\n\r\n","comments":[],"labels":["type-enh","comp-base","type-test"]},{"title":"ENH: fit_collinear, unit tests and make public, advertise","body":" see also #4705 and #6935\r\n\r\n`_fit_collinear` is a private method\r\n\r\nWe need something public that drops collinear columns.\r\nMain work will be to add unit tests for extra keywords in `model.__init__` for `_fit_zeros` which is also private and not in docs.\r\npossible solution: add public `fit_collinear` only to models that we have checked, unit tested, but not generically in `base`.\r\n\r\ncurrent fit_collinear uses a single QR decomposition.\r\nIn a multicollinearity PR, I also have sequential dropping, new QR after dropping a column (AFAIR)\r\n\r\nI was trying out scipy pivoting QR `q, r, p = linalg.qr(a, mode='economic', pivoting=True)`\r\nbut again it looks kind of arbitrary we columns need to be dropped\r\n\r\n","comments":["for linear models like OLS\r\n\r\nI guess if users have to explicitly request method - \"qr-piv\" or \"qr-seq\", then it is explicit enough that there are multi- or perfect collinearities.\r\n\r\nrelated discussion, PR in scipy https:\/\/github.com\/scipy\/scipy\/pull\/18097\r\n"],"labels":["type-enh","comp-base","comp-regression"]},{"title":"[ENH] Add Wild Cluster Bootstrapping to `OLS`","body":"This PR adds wild cluster bootstrapping as a `cov_type` into OLS based on the package [`wildboottest`](https:\/\/github.com\/s3alfisc\/wildboottest). \r\n\r\nThis PR includes all functionality built into OLS with `cov_type` and `cov_params`. \r\n\r\nI added a test in `statsmodels\/regression\/tests\/test_robustcov.py` and updated the docstring in `linear_model.py`.\r\n\r\nA few caveats:\r\n\r\n1. Estimating standard errors and confidence intervals  is not trivial in certain instances, so `summary` only includes t-stats and p-values for now.\r\n2. The tests inherit from `CheckOLSRobustCluster` and `CheckOLSRobustNewMixin` and so tests fail for those reasons as well.\r\n3. Not sure where to include this in the docs, as from what I can see, the robust covariance docs are usually generated from docstrings?\r\n","comments":["sorry, I missed this\r\nI will try to catch up with this after release preparation","> sorry, I missed this\r\nI will try to catch up with this after release preparation\r\n\r\nNo worries, I hope you had a good new year!\r\n\r\n","I guess test runs failed with unavailable import.  azure test run is not available (anymore)","question in terms of design\r\n\r\nIf `wildboottest` function is mainly for hypothesis test on parameters, then we might want to split cov from bootstrap from a hypothesis test method.\r\nDoes `wildboottest` function work for linear\/affine hypothesis of parameters?\r\n","`linearmodels` has statsmodels as dependency, so including it here would make it available also for linearmodels.\r\nIt might require some compatibility code if model design differs.\r\n\r\n","Hi @josef-pkt , a few quick comments (I am traveling and will reply more exhaustively once back): \r\n\r\n- @amichuda and I have started an 'internal' discussion on whether to 'fully' integrate wildboottest into statsmodels, which is a charming idea. Beyond `linearmodels` integration, the key arguments against are that the package is not yet 'feature compete' (e.g. the non-clustered wild bootstrap is still in development) and that we'd like to provide an API to [WildBootTests.jl](https:\/\/github.com\/droodman\/WildBootTests.jl), which is 'best in class' in terms of speed and feature completeness (e.g. it offers highly optimized wild cluster bootstrap inference for IVs). But all of this could definitely be solved \ud83d\ude04 \r\n- I agree that having a hypothesis testing method instead of a cov method is likely a better place for wildboottest. \r\n- In principle, it would also be possible to compute a full 'bootstrapped' variance-covariance matrix, as e.g. R's `sandwich::vcovBS` does, though this is not in line with other wild cluster bootstrap implementations (e.g. boottest, WildBootTests.jl, fwildclusterboot).\r\n- In general, all sorts of linear&affine hypotheses can be supported but currently are not (tests of multiple parameters, Wald Tests). This is on the schedule for the next release. \r\n\r\nBest, Alex \r\n","Hi @josef-pkt, @amichuda and I had a brief catch up about integrating wildboottest as a module into statsmodels, and if you'd be interested, we would love to do so! \r\n\r\nIn this case, I would suggest to add the `WildboottestCL` and `WildboottestHC` methods to statsmodels and to add a hypothesis test method for OLS\/WLS instead of linking to the module via covariance methods (as covariances are only computed on the fly). \r\n\r\nwildboottest currently allows to compute percentile-t p-values via wild bootstraps (both clustered and non-clustered), mostly following algorithms in MacKinnon, Nielsen and Webb (2022). Allowing to compute confidence intervals by test inversion and affine hypotheses & Wald tests are the next thing on the to do list. In the long-term, I would further like to add support for bootstrap based multiple testing corrections, as the ones by [Romano and Wolf ](https:\/\/journals.sagepub.com\/doi\/full\/10.1177\/1536867X20976314) and [Westfall and Young](https:\/\/www.jstor.org\/stable\/2532216). \r\n\r\nI'd suggest the following steps for this PR: \r\n\r\n1. we refactor the PR to include wildboottest's core methods as a module to statsmodels. Additionally, we create a hypothesis test method for OLS \/ WLS\r\n2. we add functionality for confidence intervals & affine hypotheses\r\n3. we add functionality for Wald tests\r\n4. we add functionality for bootstrap based multiple testing corrections\r\n\r\nBased on your preferences, we could split steps into multiple separate PRs, @josef-pkt . "],"labels":["type-enh","comp-regression","prio-elev"]},{"title":"SUMM: finite number of parameters of interest, large number of counfounders","body":"Consider that we are only interested in a few parameters in a (linear or non-gaussian) regression model, but have a large number of possible extra explanatory variables (confounders\/controls).\r\n\r\nWhat's the implication for cov_params and inference for the parameter of interest if we use regularization or penalization on the extra counfounders or controls?\r\n\r\nBoth GAM and sure independence screening allow for unpenalized exog (parameters of interest).\r\n\r\nI ran into this a few times, but never looked at the theory.\r\n\r\nrecent article with references\r\n\r\nGalbraith, John W., and Victoria Zinde-Walsh. \u201cSimple and Reliable Estimators of Coefficients of Interest in a Model with High-Dimensional Confounding Effects.\u201d Journal of Econometrics 218, no. 2 (October 1, 2020): 609\u201332. https:\/\/doi.org\/10.1016\/j.jeconom.2020.04.031.\r\nuses PCA on controls\r\n\r\nan older article I saw some time ago\r\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. \"Inference on treatment effects after selection among high-dimensional controls.\" The Review of Economic Studies 81, no. 2 (2014): 608-650.\r\n\r\nthis looks also relatedd\r\n\r\nCattaneo, Matias D., Michael Jansson, and Whitney K. Newey. \u201cInference in Linear Regression Models with Many Covariates and Heteroscedasticity.\u201d Journal of the American Statistical Association 113, no. 523 (July 3, 2018): 1350\u201361. https:\/\/doi.org\/10.1080\/01621459.2017.1328360.\r\nwithout regularization of confounders\r\n\r\nwe need some explicit penalized models #7336 #4590 #7128 ... ","comments":["A very recent article (I only read abstract)\r\n\r\nMoosavi, Niloofar, Jenny H\u00e4ggstr\u00f6m, and Xavier de Luna. \u201cThe Costs and Benefits of Uniformly Valid Causal Inference with High-Dimensional Nuisance Parameters.\u201d Statistical Science 38, no. 1 (February 2023): 1\u201312. https:\/\/doi.org\/10.1214\/21-STS843.\r\n\r\none of the articles citing Moosavi et al.:\r\nTang, Dingke, Dehan Kong, Wenliang Pan, and Linbo Wang. \u201cUltra-High Dimensional Variable Selection for Doubly Robust Causal Inference.\u201d Biometrics n\/a, no. n\/a. Accessed February 16, 2023. https:\/\/doi.org\/10.1111\/biom.13625.\r\n"],"labels":["type-enh","comp-regression","topic-penalization"]},{"title":"SUMM\/ENH: multivariate endog model and infrastructure","body":"related\r\n\r\n- #3137\r\n- #4543\r\n- #3153\r\n\r\ncase to get started is exog are exogenous, i.e. no structural equation modelling\r\nHowever, we want to impose zero (or linear) restriction on B in y = B x\r\n\r\ndual representation B versus params\r\ne.g. \r\nmultinomial\/MNLogit and MANOVA use full 2-dim params\r\nOrderedModel uses 1-dim params but predicts 2-dim probabilities.\r\nSVAR has zero restrictions on \"A_endog\" and converts between 1-dim params and 2-dim parameter matrix \"A\" (or B ?)\r\nIIRC, SVAR has already many of the pieces\r\n\r\n\r\nalternative model \r\nflattened endog with cross-observation relationship\r\ne.g. flattened panel or clustered data and stacked multi-equation models.\r\n\r\nGMM: we can horizontally stack \"equations\" (as in treatment effect model) but we have no single prediction variable, i.e. predict might not be defined.\r\nBut we could have GMM models that define either univariate or multivariate predictions. Then we need to support those.\r\n\r\n\r\nmotivation:\r\nI ran by chance into the SEM, covariance pattern analysis literature, recent articles with regularization\/penalization, e.g. regularized GLS\r\n\r\nstarting points:\r\nmultivariateLS in support of ANOVA\r\nextension to MLE with zero or linear restriction on individual parameters.\r\n\r\nManova has row or column parameter restriction for wald tests, but no wald tests for individual or raveled parameter.\r\nMNLogit has ???\r\n","comments":["for linear models there will be some overlap with `linearmodels`, SUR and system of equations.\r\nBut we need infrastructure for more general cases, e.g. copula models will have 2-dim predict.\r\n\r\none possible complication: several options for objective function for same \"model\"\r\ne.g. SEM, variance structure literature uses gaussian MLE or GLS or some variation of them as objective function.\r\nWe can have several different model classes or different estimators in one model class (similar problem as in `Heckman` which is still undecided)"],"labels":["type-enh","comp-base","comp-multivariate"]},{"title":"ENH: simultaneous confint in get_prediction","body":"(just a random thought)\r\n\r\nwe still have simultaneous confint in the sandbox.\r\n\r\nsimplest would be Bonferroni if we predict just a few observations. That would be the same as using alpha \/ n.\r\n\r\nrelated: TBFound\r\n\r\naside:\r\nfor GLM, one parameter families, can we use endpoint transformation on the simultaneous confint of linpred?\r\n(I don't see why not.)\r\n\r\n","comments":[],"labels":["type-enh","comp-base","comp-regression","topic-predict"]},{"title":"ENH: OLS get_prediction `which`, endog transform","body":"Do we need more `which`, options for predicting other statistics in OLS\/WLS?\r\n\r\nexample\r\nnonlinear transformation of endog if user used that in the model, e.g. log(y) = xb + u\r\n\r\nrelated\r\nDo we need to refactor OLS\/WLS\/linear model prediction results class to make it more similar or consistent with newer generic prediction results classes?\r\n\r\nWhat do we need to change to use linear prediction results class for other models, e.g. for RLM #8304","comments":[],"labels":["type-enh","comp-regression","comp-robust","topic-predict"]},{"title":"DOC: raises and warns sections in docstrings","body":"We have now several informative warnings for, e.g., problems with data.\r\nThose would be worth adding to the docstring with some explanations providing hints for common problems.\r\nhttps:\/\/numpydoc.readthedocs.io\/en\/latest\/format.html#raises\r\n\r\nThis could provide a complement to error or warning message with more information, and alert users in advance.\r\n\r\nexample\r\ndfuller raises if series is constant, same problem would apply if rmse=0 in more general cases. For those we don't have a specific exception.\r\n\r\nConvergenceWarning in generic fit. might be worth adding. but warning name and message are pretty obvious, generic docstring would not provide much more information.\r\n\r\nmain candidates might be ValueWarnings (at least of those I recently looked at)","comments":[],"labels":["type-enh","comp-docs"]},{"title":"MAINT: Release 0.14","body":"What's left to do?\r\n\r\nrelease notes","comments":["@bashtage @ChadFulton \r\n\r\nI'm essentially done with 0.14 except for release notes. (one last merge coming up very soon)\r\nI keep looking around but I don't have anything left that needs to go into 0.14\r\n\r\nWhat do we still need to do for an 0.14rc?","Just wanted to send you some energy for a new release version. It would be really great to have an actual version to install instead of installing it from git. Especially the added MSTL got quite some attention and I found tutorials referencing this, which are already 9 month old.","There are some older PR in tsa that are most likely not in 0.13 and do not have a milestone set\r\n\r\nsome of those look large enough to get advertised in the release note\r\n- #8160\r\n- #8002\r\n- ... ?\r\n\r\nI was going partially through the list of merged PRs without milestone, trying to figure out for which version non-maintenance PRs were merged.\r\n\r\n","Hey @josef-pkt! Out of curiosity, do you see 0.14.0 being released in the near future?","Rc likely next week.  Release in a few weeks.  "],"labels":["maintenance"]},{"title":"REF: GLMInfluence class uses old get_prediction, switch linear to which","body":"GLMInfluence uses attributes of old get_prediction class that is returned if linear keyword is used.\r\n\r\nneed to switch or adjust to \"which\" keyword in get_prediction\r\n\r\nsee https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8559#issuecomment-1339755447\r\n","comments":["unit test in regression also linear DeprecationWarning, 7 times, too many get_prediction calls to catch_warning all of them.\r\n\r\nNeed checking and updating to see which calls require the old GLM get_prediction return class\r\n\r\n```\r\nstatsmodels\/regression\/tests\/test_predict.py::TestWLSPrediction::test_glm\r\n  d:\\josef\\eclipse-workspace\\statsmodels_gh\\statsmodels\\statsmodels\\genmod\\generalized_linear_model.py:896: DeprecationWarning: linear keyword is deprecated, use which=\"linear\"\r\n    warnings.warn(msg, DeprecationWarning)\r\n```"],"labels":["comp-genmod","comp-discrete","comp-stats","type-refactor","prio-elev"]},{"title":"REF: BayesMixedGLM predict still uses linear=True","body":"should switch to `predict(which=\"xxx\")`\r\n\r\nbut I never looked at BayesMixedGLM predict before","comments":[],"labels":["comp-genmod","type-refactor"]},{"title":"ENH: detectable proportion for given power, proportion_2indep","body":"```\r\nLet's say I want to compare two proportions with a z-test and I use NormalIndPower.solve_power from the \r\nstatsmodels library to solve for (standardized) effect size. \r\nHow do I convert the standardized result back into a proportion? \r\nThe standardization seems to be cohen's d?\r\n```\r\nhttps:\/\/twitter.com\/ryanstravis\r\n\r\nneeds convenience function and\/or more options for power_proportion_2indep.\r\nWe don't have Power classes yet for binomial proportions and poisson rates.\r\n\r\nI guess it's just rootfinding on a nonlinear equation. But what parameters are given, and what are the assumptions?\r\n\r\nrelated: #8159 and others (I guess)\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"ENH: add var_weights or weights to RLM","body":"(just a random idea)\r\n\r\nIf we have prior information about heteroscedasticity, then we need to include var_weights in RLM.\r\n\r\ne.g. observations are averages of subsamples with different nobs\r\nor inherent heteroscedasticity as in GLM, discrete.\r\n\r\nalso we might want to downweigh x-outliers. (as in literature for GLM)\r\nThen we need more general weights, like importance weights. How do those affect inference?\r\n\r\n**update**\r\n\r\nold issue for weights including RLM #505\r\n\r\nI found issue searching for carroll ruppert because I looked at the reference again by chance\r\n\r\nCarroll, Raymond J., and David Ruppert. \"Robust estimation in heteroscedastic linear models.\" The annals of statistics (1982): 429-441.\r\n\r\n","comments":["related: https:\/\/stackoverflow.com\/questions\/77137434\/robust-sigma-estimate-in-statsmodels-api-rlm\r\nsee comments to question\r\n\r\nmatlab multiplies weight in irls by 1 \/ (1 - h), i.e. include weighting factor for influential observations.\r\n\r\nrelated issues: weighting for influence in robust GLM\r\ni.e. add weights based on robust mahalanobis distance of exog.\r\nwhat's the status there ?\r\n#4266\r\n#3284\r\n\r\n~(I don't even remember what those weights are called, i.e. name of some author)~\r\nMallow's type weights\r\n\r\n\r\n","weights are again a bit of a mess\r\nvar_weights or freq_weights or a combination of those\r\n\r\nDu, Wiens 2000 use objective function\r\n`sum rho(resid \/ s \/ w^a) * w^(1+a)`, i.e.\r\nfreq_weights `w^(1+a)`\r\n(inverse) var_weights `w^a`\r\nthis implies factor in moment condition is always w, (power alpha `a` cancels)\r\n\"\r\nCommon choices of alpha are alpha =0;1;\u22121, yielding estimates of the Mallows, Schweppe and Hill-Ryan types,\r\n\"\r\n\r\nthey refer to Hampel et al book 1986, p 315, 316\r\nin the book they use different weights\r\nmoment condition is\r\n`w(x) * psi(r * v(x))`\r\n\"\r\nHuber (1973a) uses w ( x ) = 1, v ( x ) = 1, and \r\nMallows's and Andrews's proposals (see Hill, 1977) set v ( x ) = 1 and w ( x ) = 1, respectively.\r\nHill and Ryan (see Hill, 1977) proposed v ( x ) = w ( x ) and, finally,\r\nSchweppe (see Merrill and Schweppe, 1971; Handschin et al., 1975) suggested\r\nchoosing v ( x ) = 1 \/ w ( x ) .\r\n\"\r\n\r\nDu, Zhiyi, and Douglas P. Wiens. \u201cJackknifing, Weighting, Diagnostics and Variance Estimation in Generalized M-Estimation.\u201d Statistics & Probability Letters 46, no. 3 (February 1, 2000): 287\u201399. https:\/\/doi.org\/10.1016\/S0167-7152(99)00117-0.\r\n\r\nHampel, Frank R., Elvezio M. Ronchetti, Peter J. Rousseeuw, and Werner A. Stahel. Robust statistics: the approach based on influence functions. John Wiley & Sons, 1986.\r\n\r\n\r\nAside: Du and Wiens\r\nincludes jackknife, looo parameters with one-step formula (no looo loop)\r\nincludes also some outlier\/influence measures, studentized residuals, dffits and Cooks distance\r\n\r\n"],"labels":["type-enh","comp-robust"]},{"title":"Explosive forecast when using MAdM (ETSModel)","body":"#### Describe the bug\r\n\r\nHello, this may not be a bug but I need help trying to understand the behaviour of this specific model on this specific time series.\r\nI currently try several ETS models and choose the one with the minimal AICc. For this time series, the MAdM method is chosen.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nimport numpy as np\r\n    import pandas as pd\r\n    import matplotlib.pyplot as plt\r\n    from statsmodels.tsa.exponential_smoothing.ets import ETSModel\r\n\r\n    endog = np.array(\r\n        [\r\n            52.0,\r\n            250.43,\r\n            261.01,\r\n            265.0,\r\n            56.77,\r\n            16.0,\r\n            50.09,\r\n            65.0,\r\n            21.0,\r\n            89.5,\r\n            20.0,\r\n            1.0,\r\n            123.0,\r\n            84.0,\r\n            91.0,\r\n            163.0,\r\n            16.0,\r\n            27.0,\r\n            78.5,\r\n            38.0,\r\n            61.0,\r\n            5.0,\r\n            82.0,\r\n            15.0,\r\n        ]\r\n    )\r\n\r\n    fit = ETSModel(\r\n        endog,\r\n        error='mul',\r\n        trend='add',\r\n        seasonal='mul',\r\n        damped_trend=True,\r\n        seasonal_periods=12,\r\n        initialization_method='heuristic',\r\n    ).fit(full_output=False, disp=False)\r\n\r\n    plt.plot(endog, label='Demand history')\r\n    plt.plot(fit.fittedvalues, label='Fitted values')\r\n    plt.plot(\r\n        pd.Series(fit.forecast(12), index=range(24, 36)), label='Forecasted values'\r\n    )\r\n    plt.legend(loc='upper left')\r\n```\r\n\r\nI've added a simple plot to showcase the forecast results.\r\n![Explosive_Forecast](https:\/\/user-images.githubusercontent.com\/108351774\/205330985-9cadf577-0d24-4fa3-8b04-27c8a1adf562.png)\r\n\r\nLet me know if you need anything else to be able to explain why this behaviour occurs.\r\n<details>\r\n\r\n**Note**: As you can see, there are many issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates.\r\n\r\n**Note**: Please be sure you are using the latest released version of `statsmodels`, or a recent build of `main`. If your problem has been fixed in an unreleased version, you might be able to use `main` until a new release occurs. \r\n\r\n**Note**: If you are using a released version, have you verified that the bug exists in the main branch of this repository? It helps the limited resources if we know problems exist in the current main branch so that they do not need to check whether the code sample produces a bug in the next release.\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nA clear and concise description of what you expected to happen.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.9.12.final.0\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: 0.29.32 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\Cython)\r\nnumpy: 1.22.3 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\numpy) \r\nscipy: 1.7.3 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\scipy)  \r\npandas: 1.4.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\pandas)\r\n\r\n    dateutil: 2.8.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\dateutil)\r\npatsy: 0.5.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\patsy)  \r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.5.1 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\matplotlib)\r\n    backend: QtAgg \r\ncvxopt: Not installed\r\njoblib: 1.1.0 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\joblib)\r\n\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 8.4.0 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\IPython)\r\n    jinja2: 3.0.3 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\jinja2)\r\nsphinx: Not installed\r\n    pygments: 2.11.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\pygments)\r\npytest: 7.1.2 (C:\\Users\\user\\miniconda3\\envs\\development_FE\\lib\\site-packages\\pytest)\r\n\r\nvirtualenv: Not installed\r\n\r\n<\/details>\r\n","comments":["Any updates on this one?","Thanks for posting (and reminding) this issue. Generally I think the multiplicative models can be somewhat fragile, especially with data like yours (short dataset, complicated model, large spike at the beginning that may be an outlier), which is probably why you haven't gotten a good reply here yet.\r\n\r\nIn these scenarios I am generally not at all surprised to see that a good in-sample fit (i.e. low AICc) doesn't translate into good out-of-sample performance.\r\n\r\nHowever in this case, I think there may also potentially be a bug in the multiplicative ETS case.\r\n","Thank you for the reply, I have tried different combinations and it seems the multiplicative seasonality creates this behaviour in my case.\r\n\r\nI'll use additive models for now, but please let me know if you find any bug regarding the multiplicative ETS case.","Update on this:\r\n\r\n- What I had originally thought might be a bug turned out to be okay, so the model is working as expected\r\n- But the initial values used for the seasonal states seems problematic. The `ETSModel` is attempting to normalize the seasonals to prevent identification issues, but is going about it in the wrong way, at least in the multiplicative case.","i.e. when I normalize the initial seasonals in a more standard way, the estimation works fine."],"labels":["comp-tsa","type-bug"]},{"title":"BUG\/ENH: add random state option to mice and mediation","body":"It looks like in \"pre\"-testing, some random numbers are screwed up, not backwards compatible. I'm still not sure where the test failures come from. Maybe using a given user (unit test) defined random state will fix the issue. (but problem in those unit tests could also be somewhere else)\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/8540#issuecomment-1329323850\r\n\r\nBoth mediation and mice use simulation in the estimation, but use only the global np random state.\r\nWe need option for users to provide random state, e.g. in `Mediation.fit`\r\n\r\noldest issue #306\r\ndiscussion on defaults and new numpy random generators #7650\r\n","comments":["note tools check_random_state is not backwards compatible, does not default to global numpy random state.\r\n\r\nNOT correct\r\ndefault for None is global random state with future warning\r\nchanged in #7652\r\n\r\nto keep backwards compatibility option even after changing default for `seed=None`, we could add `seed=\"global\"`\r\n\r\n\r\n","one design decision is what to do if we have several cases of random in a function or class or through delegation.\r\nDo we propagate one random state to all, or do we allow a list (or separate keywords) of random states.\r\n\r\nexample: Mediation fit has simulate and bootstrap options, but AFAICS they are exclusive, i.e. only one of them is used. So, effectively we only need one random state."],"labels":["type-bug","type-enh","comp-stats","comp-imputation"]},{"title":"ENH: helper functions loo, influence, residuals","body":"(random thought)\r\n\r\nrelated to outlier measures in outlier_influence\r\n#7897 score residuals\r\n\r\nAnother idea for residuals is to look at loglikeobs. However we need to standardize it to have interpretable magnitude.\r\n\r\none possibility is use loo standard deviation, and maybe loo mean, for standardizing or zscoring.\r\n(We might also want to use outlier robust variance measures if we have possibly many outliers)\r\n\r\nCurrently, we have explicit loo statistics in outlier_influence or Influence classes that do not require the loo loop.\r\nThose are hardcoded for the predefined statistics.\r\n\r\nInstead or as complement we could add standalone helper functions.\r\ne.g.\r\nvar_loo, std_loo, mean_loo\r\n\r\nresid_xxx_loo_zscored = (resid_xxx - mean_loo) \/ std_loo\r\nwhere resid_xxx could, for example, be loglike_obs\r\n\r\n\"empirically standardized\".\r\n(I guess it's a bit similar to Vuong test for non-nested models)\r\n\r\nsimple case: assume data is given, e.g\r\nmean_loo = (resid_xxx.sum() - resid_xxx) \/ (nobs-1)\r\n\r\ncase: taking `dparams` into account\r\n???\r\nWe might have to compute all resid_xxx, e.g. loglikeobs or score_obs, at each params + dparams. That might not work without a nobs loop or (nobs, nobs) arrays.\r\nfor each i:\r\nllo_i = loglikeobs(params + dparams[i]).sum() \r\nmean_loo[i] = llo_i.sum() - llo_i[i]) \/ (nobs - 1)\r\n\r\n\r\nextra: out-of-sample computation (for control charts or cross-validation)\r\nStandardizing needs to be a stateful transform, i.e. use sample mean and std to standardize out-of-sample observations.\r\nHowever, that doesn't need a LOO because obs are out of sample already.\r\n\r\n(maybe `_stdz` as postfix for names, for standardized, (empirically) zscored )\r\n\r\ne.g.\r\nshould we provide standardized resid_pearson, but it already has restriction that mean is 1 if correctly specified.\r\n\r\n\r\naside, not clear to me:\r\nDoes inherent heteroscedasticity as in GLM\/LEF like poisson induce heteroscedasticity in the resid_xxx, because higher moments of the distribution are also functions of mean.\r\n\r\nmaybe look at PIT residuals which should be uniform (under the null that model is correct specified for all obs)\r\npit_loo, i.e. ppf at params + dparams ?\r\n  ","comments":[],"labels":["type-enh","comp-robust","topic-diagnostic"]},{"title":"ENH: Likelihood ratio test for logistic regeression","body":"#### Is your feature request related to a problem? Please describe\r\nIt is useful to compare the value of new predictors and their contribution to a model fit.\r\n\r\n#### Describe the solution you'd like\r\nImplementation of the likelihood ratio test for `BinaryResultsWrapper` object.\r\n","comments":["see also #5372 for GLM\r\n\r\nI don't see another issue with a request for compare_lr_test specifically for discrete models.\r\n"],"labels":["type-enh","comp-discrete"]},{"title":"SUMM\/ENH\/Design overview control charts","body":"trying again for an overall design or design requirements for control chart\r\nmain original issue #4191 plus 2 PRs for earlier implementation\r\n\r\nnewer topics\r\n- control charts based on beta distribution or regression and other distributions on (0, 1) or [0, 1] \r\n- tolerance interval, currently only for poisson stats\r\n\r\n**traditional control charts** \r\none letter or short names, x, u, p, np, c, u, xbar, ...., based on k * sigma limits\r\nwe can add it mainly for familiarity by users \r\nalmost one liners, but it would be good to have a collection\r\nall that is needed is mean or expected value and variance of mean\r\n\r\nmore general and more interesting components\r\n\r\n**quantiles**\r\nuse quantile estimate for control limits instead of k*sigma limits\r\neither parametric or nonparametric\/flexible distribution, misspecified model (eg. over dispersion)\r\nwith or without parameter uncertainty\r\nsimple example: one sample tolerance interval for Poisson\r\nWe could compare with QuantReg or M-quantiles using less restrictive distributional assumptions.\r\n\r\n**parameter estimation using clean data**\r\n`fit` assuming data does not have outliers or out of control observations\r\nThis can be either simple estimators like mean, or we connect to models, e.g. beta regression, or regression control charts.\r\nAdvantage of using models is that we have built-in inference, e.g. standard errors of parameter estimates, including misspecification robust inference, and get_distribution method.\r\n\r\n**parameter estimation and outlier detection using dirty data**\r\n`fit` assuming historical data might have outlier, out-of-control periods\r\n- robust estimation, currently we only have RLM, but nothing for discrete, nonlinear or link models.\r\n- outlier identification\r\n   We have get_influence for some outlier candidate identification, but no robust estimation besides dropping outliers.\r\n  GLM has freq_weights, that we could use to adaptively downweigh outlier candidates.\r\n  We also need something for grouped outlier, e.g. several outlier periods in a row, need to look at outlier windows.  \r\n  We can also reuse control chart itself iteratively to identify outliers, e.g. if we have tolerance intervals, ppf, isf, quantiles\r\n\r\n\r\nI did not look at implementation issues again.\r\n\r\nWe could start with Poisson, tolerance interval versus k-sigma limits versus quantreg.\r\nWith overdispersion and count distributions that are alternatives to Poisson.\r\nExample: u-chart versus Laney u' Chart (u Prime) versus NBP or GPP, versus zero-inflated, ...\r\nu prime chart adds heterogeneity in mean across batches, (a bit similar to meta-analysis correction for random effects), e.g. empirical standard deviation versus standard deviation implied by the distribution.\r\nhttps:\/\/www.qimacros.com\/control-chart-formulas\/u-prime-chart-formula\/\r\n\r\nmore difficult multi-link or multi distribution parameter models\r\ne.g. equivalence to tolerance interval or quantiles under parameter uncertainty for BetaModel.\r\n","comments":["meta: monitor or control the control chart\r\n\r\nIf we use control charts based on predictive distribution properties using a specific model, then we need specification testing or model comparisons to choose the model appropriate for historical data.\r\n\r\nSimilarly, we could base control charts on distributional properties other than a mean measure.\r\nE.g. a control chart for variance is also a monitor for the mean control chart. Even if mean stays unchanged, the control limits might change, i.e. we need to monitor for changes in the control limits.\r\nOther gof and specification measures could also be monitored to check whether the control chart and it's underlying distributional assumptions are still appropriate. Main problem is that many of those statistics require larger sample sizes, e.g. quantiles close to the tails, or higher order moments, or cdf in tails.\r\n\r\n\r\n\r\n\r\n\r\n\r\n","possible class SigmaControlChart:\r\ngeneric class that uses k * sigma control charts, where sigma could be empirical or from model (poisson, binomial, normal, ...) and k is user given or ppf of standard normal.\r\n\r\nwith different chart updating (method?) types\r\n- shewart: expected value of each observation\r\n- EWMA\r\n- CUSUM\r\n- MA\r\n- batched\r\n- ...\r\n","two more on integration with models and `stats`\r\n\r\n- prediction\/tolerance interval using predict `which`. What should be the api?\r\n  - confint_obs or similar name as currently in OLS get_prediction, is extra when predicting mean, or\r\n  - use separate `which` for prediction interval, e.g. `predict=\"obs\"`. disadvantage duplicate computation if we want confint and confint_obs\r\n- reusing 2 sample tests for batched control charts like xbar chart\r\n  - consider historical sample and test sample as the `2indep` samples, with null that they have the same mean. We can just call current functions directly.\r\n  - asymmetric treatment of samples: \r\n    Add shortcut for fixed reference sample to avoid recomputing statistics for it. e.g. summary data for reference sample, full data for test sample. \r\n   Possibly or optionally, fix auxiliary parameters like variance to be only from reference sample   \r\n","again, semi-random design idea\r\n\r\nchart class (for independent, i.i.d. data, not sure what changes with cusum and emwa with moving limits):\r\n\r\n- `__init__`: method for control limits, optional historical data (phase 1)\r\n  - method could be for fixed thresholds (e.g. 3 sigma), or \r\n  - estimation method plus confint method (e.g. poisson tolerance interval)\r\n  - possibly option for dirty (with out of control observations) or clean data (dirty data needs robust estimator)\r\n- update_fit: recompute control limits with possible new, updated phase 1 data, using same method as in `__init__`\r\n- get_limits: helper method (maybe attribute with also set_limits)\r\n- method to add phase 2 data\r\n- method to evaluate phase 2 data\r\n- plot method\r\n\r\nfor univariate data (y, endog), we might need extra data, e.g. sample size for xbar, binomial, exposure for Poisson, ...\r\nplus exog for attribute charts, regression prediction intervals\r\n\r\nchoice of distribution\r\ne.g. for count data, we could have any available model, poisson, binomial, negative binomial, zip, ...\r\nfor continuous data, we could have normal, t, ...\r\nThis is mainly relevant if we estimate control limits from quantile function, ppf (or tolerance interval)\r\n\r\nmodel diagnostics:\r\nIf we estimate phase 1 parameters using a model, then returning its results would provide additional diagnostics for, e.g., goodness-of-fit.\r\n(simplest case: Poisson count chart for individual i.i.d. observations)\r\n\r\n(aside: if we trim outlying observations by threshold, then using truncated model would be more appropriate for estimation, e.g. tail truncated Poisson)\r\n\r\n"],"labels":["type-enh","design","comp-stats"]},{"title":"ENH: robust cov, HAC with uniform kernel for moving average, rollingols","body":"What is the appropriate cov_params for moving average or rolling regression?\r\n\r\nI'm looking at this in context of control charts with moving mean or moving aggregated counts.\r\n\r\nIf we assume that the individual observations are uncorrelated, then the moving averaging filter induces autocorrelation.\r\nMy guess is that we should use HAC with a uniform filter, or newey west filter truncated at window length.\r\n\r\nAFAICS, RollingOLS only allows for HC robust cov_type, but does not include correlation ?\r\n\r\naside:\r\nFor control charts we can use either\r\n- nonoverlapping windows, i.e. aggregate\/average observations into batches, or\r\n- overlapping, moving windows\r\n\r\nThe question is what the appropriate cov_params and inference is for the second case.\r\n\r\nAlso related, in peak over threshold in extreme value theory we use nonoverlapping windows with `max` aggregation.\r\n\r\ntechnical aside:\r\nlinear filter changes autocorrelation, functions for this are in tsa_tools, AFAIR but I have not looked at those in a very long time.\r\n","comments":["@bashtage \r\n\r\nDo you know any literature for this?\r\nDoes RollingOLS take autocorrelation into account?  (I never used it and just briefly skimmed the code.)\r\n"],"labels":["type-enh","comp-regression","comp-stats","topic-covtype"]},{"title":"STL implementation cannot handle missing values","body":"#### Describe the bug\r\n\r\nThe current STL season-trend decomposition in statsmodels does not handle missing values from the input. This can be seen in the use of `ffil()` in the provided example, see [here](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.seasonal.STL.html#statsmodels.tsa.seasonal.STL). \r\n\r\nHowever, one of the properties of the original STL algorithm is that it can handle the missing values. See design goal 4 in Sect. 1.1 (Design goal) of the original article\r\n> R. B. Cleveland, W. S. Cleveland, J.E. McRae, and I. Terpenning (1990) STL: A Seasonal-Trend Decomposition Procedure Based on LOESS. Journal of Official Statistics, 6, 3-73.\r\n\r\nSo I was wondering if this was intentional or a bug. Will it be implemented? If not, should we update the documentation to say that the algorithm has not been fully implemented? \r\n\r\nMy understanding is that the algorithm is still widely used, so it would be beneficial to implement this part as well.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nfrom statsmodels.datasets import co2\r\nimport matplotlib.pyplot as plt\r\nfrom pandas.plotting import register_matplotlib_converters\r\nfrom statsmodels.tsa.seasonal import STL\r\n\r\nregister_matplotlib_converters()\r\ndata = co2.load().data\r\ndata = data.resample('M').mean()\r\nres = STL(data).fit()\r\nres.plot()\r\nplt.show()\r\n# As can be seen from the plot above, fitting fails without a warning for an input that is within the parameters of the original article.\r\n```\r\n\r\n#### Expected Output\r\n\r\nImplement the nan fitting from the original article or at least clearly state in the documentation that it is not currently treated.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.8.13.final.0\r\nOS: Darwin 22.1.0 Darwin Kernel Version 22.1.0: Sun Oct  9 20:14:30 PDT 2022; root:xnu-8792.41.9~2\/RELEASE_ARM64_T8103 arm64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nstatsmodels\r\n===========\r\nInstalled: 0.13.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/statsmodels)\r\nRequired Dependencies\r\n=====================\r\ncython: Not installed\r\nnumpy: 1.23.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/numpy)\r\nscipy: 1.9.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/scipy)\r\npandas: 1.5.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/patsy)\r\nOptional Dependencies\r\n=====================\r\nmatplotlib: 3.6.1 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/matplotlib)\r\n    backend: module:\/\/backend_interagg \r\ncvxopt: Not installed\r\njoblib: 1.2.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/joblib)\r\nDeveloper Tools\r\n================\r\nIPython: 8.6.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/IPython)\r\n    jinja2: 3.1.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/jinja2)\r\nsphinx: Not installed\r\n    pygments: 2.13.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pygments)\r\npytest: 7.1.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pytest)\r\nvirtualenv: 20.16.5 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/virtualenv)\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.8.13.final.0\r\nOS: Darwin 22.1.0 Darwin Kernel Version 22.1.0: Sun Oct  9 20:14:30 PDT 2022; root:xnu-8792.41.9~2\/RELEASE_ARM64_T8103 arm64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nstatsmodels\r\n===========\r\nInstalled: 0.13.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/statsmodels)\r\nRequired Dependencies\r\n=====================\r\ncython: Not installed\r\nnumpy: 1.23.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/numpy)\r\nscipy: 1.9.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/scipy)\r\npandas: 1.5.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/dateutil)\r\npatsy: 0.5.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/patsy)\r\nOptional Dependencies\r\n=====================\r\nmatplotlib: 3.6.1 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/matplotlib)\r\n    backend: module:\/\/backend_interagg \r\ncvxopt: Not installed\r\njoblib: 1.2.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/joblib)\r\nDeveloper Tools\r\n================\r\nIPython: 8.6.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/IPython)\r\n    jinja2: 3.1.2 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/jinja2)\r\nsphinx: Not installed\r\n    pygments: 2.13.0 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pygments)\r\npytest: 7.1.3 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/pytest)\r\nvirtualenv: 20.16.5 (\/Users\/user\/miniforge3\/envs\/repo\/lib\/python3.8\/site-packages\/virtualenv)\r\n```\r\n<\/details>\r\n","comments":["The implementation in sm is identical to that of the original authors.  It was implemented as a line by line translation of the original fortran.  The docs should be updated to state that the input vector must have missing values. ","Ok, thanks @bashtage! But we should keep in mind that despite the original code, an implementation working with missing values should still be possible (according to the article)!","> The implementation in sm is identical to that of the original authors. It was implemented as a line by line translation of the original fortran. The docs should be updated to state that the input vector must have missing values.\r\n\r\n\"must not\" maybe?","Sad to discover that stl in statsmodel can't handle missing values. I think that's one of the best ways to impute missing values in a ts.","PR to enable missing value support are welcome.  I assume method that do this use some form of EM algorithm where missing values are naively interpolated and then STL is run.  The STL is then used to get better replacements values, and the process is repeated until convergence. "],"labels":["comp-tsa","type-enh"]},{"title":"ENH\/TST\/SUMM check all get_prediction methods outside tsa","body":"(more general than #8519 specific for checking get_prediction across models)\r\n\r\nbased on the doc index the following classes outside tsa have a `get_prediction` method\r\n\r\nso far I mainly checked discrete models, GLMResults and new models BetaModel and OrderedModel\r\nOLS as original implementation\r\nI have not recently checked any of the other ones, which might have an inherited method or inconsistent api\r\n\r\nseveral models are also still missing the method, e.g. Mixed, RLM, .... (which ?)\r\nwhat about WLS, GLS and their subclasses, they are not listed but might inherit \r\n\r\nbase.model.GenericLikelihoodModelResult\r\n\r\ndiscrete.count_model.ZeroInflatedGeneralizedPoissonResult\r\ndiscrete.count_model.ZeroInflatedNegativeBinomialResult\r\ndiscrete.count_model.ZeroInflatedPoissonResult\r\ndiscrete.discrete_model.BinaryResult\r\ndiscrete.discrete_model.CountResult\r\ndiscrete.discrete_model.DiscreteResult\r\ndiscrete.discrete_model.GeneralizedPoissonResult\r\ndiscrete.discrete_model.LogitResult\r\ndiscrete.discrete_model.MultinomialResult\r\ndiscrete.discrete_model.NegativeBinomialResult\r\ndiscrete.discrete_model.ProbitResult\r\ndiscrete.truncated_model.HurdleCountResult\r\ndiscrete.truncated_model.TruncatedLFPoissonResult\r\ndiscrete.truncated_model.TruncatedNegativeBinomialResult\r\n\r\ngam.generalized_additive_model.GLMGamResult\r\ngenmod.generalized_estimating_equations.GEEResult\r\ngenmod.generalized_linear_model.GLMResult\r\n\r\nmiscmodels.ordinal_model.OrderedResult\r\nothermod.betareg.BetaResult\r\n\r\nregression.linear_model.OLSResult\r\nregression.linear_model.RegressionResult\r\nregression.process_regression.ProcessMLEResult\r\nregression.quantile_regression.QuantRegResult\r\nregression.recursive_ls.RecursiveLSResult\r\n\r\nsandbox.regression.gmm.IVRegressionResult\r\n","comments":[],"labels":["type-enh","type-test"]},{"title":"ENH\/TST: inherited post-estimation features for GLMGAM","body":"from https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8505#issuecomment-1312040074\r\n\r\nI just saw that GAM inherits from GLM `GLMGamResults(GLMResults)`\r\nget_prediction calls the super method\r\nextra keywords are in kwargs, so `which` support and behavior should be inherited from GLM\r\n\r\nneeds specific unit tests\r\nget_distribution and other post-estimation features will also be inherited\r\n\r\n\r\nI guess most postestimation should work without problems.\r\nBut I never checked any of the new inherited generic features for gam\r\n\r\nThis will be also another consistency check and check for how generic our generic features are.\r\n\r\npenalization should have no effect beyond, params, cov_params and df_xxx (AFAIR)\r\n\r\n","comments":[],"labels":["type-enh","comp-genmod","type-test","topic-diagnostic","comp-gam","topic-post_estim"]},{"title":"ENH: cox proportional hazard, PHReg, survival function","body":"https:\/\/stackoverflow.com\/questions\/74403023\/how-to-plot-survival-curve-for-cox-survival-from-statsmodels-api\r\n\r\nguessing, I didn't look at it\r\nWe need to stitch it together, cox ph maximizes only partial likelihood\r\n\r\nHazard function should be just multiplication by proportionality factor.\r\nBut we want also survival function, How do we recover it from the hazard function?\r\n\r\nI guess inference like conf_int for it will be very difficult combining baseline uncertainty with PH parameter uncertainty.\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-duration"]},{"title":"BUG\/Design GLM predict, get_prediction does not include weights","body":"GLM predict only uses the linpred\r\n\r\n- we don't have freq_weights in predict\r\n- I guess var_weights will only affect inference for predict\r\n- binomial count, predict is for 1 trial (i.e. prob), no option for n_trials, i.e. predicting counts. `resid_xxx` include n_trials separately\r\n\r\nIt looks like current behavior is by design, but we might want to have more options.\r\n\r\njust an observation while reading the code for #8505 and thinking about which other \"which\" options to add\r\ne.g. currently my `which=\"var\"` returns only the family var function without extras (no var_weights, no n_trials\r\n\r\n","comments":["adding additional keywords like xxx_weights or n_trials is for future, 0.15.\r\nIt will take some time to design and check this.\r\n\r\nAlso, it's not clear to me yet how to handle family specific `which`.\r\neg. probs as in discrete count models.\r\n\"mean_count\" for binomial with n_trials ?\r\n\r\nTo not tie our hands:\r\nI use now `which=\"var_unscaled\"` for plain family varfunc\r\n\r\n","similar issue, get_margeff does not support (freq) weights #8889\r\n\r\nGLM predict, get_prediction might need a warning if freq_weights have been used in the model.\r\n`get_prediction` has agg_weights so user can specify freq_weights as agg_weights for average effect.\r\n\r\nmaybe:\r\nincluding freq_weights is important only in average effect, \"overall\" in margeff, i.e. similar to agg_weights.\r\nFor individual prediction assuming `freq_weights = 1.` will be more useful than using sampling freq_weight.\r\n\r\n"],"labels":["type-bug","type-enh","comp-genmod","topic-predict","topic-post_estim"]},{"title":"BUG\/TST:  df, k_extra in GenericLikelihoodModel  subclasses, copula models","body":"cases that were missed in #8476\r\n\r\nI just saw more warnings in unit test suite\r\n\r\nCopulaModel in unit tests in statsmodels.distributions.copula.tests.test_model\r\n\r\n```\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestGumbel::test\r\n  \/home\/vsts\/work\/1\/s\/statsmodels\/base\/model.py:2741: UserWarning: df_model + k_constant + k_extra differs from k_params\r\n    warnings.warn(\"df_model + k_constant + k_extra \"\r\n```","comments":[],"labels":["comp-distributions","comp-base","type-test"]},{"title":"TST: noop unit tests in test_glm_weights","body":"While working on DeprecationWarnings for linear keyword in GLM predict and get_prediction (#8505), I saw this\r\n\r\nThe first test seems to use hasattr on a non-existing attribute which would skip the test\r\nThe second compares a result with itself\r\n\r\nI didn't investigate the details, or how to fix those\r\n\r\nnote DeprecationWarning assert is being added in my PR 8505\r\n\r\n\r\n```\r\n    def test_pearson_chi2(self):\r\n        if hasattr(self.res2, 'chi2'):\r\n            assert_allclose(self.res1.pearson_chi2, self.res2.deviance_p,\r\n                            atol=1e-6, rtol=1e-6)\r\n\r\n    def test_getprediction(self):\r\n        with pytest.warns(DeprecationWarning):\r\n            # deprecation warning for linear keyword\r\n            pred = self.res1.get_prediction()\r\n        assert_allclose(pred.linpred.se_mean, pred.linpred.se_mean, rtol=1e-10)\r\n```","comments":["temporarily prio-elev, so I don't forget about it"],"labels":["comp-genmod","type-test","prio-elev"]},{"title":"BUG\/ENH: constrained\/collinear fit and degrees of freedom for terms, df_terms,  anova","body":"followup question to #8506 and #8336 \r\n\r\nIf we (semi-) automatically drop exog columns (e.g. fit_colinear), estimated (pinv) regularized params or fit under constraints, then this can effect hypothesis tests for terms. \r\n\r\nquestion is what is the relevant number of constraints for a joint hypothesis that affect terms.\r\nHypothesis tests on individual columns or on all params will effectively not be affected, I think. Those hypotheses we already have now.\r\n\r\nWith categorical exog and two way effects, especially nested effects, we might have zero cells, i.e. a column of zeros.\r\nIf we drop those, then the df of the term will be reduced.\r\n\r\nAFAIR, we don't have any support for keeping track of df, number of effective parameters for terms (except, AFAIR we added it to GAM)\r\n\r\nThat is we need effective number of parameters for each term.\r\n\r\nim_ratio might apply for pinv OLS\r\nWhen we only drop columns, then we would just need to keep track of \"nonzero\" or unrestricted columns.\r\n\r\nI don't have any idea (for now) what would be useful for general affine transformation of params as in fit_constrained.\r\n\r\nIn general, maybe just looking at the matrix rank of the sub-exog or terms would be enough.\r\nI think, I do this already in some specification, lm tests, using rank to determine df_constrained for chi2 test.\r\n  ","comments":[],"labels":["comp-base","comp-regression","comp-stats"]},{"title":"BUG: Nested ANOVA incorrect results -- wrong matrix definition","body":"#### Description of the bug\r\n\r\nstatsmodels.formula.api.ols run on data with nested factors (i.e. not complete crossing) gives incorrect results.\r\n\r\nThis problem has been reported at least twice independently\r\n\r\n1) https:\/\/stackoverflow.com\/questions\/72950135\/nested-anova-in-statsmodels\r\n2) https:\/\/stats.stackexchange.com\/questions\/594889\/nested-anova-in-python-and-singular-matrices\r\n\r\n\r\n#### Code Sample\r\n\r\nI'll replicate the code from example 2) above:\r\n```python\r\n\r\nimport pandas as pd\r\nimport statsmodels.formula.api as smf\r\n\r\nnobs = 5\r\nnB = 4\r\nnA = 3\r\nA = [f\"A{i}\" for i in range(1,nA+1) for h in range(nobs*nB)]\r\nB = [f\"B{i}{j}\" for i in range(1,nA+1) for j in range(1,nB+1) for h in range(nobs)]\r\nobs = list(range(5))*nA*nB\r\n\r\ndata = (30,-19,-31,-14,-14,0,20,32,11,13,7,5,3,-5,8,28,15,20,20,-48,\r\n       24,16,-18,11,-10,14,11,27,11,8,20,18,8,32,-25,20,-12,0,-5,44,\r\n       14,-18,33,-9,-7,14,8,-19,6,-38,18,16,7,-4,21,-25,13,36,18,-7)\r\n\r\ndf = pd.DataFrame(data ={\r\n    \"A\": A,\r\n    \"B\": B,\r\n    \"Observation_number\": obs,\r\n},dtype=\"category\")\r\ndf = df.join(pd.Series(data=data,name=\"y\"))\r\n\r\nmodel = smf.ols(\"y ~ A + A:B\",data=df).fit()\r\nanova_df = sm.stats.anova_lm(model)\r\n```\r\n\r\nThe crossing factor A:B has 33 DOF, which is only right if Statsmodels crossed factors nested in A1 with A2.\r\n```\r\n           df        sum_sq     mean_sq         F    PR(>F)\r\nA          3.0   2506.300000  835.433333  2.180719  0.102465\r\nA:B       33.0  10338.241297  313.280039  0.817750  0.726097\r\nResidual  48.0  18388.800000  383.100000       NaN       NaN\r\n```\r\n\r\nRunning model.summary() shows the condition number is very low:\r\n> [2] The smallest eigenvalue is 4.14e-33. This might indicate that there are strong multicollinearity problems or that the design > matrix is singular.\r\n\r\nwhich you would expect when all-zero columns are left in the model matrix.\r\n\r\n#### Requested change\r\n\r\n**Add automatic detection and ignore of all-zero columns (inappropriately crossed factors) or give user capability to manually do so.** As Statsmodel works currently, any nested model fitted with OLS will give completely erroneous results.\r\n\r\n\r\n","comments":["Problem #8336 is seemingly similar (wrong number of DOF) but does not reference the zero columns, high condition number issue.\r\n","I just saw the statsstackexchange question \r\n\r\nDo you have the expected output?\r\n\r\nI never really figured out nested effects and formulas for it.\r\n\r\ntrying out some things\r\nThe following has a full rank exog, note I'm recoding the nested category to use the same names in each higher level\r\n\r\n```\r\nnobs = 5\r\nnB = 4\r\nnA = 3\r\nA = [f\"A{i}\" for i in range(1,nA+1) for h in range(nobs*nB)]\r\nB = [f\"B{j}\" for i in range(1,nA+1) for j in range(1,nB+1) for h in range(nobs)]\r\nobs = list(range(5))*nA*nB\r\n\r\ndata = (30,-19,-31,-14,-14,0,20,32,11,13,7,5,3,-5,8,28,15,20,20,-48,\r\n       24,16,-18,11,-10,14,11,27,11,8,20,18,8,32,-25,20,-12,0,-5,44,\r\n       14,-18,33,-9,-7,14,8,-19,6,-38,18,16,7,-4,21,-25,13,36,18,-7)\r\n\r\ndf = pd.DataFrame(data ={\r\n    \"A\": A,\r\n    \"B\": B,\r\n    \"Observation_number\": obs,\r\n},dtype=\"category\")\r\ndf = df.join(pd.Series(data=data,name=\"y\"))\r\n\r\nresults = smf.ols(\"y ~ A\/B\",data=df).fit()\r\nanova_df = sm.stats.anova_lm(results)\r\nprint(anova_df)\r\n\r\n            df        sum_sq     mean_sq         F    PR(>F)\r\nA          2.0    441.233333  220.616667  0.575872  0.566051\r\nA:B        9.0   2656.900000  295.211111  0.770585  0.643738\r\nResidual  48.0  18388.800000  383.100000       NaN       NaN\r\n```","We can keep this issue because it has a complete example.\r\n\r\nrelated possibility: \r\nIs R just dropping singular columns or is there a special treatment for nested models.\r\nThat `a\\b` is supposed to be equivalent to `a + a:b` does not sound like recoding the nested levels, so maybe R just uses the default dropping of collinear variables.\r\n","@josef-pkt yes, I think it does drop the collinear variables; in particular, looking into the DesignMatrix produced by Patsy I see some columns are all-empty, which are the interactions between the nested factors (say B) and the levels of A in which they do not appear. \r\n\r\nI tried dropping them manually but then Statsmodels complains that the DesignInfo is no longer matching the DesignMatrix.\r\n\r\nIs there any way when I build the model that I can specify to drop collinear columns?","Aun con este cambio no realiza correctamente el Anova anidada"],"labels":["comp-regression","comp-stats","prio-elev"]},{"title":"DOC: sm.datasets attribute table","body":"#### Is your feature request related to a problem? Please describe\r\nI was hoping there would be a table with the sm.datasets attributes in the [documentation](https:\/\/www.statsmodels.org\/dev\/datasets\/index.html), but it doesn't exist. This info wasn't too much trouble to figure out, but I think a nice table there would be a good addition.\r\n\r\n#### Describe the solution you'd like\r\nI would recommend adding something like the following table (below) to [docs\/source\/datasets\/index.rst](https:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/docs\/source\/datasets\/index.rst)\r\n\r\nI tried to figure out how to change this myself and do a PR, but I'm not at all farmiliar with sphynx and don't want to break anything :)\r\n\r\nThanks for considering!\r\n\r\n#### Additional context\r\n\r\n|Available Datasets                                                                                                               |Attribute Names    |\r\n|---------------------------------------------------------------------------------------------------------------------------------|------------------|\r\n|[American National Election Survey 1996](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/anes96.html)                         |anes96            |\r\n|[Breast Cancer Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/cancer.html)                                             |cancer            |\r\n|[Bill Greene\u2019s credit scoring data.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/ccard.html)                              |ccard             |\r\n|[Smoking and lung cancer in eight cities in China.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/china_smoking.html)       |china_smoking     |\r\n|[Mauna Loa Weekly Atmospheric CO2 Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/co2.html)                             |co2               |\r\n|[First 100 days of the US House of Representatives 1995](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/committee.html)      |committee         |\r\n|[World Copper Market 1951-1975 Dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/copper.html)                          |copper            |\r\n|[US Capital Punishment dataset.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/cpunish.html)                                |cpunish           |\r\n|[Danish Money Demand Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/danish_data.html)                                  |danish_data       |\r\n|[El Nino - Sea Surface Temperatures](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/elnino.html)                             |elnino            |\r\n|[Engel (1857) food expenditure data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/engel.html)                              |engel             |\r\n|[Affairs dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/fair.html)                                                  |fair              |\r\n|[World Bank Fertility Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/fertility.html)                                   |fertility         |\r\n|[Grunfeld (1950) Investment Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/grunfeld.html)                              |grunfeld          |\r\n|[Transplant Survival Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/heart.html)                                        |heart             |\r\n|[(West) German interest and inflation rate 1972-1998](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/interest_inflation.html)|interest_inflation|\r\n|[Longley dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/longley.html)                                               |longley           |\r\n|[United States Macroeconomic data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/macrodata.html)                            |macrodata         |\r\n|[Travel Mode Choice](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/modechoice.html)                                         |modechoice        |\r\n|[Nile River flows at Ashwan 1871-1970](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/nile.html)                             |nile              |\r\n|[RAND Health Insurance Experiment Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/randhie.html)                         |randhie           |\r\n|[Taxation Powers Vote for the Scottish Parliament 1997](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/scotland.html)        |scotland          |\r\n|[Spector and Mazzeo (1980) - Program Effectiveness Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/spector.html)        |spector           |\r\n|[Stack loss data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/stackloss.html)                                             |stackloss         |\r\n|[Star98 Educational Dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/star98.html)                                     |star98            |\r\n|[Statewide Crime Data 2009](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/statecrime.html)                                  |statecrime        |\r\n|[U.S. Strike Duration Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/strikes.html)                                     |strikes           |\r\n|[Yearly sunspots data 1700-2008](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/sunspots.html)                               |sunspots          |\r\n","comments":["what would also be a good addition is the type of the endog for which the dataset is used, e.g. continuous, binary, count, time series.","Sometimes the endog type is clear, such as with the [Transplant Survival Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/heart.html), but other times it's kind of arbitrary depending on the analysis in question. In some cases, such as [Yearly sunspots data 1700-2008](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/sunspots.html), there is only 1 variable. I'm happy add some columns to the table w\/ whatever folks recommend\/want.","We can build the table here, I added FAQ label so it's easier to find, and then copy it into the docs.\r\n\r\nWhat I often needed and would have liked is to quickly find which dataset I could use for a specific model. I usually have to go through our unit tests to check which dataset is used.\r\n\r\nSo best would be two additional columns, data type and usage\/models. Those could have multiple entries if the dataset is used on different ways.\r\n\r\nThis also could include the structure of the dataset, most are used as cross-sectional, but others have a specific structure for their main applications, e.g. survival, panel, time series data, \r\nMaybe \"multivariate\" if used for multivariate endog.\r\n\r\nThanks for working on this. \r\n(I didn't find any direct issue for this when searching the issues)\r\n\r\n\r\n","There are a couple good ways to go on this I think.\r\nI did a some of the groundwork today by getting some structured data to work from.\r\nI took some inspiration from [this R documentation](https:\/\/vincentarelbundock.github.io\/Rdatasets\/datasets.html)\r\n\r\nLet me know how you would like me to adjust these.\r\n\r\nPlease see the following:\r\n[dataset_type_counts_table.md](https:\/\/github.com\/statsmodels\/statsmodels\/files\/9934058\/dataset_type_counts_table.md)\r\n[dataset_table.md](https:\/\/github.com\/statsmodels\/statsmodels\/files\/9934059\/dataset_table.md)\r\n[dataset.yml](https:\/\/github.com\/coup321\/statsmodels_datasets\/blob\/master\/dataset.yml)\r\n(also easy to view [here](https:\/\/github.com\/coup321\/statsmodels_datasets))\r\nC","dataset_table.md\r\n\r\n\r\n| dataset | attr | shape|columns|usage|\r\n| --------------------------------------------------------------------------------------------------------------------------------- | ------------------ | --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |--|\r\n| [American National Election Survey 1996](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/anes96.html)| anes96| 944, 11| popul: int<br>TVnews: int<br>PID: category<br>age: int<br>educ: category<br>income: category<br>vote: category<br>selfLR: category<br>ClinLR: category<br>DoleLR: category<br>logpopul: float|Regression<br>Stats|\r\n| [Breast Cancer Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/cancer.html)| cancer| 301, 2| population: int<br>cancer: int|Regression|\r\n| [Bill Greene\u2019s credit scoring data.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/ccard.html)| ccard| 72, 5| AVGEXP: float<br>AGE: int<br>INCOME: float<br>INCOMESQ: float<br>OWNRENT: bool|Regression|\r\n| [Smoking and lung cancer in eight cities in China.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/china_smoking.html)| china_smoking| 8, 4| smoking_yes_cancer_yes: int smoking_yes_cancer_no: int smoking_no_cancer_yes: int smoking_no_cancer_no: int|Regression|\r\n| [Mauna Loa Weekly Atmospheric CO2 Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/co2.html)| co2| 2284, 1| time: date <br>co2: float|Time series|\r\n| [First 100 days of the US House of Representatives 1995](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/committee.html)| committee| 20, 6| BILLS104: int<br>SIZE: int<br>SUBS: int<br>STAFF: int<br>PRESTIGE: bool <br> BILLS103: int|Regression|\r\n| [World Copper Market 1951-1975 Dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/copper.html)| copper| 25, 6| WORLDCONSUMPTION: int<br>COPPERPRICE: float<br>INCOMEINDEX: float<br>ALUMPRICE: float<br>INVENTORYINDEX: float<br>TIME: int|Regression|\r\n| [US Capital Punishment dataset.](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/cpunish.html)| cpunish| 17, 7| EXECUTIONS: int<br>INCOME: int<br>PERPOVERTY: float<br>PERBLACK: float<br>VC100k96: int<br>SOUTH: bool<br>DEGREE: float|Regression|\r\n| [Danish Money Demand Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/danish_data.html)| danish_data| 55, 5| period: date<br>lrm: float<br>lry: float<br>lpy: float<br>ibo: float<br>ide: float|Regression<br>Time series|\r\n| [El Nino - Sea Surface Temperatures](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/elnino.html)| elnino| 61, 13| YEAR: date<br>JAN: float<br>FEB: float<br>MAR: float<br>APR: float<br>MAY: float<br>JUN: float<br>JUL: float<br>AUG: float<br>SEP: float<br>OCT: float<br>NOV: float<br>DEC: float|Regression<br>Time series|\r\n| [Engel (1857) food expenditure data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/engel.html)| engel| 235, 2| income: float<br>foodexp: float|Regression|\r\n| [Affairs dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/fair.html)| fair| 6366, 8| rate_marriage: category<br>age: int<br> yrs_married: int<br>children: int<br>religious: category<br>educ: category<br>occupation: category <br>occupation_husb: category<br>affairs: float|Regression<br>Stats|\r\n| [World Bank Fertility Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/fertility.html)| fertility| 209, 58| Country Name': category<br>'Country Code': category <br>'Indicator Name': category<br>'Indicator Code': int<br>'1960': float<br>\u2026<br>'2013': float|Regression<br>Stats<br>Timeseries|\r\n| [Grunfeld (1950) Investment Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/grunfeld.html)| grunfeld| 220, 5| invest: float<br>value: float<br>capital: float<br>firm: category<br>year: date|Regression<br>Stats|\r\n| [Transplant Survival Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/heart.html)| heart| 69, 3| survival: int<br>censors: bool<br>age: float|Survival Analysis|\r\n| [(West) German interest and inflation rate 1972-1998](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/interest_inflation.html) | interest_inflation | 107, 4| year: date<br>quarter: int<br>Dp: float<br>R: float|Timeseries|\r\n| [Longley dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/longley.html)| longley| 16, 7| TOTEMP: int<br>GNPDEFL: int<br>GNP: int<br>UNEMP: int<br>ARMED: int<br>POP: int<br>YEAR: date|Regression<br>Timeseries|\r\n| [United States Macroeconomic data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/macrodata.html)| macrodata| 203, 14| year: date<br>quarter: int<br>realgdp: float<br>realcons: float<br>realinv: float<br>realgovt: float<br>realdpi: float<br>cpi: float <br>m1: float<br>tbilrate: float<br>unemp: float<br>pop: float<br>infl:float<br>realint: float |Regression<br>Timeseries|\r\n| [Travel Mode Choice](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/modechoice.html)| modechoice| 840, 9| individual: int<br>mode: category<br>choice: bool<br>ttme: int<br>invc: int<br>invt: int<br>gc: int<br>hinc: int<br>psize: int|Regression<br>Stats|\r\n| [Nile River flows at Ashwan 1871-1970](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/nile.html)| nile| 100, 2| year: date<br>volume: int|Timeseries|\r\n| [RAND Health Insurance Experiment Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/randhie.html)| randhie| 20190, 10 | mdvis: int<br>lncoins: float<br>idp: bool<br>lpi: float<br>fmde: bool<br>physlm: bool<br>disea: int<br>hlthg: bool<br>hlthf: bool<br>hlthp: bool|Regression<br>Stats|\r\n| [Taxation Powers Vote for the Scottish Parliament 1997](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/scotland.html)| scotland| 32, 8| YES: float<br>COUTAX: int<br>UNEMPF: float<br>MOR: float<br>ACT: float<br>GDP: float<br>AGE: float <br>COUTAX_FEMALEUNEMP: float|Regression|\r\n| [Spector and Mazzeo (1980) - Program Effectiveness Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/spector.html)| spector| 32, 4| GPA: float<br>TUCE: float<br>PSI: bool<br>GRADE: bool|Regression<br>Stats|\r\n| [Stack loss data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/stackloss.html)| stackloss| 21, 4| STACKLOSS:  float<br>AIRFLOW: float<br>WATERTEMP: float<br>ACIDCONC: float|Regression|\r\n| [Star98 Educational Dataset](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/star98.html)| star98| 303, 22| NABOVE: int<br>NBELOW: int<br>LOWINC: float<br>\u2026|Regression|\r\n| [Statewide Crime Data 2009](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/statecrime.html)| statecrime| 51, 7| state: category<br>violent: float<br>murder: float<br>hs_grad: float<br>poverty: float<br>single: float<br>white: float<br>urban: float|Regression<br>Stats|\r\n| [U.S. Strike Duration Data](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/strikes.html)| strikes| 62, 2| duration: int<br>iprod: float|Regression|\r\n| [Yearly sunspots data 1700-2008](https:\/\/www.statsmodels.org\/dev\/datasets\/generated\/sunspots.html)| sunspots| 309, 2| YEAR: date<br>SUNACTIVITY: float|TImeseries|","It's not as compact as the other table, but the more detailed variable types are useful to include.\r\n\r\nTwo possible improvements\r\n\r\n- add (for example) a star `*` to the variable that is the `endog`\r\n- `Regression` is too generic, i.e. regression should be only for continuous endog, count or binary or multiple choice should have different name."],"labels":["comp-docs","FAQ","comp-datasets"]},{"title":"ENH: support some integer indexes in tsa models","body":"- [X] closes #8487\r\n- [x] tests added \/ passed. \r\n- [X] code\/documentation is well formatted.  \r\n- [X] properly formatted commit message. See \r\n      [NumPy's guide](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/dev\/gitwash\/development_workflow.html#writing-the-commit-message). \r\n\r\n<details>\r\n\r\n**Notes**:\r\n\r\nThe PR is fixing the issue and not breaking any tests in statsmodels\/tsa\/statespace\/tests\/test_simulate.py. \r\nI will also add a test for this edge case before making this ready for review.\r\n\r\n<\/details>\r\n","comments":["@ChadFulton - Thanks for the feedback! I changed the code based on your recommendation in https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8487 and added a warning and a test.","@ChadFulton - Thanks for the review!\r\n\r\n I have addressed the feedback now. One point that I'm not sure if you would agree with is that I now split the unsupported (but valid) indexes into 2 lists where one is the integer index (with step 1) not starting from zero to allow for testing the different warning messages and `index_generated` flags. The other option I can think of is to add this type of index into the supported ones now.\r\n\r\nThere's one failing test in the CI: [statsmodels\/tsa\/statespace\/tests\/test_dynamic_factor.py::TestDynamicFactor_ar2_errors::test_mle](https:\/\/dev.azure.com\/statsmodels\/statsmodels-testing\/_build\/results?buildId=4826&view=logs&j=3f9beef4-2a87-5b9a-d136-eeaf4b842580&t=bde20e0b-dfe9-50c5-4f8a-b07f464aecfb&l=399)\r\nThe test is failing on a tolerance issue that seems unrelated to this PR, but I'm not sure!\r\nI can't reproduce the failure on my machine.\r\n![image](https:\/\/user-images.githubusercontent.com\/64217214\/202902388-93e181a7-569b-4520-9878-f794eeefc727.png)\r\n","That failure is pretty common on CI\/Python 3.9 now.  Need to fix before release, or skip. "],"labels":["type-enh","comp-tsa-statespace"]},{"title":"BUG\/REF: OrderedModel, df_model is k_params, not extra over null model","body":"OrderedModel uses\r\n`self.df_model = self.k_vars + self.k_extra`\r\n\r\nThe definition of df_model should be k_params - k_null\r\n\r\nsee #8481 and #8475\r\n\r\nI'm working on changing and fixing df_resid, df_model in GenericLikelihoodModel in #8476\r\nWith my latest changes df_model definition raises a Warning\r\n\r\nThe constants only model should have `k_null = number of thresholds`  (for empty exog which cannot include a constant)\r\nso df_model should be k_vars, exog.shape[1]\r\n\r\naside: the null model Results.llnull uses parameter estimates from `OrderedModel.start_params` \r\n\r\n","comments":["fixed in #8476\r\n\r\nchanged to\r\n\r\n```\r\n        self.k_extra = self.k_levels - 1\r\n        self.df_model = self.k_vars\r\n        self.df_resid = self.nobs - (self.k_vars + self.k_extra)\r\n```\r\n\r\nI don't see unit tests for it in the merged PR.\r\n???"],"labels":["type-bug","comp-discrete"]},{"title":"REF\/ENH: change df_resid to include extra parameters in MLE, add k_null","body":"many related issues, and old problem, which finally needs a solution\r\nmain design FAQ-D issue:  #1723\r\n\r\nbug in NB #1624\r\nconsistent definitions in GenericLikelihoodModel #8476\r\n\r\nThis is mainly to have a consistent definition of df_resid, df_model and df_null\/k_null, and k_params, especially for more models with extra parameters and multi-link models.\r\nExisting models with extra params like NB, GP, ... should already have correct results statistics like aic, llr_pvalue, so those will return the same values as before if df_resid is redefined, the change for those is internal.\r\n\r\nfor discrete, e.g. count models, include k_extra in df_resid so we don't need to add it specifically in aic, bic and other \r\n\r\nMy current thinking is that this should not affect the definitions in GLM (at least not the default). We treat GLM\/1-parameter-LEF as QMLE or moment estimator like OLS.\r\n\r\n\r\n\r\n\r\n","comments":["related: we need to take into account that there might be a constrained fit.\r\nfitted k_params is not equal to len(params)\r\n\r\ne.g. \r\nTestMyParetoRestriction in miscmodels test_generic_mle has unit test for df_resid. I'm not sure that's correct.\r\nAlso we have base `fit_constrained`, `_fit_zeros`, `_fit_collinear`, but I don't remember if that already works for some models with extra shape params, k_extra>0, like NB, GPP.\r\n\r\n"],"labels":["prio-high","comp-base","design","comp-discrete","backwards-incompat"]},{"title":"BUG: perfect prediction check in DiscreteModel only applies to binary? no offset","body":"while browsing and skimming some code\r\n\r\nThe perfect prediction check in DiscreteModel ignores offset (new enhancement) and code uses properties that apply only for binary models, Logit and Probit.\r\nIf we would check for perfect prediction in countmodels like Poisson, then we would also need to include exposure.\r\n\r\n\r\n```\r\n    def _check_perfect_pred(self, params, *args):\r\n        endog = self.endog\r\n        fittedvalues = self.cdf(np.dot(self.exog, params[:self.exog.shape[1]]))\r\n        if (self.raise_on_perfect_prediction and\r\n                np.allclose(fittedvalues - endog, 0)):\r\n            msg = \"Perfect separation detected, results not available\"\r\n            raise PerfectSeparationError(msg)\r\n```\r\n\r\nI guess currently this method is only called by binary models.\r\n","comments":["simple fix is to replace the explicit computation by call to predict","bump, this is still a bug, now that we have offset in discrete binary models"],"labels":["prio-high","type-bug","comp-discrete"]},{"title":"BUG: default df_resid, df_model in GenericLikelihoodModel","body":"It looks like the current GenericLikelihoodModel does not set df_resid, df_model correctly.\r\nsee also #7767 and #7759 for df and #4679 for more refactoring\r\n\r\nI converted an old gist from a blog article to work with current version.\r\nIt uses GenericLikelihoodModel for ZIP (without regressors)\r\nhttps:\/\/www.austinrochford.com\/posts\/2015-03-03-mle-python-statsmodels.html\r\n\r\nmy notebook for checking this is `mle_custom_zip.ipynb`\r\n\r\nfitting the model emits warnings\r\n```\r\n...statsmodels\\base\\model.py:2733: UserWarning: df_model + k_constant differs from nparams\r\n  warnings.warn(\"df_model + k_constant differs from nparams\")\r\n...statsmodels\\statsmodels\\base\\model.py:2735: UserWarning: df_resid differs from nobs - nparams\r\n  warnings.warn(\"df_resid differs from nobs - nparams\")\r\n```\r\n\r\nand df_resid, df_model are wrong\r\n\r\n```\r\nN, model.df_resid, model.df_model, results.df_resid, results.df_model\r\n(1000, 1000.0, -1.0, 1000.0, -1.0)\r\n```\r\n\r\nThe model code does not set anything related to df_xxx, \r\n\r\n```\r\nclass ZeroInflatedPoisson(GenericLikelihoodModel):\r\n    def __init__(self, endog, exog=None, **kwds):\r\n        if exog is None:\r\n            exog = np.zeros_like(endog)\r\n            \r\n        super(ZeroInflatedPoisson, self).__init__(endog, exog, **kwds)\r\n    \r\n    def nloglikeobs(self, params):\r\n        pi = params[0]\r\n        lambda_ = params[1]\r\n\r\n        return -np.log(zip_pmf(self.endog, pi=pi, lambda_=lambda_))\r\n    \r\n    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\r\n        if start_params is None:\r\n            lambda_start = self.endog.mean()\r\n            excess_zeros = (self.endog == 0).mean() - stats.poisson.pmf(0, lambda_start)\r\n            \r\n            start_params = np.array([excess_zeros, lambda_start])\r\n            \r\n        return super(ZeroInflatedPoisson, self).fit(start_params=start_params,\r\n                                                    maxiter=maxiter, maxfun=maxfun, **kwds)\r\n```\r\n\r\n~~One guess for the bug is that the model code specifies neither n_params\/k_params, nor a `start_params` with correct `len`.~~\r\nusing explicit start_params does not help. The `__init__` does not seem to have an option to set k_params.\r\n\r\nI guess there is some problem also if exog=None.\r\n\r\n`fit` sets the `_set_extra_params_names` correctly, but does not change df_xxx.\r\n","comments":["likely problem\r\n\r\nGenericLikelihoodModelResults has\r\n```\r\n        if hasattr(model, 'df_model'):\r\n            self.df_model = model.df_model\r\n        else:\r\n            self.df_model = len(mlefit.params)\r\n            # retrofitting the model, used in t_test TODO: check design\r\n            self.model.df_model = self.df_model\r\n```\r\n\r\nhasattr might never apply,\r\ncreating the model will always set df_resid, df_model, but they could be nan if exog is None.\r\n\r\nHowever exog is a column of zeros even though it is not used (except for k_params, df_xxx setting)\r\n\r\n```\r\nmodel2 = ZeroInflatedPoisson(x)\r\nmodel2.df_resid\r\n1000.0\r\nmodel2.df_model\r\n-1.0\r\nmodel2.exog.shape, model.exog.mean(), model.exog.var()\r\n((1000, 1), 0.0, 0.0)\r\n```\r\n","more experimenting:\r\n\r\nIf df_resid depends on exog, then it is correct:\r\n\r\n```\r\nexog=np.zeros((N, 2))   # rank is zero, no change for df_xxx\r\nexog=np.ones((N, 2))   # rank is one, df_resid decreases by one, df_model=0\r\nexog=np.random.randn(N, 2)   # rank is one, df_resid decreases by two, df_model=1\r\n\r\nmodel2 = ZeroInflatedPoisson(x, exog=exog) # no effect for df_xxx: hasconst=False) #, nparams=2)\r\n```\r\n\r\nusing extra_params_names does not help for df_resid\r\n\r\n```\r\nmodel2 = ZeroInflatedPoisson(x, extra_params_names=[\"pi\", \"lambda\"])\r\nresults2 = model2.fit()\r\nN, model2.df_resid, model2.df_model, results2.df_resid, results2.df_model\r\n\r\nOptimization terminated successfully.\r\n         Current function value: 1.586641\r\n         Iterations: 37\r\n         Function evaluations: 70\r\n...\\statsmodels\\statsmodels\\base\\model.py:1031: ValueWarning: more exog_names than parameters\r\n  warnings.warn('more exog_names than parameters', ValueWarning)\r\n\r\n(1000, 1000.0, -1.0, 1000.0, -1.0)\r\n```","back to hasattr in Generic Results class:\r\n\r\nI just realized that setting exog to column of zeros is in custom code, i.e. user defined subclass.\r\nWhen I drop this, then I get the nan in model.df_resid and model.df_model\r\n\r\n```\r\nmodel2 = ZeroInflatedPoisson(x, extra_params_names=[\"pi\", \"lambda\"])\r\nresults2 = model2.fit()\r\nN, model2.df_resid, model2.df_model, results2.df_resid, results2.df_model\r\nOptimization terminated successfully.\r\n         Current function value: 1.586641\r\n         Iterations: 37\r\n         Function evaluations: 70\r\n(1000, nan, nan, nan, nan)\r\n\r\n# with start_params, no effect\r\nmodel2 = ZeroInflatedPoisson(x, extra_params_names=[\"pi\", \"lambda\"])\r\nresults2 = model2.fit(start_params=np.array([0.5, 2]))\r\nN, model2.df_resid, model2.df_model, results2.df_resid, results2.df_model\r\nOptimization terminated successfully.\r\n         Current function value: 1.586641\r\n         Iterations: 36\r\n         Function evaluations: 69\r\n(1000, nan, nan, nan, nan)\r\n\r\nrepr(model2.exog)\r\n'None'\r\n```\r\n\r\nThis means that the hasattr check needs to be replaced or combined with a `nan` check in `GenericLikelihoodModelResults.__init__`\r\n\r\nTo confirm that the hasattr check works, if I `del` the two df_xxx, then the df are correctly set.\r\n\r\n```\r\n'\r\nmodel2 = ZeroInflatedPoisson(x, extra_params_names=[\"pi\", \"lambda\"])\r\ndel model2.df_resid\r\ndel model2.df_model\r\nresults2 = model2.fit(start_params=np.array([0.5, 2]))\r\nN, model2.df_resid, model2.df_model, results2.df_resid, results2.df_model\r\n\r\nOptimization terminated successfully.\r\n         Current function value: 1.586641\r\n         Iterations: 36\r\n         Function evaluations: 69\r\n(1000, 998, 2, 998, 2)\r\n```\r\n\r\nOne remaining (old) problem, df_model and hasconst.\r\nby default the code often assumes hasconst is true and we have one constant.\r\nsee df_model should be excess compared to df_null. issue ?\r\n","adding the nan check, df_resid works correctly in the example case\r\n\r\n```\r\n        if hasattr(model, 'df_resid') and not np.isnan(model.df_resid):\r\n            self.df_resid = model.df_resid\r\n        else:\r\n            self.df_resid = self.endog.shape[0] - self.df_model\r\n            # retrofitting the model, used in t_test TODO: check design\r\n            self.model.df_resid = self.df_resid\r\n```\r\n\r\nHowever, df_model will be 2 in this case, i.e. the same as k_params.\r\nHowever, if we add df_null or k_null, and define df_model as number of \"excess\" parameters, `df_model = k_params - k_null`, then df_model should be zero in this case.\r\n\r\nBetaReg seems to define df_model currently as `self.df_model = self.nparams - 1`\r\nsee https:\/\/github.com\/statsmodels\/statsmodels\/issues\/7584#issuecomment-881689009 and related comments\r\n","case 1 exog, 1 extra parameter: \r\ndf_resid subtracts only 1 parameter, i.e. the exog parameters\r\n\r\nAFAICS, there is no `k_extra` handling in generic MLE as in discrete negbin, GPP, ...\r\nAsides:\r\n `base.model` has k_extra only in `LikelihoodModel._fit_zeros` \r\nLikelihoodModel also has `_fit_collinear` which I have not use in years and is, I guess, not advertised at all.\r\n\r\n```\r\nmodel2 = ZeroInflatedPoisson(x, exog=np.ones(N), extra_params_names=[\"lambda\"])\r\nresults2 = model2.fit()\r\nN, model2.df_resid, model2.df_model, results2.df_resid, results2.df_model, model2.exog_names\r\nOptimization terminated successfully.\r\n         Current function value: 1.586641\r\n         Iterations: 37\r\n         Function evaluations: 70\r\n(1000, 999.0, 0.0, 999.0, 0.0, ['const', 'lambda'])\r\n```\r\n\r\ncompared to discrete count models: NBP, GPP\r\n\r\nAFAICS, exog_extra is handled in individual results methods like aic, but does not affect df_model.\r\nThat means extra params are correctly excluded from the model excess over the Null model, e.g. in llr_pvalue.\r\nBut we don't have generic code based on k_params, i.e. the number of all estimated parameters.\r\nI don't see (yet) in the code if or where k_extra is included in df_resid computation.\r\n\r\n","`nparams` is set in `GenericLikelihoodModel._set_extra_params_names` based on `len(self.exog_names)`. It only applies if the user provides extra_params_names\r\nHowever, AFAIR, `nparams` is not really used anymore. (It was an idea that didn't take hold yet. Newer convention is k_params.)\r\n\r\nexample previous model has correct nparams\r\n```\r\nmodel2.nparams\r\n2\r\n```\r\n\r\nwe can add setting k_extra in the same or similar way.","one more\r\n\r\nunit test run on azur has warnings for copula models\r\n\r\n```\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVHR::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVAsymMixed::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVAsymLogistic::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestFrank::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestGaussian::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestClayton::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestGumbel::test\r\n  \/home\/vsts\/work\/1\/s\/statsmodels\/base\/model.py:2741: UserWarning: df_model + k_constant + k_extra differs from k_params\r\n    warnings.warn(\"df_model + k_constant + k_extra \"\r\n\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVHR::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVAsymMixed::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestEVAsymLogistic::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestFrank::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestGaussian::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestClayton::test\r\nstatsmodels\/distributions\/copula\/tests\/test_model.py::TestGumbel::test\r\n  \/home\/vsts\/work\/1\/s\/statsmodels\/base\/model.py:2745: UserWarning: df_resid differs from nobs - k_params\r\n    warnings.warn(\"df_resid differs from nobs - k_params\")\r\n```\r\n"],"labels":["type-bug","comp-base","prio-elev"]},{"title":"Adding a Jackknife CRV3 Covariance Matrix Estimator","body":"Hi all, \r\n\r\nWhile implementing [support for wild cluster bootstrapping ](https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8437)for the OLS method, @amichuda and I also plan to implement a fast jackknife based CRV3 variance covariance estimator based on recent work by MacKinnon, Nielsen & Webb (here is a [brief summary](https:\/\/s3alfisc.github.io\/summclust\/articles\/Cluster-Robust-Variance-Estimators-CRV-1-3.html) I've written at some point). The major innovation by MNW is that their Jackknife interpretation makes it feasible to compute a CRV3 estimator with little computational effort. Would you be interested in such a feature not only for use with the wild cluster bootstrap, but in general for inference for `OLS` models? Again, the idea would be to add a new option to the `cov_type` argument in `.fit()`. If yes, we'll go ahead and prepare a second PR for it. \r\n\r\nBest, Alex ","comments":["Yes, very interested, that would also be a great addition.\r\ntwo older issues that mention CR3: [#1099](https:\/\/github.com\/statsmodels\/statsmodels\/issues\/1099#issuecomment-25642148) https:\/\/github.com\/statsmodels\/statsmodels\/issues\/1201#issuecomment-307647209\r\n\r\nreference MNW is, I guess\r\nJames G. MacKinnon, Morten \u00d8rregaard Nielsen, Matthew D. Webb,\r\nCluster-robust inference: A guide to empirical practice,\r\nJournal of Econometrics,\r\n2022,\r\nhttps:\/\/doi.org\/10.1016\/j.jeconom.2022.04.001.","Great - we'll work on setting this up as well (it should be fairly straightforward). You can expect to hear back from us rather in a matter of weeks than months. :) \r\n\r\nA reference which walks through the equivalence of the jackknife and CRV3 can be found in yet [another MNW](https:\/\/www.econ.queensu.ca\/sites\/econ.queensu.ca\/files\/wpaper\/qed_wp_1485.pdf) paper. \r\n"],"labels":["type-enh","comp-regression","topic-covtype"]},{"title":"`pairwise_tukeyhsd` output table and `meandiff` confusing convention","body":"The meandiff column of the result's table reports the difference of means for group 2 vs group 1. However, the table construction suggests otherwise. \r\n\r\nWhy is that?\r\n\r\nLet's consider the following code:\r\n\r\n```\r\nimport numpy as np\r\n\r\nnp.random.seed(0)\r\nx = np.random.choice(['A','B','C'], 50)\r\ny = np.random.rand(50)\r\n\r\nprint(pairwise_tukeyhsd(y,x))\r\n\r\n\r\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\r\n===================================================\r\ngroup1 group2 meandiff p-adj   lower  upper  reject\r\n---------------------------------------------------\r\n     A      B   0.1506 0.2347   -0.07 0.3712  False\r\n     A      C   0.1105 0.5064 -0.1278 0.3487  False\r\n     B      C  -0.0401    0.9 -0.2865 0.2063  False\r\n---------------------------------------------------\r\n\r\n```\r\n\r\nby looking at the above table, one would say that the difference of **groupA** and **group2** means is **0.15.** However, this is the difference of group2 - group1. \r\n\r\n```\r\ny[x=='A'].mean() -  y[x=='B'].mean()\r\n-0.1505750775332967\r\n```","comments":["I'm not sure what to do here. \r\n\r\nI was using mostly triu_indices (upper triangular) and comparison using groups 1 (the diagonal group) as reference looks nicer for direct comparisons.\r\nBut it's less obvious, the new standard in two-sample is always diff = group1 - group2\r\n\r\nAlso, based on a quick look at the code, both allpairstest and kruskal methods of MultiComparison use group1 - group2 as diff\r\n\r\nChanges for this will not be backwards compatible, but the plan is to get multicomp out of the sandbox for 0.15\r\nwith design review and enhancements which might not be backwards compatible.\r\n","one quick possibility would be to change column name \"meandiff\" to \"meandiff (g2-g1)\"","another consistency check\r\n\r\nbase LikelihoodModelResults.t_test_pairwise uses contrast_allpairs from sandbox multicomp\r\nwhich also looks like it has group1 - group2 diff\r\n","I guess what would be best would be tril_indices but sorted in Fortran order for display, or flipping group1 and group2 indices of triu_indices."],"labels":["comp-stats","type-refactor","backwards-incompat"]},{"title":"REF: inconsistencies in GLM, discrete","body":"for now just to park some issues that need additional checking, review and possibly refactoring\r\n\r\n- GLM scale, df_resid, score_test, fit_constrained #7840  ","comments":[],"labels":["comp-genmod","comp-discrete","type-refactor"]},{"title":"adfuller gives me an incorrect result. Is the numerical stability low ?","body":"#### Describe the bug\r\n\r\nadfuller gives me very different results if I just slighlty change (add 1e-8 to) my series [1,3,5,7,...,195,197,199].\r\n\r\n**edit** see also #8456  results differ across environments\r\n\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```\r\nfrom statsmodels.tsa.stattools import adfuller\r\n\r\nprint(adfuller([2*i+1 for i in range(99)]))\r\n# It is stationary: (-8.042419324497985, 1.8319240045049658e-12, 7, 91, {'1%': -3.50434289821397, '5%': -2.8938659630479413, '10%': -2.5840147047458037}, -5852.425844993292)\r\nprint(adfuller([2*i+1+1e-8 for i in range(99)]))\r\n# It is not stationary: (0.9120132371109988, 0.9932451165937419, 2, 96, {'1%': -3.5003788874873405, '5%': -2.8921519665075235, '10%': -2.5830997960069446}, -5522.232946110268)\r\nprint(adfuller([2*i+1 for i in range(98)]))\r\n# It is not stationary: (2.5578687056001446, 0.9990661662898507, 8, 89, {'1%': -3.506057133647011, '5%': -2.8946066061911946, '10%': -2.5844100201994697}, -5852.890084170324)\r\nprint(adfuller([2*i+1 for i in range(100)]))\r\n# It is not stationary: (4.298488441509754, 1.0, 4, 95, {'1%': -3.5011373281819504, '5%': -2.8924800524857854, '10%': -2.5832749307479226}, -5904.713330313655)\r\n```\r\n\r\n#### Expected Output\r\n\r\nThe results of the different `print` should be similar. The series [1,3,5,7,...,195,197,199] is definitely not stationary.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.10.4.final.0\r\nOS: Linux 5.15.0-50-generic #56-Ubuntu SMP Tue Sep 20 13:23:26 UTC 2022 x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.2 (\/home\/victor\/.local\/lib\/python3.10\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.21.5 (\/usr\/lib\/python3\/dist-packages\/numpy)\r\nscipy: 1.8.0 (\/usr\/lib\/python3\/dist-packages\/scipy)\r\npandas: 1.4.3 (\/home\/victor\/.local\/lib\/python3.10\/site-packages\/pandas)\r\n    dateutil: 2.8.1 (\/usr\/lib\/python3\/dist-packages\/dateutil)\r\npatsy: 0.5.3 (\/home\/victor\/.local\/lib\/python3.10\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.5.1 (\/usr\/lib\/python3\/dist-packages\/matplotlib)\r\n    backend: TkAgg \r\ncvxopt: Not installed\r\njoblib: 1.1.0 (\/home\/victor\/.local\/lib\/python3.10\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 7.31.1 (\/usr\/lib\/python3\/dist-packages\/IPython)\r\n    jinja2: 3.0.3 (\/usr\/lib\/python3\/dist-packages\/jinja2)\r\nsphinx: Not installed\r\n    pygments: 2.11.2 (\/usr\/lib\/python3\/dist-packages\/pygments)\r\npytest: Not installed\r\nvirtualenv: 20.13.0+ds (\/usr\/lib\/python3\/dist-packages\/virtualenv)\r\n\r\n<\/details>\r\n","comments":["There is no noise, there is perfect prediction so ssr is 0 + numerical noise\r\n\r\nI get\r\n```\r\nstatsmodels\\statsmodels\\regression\\linear_model.py:951: RuntimeWarning: divide by zero encountered in log\r\n  llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr \/ nobs) - nobs2\r\n```","Thank you. I guess I will make my own simple test to catch those edge cases (perfect prediction) before applying `adfuller`.\r\n\r\n---\r\n\r\n```\r\nRuntimeWarning: divide by zero encountered in log\r\n  llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr \/ nobs) - nobs2\r\n```\r\n\r\nI only get this warning when I try `adfuller` on a constant array or on a multiple of `np.arange`.","converting issue to a FAQ issue, so we can keep it open.\r\n\r\npossible change in statsmodels: \r\nWe could catch almost perfect prediction, e.g. if ssr \/ tss < tol, \r\nwith maybe tol = 1e-14 for numerical precision, or larger for \"statistical\" precision.  (tss might also be zero if endog is constant)\r\nand raise or return nan","adfuller raises now #8537 exception for constant series, but not for the general perfect prediction case\r\n"],"labels":["comp-tsa","FAQ"]},{"title":"ENH: OrderedModel - make params_table for linear thresholds available","body":"two statsexchange question why threshold parameters in OrderedModel differ from SPSS and R.\r\n\r\nhttps:\/\/stats.stackexchange.com\/questions\/592680\/why-do-statsmodels-python-and-mass-r-output-different-intercept-values-for-m\r\n\r\nwe already have helper methods for converting threshold params\r\n\r\nWe could also make standard errors available so that we can create a params_table with linear coefficients instead of the current parameterization.\r\nThis will require NonlinearDeltaCov because of multiple correlated parameters and nonlinear functions.","comments":["+1"],"labels":["type-enh","comp-discrete"]},{"title":"BUG: don't raise if initial x is singular in recursive_olsresiduals, linear_harvey_collier","body":"linear_harvey_collier raises exception if initial ols exog is singular.\r\nharvey_collier uses recursive_olsresiduals which raises the exception\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/74084265\/statsmodels-linear-harveu-collier-test-valueerror\/74090601#74090601\r\n\r\nThis is not necessary, we already allow for using Ridge, but could use pinv also.\r\nRidge is only used for the initial estimate but not for updating in recursive residuals.\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/statsmodels\/stats\/diagnostic.py#L1461\r\n\r\nEssentially, the requirement of nonsingular initial exog is too strong.\r\nBecause initial nobs = k_vars, we loose rank as soon as one row is duplicate which could happen if we have discrete or discretized continuous variables.\r\n\r\nalso:\r\nharvey_collier does not have option for ridge coefficient lamda to be used in recursive_olsresiduals\r\n\r\n ","comments":["possibly another bug\r\n\r\nWhy does harvey_collier skip the first 3 obs of rresid_standardized ?\r\n`return stats.ttest_1samp(rr[3][3:], 0)`\r\n\r\nShouldn't this depend on `skip`?\r\n"],"labels":["type-bug","type-enh","comp-stats","topic-diagnostic"]},{"title":"ENH: new ordered models, add generalized ordered logit and generalized ordered link models","body":"see comment https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8442#issuecomment-1279662530 and following\r\n\r\nIt's also a cumulative link model, where params differ by choice P(y < k) = F(x beta_k)\r\n\r\nMy guess the only or main difference to OrderedModel is to split up params in `_bounds` and `linpred` to compute thresholds with category level specific params_{k} and params_{k-1} \r\n\r\nloglike_obs just uses `_bounds` method\r\nscore and hessian are currently computed by numdiff\r\n\r\nnot checked yet:\r\nI guess there is a computational problem when integration `_bounds` are not increasing,\r\nI guess we need to directly impose nonnegativity of probabilities, prob = max(F(upp) - F(low), 0).\r\nDo we still get sum(probs) = 1 if non-negativity constraint is binding?\r\n\r\nWilliams 2016 has section 5 on p. 18 about negative probabilities, sounds like they are not clipped to zero\r\n\r\nWilliams, Richard. \u201cUnderstanding and Interpreting Generalized Ordered Logit Models.\u201d The Journal of Mathematical Sociology 40, no. 1 (January 2, 2016): 7\u201320. https:\/\/doi.org\/10.1080\/0022250X.2015.1112384.\r\n\r\n  \r\n\r\n\r\n\r\n","comments":["another thought\r\n\r\ngologit corresponds to mnlogit in the sense of common exog but choice specific params\r\n\r\nI guess a more general version would be the analogy to conditional mnlogit, i.e. common params but choice specific exog\r\nThis encompasses gologit but not the other way around.\r\n\r\nFor common exog, as in gologit, we would be replicating exog k times, i.e. number of choices and non-common exog should not be too large\r\nif we compose the design matrix as [exog_common, exog_k1, exog_k2, ....]\r\n\r\nIs this correct?  How can we know when creating the design matrix which exog_k belong to an observation?\r\n???\r\nIt has been a long time since I looked at this for variants of discrete choice models.\r\n\r\nI guess what this means is that we cannot create a single design matrix, we need to keep several different exog_k and have linear predictor for each k (one params, many exog_k)\r\ne.g.\r\nprob(y = k) = f(b * x_k) or = F(a_k + b x_k) - F(a_{k-1} b x_{k-1})\r\n\r\nSo, I have the option to either have one exog and choice specific params (including overlapping params) or have several exog, common and choice specific ones.\r\nI guess those two are equivalent in generality if I allow for arbitrary mappings\/selection of params -> params_k or exog_all -> exog_k.\r\nEverything is just collected inside linpred_k, i.e. params_k * exog_k\r\n`_bounds` needs to call choice specific linpred(..., choice=k), similar to choice specific threshold\/constants.\r\n\r\npossible implementation for using params_k and one exog:\r\n\r\nparams_k = params[mask_k]\r\nparams_e[] = np.zeros(len(params))\r\nparams_e[mask_e_k] = params_k\r\nwhere `mask`s can be boolean or an integer index arrays\r\n\r\nThis requires full dot product params_e * exog_all where params_e can have many zeros, but we don't need to duplicate exog data arrays.\r\n\r\nsimples case: full gologit\r\nmask_e_k is all true, or includes all indices (i.e. skip if None)\r\nparams_k = params.reshape[-1, k_exog][k]  i.e. just the kth slice\r\n\r\n\r\n**edit**\r\nanother possibility use linalg instead of masks\r\nparams_k = C_k dot params\r\n\r\nThis would make it easier to include linear restrictions on parameters,, i.e. mapping from restricted to full parameters.\r\nIn the simplest case it would be just an identity matrix in a submatrix of C_k\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","Green, Hensher Primer section 7.2 \r\nhas discussion of generalized logit and an alternative specification that can be used to impose monotonicity in integration limits to avoid negative probabilities"],"labels":["type-enh","comp-discrete"]},{"title":"ENH: post-estimation, margins for OrderedModel","body":"(just some thought while looking at nonproportional ordered Logit models)\r\n\r\nHow far can we already support post-estimation features for OrderedModel?\r\n\r\nexample:\r\n\r\nmargins: d Prob(y = k | x) \/ d x\r\nit's nonlinear, so we need to use e.g. `_get_wald_nonlinear` for delta method\r\nnote: target might be a vector, e.g. derivative of prob for all choices k,  similarly to predicted probs of choices.\r\ncontext #5387 I'm looking at different ways to compute margins for more cases\r\n\r\nsimilarly: what `which` should we add to predict\/get_prediction\r\n\r\npossible use\r\nin analogy to binary models: average risk ratio (population average margin versus log-binomial)\r\nIn Ordered model case, we might want to have other statistics of interest related to competing models, e.g. continuation-ratio prob.\r\n(separate choice of model class from target statistic, e.g. we want to estimate average risk ratio even if the underlying model does not specify constant risk ratio conditional on exog)\r\n","comments":["A brief google\/google-scholar search does not find anything for influence, outlier measures for ordinal logit and similar search terms.\r\n\r\nbrief check of `MLEInfluence(res_odered)` fails with\r\n`AttributeError: 'OrderedModel' object has no attribute '_deriv_score_obs_dendog'`\r\nfor several attributes\r\n\r\nresid is none, `resid_score()` is a 1-dim nobs array.\r\n\r\naside:\r\n`infl.hat_matrix_exog_diag` is Mahalonibis distance for exog.\r\n(same as direct computation) ","score_test is missing\r\n\r\nget_distribution:\r\n\r\nscipy has multinomial distribution\r\nHowever, rvs used from numpy is not vectorized in probabilites.\r\nAFAIR, I used my own simple function for rvs before.\r\n\r\ndistr pmf agrees with exp(loglikeobs)\r\nno cdf method\r\nthe only other interesting method is `cov`\r\n\r\n```\r\nprobs = res_prob.predict()\r\nprobs.shape\r\n(400, 3)\r\n\u200b\r\nfrom scipy.stats import multinomial\r\n\r\ndistr_m = multinomial(n=1, p=probs[:5])\r\n```","The multinomial generator in the numoy Generator object is vectorized,and\niirc supports broadcasting.\n\n\nOn Sat, Mar 4, 2023, 21:22 Josef Perktold ***@***.***> wrote:\n\n> score_test is missing\n>\n> get_distribution:\n>\n> scipy has multinomial distribution\n> However, rvs used from numpy is not vectorized in probabilites.\n> AFAIR, I used my own simple function for rvs before.\n>\n> distr pmf agrees with exp(loglikeobs)\n> no cdf method\n> the only other interesting method is cov\n>\n> probs = res_prob.predict()\n>\n> probs.shape\n>\n> (400, 3)\n>\n> \u200b\n>\n> from scipy.stats import multinomial\n>\n>\n>\n> distr_m = multinomial(n=1, p=probs[:5])\n>\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8443#issuecomment-1454891553>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ABKTSRPXF2SUDMXV5VOYWW3W2OXAZANCNFSM6AAAAAARFY22RI>\n> .\n> You are receiving this because you are subscribed to this thread.Message\n> ID: ***@***.***>\n>\n","Thanks, I will try it out","broadcasting was added in numpy 1.22\r\nI'm currently using numpy 1.21.5 which does not support vectorized probabilities "],"labels":["type-enh","comp-discrete","topic-post_estim"]},{"title":"A Class for Brant test for OrderedModel (using Python)","body":"#### Is your feature request related to a problem? Please describe\r\nAbsence of brant test for PRA in ordered models \r\n\r\n#### Solution\r\nI have written a class that works for OrderedModels.from_formula by getting inspiration from the brant test in R\r\n\r\nI am wondering whether it might be useful as an addendum to OrderedModels in statstmodels.\r\n\r\nI lack any experience with submitting patches to existing packages such as statsmodels. \r\n","comments":["Yes, it would be a good addition to statsmodels. We had a request for it before, #7892\r\nI would like to see more diagnostic and specification tests for different models in statsmodels. \r\n\r\nTo submit a full pull request, you would need to set up a development environment for statsmodels.\r\nHowever, most of it could be written in a special module and would not need a full pull request. Destination would be in a diagnostic or `_diagnostic` module in statsmodels.discrete.\r\nThe advantages of a pull request is that we have github support for discussion and changes during development.\r\n\r\nNote, we cannot translate code from R (or other packages) that is GPL licensed, because that's incompatible with our license.","Hello,\r\n\r\nAs a note it is not a translation from R. It is basically going back to Brant\u2019s original paper published in 1990 while getting some insights from how it was done in R.\r\n\r\n","> As a note it is not a translation from R. It is basically going back to Brant\u2019s original paper published in 1990 while getting some insights from how it was done in R.\r\n\r\nGreat, no problem then\r\nI often look at R (and Stata, ..) docs for information and design issues, and R and Stata example for unit tests.\r\n","My biggest obstacle is to learn how to go through the process of \u2018adding\u2019 it as a possible diagnostic test within statsmodels. By that I mean that I have never been through the steps of testing and so on. All what I did is to write the code for the class and test it for a good number of data sets, comparing the results with what one gets using the brant test in R. Any guide is this respect will be appreciated.\r\n\r\n\r\n\r\nFrom: Josef Perktold ***@***.***>\r\nDate: Friday, October 14, 2022 at 2:48 PM\r\nTo: statsmodels\/statsmodels ***@***.***>\r\nCc: Arfi,Badredine ***@***.***>, Author ***@***.***>\r\nSubject: Re: [statsmodels\/statsmodels] A Class for Brant test for OrderedModel (using Python) (Issue #8442)\r\n[External Email]\r\n\r\nAs a note it is not a translation from R. It is basically going back to Brant\u2019s original paper published in 1990 while getting some insights from how it was done in R.\r\n\r\nGreat, no problem then\r\nI often look at R (and Stata, ..) docs for information and design issues, and R and Stata example for unit tests.\r\n\r\n\u2014\r\n","First, I often help or finish up larger PRs to integrate them and make it consistent with our code.\r\nThe time consuming part is understanding and coding the underlying statistics or econometrics. \r\n\r\nabout testing\r\nunit tests are in the test modules with results from other packages (for me mainly R or Stata)\r\nfor example\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/statsmodels\/miscmodels\/tests\/test_ordinal_model.py#L171\r\ncontains some test for diagnostics in ordered Logit.\r\nexpected results are in `from .results.results_ordinal_model import res_ord_logit as res2`\r\n\r\nlocation for code diagnostic test\r\nThis is still somewhat unclear. I only started some time ago to add diagnostic and specification tests for discrete models.\r\nThe code is currently spread over several `diagnostic` modules.\r\nIn future the actual module location will be less important because we will have `get_diagnostic` in results classes.\r\nI started that with Poisson and count models.\r\n(Note, modules or functions with leading underline in name are private and might change location. Names without leading underline indicates public, user facing modules or functions.)\r\nI guess a good location would be a new module discrete `_diagnostic_multinomial` given that this is a model specific diagnostic test. \r\n(for example, discrete._diagnostics_count currently has zero-inflation tests for Poisson with public access through discrete.diagnostic and PoissonResults.get_diagnostic)\r\n \r\n","related question:\r\n\r\nDo you think there is demand from the user side for generalized order logit such as the following for Stata?\r\nWilliams, R. (2006). Generalized Ordered Logit\/Partial Proportional Odds Models for Ordinal Dependent Variables. The Stata Journal, 6(1), 58\u201382. https:\/\/doi.org\/10.1177\/1536867X0600600104\r\n\r\nI haven't looked at many details yet, but most likely it can resuse most code of the current OrderedModel classes\r\nThis would be one alternative if the proportional odds or constant parameter assumption does not hold.\r\n","Yes there is an important demand for generalized ordered logic . The brant  test is extremely important to be had in statsmodels because most of the datasets on ordinal response variable violated the parallel regression assumption, which you cannot statistically establish if there is no test for it (since it is a post-estimation test).  However, once you discover that the PRA is violated, there isn't much alternative in statstmodels. I know from my teaching years ago using stata that gologit2 was a life saver -- having an equivalent one in statsmodels will definitely be a wonderful addition. ","> #### Is your feature request related to a problem? Please describe\r\n> \r\n> Absence of brant test for PRA in ordered models\r\n> #### Solution\r\n> \r\n> I have written a class that works for OrderedModels.from_formula by getting inspiration from the brant test in R\r\n> \r\n> I am wondering whether it might be useful as an addendum to OrderedModels in statstmodels.\r\n> \r\n> I lack any experience with submitting patches to existing packages such as statsmodels.\r\n\r\nDo you mind sharing this, perhaps we can help you get this implemented. I'm personally looking at doing some ordinal regression in statsmodels right now for inference, but the lack of a way to conveniently test for the proportional odds assumption is a major turn-off and a reason to defer to R instead at the moment.","Thank you for coming back to me on this. Sure. I have I have it ready. I can attach the python file here if you want.","yes here it is attached.\r\n[pybrant.zip](https:\/\/github.com\/statsmodels\/statsmodels\/files\/10405671\/pybrant.zip)\r\n","@nkuehnle\r\nDid you get a chance to check the zip folder? thanks.","@fbarfi @nkuehnle I've checked the zip folder and unfortunately the code is not working at all. One error originating from line 78 was easy to solve:\r\n`var_names = model.model.design_info.column_names`\r\n\r\nIt should've been:\r\n`var_names = model.design_info.column_names`\r\n\r\nAfter I fixed this line, the attribute error occurs: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/56788563\/223659572-6658557c-d8f1-442d-876f-41b3cbd00e91.png)\r\n\r\nI haven't solved it yet. I wonder if you tested the class before uploading it. ","Thanks Oskar for the feedback. Yes I did test it many times on my computer and I had no such errors. I'll do it again and get back to you. ","Hi @fbarfi , any comments?","Hi, Oskar\r\nApologies I've been traveling to conferences lately. I'll get to it soon and get back to you on that. Thanks!","Hello again, did you maybe have a time to look at that problem?","I am terribly sorry for my tardiness. I am attaching a zip folder which contains an example where I try pybrant with no problem as you can see for yourself. Let me know if this example works for you too.\r\nBest. \r\n[Example.zip](https:\/\/github.com\/statsmodels\/statsmodels\/files\/12327363\/Example.zip)\r\n","HI there.\r\nSince we communicated last time, I have given the pybrant code to many of my statistics students to use it in their research papers. So far it is working as it should. Best. ","Hello @fbarfi, I have tried your Python implementation and compared it with R's `brant`: the results turned out to be completely different. I haven't dived into your code, so I'm not sure of what's going there... but all the normal estimated parameters are equal in the two languages."],"labels":["type-enh","comp-discrete","topic-diagnostic"]},{"title":"ENH: summary params table, header option for first column","body":"\r\nIt would be useful to have an option to add a header for the first column in params_table\r\n\r\nexample: from #5387\r\nusing model.t_test for margin in continuous-discrete interaction  d E(y | x2=\"a\") \/ d x1\r\nfirst column is the value of the categorical variable\r\nd E(y | x2=\"a\") \/ d x1\r\nlast row using uniform distribution over categorical (e.g. fictional average using predefined population probabilities)\r\n\r\n```\r\nprint(tt.summary(xname=[\"a\", \"b\", \"c\", \"uniform\"], title=\"Marginal Effect of weight\"))\r\n                          Marginal Effect of weight                           \r\n==============================================================================\r\n                 coef    std err          t      P>|t|      [0.025      0.975]\r\n------------------------------------------------------------------------------\r\na              1.0502      0.024     43.048      0.000       1.002       1.099\r\nb              1.0059      0.028     35.436      0.000       0.950       1.062\r\nc              1.0138      0.029     34.734      0.000       0.956       1.072\r\nuniform        1.0233      0.023     44.871      0.000       0.978       1.069\r\n==============================================================================\r\n```\r\n\r\naside: still calling the t_test summary option `xname` is not pretty\r\nfor t_test `contrast_name` would be appropriate, but in the current case using a contrast is just a technical detail and not subject relevant.\r\nmaybe `row_names`\r\n","comments":["adding column label to each row name does not look as good as a header\r\n\r\n```\r\n\r\nprint(tt.summary(xname=[\"species \" + i for i in [\"a\", \"b\", \"c\", \"uniform\"]], \r\n                 title=\"Marginal Effect of weight\"))\r\n                             Marginal Effect of weight                             \r\n===================================================================================\r\n                      coef    std err          t      P>|t|      [0.025      0.975]\r\n-----------------------------------------------------------------------------------\r\nspecies a           1.0502      0.024     43.048      0.000       1.002       1.099\r\nspecies b           1.0059      0.028     35.436      0.000       0.950       1.062\r\nspecies c           1.0138      0.029     34.734      0.000       0.956       1.072\r\nspecies uniform     1.0233      0.023     44.871      0.000       0.978       1.069\r\n===================================================================================\r\n```"],"labels":["type-enh","comp-io"]},{"title":"FAQ: Hardcoded method in `ETSModel.fit()`","body":"In `ETSModel.fit()`, the optimization method is hardcoded as \"lbfgs\": https:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/statsmodels\/tsa\/exponential_smoothing\/ets.py#L1068\r\n\r\nCould it be parameterized like how it does in Holt-Winters? \r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/main\/statsmodels\/tsa\/holtwinters\/model.py#L941\r\n\r\nRefers to this post [here](https:\/\/github.com\/sktime\/sktime\/issues\/3162#issuecomment-1203150643), suspects that ETS model is not robust compared to R and `StatsForecast` due to the current optimization method. Would suggest having the option to apply `Nelder-Mead` method, which is the implementation from both `StatsForecast` and R. Refers to this [post](https:\/\/community.rstudio.com\/t\/parameter-estimation-in-ets-function-forecasting\/92508\/2) that stated R has implemented `Nelder-Mead` method as well:\r\n> It is a nonlinear constrained optimization using the Nelder-Mead algorithm\r\n","comments":["Thanks for the issue, this sounds like a good idea and taking a quick look at the code, it doesn't seem like there is anything preventing other optimization routines like Nelder-Mean from being used.\r\n\r\nThe post you're referencing gave a pathological example dataset for which the default method of choosing starting parameters in statsmodels - which is based on the first few observations - simply provided a very poor fit.  It is possible that Nelder-Mead would help get away from these very poor starting parameters.\r\n\r\nHowever, it is more likely that StatsForecast simply has a different default for computing starting parameters that didn't imply such bad starting parameters for this particular dataset. To be clear, though, that doesn't mean that StatsForecast outperforms Statsmodels' implementation generally, just that in this particular pathological example it did a better job."],"labels":["comp-tsa","type-enh"]},{"title":"ENH: predict tools, helper function using pandas for exog grid","body":"helper function for lsmeans, emmeans, predict exog\r\nrelated: #5387 ...\r\n\r\nCurrently we don't have any functions to create a grid of exog values for predict that affect more than a single column.\r\nRelevant for predict and marginal\/derivative effects.\r\n\r\nlsmeans, emmeans and other margin packages in R (and SAS, ...) have functions that create grids of exog values.\r\n\r\nFor us, the models do not have enough information about the original data, trying to build the grid there is too late.\r\n\r\nSo, what we need are helper functions for the user to process the original dataframe assuming there are no irrelevant columns.\r\nThis would be all using pandas dataframes and the corresponding methods for categorical variables and quantiles.\r\nUsing formulas and formula `transform` will convert this to a design matrix for predict and other methods consistent with the model specification.\r\n(Do we have `_get_predict_exog` in general?  AFAIR, I only added it in a few models.)\r\n\r\nbonus:\r\nfor purely categorical exog, we might also want to have freq or prob weights for cell frequencies or probabilities in the original sample.\r\n(new get_prediction in discrete allows for aggregation weights.)\r\n\r\n\r\n\r\n","comments":["a not very quick try:\r\nI didn't find any useful pandas methods, or I don't know it well enough to figure it out\r\n\r\nThe following works in my example (speed is the endog)\r\nThe get_col_values is an iterator to work with python `product` and can be extended to handle other dtypes like categorical.\r\nNot sure how to handle count data in exog. e.g. user provides list of count varnames, then either use all or round quantiles to int.\r\n\r\n```\r\n\r\nincluded = []\r\ndef get_col_values(data2, exclude=[\"const\", \"speed\"]):\r\n    for col in data2:\r\n        ser = data2[col]\r\n        if ser.name in exclude:\r\n            continue\r\n        if ser.dtype == np.float64:\r\n            values = ser.quantile(q=[0.1, 0.5, 0.9]).to_list()\r\n\r\n        if ser.dtype == object:\r\n            values = ser.unique().tolist()  # returns ndarray\r\n\r\n        included.append(ser.name)\r\n        yield values\r\n\r\n# based on https:\/\/stackoverflow.com\/a\/37755303\/333700  \r\n# preserves dtypes of values (but not meta info, e.g. categorical)\r\nresult = pd.DataFrame(list(product(*(get_col_values(data2)))), columns=included)\r\nresult.head()\r\n```","another question for predicted means at some exog values\r\nStata margin command defaults to predicted marginal means, not a marginal effect with \"marginal\" as derivative or difference.\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/75772170\/produce-predictive-margins-in-statsmodels-output-for-logistic-regression","something that would give us an original array\r\n\r\npredict_at where user needs to provide a DataFrame that includes all original variables used in `exog` and only those.\r\nThen we automatically construct a grid with whatever options for `at` are specified.\r\nThen we call get_prediction with the constructed \"exog\", which will still be formula transformed in get_prediction.\r\n\r\nThe same would be possible if we have the original DataFrame attached to the model and we can identify columns that were used in the formula.\r\n\r\npossible ambiguity, we need to know which variables are categorical if they have numeric levels, `C(cat)` in formula. those should use unique instead of mean.\r\nFractional exog for categorical variables (like mean gender in the population) would not be possible.\r\n"],"labels":["type-enh","comp-base","pandas-integration","comp-tools","topic-predict"]},{"title":"Adding a wild cluster bootstrap `cov_type` to models","body":"Hi! @s3alfisc and I are working on implementing [wild cluster bootstrapping](https:\/\/github.com\/s3alfisc\/wildboottest) into python and we wanted to add it as a pull request to `statsmodels`. It's still a work in progress, but is there a straightforward way of adding a new `cov_type`?\r\n\r\nWould you be able to tell me which files to look at and which classes to inherit to implement?\r\n\r\nI can see that there's a `cov_type` file in `base` but not sure if adding to that is enough\r\n\r\nThank you!","comments":["Look in the linear regression code to see how the covariance estimators are defined.  It isn't that clean IIRC.  ","That would be great\r\nFor which model do you want to add the new cov_type?\r\n\r\nsome details:\r\nIf you use bootstrapping, then it will not go through the sandwich code.\r\n\r\nThe selection and dispatch of robust cov_type methods is in  `get_robustcov_results`\r\nThose are not fully standardized and there is still some code duplication that never got cleaned up\r\n\r\nFor OLS and other linear model, the methods is in class statsmodels.regression.linear_model.RegressionResults\r\n\r\nMost other models (at least outside tsa), like discrete, GLM, use the generic implementation from `base`, \r\n`base.model.LikelihoodModelResults.__init__` calls  function `statsmodels.base.covtype.get_robustcov_results`\r\n\r\nI just see that `GLMResults.__init__` also explicitly calls that function.\r\n(I guess GLMResults does this because super is called at the beginning, but there is some processing before being able to create the final robust cov_params.)\r\n\r\nIf there are model specific cov_types, then they may be included in the `fit` method,\r\nfor example GLM has adjustments for OIM versus EIM and for excess dispersion, scale different from one in e.g. quasi-Poisson.\r\n\r\n","Great! We're planning on adding it to `linearmodels` as well ;)\r\n\r\nA cursory glance tells me that if we just add to `get_robustcov_results` in `base` that should propagate?\r\n\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/blob\/55aff1e2268f56cc26b45ddcc1f44e5f924f8d05\/statsmodels\/base\/covtype.py#L67\r\n\r\nThe aim is to apply it to linear models, so OLS and the like. I'm not actually sure about the properties in adding it to likelihood models or non-linear estimators.\r\n\r\n> For OLS and other linear model, the methods is in class statsmodels.regression.linear_model.RegressionResults\r\n\r\nI think I can add it here and provisionally to `base.get_robustcov_results` and see what happens \ud83d\udca5 ","Write most code outside the classes as in base.covtype so it can be generalized or reused later, and then just add it to \r\n statsmodels.regression.linear_model.RegressionResults.get_robustcov_results \r\n\r\nFor example `if cov_type in ['fixed scale', 'fixed_scale']:`\r\nwas added specifically and only to the linear Regression models.\r\n\r\nImplementing it only for the linear models will be easier than generically or for other models. \r\nRecreating a model with different options or data is  more fragile in other models because of many different extra keywords used in the XXXModel `__init__` across models","quick question\r\n\r\nThe wild clusterbootstrap is a residual bootstrap with fixed exog, or do you also randomize over which clusters are included?","Hi @josef-pkt and @bashtage , thanks so much for your feedback! \r\n\r\nIndeed, the exogenous variables are fixed in the wild bootstrap. We decided to implement the wild bootstrap instead of a block bootstrap because a) it seems to perform really well in a range of edge cases (e.g. few clusters, wildly different cluster sizes, few treated clusters etc) and b) because it can be implemented in really performant ways. \r\n\r\nThe simulation evidence that it works well is (to the best of my knowledge) so far restricted to linear models estimated by OLS - some of the main researchers in the literature do not consider it a good idea to wild (cluster) bootstrap nonlinear models (see e.g. this reply by @droodman [here](https:\/\/github.com\/s3alfisc\/fwildclusterboot\/issues\/46#issuecomment-1166324427) or these [slides by MacKinnon](https:\/\/www.math.kth.se\/matstat\/gru\/sf2930\/papers\/wild.bootstrap.pdf), which form a very good introduction to the wild bootstrap. \r\n\r\nWe plan to implement a range of new versions of the wild bootstrap, as discussed in the paper by [MacKinnon, Nielsen & Webb (2022)](https:\/\/www.econ.queensu.ca\/sites\/econ.queensu.ca\/files\/wpaper\/qed_wp_1485.pdf), which seem to perform even better than the \"classical\" wild cluster bootstrap as introduced in [Cameron et al](https:\/\/direct.mit.edu\/rest\/article-abstract\/90\/3\/414\/57731\/Bootstrap-Based-Improvements-for-Inference-with). As a side product, we might also implement a very fast jackknifed based estimator for CRV3 covariance matrices as described in in yet another paper by [MacKinnon, Nielsen and Webb](https:\/\/arxiv.org\/abs\/2205.03288) =) ","Great,, I'm looking forward to the PR.\r\n\r\nThanks for the references. \r\nI will try to catch up with those after the upcoming release. I have not looked at this literature since I implemented the cluster robust sandwiches.\r\n\r\nAFAIR, I have seen only bootstrap with resampling clusters for nonlinear, nongaussian or asymmetric models, GLM, Poisson,...\r\n\r\nAside to the model under null constraints:\r\nGLM has fit_constrained by reparameterizing the design matrix. However that is not implemented for OLS, WLS. At the time I implemented fit_constrained, I skipped OLS mainly because of the missing `offset` option in the linear regression models.\r\n","mainly as reference for myself: #2054 seems to be the last time I looked at this\r\n\r\nWe should have cluster robust score or lagrange multiplier test which would be the asymptotic competitor to the bootstrap version.\r\nHowever, I never tried it out for cluster robust (AFAIR)\r\n"],"labels":["type-enh","comp-regression","topic-covtype"]},{"title":"DOC\/REF follow-up to merge of notebooks","body":"some random observations while skimming new notebooks\r\n\r\n- rank_compare https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/stats_rankcompare.html\r\n  - test_prob_superior return does not include `value`\r\n  - summary: no indication that t-statistic is for value 0.5 and not 0, if it's easy to change use header `t (p=0.5)` instead of `t`\r\n- stats_poisson\r\n  - code lines too long for tolerance interval, confint quantile and others\r\n- ...\r\n- ...","comments":[],"labels":["comp-docs","comp-stats"]},{"title":"ENH\/REF: rank_compare_2ordinal has None attributes, missing","body":"The rankcompareresult instance of rank_compare_2ordinal does not have the statistics from the hypothesis test, those are None\r\nValues and explicitly calling the test agrees with rank_compare_2indep. see below\r\n\r\n`rank_compare_2indep` function has some code duplication with methods of the results holder class.\r\nIt looks like I didn't DRY this after adding the methods and extended to rank_compare_2ordinal.\r\n\r\nI think we can refactor this to have all the test code only in the results\/holder class. i.e. don't duplicate the part that `_tstat_generic` or similar are doing.\r\nIf we keep the hypothesis test as default statistic, pvalue, ..., then we can create or override None in `RankCompareResult.__init__` so that users don't need to add an extra method call to get the test results.\r\n\r\nA quickfix would be to do this based on `statistic is None`, so it adds the attributes for the ordinal version\r\n\r\n```\r\nres_o = rank_compare_2ordinal(new, active)\r\n\r\nres_o\r\n<class 'statsmodels.stats.nonparametric.RankCompareResult'>\r\nstatistic = None\r\npvalue = None\r\ns1 = None\r\ns2 = None\r\nvar1 = 0.0919524144954732\r\nvar2 = 0.06075972346751607\r\nvar = 0.3098544504968023\r\nvar_prob = 0.0014148605045516088\r\nnobs1 = 107\r\nnobs2 = 112\r\nnobs = 219\r\nmean1 = None\r\nmean2 = None\r\nprob1 = 0.4557743658210948\r\nprob2 = 0.5442256341789052\r\nsomersd1 = -0.08845126835781036\r\nsomersd2 = 0.08845126835781048\r\ndf = 204.29842398679557\r\nuse_t = True\r\ntuple = (None, None)\r\n```","comments":[],"labels":["type-enh","comp-stats","type-refactor"]},{"title":"SUM\/ENH: extension to and missing in truncated and hurdle count models","body":"overview issues for missing parts and extensions to hurdle, truncated, and 0-1 censored count models added in #7973\r\n\r\nmissing \r\n\r\n- cov_type: only nonrobust is available, others explicitly raise\r\n- post-estimation, the following currently raise exception #8422\r\n  - [ ] score_test\r\n  - [ ] get_distribution\r\n  - [ ] get_influence\r\n  - [ ] get_diagnostic test_chisquare_prob raises score_obs not available\r\n  - [ ] margins #7193\r\n- distributions currently only pmf of truncatedpoisson and truncatednegbin\r\n  - need rvs to simulate data \r\n- ...\r\n- ... \r\n\r\nbigger enhancements, more flexibility in model\r\n\r\n- [ ] add `exog_zero` as option, see comment below\r\n- [ ] more model choices for zero model, e.g. Logit, Probit, GLM #8021\r\n- [ ] joint estimation of zero and truncated params #8020\r\n   I think this is also a requirement to add penalized (Mixin) estimation, \r\n  e.g. NB-NB hurdle looks like dispersion parameter is not identified, needs restriction or penalization (to equal dispersion coefficients)\r\n- [ ] GPP hurdle (maybe)\r\n- [ ] ...\r\n- [ ] (later) more flexible truncation, hurdles, modified counts, maybe mainly as generic \"modifier\" class\r\n- [ ] ...","comments":["I just see that the HurdleCountModel only has one `exog`, which is used for both the zero model and the count model.\r\nThere is no option to have a different `exog_zero`\r\n\r\ndocstring of model `predict` has wrong parameter explanations, it include `exog_infl`. I guess copied from zero-inflation model.\r\n\r\nfuture proofness, when we add exog_zero:\r\nThe default if `exog_zero` is None` would have to be `exog_zero = exog`, i.e. same exog for both submodels.\r\nIn contrast, zero-inflation models default to constant zero inflation `if exog_infl is None`.\r\n\r\nI think that's ok, zi model adds and effect, hurdle model replaces an effect.\r\nIn both cases we can model the special case of no-inflation and no hurdle effect. (No inflation is at the boundary of the parameter space, hurdle effect does not have bounds of the parameter space.)\r\n\r\n\r\n","Generalized Poisson Hurdle with Logit link for zeros.\r\nestimated using GMM for y and y**2 moment equations (first and second noncentral moment), with combined exog as instruments.\r\nThose are not the score equations AFAIR. So, this does not look like efficient instruments for GP hurdle.\r\nThe ear disease in their application are available in an R package.\r\n\r\nZuo, Guoxin, Kang Fu, Xianhua Dai, and Liwei Zhang. \u201cGeneralized Poisson Hurdle Model for Count Data and Its Application in Ear Disease.\u201d Entropy 23, no. 9 (September 2021): 1206. https:\/\/doi.org\/10.3390\/e23091206.\r\n"],"labels":["type-enh","comp-discrete"]},{"title":"ENH: missing post-estimation for truncated and hurdle count models","body":"\r\n`score_test`, `get_distribution` and `get_influence` are not supported by truncated and hurdle models.\r\nThey raise exceptions a various places mainly indicating a missing model methods, \r\ne.g. score_factor, hessian (in hurdle model), `_deriv_score_obs_dendog` for influence\r\n\r\nfound while trying to work on a notebook for the docs\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/8359#issuecomment-1262627543","comments":["get_prediction has `dist` attribute but it is the normal distribution in the hurdle model\r\n\r\nwrong default ?\r\n\r\nNo, just misleading naming. \r\nThis should be the distribution for inference using delta method. It's not related to the distribution of endog in the model.\r\n\r\nThere is a lot of indirection in get_prediction with delta method (based on _get_wald_nonlinear, ... )\r\n`dist` attribute is added in and inherited from `PredictionResultsBase`.\r\n","get_distribution for hurdle count models is still discrete, so we can still subclass rv_discrete\r\n\r\ncensored continuous model like Tobit will be mixed, continuous distribution plus mass points, also Tweedie has that but Tweedie also has no `get_distribution` yet.\r\n\r\nFor rvs of truncated distributions, we have to decide whether to drop samples in truncated part or use `ppf`.\r\n\r\nAside: zi distributions like zinegativebinomial_gen don't have docstrings that explain what the parameters are.\r\n\r\n"],"labels":["type-enh","topic-post_estim"]},{"title":"ENH: tools numerical derivatives for only part of parameters","body":"related #7891\r\n\r\nWe could add a helper function that computes first and second derivatives for only one element of a partition of the parameter space.\r\nWe have several applications where we can compute analytical derivatives for part of the parameters, but need numerical derivatives with respect to parameters where we do not have analytical derivatives.\r\n\r\nCurrently, we either use only numerical derivatives, or hand code it for the special case (example ?).\r\n\r\nMain applications\r\n\r\n- models with extra params, e.g. negative binomial, generalized poisson, more to come\r\n- score or conditional moment tests, GMM: derivative of extra moment conditions, or cross-derivative in stacked moment conditions as in treatment effect GMM. (??? *1)\r\n\r\nIn those cases we have partition params = [known, unknown]\r\nFirst derivative is easy, we just need to write a wrapper function to keep some parameters fixed.\r\nSecond derivatives: more difficult because we also need cross-derivatives, off-diagonal block of Hessian.\r\n\r\nIn the score_test\/cmt\/GMM application, we have first derivative of moment conditions instead of second derivative. This requires more vectorization because return of target function may be 2-dim. But in most cases we only need the derivative of the sum or mean of moment conditions for that (like in Hessian case).\r\n\r\n*1: This might be different. \r\nWe need cross derivatives of extra moment conditions w.r.t. original parameters and derivative of extra moment condition w.r.t. new parameters. We also have a partition of the moment conditions and not just of params.\r\n\r\nexample\r\nfrom https:\/\/github.com\/HDembinski\/jacobi\/blob\/main\/src\/jacobi\/_jacobi.py (under discussion to be added to scipy)\r\n```\r\n    mask : array or None, optional\r\n        If `x` is an array and `mask` is not None, compute the Jacobi matrix only for the\r\n        part of the array selected by the mask.\r\n```","comments":["much easier special case, block triangular form with extra moment condition h(a, beta) = a - g(beta)\r\n\r\nmomcond\r\nh_1(beta)   # hessian of model with params beta\r\nh_a(a, beta) = a - g(beta)  # extra moment condition that defines a desired (average) effect\r\n\r\nd h_1(beta) \/ d a = 0\r\nd h_a(a, beta) \/ d a = 1\r\nd h_a(a, beta) \/ d beta = d g(beta) \/ d beta\r\n\r\nSo, we only need the derivative of the effect w.r.t. params, which is the same as what we need for delta method.\r\n\r\nSame, if a is a vector of parameter where effects g_k (beta) do not depend on each others parameter, \r\nmomconds:  h_a_k(a_k, beta) = a_k - g_k(beta) for all k"],"labels":["type-enh","comp-tools"]},{"title":"DynamicFactorMQ quarterly forecast does not update when data in quarter-end month is added","body":"#### Describe the bug\r\n\r\nQuarterly forecast for current quarter using DynamicFactorMQ does not update when new data for the month **ending** that quarter is added. I would expect it to do so based on the 'news' output, which suggests there is a surprise in that data (and where the absolute impacts do not sum to zero).\r\n\r\n#### Code Sample: \r\n\r\nm_data and q_data are dicts containing a number of data vintages. Dates contains the dates of each vintage, which are also the keys of the dicts.\r\nfactors, factor_orders and factor_multiplicities are specified earlier.\r\n\r\n_Creating the model on the baseline data_\r\n\r\n```python\r\nmodel = sm.tsa.DynamicFactorMQ(m_data[dates[0]], \r\n                               endog_quarterly=q_data[dates[0]], \r\n                               factors=factors, factor_orders=factor_orders, \r\n                               factor_multiplicities=factor_multiplicities)\r\nbase_results=model.fit(disp=10)\r\nall_results = {dates[0]: base_results}\r\nall_forecasts = {}\r\nall_nowcasts = {}\r\n```\r\n\r\n_Getting updated results based on the newer vintages; new dates contains the dates on which I pulled updated monthly data. \r\nI am using .apply() in case of data revisions_\r\n\r\n```python\r\nfor vintage in new_dates:\r\n    all_results[vintage] = base_results.apply(\r\n        m_data[vintage], endog_quarterly=q_data[vintage])\r\n```\r\n\r\n_Calculating forecasts and nowcasts for each vintage. I am using 'smoothed' to try and overcome the issue explained above.\r\nHowever it does not seem to work._\r\n\r\n```python\r\nfor vintage, results in all_results.items():\r\n    all_nowcasts[vintage] = results.predict(\r\n        start=end_curr_q, end=end_curr_q, \r\n        information_set='smoothed')[fc_var_name].resample('Q').last()\r\n    all_forecasts[vintage] = results.predict(\r\n        start=end_curr_q, end=fc_end_date, \r\n        information_set='smoothed')[fc_var_name].resample('Q').last()\r\n```\r\n\r\n_Computing the effect of the new data on the GDP nowcast (getting the 'news')\r\nWhen I do this, there does appear to be an impact from adding datapoints in the quarter-end month\r\nBut this impact does not match the nowcast (which does not update)_\r\n\r\n```python\r\nfor i in range(1, len(dates)):\r\n    vintage = dates[i]\r\n    prev_vintage = dates[i-1]\r\n    news_results[vintage] = all_results[vintage].news(\r\n        all_results[prev_vintage], impact_date = end_curr_q, \r\n        impacted_variable=fc_var_name, comparison_type='previous')\r\n    print(news_results[vintage].summary)\r\n    print(news_results[vintage].summary_details)\r\n    print(news_results[vintage].summary_impacts)\r\n```\r\n\r\n#### Expected Output\r\n\r\nI expected the nowcast for `end_curr_q` to update. However, it does not when I add new datapoints referring to September (i.e., the month at the end of Q3). I am unsure why this is. I tried to resolve by using `information_set='smoothed'`.\r\n\r\nInterestingly, when I calculate the news after adding new datapoints for Sept, the absolute impact column does not sum to zero, suggesting the news may be incorporating the Sept data even if the forecast is not? \r\n\r\nAm i not using `information_set=...` correctly? Why does `.predict()` not update the current-quarter prediction when I add end-quarter month data, but `.news()` seems to?\r\n\r\n","comments":[],"labels":["type-bug","comp-tsa-statespace"]},{"title":"Is `RollingOLS()` as vectorized as possible?","body":"I noticed an extremely slow performance of that class on very large dataframes at the point of running `fit()`(it may take more than 1 second per series), compared to the performance of the regular vectorized functions of pandas and numpy so I wonder if it has already been optimized as much as possible or if it can take improvements and if any other alternatives exist to achieve higher performance.","comments":[],"labels":["comp-regression","Performance"]},{"title":"MarkovRegression gives SVD did not converge","body":"#### Describe the bug\r\n\r\nHi, I am using the Markov Regime Switching algorithms implemented in statsmodels to make inference about the regimes in my data. I have learnt from the examples given by the MarkovRegression and the MarkovAutoregression ones, and I have also read the paper by Hamilton 1989 and 1990. I have now 2 time series, and I have taken the log form of both, each series have 100 numbers without nan of inf, and I can run the algorithm with the differenced data, but not the original log data, as the log data without differencing always give me the SVD does not converge error. \r\n\r\n#### Code Sample, a copy-pastable example if possible\r\nmy data frame that is already in the log form outputs to this, and I also attached a data file with the same numbers just for convenience.\r\n                  VariableA       VariableB\r\n07\/02\/1999  7.248433  7.773384\r\n07\/09\/1999  7.253966  7.790489\r\n07\/16\/1999  7.263680  7.813592\r\n07\/23\/1999  7.217297  7.744938\r\n07\/30\/1999  7.194287  7.730394\r\n08\/06\/1999  7.175490  7.706163\r\n08\/13\/1999  7.195112  7.748460\r\n08\/20\/1999  7.200574  7.753409\r\n08\/27\/1999  7.209118  7.785721\r\n09\/03\/1999  7.185576  7.783224\r\n09\/10\/1999  7.222274  7.849909\r\n09\/17\/1999  7.208970  7.852828\r\n09\/24\/1999  7.161234  7.793587\r\n10\/01\/1999  7.165107  7.797497\r\n10\/08\/1999  7.205264  7.855738\r\n10\/15\/1999  7.136722  7.791006\r\n10\/22\/1999  7.175643  7.822645\r\n10\/29\/1999  7.227118  7.883823\r\n11\/05\/1999  7.233094  7.931285\r\n11\/12\/1999  7.245298  7.975393\r\n11\/19\/1999  7.263680  8.021749\r\n11\/26\/1999  7.254531  8.045909\r\n12\/03\/1999  7.271356  8.062905\r\n12\/10\/1999  7.268920  8.087948\r\n12\/17\/1999  7.275519  8.135640\r\n12\/23\/1999  7.284478  8.194091\r\n12\/31\/1999  7.302665  8.230577\r\n01\/07\/2000  7.286534  8.176813\r\n01\/14\/2000  7.298445  8.229911\r\n01\/21\/2000  7.281902  8.262688\r\n01\/28\/2000  7.220008  8.153350\r\n02\/04\/2000  7.266478  8.270397\r\n02\/11\/2000  7.241903  8.292173\r\n02\/18\/2000  7.210264  8.286017\r\n02\/25\/2000  7.198931  8.341410\r\n03\/03\/2000  7.251877  8.401670\r\n03\/10\/2000  7.256826  8.448272\r\n03\/17\/2000  7.305860  8.415493\r\n03\/24\/2000  7.349552  8.477204\r\n03\/31\/2000  7.323336  8.402343\r\n04\/07\/2000  7.332369  8.374938\r\n04\/14\/2000  7.220740  8.076360\r\n04\/20\/2000  7.277420  8.170469\r\n04\/28\/2000  7.286192  8.245384\r\n05\/05\/2000  7.271704  8.214736\r\n05\/12\/2000  7.265919  8.136665\r\n05\/19\/2000  7.254885  8.095599\r\n05\/26\/2000  7.230744  8.048788\r\n06\/02\/2000  7.297091  8.229911\r\n06\/09\/2000  7.302833  8.246434\r\n06\/16\/2000  7.305188  8.249967\r\n06\/23\/2000  7.288586  8.223627\r\n06\/30\/2000  7.291656  8.247220\r\n07\/07\/2000  7.309714  8.262946\r\n07\/14\/2000  7.329094  8.313362\r\n07\/21\/2000  7.306196  8.278682\r\n07\/28\/2000  7.266827  8.162231\r\n08\/04\/2000  7.294207  8.197539\r\n08\/11\/2000  7.298783  8.206174\r\n08\/18\/2000  7.313720  8.252055\r\n08\/25\/2000  7.322180  8.283747\r\n09\/01\/2000  7.328601  8.323123\r\n09\/08\/2000  7.325808  8.261139\r\n09\/15\/2000  7.302159  8.221076\r\n09\/22\/2000  7.291997  8.236421\r\n09\/29\/2000  7.281902  8.194506\r\n10\/06\/2000  7.263154  8.118952\r\n10\/13\/2000  7.233816  8.094073\r\n10\/20\/2000  7.252054  8.160375\r\n10\/27\/2000  7.244942  8.077913\r\n11\/03\/2000  7.269791  8.117611\r\n11\/10\/2000  7.224389  7.970395\r\n11\/17\/2000  7.222931  7.985995\r\n11\/24\/2000  7.205264  7.947679\r\n12\/01\/2000  7.185766  7.852633\r\n12\/08\/2000  7.212663  7.909306\r\n12\/15\/2000  7.189922  7.864228\r\n12\/22\/2000  7.190488  7.811568\r\n12\/29\/2000  7.196687  7.772542\r\n01\/05\/2001  7.173575  7.737834\r\n01\/12\/2001  7.193122  7.839919\r\n01\/19\/2001  7.206748  7.895995\r\n01\/26\/2001  7.217627  7.880615\r\n02\/02\/2001  7.210264  7.819435\r\n02\/09\/2001  7.186523  7.730614\r\n02\/16\/2001  7.174341  7.708635\r\n02\/23\/2001  7.128897  7.629247\r\n03\/02\/2001  7.119029  7.537164\r\n03\/09\/2001  7.126087  7.516161\r\n03\/16\/2001  7.057683  7.422374\r\n03\/23\/2001  7.050556  7.458186\r\n03\/30\/2001  7.064118  7.371489\r\n04\/06\/2001  7.039003  7.293018\r\n04\/12\/2001  7.083598  7.455877\r\n04\/20\/2001  7.131499  7.577378\r\n04\/27\/2001  7.137676  7.508239\r\n05\/04\/2001  7.149328  7.570701\r\n05\/11\/2001  7.140057  7.518879\r\n05\/18\/2001  7.166652  7.567346\r\n05\/25\/2001  7.153834  7.585027\r\n06\/01\/2001  7.142827  7.523751\r\n\r\nAnd the code I am using to run this:\r\n\r\nimport pandas as pd\r\nimport statsmodels.api as sm\r\n\r\n# the above data frame is named as logdata\r\nmodel = sm.tsa.MarkovRegression(endog=logdata['VariableA'], k_regimes=2, exog=logdata['VariableB'], switching_exog=True, switching_variance=True)\r\nres = model.fit(maxiter=100000)\r\n\r\n#### Expected Output\r\n\r\nAssuming that I use the logreturns = (logdata - logdata.shift(1)).dropna(), and then use logreturns['VariableA'] as endogenous and use logreturns['VariableB'] as exogenous variables, I have tested many more data like this in my full data set, and most of the time I can get the fitted model and all the properties. However, when using logdata, I always get these error messages:\r\n\r\nFile \"C:\\..\\markovSwitching.py\", line 14, in MSDR\r\n    res = model.fit(maxiter=maxiter)\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\regime_switching\\markov_switching.py\", line 1113, in fit\r\n    start_params = self._fit_em(start_params, transformed=transformed,\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\regime_switching\\markov_switching.py\", line 1205, in _fit_em\r\n    out = self._em_iteration(params[-1])\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\regime_switching\\markov_regression.py\", line 214, in _em_iteration\r\n    coeffs = self._em_exog(result, self.endog, self.exog,\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\regime_switching\\markov_regression.py\", line 250, in _em_exog\r\n    np.dot(np.linalg.pinv(tmp_exog), tmp_endog))\r\n  File \"<__array_function__ internals>\", line 180, in pinv\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 1998, in pinv\r\n    u, s, vt = svd(a, full_matrices=False, hermitian=hermitian)\r\n  File \"<__array_function__ internals>\", line 180, in svd\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 1657, in svd\r\n    u, s, vh = gufunc(a, signature=signature, extobj=extobj)\r\n  File \"C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 98, in _raise_linalgerror_svd_nonconvergence\r\n    raise LinAlgError(\"SVD did not converge\")\r\nnumpy.linalg.LinAlgError: SVD did not converge\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.10.5.final.0\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.2 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.23.0 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy)\r\nscipy: 1.8.1 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy)\r\npandas: 1.4.3 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas)\r\n    dateutil: 2.8.2 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dateutil)\r\npatsy: 0.5.2 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.5.2 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib)\r\n    backend: TkAgg \r\ncvxopt: Not installed\r\njoblib: Not installed\r\n[Log.xlsx](https:\/\/github.com\/statsmodels\/statsmodels\/files\/9635216\/Log.xlsx)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: Not installed\r\n    jinja2: 3.1.2 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jinja2)\r\nsphinx: Not installed\r\n    pygments: Not installed\r\npytest: 7.1.3 (C:\\Users\\..\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytest)\r\nvirtualenv: Not installed\r\n\r\n\r\nThank you very much in advance for all of your help!!","comments":["Does the Markov Regression functions here assume that my data input, at least need to be stationary in each of the potential regimes? Is this the reason why I get good fitted model with the differenced data but not the log prices?","Did you find a solution? I am experienxing a similar issue with the MarkovRegression ","> Did you find a solution? I am experienxing a similar issue with the MarkovRegression\r\n\r\nUnfortunately, no. I tried different types of input data, and usually when the data itself is stationary, or the two series are cointegrated for the endo and exog case, I would not run into that issue. However, this is somehow different from the cases I pictured to apply this algorithm.","The problem may be related to a non-stationary issue with the data and\nmodel design, but errors such as\n\nraise LinAlgError(\"SVD did not converge\")\nnumpy.linalg.LinAlgError: SVD did not converge\n\nare typically due to redundant or unidentified parameters  (at\nthe parameter space where the optimization procedures were trying to\nconverge).\n\nYou could try different starting values or a different optimization method\nthat does not use have a SVD step\n\nIt is most likely a case where the log level model design makes it hard to\nfind 2 distinct regimes. This can come from a case where one regime fits\nthe data almost perfectly, or from a case where neither regime fits the\ndata well, or just a bad choice in starting values.  Probably not a\nsoftware bug.\n\n\n\n\nOn Fri, Sep 23, 2022 at 12:04 PM ARBQuant ***@***.***> wrote:\n\n> Does the Markov Regression functions here assume that my data input, at\n> least need to be stationary in each of the potential regimes? Is this the\n> reason why I get good fitted model with the differenced data but not the\n> log prices?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8416#issuecomment-1256396071>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AASXOMV3AEWWIM6TW4EERFDV7XIHDANCNFSM6AAAAAAQUDPNUU>\n> .\n> You are receiving this because you are subscribed to this thread.Message\n> ID: ***@***.***>\n>\n"],"labels":["comp-tsa"]},{"title":"freq_weights not used correctly?","body":"https:\/\/github.com\/statsmodels\/statsmodels\/blob\/83032dc493641ebc84a88055137826143102a55b\/statsmodels\/genmod\/families\/family.py#L931\r\n\r\nWhy is this comment here, and not applied? My thinking is that frequency weights are now an array of 1, while the user has passed sufficient information to the program for calculating freq_weights. Instead, the program calculated n_trials, and freq_weights will be defaulted to an array of 1s.\r\n\r\nIn later steps in def _fit_irls() (in generalized_linear_model.py) the program makes the computation self.iweights * self.n_trials, the self.iweights variable is initially constructed as self.freq_weights * self.var_weights. If the user then specified freq_weights to the program in the same fashion as n_trials is computed, it looks like these are squared, which I am not sure why it would be.\r\n\r\nMaybe someone who understands this better could clear this out.","comments":["AFAICS after browsing the code a bit:\r\n\r\nThis comment might be outdated or just mention that freq_weights are not included in n_trials.\r\n`deviance` methods is not defined anymore in each family. This was later refactored to a generic `deviance` in the Family base class.\r\n\r\n> My thinking is that frequency weights are now an array of 1\r\n\r\nThe initialize method only defines n_trials as ones if original endog is 2-dim. it does not affect freq_weights.\r\naside: The family `initialize` method is called for binomial in GLM._setup_binomial and not in the family initialize, because the family `__init__` does not get endog.\r\n\r\nI never remember about square or not squared var_weights, but squaring or square-roots should be correct.\r\nWeights in irls enter in quadratic or product terms as in WLS, so whether they are explicitly or implicitly squared is and implementation detail.  (I didn't check the code for irls again, so here I'm just guessing from what I remember.)\r\n\r\n\r\n","Ah okay. But what if I have binomial data, so that y_i is the number of successes out of n_i trials, and the proportion is p_i = y_i \/ n_i. If I fit a binomial GLM with weights equal to n_i (or rather the array of n_i:s), should I use freq_weights or var_weights in this case?\r\n\r\nE.g.\r\nGLM('p ~ x', family=Binomial, freq_weights = n)\r\nor \r\nGLM('p ~ x', family=Binomial, var_weights = n)\r\n\r\nIt is not really straightforward to me, therefore I was trying to grasp the source code, because at first I was inputting endog as 2-dim, such that the function computed the proportion for me, but then it looked like it used both n_trials and freq_weights instead of just freq_weights (if I only input 1d endog as proportion). ","GLM binomial for count data requires a 2-dim `endog` with 2 columns, counts of (success, fail). `n_trials` will be the (row)sum of the 2 columns computed internally during initialization. No var or freq weights are needed in this case.\r\n`endog = np.columnstack((y_i, n_i - y_i))`\r\nThe internally used weights should just be ones, i.e. no weighting of observations. All the weighting comes from `n_trials`.\r\n\r\nThe equivalent specification:\r\nIf the endog is specified as a proportion, then the variance will be a function of `n`. So you need to use var_weights.\r\nIn this case `n_trials` will be just ones.\r\n\r\nSo you can check that\r\n`GLM('p ~ x', family=Binomial, var_weights = n)`\r\ngives the same result as the count version with 2-dim endog.\r\n\r\nI'm not sure about squares without checking this, i.e. are var_weights in this case\r\n`GLM('p ~ x', family=Binomial, var_weights = n**2)` or \r\n``GLM('p ~ x', family=Binomial, var_weights = np.sqrt(n))`   (very unlikely)\r\n(same for WLS where I always have to look this up.)\r\n\r\n\r\n"],"labels":["comp-docs","comp-genmod","question"]},{"title":"fit_regularised() is not working for GAM model","body":"#### Describe the bug\r\n\r\nI tried elastic net fitting of the GAM model. It returns an Attribution Error that the smoother is None. However, standard fitting works.\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nfrom statsmodels.gam.api import GLMGam, BSplines\r\n\r\n#define smoother\r\nq = np.linspace(0.01,0.90,10)\r\nknots = np.array([train_data[['tmp']].quantile(x).item() for x in q])\r\nbs = BSplines(train_data[['tmp']],\r\n              df = n_knots+degree+1 ,   #n_knots+degree, #number of basis functions-1=degrees of freedom; df >= degree+1\r\n              degree  =   degree, \r\n\r\n              #,\r\n              #knot_kwds=[{'spacing':'quantile','lower_bound':-17,'upper_bound':38}] # deafult is quantile\r\n              knot_kwds=[{'knots':knots}]\r\n              )\r\n\r\n#instantiate model\r\ngam_bs = GLMGam(train_data['load'],\r\n                exog = train_data[['trend']], \r\n                           #data=train_data,\r\n                           smoother=bs,\r\n                           alpha=[630.9573]\r\n                           \r\n                           )\r\n# fit\r\nres_bs = gam_bs.fit() # this works\r\n\r\nres_bs = gam_bs.fit_regularized(alpha = 0.1) # this returns error\r\n```\r\n\r\n\r\n```\r\nin GLMGam.__init__(self, endog, exog, smoother, alpha, family, offset, exposure, missing, **kwargs)\r\n    540 self.exog_linear = exog_linear\r\n    542 self.smoother = smoother\r\n--> 543 self.k_smooths = smoother.k_variables\r\n    544 self.alpha = self._check_alpha(alpha)\r\n    545 penal = MultivariateGamPenalty(smoother, alpha=self.alpha,\r\n    546                                start_idx=k_exog_linear)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'k_variables'\r\n```","comments":["I guess I never checked this. \r\n\r\nThe `fit_regularized` method is inherited from the GLM class.\r\nI doubt we can get that to work with GAM because it use coordinate descent as optimizer.\r\nGAM uses a L2 penalty on an entire smooth component and not a columnwise penalty.\r\n\r\n`fit_regularized` should raise NotImplementedError.\r\n\r\nDid you want to use fit_regularized to regularize non-smooth components or a smooth component?\r\n\r\n\r\n\r\n\r\n","I wanted to regularise both spline terms and non-spline terms. I think regularising spline terms is already done using the smoothing penalty. So, figuring out how to regularise non-spline terms could be beneficial."],"labels":["type-enh","comp-gam"]},{"title":"FAQ\/DOC: econometrics examples R and Python ","body":"Great example collection with \"equivalent\" code in R and Python (statsmodels, linearmodels, arch)\r\nSome examples are not available in packages and coded directly in example, e.g. Tobit\r\n\r\nhttps:\/\/web.pdx.edu\/~crkl\/ceR\/\r\n\r\nThis is a bit similar to UCLA stats examples for statistics but for econometrics.\r\n(UCLA stats does not have python examples)\r\n\r\n","comments":["https:\/\/web.pdx.edu\/~crkl\/ceR\/Python\/example12_1.py\r\nuses the Gamma GMM example from my notebook, i.e. use basic GMM classes with user specified moment conditions.\r\n(I should check what the R example code does.)\r\n"],"labels":["comp-docs","FAQ"]},{"title":"ENH: warn if (near) singular design in discrete, GLM","body":"Optimization in discrete models and GLM might not fail with singular or near-singular matrix.\r\n\r\nWe should add a warning text to summary similarly as in linear models OLS, ...\r\n\r\nexample blog post that has collinear dummies\r\ndummy for each weekday except reference and a weekend dummy which is the same as Saturday plus Sunday. \r\n\r\nMy guess is that in this large sample case (nobs= 1.6 million), the small ridge penalty in the newton optimizers works as regularized estimator similar to `pinv`  (I haven't looked at this in a while, #953 is still open)\r\n\r\nHowever, standard errors for the collinear variables are huge, so that should point out to users that something is strange or wrong.\r\n\r\nfrom https:\/\/medium.com\/@lucydickinson\/how-to-interpret-the-odds-ratio-with-categorical-variables-in-logistic-regression-5bb38e3fc6a8\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/440735\/190906272-1389e7fa-07e5-4105-b8a4-2b8d3819fdca.png)","comments":[],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"ENH: t-test, oneway, variance under null, score_test","body":"(just a semi-random thought)\r\n\r\nAFAIR, our t-test, oneway tests and similar compute variance using estimated means and not means estimated under the null hypothesis.\r\n\r\ne.g Welsh anova allows for heteroscedasticity but does not impose equal means hypothesis\r\n`vars_ = np.array([x.var(ddof=1) for x in args])`\r\nThe heteroscedasticity assumption is independent of which means we use to compute variances, i.e. we can allow for heteroscedasticity even if we impose the null hypothesis on the means.\r\n\r\nUnder score_test assumptions, we estimate auxiliary parameters under the null hypothesis.\r\nWe have this now for proportions and poisson rates, but the same applies to variance as auxiliary parameter for loc-scale distributions.\r\n\r\ni.e. we might want to add an option to t-tests, z-tests and oneway to whether variances are computed under the null restricted model or under the unrestricted (wald) model.\r\n\r\nUsing equal means will increase the estimated variance(s) and make score_test more conservative. If wald version overrejects, then score test can underreject similar to the cases in count and binary models.\r\n(Similar to score versus wald tests in models, score_tests are explicitly for null hypothesis testing, while wald statistics also work if the null hypothesis does not hold, e.g. standard errors for non-null parameters.)\r\n\r\nI have not seen a direct reference for this.\r\nHowever, there is a literature for LR-like or F tests that recommend keeping the variance estimate, `scale`,  the same across models that are compared. (I don't remember whether the recommendation was to use the variance of the largest model.)\r\n\r\nRelated: The bootstrap literature for heteroscedasticity or cluster robust standard errors recommend bootstrapping based on the model that imposes the Null (e.g. wild bootstrap and other residual bootstraps)\r\n(not clear to me: permutation bootstrap using heteroscedasticity robust t-test still permutes under the null assumption on means. So the permutation DGP assumes the null on means, even if the test statistic is wald. ???)\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"FAQ: Robust Linear Model ZeroDivisionError better error message","body":"Hi, I am having issue with fitting robust linear model. It seems that whenever the number of predictors > the number of samples, it outputs `ZeroDivisionError: float division by zero` without any helpful message. So, I would like to confirm if this is related to degree of freedom (which is equal to number of sample)? If so, is there any solution to perform this, or this just does not makes sense?\r\n\r\nAlso, I think it would be great if the error message is propagated properly, i.e. output something like:\r\n\r\n```\r\nValueError: We do not have enough degree of freedom to estimate, i.e. the number of sample is less than the number of predictors.\r\n```","comments":["All our models assume (in general) number of predictors < the number of samples\r\n\r\nAsymptotic inference assumes that number of observations per estimated parameter goes to inf. \r\n\r\nIf number of parameters is larger then nobs, then we need either parameter restrictions or penalization\/variable selection to reduce the effective degrees of freedom of the model.\r\n\r\nI'm not sure whether that should get a generic warning.\r\nSome more specific warnings might be possible, e.g. estimated residual variance `scale` is zero.\r\n","leaving this as FAQ issue\r\n\r\nadding more generic warnings is still open design question"],"labels":["comp-base","comp-robust","FAQ"]},{"title":"FAQ: Cochran-Armitage trend test","body":"#### Describe the bug\r\n\r\nI am getting different statistical results using the statsmodels test_ordinal_association method compared to the CochranArmitageTest method from DescTools in R.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\nimport statsmodels.api as sm\r\n\r\ntable_tumors = pd.DataFrame(\r\n    data = {\r\n        0: [0,50],\r\n        1: [1,49],\r\n        2: [3,47],\r\n        3: [6,44]\r\n    },\r\n    index = [1,0],\r\n)\r\n\r\ntable = sm.stats.Table(table_tumors)\r\n\r\nprint(table.test_ordinal_association(\r\n    row_scores=np.array([1,0]),\r\n    col_scores=np.array([0,1,2,3]),\r\n))\r\n```\r\n\r\n#### Expected Output\r\n\r\nUsing R, we obtain the following results:\r\n\r\n```R\r\nlibrary(DescTools)\r\n\r\ndose <- matrix(c(0,1,3,6,50,49,47,44), byrow=TRUE, nrow=2, dimnames=list(resp=c(1,0), dose=0:3))\r\n\r\nDesc(dose)\r\n#CochranArmitageTest(dose)\r\n#\r\n#        Cochran-Armitage test for trend\r\n#\r\n#data:  dose\r\n#Z = 2.9019, dim = 4, p-value = 0.003709\r\n#alternative hypothesis: two.sided\r\n```\r\n\r\nUsing statsmodels, we obtain:\r\n\r\n```\r\nnull_mean   15.71072319201995\r\nnull_sd     3.5390390971784127\r\npvalue      0.008669937476153359\r\nstatistic   25.0\r\nzscore      2.624801973899118\r\n```\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n```\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.10.7.final.0\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.2 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels)\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: Not installed\r\nnumpy: 1.23.2 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy)\r\nscipy: 1.9.1 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy)\r\npandas: 1.4.4 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas)\r\n    dateutil: 2.8.2 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dateutil)\r\npatsy: 0.5.2 (C:\\Users\\S\u00e9bastien\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\patsy)\r\n```","comments":["main problem is that contingency tables default to shifting zeros, add 0.5 to all zeros, but not to other entries\r\n\r\n\r\nfrom coin package\r\n```\r\n> lbl_test(as.table(dose))\r\n\r\n        Asymptotic Linear-by-Linear Association Test\r\n\r\ndata:  dose (ordered) by resp (1, 2)\r\nZ = 2.8946, p-value = 0.003796\r\nalternative hypothesis: two.sided\r\n```\r\n\r\nstatsmodels without shift_zeros\r\n\r\n```\r\ntable = sm.stats.Table(table_tumors, shift_zeros=False)\r\n\u200b\r\nprint(table.test_ordinal_association(\r\n    row_scores=np.array([1,0]),\r\n    col_scores=np.array([0,1,2,3]),\r\n))\r\n\u200b\r\nnull_mean   15.0\r\nnull_sd     3.454659660058306\r\npvalue      0.0037959220933673713\r\nstatistic   25.0\r\nzscore      2.894641146743591\r\n```\r\n\r\nhowever, 2 other R packages give\r\n\r\n```\r\nstats\r\n\r\n> prop.trend.test(dose[1, ], dose[1, ] + dose[2, ])\r\n\r\n        Chi-squared Test for Trend in Proportions\r\n\r\ndata:  dose[1, ] out of dose[1, ] + dose[2, ] ,\r\n using scores: 1 2 3 4\r\nX-squared = 8.4211, df = 1, p-value = 0.003709\r\n\r\n\r\nDescTools\r\n\r\n> CochranArmitageTest(dose)\r\n\r\n        Cochran-Armitage test for trend\r\n\r\ndata:  dose\r\nZ = 2.9019, dim = 4, p-value = 0.003709\r\nalternative hypothesis: two.sided\r\n```","sorry for the delay. It took me some time to get back to this.\r\n\r\nI don't know why R stats and DescTools differ from the linear by linear test"],"labels":["comp-stats","FAQ"]},{"title":"ENH\/REF: Signature of power() between parent class Power and child class FTestPower does not fit.","body":"#### Describe the bug\r\n\r\nWhen using `plot_power()` or `solve_power` with a FTestPower object, one gets an error that positional argument alpha is not specified. This is because, we call the `plot_power()` function of parent class which, expects arguments for `power()` in all inheriting classes in the form of `effect_size, nobs, alpha`. However, inside FTestPower the signature is different.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\n\r\n```python\r\ntest = FTestPower()\r\ntest.plot_power(dep_var='nobs',\r\n                    nobs=np.range(100),\r\n                    effect_size=0.8,\r\n                    alpha=0.05,\r\n```\r\n<details>\r\n\r\n**Note**: As you can see, there are many issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates.\r\n\r\n**Note**: Please be sure you are using the latest released version of `statsmodels`, or a recent build of `main`. If your problem has been fixed in an unreleased version, you might be able to use `main` until a new release occurs. \r\n\r\n**Note**: If you are using a released version, have you verified that the bug exists in the main branch of this repository? It helps the limited resources if we know problems exist in the current main branch so that they do not need to check whether the code sample produces a bug in the next release.\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nWe would expect to see the power analysis plot. \r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\n\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.9.12.final.0\r\nOS: Darwin 21.5.0 Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:29 PDT 2022; root:xnu-8020.121.3~4\/RELEASE_ARM64_T8101 x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nstatsmodels\r\n===========\r\nInstalled: 0.13.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/statsmodels)\r\nRequired Dependencies\r\n=====================\r\ncython: Not installed\r\nnumpy: 1.22.4 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/numpy)\r\nscipy: 1.9.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/scipy)\r\npandas: 1.4.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/dateutil)\r\npatsy: 0.5.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/patsy)\r\nOptional Dependencies\r\n=====================\r\nmatplotlib: 3.5.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/matplotlib)\r\nBackend MacOSX is interactive backend. Turning interactive mode on.\r\n    backend: MacOSX \r\ncvxopt: Not installed\r\njoblib: 1.1.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/joblib)\r\nDeveloper Tools\r\n================\r\nIPython: 8.4.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/IPython)\r\n    jinja2: 3.0.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/jinja2)\r\nsphinx: 3.5.4 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/sphinx)\r\n    pygments: 2.13.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pygments)\r\npytest: 7.1.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pytest)\r\nvirtualenv: 20.16.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/virtualenv)\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.9.12.final.0\r\nOS: Darwin 21.5.0 Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:29 PDT 2022; root:xnu-8020.121.3~4\/RELEASE_ARM64_T8101 x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: None\r\nstatsmodels\r\n===========\r\nInstalled: 0.13.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/statsmodels)\r\nRequired Dependencies\r\n=====================\r\ncython: Not installed\r\nnumpy: 1.22.4 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/numpy)\r\nscipy: 1.9.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/scipy)\r\npandas: 1.4.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/dateutil)\r\npatsy: 0.5.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/patsy)\r\nOptional Dependencies\r\n=====================\r\nmatplotlib: 3.5.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/matplotlib)\r\n    backend: MacOSX \r\ncvxopt: Not installed\r\njoblib: 1.1.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/joblib)\r\nDeveloper Tools\r\n================\r\nIPython: 8.4.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/IPython)\r\n    jinja2: 3.0.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/jinja2)\r\nsphinx: 3.5.4 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/sphinx)\r\n    pygments: 2.13.0 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pygments)\r\npytest: 7.1.2 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/pytest)\r\nvirtualenv: 20.16.3 (\/Users\/pengfei_zhao\/miniconda3\/envs\/witte\/lib\/python3.9\/site-packages\/virtualenv)\r\n<\/details>\r\n","comments":["Thanks for reporting\r\n\r\nthe unit test for plot skips this test\r\n`pytest.skip('skip FTestPower plot_power')`\r\n\r\nBased on the unit tests, it looks like I was matching the R `pwr` package arguments for it without thinking about consistency across our power classes\r\n`pwr.f2.test(u=5, v=19, f2=0.3**2, sig.level=0.1)`\r\n\r\nI guess a proper solution would be to change the arguments of methods of FTestPower class which would not be backwards compatible.\r\nThe only backwards compatible fix is to add a specific FTestPower.plot_power methods using copy-paste-adjust.\r\nI think changing the super class methods is not good, because it messes up the code for a temporary special case fix.\r\n\r\nfor the background, I have not looked at ftestpower in a long time AFAIR (I only needed FTestAnovaPower in the last years):\r\nFor which F-test are you computing the power?\r\nDo you know which effect size measure is used for FTestPower?\r\n\r\nsearching around a bit shows:\r\n\r\nThe ftestpower function uses \r\n`nc = effect_size**2 * (df_denom + df_num + ncc)`\r\nto compute noncentrality parameter\r\n\r\nrelated issues for power computation for Models\r\n#6558 for OLS\r\n#6759 for GLM\r\n#6983 for effect size measures for tests applicable to models\r\n\r\n6558 issue mentions:\r\n`(still a bit annoying stats.power.FTestPower defines effect size as sqrt, i.e.\r\neffectsize = sqrt(nc \/ nobs)`\r\n\r\n\r\nTo fix this issue:\r\nFTestPower is too generic with using df_denom and df_num instead of effectsize or normed noncentrality (nc\/nobs) and nobs, and we won't fit it into the standard pattern. (We need additional FTest power classes that use nc\/nobs as effect size and nobs as sample size argument.)\r\nSo, we need to add a special cased plot_power.\r\n","see https:\/\/github.com\/statsmodels\/statsmodels\/issues\/8646#issuecomment-1416851630\r\nfor a bug in names df_num, df_denom\r\n\r\nStill need to fix the bug in a backwards compatible way and add class with \"standard signature\" with nobs as basic arg.\r\nCurrent FTestPower class has not enough information to compute nobs from df_denum.\r\n\r\nif effect size is defined as nc \/ nobs, then there is a different denominator than df_denum.\r\nIt would be consistent if \r\n\r\nin `ftest_power` function we implicitly use `nobs = (df_denom + df_num + ncc) ` if effect_size**2 is defined as nc \/ nobs\r\n`nc = effect_size**2 * (df_denom + df_num + ncc) `\r\n\r\nI should read up in how the get to this, and how ncc is defined in general.","NCSS PASS has interesting docs https:\/\/www.ncss.com\/wp-content\/themes\/ncss\/pdf\/Procedures\/PASS\/Multiple_Regression_using_Effect_Size.pdf\r\n\r\nsee the parts on conditional effect, i.e. test significant of subset of regressors  (using some partial R2 measure)\r\ndf_xxx depend on size of subsets (partition of regressors)\r\nfrom this we should get the `ncc` correction and nobs.\r\nI still need to check the details.\r\n","for rewritten new power class:\r\n\r\nmaybe use df_num and ddf_denum as relevant arguments\r\nwhere `df_denum = nobs - ddf_denum`\r\nand then nc = effect_size * nobs = f2 * nobs\r\n","I guess I can add a new FTestRegressionPower class, that uses f^2 and nobs ddf_denom\r\nThat should be pretty quick and can use results from current unit test, and maybe examples from PASS docs\r\n\r\nI think that could be used then directly for results from a regression table (anova_lm), I guess, with eta^2 instead of R2 to compute f2.\r\n","Just remembered: Stata has a new topic manual pss for power and sample size https:\/\/www.stata.com\/manuals\/pss.pdf with power and sample size computation based on rsquared.\r\n\r\nAFAIR, it was added later than the version of Stata that I have.\r\n\r\narguments are more specific to the regression case\r\n\r\n\u2217 n(numlist) sample size; required to compute power or effect size\r\n\u2217 ntested(numlist) number of tested covariates\r\n\u2217 ncontrol(numlist) number of control covariates; required for testing a subset of coefficients\r\n\u2217 diff(numlist) difference between the R2 of the full and the reduced model, R2_F \u2212 R2_R; specify instead of the R2 of the full model, R2_F , when testing a subset of coefficients\r\n\r\nStata also has a separate similar function power pcorr using partial correlation.\r\n\r\nwe still need to have a generic F-test power class for generic F-tests, or\/and maybe something for two-sample comparisons based on F statistic. Main issue is how to handle, nobs, df_num and df_denom for various cases. Power computation itself can be delegated to the function ftest_power (with corrected df names)\r\ne.g. allow subclasses of a generic F test power that just adds a method that defines the relationship between those 3, nobs, df_num and df_denom, \r\n \r\nThis goes back to my original question after adding generic power classes. \r\nFor which special cases do we need to add specific power classes?\r\n\r\nexample: \r\nF-test for ratio of two variances, in Stata `power twovariances`\r\nwe need nobs1 and ratio, nobs2 = nobs1 * ratio\r\nThen both df_num and df_denom depend on nobs1\r\nWhat's the appropriate effect size?  nc \/ nobs"],"labels":["type-bug","type-enh","comp-stats","type-refactor"]},{"title":"STR decomposition","body":"The R package stR has recently appeared on my radar. The seasonal decomposition scheme that they use is quite robust, and it allows for multiple types of seasonality along with the inclusion of exogenous regressor's and complex seasonal smoothing schemes. I've decomposed using stats models GLM, UC, the classical and STL model and believe this is one of the best methods I've used. Wondering if it's on the road map at all to implement this in stats models as there is no implementation on the python side. \n\nhttps:\/\/www.monash.edu\/business\/ebs\/research\/publications\/ebs\/wp13-15.pdf\n\nThanks,\nJ","comments":["Hi! Is this in a roadmap somewhere? I'd hate to have to switch to R for just this functionality \ud83d\ude05 \r\n"],"labels":["comp-tsa","type-enh"]},{"title":"ENH: use shrinkage variance (semi-pooled) in oneway anova, multiple tests","body":"(mainly parking a reference for an old idea)\r\n\r\nvariance and covariance estimates are not very good in small or very small samples.\r\nOne idea is to use penalized or shrinkage (co)variance to get better small sample properties. In oneway and similar cases with heteroscedasticity we can shrink to a pooled estimate.\r\n\r\nrelated\r\n#3197 cov shrinkage, penalized (also related outlier robust cov methods)\r\n#2942 applied to GMM weight\/cov matrix\r\n#2882 similar issue for mean params shrinkage\/penalization to a target\r\n\r\n\r\nThe following looks like a good starting point for oneway anova and multiple, e.g. pairwise, comparisons (context in #8396 )\r\nAdding this to individual hypothesis test functions in `stats` is much easier than integrating it into `cov_type` in models (which I have not yet tried out).\r\n\r\nCui, Xiangqin, J. T. Gene Hwang, Jing Qiu, Natalie J. Blades, and Gary A. Churchill. 2005. \u201cImproved Statistical Tests for Differential Gene Expression by Shrinking Variance Components Estimates.\u201d Biostatistics 6 (1): 59\u201375. https:\/\/doi.org\/10.1093\/biostatistics\/kxh018.\r\n\r\nthey include references to previous literature for semi-pooled variances in tests\r\n608 citations, so there might be a lot more in this direction.\r\n\r\nextension idea: \r\nThe pooled estimate would have enough observations to use also other estimators than simple variance, e.g. outlier robust estimators, i.e. we could shrink to a robust, pooled variance estimate. (possible problem if robust \"variance\" is just a dispersion measure possibly calibrated to normal distribution.)\r\n\r\n\r\nThe main objective here is to improve heteroscedasticity robust hypothesis tests in small samples. If some group sizes are small, then variance estimate using only within group information is too noisy.\r\ne.g. Welch anova and multiple comparisons with unequal variance.\r\n(related AFAIR\r\nBF mean anova does not use weights in computing average, but uses group variances for inference.\r\nIn meta-analysis we also have the option to use variance weights or not for the weighted average, IIRC\r\n)\r\n\r\nThe main decision is how much too shrink, e.g. choosing weights between sample and pooled variances. Those weights should depend on sample sizes. If group is large, then we do not need to shrink the group variance.\r\n","comments":["going from oneway `stats` to models:\r\n\r\nI was browsing a bit the citations of the above article in google scholar, which are mostly application to multiarray data.\r\nI did not read any articles but a few related ideas.\r\n\r\n- OLS with a oneway categorical plus possibly other exog\r\n  If we allow for group heteroscedasticity, then the residual variances depend on groups (e.g. cov_type HC depending only on group dummies). If groups are small, then we might want to shrink to pooled estimate of residual variance.\r\n  The cov_type correction would use group specific `scale`, variance but would otherwise be nonrobust.\r\n- In WLS we could use var_weights based on shrinkage group variances\r\n- vectorized OLS\r\n  Each group forms separate regression with separate group specific residual variance, i.e. no pooling of any information across groups. In the current implementation we estimate `scale` for each group separately. Instead we could add an option to shrink `scales` towards a given target. \r\n  If the vectorized OLS model includes all groups, then the pooled residual variance can be computed from the data in the model. If we batch groups, then the target needs to be predefined, i.e. we need separate step to compute pooled variance.\r\n\r\nIn WLS: using the shrinkage var_weights would correspond to estimating FGLS with a penalized variance function.\r\nVectorized OLS would correspond to SUR with a diagonal cross-sectional covariance matrix with a penalized variance function on the diagonal.\r\n\r\n(related to SUR: we could extend this to shrinking cross-sectional cov to a target cov)\r\n\r\nBoth standard SUR\/panel data and vectorized OLS assume balanced panel, which means shrinkage does not differ by individual group sample sizes as in oneway comparison or OLS with group dummies.\r\n\r\n\r\nSimilar methods could apply to other models, e.g. Poisson QMLE with excess dispersion or NegBin and other (excess) dispersion models. Binary\/Binomial ? Beta-Binomial if we have counts.\r\n\r\n","It can also apply to two-sample t-test.\r\ne.g. add additional option use_var=\"shrink\" and extra keywords, `method_shrink` and shrink_arg\r\nwhere we shrink initially only by number of \"prior\" observations, e.g. shrink_arg=10 means assume pooled variance corresponds to 10 observations and shrinking weights are nobsi \/ (nobsi + 10) and 10 \/ (nobsi + 10).\r\n\r\nAdding data dependent shrink methods will take more time and we first need to figure out which available \"optimal\" shrinking we implement.\r\n\r\nThis empirical Bayes version has a large number of citations:\r\nSmyth, Gordon K. 2004. \u201cLinear Models and Empirical Bayes Methods for Assessing Differential Expression in Microarray Experiments.\u201d Statistical Applications in Genetics and Molecular Biology 3 (1). https:\/\/doi.org\/10.2202\/1544-6115.1027.\r\n\r\nbut it estimates the prior variance and does not just use the standard pooled variance as shrinking target (AFAICS from brief skimming)\r\n\r\nI found the article because the following article is based on it, uses Smyth as main reference for shrinkage\r\nQiu, Jing, Yue Qi, and Xiangqin Cui. 2014. \u201cApplying Shrinkage Variance Estimators to the TOST Test in High Dimensional Settings.\u201d Statistical Applications in Genetics and Molecular Biology 13 (3): 323\u201341. https:\/\/doi.org\/10.1515\/sagmb-2013-0045.\r\n\r\n\r\nAside, problem for implementation:\r\nAFAICS, the literature on variance shrinking for Microarray that include optimal shrinkage weights are all for balanced panels, i.e. equal nobs in all samples.\r\nThis makes it more difficult to shrink mainly variances of groups that have fewer observations.\r\n","interface idea:\r\n\r\nuse a dict for var shrinkage if not None, `shrink_var=None` in relevant test functions or methods of classes:\r\ne.g.\r\nshrink_option = {\r\n\"???\": \"add\",   # additive or multiplicative, keyword name ?\r\n\"weight\": None  # value of shrinkage parameter, additive or power coefficient, float in (0, 1), maybe tuple if sum < 1.\r\n      possibly string for \"optimal\" shrinkage method (if we have data to compute it)\r\n\"target\": \"mean\"  # string or float, shrinkage target, \"mean\" works if we have two or more samples.\r\n       and e.g. target=\"geom\" for geometric mean instead of arithmetic mean.\r\n\"ddof\": 0  # degrees of freedom correction, not sure whether we need to use it. e.g. Do we correct Welch df for shrinkage?\r\n      Which effective df do we use?\r\n}\r\n\r\nand maybe an option for handling cases where the variance in a sample is zero, lower bound or ignore, (eg. nobsi=1 ?)\r\n(geometric mean is zero if any variance is zero)\r\n\r\nAside:\r\nwe might need to validate dict keys that they are valid and not misspelled or unused keywords.\r\nmaybe a helper function to update a dict with another dict or just one that validates keys.\r\n`kwds_default.update(kwds_user)`\r\n\r\ncurrently, e.g. for cov_kwds of cov_type, we validate mainly case by case and not at source.\r\nThat was easier in that case because valid cov_kwds depends on cov_type.\r\n"],"labels":["comp-base","comp-stats","topic-penalization"]},{"title":"SUMM\/ENH\/REF  multiple comparison - overview, open, roadmap","body":"I don't have a good overview what the current status is and the roadmap for it should be.\r\nWe have several open issues and prs, enhancement requests, code that needs refactoring and more testing and likely bug fixes.\r\nI put it on the 0.15 roadmap #8217\r\n\r\nsome of it also applies to statsmodels.base, e.g. extension of t_test_pairwise\r\n\r\nan older generic issue **Multiple Comparisons** #852 includes discussion on rank based Nemenyi and dunnet's comparison to a control\r\n\r\n\r\n- oneway, continuous, e.g. tukeyhsd\r\n  - cleanup, open issues (check)\r\n  - #4379\r\n  - #4570\r\n  - dunnet\r\n  - unequal var, #7332\r\n- oneway, ranks\r\n  - #799\r\n  - #6994\r\n  - some discussion in #852\r\n  - \r\n- two-way ???\r\n  - #4323\r\n- noncontinuous data, binary, count ???\r\n- model post-estimation like t_test_pairwise\r\n  - #3297\r\n- other contrasts than all pairs\r\n  - #4361\r\n  - all versus mean ?\r\n  - sequential difference for ordinal, ordered groups\r\n- helper functions, reusable pieces\r\n  - t-max max-range, p-values using mutlivariate normal or t cdf\r\n    - #3297\r\n  - functions for contrasts and categorical codings (reference ???, I just had one but lost it again)\r\n  - grouping letters #3674\r\n- ...\r\n\r\n(still incomplete, there are likely open issues and prs missing from this list)\r\n\r\n","comments":["aside naming \r\n\r\nMaybe I should not use the term \"contrasts\", and should use \"constraints\" or \"restrictions\" instead.\r\nStandard definition of contrasts in statistics has a summation requirement (sum to zero?), that we don't always use.\r\nWe can match terminology in wald_test and fit_constrained.\r\n\r\n","Monte Carlo for continuous, compares tukey-hsd, games howell (unequal variance) and others\r\n\r\ndisadvantage: \r\nThey don't show the actual size for unequal variance case, table has adjusted alpha so that size is around 0.05.\r\nThey simulate different distributions but assume all samples come from the same distribution family with and without equal variance.\r\nMaybe there are more recent monte carlo studies.\r\n\r\nPhilip H. Ramsey , Kyrstle Barrera , Pri Hachimine-Semprebom & Chang-Chia Liu (2011) Pairwise comparisons of means under realistic nonnormality, unequal variances, outliers and equal sample sizes, Journal of Statistical Computation and Simulation, 81:2, 125-135, DOI: 10.1080\/00949650903219935\r\n"],"labels":["type-enh","comp-base","comp-stats","type-refactor"]},{"title":"ENH: closed testing procedures for multiple comparisons, pairwise","body":"(post-hoc) multiple comparisons like all pairs comparisons need p-value corrections.\r\n\r\nWe don't have any closed testing procedure that \r\n\r\nHowever, sandbox.stats.multicomp has several functions or classes for subsets or partitions of comparisons.\r\nThis is unfinished, I never spent enough time on this to get a proper version, e.g. R book for multicomp has chapter(s) for it.\r\n\r\ncurrent context\r\nnon-parametric comparisons for oneway data: several references describe the closed procedure (but it's a bit messy and I didn't look at the details.\r\nessentially all subset or intersections need to be checked sequentially\/recursively (tree form ?) for rejection.\r\n\r\nbased on comments in the nonparametric comparison literature:\r\nAll against one comparisons have proper ordering\/monotonicity\/positive correlation to use e.g. Holm.\r\nAll pairwise comparisons can\/does have negative and positively correlated pvalues and generic step up\/down procedure are not valid, \r\nClosed procedures for it need to check all intersection hypothesis.\r\nI'm not sure: AFAIR, the literature for closed procedures use reject decisions for each hypothesis, corrected pvalues would need to have a max or cumulative max along the intersection hypothesis paths.\r\n\r\n(That's not a target for me anytime soon, but an idea for using or finishing code in sandbox multicomp.)\r\n\r\nrelated:\r\nIs there a connection between `set_partition` function and grouping\/letter codes in #3674 ?\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"ENH: Critical values for no trend parameter in Coint function","body":"I am trying to run an Engle-Granger cointegration test using `statsmodels.tsa.stattools`'s `coint` function with no constant and no trend (using parameter `trend=\"n\"`). For some reason it appears that there is no critical values available for this test. A critical value array is simple returned with three NaN values. See below for a minimum reproducible example. I believe this has something to do with MacKinnon (1991) not including critical values for the no trend, no constant version of the test. The `coint` function is calculating its critical values based on this paper and 2010 update to this paper, I believe. Unfortunately, I have not been able to dig in enough to determine whether this is true and if so what appropriate values would be. \r\n\r\nMinimum reproducible examples:\r\n```\r\nimport yfinance as yf\r\nfrom statsmodels.tsa.stattools import coint\r\n\r\ndf = yf.download(\"AAPL, MSFT\", start=\"2020-01-01\").get(\"Adj Close\")\r\n\r\nres = coint(df.iloc[:,1], df.iloc[:,0], trend=\"n\", maxlag=1)\r\n\r\nprint(res)\r\n```\r\n\r\nReturns: \r\n`>>> (-1.475451313104536, 0.4798252707083076, [nan, nan, nan])`\r\n\r\nHere is a (potentially) useful researchgate thread regarding MacKinnon (1991) no trend, no constant critical values:\r\nhttps:\/\/www.researchgate.net\/post\/Critical_values_Engle-Granger_cointegration_test","comments":["I have not looked at this in a very long time.\r\n\r\nThe code mentions that critical values are not available in the article used for critical values\r\nI don't know if there are other tables or approximate critical values in other references\r\n\r\n```\r\nif trend == \"n\":\r\n        crit = [np.nan] * 3  # 2010 critical values not available\r\n```","The arch package has `engle_granger`, which has critical values for no trend. \r\n\r\n\r\n\r\n","Thank you for both the comments!"],"labels":["comp-tsa","type-enh"]},{"title":"ENH: Improve detection of number of CPUs ","body":"Hello\r\n\r\n`tools\/parallel.py` uses `multiprocessing.cpu_count()` to ge the number of available cpus which retruns the number of cpu in the machine. But this is not the same as the number of cpu available to the process. For example, you can run in a taskset context or a batch scheduler like slurm.\r\n\r\nsee:\r\n```\r\n$ nproc\r\n96\r\n$ taskset -c 1 nproc\r\n1\r\n$ taskset -c 1 python3 -c \"import multiprocessing; print(multiprocessing.cpu_count())\"\r\n96\r\n```\r\nI would suggest to use len(os.sched_getaffinity(0)) instead of multiprocessing.cpu_count()\r\n```\r\n$ python3 -c \"import os; print(len(os.sched_getaffinity(0)))\"\r\n96\r\n$ taskset -c 1 python3 -c \"import os; print(len(os.sched_getaffinity(0)))\"\r\n1\r\n```\r\nregards\r\n\r\nEric","comments":[],"labels":["type-enh"]},{"title":"X13 temp file error","body":"#### Describe the bug\r\n\r\nTrying to run x13 on Windows 10 with statsmodels 0.12.2 and python 3.8.8  Ran the code below\r\n\r\n\r\nimport statsmodels.api as sm\r\n\r\n ###### Set path to folder where the x13 package is downloaded and extracted from census.gov\r\nXPATH = r'C:\\Users\\fall-\\Documents\\Job Python\\Alpha\\TAA\\winx13html_v3-1\\WinX13\\x13as'\r\nos.chdir(XPATH)\r\nos.environ['X13PATH'] = XPATH\r\n\r\ndata = sm.datasets.macrodata.load_pandas()\r\ndf = data.data.set_index(pd.period_range('1\/1\/1959', '9\/1\/2009', freq='Q'))\r\ndf = df.dropna()\r\nres = sm.tsa.x13_arima_select_order(df['realgdp'], x12path=XPATH, print_stdout=False)\r\n\r\n\r\n\r\nthe output is shown below\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-28-346f73075be7> in <module>\r\n----> 1 res = sm.tsa.x13_arima_select_order(df['realgdp'], x12path=XPATH, print_stdout=False)\r\n      2 res.plot()\r\n      3 plt.show()\r\n\r\n~\\anaconda3\\envs\\AlgoAddict_Datacamp\\lib\\site-packages\\pandas\\util\\_decorators.py in wrapper(*args, **kwargs)\r\n    205                 else:\r\n    206                     kwargs[new_arg_name] = new_arg_value\r\n--> 207             return func(*args, **kwargs)\r\n    208 \r\n    209         return cast(F, wrapper)\r\n\r\n~\\anaconda3\\envs\\AlgoAddict_Datacamp\\lib\\site-packages\\statsmodels\\tsa\\x13.py in x13_arima_select_order(endog, maxorder, maxdiff, diff, exog, log, outlier, trading, forecast_periods, start, freq, print_stdout, x12path, prefer_x13)\r\n    572     in.\r\n    573     \"\"\"\r\n--> 574     results = x13_arima_analysis(endog, x12path=x12path, exog=exog, log=log,\r\n    575                                  outlier=outlier, trading=trading,\r\n    576                                  forecast_periods=forecast_periods,\r\n\r\n~\\anaconda3\\envs\\AlgoAddict_Datacamp\\lib\\site-packages\\pandas\\util\\_decorators.py in wrapper(*args, **kwargs)\r\n    205                 else:\r\n    206                     kwargs[new_arg_name] = new_arg_value\r\n--> 207             return func(*args, **kwargs)\r\n    208 \r\n    209         return cast(F, wrapper)\r\n\r\n~\\anaconda3\\envs\\AlgoAddict_Datacamp\\lib\\site-packages\\statsmodels\\tsa\\x13.py in x13_arima_analysis(endog, maxorder, maxdiff, diff, exog, log, outlier, trading, forecast_periods, retspec, speconly, start, freq, print_stdout, x12path, prefer_x13)\r\n    449             print(p.stdout.read())\r\n    450         # check for errors\r\n--> 451         errors = _open_and_read(ftempout.name + '.err')\r\n    452         _check_errors(errors)\r\n    453 \r\n\r\n~\\anaconda3\\envs\\AlgoAddict_Datacamp\\lib\\site-packages\\statsmodels\\tsa\\x13.py in _open_and_read(fname)\r\n    204 def _open_and_read(fname):\r\n    205     # opens a file, reads it, and make sure it's closed\r\n--> 206     with open(fname, 'r', encoding=\"utf-8\") as fin:\r\n    207         fout = fin.read()\r\n    208     return fout\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\fall-\\\\AppData\\\\Local\\\\Temp\\\\tmp_iu7e69r.err'\r\n\r\n\r\n\r\n\r\n","comments":["I can't reproduce.  When I put the binary in the XPATH location it works without errors.\r\n","> \r\n\r\n\r\n\r\n> I can't reproduce. When I put the binary in the XPATH location it works without errors.\r\n\r\nPlease clarify, what is the binary? Is it the folder that contains the file \"x13as_html.exe\"  generated from \"WinX13.exe\" ?\r\n\r\nI might think that the file was generated incorrectly.","x13as_ascii is the required binary.","> x13as_ascii is the required binary.\r\n\r\nI didn't load the ascii below. When I put it in the path, it works without errors. Many thanks!\r\n![image](https:\/\/user-images.githubusercontent.com\/60224349\/189149108-bf5d753d-d4b6-40ed-8927-a2105f211347.png)\r\n\r\n","Just ask for a confirmation that I download the package correctly. Because I downloaded both the HTML and ASCII files and put them in the same folder.","I am getting same error and correctly downloaded packages\r\n--------------------------------------------------------------------------\r\nX13NotFoundError                          Traceback (most recent call last)\r\n<ipython-input-56-c0a1d58177f0> in <module>\r\n----> 1 res = sm.tsa.x13_arima_analysis(s,x12path=XPATH)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\pandas\\util\\_decorators.py in wrapper(*args, **kwargs)\r\n    212                 else:\r\n    213                     kwargs[new_arg_name] = new_arg_value\r\n--> 214             return func(*args, **kwargs)\r\n    215 \r\n    216         return cast(F, wrapper)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\statsmodels\\tsa\\x13.py in x13_arima_analysis(endog, maxorder, maxdiff, diff, exog, log, outlier, trading, forecast_periods, retspec, speconly, start, freq, print_stdout, x12path, prefer_x13)\r\n    411     back in.\r\n    412     \"\"\"\r\n--> 413     x12path = _check_x12(x12path)\r\n    414 \r\n    415     if not isinstance(endog, (pd.DataFrame, pd.Series)):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\statsmodels\\tsa\\x13.py in _check_x12(x12path)\r\n     84     x12path = _find_x12(x12path)\r\n     85     if not x12path:\r\n---> 86         raise X13NotFoundError(\"x12a and x13as not found on path. Give the \"\r\n     87                                \"path, put them on PATH, or set the \"\r\n     88                                \"X12PATH or X13PATH environmental variable.\")\r\n\r\nX13NotFoundError: x12a and x13as not found on path. Give the path, put them on PATH, or set the X12PATH or X13PATH environmental variable.\r\n","\r\n`Got it to work after many trials\r\n\r\n1.  download directory from \r\n\r\nhttps:\/\/www.census.gov\/data\/software\/x13as.X-13ARIMA-SEATS.html\r\ndownload ascii version\r\nchange the exe file to x13as\r\n\r\nuse the following code\r\nimport os\r\nXPATH = r'C:\/Users\/lenny\/Documents\/Seats-10-29-22\/x13as'\r\nos.chdir(XPATH)\r\nos.environ['X13PATH'] = XPATH\r\n\r\n\r\nres1=sm.tsa.x13_arima_analysis(empl_1939,prefer_x13=True)\r\n","converting to FAQ issue for listing problems with x13 file location","I was having the same issue with the Windows X-13 html version of the model as in:\r\n![image](https:\/\/user-images.githubusercontent.com\/50458445\/208166044-7d73f2d6-c518-4af0-a4ea-5b85a650bae8.png)\r\nDirect link: [census](https:\/\/www.census.gov\/data\/software\/x13as.Win_X-13.html#list-tab-635278563)\r\n\r\nI was able to debug it and found that the problem was in the function **_x13_arima_analysis_** in lines 438 to 463:\r\n\r\n```\r\n  ftempin = tempfile.NamedTemporaryFile(delete=False, suffix='.spc')\r\n  ftempout = tempfile.NamedTemporaryFile(delete=False)\r\n  try:\r\n      ftempin.write(spec.encode('utf8'))\r\n      ftempin.close()\r\n      ftempout.close()\r\n      # call x12 arima\r\n      p = run_spec(x12path, ftempin.name[:-4], ftempout.name)\r\n```\r\n\r\n Is my understanding that the function call run_spec was supposed to create all temporary files necessary for use. However, it does not create files:\r\n```\r\nftempout.name + '.err'\r\nftempout.name + '.out'\r\n```\r\nHence, function calls:\r\n```\r\n# check for errors\r\nerrors = _open_and_read(ftempout.name + '.err')\r\n_check_errors(errors)\r\n\r\n# read in results\r\nresults = _open_and_read(ftempout.name + '.out')\r\n```\r\nreturns **file not found errors**.\r\n\r\nAdding:\r\n```\r\nopen(ftempout.name + '.err', 'a').close()\r\nopen(ftempout.name + '.out', 'a').close()\r\n```\r\nbefore _**_open_and_read**_ calls fixed the error.\r\n\r\nIs this version not supported by default? And hence a fix wouldnt be planned?\r\nThanks in advance.","see comments above. you need the ascii version and not the html version.\r\nI recently installed x13 on windows 11 without problems. https:\/\/github.com\/statsmodels\/statsmodels\/pull\/8564#issuecomment-1342882132\r\n\r\nI guess there should be a way to figure out that the ascii version is not available and raise an exception\r\n\r\n\r\nbrief check:\r\n`_find_x12` only checks that file is a executable\r\n`_binary_names` includes 'x13as_html' which should not be valid, added in https:\/\/github.com\/statsmodels\/statsmodels\/commit\/529d72bbfa1c921994973e2131c154e9b5bc2091\r\n\r\neither we delete the html version from `_binary_names`  \r\nor we check that \"html\" is in the file name and then raise a specific informative message.\r\n"],"labels":["comp-tsa","FAQ"]},{"title":"ENH: residual diagnostic test for cluster correlation, is something available?","body":"I would like to have a diagnostic test, hopefully based on residuals like LM\/score tests, for cluster correlation, i.e. specification test equivalent of cluster robust standard errors.\r\n\r\nThere is a quite a large literature for lm tests for correlation or dependence in panel data #2756 \r\nBut I don't find anything for arbitrary cluster correlation, as for example in GEE, in a brief google scholar search.\r\n\r\nThere is some literature in mixed effects models for testing variance components.\r\n\r\nWe might be able to get something based on intraclass correlation\r\n\r\nAs a measure of cluster correlation, we could compute the information matrix ratio, im_ratio, estimated under the no cluster correlation model, (similarly to looking at ratio between cluster robust and nonrobust cov_params or bse, possibly using GEE)\r\n\r\nmaybe related: hausman test for fixed versus random effect.\r\nbut here I want random\/mixed effects (that capture correlation) versus no effect.\r\n\r\ncurrent motivation: looking at cluster randomized trials.\r\nIn analogy to misspecified heteroscedasticity versus misspecified (serial or cross-sectional) correlation in panel data.\r\nIn count data we could also have excess dispersion.\r\n\r\n(open question)","comments":[],"labels":["type-enh","comp-stats","topic-diagnostic"]},{"title":"ENH: stats for cluster randomized trials - count, binomial","body":"PASS has power and sample size computation for cluster randomized trials, \r\nI never looked at the details\r\n\r\nSong and Ahn looks like a good starting point, although it has only very few citation. It describes and compares several methods, including GEE.\r\nThe variance correction versions take intracluster correlation into account and looks very similar to excess dispersion in beta-binomial, it's the same formula as far as I remember for dispersion coefficient. So we could use those also for over-dispersion.\r\n\r\nSong, James X., and Chul Ahn. 2002. \u201cCluster Randomization Trials: A Simulation Study.\u201d Biometrical Journal 44 (3): 375\u201390. [https:\/\/doi.org\/10.1002\/1521-4036(200204)44:3<375::AID-BIMJ375>3.0.CO;2-S](https:\/\/doi.org\/10.1002\/1521-4036(200204)44:3<375::AID-BIMJ375>3.0.CO;2-S)\r\n\r\nSong and Ahn is for odds-ratio in 2x2xk stratified two sample proportion Cochran-Mantel-Haenszel Test\r\n\r\n2 cases among many others that PASS has\r\nTests_for_Two_Proportions_in_a_Stratified_Cluster-Randomized_Design-Cochran-Mantel-Haenszel_Test.pdf\r\nTests_for_the_Difference_Between_Two_Poisson_Rates_in_a_Cluster-Randomized_Design.pdf\r\n\r\nmain reference for sample size and power in cluster CMH in PASS is \r\nXu, X., Zhu, H., and Ahn, C. 2019. 'Sample size considerations for stratified cluster randomization design with\r\nbinary outcomes and varying cluster size', Statistics in Medicine, Volume 38, Number 18, pages 3395-3404.\r\n\r\n\r\nrelated\r\ncorrelated, paired samples (cluster size = 2)  (mostly missing except for differenced endog in linear models)\r\nexcess dispersion in Binomial and Poisson #2890\r\n2x2xk samples #7184\r\nseveral of new stats functions for poisson rates allow for dispersion coefficient different from 1.\r\n\r\nI guess, the same as for odds-ratio in CMH will apply to other proportions_2indep comparisons #4828\r\ne.g. different effect sizes in meta-analysis\r\n\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-genmod","comp-discrete","comp-stats"]},{"title":"ENH: weighted average treatment effect, subgroup analysis, target population","body":"(just parking some references, I only read parts of abstracts, list collected by citations and related articles)\r\n\r\nThis sounds like the weighted ATE equivalent of get_prediction with weights and average is True.\r\ni.e. instead of just ATT and ATNT we also want AT for a target population or subgroup.\r\n\r\n\r\nChoi, Byeong Yeob. 2021. \u201cSubclassification Estimation of the Weighted Average Treatment Effect.\u201d Biometrical Journal 63 (8): 1706\u201328. https:\/\/doi.org\/10.1002\/bimj.202000310.\r\n\r\nLi, Fan, Ashley L. Buchanan, and Stephen R. Cole. 2022. \u201cGeneralizing Trial Evidence to Target Populations in Non-Nested Designs: Applications to AIDS Clinical Trials.\u201d Journal of the Royal Statistical Society: Series C (Applied Statistics) 71 (3): 669\u201397. https:\/\/doi.org\/10.1111\/rssc.12550.\r\n\r\nTao, Yebin, and Haoda Fu. 2019. \u201cDoubly Robust Estimation of the Weighted Average Treatment Effect for a Target Population.\u201d Statistics in Medicine 38 (3): 315\u201325. https:\/\/doi.org\/10.1002\/sim.7980.\r\n\r\nYang, Siyun, Fan Li, Laine E Thomas, and Fan Li. 2021. \u201cCovariate Adjustment in Subgroup Analyses of Randomized Clinical Trials: A Propensity Score Approach.\u201d Clinical Trials 18 (5): 570\u201381. https:\/\/doi.org\/10.1177\/17407745211028588.\r\n\r\nYang, Siyun, Elizabeth Lorenzi, Georgia Papadogeorgou, Daniel M. Wojdyla, Fan Li, and Laine E. Thomas. 2021. \u201cPropensity Score Weighting for Causal Subgroup Analysis.\u201d Statistics in Medicine 40 (19): 4294\u20134309. https:\/\/doi.org\/10.1002\/sim.9029.\r\n\r\nZeng, Shuxi, Fan Li, Rui Wang, and Fan Li. 2021. \u201cPropensity Score Weighting for Covariate Adjustment in Randomized Clinical Trials.\u201d Statistics in Medicine 40 (4): 842\u201358. https:\/\/doi.org\/10.1002\/sim.8805.\r\n\r\n\r\nOne article linked to R package PSweight in abstract    https:\/\/github.com\/thuizhou\/PSweight\r\n","comments":["aside: some related articles referred in abstracts to overlap weighting, OW,\r\n\r\nThis might be similar to matching estimators but using weights based on covariate overlap. It oes not really look like adjustments for a target population (based on only partial skimming of abstracts).","another article: adding random selection from eligible trial population to trial participants\r\nestimates ATE for eligible trial population instead of ATE of trial sample.\r\n\r\nDahabreh, Issa J., Sarah E. Robertson, Eric J. Tchetgen, Elizabeth A. Stuart, and Miguel A. Hern\u00e1n. 2019. \u201cGeneralizing Causal Inferences from Individuals in Randomized Trials to All Trial-Eligible Individuals.\u201d Biometrics 75 (2): 685\u201394. https:\/\/doi.org\/10.1111\/biom.13009.\r\n\r\n\r\npresents 3 estimators: outcome model, IPW and augmented IPW\r\nweighting combines selection probability and treatment probability\r\n\r\nselection probability weighting is similar to sampling probability weighting in survey data analysis\r\n\r\ne.g. AIPW has 3 models: (i) trial participation model, (ii) treatment assignment model and (iii) outcome model\r\nand is doubly robust\r\n\r\noutcome model uses average prediction where average is over exog of eligible trial population and not over sample exogs.\r\n\r\nThis looks good and looks like a relatively straightforward extension of TE implementation adding one more model and combining two selection probabilities.\r\n\r\n","similar articles with target population\r\n\r\nDahabreh, Issa J., Sarah E. Robertson, Jon A. Steingrimsson, Elizabeth A. Stuart, and Miguel A. Hern\u00e1n. \u201cExtending Inferences from a Randomized Trial to a New Target Population.\u201d Statistics in Medicine 39, no. 14 (2020): 1999\u20132014. https:\/\/doi.org\/10.1002\/sim.8426.\r\n\r\nLi, Fan, Ashley L. Buchanan, and Stephen R. Cole. \u201cGeneralizing Trial Evidence to Target Populations in Non-Nested Designs: Applications to AIDS Clinical Trials.\u201d Journal of the Royal Statistical Society: Series C (Applied Statistics) 71, no. 3 (2022): 669\u201397. https:\/\/doi.org\/10.1111\/rssc.12550.\r\n\r\nMorris, Tim P., A. Sarah Walker, Elizabeth J. Williamson, and Ian R. White. \u201cPlanning a Method for Covariate Adjustment in Individually Randomised Trials: A Practical Guide.\u201d Trials 23, no. 1 (April 18, 2022): 328. https:\/\/doi.org\/10.1186\/s13063-022-06097-z.\r\n\r\nZhang, Shixiao, Peisong Han, and Changbao Wu. \u201cCalibration Techniques Encompassing Survey Sampling, Missing Data Analysis and Causal Inference.\u201d International Statistical Review n\/a, no. n\/a. Accessed January 30, 2023. https:\/\/doi.org\/10.1111\/insr.12518.\r\n\r\n"],"labels":["type-enh","comp-treatment"]},{"title":"ENH: multivariate count models, including count MNLogit","body":"based on discussion #8380\r\n\r\n- multinomial Logit can be extended to counts similar to GLM-Binomial versus Logit (we can reuse or copy-paste large parts of code for MNLogit)\r\n- other multivariate distributions, I have not looked much at it. I had looked at some literature using copulas for multivariate counts.\r\n- maybe: check how much we can reuse a Poisson loglinear model.\r\n\r\njust parking some references:\r\n(I didn't read much more than abstracts)\r\n\r\na good starting point is Zhang, Yiwen, Hua Zhou, Jin Zhou, and Wei Sun. 2017. Their online supplement includes score and hessian formulas.\r\nBhat 2022 seems to have a good introduction with overview of different approaches\r\n\r\nBhat, Chandra R. 2022. \u201cA Closed-Form Multiple Discrete-Count Extreme Value (MDCNTEV) Model.\u201d Transportation Research Part B: Methodological 164 (October): 65\u201386. https:\/\/doi.org\/10.1016\/j.trb.2022.08.001.\r\n\r\nCorsini, Noemi, and Cinzia Viroli. 2022. \u201cDealing with Overdispersion in Multivariate Count Data.\u201d Computational Statistics & Data Analysis 170 (June): 107447. https:\/\/doi.org\/10.1016\/j.csda.2022.107447.\r\n\r\nDas, Ishapathik, Sumen Sen, N. Rao Chaganty, and Pooja Sengupta. 2019. \u201cRegression for Doubly Inflated Multivariate Poisson Distributions.\u201d Journal of Statistical Computation and Simulation 89 (13): 2549\u201361. https:\/\/doi.org\/10.1080\/00949655.2019.1625051.\r\n\r\nInouye, David I., Eunho Yang, Genevera I. Allen, and Pradeep Ravikumar. 2017. \u201cA Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution.\u201d WIREs Computational Statistics 9 (3): e1398. https:\/\/doi.org\/10.1002\/wics.1398.\r\n\r\nKim, Juhyun, Yiwen Zhang, Joshua Day, and Hua Zhou. 2018. \u201cMGLM: An R Package for Multivariate Categorical Data Analysis.\u201d The R Journal 10 (1): 73\u201390.\r\n\r\nKoochemeshkian, Pantea, Nuha Zamzami, and Nizar Bouguila. 2020. \u201cFlexible Distribution-Based Regression Models for Count Data: Application to Medical Diagnosis.\u201d Cybernetics and Systems 51 (4): 442\u201366. https:\/\/doi.org\/10.1080\/01969722.2020.1758464.\r\n\r\nMa, Zichen, Timothy E. Hanson, and Yen-Yi Ho. 2020. \u201cFlexible Bivariate Correlated Count Data Regression.\u201d Statistics in Medicine 39 (25): 3476\u201390. https:\/\/doi.org\/10.1002\/sim.8676.\r\n\r\nPeyhardi, Jean, Pierre Fernique, and Jean-Baptiste Durand. 2021. \u201cSplitting Models for Multivariate Count Data.\u201d Journal of Multivariate Analysis 181 (January): 104677. https:\/\/doi.org\/10.1016\/j.jmva.2020.104677.\r\n\r\nTerza, Joseph V., and Paul W. Wilson. 1990. \u201cAnalyzing Frequencies of Several Types of Events: A Mixed Multinomial- Poisson Approach.\u201d The Review of Economics and Statistics 72 (1): 108\u201315. https:\/\/doi.org\/10.2307\/2109745.\r\n\r\nZhang, Yiwen, Hua Zhou, Jin Zhou, and Wei Sun. 2017. \u201cRegression Models for Multivariate Count Data.\u201d Journal of Computational and Graphical Statistics 26 (1): 1\u201313. https:\/\/doi.org\/10.1080\/10618600.2016.1154063.\r\n\r\n","comments":[],"labels":["type-enh","comp-discrete"]},{"title":"ENH: MNLogit for fractional data, allow and verify","body":"based on discussion in #8380\r\n\r\nloglike score and hessian in MNLogit seem to work for fractional data.\r\nThis needs to be verified with some examples and results need to be checked.\r\n\r\nAs it seems to be ok, we can allow 2-dim endog in MultinomialModel._handle_data, i.e. don't convert endog to wendog from categorical labels to dummy representation and use provided endog as wendog. with I guess some checks like rowsum = 1.\r\n\r\nThis should work as QMLE with cov_type=\"HC0\"\r\n\r\n","comments":["related question\r\nsimple case based on subpopulation as observations, one categorical treatment variable\r\n\r\nhttps:\/\/stats.stackexchange.com\/questions\/603211\/estimating-the-parameter-in-a-mixed-population\r\n"],"labels":["type-enh","comp-discrete"]},{"title":"ENH: add Diebold-Mariano forecast comparison test","body":"- [ ] closes #xxxx\r\n- [x] tests added \/ passed. \r\n- [x] code\/documentation is well formatted.  \r\n- [x] properly formatted commit message. See \r\n      [NumPy's guide](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/dev\/gitwash\/development_workflow.html#writing-the-commit-message). \r\n\r\n<details>\r\nRelated to issue #8374 \r\n\r\nI found an implementation on this github https:\/\/github.com\/johntwk\/Diebold-Mariano-Test which I modified and adapted to the code base. It produces the same results has the R package. Tests are implemented, but I am not very familiar with unit tests and therefore they may not be well done. \r\n\r\nI am currently working on the multivariate version of the test (more then 2 forecasts).\r\n**Notes**:\r\n\r\n* It is essential that you add a test when making code changes. Tests are not \r\n  needed for doc changes.\r\n* When adding a new function, test values should usually be verified in another package (e.g., R\/SAS\/Stata).\r\n* When fixing a bug, you must add a test that would produce the bug in main and\r\n  then show that it is fixed with the new code.\r\n* New code additions must be well formatted. Changes should pass flake8. If on Linux or OSX, you can\r\n  verify you changes are well formatted by running \r\n  ```\r\n  git diff upstream\/main -u -- \"*.py\" | flake8 --diff --isolated\r\n  ```\r\n  assuming `flake8` is installed. This command is also available on Windows \r\n  using the Windows System for Linux once `flake8` is installed in the \r\n  local Linux environment. While passing this test is not required, it is good practice and it help \r\n  improve code quality in `statsmodels`.\r\n* Docstring additions must render correctly, including escapes and LaTeX.\r\n\r\n<\/details>\r\n","comments":["Hello @alekracicot! Thanks for updating this PR. We checked the lines you've touched for [PEP\u00a08](https:\/\/www.python.org\/dev\/peps\/pep-0008) issues, and found:\n\n\n\n\n\nThere are currently no PEP 8 issues detected in this Pull Request. Cheers! :beers: \n\n##### Comment last updated at 2022-08-26 03:40:05 UTC","Thank you for submitting this pull request.  I notice a couple of outstanding issues:\r\n\r\n1. Given how closely this code follows \/ uses the reference implementation, I think we should at least reach out to the author to see if they are okay with this being included in Statsmodels.\r\n2. At the very minimum, we need to include attribution, the copyright notice for John Tsang, and a note that this code is based on code licensed under MIT.\r\n3. Thanks for writing unit tests. You should also include the code for the package that generated the results you are testing against (e.g. in R, Stata, Eviews, or whatever you used).\r\n4. Statsmodels style is to include spaces around all operators other than `**`.  So instead of `x+y` we would have `x + y`.","Hi, I will reach out to the author & implement de changes.","> 3\\. Thanks for writing unit tests. You should also include the code for the package that generated the results you are testing against (e.g. in R, Stata, Eviews, or whatever you used).\r\n\r\nI reached out to the author and he is fine with including this to statsmodels. I also included the license. As for the code generating the results, should I add it to the docstring of the test? \r\n\r\nBest, \r\nAlek ","I think we should make criterion flexible.  Specifically, it should accept either a string or a function with the signature `Callable[[ndarray, ndarray], ndarray]` where the inputs would be `y`, and either  `f1` or `f2`. It would then return `L(y, fj)` for j=1,2.  This would let the user provide other loss functions, but otherwise use the same function. Something like\r\n\r\n```python\r\ndef qlik(y, f):\r\n    return np.log(f) + y \/ f\r\n\r\ndiebold_mariano_test(y, forecast1, forecast2, horizon=1, criterion=qlik)\r\n```"],"labels":["comp-tsa","type-enh"]},{"title":"[BUG] Cannot predict after SARIMAXResults `remove_data` method called","body":"#### Describe the bug\r\n\r\nCannot predict after SARIMAXResults `remove_data` method called. As an example, for my project the statsmodel results object is ~2 Gb and `remove_data` reduced this to ~200 Mb (which I was super excited about!), but then unable to reuse results object to make predictions and so rather useless.\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\nCode example taken from statsmodels sarimax documentation here: https:\/\/github.com\/statsmodels\/statsmodels\/issues\/new?assignees=&labels=&template=bug_report.md&title=\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom scipy.stats import norm\r\nimport statsmodels.api as sm\r\nimport matplotlib.pyplot as plt\r\nfrom datetime import datetime\r\nimport requests\r\nfrom io import BytesIO\r\n\r\n# Dataset\r\nwpi1 = requests.get('https:\/\/www.stata-press.com\/data\/r12\/wpi1.dta').content\r\ndata = pd.read_stata(BytesIO(wpi1))\r\ndata.index = data.t\r\n# Set the frequency\r\ndata.index.freq=\"QS-OCT\"\r\n\r\n# Fit the model\r\nmod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,1))\r\nres = mod.fit(disp=False)\r\nprint(res.summary())\r\n\r\n# Prediction\r\nres.forecast(5)  # this works, but file size is very big\r\n\r\n# Remove data & re-run prediction\r\nres.remove_data()  # reduces file size substantially\r\nres.forecast(5)  # does not work! (error msg below)\r\n```\r\n<details>\r\n\r\n**Note**: As you can see, there are many issues on our GitHub tracker, so it is very possible that your issue has been posted before. Please check first before submitting so that we do not have to handle and close duplicates.\r\n\r\n**Note**: Please be sure you are using the latest released version of `statsmodels`, or a recent build of `main`. If your problem has been fixed in an unreleased version, you might be able to use `main` until a new release occurs. \r\n\r\n**Note**: If you are using a released version, have you verified that the bug exists in the main branch of this repository? It helps the limited resources if we know problems exist in the current main branch so that they do not need to check whether the code sample produces a bug in the next release.\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\n```\r\n1991-01-01    118.358862\r\n1991-04-01    120.340500\r\n1991-07-01    122.167204\r\n1991-10-01    123.858458\r\n1992-01-01    125.431301\r\nFreq: QS-OCT, Name: predicted_mean, dtype: float64\r\n```\r\n\r\n#### Error message\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/workspace\/scripts\/..\/smartshift_load_forecasting\/model\/arima_fourier_nem\/model.py\", line 37, in get_forecast_results\r\n    new_forecast = fitted_model.forecast(steps=n_steps, exog=exog)\r\n  File \"\/virtualenvs\/smartshift-load-forecasting-9TtSrW0h-py3.9\/lib\/python3.9\/site-packages\/statsmodels\/base\/wrapper.py\", line 113, in wrapper\r\n    obj = data.wrap_output(func(results, *args, **kwargs), how)\r\n  File \"\/virtualenvs\/smartshift-load-forecasting-9TtSrW0h-py3.9\/lib\/python3.9\/site-packages\/statsmodels\/tsa\/statespace\/mlemodel.py\", line 3442, in forecast\r\n    return self.predict(start=self.nobs, end=end, **kwargs)\r\n  File \"\/virtualenvs\/smartshift-load-forecasting-9TtSrW0h-py3.9\/lib\/python3.9\/site-packages\/statsmodels\/tsa\/statespace\/mlemodel.py\", line 3403, in predict\r\n    prediction_results = self.get_prediction(start, end, dynamic, **kwargs)\r\n  File \"\/virtualenvs\/smartshift-load-forecasting-9TtSrW0h-py3.9\/lib\/python3.9\/site-packages\/statsmodels\/tsa\/statespace\/mlemodel.py\", line 3287, in get_prediction\r\n    self.model._get_prediction_index(start, end, index))\r\n  File \"\/virtualenvs\/smartshift-load-forecasting-9TtSrW0h-py3.9\/lib\/python3.9\/site-packages\/statsmodels\/tsa\/base\/tsa_model.py\", line 833, in _get_prediction_index\r\n    nobs = len(self.endog)\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\nINSTALLED VERSIONS\r\n------------------\r\nPython: 3.10.4.final.0\r\nOS: Linux 5.14.0-1048-oem #55-Ubuntu SMP Mon Aug 8 14:58:10 UTC 2022 x86_64\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_AU.UTF-8\r\n\r\nstatsmodels\r\n===========\r\n\r\nInstalled: 0.13.0 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/statsmodels)\r\n\r\nRequired Dependencies\r\n=====================\r\n\r\ncython: 0.29.30 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/Cython)\r\nnumpy: 1.23.1 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/numpy)\r\nscipy: 1.7.3 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/scipy)\r\npandas: 1.4.0 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/pandas)\r\n    dateutil: 2.8.2 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/dateutil)\r\npatsy: 0.5.2 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/patsy)\r\n\r\nOptional Dependencies\r\n=====================\r\n\r\nmatplotlib: 3.5.2 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/matplotlib)\r\n    backend: module:\/\/matplotlib_inline.backend_inline \r\ncvxopt: Not installed\r\njoblib: 1.1.0 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/joblib)\r\n\r\nDeveloper Tools\r\n================\r\n\r\nIPython: 8.4.0 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/IPython)\r\n    jinja2: Not installed\r\nsphinx: Not installed\r\n    pygments: 2.11.2 (\/home\/julian\/anaconda3\/envs\/smartshift\/lib\/python3.10\/site-packages\/pygments)\r\npytest: Not installed\r\nvirtualenv: Not installed\r\n\r\n<\/details>\r\n","comments":["Thanks for reporting this.  I think we should probably be able to support forecasting after removing data, so this is probably just about what assumptions the forecasting code makes (e.g. in the error message above, it checks the length of `endog` which doesn't exist after the data is removed).","Yep perfect, and I think similar for any exogenous data. Happy to work on a PR but not possible for me this week."],"labels":["type-enh","comp-tsa-statespace"]},{"title":"ENH: power, sample size computation for rank_compare, brunner-munzel, Wilcoxon-Mann-Whitney","body":"Currently we don't have power and sample size functions for statsmodels.stats.nonparametric.RankCompareResult\r\n\r\nI just found this reference, but didn't specifically search for this topic\r\n\r\nHapp, Martin, Arne C. Bathke, and Edgar Brunner. 2019. \u201cOptimal Sample Size Planning for the Wilcoxon-Mann-Whitney Test.\u201d Statistics in Medicine 38 (3): 363\u201375. https:\/\/doi.org\/10.1002\/sim.7983.\r\n\r\nrelated, but we don't have anything yet on group-sequential trials\r\nNowak, Claus P, Tobias M\u00fctze, and Frank Konietschke. 2022. \u201cGroup Sequential Methods for the Mann-Whitney Parameter.\u201d Statistical Methods in Medical Research, June, 09622802221107103. https:\/\/doi.org\/10.1177\/09622802221107103.\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"mixed effects absence: No way to model heteroskedasticity, choose covariance format of residuals, compute normalized residuals","body":"Definition of \"normalized residuals\": the normalized residuals=resid(x,type='normalized')= (standardized residuals pre-multiplied by the inverse square-root factor of the estimated error\/working correlation matrix) as is available from the nlme package.","comments":[],"labels":["type-enh","comp-regression"]},{"title":"ENH: MNLogit for counts or frequencies, similar to Binomial","body":"Maybe I have missed it in the library, but is there a way to use frequency weights for multinomial logistic regression?  If not, I would like to request it.\r\n\r\n**edit** question is for fractional or count endog in MNLogit and not for `freq_weights`\r\nsee discussion below","comments":["Looking more, is this request as simple as documenting that you can modify model.wendog with rows that sum to 1?","No, that won't work, rows of (multivariate version of) endog have to add to 1, i.e. they are either dummy variables (with or without reference choice) or the index of the choice, \r\nThe predicted endog will be choice probabilities which have to add also to 1 when reference choice is included.\r\n\r\nAdding freq_weights is quite a bit of work, only GLM has it so far.\r\nThe main part is that loglike, score, and hessian computation need to add weighted sums. For inference we need weighted\/total nobs and not nobs defined as number of rows.\r\n\r\nfreq_weights are conceptually relatively simple, but they will have to be added at many different places so that most methods take them correctly into account.\r\n\r\nOne complication is that large parts of classes use inherited, more generic methods inside the discrete classes, so changes will not be restricted to the MNLogit model and result classes.\r\n","Just to make sure I understand, if I do something like\r\n```\r\nmodel = MNLogit(Y,X)\r\nmodel.wendog = <n x j array whose rows contain non-negative potentially fractional elements that **do** sum to 1>\r\nmodel.fit().summary()\r\n```\r\nyou are saying this will not work?  I briefly looked at the code and thought your computations of the log-likelihood, score, and Hessian would all still be correct, but maybe I misread.  Assuming these three functions work correctly, will the base class functionality not continue to work based on this, or am I missing some assumptions made by those methods?\r\n","maybe I misunderstood what you want.\r\n\r\nDo you want fractional choices, i.e. instead of a discrete 0-1 choice for each level, you have fractions for each choice, i.e. a continuous wendog?\r\n\r\n`freq_weights` just assumes that each row represents more than one observation, e.g. if we have only categorical regressors and we want to combine observations with identical regressors and identical endog.\r\n\r\nTo the first, I don't know the answer, I would have to check details of the code.\r\ne.g. probit and logit use some shortcuts in the computation that only work with 0-1 endog and not with fractions. GLM does not have that problem. #7210\r\nThis depends on implementation details and I or we never checked this for MNLogit.\r\n(In OrderedModel, I also use a computational shortcut that assumes discrete 0-1 choice.)\r\n\r\nThe only other related issue that I find is #3537 which ended up discussing compositional data.\r\n(I had thought about fractional extension to OrderedModel, but decided to stick with not supporting fractional data because of much larger computational and memory requirements.)\r\n\r\n\r\n\r\n","Yes, fractional choices.  I realized by calling it \"freq_weights\" I may have derailed the conversation.","Another thought:\r\n\r\nGLM binomial is a model for both binary choices and counts. Internally it works with choice fractions (events) and the number of observation for it (number of trials).\r\n\r\nThe same idea would apply to multinomial logit models. In this case we would either need count frequencies for each choice or fraction for each choice plus the number of trials in that row of data. We might not need the number of trials if they are the same for all observations\/rows.)\r\nThis would be a proper MLE model and not a Quasi-MLE model for fractional or compositional data. (I was previously thinking only of QMLE for multivariate fractional data.)\r\n\r\nIt's a good enhancement request, but I don't know how much support for it there is already in the current MNLogit. (We would have to add at least something equivalent to n_trials.)\r\n\r\n","I edited my previous reply on freq_weights\r\n\r\nto clarify:\r\n\r\nfreq_weights can be used to combine observations with identical endog and identical exog.\r\nvar_weights and number of trials\/exposure can be used to include observations that have identical exog but not identical endog, but where endog is the observed mean (or sum in case of Poisson) of several observations.","AFAICS, MNLogit loglike, score and hessian use the full endog version, e.g. compute all `logprob` and not just the one corresponding to the selected discrete choice (as I do in OrderedModel).\r\n\r\nSo it should work with fractional wendog. \r\nThis assumes the underlying n_trials is the same across rows for the estimation.\r\nMost likely inference will not have the correct number of total underlying observations if each data row represents cases of several discrete choices.\r\nI think in terms of var_weights we are missing something like n_trials. Maybe scale = 1 \/ n_trials would work if n_trials array is constant. (thinking about the analogy to Binomial)\r\n\r\n","loglike doesn't have the ratio of factorial terms (for permutation) in loglike. It's constant 1 for single choice.\r\nHowever, AFAICS the term does not depend on params and so will drop out in score and hessian and will not affect estimation. \r\n","Agreed (that it looks like it will all work for my use-case), and given that my use-case seems to be handled, I will be totally satisfied with the current state of MNLogit.  Thank you for the discussion and the great package!","This issue should be split in two\r\n\r\n1) QMLE for fractional data, check that it works and is correct with robust cov_type\r\n2) add\/extend to multinomial count model\r\n\r\nto 2)\r\nI checked Stata which also seems to have only discrete choice version, choose 1 out of k choices. I didn't see a count version.\r\n\r\nThe count model version seems to be more common in statistics than econometrics\r\ne.g. Zhang et al refer to McCullagh and Nelder 1983 for the count model version of multinomial logit\r\n\r\nZhang, Yiwen, Hua Zhou, Jin Zhou, and Wei Sun. 2017. \u201cRegression Models for Multivariate Count Data.\u201d Journal of Computational and Graphical Statistics 26 (1): 1\u201313. https:\/\/doi.org\/10.1080\/10618600.2016.1154063.\r\n\r\nWe currently don't have a model version for a 2 x k contingency table with 2 samples and k multinomial categories of counts.\r\nUsing poisson loglinear model does not impose n_trials is fixed.\r\nMNLogit could be used if we blow up the number of rows to have individual choices instead of summary counts for each exog pattern.\r\n\r\n(Most likely a reason for the focus on individual discrete choice in econometrics is the usual presence of continuous explanatory variables so we have as many exog patterns as individuals.)\r\n\r\n\r\n"],"labels":["type-enh","comp-discrete"]},{"title":"ENH: add one-sided option to test_ordinal_association, cochran armitage trend test","body":"Currently test_ordinal_association only allows for two-sided trend test.\r\nrelated issue #1130 k-sample trend tests\r\n\r\ninstead we often want to test also for one-sided alternatives, e.g. positive trend in dose-response models\r\n\r\nreference, e.g.\r\n\r\nNeuh\u00e4user, Markus, and Ludwig A. Hothorn. 1999. \u201cAn Exact Cochran\u2013Armitage Test for Trend When Dose\u2013Response Shapes Are a Priori Unknown.\u201d Computational Statistics & Data Analysis 30 (4): 403\u201312. https:\/\/doi.org\/10.1016\/S0167-9473(98)00091-7.\r\n\r\n\r\nthis looks easy because the implementation already uses the normal distribution.\r\n\r\nI haven't checked yet: Can we add non-zero hypothesis, e.g. linear trend > low(er bound)?\r\n(similar to non-inferiority tests, minimum non-negligible trend)\r\nI guess it should be fine, it's just a wald-test on the trend slope coefficient.\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"LinAlgError: SVD did not converge error when fitted with SARIMAX model","body":"When I fitted the SARIMAX model with 10 data points, it failed to converge when the model tried to fit. I found this when I tried to look at the summary.\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\r\nimport random\r\n\r\ny = []\r\n# Set a length of the list to 10\r\nfor i in range(0, 10):\r\n    x = round(random.uniform(50, 500), 2)\r\n    y.append(x)\r\n\r\nmodel = SARIMAX(y, order=(0,2,2), seasonal_order=(0,0,0,0))\r\nfitted_model = model.fit()\r\nforecast_results = fitted_model.forecast(12)\r\n\r\n```\r\n<details>\r\n\r\nConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\r\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\r\n\r\n<\/details>\r\n\r\n\r\nIf the issue has not been resolved, please file it in the issue tracker.\r\n\r\n#### Expected Output\r\n\r\nA clear and concise description of what you expected to happen.\r\n\r\n#### Output of ``import statsmodels.api as sm; sm.show_versions()``\r\n\r\n<details>\r\n\r\n[paste the output of ``import statsmodels.api as sm; sm.show_versions()`` here below this line]\r\n0.13.2\r\n\r\n<\/details>\r\n","comments":["Thanks for reporting, but I can't replicate this error. Can you post the full exception message including the stack trace?","Sure. See below:\r\n\r\n`\/Users\/***\/opt\/anaconda3\/lib\/python3.8\/site-packages\/statsmodels\/tsa\/statespace\/sarimax.py:902: RuntimeWarning: Mean of empty slice.\r\n  params_variance = (residuals[k_params_ma:] ** 2).mean()\r\n\/Users\/***\/opt\/anaconda3\/lib\/python3.8\/site-packages\/numpy\/core\/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\r\n  ret = ret.dtype.type(ret \/ rcount)\r\n\/Users\/***\/opt\/anaconda3\/lib\/python3.8\/site-packages\/statsmodels\/tsa\/statespace\/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\r\n  warn('Non-invertible starting MA parameters found.'\r\n\/Users\/***\/opt\/anaconda3\/lib\/python3.8\/site-packages\/statsmodels\/base\/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\r\n  warnings.warn(\"Maximum Likelihood optimization failed to \"`","@ChadFulton \r\n\r\nHere is the y vector input used in the model. Does this help?\r\n\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\r\nimport random\r\n\r\ny = []\r\n# Set a length of the list to 10\r\nfor i in range(0, 10):\r\n    x = round(random.uniform(50, 500), 2)\r\n    y.append(x)\r\n    \r\n# y = [454.85, 367.44, 411.17, 428.79, 302.61, 164.83, 165.9, 412.9, 413.64, 431.65]\r\nmodel = SARIMAX(y, order=(0,2,2), seasonal_order=(0,0,0,0))\r\nfitted_model = model.fit()\r\n\r\n# forecast_results = [nan nan nan nan nan nan nan nan nan nan nan nan]\r\nforecast_results = fitted_model.forecast(12)\r\n```","This appears to be fixed in the current (unreleased) main branch, since when I run the above code, using the `y = [454.85, 367.44, 411.17, 428.79, 302.61, 164.83, 165.9, 412.9, 413.64, 431.65]` that you have commented out, I get:\r\n\r\n```python\r\n> print(forecast_results)\r\n[447.08657294 443.74877594 440.41097895 437.07318195 433.73538496\r\n 430.39758796 427.05979097 423.72199397 420.38419698 417.04639998\r\n 413.70860299 410.37080599]\r\n```","Which version is the unreleased one that you tested on? @ChadFulton "],"labels":["type-bug","comp-tsa-statespace"]},{"title":"Adding DM & HLN tests to statsmodels","body":"#### Is your feature request related to a problem? \r\nNot related to a problem, but I often want to test if a forecast is statistically different from another forecast. I don't see any implementation of the DM or HLN test in statsmodels.  \r\n\r\n#### Describe the solution you'd like\r\nA function taking three arrays of equal lengths representing the forecast 1, forecast 2 & the observed values as input and outputs the test statistic and p-value.\r\n\r\n#### Describe alternatives you have considered\r\nOtherwise, maybe a function taking fitted models, a regressor set and a dependent variable vector as input  and outputs the test statistic and p-value.\r\n\r\n#### Additional context\r\nLet me know if this is something that belongs in statsmodels. I already have \"bad\" code for the two tests. I would be interested to implement it myself in statsmodels. I would need directions regarding where such a test should be implemented.","comments":["A PR would be welcome.","@bashtage perfect I will be working on this. "],"labels":["comp-tsa","type-enh"]},{"title":"ENH: discrete (count) models, implement cov_type excess dispersion","body":"We could add excess dispersion `scale` option to cov_type in discrete models similar to GLM.\r\n\r\nThis is mainly relevant for count models, Poisson, NB, GP. \r\nI don't remember the details how excess dispersion affects binary models. (there is no extra variance parameter in binary models, it just rescales the \"true\" underlying params.) \r\n\r\nThe motivatin usecase is estimating NB2 with dispersion parameter alpha (available in discrete, but not in GLM) but with simple excess dispersion, scale factor for variance function, as in GLM, as alternative to using HC0.","comments":[],"labels":["type-enh","comp-genmod","comp-discrete"]},{"title":"DOC\/SUMM  missing notebooks (outside tsa)","body":"What notebooks do we need to add, especially for recently added features?\r\n\r\nI don't have an overview and most of my private notebooks are dev notebooks with extra experiments or using outdated pre-merge api.\r\nmy 0.14 roadmap #7720\r\noverview postestimation #7707\r\n\r\n- generic, general\r\n  - cov_type, AFAIR we don't have a notebook in examples nor in rst docs \r\n- for old code\r\n  - still no comprehensive doc, notebook for cov_type options\r\n  - no GAM notebook\r\n  - asymmetric kernel pdf, cdf, bernstein\r\n- post-estimation diagnostics\r\n  - influence: docs include influence for logit\/glm https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/influence_glm_logit.html , more recent version in my folder includes additional trys\/experiments\r\n  - diagnostic: a few other notebooks when I was working on diagnostic (2021-12-25)  try_betareg_prediction_scoretest.ipynb try_influence_zip.ipynb try_truncatedpoisson_new.ipynb try_discrete_get_distribution.ipynb Poisson_diagnostic_.ipynb (I didn't look at those)\r\n  - get_prediction need some examples for new features similar to margins and predicted means\r\n  - nonlinear prediction wald_nonlinear, not yet public, notebook for use for margins, see issue #5387\r\n  - wald tests, I think we don't have any good summary, quick overview, see e.g. https:\/\/stackoverflow.com\/a\/34232975\/333700\r\n    good would be full example including wald_test_terms and pairwise \r\n  - get_prediction: A good example would be for how to use it to simulate a model given exog. Monte Carlo.\r\n  -  merged\r\n     - [x] overview with poisson merged #8420\r\n- stats\r\n  has very few notebooks in current docs\r\n  - oneway\r\n    several notebooks with examples, dev version and experiments, not systematic, \r\n    try_welsh_anova.ipynb (includes oneway for scale\/dispersion),  try_oneway_equivalence_power_newnames.ipynb, ...\r\n  - nonparametric brunner-munzel, rank_compare\r\n    - `ex_rank_compate.ipynb` mostly dev notebook, 6000 lines, uses merged functions, Munzel and Hauschke example\r\n    - notebook `try_brunner_munzel.ipynb` is original dev notebook, does not use merged functions, has Example from Neuhaeuser and Poulin 2004 and Example from Munzel and Hauschke 2003\r\n    - [x]  #8427\r\n  - poisson: no good notebook, outdated api, not much text, \r\n    too many functions and methods, need to write short overview version\r\n    - most recent is `check_poisson_inference_pr.ipynb`\r\n    - [x] #8412\r\n  - proportion (can wait until after refactoring)\r\n    - `ex_proportion_2indep_short.ipynb` basic example with all compare and methods in folder notebook_old\r\n    - `try_anovapower_proportionztest.ipynb` basic example for how to use FTestAnovaPower with effect size\r\n  - ...\r\n- newer models\r\n  - count models, only poisson and negbin in https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/discrete_choice_overview.html#Poisson\r\n    missing: zero-inflated, new hurdle, NBP, GPP, ... \r\n    - [x] hurdle: no approximately clean notebook, `try_truncated_hurdle_count.ipynb` includes racd10 example, other notebooks use simulated data\r\n       hurdle notebook merged #8424 \r\n    - truncated not included in hurdle notebook\r\n  - beta\r\n    - notebook in a gist, might be the cleanest https:\/\/gist.github.com\/josef-pkt\/4d0fd829c8fbc1c4237d989ba3dbb088\r\n  - ...\r\n- treatment\r\n  - `ex_treatment_effect_after_merge.ipynb` example ok, needs some cleanup and more text  \r\n  - [x] #8418\r\n- multivariate\r\n  only has PCA\r\n- ... ","comments":["an idea for easier discovery of notebooks:\r\n\r\nAdd a section on top of https:\/\/www.statsmodels.org\/dev\/examples\/index.html with \"recently added notebooks\" with just a plain list of notebook links (no plots, screen shots)\r\n\r\nAnother idea would be a \"What's new\" section with plain list of links to notebook.\r\nThe release notes do not have links to notebooks.","aside: 0.13.1 and 0.13.2 do not have release notes in docs\r\nhttps:\/\/www.statsmodels.org\/dev\/release\/index.html","related: missing theory and no references in docs, for \"Things that I made up\"\r\n\r\nexample Influence for zero-inflated:\r\nI have a private dev notebook `try_influence_zip.ipynb` which prints several warnings about which statistic \/ variable is used in plots.\r\nThe docstring has a notes section to point out some problems with the generic MLE definition of influence measures\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.discrete.count_model.ZeroInflatedPoissonResults.get_influence.html\r\n\r\nThe class returned by get_influence is MLEInfluence which does not have references for the generic definitions, e.g. of generalized leverage\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.outliers_influence.MLEInfluence.html\r\n","Notes: notebook\/rst docs ? for postestimation overview\r\n\r\ntraditional: wald inference and `predict`\r\ngeneral: GLM, OLS\/WLS have model specific implementation, eg. influence\r\nMLE models: generic and model specific \r\n\r\nmore details mainly for MLE models, currently discrete models and BetaModel\r\n\r\nget_prediction\r\nget_influence\r\nget_diagnostic\r\nget_distribution\r\nscore_test\r\n\r\nwork in progress, and not all are available for all main models\r\nboth generic and model specific methods\r\nexperimental, api and, possibly, definitions and defaults will change (e.g. if generic methods are replaced by model specific methods)\r\nalso, there are still some api inconsistencies\r\n\r\nexample: Poisson\r\n\r\n\r\n\r\n\r\n","get prediction, wald_nonlinear, deltacov:\r\n\r\nI don't remember what is actually available, beside get_prediction what=\"xxx\".\r\ne.g. usages similar to margins and prediction average means.\r\nexamples for nonlinear functions, e.g. comparison of two predictions,\r\n???\r\n\r\n**update**\r\nAFAICS, my latest notebooks when working on the prediction PR 2021-11-27\r\ncheck_nonlinear_delta_prediction.ipynb  several nonlinear functions, including margins for count models\r\ntry_get_prediction_docvis.ipynb  docvis data Cameron\/Trivedi book (?),  countmodels, predict and some diagnostic\r\n","hurdle model and related is missing from doc, based on sphinx search\r\nand I guess also from `api`\r\nmerged PR is #7973\r\n\r\nMy `try_truncated_hurdle_count` has examples including usage of `get_prediction` and `get_diagnostic` for some of the new count models.\r\nOther than that, I don't know what post-estimation is available and verified\r\n\r\nchecking:\r\nThe hurdle, truncated results classes don't define any of the `get_xxx` methods. Those are inherited from `CountResults`.\r\nunit test in test_truncated_model have good coverage for get_prediction and basic coverage for get_diagnostic.\r\nAlso, I don't know if score_test works. (Two part was made to work for BetaModel, but I don't remember about two-submodel cases as in ZI and hurdle models.)\r\n\r\nWe need to check inherited get_distribution and get_influence\r\n\r\npost-estimation overview issue #7707 does not include truncated and hurdle models\r\n\r\nnaming not clear TruncatedLF\r\n\"L\" means left, but I didn't remember why \"F\"\r\ncomments in PR are not clear, but they mention \"fixed\" truncation\r\nThis should be added to docstrings\r\n\r\n**update**\r\nI think I remember now after working on an example\r\n\"F\"  stands for truncation point is constant across observation, a fixed truncation point and not observation specific.\r\nkeyword: `truncation : int, optional`  is scalar and cannot be a nobs array\r\n`TruncatedL`, without \"F\", would allow observation specific left truncation points.\r\nAFAIR, truncation can be at any count in the implementation is (almost ?) fully supported\r\n\r\n`get_distribution` is not available, `res_h.get_distribution()` with hurdle model raises\r\n`AttributeError: 'HurdleCountModel' object has no attribute 'get_distribution'`\r\n\r\n`get influence` in hurdle model `infl_h = res_h.get_influence()` also raises exception with missing model hessian called by MLEInfluence\r\n```\r\n...\\statsmodels\\statsmodels\\base\\model.py in hessian(self, params)\r\n    344             The hessian evaluated at the parameters.\r\n    345         \"\"\"\r\n--> 346         raise NotImplementedError\r\n    347 \r\n    348     def fit(self, start_params=None, method='newton', maxiter=100,\r\n\r\nNotImplementedError:\r\n```\r\n\r\n`score_test` in hurdle model raises\r\n```\r\nlinpred = res_h.predict(which=\"linear\")\r\nres_h.score_test(exog_extra=linpred**2)\r\n...\r\nAttributeError: 'HurdleCountModel' object has no attribute 'score_factor'\r\n```\r\n\r\nchecking the zero-truncation submodel\r\n\r\n```\r\nlinpred = res_h.predict(which=\"linear\")\r\nres_t = res_h.results_count\r\nres_t.score_test(exog_extra=linpred**2)\r\n```\r\nraises with shape mismatch, I guess because the truncation model only has the nonzero observations\r\n\r\n```\r\nres_t = res_h.results_count\r\nlinpred = res_t.predict(which=\"linear\")\r\nres_t.score_test(exog_extra=linpred**2)\r\n```\r\nraises with `AttributeError: 'TruncatedLFPoisson' object has no attribute 'score_factor'`","I accidentially closed tab with open edits.\r\n\r\nshort version:\r\nscore_test, get_influence and get_distribution raise in hurdle model because of missing, not implemented model methods.\r\nSimilarly for the attached truncated model res_h.results_count. get_influence works but raises when accessing cached attributes.\r\n\r\n","missing docs, examples and unit tests: get_prediction with formula\r\n\r\nWe don't have support for creating prediction design matrices for grids (e.g. emmeans, multicolumn margins), mainly because we don't have enough information from the formula design_info, or it will be a lot of work to get the information for what is available (e.g. interaction effects).\r\nHowever, users can specify the predict exog, e.g. a grid, using the original data which is preprocessed by the formula transformation in `predict`.\r\n\r\nAFAIR, I never looked at this with an example for new get_prediction features (dmatrix with existing design_info).\r\n\r\nfor example, \r\nIt would be good to have a helper function that creates one observation for each cell of k categorical variables, including interactions defined by the formula. This could be combined with mean for continuous variables, or expanded with a grid or all observations of continuous variables. (predict_at)\r\n\r\nBut first we need to check and unit test how much is currently supported. It should work correctly by inheritance, delegation or because new features copied the working existing code.\r\n","for missing docs\/notebooks for robust cov_type\r\n\r\nhttps:\/\/stats.stackexchange.com\/a\/206557\/14187\r\n\r\nhttps:\/\/nbviewer.org\/github\/josef-pkt\/misc\/blob\/compare_discrete_glm_cluster\/notebooks\/compare_discrete_glm_gee_cluster_robust.ipynb\r\n\r\nhttps:\/\/github.com\/vgreg\/python-se\/blob\/master\/Standard%20errors%20in%20Python.ipynb\r\n","gam again, missing notebook\r\nhttps:\/\/stackoverflow.com\/questions\/75052258\/how-to-understand-the-output-of-glmgam-and-apply-the-gam-predict-to-real-case\r\n\r\n**update**\r\nbasic example is in gam.rst\r\nhttps:\/\/www.statsmodels.org\/dev\/gam.html\r\n\r\nI don't find a published notebook, the relevant gist seems to be `ex_gam_mpg_basic.ipynb`\r\nhttps:\/\/gist.github.com\/josef-pkt\/29ad2116e9af0864e5100ded89efe1f5\r\n\r\n(other gam gist notebooks look messier, i.e. include dev\/trying-out code)","VAR, vector_ar is also missing a notebook.\r\nexample code is in rst doc, but not in notebook that can be immediately used.\r\nAn old conversion to a notebook (gist) is linked to in #2272  (this version from 2015 is most likely outdated)","bump for oneway: still no notebook\r\n\r\nalso docstring is stingy on references, e.g. no reference for Yuen trimmed version\r\nAFAIR, I also used articles that compare anova versions - references for that ?\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.oneway.anova_oneway.html#statsmodels.stats.oneway.anova_oneway\r\n\r\nalso other functions\r\ne.g. equivalence_oneway has no references, it only mentions Wellek's Anova\r\n\r\nmerged PR is #6789 (rebased version)\r\nnot many references in this and precursor PRs\r\nsuperseded PR #6526 has links to other packages, used for unit tests\r\n\r\n\r\n"],"labels":["comp-docs","prio-elev"]},{"title":"MixedLM Predict issue","body":"Hi,\r\n\r\nI am doing some research on Mixed effects models and using Stats Mixedlm for same. I have Units to be predicted from Price and size at different UPC levels.\r\n\r\nSo below is the code I used - \r\n\r\nmd = smf.mixedlm(\"Units ~ Avg_Units_Price + C_CUSTOM_BASE_SIZE\", file3, groups=file3[\"UPC_10_digi\"], re_formula=\"~Avg_Units_Price + C_CUSTOM_BASE_SIZE\")\r\nmdf = md.fit()\r\nprint(mdf.summary())\r\n\r\n## Output\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/82749367\/182370799-9ce53179-3bfa-4258-8ce2-423b734de531.png)\r\n\r\nmdf.random_effects\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/82749367\/182370934-63aaeddb-15cb-4171-ab1a-0eac9c8ba0a5.png)\r\n\r\nI am getting moderate avp fit on predicted values however I have below needs - \r\n\r\n1) I want to predict Units using both fixed and random effects as in below case - \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/82749367\/182371361-663d832c-7e55-49ed-990c-551ed7285ded.png)\r\n\r\n\r\n- so I want to predict units at size 20 and Price 11.1. I know for predicting these values both fixed effects and random effects would be used, however I am not able to use predict function as I couldnt get clear example\/idea for this.\r\nI have used below code for prediction but its showing multiple arguments error -\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/82749367\/182371874-69db7c35-fede-41c0-bb18-9f01456ae654.png)\r\n\r\n\r\nCan you please help me in this single case prediction using Mixedlm?\r\n\r\nThanks a lot in advance.  ","comments":["There is no convenience method for this.  You would need to take the BLUPs (from the random_effects attribute as above) and add them to the fitted values from the fixed effects (which is what you get from 'predict').  For example, for all observations in group 7023001154 you would take the fixed effects prediction and add to it `-327.6 + 66*Avg_Units_Price - 72*C_CUSTOM_BASE_SIZE`.","One way to get the random effect is through the `predict` function in Linear Mixed Effects Model `md`.\r\n\r\n`md.predict(mdf.random_effects[group_a1_df_2[0]], exog=a_df)`\r\nwould get you the random effect prediction.\r\n\r\n`model.predict(mdf.fe_params, exog=a_df)`\r\nwould get you the fixed effect prediction.\r\n\r\nYou probably still need to add them together after the calculation. Got this from https:\/\/stackoverflow.com\/questions\/50300224\/statsmodels-mixed-linear-model-predictions. It works for me. But I have to do some preprocessing for exog as it does not take my pandas dataframe directly for categorical values."],"labels":["type-enh","comp-regression"]},{"title":"Nested ANOVA","body":"#### The issue\r\n\r\nWhen trying to compute two-way nested ANOVA, the results do not equal the appropriate results from R (formulas and data are the same). \r\n\r\n\r\n####  Sample\r\n\r\nWe use \"atherosclerosis\" dataset from here: https:\/\/stepik.org\/media\/attachments\/lesson\/9250\/atherosclerosis.csv.\r\nTo get nested data we replace dose values for age == 2:\r\n```python\r\ndf['dose'] = np.where((df['age']==2) & (df['dose']=='D1'),'D3', df.dose)\r\ndf['dose'] = np.where((df['age']==2) & (df['dose']=='D2'),'D4', df.dose)\r\n```\r\n\r\nSo we have dose factor nested into age: values D1 and D2 are in first age and values D3 and D4 are only in the 2nd age.\r\nAfter getting ANOVA table we have the results below:\r\n```python\r\nmod = ols('expr~age\/C(dose)', data=df).fit()\r\nanova_table = sm.stats.anova_lm(mod, typ=1); anova_table\r\n```\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2022-07-01 15-13-30](https:\/\/user-images.githubusercontent.com\/108528316\/176892226-2a412afa-3733-460e-8721-ce7b5f09872a.png)\r\n\r\nThe total sum of the 'sum_sg' = 1590.257424 + 47.039636 + 197.452754 = 1834.7498139999998 that is NOT equal the right total sum  (computed below) = 1805.5494956433238\r\n```python\r\ngrand_mean = df['expr'].mean()\r\nssq_t = sum((df.expr - grand_mean)**2)\r\n```\r\n\r\n#### Expected Output\r\n\r\nLet's try to get ANOVA table in R:\r\n\r\n```python\r\ndf <- read.csv(file = \"\/mnt\/storage\/users\/kseniya\/platform-adpkd-mrwda-aim-imaging\/mrwda_training\/data_samples\/athero_new.csv\") \r\nnest <- aov(df$expr ~ df$age \/ factor(df$dose))\r\nprint(summary(nest))\r\n```\r\nThe results:\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2022-07-01 15-22-44](https:\/\/user-images.githubusercontent.com\/108528316\/176893579-af796a0f-1c3a-4c49-93a7-91850da342a3.png)\r\n\r\nWhy they are not equal? The formulas are the same. Are there any mistakes in computing ANOVA through statsmodels?\r\n\r\nThe results from R seem to be right, because the total sum 197.5 + 17.8 + 1590.3 = 1805.6 is equal to the total sum computed manually.\r\n","comments":["I never looked at nested effects in details. So I don't know what's going on.\r\n\r\nstrange is that df =3 in statsmodels, and df=2 in R aov.\r\n\r\nMy main guess is that patsy encodes the nested effect differently than R. (A check would be to compare OLS with R lm.)\r\nThe second possibility is that statsmodels anova_lm does not recover type 1 anova correctly in this case. \r\n(anova_lm backs out the terms for different types from the full model estimate. Using type 1 sequential models in anova_lm could be used to verify type 1 anova.  That's possible in this case, the full model will not have SS decomposition because of the correlation of regressors, I guess.)\r\n\r\nmaybe bug, maybe different specification\r\n ","I found out how to get right results using statsmodels in the example. We should: \r\n\r\n1) use same values of the nested factor within every level of the main factor (in my case only D1 and D2 values in the dose field). \r\n\r\n2) note that age factor as a categorical one, so use C(age) in the formula: C(age)\/C(dose) or even C(age)\/dose\r\n\r\n```\r\nmod = ols('expr~C(age) + C(age):C(dose)', data=data_new).fit()\r\n\r\nanova_table = sm.stats.anova_lm(mod, typ=1); anova_table\r\n```\r\n\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2022-11-10 13-15-13](https:\/\/user-images.githubusercontent.com\/108528316\/201064333-8d0fb6c1-a863-47bd-a91d-cc4e72d6d148.png)\r\n\r\nOnly both conditions applied can give the right results (that are totally the same R gives)","very good, thanks for the solution\r\n\r\nThis was my guess in #8506\r\n\r\nHowever, I guess the will still be zero columns if not all nested categories have the same number of levels.\r\nFor example if number of dose levels differs by age.\r\n"],"labels":["type-bug","comp-regression","comp-stats"]},{"title":"FAQ\/SUMM: Overview of hypothesis tests and related inference (stats)","body":"(I don't have an overview and I didn't find an overview issue)\r\n\r\nWhat functions do we have for hypothesis tests and related inference outside of models? \r\nWhat functionality should we still add?\r\nThere is some overlap with diagnostic and specification tests in post-estimation. #7894\r\nSome overlap with inference in models, that are equivalent or close to standalone inference functions #8145 (issue still empty)\r\n\r\nCurrently the best overview is the rst doc for `stats`.\r\nWe have several large overview issues for proportion and rates, but not much that provides an overview of all different cases.\r\nWe also have some overview issues for specific kind of analysis, for example equivalence testing #6497 and power functions #8162.\r\n\r\n","comments":["try a compact overview:\r\n\r\n- mean, continuous asymptotic,  (zt): \r\n  - 1 sample: weightstats, scipy \r\n  - 2 sample: weightstats, scipy \r\n  - k sample: oneway, meta-analysis, scipy\r\n  - k sample multiple comparisons: multicomp\r\n  - model: OLS, GLM\r\n- variance\r\n  - 1 sample: PR\r\n  - 2 sample: ?, scipy\r\n  - k sample: oneway, scipy\r\n- correlation\r\n  - scipy, nothing in statsmodels\r\n- nonparametric \r\n  - 1 sample: ?\r\n  - 2 sample: ?\r\n  - k sample:\r\n- mean multivariate\r\n  - ? \r\n- cov, corr (multivariate)\r\n  - ?\r\n- proportion\r\n  - 1 sample: proportion, needs refactoring\r\n  - 2 sample: proportion, contingency tables, scipy\r\n  - k sample: contingency tables, oneway (normal based)\r\n  - model: Logit, Probit, GLM, GEE\r\n- rates (Poisson)\r\n  - 1 sample: rates\r\n  - 2 sample: rates\r\n  - k sample:\r\n  - model: GLM, count models (Poisson, NegativeBinomial, ZI, ...)\r\n- choice, multinomial\r\n  - 1 sample:\r\n  - 2 sample:\r\n  - k sample:\r\n  - model: MNLogit, OrderedModel, GEE\r\n- ...\r\n  - 1 sample:\r\n  - 2 sample:\r\n  - k sample:\r\n  - model:\r\n- ..."],"labels":["comp-docs","comp-stats","FAQ","topic-post_estim"]},{"title":"ENH: Functions for summarizing multiple model fitting results.","body":"- [ ] closes (None)\r\n- [ ] tests (not tested)\r\n- [x] code\/documentation is well formatted.  \r\n- [x] properly formatted commit message. See \r\n      [NumPy's guide](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/dev\/gitwash\/development_workflow.html#writing-the-commit-message). \r\n\r\n\r\nAlthough there is a question about how to handle multiple summaries with `summary_col`. \r\n[Python: Do not show dummies in statsmodels summary](https:\/\/stackoverflow.com\/questions\/49471014\/python-do-not-show-dummies-in-statsmodels-summary)\r\n\r\nHowever, I thought more featured functions to achieve easy functions of handling with multiple fitting results were needed, and I tried to implement simple one with just horizontal concatenations, and one version which lets a mosaic variable to control layout of outputs. \r\n\r\nSee the link below for practical usage. \r\nhttps:\/\/github.com\/toshiakiasakura\/statsmodels\/blob\/summary_multi\/examples\/notebooks\/summary_multi_usage.ipynb\r\n\r\nIf there is a further suggestion, improvement or need to open a issue, please tell me. ","comments":["Hello @toshiakiasakura! Thanks for updating this PR. We checked the lines you've touched for [PEP\u00a08](https:\/\/www.python.org\/dev\/peps\/pep-0008) issues, and found:\n\n\n\n\n\n\n\n\n\n\n\nThere are currently no PEP 8 issues detected in this Pull Request. Cheers! :beers: \n\n##### Comment last updated at 2022-06-29 11:01:43 UTC","@toshiakiasakura  \r\nThanks for the PR\r\n\r\nI might not get around looking at this very soon.\r\nI'm setting up a new notebook with dev environment and I will soon go on vacation and travelling.\r\n(I'm trying to get Window 11 to behave like my old Windows 7 or 8)","Currently, I'm working on your code reviews, and I happened to remember to ask you how to add attributes...\r\nWhan I run the following code, I obtain the result as below.\r\n```python\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\ndf = sm.datasets.anes96.load_pandas().data\r\nres1 = smf.glm(formula=\"vote ~ logpopul\" ,data=df, family = sm.families.Binomial()).fit()\r\nres2 = sm.add_params_summary(res1)\r\nprint(res2.params_summary.to_string())\r\n```\r\n```\r\n               coef   std err         t         P>|t|    [0.025    0.975]                      str\r\nIntercept -0.073026  0.083061 -0.879178  3.793048e-01 -0.235823  0.089772   -0.073 (-0.236, 0.090)\r\nlogpopul  -0.110967  0.021216 -5.230224  1.693043e-07 -0.152550 -0.069383  -0.111 (-0.153, -0.069)\r\n```\r\n\r\nHowever, when I run `dir(res2)`, `.params_summary` does not appear. \r\nI want to let `.params_summary` can be found from dir method.\r\nI can not figure out which should be modified. ","I'm sorry to rush you, but what's going on for this pull request?  \r\nAre there some problems, or should I improve my pull request? ","I just came back from my vacation and travelling. I will try to look at it during the next few days.\r\n\r\nAPI decisions are difficult and I need to try this out to see how it works.","These look like very useful functions, thanks for this PR!"],"labels":["type-enh","comp-base","comp-io","prio-elev"]},{"title":"DOC\/REF\/ENH: follow-up to merging treatment effect","body":"I merged treatment effect #8034 essentially in the version where I stopped 4 months ago\r\n\r\nDocs still need a lot of work, and we need a notebook (unit test has full example).\r\nSimple example in class docstring would also be helpful.\r\nThe current docs are not very informative, no explanations and details (users would have to read the Stata docs)\r\n`TreatmentEffectResults` class has no docstring.\r\n\r\nThe docs don't indicate that only OLS is supported as outcome model.\r\nNeed to warn or raise for current limitations.\r\ncurrent unit tests are OLS + Probit\r\nI guess GLM-Binomial is not supported for treatment model, only Logit and Probit. ???\r\n\r\nfrom_data method is not implemented\r\n\r\n\r\n**update**\r\n\r\nsignature of treatment class might not be good\r\n`__init__(self, model, treatment, results_select=None, _cov_type=\"HC0\",\r\n                 **kwds):`\r\n\r\nwhy is results select_optional?\r\nwhy is results_select after treatment?\r\ndesign\/signature if only one model is used,  ipw and ra?\r\n\r\nwe always need the outcome and the treatment variable, \r\noutcome variable is only in outcome `model`, \r\ntreatment indicator is `treatment` argument but also in results_select if that is provided\r\n\r\nsignature might be a left over of the original implementation for ra\r\n\r\n\r\n\r\n","comments":["my notebook `ex_treatment_effect.ipynb` looks pretty clean\r\nseems to be the version I wrote for the docs, same case as in unit test\r\n\r\n(my other notebooks are dev notebooks that also include earlier versions of the classes)\r\n\r\n**update**\r\n\r\nAn earlier version of the notebook is in a gist announced in the mailing list\r\nhttps:\/\/groups.google.com\/g\/pystatsmodels\/c\/KHhtyZgv74k\r\nhttps:\/\/gist.github.com\/josef-pkt\/8a8755d2930cb23f7282cf683232bf17\r\n","need to collect possible (smaller) improvements, e.g.\r\n\r\n- GMM uses generic param names, e.g. `print(res.results_gmm.summary())` is not informative about what the params are.\r\n   this will need some multi-model naming convention \r\n- ...","check and unit test other options, or maybe warn\r\nFor example:\r\nLogit and Probit now have `offset`. I have not looked at the offset case in treatment effect given that it was added after my main treatment effect work.\r\nWhen we support GLM, then we also need to check or disallow other options like freq_ or var_weights","a tutorial with several treatment effects methods\r\nalso with G-formula (which I was\/am not clear about based on skimming part of the literature a while ago)\r\n\r\nSmith, Matthew J., Camille Maringe, Bernard Rachet, Mohammad A. Mansournia, Paul N. Zivich, Stephen R. Cole, and Miguel Angel Luque-Fernandez. \u201cTutorial: Introduction to Computational Causal Inference Using Reproducible Stata, R and Python Code.\u201d arXiv, December 21, 2020. https:\/\/doi.org\/10.48550\/arXiv.2012.09920.\r\n\r\nhttps:\/\/github.com\/migariane\/TutorialCausalInferenceEstimators\/tree\/master\r\n\r\nThere is already the published version (that I downloaded half a year ago)\r\n\r\nSmith, Matthew J., Mohammad A. Mansournia, Camille Maringe, Paul N. Zivich, Stephen R. Cole, Cl\u00e9mence Leyrat, Aur\u00e9lien Belot, Bernard Rachet, and Miguel A. Luque-Fernandez. \u201cIntroduction to Computational Causal Inference Using Reproducible Stata, R, and Python Code: A Tutorial.\u201d Statistics in Medicine 41, no. 2 (2022): 407\u201332. https:\/\/doi.org\/10.1002\/sim.9234.\r\n\r\nrelated\r\nG-formula for \"panel\" data, i.e. repeated observations with different treatment histories\r\n\r\nZivich, Paul N., Rachael K. Ross, Bonnie E. Shook-Sa, Stephen R. Cole, and Jessie K. Edwards. \u201cEmpirical Sandwich Variance Estimator for Iterated Conditional Expectation G-Computation.\u201d arXiv, June 19, 2023. https:\/\/doi.org\/10.48550\/arXiv.2306.10976.\r\n\r\n"],"labels":["type-enh","comp-docs","comp-treatment"]},{"title":"statsmodels.distributions.edgeworth.ExpandedNormal - 4th cumulant higher than 4","body":"In line 166 of edgeworth.py  is checked if the imag-part is zero and if abs(r) is smaller than 4. If I choose a fourth cumulant higher than 4 I get this warning and yes in this case the imag-parts are zero and abs(r) is smaller 4. But: where does this limit of 4 comes from? \r\nIs it based on a paper? \r\nBy the way: I assume that the first cumulant is zero and the second is 1. (scaling, centering in line 163,164) I use statsmodels 0.13.2","comments":["original PR by @ev-br is #1325\r\n\r\nI haven't looked at this in a long time, and don't remember details.\r\nMaybe Evgeni found the condition and bound in one of the cited articles.\r\n\r\nThe problem is that the simple orthogonal polynomials can have negative pdf intervals and nonmonotonic cdf if the distribution is too far away from the base distributions.\r\nI had looked at several methods to \"fix\" those negative regions, but they all require numerical integration to compute or adjust the integration constant, AFAIR. This removes the main advantage of orthogonal polynomials which are simple to compute.\r\n\r\n\r\n> By the way: I assume that the first cumulant is zero and the second is 1. (scaling, centering in line 163,164)\r\n\r\nThe first to cumulants are mean and variance\r\nline 190 :      `mu, sigma = cum[0], np.sqrt(cum[1])`\r\n\r\nbut the polynomials are computed in terms of the standardized distribution, loc=0, scale=1.\r\n\r\n\r\n","(semi-random idea)\r\n\r\nIt should be possible to use a orthogonal polynomial distribution approximation after a nonlinear transformation that brings the distribution closer to the normal or other base distribution.\r\nPR #7246\r\n\r\nI'm not sure how this would work out, some of those flexible transformation distributions already have a large number of parameters, and we would mainly need a transformation that reduces skewness and\/or kurtosis. We would loose the simple parameterization in terms of cumulants.\r\nThere is possibly a problem: we don't have a simple way to compute ppf of the orthogonal polynomial expansion distribution and we might need it for to apply additional transformations.\r\n","Thanks for your comments. I had a look at the references in edgeworth.py, but I did not find this bound. I'm trying to generate slightly degenerated normal distributions and I compare the two methods ExpandedNormal and pdf_moments. If I generate some samples from the distributions, especially from the one made with ExpandedNormal and a high fourth cumulant, I get two different results one with rvs-sampling and one with itsample package. The latter does not show the expected tails of the distribution while the former does. So I try to investigate what is the reason for this and it came out that if the fourth cumulant is not as high (below 4, sometimes 5) both methods produce similiar samples. I'm wondering that the two methods can give different results (beside the PRNG) because both use inverse cdf tool (line 1008 in _distn_infrastructure.py). Is it possible that this observation is interrelated to the negative pdf?","That's possible or likely, if the cdf is not monotonic, then it will mess up the ppf and inverse cdf random sampling.\r\n\r\nIn the mailing list thread linked to in the original PR, Evgeni mentions negative pdf regions in the tail for the Edgeworth expansion.\r\nI was working mainly on Gram-Charlier expansion, and there the negative regions where just outside the \"shoulders\", i.e. when pdf became small after the initial peak area, in the examples that I looked at.\r\n\r\nYou can check by computing the pdf on a grid in the possibly affected region.","> You can check by computing the pdf on a grid in the possibly affected  region.<\r\n\r\nYou mean to vary the value of the cumulants and check the different methods, similiar to a Monte Carlo?","I don't remember details, sorry. The git history of my sandbox repo (github remembers!) shows a separate commit, but not much else. It's possible that the threshold is from some playing around for a couple of distrubtions. \r\n\r\nhttps:\/\/github.com\/ev-br\/edgeworth\/commits\/master","@cube2022 \r\nno, compute the pdf for different values of x for given cumulants with a 4th cumulant like 5.\r\ni.e. compute and plot the pdf(x) function for a parameter that might have a negative valued region.\r\n"],"labels":["comp-distributions"]},{"title":"REF: deprecate lower case link classes, duplicates that differ only in capitalization","body":"We have many links with lower case names that are duplicate, aliases of link classes with capitalized class names.\r\n\r\nWe should deprecate them with longer deprecation cycle given that both types of names are commonly used.\r\nRemoving names that only differ in capitalization, will remove many doc build warnings.\r\n\r\nProblem is that user provides a link instance, so we cannot just switch link names internally in GLM, GEE.\r\nWe need to figure out where to put the deprecation warning. (I haven't checked yet.\r\n","comments":[],"labels":["comp-genmod","type-refactor","backwards-incompat"]},{"title":"FAQ-D: doc build, sphinx warnings","body":"(notes on doc build, sphinx warnings, put here to make it easier to find)\r\n\r\n### section title in properties of classes\r\n\r\nexample reference section in a cached property\r\n`...\/statsmodels\/miscmodels\/ordinal_model.py:docstring of statsmodels.miscmodels.ordinal_model.OrderedResults:81: CRITICAL: Unexpected section title.`\r\n\r\ncan be ignored, we have many of those\r\nsections in docstrings of properties are removed in the class docstring, but rendered correctly in docstring of property\r\n\r\n### duplicate label, names differ only in capitalization\r\n\r\nexample: links have many duplicates with capitalized and lower case names, e.g.\r\n`\/statsmodels\/docs\/source\/generated\/statsmodels.genmod.families.links.cloglog.rst:2: WARNING: duplicate label generated\/statsmodels.genmod.families.links.cloglog:statsmodels.genmod.families.links.cloglog, other instance in \/home\/runner\/work\/statsmodels\/statsmodels\/docs\/source\/generated\/statsmodels.genmod.families.links.CLogLog.rst`\r\n\r\ncan be ignored in docbuild, \r\nbut try to avoid public, documented names of objects that only differ in capitalization\r\n\r\n","comments":["The property bug is annoying. "],"labels":["comp-docs","FAQ"]},{"title":"DOC\/REF followup to 8166, refactoring and enhancing poisson stats","body":"rates.rst wrong function names\r\n\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.rates.test_poisson_2indep.html\r\n- [x] `alternative` description is only for `ratio`\r\n\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.rates.tost_poisson_2indep.html\r\n- [x] docstring not updated for \"compare\" option, i.e. support for \"diff\"\r\n- [x] same for nonequivalence https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.rates.nonequivalence_poisson_2indep.html\r\n\r\nhttps:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.stats.rates.power_poisson_diff_2indep.html\r\nrendering of References uses extra lines\r\n\r\nmethod = \"auto\"  check defaults and their description again\r\n","comments":["for consistency in naming, we should rename (we use tost in names for basic stats instead of equivalence)\r\n`power_equivalence_poisson_2indep` -> `power_tost_poisson_2indep`\r\n\r\nin future we might have\r\n`power_confint_poisson_2indep`","nonequivalence does not bound pvalue to be <=1\r\n\r\n```\r\ncount1, n1, count2, n2 = 2000, 10000, 4200, 20000\r\nlow = 0.95\r\nupp = 1 \/ low\r\nnonequivalence_poisson_2indep(count1, n1, count2, n2, low, upp, method='score', compare='ratio').pvalue\r\n1.0734096768274033\r\n```\r\n\r\nrelated:\r\nIt shouldn't be difficult to add the ump p-value to the results instance of nonequivalence_poisson_2indep.\r\nIIUC\/IIRC, we just need to add the prob of the \"other\" tail instead of 2 * min(pvalues)\r\n**correction** no, it will not be easy because nonequivalence just delegates to the one sided test and we don't get any information about the \"other\" tail. \r\n","another one:\r\n\r\ncurrently we assume in the 2-indep case that both samples come from the same distribution and in the case of NegBin or quasi-poisson that the dispersion coefficient is the same.\r\n\r\nrelated \r\n- brunner-munzel versus manwhiteney-wilcoxon, the former allows comparison of samples from different distributions\r\n- modelling heteroscedasticity\r\n\r\ne.g. in the 2-indep test, we could allow for different dispersion, e.g. Poisson versus Negbin samples, or 2 negin with different dispersion\r\n\r\nThis can currently already be handled with HC cov_type in Poisson or Negbin. but we don't adjust params for it.\r\nInteresting would be a MonteCarlo to compare small sample properties versus robustness in `stats` versus `discrete` models\r\n\r\n\r\n\r\n"],"labels":["comp-docs","comp-stats","type-refactor"]},{"title":"Run VARMAX with conditinal MLE","body":"Hi everyone, \r\n\r\nis it possible to **run the statsmodels VARMAX function with a conditinal MLE** like in the statsmodels VAR function? The approximate solutions of the VAR are sufficient for my case and the VARMAX has a very long runtime. \r\nAfterwards, I need to get the impulse response function of the VAR with impulses different from the identity matrix which is the reason why I cannot simply use the statsmodels VAR function. \r\n\r\nThank you a lot in advance!!","comments":["There's no way to do this automatically right now, but it is a good idea. Still, we don't have anyone actively working on VARs right now, so I don't have a timeline."],"labels":["type-enh","comp-tsa-statespace"]},{"title":"ENH: add get_prediction for RLM","body":"there is no get_prediction for RLM yet\r\nhttps:\/\/stats.stackexchange.com\/questions\/578398\/getting-confidence-interval-for-prediction-from-statsmodel-robust-linear-model\r\n\r\ngiven that prediction is a linear function we can use the standard method for the confidence interval for predicted mean, conditional expectation.\r\n\r\nNot clear whether we want to assume normality to get a prediction interval (for new endog) as in OLS.\r\n\r\n","comments":["I'm interested in this functionality too. If you can point me in the direction of what method to implement, I'd be happy to give it a shot and submit a pull request.","The main work here is checking what other packages or articles\/textbooks are doing and writing unit tests.\r\n\r\nWe have several get_prediction functions and classes that can be reused, most likely something like get_prediction in OLS.\r\n\r\nIn cases like this it's often a few hours writing the code, and several days getting the unit tests and deciding on api or methods.\r\n\r\nI have not looked yet at what R, Stata or SAS are doing for their RLM equivalent, so I cannot tell yet what needs to be done.","SAS does the same as in OLS\r\nhttps:\/\/documentation.sas.com\/doc\/en\/pgmsascdc\/9.4_3.4\/statug\/statug_rreg_syntax11.htm\r\n\r\n","Thanks for the links here and in the linked issues.","Warning: robust norms allow for complex evaluation (mostly?) however the np.abs usage makes complex step derivatives incorrect.\r\nhttps:\/\/github.com\/statsmodels\/statsmodels\/pull\/8801#issuecomment-1509919138\r\n\r\ni.e. inheriting usage in nonlinear deltacov `_wald_nonlinear` will likely be wrong.\r\n(trying out cs derivatives will or might not raise, but return incorrect derivatives instead)"],"labels":["type-enh","comp-robust","prio-elev"]},{"title":"ENH: power for poisson 2indep, more options, e.g. diff with nonzero value, Gu and others","body":"#8251 similar issue for proportion\r\nfollowup to #8166 \r\n#8138 general issue for rates, poisson\r\n\r\nThe power functions in PR #8166 do not cover `compare` and methods systematically.\r\nIt currently includes Zhu and Lakkis for ratio, and basic equality test for diff.\r\nAlso there is no support yet for different exposures in the two samples. Zhu and Lakkis use average exposure for power and sample size computation.\r\n(more important for NegBin than Poisson)\r\n\r\nBasic extension as in #8251 is to add value to power for diff.\r\ne.g. Stucke and Kieser 2013\r\nwhich just looks like an extension of the current power-diff function.\r\n\r\none-tail sample size function equivalent of samplesize_proportions_2indep_onetail are also missing\r\ne.g. Stucke, Kieser equ (5) and (6)\r\n\r\nOthers:\r\nPASS also has power function for tests by Gu et al for ratio which would match test_poisson_2indep more closely.\r\n\r\nrelated:\r\n#8166 also does not include power function for 1-sample case\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"FAQ: NegativeBinomial regression's default uses quassi method? Reason for differences with R's glm.nb.","body":"I recently tried to use negative binomial regression, but I found the result was not matched with R' mass glm.nb, and the same report was raised also in the StackOverflow. In my case, standard deviation was largely differed.\r\n- [Python Negative Binomial Regression - Results Don't Match those from R](https:\/\/stackoverflow.com\/questions\/42277532\/python-negative-binomial-regression-results-dont-match-those-from-r)\r\n\r\nAfter some investigations, I found the results can be matched if I set cov_type=\"HC0\". \r\nand so, my question is that which is corresponding to quasi-likelihood method? or right implementation? or what causes this difference?  \r\nSince in the case of poisson regression, if over-dispersion is taken into consideration, cov_type =\"HC0\" is specified. \r\nHowever, in the case of negative binomial regression, the estimated results are matched when cov_type is specified. \r\n\r\nHere is the code to reproduce my results. Point estimates are similar but standard deviations are different.\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\ndata = sm.datasets.get_rdataset(\"warpbreaks\", \"datasets\").data\r\nmodel = smf.glm(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data, family=sm.families.NegativeBinomial() ).fit()\r\ndisplay(model.summary())\r\nmodel = smf.glm(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data, family=sm.families.NegativeBinomial() ).fit(cov_type='HC0')\r\ndisplay(model.summary())\r\n```\r\n\r\n```\r\n  Generalized Linear Model Regression Results                  \r\n==============================================================================\r\nDep. Variable:                 breaks   No. Observations:                   54\r\nModel:                            GLM   Df Residuals:                       50\r\nModel Family:        NegativeBinomial   Df Model:                            3\r\nLink Function:                    Log   Scale:                          1.0000\r\nMethod:                          IRLS   Log-Likelihood:                -233.75\r\nDate:                Tue, 07 Jun 2022   Deviance:                       7.1640\r\nTime:                        02:25:38   Pearson chi2:                     7.05\r\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.05123\r\nCovariance Type:            nonrobust                                         \r\n===================================================================================================\r\n                                      coef    std err          z      P>|z|      [0.025      0.975]\r\n---------------------------------------------------------------------------------------------------\r\nIntercept                           3.6693      0.276     13.290      0.000       3.128       4.210\r\nwool[T.B]                          -0.1823      0.277     -0.658      0.511      -0.726       0.361\r\nC(tension, Treatment('L'))[T.H]    -0.5102      0.340     -1.503      0.133      -1.176       0.155\r\nC(tension, Treatment('L'))[T.M]    -0.2934      0.339     -0.866      0.386      -0.957       0.371\r\n===================================================================================================\r\n                 Generalized Linear Model Regression Results                  \r\n==============================================================================\r\nDep. Variable:                 breaks   No. Observations:                   54\r\nModel:                            GLM   Df Residuals:                       50\r\nModel Family:        NegativeBinomial   Df Model:                            3\r\nLink Function:                    Log   Scale:                          1.0000\r\nMethod:                          IRLS   Log-Likelihood:                -233.75\r\nDate:                Tue, 07 Jun 2022   Deviance:                       7.1640\r\nTime:                        02:25:38   Pearson chi2:                     7.05\r\nNo. Iterations:                     6   Pseudo R-squ. (CS):            0.05123\r\nCovariance Type:                  HC0                                         \r\n===================================================================================================\r\n                                      coef    std err          z      P>|z|      [0.025      0.975]\r\n---------------------------------------------------------------------------------------------------\r\nIntercept                           3.6693      0.107     34.217      0.000       3.459       3.879\r\nwool[T.B]                          -0.1823      0.099     -1.840      0.066      -0.377       0.012\r\nC(tension, Treatment('L'))[T.H]    -0.5102      0.122     -4.168      0.000      -0.750      -0.270\r\nC(tension, Treatment('L'))[T.M]    -0.2934      0.126     -2.320      0.020      -0.541      -0.046\r\n===================================================================================================\r\n\r\n```\r\n\r\nAnd here is the R output.\r\n```R\r\nlibrary(MASS)\r\nlibrary(datasets)\r\nwb.negbin <- glm.nb(breaks~wool+tension, data=warpbreaks)\r\nsummary(wb.negbin)\r\n```\r\n```\r\nCall:\r\nglm.nb(formula = breaks ~ wool + tension, data = warpbreaks, \r\n    init.theta = 9.944385436, link = log)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-2.0144  -0.9319  -0.2240   0.5828   1.8220  \r\n\r\nCoefficients:\r\n            Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)   3.6734     0.0979  37.520  < 2e-16 ***\r\nwoolB        -0.1862     0.1010  -1.844   0.0651 .  \r\ntensionM     -0.2992     0.1217  -2.458   0.0140 *  \r\ntensionH     -0.5114     0.1237  -4.133 3.58e-05 ***\r\n---\r\nSignif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\r\n\r\n(Dispersion parameter for Negative Binomial(9.9444) family taken to be 1)\r\n\r\n    Null deviance: 75.464  on 53  degrees of freedom\r\nResidual deviance: 53.723  on 50  degrees of freedom\r\nAIC: 408.76\r\n\r\nNumber of Fisher Scoring iterations: 1\r\n\r\n\r\n              Theta:  9.94 \r\n          Std. Err.:  2.56 \r\n\r\n 2 x log-likelihood:  -398.764 \r\n```\r\n\r\n","comments":["I guess this is because our GLM does not estimate the dispersion parameter for negative binomial. The parameter is taken as fixed in the family using the default if not provided by the users.\r\nR glm estimates theta, but I'm not sure or don't remember what they use as estimator for theta.\r\n\r\nNote, R uses a slightly different parameterization of the NB dispersion parameter, AFAIR\r\ntheta in R is the inverse of our alpha, i.e. alpha = 1 \/ theta\r\n\r\nTwo options:\r\n\r\n- use `discrete` NegativeBinomial, which estimates the dispersion parameter by MLE, or NegativeBinomialP, which both have more options for the parameterization, NB1, NB2, NBP.\r\n- use GLM in a loop over llf, to compute profile likelihood, and find best fitting dispersion parameter.\r\n\r\nBecause we have the full MLE version in discrete NegativeBinomial, we have not put much effort in supporting dispersion parameter search in GLM.\r\nThe current options in GLM only allow for `scale` estimates, i.e. multiplicative factor for dispersion, but not parameter of dispersion function as needed in NB.\r\n ","about cov_type:\r\n\r\n`cov_type=\"HC0\"` does not reproduce the same results except in some special cases. You see small numerical differences in your example compared to R, that are too large to be just the results of optimization and convergence tolerance.\r\n\r\nHC cov_type does not affect the parameter estimates but corrects standard errors for general heteroscedasticity, i.e. has corrected standard errors even if the variance function is not the one of negative binomial family.\r\n\r\nThe standard \"quasi\" in GLM, as implemented in R, only allows for excess dispersion, i.e. a `scale` estimate different from 1, but not arbitrary heteroscedasticity.\r\n\r\nThe dispersion parameter in the negative binomial GLM family also affects the parameter estimates because the implied weights in GLM optimization are different from those implied by the variance function with default dispersion parameter.\r\n","Thank you for your reply. \r\nI am very happy with almost same results of R's glm.nb using the following code. (Just for my memo, or other people.)\r\n```python\r\nfrom patsy import dmatrix\r\nimport statsmodels.api as sm\r\n\r\ndata = sm.datasets.get_rdataset(\"warpbreaks\", \"datasets\").data\r\nX = dmatrix(\"~ wool + C(tension, Treatment('L'))\", data=data, return_type=\"dataframe\")\r\ny = data[\"breaks\"]\r\nmodel = sm.NegativeBinomial(y, X).fit()\r\nmodel.summary()\r\n```\r\nor using formula\r\n```python\r\nimport statsmodels.formula.api as smf\r\nimport statsmodels.api as sm\r\ndata = sm.datasets.get_rdataset(\"warpbreaks\", \"datasets\").data\r\nmodel = smf.negativebinomial(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  ).fit()\r\nmodel.summary()\r\n```\r\n\r\n```\r\n                     NegativeBinomial Regression Results                      \r\n==============================================================================\r\nDep. Variable:                 breaks   No. Observations:                   54\r\nModel:               NegativeBinomial   Df Residuals:                       50\r\nMethod:                           MLE   Df Model:                            3\r\nDate:                Fri, 10 Jun 2022   Pseudo R-squ.:                 0.04391\r\nTime:                        07:36:08   Log-Likelihood:                -199.38\r\nconverged:                       True   LL-Null:                       -208.54\r\nCovariance Type:            nonrobust   LLR p-value:                 0.0003792\r\n===================================================================================================\r\n                                      coef    std err          z      P>|z|      [0.025      0.975]\r\n---------------------------------------------------------------------------------------------------\r\nIntercept                           3.6734      0.095     38.516      0.000       3.486       3.860\r\nwool[T.B]                          -0.1862      0.101     -1.835      0.066      -0.385       0.013\r\nC(tension, Treatment('L'))[T.H]    -0.5114      0.124     -4.134      0.000      -0.754      -0.269\r\nC(tension, Treatment('L'))[T.M]    -0.2992      0.122     -2.446      0.014      -0.539      -0.059\r\nalpha                               0.1006      0.026      3.882      0.000       0.050       0.151\r\n===================================================================================================\r\n```\r\n\r\nHowever, as your suggestion, I tried to fit the data with `sm.NegativeBinomialP\r\n```python\r\nfrom patsy import dmatrix\r\nimport statsmodels.api as sm\r\n\r\ndata = sm.datasets.get_rdataset(\"warpbreaks\", \"datasets\").data\r\nX = dmatrix(\"~ wool + C(tension, Treatment('L'))\", data=data, return_type=\"dataframe\")\r\ny = data[\"breaks\"]\r\nmodel = sm.NegativeBinomialP(y, X, p =2).fit()\r\nmodel.summary()\r\n```\r\nit produced a lot errors (parts of it) and converge failed. (the following is the part of error code.) \r\n```\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/statsmodels\/discrete\/discrete_model.py:3122: RuntimeWarning: invalid value encountered in log\r\n  a1 * np.log(a1) + y * np.log(mu) -\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/statsmodels\/discrete\/discrete_model.py:3123: RuntimeWarning: invalid value encountered in log\r\n  (y + a1) * np.log(a2))\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/statsmodels\/discrete\/discrete_model.py:3159: RuntimeWarning: invalid value encountered in log\r\n  dgterm = dgpart + np.log(a1 \/ a2) + 1 - a3 \/ a2\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/statsmodels\/discrete\/discrete_model.py:3439: RuntimeWarning: overflow encountered in exp\r\n  return np.exp(linpred)\r\n```\r\n\r\nAlso, I want to fit the model with a consistent expression, like \r\n```\r\nmodel = smf.glm(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  family=sm.NegativeBinomial() ).fit()\r\n```\r\nbut, it raised the following error.\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [12], in <cell line: 1>()\r\n----> 1 model = smf.negativebinomial(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  family=sm.NegativeBinomial()).fit()\r\n\r\nTypeError: __init__() missing 2 required positional arguments: 'endog' and 'exog'\r\n```\r\nMaybe it is currently in progress to allow this expression of implementation?\r\n\r\nIn addition, I am afraid that a lot of users would mistakenly use negative binomial regression like me, not discrete version... To avoid this problem, some cautions should be raised when using `sm.families.NegativeBinomial`...","Thanks to coming back with the examples.\r\n\r\nI will look into the convergence failure for NegativeBinomialP. Most likely it doesn't have enough checks that we have positive dispersion coefficient during optimization.\r\n\r\nYour last exception is not clear to me:\r\n\r\n> model = smf.negativebinomial(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  family=sm.NegativeBinomial()).fit()\r\n\r\nThis should raise a ValueError or TypeError because of the invalid \"family\" keyword argument (but maybe that's in only in the dev version.) The discrete classes are all for specific distributions and don't need a family argument.\r\nThe TypeError that you get is strange and I need to check that.\r\n\r\nWithout \"family\" keyword, smf.negativebinomial worked in your earlier example.\r\n\r\n","Sorry I mis-pasted the code error. \r\nI hope to access discrete negative binomial with this syntax. (consistent with `smf.glm`).\r\n```python\r\nmodel = smf.glm(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  family=sm.NegativeBinomial()).fit()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nInput In [4], in <cell line: 1>()\r\n----> 1 model = smf.glm(\"breaks ~ wool + C(tension, Treatment('L'))\", data=data,  family=sm.NegativeBinomial()).fit()\r\n\r\nTypeError: __init__() missing 2 required positional arguments: 'endog' and 'exog'\r\n```\r\n\r\nHowever, it is just a syntax problem caused from my preference, not an internal function problem.","we keep it open as a FAQ","note\r\nthe answer here is outdated for scale handling and does not point to fixed dispersion coefficient\r\nhttps:\/\/stackoverflow.com\/questions\/42277532\/python-negative-binomial-regression-results-dont-match-those-from-r","I am also receiving an error `invalid value encountered in log a1 * np.log(a1) + y * np.log(mu)` when running ZeroInflatedNegativeBinomialP, which uses NegativeBinomialP. Has this been fixed? I thought about swapping it out for NegativeBinomial (non-generalized), which does work.","@Jfhawkin\r\nThat's not really related to this issue.\r\nZINB needs over dispersion relative to Poisson and excess zeros relative. If the data does not have both of them, then ZINB will not be able to estimate the model, parameter estimates will be outside the supported range.\r\nThere can also be other numerical problems mainly in the optimization.\r\n\r\nIf you can provide a full example that we can run, then open a new issue for this. Then I can look at the details."],"labels":["comp-genmod","comp-discrete","FAQ"]},{"title":"ENH: add inference for variance without normality assumptions","body":"first version, mainly bonett for variance confidence interval using kurtosis\r\nwritten while reading references in 8261\r\n\r\nissues:\r\n#8261 inference for variance\r\n#8286 kurtosis and jarque-bera\r\n#8289 inference for correlation (just references without details)\r\n\r\ncurrent plan\r\n\r\nI just want to get the basic versions in mainly bonett, plain kurtosi-adjusted and traditional inference under normality assumptions.\r\nThere are too many options for additional variations of methods, e.g. for kurtosis small sample and bias corrections, and there is no obvious winner for those.\r\n\r\ncases:\r\n\r\n- one-sample\r\n- variances_2indep \r\n  - ratio: ftest and exp (as in bonnet)\r\n  - diff not a large literature, confint based on MOVER, but doesn't easily translate to hypothesis test (would need inversion of confint)\r\n  - tost: not implemented yet, a few references are available\r\n- power: just the basics, mainly based on NCSS\/PASS (but that only includes methods based on normality assumption of data)\r\n\r\nI would like to get some \"score\" versions (variance of test statistic base on null assumption), but there isn't much literature on it, confint requires derivation and needs to be verified by Monte Carlo.\r\nCurrent versions and almost all the literature uses estimated variance in variance of test statistic, i.e. is wald-type.\r\n\r\nHowever, exp transformation of variance ratio removes the variance from the variance of the test statistic. variance only enters through the kurtosis estimate.\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":["no unit tests yet, I might have some test cases in notebooks.\r\n\r\nI would like to merge the basic version with experimental api for 0.14, without restrictions on backwards compatibility in future changes except for basic methods like \"bonett\".\r\n\r\n"],"labels":["type-enh","comp-stats"]},{"title":"ENH: tools reusable local (kernel) density estimation function - variance of quantiles","body":"It would be useful to have standalone functions for local kde or similar density estimates as used in QuantReg.\r\n\r\nmy current case:\r\nquantile based skew and kurtosis estimates have variance that depends on the unknow local pdf value similar to how cov_params is computed in QuantReg.\r\nno reference right now, I saw it in something by Gastwirth (IIRC) in context of #8286 and #8261\r\n(however for those cases nonparametric standard errors seem to be very large and not very useful for getting good confidence intervals of quantile based kurtosis)\r\n\r\ncurrently we don't have standalone functions that could be easily reused for other cases  in stats or robust. \r\n(usecase in robust would be for mquantile link function as approximation to L1-type \"check\" function)\r\n\r\nAlso there might be other approaches that don't user kernels for estimating the density at just one point.\r\ne.g. higher order polynomials extension to gaussian or similar, or maybe the \"differencing\" approximation could be used.\r\n\r\n\r\n\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-stats","comp-tools"]},{"title":"SUMM\/ENH: one correlation coefficient, test, confint  robust to nonnormality","body":"related to #8261  inference for variance robust to nonormality using skew and kurtosis\r\nrelated #6696 for multivariate case which already includes options for using 4th moments\r\n\r\na similar restriction of standard tests applies to inference for correlation coefficient\r\n\r\nAFAIU from skimming parts of motivation for confint for pearson correlation coefficient:\r\nUnder null of no-correlation the standard test statistic is correct, but does not hold under nonzero correlation, i.e. alternatives or confidence intervals away from zero need different distribution\/standard errors.\r\n\r\n\r\ncurrently just parking some references\r\nI haven't looked at those and selection mainly based on abstracts\r\n\r\nBishara, Anthony J., and James B. Hittner. 2012. \u201cTesting the Significance of a Correlation with Nonnormal Data: Comparison of Pearson, Spearman, Transformation, and Resampling Approaches.\u201d Psychological Methods 17 (3): 399\u2013417. https:\/\/doi.org\/10.1037\/a0028087.\r\n\r\n\u2014\u2014\u2014. 2015. \u201cReducing Bias and Error in the Correlation Coefficient Due to Nonnormality.\u201d Educational and Psychological Measurement 75 (5): 785\u2013804. https:\/\/doi.org\/10.1177\/0013164414557639.\r\n\r\nBishara, Anthony J., Jiexiang Li, and Thomas Nash. 2018. \u201cAsymptotic Confidence Intervals for the Pearson Correlation via Skewness and Kurtosis.\u201d British Journal of Mathematical and Statistical Psychology 71 (1): 167\u201385. https:\/\/doi.org\/10.1111\/bmsp.12113.\r\n\r\nBonett, Douglas G., and Thomas A. Wright. 2000. \u201cSample Size Requirements for Estimating Pearson, Kendall and Spearman Correlations.\u201d Psychometrika 65 (1): 23\u201328. https:\/\/doi.org\/10.1007\/BF02294183.\r\n\r\nDUNCAN, G. T., and M. W. J. LAYARD. 1973. \u201cA Monte-Carlo Study of Asymptotically Robust Tests for Correlation Coefficients.\u201d Biometrika 60 (3): 551\u201358. https:\/\/doi.org\/10.1093\/biomet\/60.3.551.\r\n\r\nHawkins, D. L. 1989. \u201cUsing U Statistics to Derive the Asymptotic Distribution of Fisher\u2019s Z Statistic.\u201d The American Statistician 43 (4): 235\u201337. https:\/\/doi.org\/10.2307\/2685369.\r\n\r\nHu, Xinjie, Aekyung Jung, and Gengsheng Qin. 2020. \u201cInterval Estimation for the Correlation Coefficient.\u201d The American Statistician 74 (1): 29\u201336. https:\/\/doi.org\/10.1080\/00031305.2018.1437077.\r\n\r\nPuth, Marie-Therese, Markus Neuh\u00e4user, and Graeme D. Ruxton. 2014. \u201cEffective Use of Pearson\u2019s Product\u2013Moment Correlation Coefficient.\u201d Animal Behaviour 93 (July): 183\u201389. https:\/\/doi.org\/10.1016\/j.anbehav.2014.05.003.\r\n\r\n\u2014\u2014\u2014. 2015. \u201cEffective Use of Spearman\u2019s and Kendall\u2019s Correlation Coefficients for\u00a0Association between Two Measured Traits.\u201d Animal Behaviour 102 (April): 77\u201384. https:\/\/doi.org\/10.1016\/j.anbehav.2015.01.010.\r\n\r\nRuscio, John. 2008. \u201cConstructing Confidence Intervals for Spearman\u2019s Rank Correlation with Ordinal Data: A Simulation Study Comparing Analytic and Bootstrap Methods.\u201d Journal of Modern Applied Statistical Methods 7 (2). https:\/\/doi.org\/10.22237\/jmasm\/1225512360.\r\n\r\n\r\n\r\n","comments":[],"labels":["type-enh","comp-stats"]},{"title":"ENH: score- LM-test under misspecified alternative","body":"related to #2041\r\nI need to look at the \"misspecified alternative\" case again.\r\nI'm not sure the current score test works this way for the \"vectorized\" version (not joint test of variable addition)\r\n\r\nmain old reference is Bera and Yoon\r\n\r\nrecent article, there are a few more but I have not looked at them\r\n\r\nextension to nonlinear hypothesis tests in GMM\r\n\r\nBera, Anil, Gabriel Montes-Rojas, Walter Sosa-Escudero, and Javier Alejo. 2021. \u201cTests for Nonlinear Restrictions under Misspecified Alternatives with an Application to Testing Rational Expectation Hypotheses.\u201d The Econometrics Journal 24 (1): 41\u201357. https:\/\/doi.org\/10.1093\/ectj\/utaa010.\r\n\r\n\r\nmy current context #8286\r\nJarque-Bera joint normality (skew plus kurtosis) test versus separate skew and kurtosis tests\r\n\r\nBera, Anil K., and Gamini Premaratne. 2001. \u201cAdjusting the Tests for Skewness and Kurtosis for Distributional Misspecifications.\u201d SSRN Scholarly Paper 304465. Rochester, NY: Social Science Research Network. https:\/\/doi.org\/10.2139\/ssrn.304465.\r\n\r\nIt's only a working paper. I didn't see it referenced in the kurtosis literature, a only partially looked at literature for skewness\/symmetry tests.\r\n**update** I found the published version\r\nPremaratne, Gamini, and Anil K. Bera. 2017. \u201cAdjusting the Tests for Skewness and Kurtosis for Distributional Misspecifications.\u201d Communications in Statistics - Simulation and Computation 46 (5): 3599\u20133613. https:\/\/doi.org\/10.1080\/03610918.2014.988254.\r\nand related, an older article for skewness using higher moments for nonnormality\r\nGodfrey, L. G., and C. D. Orme. 1991. \u201cTesting for Skewness of Regression Disturbances.\u201d Economics Letters 37 (1): 31\u201334. https:\/\/doi.org\/10.1016\/0165-1765(91)90238-G.\r\n\r\nThere a many more traditional examples for Bera\/Yoon robust testing, e.g. i.i.d. versus correlation and\/or heteroscedasticity\r\n\r\n\r\nThe point here: \r\nCheck and implement the generic version in score_test and the cm-tests (conditional moment tests).\r\n\r\nOne implementation problem is that the references partial out the extra effects, while I work mostly with the joint matrix versions.\r\ni.e. the question is which matrices are inverted to get the cov_params of the moment conditions.\r\nI'm not sure the current version of the score_test does this because it looked like it tests the test separately for each variable addition.\r\n(Maybe variable addition in OLS doesn't have the misspecified alternative problem. ???)\r\n\r\nalso related:\r\nIn (almost sure independence) screening, we add several variables at once to capture multicolinearities.\r\n\r\n**update**\r\n\r\npartially reading Godfrey-Orme 2001 again\r\nAFAIU based on the quick read, we need a one-step estimate to capture a possible parameter change from other restrictions that might also not hold (misspecified alternative) when evaluating the test for the parameter under consideration.\r\n(I didn't look at Godfrey Orme 1996 again, both first added to Zotero in 2013)\r\n\r\nIt seems to be essentially the same as C(alpha) tests #2052, 2052 has link to #2050 (one-step estimators)\r\n\r\nGodfrey, Leslie G., and Chris D. Orme. 1996. \u201cOn the Behavior of Conditional Moment Tests in the Presence of Unconsidered Local Alternatives.\u201d International Economic Review 37 (2): 263\u201381. https:\/\/doi.org\/10.2307\/2527323.\r\n\r\nGodfrey, L.G., and C.D. Orme. 2001. \u201cOn Improving the Robustness and Reliability of Rao\u2019s Score Test.\u201d Journal of Statistical Planning and Inference 97 (1): 153\u201376. https:\/\/doi.org\/10.1016\/S0378-3758(00)00351-7.\r\n\r\n\r\nGodfrey-Orme 2001 p.159\r\n\"The use of <theta_restricted> as the first-stage estimator to derive robust tests is acceptable when, as\r\nin this paper, only local departures from the null are considered in asymptotic analyses.\"\r\n\r\nThey refer to other articles for case when the other parameter restriction might deviate by a large, O(1), amount. Then sqrt(n) local alternative is not appropriate. (This seems to be allowed in C(alpha) tests.)\r\n\r\nAlso, they include the m-estimator sandwich form of the robust score test e.g. equ. (3.11)","comments":["One more:\r\n\r\nL. G. Godfrey & M. R. Veall (2000) Alternative approaches to testing by variable addition, Econometric Reviews, 19:2, 241-261, DOI: 10.1080\/07474930008800470\r\nhttps:\/\/doi.org\/10.1080\/07474930008800470\r\n\r\nIt looks like variable addition robust to misspecified alternative is just wald test on full model, i.e. auxiliary model with added variables, in their example for omitted variable (RESET test), structural break (Chow test), and autocorrelation  (LM test).\r\n\r\n(one-step in linear model is fully unrestricted estimate)","This is a possible target for 0.15 #8217\r\n\r\n- for linear model we could combine current diagnostics as in Godfrey Veall   (IIUC, then this should be easy)\r\n- for MLE:  Poisson would be a good candidate for applying a generic test method, i.e. specification tests based on variable addition score test, e.g. zero-inflation and reset\r\n- M-estimator, sandwiches: I'm not sure how easy this is. maybe based on Godfrey\/Orme 2001\r\n- possibly use wald test for variable addition instead of score test. This would provide inference automatically with current wald and t tests methods.\r\n\r\ne.g. #5564\r\n\r\n","puzzling related\r\nPremaratne, Bera inhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/8286#issuecomment-1135958111\r\nno skewness robust to misspecified no excess kurtosis.\r\n\r\nIn there derivation of score test, they don't use directly something like Bera, Yoon. They just use the robust cov_type for score_test (referencing White for the general case, which I think is the same as what I implemented based on Boos)\r\n\r\naside:\r\n for kurtosis they don't actually use the score_test, To get an additional (b1 or m3) term they use the variance of the kurtosis estimate directly, not the one implied by the pearson system alternative to normality.\r\n"],"labels":["type-enh","comp-stats","topic-diagnostic"]},{"title":"ENH: robust jarque-bera normality (skew and kurtosis) tests, including multivariate","body":"related\r\n#8274\r\n#8269\r\n#8261\r\n\r\nI don't actually want normality tests, I want information about skew and kurtosis. (e.g. inference for variance depends on 4th moment but not on other features of a distribution, joint mean-variance inference depends on 3rd and 4th moments).\r\nHowever the test statistics are still derived under normality assumptions (relevant for higher than 4th moments)\r\nThe likely target is `variance_moments` module from #8261 for univariate case.\r\nThe multivariate versions (like Doornik-Hansen #382) can go into some some multivariate diagnostic.\r\n\r\nsome references (just what I saw right now)\r\n\r\nStehl\u00edk et al. 2012 has mean, trimmed mean and median as location parameter, has a large class of robustified jarque-bera type tests.\r\nincludes proof that they all converge to the corresponding k-th moment, with asymptotic chi2 for JB type statistics.\r\nSo far I have kurtosis based on mean and trimmed mean in my draft version for #8261 \r\n\r\nBonett for variance of variance only changes the center in the computation of the 4th moment. This is not outlier robustness.\r\nOutlier robust estimates and tests as in Stehlik also trim the moment computation itself.\r\n\r\nKim, Namhyun. 2015. \u201cTests Based on Skewness and Kurtosis for Multivariate Normality.\u201d Communications for Statistical Applications and Methods 22 (4): 361\u201375. https:\/\/doi.org\/10.5351\/CSAM.2015.22.4.361.\r\n\r\n\u2014\u2014\u2014. 2016. \u201cA Robustified Jarque\u2013Bera Test for Multivariate Normality.\u201d Economics Letters 140 (March): 48\u201352. https:\/\/doi.org\/10.1016\/j.econlet.2016.01.007.\r\n\r\nStehl\u00edk, Milan, Zden\u011bk Fabi\u00e1n, and Lubo\u0161 St\u0159elec. 2012. \u201cSmall Sample Robust Testing for Normality against Pareto Tails.\u201d Communications in Statistics - Simulation and Computation 41 (7): 1167\u201394. https:\/\/doi.org\/10.1080\/03610918.2012.625849.\r\n\r\n\r\n","comments":["Notes\r\n\r\n- jarque bera uses weighted average of \"raw\" skew and kurtosis, weights correspond to standard error of estimate under normality\r\n- Our omnitest which uses scipy normaltest uses Agostini correction and unweighted sum, and directly delegates to scipy's skewtest and kurtosistest for the test statistic.\r\n- scipy skewtest and kurtosistest have the `alternative` option\r\n- scipy skew and kurtosis functions have a `bias` correction option\r\n","quick read through (referenced in #8288\r\n\r\nGamini Premaratne & Anil K. Bera (2017) Adjusting the tests for skewness\r\nand kurtosis for distributional misspecifications, Communications in Statistics - Simulation and\r\nComputation, 46:5, 3599-3613, DOI: 10.1080\/03610918.2014.988254\r\nhttps:\/\/doi.org\/10.1080\/03610918.2014.988254\r\n\r\nlooks good and doesn't look too difficult to implement, includes skewness test and kurtosis test without imposing the other.\r\n\r\nstandard error for skew test imposes symmetric odd moments (3rth and 5th moment are zero) in Null, but does not restrict kurtosis.\r\njarque-bera standard errors for skewness are too small for longer, heavier tailed distributions like t(7) and overreject in those cases.\r\n"],"labels":["type-enh","comp-stats"]},{"title":"QUESTION: Should the design for aic\/bic reported by OLS be changed","body":"AIC is a relative measure used for model selection - and for feature selection in particular. It's confusing that AIC can be reported by a single model - it is useless and misleading for feature selection task.\r\nRight approach for AIC feature selection algorithm implies to compute variance on all features included and then (for each subset of features) to compute model log likelihood using that variance (computed on all features included). If user does not know that, then he can solve feature selection task by computing AIC on individual models, which is absolutely wrong for feature selection (but still looks intuitively correct from API point of view).\r\n\r\n#### Example of code that reproduces wrong solution for feature selection task:\r\n\r\n```python\r\nimport pandas as pd\r\nimport statsmodels.api as sm\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import train_test_split\r\nfrom itertools import combinations\r\n\r\nX, y = make_classification(n_samples=2000, n_features=10, n_informative=5, n_redundant=5,\r\n                           flip_y=0.05, random_state=0)\r\n\r\nX = pd.DataFrame(X, columns=list(range(10)))\r\ny = pd.Series(y)\r\n\r\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, train_size=1500, random_state=1)\r\n\r\nbest_feature_set = []\r\nbest_aic = 0\r\n\r\nfor i in range(train_X.shape[1]):\r\n    for subset in combinations(train_X.columns.tolist(), i+1):\r\n\r\n        model = sm.OLS(train_y, train_X[list(subset)])\r\n        \r\n        this_aic = model.fit().aic\r\n        \r\n        if this_aic > best_aic:\r\n            best_feature_set = subset\r\n            best_aic = this_aic\r\n            \r\nprint(\"best feature set is:\", best_feature_set, \" with AIC \", best_aic)\r\n```","comments":["Absolutely agreed, statsmodels is just about the worst library ever written in python and should be considered an insult to the python community. Usually when code doesn't work it's a bug, but in this case it's not simply a failure to work, the code gives completely wrong results leading to catastrophic consequences. Imagine getting on a plane which instead of flying would do the function of a cinema. You would watch a movie, thinking you traveled to another country, while you're simply sitting on your ass all day only to see some crappy film of the same quality as this awful code of statsmodels.","Our aic and bic are the standard definition for loglikelihood based information criteria.\r\n\r\nThese information criteria **can** be compared across models as long as they have the same endogenous target variable.\r\n\r\nI'm not sure what you are saying, but there are other selection criteria besides the standard information criteria.  But if we don't have those, then it's a missing enhancement and not a problem with the definition of aic, bic, ...\r\n\r\n(Another definition for aic and bic in the linear regression case is based on residual sum of squares available in `eval_measures` module. However, those definitions don't include the full loglikelihood and therefore are not comparable across models anymore, just for feature selection within a model class.)\r\n\r\nFor example, penalization parameter selection using aic with effective degrees of freedom is our main choice for selecting the penalization parameter in our GAM.","> AIC is a relative measure used for model selection - and for feature selection in particular. It's confusing that AIC can be reported by a single model - it is useless and misleading for feature selection task.\r\n\r\nYes, it is not particularly meaningful for a single model.  IT is attached to results to let users capture its value so that they can use it further down the line.  It is printed as part of a summary so that users who which do perform interactive modeling can rapidly compare AIC values simply by checking the console and doing some mental math. \r\n\r\nThere is some expectation that users have a reasonable understanding of statistics. While ti tries to make reasonable default choices (e.g., homoskedastic errors for regression), it needs to present a wide range of features to let users write custom code to accomplish their goals.\r\n\r\n\r\nThere are a couple of mistakes in your code.  \r\n\r\n1. AIC should be as small as possible, so the check should be `this_aic < best_aic`. \r\n2. AIC can take any value, so `best_aic = np.inf` should be used to initialize the algo.\r\n"],"labels":["maybe_close","question"]}]